@Article{Kobayashi2004,
  author   = {Hideki Kobayashi and Mads Kærn and Michihiro Araki and Kristy Chung and Timothy S. Gardner and Charles R. Cantor and James J. Collins},
  journal  = {Proceedings of the National Academy of Sciences of the United States of America},
  title    = {Programmable cells: Interfacing natural and engineered gene networks},
  year     = {2004},
  note     = {https://europepmc.org/articles/pmc420408?pdf=render},
  number   = {22},
  pages    = {8414-8419},
  volume   = {101},
  abstract = {Novel cellular behaviors and characteristics can be obtained by coupling engineered gene networks to the cell's natural regulatory circuitry through appropriately designed input and output interfaces. Here, we demonstrate how an engineered genetic circuit can be used to construct cells that respond to biological signals in a predetermined and programmable fashion. We employ a modular design strategy to create Escherichia coli strains where a genetic toggle switch is interfaced with: (i) the SOS signaling pathway responding to DNA damage, and (ii) a transgenic quorum sensing signaling pathway from Vibrio fischeri. The genetic toggle switch endows these strains with binary response dynamics and an epigenetic inheritance that supports a persistent phenotypic alteration in response to transient signals. These features are exploited to engineer cells that form biofilms in response to DNA-damaging agents and cells that activate protein synthesis when the cell population reaches a critical density. Our work represents a step toward the development of "plug-and-play" genetic circuitry that can be used to create cells with programmable behaviors.},
  doi      = {10.1073/pnas.0402940101},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1010407598},
}

@Article{Hasty2002,
  author   = {Jeff Hasty and David McMillen and J. J. Collins},
  journal  = {Nature},
  title    = {Engineered gene circuits},
  year     = {2002},
  number   = {6912},
  pages    = {224-230},
  volume   = {420},
  abstract = {A central focus of postgenomic research will be to understand how cellular phenomena arise from the connectivity of genes and proteins. This connectivity generates molecular network diagrams that resemble complex electrical circuits, and a systematic understanding will require the development of a mathematical framework for describing the circuitry. From an engineering perspective, the natural path towards such a framework is the construction and analysis of the underlying submodules that constitute the network. Recent experimental advances in both sequencing and genetic engineering have made this approach feasible through the design and implementation of synthetic gene networks amenable to mathematical modelling and quantitative analysis. These developments have signalled the emergence of a gene circuit discipline, which provides a framework for predicting and evaluating the dynamics of cellular processes. Synthetic gene networks will also lead to new logical forms of cellular control, which could have important applications in functional genomics, nanotechnology, and gene and cell therapy.},
  doi      = {10.1038/nature01257},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1022279877},
}

@Article{Martin2003,
  author   = {Vincent J J Martin and Douglas J Pitera and Sydnor T Withers and Jack D Newman and Jay D Keasling},
  journal  = {Nature Biotechnology},
  title    = {Engineering a mevalonate pathway in Escherichia coli for production of terpenoids},
  year     = {2003},
  number   = {7},
  pages    = {796-802},
  volume   = {21},
  abstract = {Isoprenoids are the most numerous and structurally diverse family of natural products. Terpenoids, a class of isoprenoids often isolated from plants, are used as commercial flavor and fragrance compounds and antimalarial or anticancer drugs. Because plant tissue extractions typically yield low terpenoid concentrations, we sought an alternative method to produce high-value terpenoid compounds, such as the antimalarial drug artemisinin, in a microbial host. We engineered the expression of a synthetic amorpha-4,11-diene synthase gene and the mevalonate isoprenoid pathway from Saccharomyces cerevisiae in Escherichia coli. Concentrations of amorphadiene, the sesquiterpene olefin precursor to artemisinin, reached 24 μg caryophyllene equivalent/ml. Because isopentenyl and dimethylallyl pyrophosphates are the universal precursors to all isoprenoids, the strains developed in this study can serve as platform hosts for the production of any terpenoid compound for which a terpene synthase gene is available.},
  doi      = {10.1038/nbt833},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1022297654},
}

@Article{Canton2008,
  author   = {Barry Canton and Anna Labno and Drew Endy},
  journal  = {Nature Biotechnology},
  title    = {Refinement and standardization of synthetic biological parts and devices},
  year     = {2008},
  number   = {7},
  pages    = {787-793},
  volume   = {26},
  abstract = {The ability to quickly and reliably engineer many-component systems from libraries of standard interchangeable parts is one hallmark of modern technologies. Whether the apparent complexity of living systems will permit biological engineers to develop similar capabilities is a pressing research question. We propose to adapt existing frameworks for describing engineered devices to biological objects in order to (i) direct the refinement and use of biological 'parts' and 'devices', (ii) support research on enabling reliable composition of standard biological parts and (iii) facilitate the development of abstraction hierarchies that simplify biological engineering. We use the resulting framework to describe one engineered biological device, a genetically encoded cell-cell communication receiver named BBa_F2620. The description of the receiver is summarized via a 'datasheet' similar to those widely used in engineering. The process of refinement and characterization leading to the BBa_F2620 datasheet may serve as a starting template for producing many standardized genetically encoded objects.},
  doi      = {10.1038/nbt1413},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1021275892},
}

@Article{Sprinzak2005,
  author   = {David Sprinzak and Michael B. Elowitz},
  journal  = {Nature},
  title    = {Reconstruction of genetic circuits},
  year     = {2005},
  number   = {7067},
  pages    = {443-448},
  volume   = {438},
  abstract = {The complex genetic circuits found in cells are ordinarily studied by analysis of genetic and biochemical perturbations. The inherent modularity of biological components like genes and proteins enables a complementary approach: one can construct and analyse synthetic genetic circuits based on their natural counterparts. Such synthetic circuits can be used as simple in vivo models to explore the relation between the structure and function of a genetic circuit. Here we describe recent progress in this area of synthetic biology, highlighting newly developed genetic components and biological lessons learned from this approach.},
  doi      = {10.1038/nature04335},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1048602129},
}

@Article{Gardner2000,
  author   = {Timothy S. Gardner and Charles R. Cantor and James J. Collins},
  journal  = {Nature},
  title    = {Construction of a genetic toggle switch in Escherichia coli},
  year     = {2000},
  number   = {6767},
  pages    = {339-342},
  volume   = {403},
  abstract = {It has been proposed1 that gene-regulatory circuits with virtually any desired property can be constructed from networks of simple regulatory elements. These properties, which include multistability and oscillations, have been found in specialized gene circuits such as the bacteriophage λ switch2 and the Cyanobacteria circadian oscillator3. However, these behaviours have not been demonstrated in networks of non-specialized regulatory components. Here we present the construction of a genetic toggle switch—a synthetic, bistable gene-regulatory network—in Escherichia coli and provide a simple theory that predicts the conditions necessary for bistability. The toggle is constructed from any two repressible promoters arranged in a mutually inhibitory network. It is flipped between stable states using transient chemical or thermal induction and exhibits a nearly ideal switching threshold. As a practical device, the toggle switch forms a synthetic, addressable cellular memory unit and has implications for biotechnology, biocomputing and gene therapy.},
  doi      = {10.1038/35002131},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1002786107},
}

@Article{Elowitz2000,
  author   = {Michael B. Elowitz and Stanislas Leibler},
  journal  = {Nature},
  title    = {A synthetic oscillatory network of transcriptional regulators},
  year     = {2000},
  number   = {6767},
  pages    = {335-338},
  volume   = {403},
  abstract = {Networks of interacting biomolecules carry out many essential functions in living cells1, but the ‘design principles’ underlying the functioning of such intracellular networks remain poorly understood, despite intensive efforts including quantitative analysis of relatively simple systems2. Here we present a complementary approach to this problem: the design and construction of a synthetic network to implement a particular function. We used three transcriptional repressor systems that are not part of any natural biological clock3,4,5 to build an oscillating network, termed the repressilator, in Escherichia coli. The network periodically induces the synthesis of green fluorescent protein as a readout of its state in individual cells. The resulting oscillations, with typical periods of hours, are slower than the cell-division cycle, so the state of the oscillator has to be transmitted from generation to generation. This artificial clock displays noisy behaviour, possibly because of stochastic fluctuations of its components. Such ‘rational network design’ may lead both to the engineering of new cellular behaviours and to an improved understanding of naturally occurring networks.},
  doi      = {10.1038/35002125},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1016534270},
}

@Article{Basu2005,
  author   = {Subhayu Basu and Yoram Gerchman and Cynthia H. Collins and Frances H. Arnold and Ron Weiss},
  journal  = {Nature},
  title    = {A synthetic multicellular system for programmed pattern formation},
  year     = {2005},
  note     = {https://authors.library.caltech.edu/records/mnhp6-w5y05/files/nature03461-s1.pdf?download=1},
  number   = {7037},
  pages    = {1130-1134},
  volume   = {434},
  abstract = {Pattern formation is in the genesMulticellular organisms, and some single-celled organisms, are capable of producing predetermined patterns. Patterning is key to developmental processes, and is also relevant to tissue engineering and biomaterials design. Basu et al. describe a new synthetic multicellular system in which cells are genetically programmed to form patterns on a surface based on cell–cell communication. Genetic circuits were constructed from well defined simple parts in bacteria that integrate transcriptional regulation with cell–cell signalling elements. These circuits transform a lawn of undifferentiated cells into two-dimensional patterns that resemble a bullseye, ellipse, heart and clover.},
  doi      = {10.1038/nature03461},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1001000963},
}

@Article{Ro2006,
  author   = {Dae-Kyun Ro and Eric M. Paradise and Mario Ouellet and Karl J. Fisher and Karyn L. Newman and John M. Ndungu and Kimberly A. Ho and Rachel A. Eachus and Timothy S. Ham and James Kirby and Michelle C. Y. Chang and Sydnor T. Withers and Yoichiro Shiba and Richmond Sarpong and Jay D. Keasling},
  journal  = {Nature},
  title    = {Production of the antimalarial drug precursor artemisinic acid in engineered yeast},
  year     = {2006},
  number   = {7086},
  pages    = {940-943},
  volume   = {440},
  abstract = {Battling malariaDrug-resistant strains of the malaria parasite are widespread, and as a result mortality due to malaria has increased significantly in recent years. Artemisinin, isolated from the herb Artemisia annua (sweet wormwood), is one drug that shows a high efficacy in killing multi-resistant strains of the parasite. The drug is extremely expensive, and high demand has led to a shortage of artemisinin, available only by extraction from the plant source. Ro et al. now report the development of a yeast strain engineered to carry a cytochrome P450 monooxygenase from A. annua that can produce the drug precursor, artemisinic acid. Artemisinin can be synthesized from this precursor. If the efficiency of this process can be improved, this engineered yeast strain has the potential to alleviate the drug shortage.},
  doi      = {10.1038/nature04640},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1006120611},
}

@Article{Gibson2008,
  author   = {Daniel G. Gibson and Gwynedd A. Benders and Cynthia Andrews-Pfannkoch and Evgeniya A. Denisova and Holly Baden-Tillson and Jayshree Zaveri and Timothy B. Stockwell and Anushka Brownley and David W. Thomas and Mikkel A. Algire and Chuck Merryman and Lei Young and Vladimir N. Noskov and John I. Glass and J. Craig Venter and Clyde A. Hutchison and Hamilton O. Smith},
  journal  = {Science},
  title    = {Complete Chemical Synthesis, Assembly, and Cloning of a Mycoplasma genitalium Genome},
  year     = {2008},
  number   = {5867},
  pages    = {1215-1220},
  volume   = {319},
  abstract = {We have synthesized a 582,970-base pair Mycoplasma genitalium genome. This synthetic genome, named M. genitalium JCVI-1.0, contains all the genes of wild-type M. genitalium G37 except MG408, which was disrupted by an antibiotic marker to block pathogenicity and to allow for selection. To identify the genome as synthetic, we inserted "watermarks" at intergenic sites known to tolerate transposon insertions. Overlapping "cassettes" of 5 to 7 kilobases (kb), assembled from chemically synthesized oligonucleotides, were joined by in vitro recombination to produce intermediate assemblies of approximately 24 kb, 72 kb ("1/8 genome"), and 144 kb ("1/4 genome"), which were all cloned as bacterial artificial chromosomes in Escherichia coli. Most of these intermediate clones were sequenced, and clones of all four 1/4 genomes with the correct sequence were identified. The complete synthetic genome was assembled by transformation-associated recombination cloning in the yeast Saccharomyces cerevisiae, then isolated and sequenced. A clone with the correct sequence was identified. The methods described here will be generally useful for constructing large DNA molecules from chemically synthesized pieces and also from combinations of natural and synthetic DNA segments.},
  doi      = {10.1126/science.1151721},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1009412636},
}

@Article{Sismour2005,
  author   = {A Michael Sismour and Steven A Benner},
  journal  = {Expert Opinion on Biological Therapy},
  title    = {Synthetic biology},
  year     = {2005},
  number   = {11},
  pages    = {1409-1414},
  volume   = {5},
  abstract = {Chemistry is a broadly powerful discipline in contemporary science because it has the ability to create new forms of the matter that it studies. By doing so, chemistry can test models that connect molecular structure to behaviour without having to rely on what nature has provided. This creation, known as 'synthesis', began to be applied to living systems in the 1980s as recombinant DNA technologies allowed biologists to deliberately change the molecular structure of the microbes that they studied, and automated chemical synthesis of DNA became widely available to support these activities. The impact of the information that has emerged has made biologists aware of a truism that has long been known in chemistry: synthesis drives discovery and understanding in ways that analysis cannot. Synthetic biology is now setting an ambitious goal: to recreate in artificial systems the emergent properties found in natural biology. By doing so, it is advancing our understanding of the molecular basis of genetics in ways that analysis alone cannot. More practically, it has yielded artificial genetic systems that improve the healthcare of some 400,000 Americans annually. Synthetic biology is now set to take the next step, to create artificial Darwinian systems by direct construction. Supported by the National Science Foundation as part of its Chemical Bonding program, this work cannot help but generate clarity in our understanding of how biological systems work.},
  doi      = {10.1517/14712598.5.11.1409},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1067589570},
}

@Article{Endy2005,
  author   = {Drew Endy},
  journal  = {Nature},
  title    = {Foundations for engineering biology},
  year     = {2005},
  number   = {7067},
  pages    = {449-453},
  volume   = {438},
  abstract = {Engineered biological systems have been used to manipulate information, construct materials, process chemicals, produce energy, provide food, and help maintain or enhance human health and our environment. Unfortunately, our ability to quickly and reliably engineer biological systems that behave as expected remains quite limited. Foundational technologies that make routine the engineering of biology are needed. Vibrant, open research communities and strategic leadership are necessary to ensure that the development and application of biological technologies remains overwhelmingly constructive.},
  doi      = {10.1038/nature04342},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1025786828},
}

@Article{Elowitz2002,
  author   = {Michael B. Elowitz and Arnold J. Levine and Eric D. Siggia and Peter S. Swain},
  journal  = {Science},
  title    = {Stochastic Gene Expression in a Single Cell},
  year     = {2002},
  note     = {https://authors.library.caltech.edu/records/wsymf-b6c81/files/ElowitzSOM.pdf?download=1},
  number   = {5584},
  pages    = {1183-1186},
  volume   = {297},
  abstract = {Clonal populations of cells exhibit substantial phenotypic variation. Such heterogeneity can be essential for many biological processes and is conjectured to arise from stochasticity, or noise, in gene expression. We constructed strains of Escherichia coli that enable detection of noise and discrimination between the two mechanisms by which it is generated. Both stochasticity inherent in the biochemical process of gene expression (intrinsic noise) and fluctuations in other cellular components (extrinsic noise) contribute substantially to overall variation. Transcription rate, regulatory dynamics, and genetic factors control the amplitude of noise. These results establish a quantitative foundation for modeling noise in genetic networks and reveal how low intracellular copy numbers of molecules can fundamentally limit the precision of gene regulation.},
  doi      = {10.1126/science.1070919},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1021654569},
}

@Article{Isaacs2006,
  author   = {Farren J Isaacs and Daniel J Dwyer and James J Collins},
  journal  = {Nature Biotechnology},
  title    = {RNA synthetic biology},
  year     = {2006},
  number   = {5},
  pages    = {545-554},
  volume   = {24},
  abstract = {RNA molecules play important and diverse regulatory roles in the cell by virtue of their interaction with other nucleic acids, proteins and small molecules. Inspired by this natural versatility, researchers have engineered RNA molecules with new biological functions. In the last two years efforts in synthetic biology have produced novel, synthetic RNA components capable of regulating gene expression in vivo largely in bacteria and yeast, setting the stage for scalable and programmable cellular behavior. Immediate challenges for this emerging field include determining how computational and directed-evolution techniques can be implemented to increase the complexity of engineered RNA systems, as well as determining how such systems can be broadly extended to mammalian systems. Further challenges include designing RNA molecules to be sensors of intracellular and environmental stimuli, probes to explore the behavior of biological networks and components of engineered cellular control systems.},
  doi      = {10.1038/nbt1208},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1026899122},
}

@Article{Smith2003,
  author   = {Hamilton O. Smith and Clyde A. Hutchison and Cynthia Pfannkoch and J. Craig Venter},
  journal  = {Proceedings of the National Academy of Sciences of the United States of America},
  title    = {Generating a synthetic genome by whole genome assembly: φX174 bacteriophage from synthetic oligonucleotides},
  year     = {2003},
  note     = {https://europepmc.org/articles/pmc307586?pdf=render},
  number   = {26},
  pages    = {15440-15445},
  volume   = {100},
  abstract = {We have improved upon the methodology and dramatically shortened the time required for accurate assembly of 5- to 6-kb segments of DNA from synthetic oligonucleotides. As a test of this methodology, we have established conditions for the rapid (14-day) assembly of the complete infectious genome of bacteriophage X174 (5386 bp) from a single pool of chemically synthesized oligonucleotides. The procedure involves three key steps: (i). gel purification of pooled oligonucleotides to reduce contamination with molecules of incorrect chain length, (ii). ligation of the oligonucleotides under stringent annealing conditions (55 degrees C) to select against annealing of molecules with incorrect sequences, and (iii). assembly of ligation products into full-length genomes by polymerase cycling assembly, a nonexponential reaction in which each terminal oligonucleotide can be extended only once to produce a full-length molecule. We observed a discrete band of full-length assemblies upon gel analysis of the polymerase cycling assembly product, without any PCR amplification. PCR amplification was then used to obtain larger amounts of pure full-length genomes for circularization and infectivity measurements. The synthetic DNA had a lower infectivity than natural DNA, indicating approximately one lethal error per 500 bp. However, fully infectious X174 virions were recovered after electroporation into Escherichia coli. Sequence analysis of several infectious isolates verified the accuracy of these synthetic genomes. One such isolate had exactly the intended sequence. We propose to assemble larger genomes by joining separately assembled 5- to 6-kb segments; approximately 60 such segments would be required for a minimal cellular genome.},
  doi      = {10.1073/pnas.2237126100},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1028250908},
}

@Article{Tigges2009,
  author   = {Marcel Tigges and Tatiana T. Marquez-Lago and Jörg Stelling and Martin Fussenegger},
  journal  = {Nature},
  title    = {A tunable synthetic mammalian oscillator},
  year     = {2009},
  number   = {7227},
  pages    = {309-312},
  volume   = {457},
  abstract = {An around-the-clock synthetic gene circuitSynthetic gene circuits producing oscillating outputs have been developed only in bacteria, and it has been difficult to make them reliable even there. Tigges et al. have now developed a synthetic sense–antisense circuit in mammalian cells that produces autonomous, self-sustained and tunable cyclic gene expression. This synthetic mammalian clock will be of use in the study of oscillations in mammalian cells, and may improve our understanding of the circadian clock and associated pathologies.},
  doi      = {10.1038/nature07616},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1036497140},
}

@Article{Guet2002,
  author   = {Călin C. Guet and Michael B. Elowitz and Weihong Hsing and Stanislas Leibler},
  journal  = {Science},
  title    = {Combinatorial Synthesis of Genetic Networks},
  year     = {2002},
  number   = {5572},
  pages    = {1466-1470},
  volume   = {296},
  abstract = {A central problem in biology is determining how genes interact as parts of functional networks. Creation and analysis of synthetic networks, composed of well-characterized genetic elements, provide a framework for theoretical modeling. Here, with the use of a combinatorial method, a library of networks with varying connectivity was generated in Escherichia coli. These networks were composed of genes encoding the transcriptional regulators LacI, TetR, and lambda CI, as well as the corresponding promoters. They displayed phenotypic behaviors resembling binary logical circuits, with two chemical "inputs" and a fluorescent protein "output." Within this simple system, diverse computational functions arose through changes in network connectivity. Combinatorial synthesis provides an alternative approach for studying biological networks, as well as an efficient method for producing diverse phenotypes in vivo.},
  doi      = {10.1126/science.1067407},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1004364700},
}

@Article{Tian2004,
  author   = {Jingdong Tian and Hui Gong and Nijing Sheng and Xiaochuan Zhou and Erdogan Gulari and Xiaolian Gao and George Church},
  journal  = {Nature},
  title    = {Accurate multiplex gene synthesis from programmable DNA microchips},
  year     = {2004},
  number   = {7020},
  pages    = {1050-1054},
  volume   = {432},
  abstract = {Testing the many hypotheses from genomics and systems biology experiments demands accurate and cost-effective gene and genome synthesis. Here we describe a microchip-based technology for multiplex gene synthesis. Pools of thousands of ‘construction’ oligonucleotides and tagged complementary ‘selection’ oligonucleotides are synthesized on photo-programmable microfluidic chips1, released, amplified and selected by hybridization to reduce synthesis errors ninefold. A one-step polymerase assembly multiplexing reaction assembles these into multiple genes. This technology enabled us to synthesize all 21 genes that encode the proteins of the Escherichia coli 30S ribosomal subunit, and to optimize their translation efficiency in vitro through alteration of codon bias. This is a significant step towards the synthesis of ribosomes in vitro and should have utility for synthetic biology in general.},
  doi      = {10.1038/nature03151},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1052700485},
}

@Article{Becskei2000,
  author   = {Attila Becskei and Luis Serrano},
  journal  = {Nature},
  title    = {Engineering stability in gene networks by autoregulation},
  year     = {2000},
  number   = {6786},
  pages    = {590-593},
  volume   = {405},
  abstract = {The genetic and biochemical networks which underlie such things as homeostasis in metabolism and the developmental programs of living cells, must withstand considerable variations and random perturbations of biochemical parameters1,2,3. These occur as transient changes in, for example, transcription, translation, and RNA and protein degradation. The intensity and duration of these perturbations differ between cells in a population4. The unique state of cells, and thus the diversity in a population, is owing to the different environmental stimuli the individual cells experience and the inherent stochastic nature of biochemical processes (for example, refs 5 and 6). It has been proposed, but not demonstrated, that autoregulatory, negative feedback loops in gene circuits provide stability7, thereby limiting the range over which the concentrations of network components fluctuate. Here we have designed and constructed simple gene circuits consisting of a regulator and transcriptional repressor modules in Escherichia coli and we show the gain of stability produced by negative feedback.},
  doi      = {10.1038/35014651},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1033127471},
}

@Article{Cello2002,
  author   = {Jeronimo Cello and Aniko V. Paul and Eckard Wimmer},
  journal  = {Science},
  title    = {Chemical Synthesis of Poliovirus cDNA: Generation of Infectious Virus in the Absence of Natural Template},
  year     = {2002},
  note     = {https://www.science.org/cms/asset/dc8f0193-bfec-42ae-b327-7fbd89b8c426/pap.pdf},
  number   = {5583},
  pages    = {1016-1018},
  volume   = {297},
  abstract = {Full-length poliovirus complementary DNA (cDNA) was synthesized by assembling oligonucleotides of plus and minus strand polarity. The synthetic poliovirus cDNA was transcribed by RNA polymerase into viral RNA, which translated and replicated in a cell-free extract, resulting in the de novo synthesis of infectious poliovirus. Experiments in tissue culture using neutralizing antibodies and CD155 receptor-specific antibodies and neurovirulence tests in CD155 transgenic mice confirmed that the synthetic virus had biochemical and pathogenic characteristics of poliovirus. Our results show that it is possible to synthesize an infectious agent by in vitro chemical-biochemical means solely by following instructions from a written sequence.},
  doi      = {10.1126/science.1072266},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1036018285},
}

@Article{Kramer2004,
  author   = {Beat P Kramer and Alessandro Usseglio Viretta and Marie Daoud-El Baba and Dominique Aubel and Wilfried Weber and Martin Fussenegger},
  journal  = {Nature Biotechnology},
  title    = {An engineered epigenetic transgene switch in mammalian cells},
  year     = {2004},
  number   = {7},
  pages    = {867-870},
  volume   = {22},
  abstract = {In multicellular systems cell identity is imprinted by epigenetic regulation circuits, which determine the global transcriptome of adult cells in a cell phenotype–specific manner1,2,3. By combining two repressors, which control each other's expression, we have developed a mammalian epigenetic circuitry able to switch between two stable transgene expression states after transient administration of two alternate drugs. Engineered Chinese hamster ovary cells (CHO-K1) showed toggle switch–specific expression profiles of a human glycoprotein in culture, as well as after microencapsulation and implantation into mice. Switch dynamics and expression stability could be predicted with mathematical models. Epigenetic transgene control through toggle switches is an important tool for engineering artificial gene networks in mammalian cells.},
  doi      = {10.1038/nbt980},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1042528786},
}

@Article{Yokobayashi2002,
  author   = {Yohei Yokobayashi and Ron Weiss and Frances H. Arnold},
  journal  = {Proceedings of the National Academy of Sciences of the United States of America},
  title    = {Directed evolution of a genetic circuit},
  year     = {2002},
  note     = {https://europepmc.org/articles/pmc139187?pdf=render},
  number   = {26},
  pages    = {16587-16591},
  volume   = {99},
  abstract = {The construction of artificial networks of transcriptional control elements in living cells represents a new frontier for biological engineering. However, biological circuit engineers will have to confront their inability to predict the precise behavior of even the most simple synthetic networks, a serious shortcoming and challenge for the design and construction of more sophisticated genetic circuitry in the future. We propose a combined rational and evolutionary design strategy for constructing genetic regulatory circuits, an approach that allows the engineer to fine-tune the biochemical parameters of the networks experimentally in vivo. By applying directed evolution to genes comprising a simple genetic circuit, we demonstrate that a nonfunctional circuit containing improperly matched components can evolve rapidly into a functional one. In the process, we generated a library of genetic devices with a range of behaviors that can be used to construct more complex circuits.},
  doi      = {10.1073/pnas.252535999},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1009258787},
}

@Article{Anderson2005,
  author   = {J. Christopher Anderson and Elizabeth J. Clarke and Adam P. Arkin and Christopher A. Voigt},
  journal  = {Journal of Molecular Biology},
  title    = {Environmentally Controlled Invasion of Cancer Cells by Engineered Bacteria},
  year     = {2005},
  number   = {4},
  pages    = {619-627},
  volume   = {355},
  abstract = {Bacteria can sense their environment, distinguish between cell types, and deliver proteins to eukaryotic cells. Here, we engineer the interaction between bacteria and cancer cells to depend on heterologous environmental signals. We have characterized invasin from Yersinia pseudotuburculosis as an output module that enables Escherichia coli to invade cancer-derived cells, including HeLa, HepG2, and U2OS lines. To environmentally restrict invasion, we placed this module under the control of heterologous sensors. With the Vibrio fischeri lux quorum sensing circuit, the hypoxia-responsive fdhF promoter, or the arabinose-inducible araBAD promoter, the bacteria invade cells at densities greater than 10(8)bacteria/ml, after growth in an anaerobic growth chamber or in the presence of 0.02% arabinose, respectively. In the process, we developed a technique to tune the linkage between a sensor and output gene using ribosome binding site libraries and genetic selection. This approach could be used to engineer bacteria to sense the microenvironment of a tumor and respond by invading cancerous cells and releasing a cytotoxic agent.},
  doi      = {10.1016/j.jmb.2005.10.076},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1014359908},
}

@Article{Stricker2008,
  author   = {Jesse Stricker and Scott Cookson and Matthew R. Bennett and William H. Mather and Lev S. Tsimring and Jeff Hasty},
  journal  = {Nature},
  title    = {A fast, robust and tunable synthetic gene oscillator},
  year     = {2008},
  note     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6791529},
  number   = {7221},
  pages    = {516-519},
  volume   = {456},
  abstract = {A tunable synthetic gene oscillatorSynthetic biologists aim to apply well-known principles of gene regulation to build living systems with desired properties, but practical applications have been disappointing. Jeff Hasty and colleagues have now combined microfluidics, single-cell microscopy and computational modelling to develop a bacterial gene oscillator that is fast, robust, persistent and whose frequency can be tuned externally. Their combination of experimental and theoretical work reveals a simplified oscillator design without the need for positive feedback.},
  doi      = {10.1038/nature07389},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1028022455},
}

@Article{Purnick2009,
  author   = {Priscilla E. M. Purnick and Ron Weiss},
  journal  = {Nature Reviews Molecular Cell Biology},
  title    = {The second wave of synthetic biology: from modules to systems},
  year     = {2009},
  number   = {6},
  pages    = {410-422},
  volume   = {10},
  abstract = {Key PointsIn the 'first wave' of synthetic biology, researchers developed basic elements and modules that allowed transcriptional, translational and post-translational control of cellular processes.The 'second wave' of synthetic biology, in which researchers must integrate these basic elements and modules to create systems-level circuitry, presents new challenges. Traditional principles for systems engineering must be supplemented with approaches that attend to details of biological systems and contexts.Several groups have developed small synthetic systems, including multicellular synthetic ecosystems in bacteria, yeast and mammalian cells; application-orientated systems, such as bacteria that sense and destroy tumours and organisms that produce drug precursors; and initial development of minimized cellular genomes.Many open questions and challenges remain, including the development of readily characterized, standardized and modular components; substantially reducing or exploiting biological noise in synthetic systems; determining the usefulness of innate or engineered epigenetic cellular functions; improving and creating new computational tools and programming abstractions for modelling and designing more complex systems; and determining safety issues and resolutions for clinical manifestations of synthetic biological systems.},
  doi      = {10.1038/nrm2698},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1046952734},
}

@Article{Gibson2010,
  author   = {Daniel G. Gibson and John I. Glass and Carole Lartigue and Vladimir N. Noskov and Ray-Yuan Chuang and Mikkel A. Algire and Gwynedd A. Benders and Michael G. Montague and Li Ma and Monzia M. Moodie and Chuck Merryman and Sanjay Vashee and Radha Krishnakumar and Nacyra Assad-Garcia and Cynthia Andrews-Pfannkoch and Evgeniya A. Denisova and Lei Young and Zhi-Qing Qi and Thomas H. Segall-Shapiro and Christopher H. Calvey and Prashanth P. Parmar and Clyde A. Hutchison and Hamilton O. Smith and J. Craig Venter},
  journal  = {Science},
  title    = {Creation of a Bacterial Cell Controlled by a Chemically Synthesized Genome},
  year     = {2010},
  number   = {5987},
  pages    = {52-56},
  volume   = {329},
  abstract = {We report the design, synthesis, and assembly of the 1.08-mega-base pair Mycoplasma mycoides JCVI-syn1.0 genome starting from digitized genome sequence information and its transplantation into a M. capricolum recipient cell to create new M. mycoides cells that are controlled only by the synthetic chromosome. The only DNA in the cells is the designed synthetic DNA sequence, including "watermark" sequences and other designed gene deletions and polymorphisms, and mutations acquired during the building process. The new cells have expected phenotypic properties and are capable of continuous self-replication.},
  doi      = {10.1126/science.1190719},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1024208323},
}

@Article{You2004,
  author   = {Lingchong You and Robert Sidney Cox and Ron Weiss and Frances H. Arnold},
  journal  = {Nature},
  title    = {Programmed population control by cell–cell communication and regulated killing},
  year     = {2004},
  number   = {6985},
  pages    = {868-871},
  volume   = {428},
  abstract = {De novo engineering of gene circuits inside cells is extremely difficult1,2,3,4,5,6,7,8,9, and efforts to realize predictable and robust performance must deal with noise in gene expression and variation in phenotypes between cells10,11,12. Here we demonstrate that by coupling gene expression to cell survival and death using cell–cell communication, we can programme the dynamics of a population despite variability in the behaviour of individual cells. Specifically, we have built and characterized a ‘population control’ circuit that autonomously regulates the density of an Escherichia coli population. The cell density is broadcasted and detected by elements from a bacterial quorum-sensing system13,14, which in turn regulate the death rate. As predicted by a simple mathematical model, the circuit can set a stable steady state in terms of cell density and gene expression that is easily tunable by varying the stability of the cell–cell communication signal. This circuit incorporates a mechanism for programmed death in response to changes in the environment, and allows us to probe the design principles of its more complex natural counterparts.},
  doi      = {10.1038/nature02491},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1037881716},
}

@Article{Ellis2009,
  author   = {Tom Ellis and Xiao Wang and James J Collins},
  journal  = {Nature Biotechnology},
  title    = {Diversity-based, model-guided construction of synthetic gene networks with predicted functions},
  year     = {2009},
  note     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2680460},
  number   = {5},
  pages    = {465-471},
  volume   = {27},
  abstract = {Ellis et al. describe a strategy for rationally assembling gene networks with predictable behaviors. Using mathematical models, they predict the responses of complex synthetic gene networks built from quantitatively characterized promoter libraries, and harness these networks to regulate an industrially relevant yeast phenotype.},
  doi      = {10.1038/nbt.1536},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1020751277},
}

@Article{Levskaya2005,
  author   = {Anselm Levskaya and Aaron A. Chevalier and Jeffrey J. Tabor and Zachary Booth Simpson and Laura A. Lavery and Matthew Levy and Eric A. Davidson and Alexander Scouras and Andrew D. Ellington and Edward M. Marcotte and Christopher A. Voigt},
  journal  = {Nature},
  title    = {Engineering Escherichia coli to see light},
  year     = {2005},
  number   = {7067},
  pages    = {441-442},
  volume   = {438},
  abstract = {These smart bacteria ‘photograph’ a light pattern as a high-definition chemical image.},
  doi      = {10.1038/nature04405},
  groups   = {Synthetic Biology},
  url      = {https://app.dimensions.ai/details/publication/pub.1035931199},
}

@Article{pub.1035441940,
  author   = {Paul Oldham and Stephen Hall and Geoff Burton},
  journal  = {PLOS ONE},
  title    = {Synthetic Biology: Mapping the Scientific Landscape},
  year     = {2012},
  note     = {https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0034368&type=printable},
  number   = {4},
  pages    = {e34368},
  volume   = {7},
  abstract = {This article uses data from Thomson Reuters Web of Science to map and analyse the scientific landscape for synthetic biology. The article draws on recent advances in data visualisation and analytics with the aim of informing upcoming international policy debates on the governance of synthetic biology by the Subsidiary Body on Scientific, Technical and Technological Advice (SBSTTA) of the United Nations Convention on Biological Diversity. We use mapping techniques to identify how synthetic biology can best be understood and the range of institutions, researchers and funding agencies involved. Debates under the Convention are likely to focus on a possible moratorium on the field release of synthetic organisms, cells or genomes. Based on the empirical evidence we propose that guidance could be provided to funding agencies to respect the letter and spirit of the Convention on Biological Diversity in making research investments. Building on the recommendations of the United States Presidential Commission for the Study of Bioethical Issues we demonstrate that it is possible to promote independent and transparent monitoring of developments in synthetic biology using modern information tools. In particular, public and policy understanding and engagement with synthetic biology can be enhanced through the use of online interactive tools. As a step forward in this process we make existing data on the scientific literature on synthetic biology available in an online interactive workbook so that researchers, policy makers and civil society can explore the data and draw conclusions for themselves.},
  doi      = {10.1371/journal.pone.0034368},
  groups   = {Synthetic Biology},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1035441940},
}

@Article{pub.1022983337,
  author   = {Martina K. Linnenluecke},
  journal  = {International Journal of Management Reviews},
  title    = {Resilience in Business and Management Research: A Review of Influential Publications and a Research Agenda},
  year     = {2015},
  number   = {1},
  pages    = {4-30},
  volume   = {19},
  abstract = {This paper identifies the development of and gaps in knowledge in business and management research on resilience, based on a systematic review of influential publications among 339 papers, books and book chapters published between 1977 and 2014. Analyzing these records shows that resilience research has developed into five research streams, or lines of enquiry, which view resilience as (1) organizational responses to external threats, (2) organizational reliability, (3) employee strengths, (4) the adaptability of business models or (5) design principles that reduce supply chain vulnerabilities and disruptions. A review of the five streams suggests three key findings: First, resilience has been conceptualized quite differently across studies, meaning that the different research streams have developed their own definitions, theories and understandings of resilience. Second, conceptual similarities and differences among these streams have not yet been explored, nor have insights been gleaned about any possible generalizable principles for developing resilience. Third, resilience has been operationalized quite differently, with few insights into the empirics for detecting resilience to future adversity (or the absence thereof). This paper outlines emerging research trends and pathways for future research, highlighting opportunities to integrate and expand on existing knowledge, as well as avenues for further investigation of resilience in business and management studies.},
  doi      = {10.1111/ijmr.12076},
  groups   = {Resilience in Business and management},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1022983337},
}

@Article{Meyer1982,
  author   = {Alan D. Meyer},
  journal  = {Administrative Science Quarterly},
  title    = {Adapting to environmental jolts.},
  year     = {1982},
  number   = {4},
  pages    = {515-37},
  volume   = {27},
  abstract = {This paper examines organizational adaptations to an environmental jolt--a sudden and unprecedented event (in this case, a doctors' strike)-- that created a natural experiment within a group of hospitals. Although adaptations were diverse and appeared anomalous, they are elucidated by considering the hospitals' antecedent strategies, structures, ideologies, and stockpiles of slack resources. Assessments of the primacy of the antecedents suggest that ideological and strategic variables are better predictors of adaptations to jolts than are structural variables or measures of organizational slack. Although abrupt changes in environments are commonly thought to jeopardize organizations, environmental jolts are found to be ambiguous events that offer propitious opportunities for organizational learning, administrative drama, and introducing unrelated changes.},
  doi      = {10.2307/2392528},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1069908041},
}

@Article{Staw1981,
  author  = {Barry M. Staw and Lance E. Sandelands and Jane E. Dutton},
  journal = {Administrative Science Quarterly},
  title   = {Threat Rigidity Effects in Organizational Behavior: A Multilevel Analysis},
  year    = {1981},
  number  = {4},
  pages   = {501},
  volume  = {26},
  doi     = {10.2307/2392337},
  groups  = {Resilience in Business and management},
  url     = {https://app.dimensions.ai/details/publication/pub.1069907901},
}

@Article{Weick1993,
  author  = {Karl E. Weick and Karlene H. Roberts},
  journal = {Administrative Science Quarterly},
  title   = {Collective Mind in Organizations: Heedful Interrelating on Flight Decks},
  year    = {1993},
  number  = {3},
  pages   = {357},
  volume  = {38},
  doi     = {10.2307/2393372},
  groups  = {Resilience in Business and management},
  url     = {https://app.dimensions.ai/details/publication/pub.1069908683},
}

@Article{Weick1993a,
  author  = {Karl E. Weick},
  journal = {Administrative Science Quarterly},
  title   = {The Collapse of Sensemaking in Organizations: The Mann Gulch Disaster},
  year    = {1993},
  number  = {4},
  pages   = {628},
  volume  = {38},
  doi     = {10.2307/2393339},
  groups  = {Resilience in Business and management},
  url     = {https://app.dimensions.ai/details/publication/pub.1069908660},
}

@Article{Luthans2002,
  author  = {Fred Luthans},
  journal = {Academy of Management Perspectives},
  title   = {Positive organizational behavior: Developing and managing psychological strengths},
  year    = {2002},
  number  = {1},
  pages   = {57-72},
  volume  = {16},
  doi     = {10.5465/ame.2002.6640181},
  groups  = {Resilience in Business and management},
  url     = {https://app.dimensions.ai/details/publication/pub.1072894681},
}

@Article{Rudolph2002,
  author   = {Jenny W. Rudolph and Nelson P. Repenning},
  journal  = {Administrative Science Quarterly},
  title    = {Disaster Dynamics: Understanding the Role of Quantity in Organizational Collapse},
  year     = {2002},
  number   = {1},
  pages    = {1-30},
  volume   = {47},
  abstract = {This article examines the role that the quantity of non-novel events plays in precipitating disaster through the development of a formal (mathematical) system-dynamics model. Building on existing case studies of disaster, we develop a general theory of how an organizational system responds to an on-going stream of non-novel interruptions to existing plans and procedures. We show how an overaccumulation of interruptions can shift an organizational system from a resilient, self-regulating regime, which offsets the effects of this accumulation, to a fragile, self-escalating regime that amplifies them. We offer a new characterization of the conditions under which organizations may be prone to major disasters caused by an accumulation of minor interruptions. Our analysis provides both theoretical insights into the causes of organizational crises and practical suggestions for those charged with preventing them.},
  doi      = {10.2307/3094889},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1070196019},
}

@Article{Coutu2002,
  author   = {Diane L Coutu},
  journal  = {Harvard Business Review},
  title    = {How resilience works.},
  year     = {2002},
  number   = {5},
  pages    = {46-50, 52, 55 passim},
  volume   = {80},
  abstract = {Why do some people bounce back from life's hardships while others despair? HBR senior editor Diane Coutu looks at the nature of individual and organizational resilience, issues that have gained special urgency in light of the recent terrorist attacks, war, and recession. In the business arena, resilience has found its way onto the list of qualities sought in employees. As one of Coutu's interviewees puts it, "More than education, more than experience, more than training, a person's level of resilience will determine who succeeds and who fails." Theories abound about what produces resilience, but three fundamental characteristics seem to set resilient people and companies apart from others. One or two of these qualities make it possible to bounce back from hardship, but true resilience requires all three. The first characteristic is the capacity to accept and face down reality. In looking hard at reality, we prepare ourselves to act in ways that allow us to endure and survive hardships: We train ourselves how to survive before we ever have to do so. Second, resilient people and organizations possess an ability to find meaning in some aspects of life. And values are just as important as meaning; value systems at resilient companies change very little over the long haul and are used as scaffolding in times of trouble. The third building block of resilience is the ability to improvise. Within an arena of personal capabilities or company rules, the ability to solve problems without the usual or obvious tools is a great strength.},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1075056726},
}

@Article{Luthans2002a,
  author   = {Fred Luthans},
  journal  = {Journal of Organizational Behavior},
  title    = {The need for and meaning of positive organizational behavior},
  year     = {2002},
  note     = {https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/job.165},
  number   = {6},
  pages    = {695-706},
  volume   = {23},
  abstract = {Abstract This essay draws from the emerging positive psychology movement and the author's recent articles on the need for and meaning of a positive approach to organizational behavior. Specifically, the argument is made that at this time, the OB field needs a proactive, positive approach emphasizing strengths, rather than continuing in the downward spiral of negativity trying to fix weaknesses. However, to avoid the surface positivity represented by the non‐sustainable best‐sellers, the case is made for positive organizational behavior (POB) to take advantage of the OB field's strength of being theory and research driven. Additional criteria for this version of POB are to identify unique, state‐like psychological capacities that can not only be validly measured, but also be open to development and performance management. Confidence, hope, and resiliency are offered as meeting such POB inclusion criteria. The overall intent of the essay is to generate some positive thinking and excitement for the OB field and ‘hopefully’ stimulate some new theory building, research, and effective application. Copyright © 2002 John Wiley & Sons, Ltd.},
  doi      = {10.1002/job.165},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1030211703},
}

@Article{Kleindorfer2005,
  author   = {Paul R. Kleindorfer and Germaine H. Saad},
  journal  = {Production and Operations Management},
  title    = {Managing Disruption Risks in Supply Chains},
  year     = {2005},
  number   = {1},
  pages    = {53-68},
  volume   = {14},
  abstract = {There are two broad categories of risk affecting supply chain design and management: (1) risks arising from the problems of coordinating supply and demand, and (2) risks arising from disruptions to normal activities. This paper is concerned with the second category of risks, which may arise from natural disasters, from strikes and economic disruptions, and from acts of purposeful agents, including terrorists. The paper provides a conceptual framework that reflects the joint activities of risk assessment and risk mitigation that are fundamental to disruption risk management in supply chains. We then consider empirical results from a rich data set covering the period 1995–2000 on accidents in the U. S. Chemical Industry. Based on these results and other literature, we discuss the implications for the design of management systems intended to cope with supply chain disruption risks.},
  doi      = {10.1111/j.1937-5956.2005.tb00009.x},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1013827626},
}

@Article{Christopher2004,
  author   = {Martin Christopher and Helen Peck},
  journal  = {The International Journal of Logistics Management},
  title    = {Building the Resilient Supply Chain},
  year     = {2004},
  note     = {https://dspace.lib.cranfield.ac.uk/bitstreams/6ce1ceca-e8c6-4b37-8f26-6fdbe3c08a8a/download},
  number   = {2},
  pages    = {1-14},
  volume   = {15},
  abstract = {In today's uncertain and turbulent markets, supply chain vulnerability has become an issue of significance for many companies. As supply chains become more complex as a result of global sourcing and the continued trend to “leaning‐down”, supply chain risk increases. The challenge to business today is to manage and mitigate that risk through creating more resilient supply chains.},
  doi      = {10.1108/09574090410700275},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1008687306},
}

@Article{Hamel2003,
  author   = {Gary Hamel and Liisa Välikangas},
  journal  = {Harvard Business Review},
  title    = {The quest for resilience.},
  year     = {2003},
  number   = {9},
  pages    = {52-63, 131},
  volume   = {81},
  abstract = {In less turbulent times, executives had the luxury of assuming that business models were more or less immortal. Companies always had to work to get better, but they seldom had to get different--not at their core, not in their essence. Today, getting different is the imperative. It's the challenge facing Coca-Cola as it struggles to raise its "share of throat" in noncarbonated beverages. It's the task that bedevils McDonald's as it tries to restart its growth in a burger-weary world. It's the hurdle for Sun Microsystems as it searches for ways to protect its high-margin server business from the Linux onslaught. Continued success no longer hinges on momentum. Rather, it rides on resilience-on the ability to dynamically reinvent business models and strategies as circumstances change. Strategic resilience is not about responding to a onetime crisis or rebounding from a setback. It's about continually anticipating and adjusting to deep, secular trends that can permanently impair the earning power of a core business. It's about having the capacity to change even before the case for change becomes obvious. To thrive in turbulent times, companies must become as efficient at renewal as they are at producing today's products and services. To achieve strategic resilience, companies will have to overcome the cognitive challenge of eliminating denial, nostalgia, and arrogance; the strategic challenge of learning how to create a wealth of small tactical experiments; the political challenge of reallocating financial and human resources to where they can earn the best returns; and the ideological challenge of learning that strategic renewal is as important as optimization.},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1075345922},
}

@Article{2006,
  journal = {Choice},
  title   = {The resilient enterprise: overcoming vulnerability for competitive advantage},
  year    = {2006},
  number  = {06},
  pages   = {43-3481-43-3481},
  volume  = {43},
  doi     = {10.5860/choice.43-3481},
  groups  = {Resilience in Business and management},
  url     = {https://app.dimensions.ai/details/publication/pub.1073407503},
}

@Article{Gittell2006,
  author   = {Jody Hoffer Gittell and Kim Cameron and Sandy Lim and Victor Rivas},
  journal  = {The Journal of Applied Behavioral Science},
  title    = {Relationships, Layoffs, and Organizational Resilience},
  year     = {2006},
  note     = {https://ink.library.smu.edu.sg/soss_research/123},
  number   = {3},
  pages    = {300-329},
  volume   = {42},
  abstract = {The terrorist attacks of September 11, 2001, affected the U.S. airline industry more than almost any other industry. Certain airlines emerged successful and demonstrated remarkable resilience while others languished. This investigation identifies reasons why some airline companies recovered successfully after the attacks while others struggled. Evidence is provided that layoffs after the crisis, although intended to foster recovery, instead inhibited recovery throughout the 4 years after the crisis. But, layoffs after the crisis were strongly correlated with lack of financial reserves and lack of a viable business model prior to the crisis. Digging deeper, the authors find that having a viable business model itself depended on the development and preservation of relational reserves over time. Our model shows that the maintenance of adequate financial reserves enables the preservation of relational reserves and vice versa, contributing to organizational resilience in times of crisis.},
  doi      = {10.1177/0021886306286466},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1031594587},
}

@Article{Luthans2006,
  author   = {Fred Luthans and James B. Avey and Bruce J. Avolio and Steven M. Norman and Gwendolyn M. Combs},
  journal  = {Journal of Organizational Behavior},
  title    = {Psychological capital development: toward a micro‐intervention},
  year     = {2006},
  number   = {3},
  pages    = {387-393},
  volume   = {27},
  abstract = {Abstract After first providing the meaning of psychological capital (PsyCap), we present a micro‐intervention to develop it. Drawn from hope, optimism, efficacy, and resiliency development, this PsyCap Intervention (PCI) is shown to have preliminary support for not only increasing participants' PsyCap, but also financial impact and high return on investment. Copyright © 2006 John Wiley & Sons, Ltd.},
  doi      = {10.1002/job.373},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1031821349},
}

@Article{Craighead2007,
  author   = {Christopher W. Craighead and Jennifer Blackhurst and M. Johnny Rungtusanatham and Robert B. Handfield},
  journal  = {Decision Sciences},
  title    = {The Severity of Supply Chain Disruptions: Design Characteristics and Mitigation Capabilities},
  year     = {2007},
  number   = {1},
  pages    = {131-156},
  volume   = {38},
  abstract = {Supply chain disruptions and the associated operational and financial risks represent the most pressing concern facing firms that compete in today's global marketplace. Extant research has not only confirmed the costly nature of supply chain disruptions but has also contributed relevant insights on such related issues as supply chain risks, vulnerability, resilience, and continuity. In this conceptual note, we focus on a relatively unexplored issue, asking and answering the question of how and why one supply chain disruption would be more severe than another. In doing so, we argue, de facto, that supply chain disruptions are unavoidable and, as a consequence, that all supply chains are inherently risky. Employing a multiple‐method, multiple‐source empirical research design, we derive novel insights, presented as six propositions that relate the severity of supply chain disruptions (i) to the three supply chain design characteristics of density, complexity, and node criticality and (ii) to the two supply chain mitigation capabilities of recovery and warning. These findings not only augment existing knowledge related to supply chain risk, vulnerability, resilience, and business continuity planning but also call into question the wisdom of pursuing such practices as supply base reduction, global sourcing, and sourcing from supply clusters.},
  doi      = {10.1111/j.1540-5915.2007.00151.x},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1047959878},
}

@Article{Youssef2007,
  author   = {Carolyn M. Youssef and Fred Luthans},
  journal  = {Journal of Management},
  title    = {Positive Organizational Behavior in the Workplace},
  year     = {2007},
  number   = {5},
  pages    = {774-800},
  volume   = {33},
  abstract = {Drawing from the foundation of positive psychology and the recently emerging positive organizational behavior, two studies (N = 1,032 and N = 232) test hypotheses on the impact that the selected positive psychological resource capacities of hope, optimism, and resilience have on desired work-related employee outcomes. These outcomes include performance (self-reported in Study 1 and organizational performance appraisals in Study 2), job satisfaction, work happiness, and organizational commitment. The findings generally support that employees' positive psychological resource capacities relate to, and contribute unique variance to, the outcomes. However, hope, and, to a lesser extent, optimism and resilience, do differentially contribute to the various outcomes. Utility analysis supports the practical implications of the study results.},
  doi      = {10.1177/0149206307305562},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1036458898},
}

@Article{Luthans2007,
  author   = {Fred Luthans and Carolyn M. Youssef},
  journal  = {Journal of Management},
  title    = {Emerging Positive Organizational Behavior},
  year     = {2007},
  number   = {3},
  pages    = {321-349},
  volume   = {33},
  abstract = {Although the value of positivity has been assumed over the years, only recently has it become a major focus area for theory building, research, and application in psychology and now organizational behavior. This review article examines, in turn, selected representative positive traits (Big Five personality, core self-evaluations, and character strengths and virtues), positive state-like psychological resource capacities (efficacy, hope, optimism, resiliency, and psychological capital), positive organizations (drawn from positive organization scholarship), and positive behaviors (organizational citizenship and courageous principled action). This review concludes with recommendations for future research and effective application.},
  doi      = {10.1177/0149206307300814},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1048916231},
}

@Article{LUTHANS2007,
  author   = {FRED LUTHANS and BRUCE J. AVOLIO and JAMES B. AVEY and STEVEN M. NORMAN},
  journal  = {Personnel Psychology},
  title    = {POSITIVE PSYCHOLOGICAL CAPITAL: MEASUREMENT AND RELATIONSHIP WITH PERFORMANCE AND SATISFACTION},
  year     = {2007},
  number   = {3},
  pages    = {541-572},
  volume   = {60},
  abstract = {Two studies were conducted to analyze how hope, resilience, optimism, and efficacy individually and as a composite higher‐order factor predicted work performance and satisfaction. Results from Study 1 provided psychometric support for a new survey measure designed to assess each of these 4 facets, as well as a composite factor. Study 2 results indicated a significant positive relationship regarding the composite of these 4 facets with performance and satisfaction. Results from Study 2 also indicated that the composite factor may be a better predictor of performance and satisfaction than the 4 individual facets. Limitations and practical implications conclude the article.},
  doi      = {10.1111/j.1744-6570.2007.00083.x},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1043814808},
}

@Article{Luthans2008,
  author  = {Fred Luthans and James B. Avey and Jaime L. Patera},
  journal = {Academy of Management Learning and Education},
  title   = {Experimental Analysis of a Web-Based Training Intervention to Develop Positive Psychological Capital},
  year    = {2008},
  number  = {2},
  pages   = {209-221},
  volume  = {7},
  doi     = {10.5465/amle.2008.32712618},
  groups  = {Resilience in Business and management},
  url     = {https://app.dimensions.ai/details/publication/pub.1072896311},
}

@Article{Avey2008,
  author   = {James B. Avey and Tara S. Wernsing and Fred Luthans},
  journal  = {The Journal of Applied Behavioral Science},
  title    = {Can Positive Employees Help Positive Organizational Change? Impact of Psychological Capital and Emotions on Relevant Attitudes and Behaviors},
  year     = {2008},
  note     = {http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1031&context=managementfacpub},
  number   = {1},
  pages    = {48-70},
  volume   = {44},
  abstract = {Although much attention has been devoted to understanding employee resistance to change, relatively little research examines the impact that positive employees can have on organizational change. To help fill this need, the authors investigate whether a process of employees' positivity will have an impact on relevant attitudes and behaviors. Specifically, this study surveyed 132 employees from a broad cross-section of organizations and jobs and found: (a) Their psychological capital (a core factor consisting of hope, efficacy, optimism, and resilience) was related to their positive emotions that in turn were related to their attitudes (engagement and cynicism) and behaviors (organizational citizenship and deviance) relevant to organizational change; (b) mindfulness (i.e., heightened awareness) interacted with psychological capital in predicting positive emotions; and (c) positive emotions generally mediated the relationship between psychological capital and the attitudes and behaviors. The implications these findings have for positive organizational change conclude the article.},
  doi      = {10.1177/0021886307311470},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1041282031},
}

@Article{Powley2009,
  author   = {Edward H. Powley},
  journal  = {Human Relations},
  title    = {Reclaiming resilience and safety: Resilience activation in the critical period of crisis},
  year     = {2009},
  number   = {9},
  pages    = {1289-1326},
  volume   = {62},
  abstract = {When external events disrupt the normal flow of organizational and relational routines and practices, an organization’s latent capacity to rebound activates to enable positive adaptation and bounce back. This article examines an unexpected organizational crisis (a shooting and standoff in a business school) and presents a model for how resilience becomes activated in such situations. Three social mechanisms describe resilience activation. Liminal suspension describes how crisis temporarily undoes and alters formal relational structures and opens a temporal space for organization members to form and renew relationships. Compassionate witnessing describes how organization members’ interpersonal connections and opportunities for engagement respond to individuals’ needs. And relational redundancy describes how organization members’ social capital and connections across organizational and functional boundaries activate relational networks that enable resilience. Narrative accounts from the incident support the induced model.},
  doi      = {10.1177/0018726709334881},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1034737787},
}

@Article{Ponomarov2009,
  author   = {Serhiy Y. Ponomarov and Mary C. Holcomb},
  journal  = {The International Journal of Logistics Management},
  title    = {Understanding the concept of supply chain resilience},
  year     = {2009},
  number   = {1},
  pages    = {124-143},
  volume   = {20},
  abstract = {Purpose
                    In the emerging disciplines of risk management and supply chain management, resilience is a relatively undefined concept. The purpose of this paper is to present an integrated perspective on resilience through an extensive review of the literature in a number of disciplines including developmental psychology and ecosystems. In addition, the paper identifies and addresses some of the current theoretical gaps in the existing research.
                  
                  
                    Design/methodology/approach
                    Supply chain resilience has been defined by a number of disciplines. An integrative literature review is conducted in an attempt to integrate existing perspectives. This review also serves as the basis for the development of a conceptual model.
                  
                  
                    Findings
                    The key elements of supply chain resilience and the relationships among them, the links between risks and implications for supply chain management, and the methodologies for managing these key issues are poorly understood. Implications for future research advocate testing the proposed model empirically.
                  
                  
                    Practical implications
                    Supply chain disruptions have adverse effect on both revenue and costs. Resilient supply chains incorporate event readiness, are capable of providing an efficient response, and often are capable of recovering to their original state or even better post the disruptive event.
                  
                  
                    Originality/value
                    Supply chain resilience has yet to be researched from the logistics perspective. Even in well‐developed disciplines the unified theory of resilience is still under development. This research leverages existing knowledge and advances an interdisciplinary understanding of the concept.},
  doi      = {10.1108/09574090910954873},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1020050164},
}

@Article{Avey2009,
  author   = {James B. Avey and Fred Luthans and Susan M. Jensen},
  journal  = {Human Resource Management},
  title    = {Psychological capital: A positive resource for combating employee stress and turnover},
  year     = {2009},
  number   = {5},
  pages    = {677-693},
  volume   = {48},
  abstract = {Abstract Workplace stress is a growing concern for human resource managers. Although considerable scholarly and practical attention has been devoted to stress management over the years, the time has come for new perspectives and research. Drawing from the emerging field of positive organizational behavior, this study offers research findings with implications for combating occupational stress. Specifically, data from a large sample of working adults across a variety of industries suggest that psychological capital (the positive resources of efficacy, hope, optimism, and resilience) may be key to better understanding the variation in perceived symptoms of stress, as well as intentions to quit and job search behaviors. The article concludes with practical strategies aimed at leveraging and developing employees' psychological capital to help them better cope with workplace stress. © 2009 Wiley Periodicals, Inc.},
  doi      = {10.1002/hrm.20294},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1032563790},
}

@Article{Pettit2010,
  author   = {Timothy J. Pettit and Joseph Fiksel and Keely L. Croxton},
  journal  = {Journal of Business Logistics},
  title    = {ENSURING SUPPLY CHAIN RESILIENCE: DEVELOPMENT OF A CONCEPTUAL FRAMEWORK},
  year     = {2010},
  number   = {1},
  pages    = {1-21},
  volume   = {31},
  abstract = {The views expressed in this article are those of the authors and do not necessarily reflect the official policy or position of the Air Force, the Department of Defense, or the U.S. Government. In a world of turbulent change, resilience is a key competency since even the most carefully designed supply chain is susceptible to unforeseen events. This article presents a new Supply Chain Resilience Framework to help businesses deal with change. The conceptual framework is based on extant literature and refined through a focus group methodology. Our findings suggest that supply chain resilience can be assessed in terms of two dimensions: vulnerabilities and capabilities. The Zone of Resilience is defined as the desired balance between vulnerabilities and capabilities, where it is proposed that firms will be the most profitable in the long term. We identified seven vulnerability factors composed of 40 specific attributes and 14 capability factors from 71 attributes that facilitate the measurement of resilience. The article concludes with managerial implications and recommendations for future research.},
  doi      = {10.1002/j.2158-1592.2010.tb00125.x},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1031829691},
}

@Article{Juettner2011,
  author   = {Uta Jüttner and Stan Maklan},
  journal  = {Supply Chain Management An International Journal},
  title    = {Supply chain resilience in the global financial crisis: an empirical study},
  year     = {2011},
  number   = {4},
  pages    = {246-259},
  volume   = {16},
  abstract = {Purpose
                    The objective of this paper is to conceptualise supply chain resilience (SCRES) and to identify and explore empirically its relationship with the related concepts of supply chain vulnerability (SCV) and supply chain risk management (SCRM).
                  
                  
                    Design/methodology/approach
                    From a review of the literature the conceptual domain of SCRES is defined and the proposed relationships with SCRM and SCV are derived. Data from a longitudinal case study with three supply chains are presented to explore the relationship between the concepts in the context of the global financial crisis.
                  
                  
                    Findings
                    The empirical data provide support for a positive impact of supply chain risk (SCR) effect and knowledge management on SCRES and from SCRES on SCV. SCR effect and knowledge management seem to enhance the SCRES by improving the flexibility, visibility, velocity and collaboration capabilities of the supply chain. Thereby, they decrease the SCV in a disruptive risk event. The positive effects manifest themselves in upstream supplier networks of supply chains as well as in distribution channels to the customers.
                  
                  
                    Research limitations/implications
                    The recession caused by the financial crisis has illustrated the importance of SCRES in today's interdependent global economy vividly. However, the concept is still in its infancy and has not received the same attention as its counterparts SCRM and SCV. The study confirms the benefit of resilient supply chains and outlines future research needs.
                  
                  
                    Practical implications
                    The paper identifies which supply chain capabilities can support the containment of disruptions and how these capabilities can be supported by effective SCRM.
                  
                  
                    Originality/value
                    To date, there has been no empirical study which has investigated supply chain resilience in a disruptive global event.},
  doi      = {10.1108/13598541111139062},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1016764583},
}

@Article{Klibi2010,
  author   = {Walid Klibi and Alain Martel and Adel Guitouni},
  journal  = {European Journal of Operational Research},
  title    = {The design of robust value-creating supply chain networks: A critical review},
  year     = {2010},
  number   = {2},
  pages    = {283-293},
  volume   = {203},
  abstract = {This paper discusses Supply Chain Network (SCN) design problem under uncertainty, and presents a critical review of the optimization models proposed in the literature. Some drawbacks and missing aspects in the literature are pointed out, thus motivating the development of a comprehensive SCN design methodology. Through an analysis of supply chains uncertainty sources and risk exposures, the paper reviews key random environmental factors and discusses the nature of major disruptive events threatening SCN. It also discusses relevant strategic SCN design evaluation criteria, and it reviews their use in existing models. We argue for the assessment of SCN robustness as a necessary condition to ensure sustainable value creation. Several definitions of robustness, responsiveness and resilience are reviewed, and the importance of these concepts for SCN design is discussed. This paper contributes to framing the foundations for a robust SCN design methodology.},
  doi      = {10.1016/j.ejor.2009.06.011},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1018062843},
}

@Article{Luthans2010,
  author   = {Fred Luthans and James B. Avey and Bruce J. Avolio and Suzanne J. Peterson},
  journal  = {Human Resource Development Quarterly},
  title    = {The development and resulting performance impact of positive psychological capital},
  year     = {2010},
  number   = {1},
  pages    = {41-67},
  volume   = {21},
  abstract = {Abstract Recently, theory and research have supported psychological capital (PsyCap) as an emerging core construct linked to positive outcomes at the individual and organizational level. However, to date, little attention has been given to PsyCap development through training interventions; nor have there been attempts to determine empirically if such PsyCap development has a causal impact on participants' performance. To fill these gaps we first conducted a pilot test of the PsyCap intervention (PCI) model with a randomized control group design. Next, we conducted a follow‐up study with a cross section of practicing managers to determine if following the training guidelines of the PCI caused the participants' performance to improve. Results provide beginning empirical evidence that short training interventions such as PCI not only may be used to develop participants' psychological capital, but can also lead to an improvement in their on‐the‐job performance. The implications these findings have for human resource development and performance management conclude the article.},
  doi      = {10.1002/hrdq.20034},
  groups   = {Resilience in Business and management},
  url      = {https://app.dimensions.ai/details/publication/pub.1030632837},
}

@Article{pub.1110896992,
  author   = {Chester J Donnally and Alexander J Butler and Augustus J Rush and Kevin J Bondar and Michael Y Wang and Frank J Eismont},
  journal  = {Journal of Spine Surgery},
  title    = {The most influential publications in cervical myelopathy},
  year     = {2018},
  note     = {https://doi.org/10.21037/jss.2018.09.08},
  number   = {4},
  pages    = {770-779},
  volume   = {4},
  abstract = {Management of cervical myelopathy (CM) has continued to evolve through a better understanding of the long-term outcomes of this diagnosis as well as improved diagnostic guidelines. More recent literature continues to expand the field, but certain publications can be distinguished from others due to their lasting impact. Using the Clarivate Analytics Web of Science, search phrases were used to identify publications pertaining to CM. The fifty most cited articles were isolated. The frequency of citations, year of publication, country of origin, journal of publication, level of evidence (LOE), article type, as well as contributing authors and institutions were recorded. We also highlighted the five most cited articles (per year) from the past 10 years. Publications included ranged from 1952-2011, with the plurality of articles published during 2000-2009 (n=21; 42%). The most cited paper was Hillibrand's 1999 reporting of adjacent segment disease rates following cervical fusions, followed by Hirabayashi's 1983 review of his cervical laminoplasty outcomes. The third most cited was Brain's 1952 review of the manifestations of cervical spondylosis. <i>Spine</i> contributed the most publications (n=26; 52%). A LOE of III was the most common (n=30; 60%). Clinical outcome articles were the most frequent type (n=28; 56%). Osaka University (Japan) and Kazou Yonenobu had the most contributions. Ames or Fehlings were the first or last author in each of the five most influential articles from the past 10 years. This bibliometric citation analysis identifies the most influential articles regarding CM. There are few publications with a high LOE, and more high powered studies are needed. Knowledge of these "classic" publications allows for a better overall understanding of the diagnosis, treatment, and future direction of research of CM.},
  doi      = {10.21037/jss.2018.09.08},
  groups   = {Cervical Myelopathy},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1110896992},
}

@Article{Satomi1994,
  author   = {Kazuhiko Satomi and Yukimi Nishu and Touru Kohno and Kiyoshi Hirabayashi},
  journal  = {Spine},
  title    = {Long-Term Follow-up Studies of Open-Door Expansive Laminoplasty for Cervical Stenotic Myelopathy},
  year     = {1994},
  number   = {5},
  pages    = {507-510},
  volume   = {19},
  abstract = {Follow-up at an average time of 7.8 years postoperatively on open-door expansive laminoplasty (EL) was carried out to determine the long-term results of surgery. Thirty-three patients had ossification of the posterior longitudinal ligament and 18 had cervical spondylotic myelopathy. The average age at operation was 54.7 years. Japanese Orthopaedic Association scores and recovery rates increased during the 3 years after surgery and then plateaued. Radiographically, average spinal canal diameter remained enlarged past 5 years' follow-up. Factors leading to worsening of clinical symptoms included age greater than 60 years (4 patients), loss of sagittal canal diameter (2 patients), progression of ossification (4 patients), and minor trauma (1 patient). Postoperative motor paresis due to C5 and C6 root damage recovered to 4 (manual muscle testing) in all patients within 6 years. The conclusion is that open-door EL is safe and leads to good results that are maintained for over 5 years.},
  doi      = {10.1097/00007632-199403000-00003},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1037654624},
}

@Article{Goffin2004,
  author   = {Jan Goffin and Eric Geusens and Nicolaas Vantomme and Els Quintens and Yannic Waerzeggers and Bart Depreitere and Frank Van Calenbergh and Johan van Loon},
  journal  = {Clinical Spine Surgery A Spine Publication},
  title    = {Long-Term Follow-Up After Interbody Fusion of the Cervical Spine},
  year     = {2004},
  note     = {https://lirias.kuleuven.be/bitstream/123456789/18631/1/Goffin_LongTerm_full.pdf},
  number   = {2},
  pages    = {79-85},
  volume   = {17},
  abstract = {The aim of this work was to add to the body of data on the frequency and severity of degenerative radiographic findings at adjacent levels after anterior cervical interbody fusion and on their clinical impact and to contribute to the insights about their pathogenesis. One hundred eighty patients who were treated by anterior cervical interbody fusion and who had a follow-up of >60 months were clinically and radiologically examined by independent investigators. For all patients, the long-term Odom score was compared with the score as obtained 6 weeks after surgery. For myelopathic cases, both the late Nurick and the Odom score were compared with the initial postoperative situation. For the adjacent disc levels, a radiologic "degeneration score" was defined and assessed both initially and at long-term follow-up. At late follow-up after anterior cervical interbody fusion, additional radiologic degeneration at the adjacent disc levels was found in 92% of the cases, often reflecting a clinical deterioration. The severity of this additional degeneration correlated with the time interval since surgery. The similarity of progression to degeneration between younger trauma patients and older nontrauma patients suggests that both the biomechanical impact of the interbody fusion and the natural progression of pre-existing degenerative disease act as triggering factors for adjacent level degeneration.},
  doi      = {10.1097/00024720-200404000-00001},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1007826013},
}

@Article{Emery1998,
  author   = {S E Emery and H H Bohlman and M J Bolesta and P K Jones},
  journal  = {Journal of Bone and Joint Surgery},
  title    = {Anterior cervical decompression and arthrodesis for the treatment of cervical spondylotic myelopathy. Two to seventeen-year follow-up.},
  year     = {1998},
  number   = {7},
  pages    = {941-51},
  volume   = {80},
  abstract = {We reviewed the cases of 108 patients with cervical spondylotic myelopathy who had been managed with anterior decompression and arthrodesis at our institution. Operative treatment consisted of anterior discectomy, partial corpectomy, or subtotal corpectomy at one level or more, followed by placement of autogenous bone graft from the iliac crest or the fibula. At the latest follow-up examination, thirty-eight of the eighty-two patients who had had a preoperative gait abnormality had a normal gait, thirty-three had an improvement in gait, six had no change, four had improvement and later deterioration, and one had a worse gait abnormality. Of the eighty-seven patients who had had a preoperative motor deficit, fifty-four had complete recovery; twenty-six, partial recovery; six, no change; and one had a worse deficit. The average grade according to the system of Nurick improved from 2.4 preoperatively to 1.2 (range, 0.0 to 5.0) postoperatively. A pseudarthrosis developed in sixteen patients, thirteen of whom had had a multilevel discectomy. Only one of thirty-eight arthrodeses that had been performed with use of a fibular strut graft was followed by a non-union. An unsatisfactory outcome with respect to pain was significantly associated with pseudarthrosis (p < 0.001). The development of complications other than non-union was associated with a history of one previous operative procedure or more (p = 0.005). Recurrent myelopathy was rare, but when it occurred it was associated with a pseudarthrosis or stenosis at a new level. The strongest predictive factor for recovery from myelopathy was the severity of the myelopathy before the operative intervention--that is, better preoperative neurological function was associated with a better neurological outcome. Anterior decompression and arthrodesis with autogenous bone-grafting can be performed safely, and is associated with a high rate of neurological recovery, functional improvement, and pain relief, in patients who have cervical spondylotic myelopathy.},
  doi      = {10.2106/00004623-199807000-00002},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1068893080},
}

@Article{Baba1993,
  author   = {Hisatoshi Baba and Nobuaki Furusawa and Shinichi Imura and Norio Kawahara and Hiroyuki Tsuchiya and Katsuro Tomita},
  journal  = {Spine},
  title    = {Late Radiographic Findings After Anterior Cervical Fusion for Spondylotic Myeloradiculopathy},
  year     = {1993},
  number   = {15},
  pages    = {2167-2173},
  volume   = {18},
  abstract = {A retrospective study was performed to evaluate the radiographic changes that occurred at spinal levels adjacent to fused vertebrae after anterior cervical fusion. One hundred six patients with cervical spondylotic myeloradiculopathy (88 men, 18 women) were followed for an average of 8.5 years. The average age at follow-up was 64 years. Forty-two patients underwent a single-level fusion, 52 had a two-level fusion, and 12 had three levels fused. Seventeen patients who underwent additional surgery after anterior fusion also were reviewed, with an average follow-up period of 2.9 years. Postoperatively, cervical flexion-extension resulted in significantly increased movement about the vertebral interspace at the upper adjacent level. An increment of posterior slip of the vertebra immediately above the fusion level, with associated spinal canal compromise of less than 12 mm, significantly affected neurologic results. Patients with multilevel fusions notably exhibited these radiographic abnormalities at adjacent levels. Spinal canal stenosis, when associated with dynamic spinal canal stenosis in the vertebra above the fusion level, affected late neurologic results. Results of salvage laminoplasty were not satisfactory. Unnecessarily extended longer fusion must be avoided.},
  doi      = {10.1097/00007632-199311000-00004},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1034074696},
}

@Article{Fountas2007,
  author   = {Kostas N. Fountas and Eftychia Z. Kapsalaki and Leonidas G. Nikolakakos and Hugh F. Smisson and Kim W. Johnston and Arthur A. Grigorian and Gregory P. Lee and Joe S. Robinson},
  journal  = {Spine},
  title    = {Anterior Cervical Discectomy and Fusion Associated Complications},
  year     = {2007},
  number   = {21},
  pages    = {2310-2317},
  volume   = {32},
  abstract = {STUDY DESIGN: Retrospective review study with literature review.
OBJECTIVE: The goal of our current study is to raise awareness on complications associated with anterior cervical discectomy and fusion (ACDF) and their early detection and proper management.
SUMMARY OF BACKGROUND DATA: It is known that ACDF constitutes one of the most commonly performed spinal procedures. Its outcome is quite satisfactory in the majority of cases. However, occasional complications can become troublesome, and in rare circumstances, catastrophic. Although there are several case reports describing such complications, their rate of occurrence is generally underreported, and data regarding their exact incidence in large clinical series are lacking. Meticulous knowledge of potential intraoperative and postoperative ACDF-related complications is of paramount importance so as to avoid them whenever possible, as well as to successfully and safely manage them when they are inevitable.
METHODS: In a retrospective study, 1015 patients undergoing first-time ACDF for cervical radiculopathy and/or myelopathy due to degenerative disc disease and/or cervical spondylosis were evaluated. A standard Smith-Robinson approach was used in all our patients, while an autologous or allograft was used, with or without a plate. Operative reports, hospital and outpatient clinic charts, and radiographic studies were reviewed for procedure-related complications. Mean follow-up time was 26.4 months.
RESULTS: The mortality rate in our current series was 0.1% (1 of 1015 patients, death occurred secondary to an esophageal perforation). Our overall morbidity rate was 19.3% (196 of 1015 patients). The most common complication was the development of isolated postoperative dysphagia, which observed in 9.5% of our patients. Postoperative hematoma occurred in 5.6%, but required surgical intervention in only 2.4% of our cases. Symptomatic recurrent laryngeal nerve palsy occurred in 3.1% of our cases. Dural penetration occurred in 0.5%, esophageal perforation in 0.3%, worsening of preexisting myelopathy in 0.2%, Horner's syndrome in 0.1%, instrumentation backout in 0.1%, and superficial wound infection in 0.1% of our cases.
CONCLUSION: Meticulous knowledge of the ACDF-associated complications allows for their proper management. Postoperative dysphagia, hematoma, and recurrent laryngeal nerve palsy were the most common complications in our series. Management of complications was successful in the vast majority of our cases.},
  doi      = {10.1097/brs.0b013e318154c57e},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1024110809},
}

@Article{BRAIN1952,
  author  = {W. RUSSELL BRAIN and DOUGLAS NORTHFIELD and MARCIA WILKINSON},
  journal = {Brain},
  title   = {THE NEUROLOGICAL MANIFESTATIONS OF CERVICAL SPONDYLOSIS},
  year    = {1952},
  number  = {2},
  pages   = {187-225},
  volume  = {75},
  doi     = {10.1093/brain/75.2.187},
  groups  = {Cervical Myelopathy},
  url     = {https://app.dimensions.ai/details/publication/pub.1059442903},
}

@Article{HIRABAYASHI1983,
  author   = {KIYOSHI HIRABAYASHI and KENICHI WATANABE and KOICHI WAKANO and NOBUMASA SUZUKI and KAZUHIKO SATOMI and YOSHIAKI ISHII},
  journal  = {Spine},
  title    = {Expansive Open-Door Laminoplasty for Cervical Spinal Stenotic Myelopathy},
  year     = {1983},
  number   = {7},
  pages    = {693-699},
  volume   = {8},
  abstract = {Although the operative results have been improving since the air drill was introduced for cervical laminectomy instead of an ordinary rongeur, post-laminectomy complications, such as postoperative fragility of the cervical spine to acute neck trauma, posterior spur formation at the vertebral body, and malalignment of the lateral curvature have still remained as unsolved problems. In order to avoid these disadvantages, a new surgical technique called "expansive open-door laminoplasty" was devised by the author in 1977, which is relatively easier, safer, and better than the ordinary laminectomy from the standpoint of structural mechanics of the cervical spine. The operative procedure is described in detail. Operative results in the patients with cervical OPLL, spondylosis, and canal stenosis were satisfactory, and optimal widening of the AP diameter of the spinal canal is considered to be over 4 mm. From this procedure a bilateral, open-door laminoplasty has been devised for extensive exploration at the intradural space.},
  doi      = {10.1097/00007632-198310000-00003},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1038222550},
}

@Article{Hilibrand1999,
  author   = {A S Hilibrand and G D Carlson and M A Palumbo and P K Jones and H H Bohlman},
  journal  = {Journal of Bone and Joint Surgery},
  title    = {Radiculopathy and myelopathy at segments adjacent to the site of a previous anterior cervical arthrodesis.},
  year     = {1999},
  note     = {https://jdc.jefferson.edu/cgi/viewcontent.cgi?article=1007&context=rothman_institute},
  number   = {4},
  pages    = {519-28},
  volume   = {81},
  abstract = {BACKGROUND: We studied the incidence, prevalence, and radiographic progression of symptomatic adjacent-segment disease, which we defined as the development of new radiculopathy or myelopathy referable to a motion segment adjacent to the site of a previous anterior arthrodesis of the cervical spine.
METHODS: A consecutive series of 374 patients who had a total of 409 anterior cervical arthrodeses for the treatment of cervical spondylosis with radiculopathy or myelopathy, or both, were followed for a maximum of twenty-one years after the operation. The annual incidence of symptomatic adjacent-segment disease was defined as the percentage of patients who had been disease-free at the start of a given year of follow-up in whom new disease developed during that year. The prevalence was defined as the percentage of all patients in whom symptomatic adjacent-segment disease developed within a given period of follow-up. The natural history of the disease was predicted with use of a Kaplan-Meier survivorship analysis. The hypothesis that new disease at an adjacent level is more likely to develop following a multilevel arthrodesis than it is following a single-level arthrodesis was tested with logistic regression.
RESULTS: Symptomatic adjacent-segment disease occurred at a relatively constant incidence of 2.9 percent per year (range, 0.0 to 4.8 percent per year) during the ten years after the operation. Survivorship analysis predicted that 25.6 percent of the patients (95 percent confidence interval, 20 to 32 percent) who had an anterior cervical arthrodesis would have new disease at an adjacent level within ten years after the operation. There were highly significant differences among the motion segments with regard to the likelihood of symptomatic adjacent-segment disease (p<0.0001); the greatest risk was at the interspaces between the fifth and sixth and between the sixth and seventh cervical vertebrae. Contrary to our hypothesis, we found that the risk of new disease at an adjacent level was significantly lower following a multilevel arthrodesis than it was following a single-level arthrodesis (p<0.001). More than two-thirds of all patients in whom the new disease developed had failure of nonoperative management and needed additional operative procedures.
CONCLUSIONS: Symptomatic adjacent-segment disease may affect more than one-fourth of all patients within ten years after an anterior cervical arthrodesis. A single-level arthrodesis involving the fifth or sixth cervical vertebra and preexisting radiographic evidence of degeneration at adjacent levels appear to be the greatest risk factors for new disease. Therefore, we believe that all degenerated segments causing radiculopathy or myelopathy should be included in an anterior cervical arthrodesis. Although our findings suggest that symptomatic adjacent-segment disease is the result of progressive spondylosis, patients should be informed of the substantial possibility that new disease will develop at an adjacent level over the long term.},
  doi      = {10.2106/00004623-199904000-00009},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1068893336},
}

@Article{Herman1994,
  author   = {J M Herman and V K Sonntag},
  journal  = {Journal of Neurosurgery},
  title    = {Cervical corpectomy and plate fixation for postlaminectomy kyphosis.},
  year     = {1994},
  number   = {6},
  pages    = {963-70},
  volume   = {80},
  abstract = {Between 1987 and 1991, 20 patients with symptomatic postlaminectomy kyphosis were treated with anterior decompression, bone graft, and anterior cervical plate. The patients were predominantly male (14:6) with a mean age of 58 years. The initial laminectomy was performed for either spondylosis (80%) or spinal tumor (20%). All patients had anterior compressive pathology, which was associated with instability (45%), neck pain (75%), myeloradiculopathy (90%), or severe neck deformity (30%). The mean degree of kyphosis was 38 degrees. Treatment consisted of a trial of cervical traction (75%), anterior corpectomy (95%), intersegmental decompression (5%), bone fusion (100%), and fixation with either Caspar (85%) or Synthes (15%) anterior plating at a mean of 3.8 levels. Halo fixation was used in 10% of patients. Postoperative complications included vocal cord paresis (15%), pneumonia (10%), wound dehiscence (5%), and screw pull-out (5%). At follow-up evaluation, a mean of 28 months after treatment, all patients had a solid fusion and a mean curvature improvement to 16 degrees residual kyphosis. Neurologically, 10% were cured, 55% were improved and returned to premorbid function, 30% were stable, and 5% had late progression. These data suggest that immediate fixation with anterior plating facilitates solid fusion, maintains spinal curvature, and promotes neurological improvement.},
  doi      = {10.3171/jns.1994.80.6.0963},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1071097240},
}

@Article{Majd1999,
  author   = {Mohammad E. Majd and Mukta Vadhva and Richard T. Holt},
  journal  = {Spine},
  title    = {Anterior Cervical Reconstruction Using Titanium Cages With Anterior Plating},
  year     = {1999},
  number   = {15},
  pages    = {1604},
  volume   = {24},
  abstract = {STUDY DESIGN: A preliminary outcome assessment study of titanium cage implants with anterior cervical plating in anterior cervical reconstruction.
OBJECTIVES: To evaluate the efficacy and safety of using titanium cage implants and anterior plating in cervical reconstruction.
SUMMARY OF BACKGROUND DATA: Anterior decompression and interbody fusion is a widely accepted surgical treatment for patients with cervical spondylosis. Tricortical iliac crest autograft has been the gold standard but is associated with morbidity at the bone graft donor site, whereas allograft fibula is associated with pseudarthrosis. Problems such as pseudarthrosis, graft collapse, and extrusion still persist with the accepted method of harvesting and implanting bone autografts.
METHODS: Thirty-four patients were treated by channel corpectomy followed by placement of a titanium cage packed with autogenous bone graft from the vertebral bodies to reconstruct the anterior column. An anterior cervical plate was added in 30 of 34 cases that involved decompression of two or more levels. The follow-up period ranged from 24 to 56 months, with an average follow-up period of 32 months, and included examination and radiography.
RESULTS: Six months after surgery, there was radiographic evidence of fusion in 97% of the patients. Eighty-eight percent of the patients (30 of 34) did not experience any complications (neither cage dislodgment nor hardware failure). Four patients had complications that included pseudarthrosis (1), extruded cage (1), cage in kyphosis (1), and radiculopathy (1).
CONCLUSIONS: Titanium cages provide immediate strong anterior column support with minimum hardware complications and avoid bone graft-site morbidity. Titanium cages, with concomitant use of anterior plating, offer an effective and safe alternative to bone autografts.},
  doi      = {10.1097/00007632-199908010-00016},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1036299701},
}

@Article{Eleraky1999,
  author   = {M A Eleraky and C Llanos and V K Sonntag},
  journal  = {Journal of Neurosurgery},
  title    = {Cervical corpectomy: report of 185 cases and review of the literature.},
  year     = {1999},
  number   = {1 Suppl},
  pages    = {35-41},
  volume   = {90},
  abstract = {OBJECT: This study was conducted to determine the indications, safety, efficacy, and complication rate associated with performing corpectomy to achieve anterior decompression of neural elements or for removing anterior lesions.
METHODS: Between 1987 and 1998, 185 patients underwent cervical corpectomy for the treatment of degenerative spondylitic disease (81 cases), ossification of posterior longitudinal ligament (16 cases), correction of postoperative kyphosis (31 cases), trauma (39 cases), tumor (10 cases), and infection (eight cases). Ninety-nine patients presented with myelopathy, 48 with radiculomyelopathy, 24 with radicular pain, and 14 with neck muscle pain. Eighty-seven patients underwent a one-level corpectomy; 45 of these patients underwent a discectomy at a different level. Seventy patients underwent a two-level corpectomy; 27 of these patients underwent a discectomy at a different level. Twenty-eight patients underwent a three-level corpectomy. Autograft (iliac crest) was used in 141 cases and allograft (fibula) in 44 cases. All but six patients underwent fixation with an anterior plate-screw system. There were no operative deaths. During the procedure the vertebral artery was injured in four patients and preserved in two of them. No neurological sequelae were encountered. Postoperative hoarseness, transient dysphagia, and pain at the graft site were transitory and successfully managed. The fusion rate was 98.8%. Six patients experienced transient deterioration after surgery but they improved. No patient experienced permanent neurological deterioration and 160 (86.5%) improved.
CONCLUSIONS: Corpectomy has an important role in the management of various degenerative, traumatic, neoplastic, or infectious disorders of cervical spine. Following treatment in this series, radiculopathy always improved and myelopathy was reversed in most patients.},
  doi      = {10.3171/spi.1999.90.1.0035},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1071103060},
}

@Article{Demir2003,
  author   = {Ayhan Demir and Mario Ries and Crit T W Moonen and Jean-Marc Vital and Joël Dehais and Pierre Arne and Jean-Marie Caillé and Vincent Dousset},
  journal  = {Radiology},
  title    = {Diffusion-weighted MR Imaging with Apparent Diffusion Coefficient and Apparent Diffusion Tensor Maps in Cervical Spondylotic Myelopathy},
  year     = {2003},
  number   = {1},
  pages    = {37-43},
  volume   = {229},
  abstract = {PURPOSE: To evaluate diffusion-weighted magnetic resonance (MR) imaging in patients with cervical spondylosis and/or myelopathy.
MATERIALS AND METHODS: A multishot echo-planar imaging sequence with calculation of apparent diffusion coefficient (ADC) and apparent diffusion tensor (ADT) was applied in 36 patients with symptomatic cervical spondylosis. Diffusion-weighted images read by two neuroradiologists were compared with T2-weighted fast spin-echo images read independently by three neuroradiologists with regard to clinical status (n = 36). MR findings in a selected subgroup of 20 patients whose clinical status was confirmed by electrophysiologic examination also were compared. Sensitivity, specificity, positive predictive value, and negative predictive value of both T2-weighted imaging and diffusion-weighted imaging (ADC and ADT) were calculated and compared.
RESULTS: Patients with myelopathy had abnormal ADC (17 of 21) and ADT (15 of 19) maps with increased ADC and ADT values and decreased anisotropy. For the detection of myelopathy, diffusion-weighted ADC maps had a sensitivity of 80% (17 of 21), while T2-weighted images had a sensitivity of 61% (13 of 21). The negative predictive value was 63% (seven of 11) and 60% (12 of 20) for ADC maps and T2-weighted images, respectively. Conversely, the specificity of diffusion-weighted images (53%; seven of 13) was lower than that of T2-weighted images (92%; 12 of 13). In patients with myelopathy confirmed at electrophysiologic examination, the sensitivity of diffusion-weighted images increased to 92% (12 of 13) and the negative predictive value increased to 75% (three of four), while T2-weighted images had a 53% (seven of 13) sensitivity and a 50% (six of 12) negative predictive value.
CONCLUSION: Diffusion weighting improved the sensitivity of imaging in cervical spondylotic myelopathy.},
  doi      = {10.1148/radiol.2291020658},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1045861847},
}

@Article{YONENOBU1985,
  author   = {KAZUO YONENOBU and TAKESHI FUJI and KEIRO ONO and KOZO OKADA and TOMIO YAMAMOTO and NORIMASA HARADA},
  journal  = {Spine},
  title    = {Choice of Surgical Treatment for Multisegmental Cervical Spondylotic Myelopathy},
  year     = {1985},
  number   = {8},
  pages    = {710-716},
  volume   = {10},
  abstract = {Three surgical procedures for multisegmental cervical spondylotic myelopathy were evaluated on the basis of a follow-up study (12-157 months) of 95 patients. Twenty-four patients were treated by extensive laminectomy, 50 by anterior interbody fusion by the Cloward and/or Smith-Robinson techniques, and 21 by subtotal spondylectomy and fusion. Results of subtotal spondylectomy were significantly (P less than 0.01) better when compared with those of the other two procedures. It was concluded that spondylosis up to three disc levels should be treated by subtotal spondylectomy and fusion regardless of the canal diameter. When involvement extended four or more levels, extensive laminectomy was recommended.},
  doi      = {10.1097/00007632-198510000-00004},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1037790804},
}

@Article{Okada1993,
  author   = {Yuji Okada and Takaaki Ikata and Hidehiro Yamada and Rintaro Sakamoto and Shinsuke Katoh},
  journal  = {Spine},
  title    = {Magnetic Resonance Imaging Study on the Results of Surgery for Cervical Compression Myelopathy},
  year     = {1993},
  note     = {https://journals.lww.com/spinejournal/abstract/1993/10001/magnetic_resonance_imaging_study_on_the_results_of.16.aspx},
  number   = {14},
  pages    = {2024-2029},
  volume   = {18},
  abstract = {The morphologic changes and signal intensity of the spinal cord on preoperative magnetic resonance images were correlated with postoperative outcomes in 74 patients undergoing decompressive cervical surgery for compressive myelopathy. The transverse area of the spinal cord on T1-weighted images at the level of maximum compression was closely correlated with the severity of myelopathy, duration of disease, and recovery rate as determined by the Japanese Orthopaedic Association score. In patients with ossification of the posterior longitudinal ligament or cervical spondylotic myelopathy, the increased intramedullary T2-weighted magnetic resonance imaging signal at the site of maximal cord compression and duration of disease significantly influenced the rate of recovery. A multiple regression equation was then developed with these three variables to predict surgical outcomes.},
  doi      = {10.1097/00007632-199310001-00016},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1020803753},
}

@Article{Fujiwara1989,
  author   = {K Fujiwara and K Yonenobu and S Ebara and K Yamashita and K Ono},
  journal  = {The Bone & Joint Journal},
  title    = {The prognosis of surgery for cervical compression myelopathy. An analysis of the factors involved.},
  year     = {1989},
  number   = {3},
  pages    = {393-8},
  volume   = {71},
  abstract = {We have studied the morphometry of the spinal cord in 50 patients with cervical compression myelopathy. Computed tomographic myelography (CTM) showed that the transverse area of the cord at the site of maximum compression correlated significantly with the results of surgery. In most patients with less than 30 mm2 of spinal cord area, the results were poor; the cord was unable to survive. Several factors, such as chronicity of disease, age at surgery and multiplicity of involvement are said to influence the results of surgery, but the transverse area of the cord at the level of maximum compression provides the most reliable and comprehensive parameter for their prediction.},
  doi      = {10.1302/0301-620x.71b3.2722928},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1079276563},
}

@Article{OGINO1983,
  author   = {HIROSHI OGINO and KOICHI TADA and KOZO OKADA and KAZUO YONENOBU and TOMIO YAMAMOTO and KEIRO ONO and HIDEO NAMIKI},
  journal  = {Spine},
  title    = {Canal Diameter, Anteroposterior Compression Ratio, and Spondylotic Myelopathy of the Cervical Spine},
  year     = {1983},
  number   = {1},
  pages    = {1-15},
  volume   = {8},
  abstract = {Nine patients with cervical spondylotic myelopathy, diagnosed during life, were subjected to detailed clinicopathologic study. The degree of cord destruction was in good correlation with the ratio of the anteroposterior diameter to the transverse diameter, designated as an anteroposterior compression ratio. Within the factors responsible for decrease in the ratio, developmental narrowing of the spinal canal was the most significant, and multiplicity of spondylotic protrusion less so. The former resulted in an extensive demyelination of the posterolateral funiculus and infarction of the gray matter. Recurrent trauma proved to cause distinct manifestations and cord pathology. Clinicopathologic correlations were also examined from the neurologic findings at the terminal stage.},
  doi      = {10.1097/00007632-198301000-00001},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1017401652},
}

@Article{Hukuda1985,
  author   = {S Hukuda and T Mochizuki and M Ogata and K Shichikawa and Y Shimomura},
  journal  = {The Bone & Joint Journal},
  title    = {Operations for cervical spondylotic myelopathy. A comparison of the results of anterior and posterior procedures.},
  year     = {1985},
  number   = {4},
  pages    = {609-15},
  volume   = {67},
  abstract = {Over the past 19 years we have operated on 269 patients with myelopathy associated with cervical spondylosis. We report our results in 191 cases which we have followed up for 1 to 12 years (average 31 months). The clinical state before and after operation was recorded using the criteria of the Japanese Orthopaedic Association. Posterior operations gave better results than anterior for the more advanced myelopathies such as transverse lesions, the Brown-Séquard syndrome and the motor syndromes, but the brachialgia and cord syndrome and the central cord syndrome were satisfactorily treated by anterior operations. Of the three anterior and three posterior techniques used, no single one showed an overall superiority. A short duration of symptoms before operation was associated with better results, but these were not influenced by the age of the patients.},
  doi      = {10.1302/0301-620x.67b4.4030860},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1080091058},
}

@Article{Sampath2000,
  author   = {Prakash Sampath and Mohammed Bendebba and John D. Davis and Thomas B. Ducker},
  journal  = {Spine},
  title    = {Outcome of Patients Treated for Cervical Myelopathy},
  year     = {2000},
  number   = {6},
  pages    = {670-676},
  volume   = {25},
  abstract = {STUDY DESIGN: This Cervical Spine Research Society (CSRS) Study is a prospective, multicenter, nonrandomized investigation of patients with cervical spondylosis and disc disease. In this analysis, only patients with cervical myelopathy as the predominant syndrome were considered.
OBJECTIVES: To determine demographics, surgeon treatment practices, and outcomes in patients with symptomatic myelopathy.
SUMMARY OF BACKGROUND DATA: Current data on patient demographics and treatment practices of surgeons do not exist. There are no published prospective studies in which neurologic, functional, pain, and activities of daily living outcomes are systematically quantified.
METHODS: Patients were recruited by participating CSRS surgeons. Demographic information, patients' symptoms, and patients' functional data were compiled from patient and physician surveys completed at the time of initial examination, and outcomes were assessed from patient surveys completed after treatment. Data were compiled and statistically analyzed by a blinded third party.
RESULTS: Sixty-two (12%) of the 503 patients enrolled by 41 CSRS surgeons had myelopathy. Patients (48.4% male; mean age, 48.7 +/- 12.03 years) had a mean duration of symptoms of 29.8 months (range, 8 weeks to 180 months). Surgery was recommended for 31 (50%) of these patients. Forty-three patients (69%) returned for follow-up and completed the questionnaire adequately for analysis. Twenty (46%) of the 43 patients on whom follow-up data are available underwent surgery, and 23 (54%) received medical treatment. Surgically treated patients had a significant improvement in functional status and overall pain, with improvement also observed in neurologic symptoms. Patients treated nonsurgically had a significant worsening of their ability to perform activities of daily living, with worsening of neurologic symptoms.
CONCLUSIONS: When medical and surgical treatments are compared, surgically treated patients appear to have better outcomes, despite exhibiting a greater number of neurologic and nonneurologic symptoms and having greater functional disability before treatment. Randomized studies, if feasible, should be performed to address outcome in cervical myelopathy further.},
  doi      = {10.1097/00007632-200003150-00004},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1021857143},
}

@Article{Morio2001,
  author   = {Yasuo Morio and Ryota Teshima and Hideki Nagashima and Koji Nawata and Daisuke Yamasaki and Yoshirou Nanjo},
  journal  = {Spine},
  title    = {Correlation Between Operative Outcomes of Cervical Compression Myelopathy and MRI of the Spinal Cord},
  year     = {2001},
  number   = {11},
  pages    = {1238-1245},
  volume   = {26},
  abstract = {STUDY DESIGN: Magnetic resonance images of cervical compression myelopathy were retrospectively analyzed in comparison with surgical outcomes.
OBJECTIVES: To investigate which magnetic resonance findings in patients with cervical compression myelopathy reflect the clinical symptoms and prognosis, and to determine the radiographic and clinical factors that correlate with the prognosis.
SUMMARY OF BACKGROUND DATA: Signal intensity changes of the spinal cord on magnetic resonance imaging in chronic cervical myelopathy are thought to be indicative of the prognosis. However, the prognostic significance of signal intensity change remains controversial.
METHODS: The participants in this study were 73 patients who underwent cervical expansive laminoplasty for cervical compression myelopathy. Their mean age was 64 years, and the mean postoperative follow-up period was 3.4 years. The pathologic conditions were cervical spondylotic myelopathy in 42 patients and ossification of the posterior longitudinal ligament in 31 patients. Magnetic resonance imaging (spin-echo sequence) was performed in all the patients. The transverse area of the spinal cord at the site of maximal compression was computed, and spinal cord signal intensity changes were evaluated before and after surgery. Three patterns of spinal cord signal intensity changes on T1-weighted sequences/T2-weighted sequences were detected as follows: normal/normal, normal/high-signal intensity changes, and low-signal/high-signal intensity changes. Surgical outcomes were compared among these three groups. The most useful combination of parameters for predicting prognosis was determined using a stepwise regression analysis.
RESULTS: The findings showed 2 patients with normal/normal, 67 patients with normal/high-signal, and 4 patients with low-signal/high-signal change patterns before surgery. Regarding postoperative recovery, the preoperative low-signal/high-signal group was significantly inferior to the preoperative normal/high-signal group. There was no significant difference between the transverse area of the spinal cord at the site of maximal compression in the normal/high-signal group and the low-signal/high-signal group. A stepwise regression analysis showed that the best combination of surgical outcome predictors included age (correlation coefficient R = -0.348), preoperative signal pattern, and duration of symptoms (correlation coefficient R = -0.231).
CONCLUSIONS: The low-signal intensity changes on T1-weighted sequences indicated a poor prognosis. The authors speculate that high-signal intensity changes on T2 weighted images include a broad spectrum of compressive myelomalacic pathologies and reflect a broad spectrum of spinal cord recuperative potentials. Predictors of surgical outcomes are preoperative signal intensity change pattern of the spinal cord on radiologic evaluations, age at the time of surgery, and chronicity of the disease.},
  doi      = {10.1097/00007632-200106010-00012},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1028542573},
}

@Article{Mehalic1990,
  author   = {Thomas F. Mehalic and Roger T. Pezzuti and Brett I. Applebaum},
  journal  = {Neurosurgery},
  title    = {Magnetic Resonance Imaging and Cervical Spondylotic Myelopathy},
  year     = {1990},
  note     = {https://journals.lww.com/neurosurgery/pages/default.aspx},
  number   = {2},
  pages    = {217-227},
  volume   = {26},
  abstract = {Abstract
                  Nineteen patients were examined for cervical spondylotic myelopathy with magnetic resonance imaging. Pre- and postoperative magnetic resonance scans were obtained in most cases. Surgical confirmation of the pathological condition was obtained for all 19 patients. On the T2-weighted scans, there was increased signal intensity within the spinal cord at the point of maximal compression. The exact cause of the increased signal intensity on the T2-weighted images is not known, but is suspected to represent edema, inflammation, vascular ischemia, myelomalacia, or gliosis. The increased signal intensity diminished postoperatively in the patients who improved clinically, and remained the same or increased in those whose conditions remained unchanged or worsened after decompression. The authors suggest that these T2-weighted images carry prognostic significance.},
  doi      = {10.1227/00006123-199002000-00006},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1015407573},
}

@Article{Iwasaki2002,
  author   = {Motoki Iwasaki and Yoshiharu Kawaguchi and Tomoatsu Kimura and Kazuo Yonenobu},
  journal  = {Journal of Neurosurgery},
  title    = {Long-term results of expansive laminoplasty for ossification of the posterior longitudinal ligament of the cervical spine: more than 10 years follow up.},
  year     = {2002},
  number   = {2 Suppl},
  pages    = {180-9},
  volume   = {96},
  abstract = {OBJECT: The authors report the long-term (more than 10-year) results of cervical laminoplasty for ossification of the posterior longitudinal ligament (OPLL) of the cervical spine as well as the factors affecting long-term postoperative course.
METHODS: The authors reviewed data obtained in 92 patients who underwent cervical laminoplasty between 1982 and 1990. Three patients were lost to follow up, 25 patients died within 10 years of surgery, and 64 patients were followed for more than 10 years. Results were assessed using the Japanese Orthopaedic Association (JOA) scoring system for cervical myelopathy. The recovery rate was calculated using the Hirabayashi method. The mean neurological recovery rate during the first 10 years after surgery was 64%, which declined to 60% at the last follow-up examination (mean follow up 12.2 years). Late neurological deterioration occurred in eight patients (14%) from 5 to 15 years after surgery. The most frequent causes of late deterioration were degenerative lumbar disease (three patients), thoracic myelopathy secondary to ossification of the ligamentum flavum (two patients), or postoperative progression of OPLL at the operated level (two patients). Postoperative progression of the ossified lesion was noted in 70% of the patients, but only two patients (3%) were found to have related neurological deterioration. Additional cervical surgery was required in one patient (2%) because of neurological deterioration secondary to progression of the ossified ligament. The authors performed a multivariate stepwise analysis, and found that factors related to better clinical results were younger age at operation and less severe preexisting myelopathy. Younger age at operation, as well as mixed and continuous types of OPLL, was highly predictive of progression of OPLL. Postoperative progression of kyphotic deformity was observed in 8% of the patients, although it did not cause neurological deterioration.
CONCLUSIONS: When the incidence of surgery-related complications and the strong possibility of postoperative growth of OPLL are taken into consideration, the authors recommend expansive and extensive laminoplasty for OPLL.},
  doi      = {10.3171/spi.2002.96.2.0180},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1071103439},
}

@Article{Sasso2003,
  author   = {Rick C. Sasso and Robert A. Ruggiero and Thomas M. Reilly and Peter V. Hall},
  journal  = {Spine},
  title    = {Early Reconstruction Failures After Multilevel Cervical Corpectomy},
  year     = {2003},
  number   = {2},
  pages    = {140-142},
  volume   = {28},
  abstract = {STUDY DESIGN: A retrospective analysis of graft and plate complications after multilevel anterior cervical corpectomy and fusion (ACF) attributed to spondylosis, stenosis, and ossification of posterior longitudinal ligament was conducted.
OBJECTIVE: To identify factors contributing to graft and plate complications in this population.
SUMMARY OF BACKGROUND DATA: Biomechanical factors contributing to the increased morbidity associated with plated multilevel ACF were evaluated.
METHODS: Graft- and/or plate-related complications were retrospectively reviewed in 33 patients undergoing two-level ACF reconstructions and in seven patients having three-level ACF reconstructions performed with iliac crest grafting and instrumentation with a fixed-plated design (cervical spine locking plate). Neurologic status was assessed before surgery and after surgery using both the Nurick Grading Scale and modified JOA (Japanese Orthopaedic Association) Score. The patients were observed an average of 31.4 months after surgery. The follow-up included lateral flexion and extension radiographs and a neurologic examination.
RESULTS: Two of the 33 patients undergoing two-level fusions available for long-term follow-up after surgery developed reconstruction failures. All of the remaining fusions were successful, demonstrated by lateral flexion and extension radiographs. Seven patients had plated three-level corpectomy reconstructions. Five of the seven who had anterior-only reconstruction failed.
DISCUSSION: A two-level ACF reconstruction is reliable with an anterior strut graft and fixed screw plate construct. A three-level ACF reconstruction is not reliably achieved with an anterior-only construct. The construct failures may be attributed in part to the fixed-plated design being used, as well as the long lever arm of the construct.
CONCLUSION: There is a 6% failure rate after fixed-plated (cervical spine locking plate) two-level ACF reconstruction but a 71% failure rate after three-level fixed-plated ACF reconstruction. Future consideration should be given to simultaneous posterior fusion.},
  doi      = {10.1097/00007632-200301150-00009},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1014052117},
}

@Article{Edwards2002,
  author   = {Charles C. Edwards and John G. Heller and Hideki Murakami},
  journal  = {Spine},
  title    = {Corpectomy Versus Laminoplasty for Multilevel Cervical Myelopathy},
  year     = {2002},
  number   = {11},
  pages    = {1168-1175},
  volume   = {27},
  abstract = {STUDY DESIGN: Matched patient cohorts using retrospective chart and radiographic review with independent clinical and radiographic follow-up were reviewed.
OBJECTIVE: To compare the clinical and radiographic outcomes of multilevel corpectomy and laminoplasty using an independent matched-cohort analysis.
SUMMARY OF BACKGROUND DATA: The treatment of choice for multilevel cervical myelopathy remains a matter of investigation. For the decompression of three or more motion segments, multilevel corpectomy and laminoplasty have proven effective while avoiding the pitfalls of laminectomy. Direct clinical comparisons of these two procedures are few in number and are limited by the heterogeneity in their patient groups.
METHODS: Medical records of all patients treated for multilevel cervical myelopathy with either multilevel corpectomy or laminoplasty between 1994 and 1999 at the Emory Spine Center were reviewed. From a pool of 38 patients meeting stringent inclusion and exclusion criteria, 13 patients who underwent multilevel corpectomy were blindly matched with 13 patients who underwent laminoplasty based on known prognostic criteria. A single physician independently evaluated each patient and their radiographs at their latest follow-up appointment.
RESULTS: The cohorts were well matched by age, duration of symptoms, severity of myelopathy (Nurick grade), and preoperative sagittal alignment (C2-C7). Mean operative time, blood loss, and hospital stay were nearly identical. The mean follow-up for multilevel corpectomy and laminoplasty were 49 and 40 months, respectively. Improvement in function averaged 1.6 Nurick grades after laminoplasty and 0.9 grades after multilevel corpectomy (P > 0.05). Subjective improvements in strength, dexterity, sensation, pain, and gait were similar for the two operations. The prevalence of axial discomfort at the latest follow-up was the same for each cohort, but the analgesic requirements tended to be greater for patients who underwent multilevel corpectomy. Sagittal motion from C2 to C7 decreased by 57% after multilevel corpectomy and by 38% after laminoplasty. One complication (C6-C7 herniated nucleus pulposus [HNP] requiring anterior discetomy with fusion) occurred in the laminoplasty group. Multilevel corpectomy complications included progression of myelopathy, nonunion, persistent dysphagia, persistent dysphonia, and subjacent motion segment ankylosis.
CONCLUSIONS: Both multilevel corpectomy and laminoplasty reliably arrest myelopathic progression in multilevel cervical myelopathy and can lead to significant neurologic recovery and pain reduction in a majority of patients. Surprisingly, the laminoplasty cohort tended to require less pain medication at final follow-up than did the multilevel corpectomy cohort. Given this and the higher prevalence of complications among multilevel corpectomy patients, it is believed that laminoplasty may be the preferred method of treatment for multilevel cervical myelopathy in the absence of preoperative kyphosis.},
  doi      = {10.1097/00007632-200206010-00007},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1034944004},
}

@Article{Wang2007,
  author   = {Marjorie C. Wang and Leighton Chan and Dennis J. Maiman and William Kreuter and Richard A. Deyo},
  journal  = {Spine},
  title    = {Complications and Mortality Associated With Cervical Spine Surgery for Degenerative Disease in the United States},
  year     = {2007},
  number   = {3},
  pages    = {342-347},
  volume   = {32},
  abstract = {STUDY DESIGN: Retrospective cohort.
OBJECTIVES: To describe the incidence of complications and mortality associated with surgery for degenerative disease of the cervical spine using population-based data. To evaluate the associations between complications and mortality and age, primary diagnosis and type of surgical procedure.
SUMMARY OF BACKGROUND DATA: Recent studies have shown an increase in the number of cervical spine surgeries performed for degenerative disease in the United States. However, the associations between complications and mortality and age, primary diagnosis and type of surgical procedure are not well described using population-based data.
METHODS: We created an algorithm defining degenerative cervical spine disease and associated complications using the International Classification of Diseases-ninth revision Clinical Modification codes. Using the Nationwide Inpatient Sample, we determined the primary diagnoses, surgical procedures, and associated in-hospital complications and mortality from 1992 to 2001.
RESULTS: From 1992 to 2001, the Nationwide Inpatient Sample included an estimated 932,009 (0.3%) hospital discharges associated with cervical spine surgery for degenerative disease. The majority of admissions were for herniated disc (56%) and cervical spondylosis with myelopathy (19%). Complications and mortality were more common in the elderly, and after posterior fusions or surgical procedures associated with a primary diagnosis of cervical spondylosis with myelopathy.
CONCLUSIONS: There are significant differences in outcome associated with age, primary diagnosis, and type of surgical procedure. Administrative databases may underestimate the incidence of complications, but these population-based studies may provide information for comparison with surgical case series and help evaluate rare or severe complications.},
  doi      = {10.1097/01.brs.0000254120.25411.ae},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1053723341},
}

@Article{Yonenobu1992,
  author   = {Kazuo Yonenobu and Noboru Hosono and Motoki lwasaki and Masatoshi Asano and Keiro Ono},
  journal  = {Spine},
  title    = {Laminoplasty Versus Subtotal Corpectomy| A Comparative Study of Results in Multisegmental Cervical Spondylotic Myelopathy},
  year     = {1992},
  number   = {11},
  pages    = {1281-1284},
  volume   = {17},
  abstract = {A comparative study of surgical results was used to determine the treatment of choice for multisegmental cervical spondylotic myelopathy. Forty-one patients who received subtotal corpectomy and strut grafting (SCS) and forty-two undergoing laminoplasty were followed up for at least 2 years after surgery. Regarding factors known to affect surgical prognosis (age at surgery, duration of symptoms, severity of neurologic deficit, anteroposterior canal diameter, transverse area of the cord at the site of maximum compression, number of levels involved), the two groups were statistically comparable with each other. The severity of neurologic deficits was assessed by the Japanese Orthopaedic Association scale. Results were evaluated in terms of postoperative score and recovery rate. The difference between the recovery rate and final score between the two groups was not statistically significant. Surgical complications were more frequent in the subtotal corpectomy and strut grafting group than in the laminoplasty group. The most frequent complications encountered in the subtotal corpectomy and strut grafting group were related to bone grafting. Spinal alignment worsened in six patients of the laminoplasty group, but none of them suffered from neurologic deterioration. Another disadvantage of subtotal corpectomy and strut grafting was the longer postoperative period of bed rest needed to secure graft stability. We conclude that laminoplasty should be the treatment of choice for multisegmental cervical spondylotic myelopathy when neurologic results, incidence of complications, and postoperative treatment are taken into consideration.},
  doi      = {10.1097/00007632-199211000-00004},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1028420740},
}

@Article{Kaptain2000,
  author   = {G J Kaptain and N E Simmons and R E Replogle and L Pobereskin},
  journal  = {Journal of Neurosurgery},
  title    = {Incidence and outcome of kyphotic deformity following laminectomy for cervical spondylotic myelopathy.},
  year     = {2000},
  number   = {2 Suppl},
  pages    = {199-204},
  volume   = {93},
  abstract = {OBJECT: The authors undertook a study to explore the predisposing risk factors, frequency of occurrence, and clinical implications of kyphosis following laminectomy for cervical spondylotic myelopathy (CSM).
METHODS: Preoperative radiological studies were available in 46 patients with CSM who had undergone laminectomy. Records were reviewed to obtain demographic data and operative reports. Preoperative radiographs were assessed to determine spinal alignment. In a follow-up interview the authors established clinical outcome and patient satisfaction. Postoperative cervical alignment and mobility was also determined by assessing lateral neutral, flexion, and extension x-ray films. Preoperatively, the cervical spine was shown to be kyphotic in four (9%) of 46, straight in 20 (43%) of 46, and lordotic in 22 (48%) of 46 patients. Nine (21%) of 42 patients with either straight or lordotic alignment demonstrated in the preoperative period developed kyphosis after surgery. Kyphosis developed in six (30%) of 20 patients in whom straight spinal alignment was demonstrated preoperatively and in only three (14%) of 22 patients in whom lordosis was found preoperatively. Clinically, 13 (29%) of 45 patients improved and 19 (42%) of 45 remained unchanged after an average 4-year follow-up period; 36 (80%) patients believed that their surgery was successful (one patient, who was mentally retarded, could not respond to the follow-up questionnaire). Spinal alignment was not predictive of outcome; cervical mobility as demonstrated on flexion and extension, however, correlated with improved functional performance (p = 0.005).
CONCLUSIONS: Kyphosis may develop in up to 21% of patients who have undergone laminectomy for CSM. Progression of the deformity appears to be more than twice as likely if preoperative radiological studies demonstrate a straight spine. In this study, clinical outcome did not correlate with either pre- or postoperative sagittal alignment.},
  doi      = {10.3171/spi.2000.93.2.0199},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1071103239},
}

@Article{Heller2001,
  author   = {J G Heller and C C Edwards and H Murakami and G E Rodts},
  journal  = {Spine},
  title    = {Laminoplasty Versus Laminectomy and Fusion for Multilevel Cervical Myelopathy},
  year     = {2001},
  number   = {12},
  pages    = {1330-1336},
  volume   = {26},
  abstract = {STUDY DESIGN: A matched cohort clinical and radiographic retrospective analysis of laminoplasty and laminectomy with fusion for the treatment of multilevel cervical myelopathy.
OBJECTIVES: To compare the clinical and radiographic outcomes of two procedures increasingly used to treat multilevel cervical myelopathy.
SUMMARY OF BACKGROUND DATA: Traditional methods of treating multilevel cervical myelopathy (laminectomy and corpectomy) are reported to have a notable frequency of complications. Laminoplasty and laminectomy with fusion have been advocated as superior procedures. A comparative study of these two techniques has not been reported.
METHODS: Medical records of all patients treated for multilevel cervical myelopathy with either laminoplasty or laminectomy with fusion between 1994 and 1999 at our institution were reviewed. Thirteen patients that underwent laminectomy with fusion were matched with 13 patients that underwent laminoplasty. All patients and radiographs were independently evaluated at latest follow-up by a single physician.
RESULTS: Cohorts were well matched based on patient age, duration of symptoms, and severity of myelopathy (Nurick grade) before surgery. Mean independent follow-up was similar (25.5 and 26.2 months). Both objective improvement in patient function (Nurick score) and the number of patients reporting subjective improvement in strength, dexterity, sensation, pain, and gait tended to be greater in the laminoplasty cohort. Whereas no complications occurred in the laminoplasty cohort, there were 14 complications in 9 patients that underwent laminectomy with fusion patients. Complications included progression of myelopathy, nonunion, instrumentation failure, development of a significant kyphotic alignment, persistent bone graft harvest site pain, subjacent degeneration requiring reoperation, and deep infection.
CONCLUSIONS: The marked difference in complications and functional improvement between these matched cohorts suggests that laminoplasty may be preferable to laminectomy with fusion as a posterior procedure for multilevel cervical myelopathy.},
  doi      = {10.1097/00007632-200106150-00013},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1036336702},
}

@Article{YONENOBU1991,
  author   = {KAZUO YONENOBU and NOBORU HOSONO and MOTOKI IWASAKI and MASATOSHI ASANO and KEIRO ONO},
  journal  = {Spine},
  title    = {Neurologic Complications of Surgery for Cervical Compression Myelopathy},
  year     = {1991},
  number   = {11},
  pages    = {1277-1282},
  volume   = {16},
  abstract = {Neurologic complications resulting from surgery for 384 cases of cervical myelopathy (cervical soft disc herniation, spondylosis, ossification of the posterior longitudinal ligament) were reviewed. Surgical procedures performed included 134 anterior interbody fusions (Cloward or Robinson-Smith technique), 70 subtotal corpectomies with strut bone graft, 85 laminectomies, and 95 laminoplasties. Twenty-one patients (5.5%) sustained neurologic deterioration related to surgery. The deterioration was classified into two types on the basis of the neurologic signs observed: deterioration of spinal cord function or of nerve root function. Manifestations of the former varied from weakness of the hand to tetraparesis. Paralysis of the deltoid and biceps brachii muscles was an exclusive feature of deterioration in the nerve root group. Causes of this paralysis included malalignment of the spine related to graft complications, and a tethering effect on the nerve root following major shifting of the spinal cord after decompression. The causes of deterioration of the cord function included spinal cord injury during surgery, malalignment of the spine associated with graft complication, and epidural hematoma.},
  doi      = {10.1097/00007632-199111000-00006},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1049890557},
}

@Article{Macdonald1997,
  author   = {R L Macdonald and M G Fehlings and C H Tator and A Lozano and J R Fleming and F Gentili and M Bernstein and M C Wallace and R R Tasker},
  journal  = {Journal of Neurosurgery},
  title    = {Multilevel anterior cervical corpectomy and fibular allograft fusion for cervical myelopathy.},
  year     = {1997},
  number   = {6},
  pages    = {990-7},
  volume   = {86},
  abstract = {This study was conducted to determine the safety and efficacy of multilevel anterior cervical corpectomy and stabilization using fibular allograft in patients with cervical myelopathy. Thirty-six patients underwent this procedure for cervical myelopathy caused by spondylosis (20 patients), ossified posterior longitudinal ligament (four patients), trauma (one patient), or a combination of lesions (11 patients). The mean age (+/- standard deviation) of the patients was 58 +/- 10 years and 30 of the patients were men. The mean duration of symptoms before surgery was 30 +/- 6 months and 11 patients had undergone previous surgery. Prior to surgery, the mean Nurick grade of the myelopathy was 3.1 +/- 1.4. Seventeen patients also had cervicobrachial pain. Four vertebrae were removed in six patients, three in 19, and two in 11 patients. Instrumentation was used in 15 cases. The operative mortality rate was 3% (one patient) and two patients died 2 months postoperatively. Postoperative complications included early graft displacement requiring reoperation (three patients), transient dysphagia (two patients), cerebrospinal fluid leak treated by lumbar drainage (three patients), myocardial infarction (two patients), and late graft fracture (one patient). One patient developed transient worsening of myelopathy and three developed new, temporary radiculopathies. All patients achieved stable bone union and the mean Nurick grade at an average of 31 +/- 20 months (range 0-79 months) postoperatively was 2.4 +/- 1.6 (p < 0.05, t-test). Cervicobrachial pain improved in 10 (59%) of the 17 patients who had preoperative pain and myelopathy improved at least one grade in 17 patients (47%; p < 0.05). Twenty-six surviving patients (72%) were followed for more than 24 months and stable, osseous union occurred in 97%. These results show that extensive, multilevel anterior decompression and stabilization using fibular allograft can be achieved with a perioperative mortality and major morbidity rate of 22% and with significant improvement in pain and myelopathy.},
  doi      = {10.3171/jns.1997.86.6.0990},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1071098440},
}

@Article{Ebersold1995,
  author   = {M J Ebersold and M C Pare and L M Quast},
  journal  = {Journal of Neurosurgery},
  title    = {Surgical treatment for cervical spondylitic myelopathy.},
  year     = {1995},
  number   = {5},
  pages    = {745-51},
  volume   = {82},
  abstract = {The long-term outcome of cervical spondylitic myelopathy after surgical treatment was retrospectively reviewed and critically evaluated in 100 patients with documented cervical myelopathy treated between 1978 and 1988 at our institution. Eighty-four patients were available for long-term study. The median duration of follow up was 7.35 years (range 3 to 9.5 years). There were 67 men and 17 women; their ages ranged from 27 to 86 years. The duration of preoperative symptoms ranged from 1 month to 10 years. Preoperative functional grade as evaluated with the Nurick Scale for the group was 2.1. Thirty-three patients with primarily anterior cord compression, one- or two-level disease, or a kyphotic neck deformity were treated by anterior decompression and fusion. Fifty-one patients with primarily posterior or cord compression and multiple-level disease were treated by posterior laminectomy. There was no difference in the preoperative functional grade in these two groups. The patients in the posterior treatment group were older (59 vs 55 years). There was no surgical mortality from the operative procedures; morbidity was 3.6%. Of the 33 patients undergoing anterior decompression and fusion, 24 showed immediate functional improvement and nine were unchanged. Of the 51 patients who underwent posterior laminectomy, 35 demonstrated improvement, 11 were unchanged, and five were worse. Six patients, one in the anterior group and five in the posterior group, demonstrated early deterioration. Late deterioration occurred from 2 to 68 months postoperatively. Four (12%) patients who had undergone anterior procedures had additional posterior procedures, and seven (13.7%) patients who had undergone posterior procedures had additional decompressive surgery. The final functional status at last follow-up examination for the 33 patients in the anterior group was improved in 18, unchanged in nine, and deteriorated in six. Of the 51 patients who underwent posterior decompression, 19 benefited from the surgery, 13 were unchanged, and 19 were worse at last follow up than before their initial surgical procedure. Age, severity of disease, number of levels operated, and preoperative grade were not predictive of outcome. The only factor related to potential deterioration was the duration of symptoms preoperatively. The results indicate that with anterior or posterior decompression, long-term outcome is variable, and a subgroup of patients, even after adequate decompression and initial improvement, will have late functional deterioration.},
  doi      = {10.3171/jns.1995.82.5.0745},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1071097586},
}

@Article{Zdeblick1989,
  author   = {T A Zdeblick and H H Bohlman},
  journal  = {Journal of Bone and Joint Surgery},
  title    = {Cervical kyphosis and myelopathy. Treatment by anterior corpectomy and strut-grafting.},
  year     = {1989},
  number   = {2},
  pages    = {170-82},
  volume   = {71},
  abstract = {Between 1976 and 1984, fourteen patients who had severe cervical kyphosis and myelopathy were treated with anterior decompression and arthrodesis. Eight had had spondylosis; five, a traumatic injury; and one, a benign intradural tumor. In eight of the fourteen patients, the severe kyphosis and myelopathy had developed after a laminectomy of three, four, or five cervical vertebrae. The laminectomy had been done for the treatment of spondylosis in five patients, of a traumatic lesion in two, and of a tumor in one. Considering all fourteen patients, an average of 2.25 vertebral bodies was removed from each, and the average extent of the subsequent fusion was 3.25 levels. Eight patients (six of whom had spondylosis; one, a traumatic lesion; and one, a tumor) were treated with a fibular graft that spanned an average of 4.10 levels, and six patients (four of whom had a traumatic lesion and two, spondylosis) were treated with an iliac graft that spanned an average of 2.70 levels. Of the five patients who had a traumatic lesion, four were treated with anterior decompression and arthrodesis, combined with posterior arthrodesis that was performed during the same period of anesthesia. In three patients, the anterior graft dislodged during the immediate postoperative period. Two of the three patients had posterior instability due to a prior laminectomy, and in the third the graft dislodged because of technical difficulties. Two of these grafts were revised to restore stability. At the latest follow-up, twelve of the fourteen fusions were solid. In the other two patients, who died six and ten months postoperatively, the fusion had been solid, as shown by radiographs, before the time of death. The average amount of correction of the kyphotic deformities was 32 degrees, a reduction from an average of 45 degrees to an average of 13 degrees. All but one patient had some recovery of neural function; nine had complete and four, partial recovery. The remaining patient had relief of pain, but he continued to be completely quadriplegic although he had some sensory sparing. Of the four patients who had been unable to walk preoperatively, three were able to walk postoperatively. No patient lost neural function after the anterior decompression and arthrodesis. We concluded that, in the presence of severe cervical kyphosis and myelopathy, adequate anterior decompression of the spinal cord, correction of the kyphosis, and anterior arthrodesis using a strut graft can yield excellent results without undue risk.},
  doi      = {10.2106/00004623-198971020-00002},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1068890012},
}

@Article{Suda2003,
  author   = {Kota Suda and Kuniyoshi Abumi and Manabu Ito and Yasuhiro Shono and Kiyoshi Kaneda and Masanori Fujiya},
  journal  = {Spine},
  title    = {Local Kyphosis Reduces Surgical Outcomes of Expansive Open-Door Laminoplasty for Cervical Spondylotic Myelopathy},
  year     = {2003},
  number   = {12},
  pages    = {1258-1262},
  volume   = {28},
  abstract = {STUDY DESIGN: This retrospective study analyzed the effects of cervical alignment on surgical results of expansive laminoplasty (ELAP) for cervical spondylotic myelopathy (CSM).
OBJECTIVE: To determine the limitation of posterior decompression by ELAP for CSM in the presence of local kyphosis.
SUMMARY OF BACKGROUND DATA: Several studies have reported that cervical malalignment affected surgical outcomes of ELAP. However, there has been no report to demonstrate crucial determinants of surgical outcomes of ELAP for CSM in relation to cervical sagittal alignment.
METHODS: The study group comprised 114 patients who underwent ELAP for CSM. All were followed up for more than 2 years. The Japanese Orthopedic Association (JOA) scoring system for cervical myelopathy (full score, 17 points) was used to evaluate surgical outcomes for each patient 2 years after surgery. Statistical analysis with multivariate logistic regression models was used to ascertain the risk factors affecting postoperative surgical outcomes.
RESULTS: The average JOA scores were 9.9 points before surgery and 14 points 2 years after surgery. The recovery rate was 60.2%. Statistical analysis showed that signal intensity change on MRI and local kyphosis were the most crucial risk factors for poor surgical outcomes. Calculated with the logistic regression model, the highest risk of poor recovery was local kyphosis exceeding 13 degrees.
CONCLUSIONS: The influence of cervical malalignment on neurologic recovery after ELAP for CSM was shown. When patients have local kyphosis exceeding 13 degrees, anterior decompression or posterior correction of kyphosis as well as ELAP should be considered. Expansive laminoplasty for CSM is best indicated for patients with local kyphosis less than 13 degrees.},
  doi      = {10.1097/01.brs.0000065487.82469.d9},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1023278334},
}

@Article{Abumi1997,
  author   = {Kuniyoshi Abumi and Kiyoshi Kaneda},
  journal  = {Spine},
  title    = {Pedicle Screw Fixation for Nontraumatic Lesions of the Cervical Spine},
  year     = {1997},
  number   = {16},
  pages    = {1853-1863},
  volume   = {22},
  abstract = {STUDY DESIGN: This retrospective study was conducted to analyze the clinical results in 45 patients with nontraumatic lesions of the cervical spine treated by pedicle screw fixation.
OBJECTIVES: To evaluate the effectiveness of pedicle screw fixation in reconstructive surgery for nontraumatic cervical spinal disorders.
SUMMARY OF BACKGROUND DATA: Pedicle screw fixation for hangman's fracture of the axis and traumatic lesions of the middle and lower cervical spine has been reported; however, there have been no reports on pedicle screw fixation for nontraumatic lesions of the cervical spine.
METHODS: Forty-five patients with nontraumatic lesions of the cervical spine underwent reconstructive surgery including pedicle screw fixation and fusion. Five patients underwent occipitocervical fixation for the lesion of the upper cervical spine, and one patient underwent separate occipitocervical fixation and cervicothoracic fixation. Cervical or cervicothoracic fixation was performed in 39 patients. Twenty-six of these patients underwent simultaneous laminectomy or laminoplasty. Supplemental anterior surgery was conducted for 15 patients.
RESULTS: Solid fusion was obtained in all patients except eight with metastatic vertebral tumors who did not receive bone graft. Correction of kyphosis was adequate. There were no neurovascular complications, except one case of transient radiculopathy caused by screw threads.
CONCLUSIONS: Pedicle screw fixation is a useful procedure for posterior reconstruction of the cervical spine. This procedure does not require the lamina for stabilization, and should be especially valuable for simultaneous posterior decompression and fusion. The risk to neurovascular structures, however, cannot be completely eliminated.},
  doi      = {10.1097/00007632-199708150-00010},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1041327125},
}

@Article{Coric2011,
  author   = {Domagoj Coric and Pierce D. Nunley and Richard D. Guyer and David Musante and Cameron N. Carmody and Charles R. Gordon and Carl Lauryssen and Donna D. Ohnmeiss and Margaret O. Boltes},
  journal  = {Journal of Neurosurgery Spine},
  title    = {Prospective, randomized, multicenter study of cervical arthroplasty: 269 patients from the Kineflex|C artificial disc investigational device exemption study with a minimum 2-year follow-up: clinical article.},
  year     = {2011},
  number   = {4},
  pages    = {348-58},
  volume   = {15},
  abstract = {OBJECT: Cervical total disc replacement (CTDR) represents a relatively novel procedure intended to address some of the shortcomings associated with anterior cervical discectomy and fusion (ACDF) by preserving motion at the treated level. This prospective, randomized, multicenter study evaluates the safety and efficacy of a new metal-on-metal CTDR implant (Kineflex|C) by comparing it with ACDF in the treatment of single-level spondylosis with radiculopathy.
METHODS: The study was a prospective, randomized US FDA Investigational Device Exemption (IDE) pivotal trial conducted at 21 centers across the US. The primary clinical outcome measures included the Neck Disability Index (NDI), visual analog scale (VAS) scores, and a composite measure of clinical success. Patients were randomized to CTDR using the Kineflex|C (SpinalMotion, Inc.) cervical artificial disc or ACDF using structural allograft and an anterior plate.
RESULTS: A total of 269 patients were enrolled and randomly assigned to either CTDR (136 patients) or to ACDF (133 patients). There were no significant differences between the CTDR and ACDF groups when comparing operative time, blood loss, length of hospital stay, or the reoperation rate at the index level. The overall success rate was significantly greater in the CTDR group (85%) compared with the ACDF group (71%) (p = 0.05). In both groups, the mean NDI scores improved significantly by 6 weeks after surgery and remained significantly improved throughout the 24-month follow-up (p < 0.0001). Similarly, the VAS pain scores improved significantly by 6 weeks and remained significantly improved through the 24-month follow-up (p < 0.0001). The range of motion (ROM) in the CTDR group decreased at 3 months but was significantly greater than the preoperative mean at 12- and 24-month follow-up. The ROM in the ACDF group was significantly reduced by 3 months and remained so throughout the follow-up. Adjacent-level degeneration was also evaluated in both groups from preoperatively to 2-year follow-up and was classified as none, mild, moderate, or severe. Preoperatively, there were no significant differences between groups when evaluating the different levels of adjacent-level degeneration. At the 2-year follow-up, there were significantly more patients in the ACDF group with severe adjacent-level radiographic changes (p < 0.0001). However, there were no significant differences between groups in adjacent-level reoperation rate (7.6% for the Kineflex|C group and 6.1% for the ACDF group).
CONCLUSIONS: Cervical total disc replacement allows for neural decompression and clinical results comparable to ACDF. Kineflex|C was associated with a significantly greater overall success rate than fusion while maintaining motion at the index level. Furthermore, there were significantly fewer Kineflex|C patients showing severe adjacent-level radiographic changes at the 2-year follow-up. These results from a prospective, randomized study support that Kineflex|C CTDR is a viable alternative to ACDF in select patients with cervical radiculopathy.},
  doi      = {10.3171/2011.5.spine10769},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1071076802},
}

@Article{Chiba2006,
  author   = {Kazuhiro Chiba and Yuto Ogawa and Ken Ishii and Hironari Takaishi and Masaya Nakamura and Hirofumi Maruiwa and Morio Matsumoto and Yoshiaki Toyama},
  journal  = {Spine},
  title    = {Long-term Results of Expansive Open-Door Laminoplasty for Cervical Myelopathy−Average 14-Year Follow-up Study},
  year     = {2006},
  number   = {26},
  pages    = {2998-3005},
  volume   = {31},
  abstract = {STUDY DESIGN: Retrospective case series on long-term follow-up results of original expansive open-door laminoplasty for cervical myelopathy due to cervical spondylosis (CSM) and ossification of posterior longitudinal ligament (OPLL).
OBJECTIVES: To elucidate efficacy and problems of original open-door laminoplasty to improve future surgical outcomes.
SUMMARY OF BACKGROUND DATA: Little information is available on long-term outcomes of original open-door laminoplasty without grafts, implants, or instruments.
METHOD: The study group included 80 patients who underwent original open-door laminoplasty and were followed for minimum 10 years. Clinical results, including Japanese Orthopedic Association scores, recovery rates, occurrences of complications, and long-term deterioration were investigated. Cervical alignments, type of OPLL, cervical range of motion, anteroposterior diameter of spinal canal, and progression of OPLL were assessed on plain radiographs. Spinal cord decompression was verified on magnetic resonance imaging.
RESULTS: Average Japanese Orthopedic Association score and recovery rate improved significantly until 3 years after surgery and remained at an acceptable level in both cervical spondylosis and OPLL patients with slight deterioration after 5 years. Segmental motor palsy developed in 8 patients. Late deterioration, mainly lower extremity motor score decline, developed in 8 CSM and 16 OPLL patients. Overall cervical range of motion decreased by 36%. Patients with cervical lordosis decreased gradually in both patient groups. Such changes in alignments did not affect surgical results in CSM patients, while OPLL patients with preoperative kyphosis had lower recovery rates than those with straight and lordotic alignments. OPLL progression that was detected in 66% of patients did not affect clinical results. Although infrequent, magnetic resonance imaging revealed atrophy of spinal cord, spinal cord compression at adjacent segments due to degenerative changes and OPLL progression.
CONCLUSIONS: Long-term results of open-door laminoplasty without bone graft, graft substitutes, or instruments were satisfactory. However, segmental motor paralysis, kyphosis, established before and after surgery, OPLL progression, and late deterioration due to age-related degeneration remain challenging problems.},
  doi      = {10.1097/01.brs.0000250307.78987.6b},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1038098923},
}

@Article{Saunders1991,
  author   = {R L Saunders and P M Bernini and T G Shirreffs and A G Reeves},
  journal  = {Journal of Neurosurgery},
  title    = {Central corpectomy for cervical spondylotic myelopathy: a consecutive series with long-term follow-up evaluation.},
  year     = {1991},
  number   = {2},
  pages    = {163-70},
  volume   = {74},
  abstract = {Since 1984, a consecutive series of patients with cervical spondylotic myelopathy has been treated by central corpectomy and strut grafting. This report focuses on 40 cases operated on between 1984 and 1987 and followed from 2 to 5 years. The perioperative complication rate was 47.5%, with a 7.5% incidence of persistent sequelae: severe C-5 radiculopathy in one patient, swallowing dysfunction in one, and hypoglossal nerve palsy in one. No single factor (age, duration of symptoms, or severity of myelopathy) was absolutely predictive of outcome; however, syndromes of short duration had the best likelihood of cure. Similar outcomes were associated, individually, with long duration of symptoms, age over 70 years, and severe myelopathy. After factoring a 5% regression of improvement, the long-term cure rate was 57.5% and the failure rate was 15%. Myelopathy worsening was not documented.},
  doi      = {10.3171/jns.1991.74.2.0163},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1071095985},
}

@Article{Goffin2002,
  author   = {Jan Goffin and Adrian Casey and Pierre Kehr and Klaus Liebig and Bengt Lind and Carlo Logroscino and Vincent Pointillart and Frank Van Calenbergh and Johannes van Loon},
  journal  = {Neurosurgery},
  title    = {Preliminary Clinical Experience with the Bryan Cervical Disc Prosthesis},
  year     = {2002},
  note     = {https://lirias.kuleuven.be/bitstream/123456789/17531/1/Goffin_NeuroSurg_full.pdf},
  number   = {3},
  pages    = {840-847},
  volume   = {51},
  abstract = {OBJECTIVE: The concept of accelerated degeneration of adjacent disc levels as a consequence of increased stress caused by interbody fusion of the cervical spine has been widely postulated. Therefore, reconstruction of a failed intervertebral disc with a functional disc prosthesis should offer the same benefits as fusion while simultaneously providing motion and thereby protecting the adjacent level discs from the abnormal stresses associated with fusion. This study was designed to determine whether a new, functional intervertebral cervical disc prosthesis can provide relief from objective neurological symptoms and signs, improve the patient's ability to perform activities of daily living, decrease pain, and provide stability and normal range of motion.
METHODS: We conducted a prospective, concurrently enrolled, multicenter trial of the Bryan Cervical Disc Prosthesis (Spinal Dynamics Corp., Mercer Island, WA) for the treatment of patients with single-level degenerative disc disease of the cervical spine. Patients with symptomatic cervical radiculopathy and/or myelopathy underwent implantation with the Bryan prosthesis after a standard anterior cervical discectomy. At scheduled follow-up periods, the effectiveness of the device was characterized by evaluating each patient's pain, neurological function, and range of motion at the implanted level.
RESULTS: Analysis included data regarding 60 patients at 6 months with 30 of those patients at 1 year. Clinical success at 6 months and 1 year after implantation was 86 and 90%, respectively, exceeding the study's acceptance criteria of 85%. These results compare favorably with the short-term clinical outcomes associated with anterior cervical discectomy and fusion reported in the literature. At 1 year, there was no measurable subsidence of the devices (based on a measurement detection threshold of 2 mm). Evidence of anterior and/or posterior device migration was detected in one patient and suspected in a second patient. There was no evidence of spondylotic bridging at the implanted disc space. The measured range of motion in flexion-extension, as determined by an independent radiologist, ranged from 1 to 21 degrees (mean range of motion, 9 +/- 5 degrees). No devices have been explanted or surgically revised.
CONCLUSION: Discectomy and implantation of the device alleviates neurological symptoms and signs similar to anterior cervical discectomy and fusion. Radiographic evidence supports normal range of motion. The procedure is safe and the patients recover quickly. Restrictive postoperative management is not necessary. However, only after long-term follow-up of at least 5 years will it become clear whether the device remains functional, thus confirming these early favorable results. In addition, the influence on adjacent motion segments can be assessed after at least 5 years of follow-up.},
  doi      = {10.1227/00006123-200209000-00048},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1016889944},
}

@Article{Yonenobu2001,
  author   = {Kazuo Yonenobu and Kuniyosi Abumi and Kensei Nagata and Eiji Taketomi and Kazumasa Ueyama},
  journal  = {Spine},
  title    = {Interobserver and Intraobserver Reliability of the Japanese Orthopaedic Association Scoring System for Evaluation of Cervical Compression Myelopathy},
  year     = {2001},
  number   = {17},
  pages    = {1890-1894},
  volume   = {26},
  abstract = {STUDY DESIGN: The inter- and intraobserver reliabilities of an assessment scale for cervical compression myelopathy were examined statistically. This scoring system consists of seven categories: motor function of fingers, shoulder and elbow, and lower extremity; sensory function of upper extremity, trunk and lower extremity; and function of the bladder. It evaluates the severity of myelopathy by allocating points based on degree of dysfunction in each category.
OBJECTIVES: To determine the inter- and intraobserver reliabilities of the revised scoring system (17 - 2 points) for cervical compression myelopathy proposed by the Japanese Orthopedic Association.
SUMMARY OF BACKGROUND DATA: Several scales to assess clinical outcome from treatment of cervical compression myelopathy have been proposed. Most of these scales include items evaluated by observers. However, no system, including the Japanese Orthopedic Association scoring system, has yet been validated in terms of interobserver reliability.
METHODS: From five different university hospitals, 10 spine surgery specialists, 10 orthopedic surgeons who had just passed the board examination of the Japanese Orthopedic Association, and 13 residents in the first or second year of orthopedic residency programs were chosen. The participants in this study were 29 patients with myelopathy secondary to ossification of the posterior longitudinal ligament selected from five participating university hospitals. Several surgeons interviewed each patient twice at intervals of 1 to 6 weeks. Inter- and intraobserver reliabilities of the total score for all categories were evaluated by the intraclass correlation coefficient. The extension of the kappa coefficient of Kraemer also was calculated for each category to assess reliability of multivariate categorical data.
RESULTS: The interobserver reliability of the total score for the first interview (intraclass correlation coefficient = 0.813) and the intra- and interobserver reliabilities of the total score (intraclass correlation coefficient = 0.826) were high. The level of experience and the hospital slightly affected the reliability of the Japanese Orthopedic Association scoring system. The kappa values for intraobserver data generally were high in each category, whereas the kappa values for interobserver data were relatively low for the categories of shoulder-elbow motor function and lower extremity sensory function.
CONCLUSIONS: The inter- and intraobserver reliabilities of the Japanese Orthopedic Association scoring system for cervical myelopathy were high, suggesting that this system is useful for assessment of cervical myelopathy in comparative studies of treatment.},
  doi      = {10.1097/00007632-200109010-00014},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1039231588},
}

@Article{Ratliff2003,
  author   = {John K Ratliff and Paul R Cooper},
  journal  = {Journal of Neurosurgery},
  title    = {Cervical laminoplasty: a critical review.},
  year     = {2003},
  number   = {3 Suppl},
  pages    = {230-8},
  volume   = {98},
  abstract = {OBJECT: The technique of cervical laminoplasty was developed to decompress the spinal canal in patients with multi-level anterior compression caused by ossification of the posterior longitudinal ligament or cervical spondylosis. There is a paucity of data confirming its superiority to laminectomy with regard to neurological outcome, preserving spinal stability, preventing postlaminectomy kyphosis, and the development of the "postlaminectomy membrane."
METHODS: The authors conducted a metaanalysis of the English-language laminoplasty literature, assessing neurological outcome, change in range of motion (ROM), development of spinal deformity, and complications. Seventy-one series were reviewed, comprising more than 2000 patients. All studies were retrospective, uncontrolled, nonrandomized case series. Forty-one series provided postoperative recovery rate data in which the Japanese Orthopaedic Association Scale was used for assessing myelopathy. The mean recovery rate was 55% (range 20-80%). The authors of 23 papers provided data on the percentage of patients improving (mean approximately 80%). There was no difference in neurological outcome based on the different laminoplasty techniques or when laminoplasty was compared with laminectomy. There was postlaminoplasty worsening of cervical alignment in approximately 35% and with development of postoperative kyphosis in approximately 10% of patients who underwent long-term follow-up review. Cervical ROM decreased substantially after laminoplasty (mean decrease 50%, range 17-80%). The authors of studies with long-term follow up found that there was progressive loss of cervical ROM, and final ROM similar to that seen in patients who had undergone laminectomy and fusion. In their review of the laminectomy literature the authors could not confirm the occurrence of postlaminectomy membrane causing clinically significant deterioration of neurological function. Postoperative complications differed substantially among series. In only seven articles did the writers quantify the rates of postoperative axial neck pain, noting an incidence between 6 and 60%. In approximately 8% of patients, C-5 nerve root dysfunction developed based on the 12 articles in which this complication was reported.
CONCLUSIONS: The literature has yet to support the purported benefits of laminoplasty. Neurological outcome and change in spinal alignment are similar after laminectomy and laminoplasty. Patients treated with laminoplasty develop progressive limitation of cervical ROM similar to that seen after laminectomy and fusion.},
  doi      = {10.3171/spi.2003.98.3.0230},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1071103618},
}

@Article{Seichi2001,
  author   = {Atsushi Seichi and Katsushi Takeshita and Isao Ohishi and Hiroshi Kawaguchi and Toru Akune and Yorito Anamizu and Tomoaki Kitagawa and and Kozo Nakamura},
  journal  = {Spine},
  title    = {Long-Term Results of Double-Door Laminoplasty for Cervical Stenotic Myelopathy},
  year     = {2001},
  number   = {5},
  pages    = {479-487},
  volume   = {26},
  abstract = {STUDY DESIGN: A retrospective study of the long-term results from double-door laminoplasty (Kurokawa's method) for patients with myelopathy caused by ossification of the posterior longitudinal ligament and cervical spondylosis was performed.
OBJECTIVE: To know whether the short-term results from double-door laminoplasty were maintained over a 10-year period and, if not, the cause of late deterioration.
SUMMARY OF BACKGROUND DATA: There are few long-term follow-up studies on the outcome of laminoplasty for cervical stenotic myelopathy.
METHODS: In this study, 35 patients with cervical myelopathy caused by ossification of the posterior longitudinal ligament in the cervical spine and 25 patients with cervical spondylotic myelopathy, including 5 patients with athetoid cerebral palsy, underwent double-door laminoplasty from 1980 through 1988 and were followed over the next 10 years. The average follow-up period was 153 months (range, 120-200 months) in patients with ossification of the posterior longitudinal ligament and 156 months (range, 121-218 months) in patients with cervical spondylotic myelopathy. Neurologic deficits before and after surgery were assessed using a scoring system proposed by the Japanese Orthopedic Association (JOA score). Patients who showed late deterioration received further examination including computed tomography scan and magnetic resonance imaging of the cervical spine.
RESULTS: In 32 of the patients with ossification of the posterior longitudinal ligament and 23 of the patients with cervical spondylotic myelopathy, myelopathy improved after surgery. The improvement of Japanese Orthopedic Association scores was maintained up to the final follow-up assessment in 26 of the patients with ossification of the posterior longitudinal ligament and 21 of the patients with cervical spondylotic myelopathy. Late neurologic deterioration occurred in 10 of the patients with ossification of the posterior longitudinal ligament an average of 8 years after surgery, and in 4 of the patients with cervical spondylotic myelopathy, including the 3 patients with athetoid cerebral palsy, an average of 11 years after surgery. The main causes of deterioration in patients with ossification of the posterior longitudinal ligament were a minor trauma in patients with residual cervical cord compression caused by ossification of the posterior longitudinal ligament and thoracic myelopathy resulting from ossification of the yellow ligament in the thoracic spine.
CONCLUSIONS: The short-term results of laminoplasty for cervical stenotic myelopathy were maintained over 10years in 78% of the patients with ossification of the posterior longitudinal ligament, and in most of the patients with cervical spondylotic myelopathy, except those with athetoid cerebral palsy. Double-door laminoplasty is a reliable procedure for individuals with cervical stenotic myelopathy.},
  doi      = {10.1097/00007632-200103010-00010},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1050443276},
}

@Article{TATSUO1985,
  author   = {TATSUO ITOH and HARUO TSUJI},
  journal  = {Spine},
  title    = {Technical Improvements and Results of Laminoplasty for Compressive Myelopathy in the Cervical Spine},
  year     = {1985},
  number   = {8},
  pages    = {729-736},
  volume   = {10},
  abstract = {The laminoplasty reported in Spine 1982 by the author (H.T.) was modified by a technical improvement to obtain a more reliable enlargement of the cervical spinal canal. The technical improvements and results are described in detail. The osteotomized laminae that floated en bloc like a hinged door must be stabilized by bone blocks with wire ligatures. Thirty patients with severe cervical myelopathy due to multisegmental spondylosis or ossification of posterior longitudinal ligament underwent surgery. The extent of the enlargement of the canal was 4.1 mm on the average in the anteroposterior diameter, and in no case was a significant reduction in the diameter of the canal noted during the follow-up period. A stable and thorough decompression of the spinal canal was noted on the postoperative computed tomograms with satisfactory surgical results.},
  doi      = {10.1097/00007632-198510000-00007},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1003399679},
}

@Article{Crandall1966,
  author  = {Paul H. Crandall and Ulrich Batzdorf},
  journal = {Journal of Neurosurgery},
  title   = {Cervical spondylotic myelopathy.},
  year    = {1966},
  number  = {1},
  pages   = {57-66},
  volume  = {25},
  doi     = {10.3171/jns.1966.25.1.0057},
  groups  = {Cervical Myelopathy},
  url     = {https://app.dimensions.ai/details/publication/pub.1071088479},
}

@Article{Wada2001,
  author   = {Eiji Wada and Shozo Suzuki and Atsunori Kanazawa and Takashi Matsuoka and Shimpei Miyamoto and Kazuo Yonenobu},
  journal  = {Spine},
  title    = {Subtotal Corpectomy Versus Laminoplasty For Multilevel Cervical Spondylotic Myelopathy},
  year     = {2001},
  number   = {13},
  pages    = {1443-1447},
  volume   = {26},
  abstract = {STUDY DESIGN: A retrospective study was conducted.
OBJECTIVE: To compare the long-term outcomes of subtotal corpectomy and laminoplasty for multilevel cervical spondylotic myelopathy.
SUMMARY OF BACKGROUND DATA: No study has compared the long-term outcomes between subtotal corpectomy and laminoplasty for multilevel cervical spondylotic myelopathy.
METHODS: In this study, 23 patients treated with subtotal corpectomy and 24 patients treated with laminoplasty were followed up for 10 to 14 years after surgery. Neurologic recovery, late deterioration, axial pain, radiographic results (degenerative changes at adjacent levels, alignment, and range of motion of the cervical spine), and surgical complications were compared between the two groups.
RESULTS: No significant difference in neurologic recovery was found between the two groups 1 and 5 years after surgery, or at the latest follow-up assessment. Neurologic status deteriorated in one patient of the subtotal corpectomy group because of adjacent degeneration, and in one patient of the laminoplasty group because of hyperextension injury. Axial pain was observed in 15% of the corpectomy group and in 40% of the laminoplasty group (P < 0.05). In the corpectomy group, listhesis exceeding 2 mm developed at 38% of the upper adjacent levels, and osteophyte formation at 54% of the lower adjacent levels. In the laminoplasty group, kyphotic deformity developed in one patient (6%) after surgery. In the corpectomy group, the mean vertebral range of motion had decreased from 39.4 degrees to 19.2 degrees (49%) by the final follow-up assessment. In the laminoplasty group, the mean vertebral range of motion had decreased from 40.2 degrees to 11.6 degrees (29%) by the final follow-up assessment. Neurologic complications related to the surgery occurred in two patients (one myelopathy from bone graft dislodgement and one C5 root palsy from bone graft fracture) of the corpectomy group and four patients (C5 root palsy) of the laminoplasty group. All of these patients recovered over time. The corpectomy group needed longer operative time (P < 0.001) and tended to have more blood loss (P = 0.24). Six patients in the corpectomy group needed posterior interspinous wiring because of pseudarthrosis.
CONCLUSIONS: Subtotal corpectomy and laminoplasty showed an identical effect from a surgical treatment for multilevel cervical spondylotic myelopathy. These neurologic recoveries usually last more than 10 years. In the subtotal corpectomy group, the disadvantages were longer surgical time, more blood loss, and pseudarthrosis. In the laminoplasty group, axial pain occurred frequently, and the range of motion was reduced severely.},
  doi      = {10.1097/00007632-200107010-00011},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1009748592},
}

@Article{Goffin2003,
  author   = {Jan Goffin and Frank Van Calenbergh and Johannes van Loon and Adrian Casey and Pierre Kehr and Klaus Liebig and Bengt Lind and Carlo Logroscino and Rosella Sgrambiglia and Vincent Pointillart},
  journal  = {Spine},
  title    = {Intermediate Follow-up After Treatment of Degenerative Disc Disease With the Bryan Cervical Disc Prosthesis: Single-Level and Bi-Level},
  year     = {2003},
  number   = {24},
  pages    = {2673-2678},
  volume   = {28},
  abstract = {STUDY DESIGN: Prospective, concurrently enrolled, multicenter trials of the Bryan Cervical Disc Prosthesis (Medtronic Sofamor Danek, Memphis, TN) were conducted for the treatment of patients with single-level and two-level (bi-level) degenerative disc disease of the cervical spine.
OBJECTIVES: The studies were designed to determine whether new functional intervertebral cervical disc prosthesis can provide relief from objective neurologic symptoms and signs, improve the patient's ability to perform activities of daily living, decrease pain, and maintain stability and segmental motion.
SUMMARY OF BACKGROUND DATA: The concept of accelerated degeneration of adjacent disc levels as a consequence of increased stress caused by interbody fusion of the cervical spine has been widely postulated. Therefore, reconstruction of a failed intervertebral disc with functional disc prosthesis should offer the same benefits as fusion while simultaneously providing motion and thereby protecting the adjacent level discs from the abnormal stresses associated with fusion.
METHODS: Patients with symptomatic cervical radiculopathy and/or myelopathy underwent implantation with the Bryan prosthesis after a standard anterior cervical discectomy. At scheduled follow-up periods, the effectiveness of the device was characterized by evaluating each patient's pain, neurologic function, and radiographically measured range of motion at the implanted level.
RESULTS: Clinical success for both studies exceeded the study acceptance criteria of 85%. At 1-year follow-up, the flexion-extension range of motion per level: Discectomy and implantation of the device alleviates neurologic symptoms and signs similar to anterior cervical discectomy and fusion. Radiographic evidence supports maintenance of motion. The procedure is safe and the patients recover quickly. At least 5 years of follow-up will be needed to assess the long-term functionality of the prosthesis and protective influence on adjacent levels.},
  doi      = {10.1097/01.brs.0000099392.90849.aa},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1043047033},
}

@Article{CLARKE1956,
  author  = {EDWIN CLARKE and PETER K. ROBINSON},
  journal = {Brain},
  title   = {CERVICAL MYELOPATHY: A COMPLICATION OF CERVICAL SPONDYLOSIS},
  year    = {1956},
  number  = {3},
  pages   = {483-510},
  volume  = {79},
  doi     = {10.1093/brain/79.3.483},
  groups  = {Cervical Myelopathy},
  url     = {https://app.dimensions.ai/details/publication/pub.1059443086},
}

@Article{Benzel1991,
  author   = {Edward C. Benzel and John Lancon and Lee Kesterson and Theresa Hadden},
  journal  = {Clinical Spine Surgery A Spine Publication},
  title    = {Cervical Laminectomy and Dentate Ligament Section for Cervical Spondylotic Myelopathy},
  year     = {1991},
  number   = {3},
  pages    = {286-295},
  volume   = {4},
  abstract = {Seventy-five patients who underwent surgical treatment for cervical spondylotic myelopathy were evaluated with respect to the operative procedure performed and their outcome. Forty patients underwent a laminectomy plus dentate ligament section (DLS), 18 underwent laminectomy alone, and 17 underwent an anterior cervical decompression and fusion (ACDF). The patients were evaluated postoperatively for both stability and for neurologic outcome using a modification of the Japanese Orthopaedic Association Assessment Scale. Functional improvement occurred in all but one patient in the laminectomy plus DLS group. The average improvement was 3.1 +/- 1.5 points in this group; whereas the average improvement in the laminectomy and the ACDF groups was 2.7 +/- 2.0 and 3.0 +/- 2.0 points respectively. All of the patients who improved substantially (greater than or equal to 6 points) in the laminectomy plus DLS and the laminectomy alone groups had normal cervical spine contours (lordosis). The remainder had either a normal lordosis or no curve (no kyphosis or lordosis). All patients in the ACDF group had either a straight spine or a cervical kyphosis. These factors implicate spine curvature, in addition to choice of operation, as factors which are important in outcome determination. No problems with instability occurred in either the laminectomy or the laminectomy plus DLS group. Two patients incurred problems with stability in the ACDF group. Both required reoperation. In addition, four patients in this group who initially improved, subsequently deteriorated. Six patients in the laminectomy plus DLS group had a several day febrile episode related to an aseptic meningitis process. Laminectomy plus DLS is a safe and efficacious alternative to laminectomy for the treatment of cervical spondylotic myelopathy. The data presented here suggests that myelopathic patients with a cervical kyphosis are best treated with an ACDF and that patients with a normal cervical lordosis are best treated with a posterior approach. Although some selected patients may benefit from DLS, no criteria are available which differentiate this small subset of patients.},
  doi      = {10.1097/00002517-199109000-00005},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1040115320},
}

@Article{Hosono1996,
  author   = {Noboru Hosono and Kazuo Yonenobu and Keiro Ono},
  journal  = {Spine},
  title    = {Neck and Shoulder Pain After Laminoplasty},
  year     = {1996},
  number   = {17},
  pages    = {1969-1973},
  volume   = {21},
  abstract = {STUDY DESIGN: The authors retrospectively analyzed the prevalence and features of neck and shoulder pain (axial symptoms) after anterior interbody fusion and laminoplasty in patients with cervical spondylotic myelopathy.
OBJECTIVES: To reveal the difference in prevalence of postoperative axial symptoms between anterior interbody fusion and laminoplasty and to clarify the pathogenesis of axial symptoms after laminoplasty.
SUMMARY OF BACKGROUND DATA: Outcome of the cervical surgery is evaluated on neurologic status alone; axial symptoms after laminoplasty rarely have been investigated. Such symptoms, however, are often severe enough to interfere with a person's daily activity.
METHODS: Ninety-eight patients had surgery for their disability secondary to cervical spondylotic myelopathy. Of those patients, 72 had laminoplasty, and 26 had anterior interbody fusion. The presence or absence of axial symptoms was investigated before and after surgery. The duration, severity, and laterality of symptoms were also recorded.
RESULTS: The prevalence of postoperative axial symptoms was significantly higher after laminoplasty than after anterior fusion (60% vs. 19%; P < 0.05). In 18 patients (25%) from the laminoplasty group, the chief complaints after surgery were related to axial symptoms for more than 3 months, whereas in the anterior fusion group, no patient reported having such severe pain after surgery.
CONCLUSIONS: The prevalence and severity of axial symptoms after laminoplasty proved to be higher and more serious than has been believed. Such symptoms should be considered in the evaluation of the outcome of cervical spinal surgery.},
  doi      = {10.1097/00007632-199609010-00005},
  groups   = {Cervical Myelopathy},
  url      = {https://app.dimensions.ai/details/publication/pub.1035509307},
}

@Article{Lelong2008,
  author   = {Camille C. D. Lelong and Philippe Burger and Guillaume Jubelin and Bruno Roux and Sylvain Labbé and Frédéric Baret},
  journal  = {Sensors},
  title    = {Assessment of Unmanned Aerial Vehicles Imagery for Quantitative Monitoring of Wheat Crop in Small Plots},
  year     = {2008},
  note     = {https://www.mdpi.com/1424-8220/8/5/3557/pdf?version=1403308610},
  number   = {5},
  pages    = {3557-3585},
  volume   = {8},
  abstract = {This paper outlines how light Unmanned Aerial Vehicles (UAV) can be used in remote sensing for precision farming. It focuses on the combination of simple digital photographic cameras with spectral filters, designed to provide multispectral images in the visible and near-infrared domains. In 2005, these instruments were fitted to powered glider and parachute, and flown at six dates staggered over the crop season. We monitored ten varieties of wheat, grown in trial micro-plots in the South-West of France. For each date, we acquired multiple views in four spectral bands corresponding to blue, green, red, and near-infrared. We then performed accurate corrections of image vignetting, geometric distortions, and radiometric bidirectional effects. Afterwards, we derived for each experimental micro-plot several vegetation indexes relevant for vegetation analyses. Finally, we sought relationships between these indexes and field-measured biophysical parameters, both generic and date-specific. Therefore, we established a robust and stable generic relationship between, in one hand, leaf area index and NDVI and, in the other hand, nitrogen uptake and GNDVI. Due to a high amount of noise in the data, it was not possible to obtain a more accurate model for each date independently. A validation protocol showed that we could expect a precision level of 15% in the biophysical parameters estimation while using these relationships.},
  doi      = {10.3390/s8053557},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1071268140},
}

@Article{Herwitz2004,
  author   = {S.R Herwitz and L.F Johnson and S.E Dunagan and R.G Higgins and D.V Sullivan and J Zheng and B.M Lobitz and J.G Leung and B.A Gallmeyer and M Aoyagi and R.E Slye and J.A Brass},
  journal  = {Computers and Electronics in Agriculture},
  title    = {Imaging from an unmanned aerial vehicle: agricultural surveillance and decision support},
  year     = {2004},
  number   = {1},
  pages    = {49-61},
  volume   = {44},
  abstract = {In September 2002, NASA’s solar-powered Pathfinder-Plus unmanned aerial vehicle (UAV) was used to conduct a proof-of-concept mission in US national airspace above the 1500ha plantation of the Kauai Coffee Company in Hawaii. While in national airspace, the transponder-equipped UAV was supervised by regional air traffic controllers and treated like a conventionally piloted aircraft. High resolution color and multispectral imaging payloads, both drawing from the aircraft’s solar power system, were housed in exterior-mounted environmental pressure pods. A local area network (LAN) using unlicensed radio frequency was used for camera control and downlink of image data at rates exceeding 5Mbits−1. A wide area network (WAN) allowed a project investigator stationed on the US mainland to uplink control commands during part of the mission. Images were available for enhancing, printing, and interpretation within minutes of collection. The color images were useful for mapping invasive weed outbreaks and for revealing irrigation and fertilization anomalies. Multispectral imagery was related to mature fruit harvest from certain fields with significant fruit display on the tree canopy exterior. During 4h “loitering” above the plantation, ground-based pilots were able to precisely navigate the UAV along pre-planned flightlines, and also perform spontaneous maneuvers under the direction of the project scientist for image collection in cloud-free zones. Despite the presence of ground-obscuring cumulus cloud cover of ca. 70% during the image collection period, the UAV’s maneuvering capability ultimately enabled collection of cloud-free imagery throughout most of the plantation. The mission demonstrated the capability of a slow-flying UAV, equipped with downsized imaging systems and line-of-sight telemetry, to monitor a localized agricultural region for an extended time period. The authors suggest that evolving long-duration (weeks to months) UAVs stand to make a valuable future contribution to regional agricultural resource monitoring.},
  doi      = {10.1016/j.compag.2004.02.006},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1027978650},
}

@Article{Hunt2010,
  author   = {E. Raymond Hunt and W. Dean Hively and Stephen J. Fujikawa and David S. Linden and Craig S. T. Daughtry and Greg W. McCarty},
  journal  = {Remote Sensing},
  title    = {Acquisition of NIR-Green-Blue Digital Photographs from Unmanned Aircraft for Crop Monitoring},
  year     = {2010},
  note     = {https://www.mdpi.com/2072-4292/2/1/290/pdf?version=1403129159},
  number   = {1},
  pages    = {290-305},
  volume   = {2},
  abstract = {Payload size and weight are critical factors for small Unmanned Aerial Vehicles (UAVs). Digital color-infrared photographs were acquired from a single 12-megapixel camera that did not have an internal hot-mirror filter and had a red-light-blocking filter in front of the lens, resulting in near-infrared (NIR), green and blue images. We tested the UAV-camera system over two variably-fertilized fields of winter wheat and found a good correlation between leaf area index and the green normalized difference vegetation index (GNDVI). The low cost and very-high spatial resolution associated with the camera-UAV system may provide important information for site-specific agriculture.},
  doi      = {10.3390/rs2010290},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1007702481},
}

@Article{Berni2009,
  author   = {Jose A. J. Berni and Pablo J. Zarco-Tejada and Lola Suárez and Elias Fereres},
  journal  = {IEEE Transactions on Geoscience and Remote Sensing},
  title    = {Thermal and Narrowband Multispectral Remote Sensing for Vegetation Monitoring from an Unmanned Aerial Vehicle},
  year     = {2009},
  note     = {https://digital.csic.es/bitstream/10261/10730/1/35.pdf},
  number   = {3},
  pages    = {722-738},
  volume   = {47},
  abstract = {Two critical limitations for using current satellite sensors in real-time crop management are the lack of imagery with optimum spatial and spectral resolutions and an unfavorable revisit time for most crop stress-detection applications. Alternatives based on manned airborne platforms are lacking due to their high operational costs. A fundamental requirement for providing useful remote sensing products in agriculture is the capacity to combine high spatial resolution and quick turnaround times. Remote sensing sensors placed on unmanned aerial vehicles (UAVs) could fill this gap, providing low-cost approaches to meet the critical requirements of spatial, spectral, and temporal resolutions. This paper demonstrates the ability to generate quantitative remote sensing products by means of a helicopter-based UAV equipped with inexpensive thermal and narrowband multispectral imaging sensors. During summer of 2007, the platform was flown over agricultural fields, obtaining thermal imagery in the 7.5–13-$\mu\hbox{m}$ region (40-cm resolution) and narrowband multispectral imagery in the 400–800-nm spectral region (20-cm resolution). Surface reflectance and temperature imagery were obtained, after atmospheric corrections with MODTRAN. Biophysical parameters were estimated using vegetation indices, namely, normalized difference vegetation index, transformed chlorophyll absorption in reflectance index/optimized soil-adjusted vegetation index, and photochemical reflectance index (PRI), coupled with SAILH and FLIGHT models. As a result, the image products of leaf area index, chlorophyll content $(C_{\rm ab})$, and water stress detection from PRI index and canopy temperature were produced and successfully validated. This paper demonstrates that results obtained with a low-cost UAV system for agricultural applications yielded comparable estimations, if not better, than those obtained by traditional manned airborne sensors.},
  doi      = {10.1109/tgrs.2008.2010457},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1061610686},
}

@Article{Schmale2008,
  author   = {David G. Schmale and Benjamin R. Dingus and Charles Reinholtz},
  journal  = {Journal of Field Robotics},
  title    = {Development and application of an autonomous unmanned aerial vehicle for precise aerobiological sampling above agricultural fields},
  year     = {2008},
  number   = {3},
  pages    = {133-147},
  volume   = {25},
  abstract = {Abstract Remote‐controlled (RC) unmanned aerial vehicles (UAVs) have been used to study the movement of agricultural threat agents (e.g., plant and animal pathogens, invasive weeds, and exotic insects) above crop fields, but these RC UAVs are operated entirely by a ground‐based pilot and often demonstrate large fluctuations in sampling height, sampling pattern, and sampling speed. In this paper, we describe the development and application of an autonomous UAV for precise aerobiological sampling tens to hundreds of meters above agricultural fields. We equipped a Senior Telemaster UAV with four aerobiological sampling devices and a MicroPilot‐based autonomous system, and we conducted 25 sampling flights for potential agricultural threat agents at Virginia Tech's Kentland Farm. To determine the most appropriate sampling path for aerobiological sampling above crop fields with an autonomous UAV, we explored five different sampling patterns, including multiple global positioning system (GPS) waypoints plotted over a variety of spatial scales. An orbital sampling pattern around a single GPS waypoint exhibited high positional accuracy and produced altitude standard deviations ranging from 1.6 to 2.8 m. Autonomous UAVs have the potential to extend the range of aerobiological sampling, improve positional accuracy of sampling paths, and enable coordinated flight with multiple UAVs sampling at different altitudes. © 2008 Wiley Periodicals, Inc.},
  doi      = {10.1002/rob.20232},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1014003350},
}

@Article{Huang2009,
  author  = {Y. Huang and W. C. Hoffmann and Y. Lan and W. Wu and B. K. Fritz},
  journal = {Applied Engineering in Agriculture},
  title   = {Development of a Spray System for an Unmanned Aerial Vehicle Platform},
  year    = {2009},
  number  = {6},
  pages   = {803-809},
  volume  = {25},
  doi     = {10.13031/2013.29229},
  groups  = {Drones in Agriculture},
  url     = {https://app.dimensions.ai/details/publication/pub.1064896140},
}

@InProceedings{Hrabar2005,
  author    = {Stefan Hrabar and Gaurav S. Sukhatme and Peter Corke and Kane Usher and Jonathan Roberts},
  booktitle = {2005 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title     = {Combined Optic-Flow and Stereo-Based Navigation of Urban Canyons for a UAV},
  year      = {2005},
  note      = {https://eprints.qut.edu.au/33825/1/33825.pdf},
  pages     = {3309-3316},
  abstract  = {We present a novel vision-based technique for navigating an Unmanned Aerial Vehicle (UAV) through urban canyons. Our technique relies on both optic flow and stereo vision information. We show that the combination of stereo and optic-flow (stereo-flow) is more effective at navigating urban canyons than either technique alone. Optic flow from a pair of sideways-looking cameras is used to stay centered in a canyon and initiate turns at junctions, while stereo vision from a forward-facing stereo head is used to avoid obstacles to the front. The technique was tested in full on an autonomous tractor at CSIRO and in part on the USC autonomous helicopter. Experimental results are presented from these two robotic platforms operating in outdoor environments. We show that the autonomous tractor can navigate urban canyons using stereo-flow, and that the autonomous helicopter can turn away from obstacles to the side using optic flow. In addition, preliminary results show that a single pair of forward-facing fisheye cameras can be used for both stereo and optic flow. The center portions of the fisheye images are used for stereo, while flow is measured in the periphery of the images.},
  doi       = {10.1109/iros.2005.1544998},
  groups    = {Drones in Agriculture},
  url       = {https://app.dimensions.ai/details/publication/pub.1094775321},
}

@Article{Sullivan2007,
  author  = {D. G. Sullivan and J. P. Fulton and J. N. Shaw and G. Bland},
  journal = {Journal of the ASABE},
  title   = {Evaluating the Sensitivity of an Unmanned Thermal Infrared Aerial System to Detect Water Stress in a Cotton Canopy},
  year    = {2007},
  number  = {6},
  pages   = {1963-1969},
  volume  = {50},
  doi     = {10.13031/2013.24091},
  groups  = {Drones in Agriculture},
  url     = {https://app.dimensions.ai/details/publication/pub.1064892999},
}

@Article{Techy2010,
  author   = {Laszlo Techy and David G. Schmale and Craig A. Woolsey},
  journal  = {Journal of Field Robotics},
  title    = {Coordinated aerobiological sampling of a plant pathogen in the lower atmosphere using two autonomous unmanned aerial vehicles},
  year     = {2010},
  note     = {https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/rob.20335},
  number   = {3},
  pages    = {335-343},
  volume   = {27},
  abstract = {Abstract  Unmanned aerial vehicles (UAVs) are an important tool to track the long‐distance movement of plant pathogens above crop fields. Here, we describe the use of a control strategy (coordination via speed modulation) to synchronize two autonomous UAVs during aerobiological sampling of the potato late blight pathogen, Phytophthora infestans . The UAVs shared position coordinates via a wireless mesh network and modulated their speeds so that they were properly phased within their sampling orbits. Three coordinated control experiments were performed August 14–15, 2008. In the first two experiments, two UAVs were vertically aligned at two different altitudes [25 and 45 m above ground level (AGL)] with identical sampling orbits (radii of 150 m). In the third experiment, two UAVs shared the same altitude (35 m AGL) with different sampling orbits (radii of 130 and 160 m). Orbit times did not vary significantly between the two UAVs across all three aerobiological sampling missions, and the phase error during sampling converged to zero within 2 min following the start of the coordinated control algorithm. Viable sporangia of P. infestans were recovered following two of the coordinated flights. This is the first detailed report of autonomous UAV coordination during the aerobiological sampling of a plant pathogen in the lower atmosphere. UAVs operating independently of one another may experience significant sampling variations during the course of a flight. Coordinating the flight of two or more UAVs ensures that the vehicles enter, sample, and exit a spore plume at consistent times. © 2010 Wiley Periodicals, Inc.},
  doi      = {10.1002/rob.20335},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1023847193},
}

@Article{Shakhatreh2019,
  author   = {Hazim Shakhatreh and Ahmad H. Sawalmeh and Ala Al-Fuqaha and Zuochao Dou and Eyad Almaita and Issa Khalil and Noor Shamsiah Othman and Abdallah Khreishah and Mohsen Guizani},
  journal  = {IEEE Access},
  title    = {Unmanned Aerial Vehicles (UAVs): A Survey on Civil Applications and Key Research Challenges},
  year     = {2019},
  note     = {https://ieeexplore.ieee.org/ielx7/6287639/8600701/08682048.pdf},
  pages    = {48572-48634},
  volume   = {7},
  abstract = {The use of unmanned aerial vehicles (UAVs) is growing rapidly across many civil application domains, including real-time monitoring, providing wireless coverage, remote sensing, search and rescue, delivery of goods, security and surveillance, precision agriculture, and civil infrastructure inspection. Smart UAVs are the next big revolution in the UAV technology promising to provide new opportunities in different applications, especially in civil infrastructure in terms of reduced risks and lower cost. Civil infrastructure is expected to dominate more than $45 Billion market value of UAV usage. In this paper, we present UAV civil applications and their challenges. We also discuss the current research trends and provide future insights for potential UAV uses. Furthermore, we present the key challenges for UAV civil applications, including charging challenges, collision avoidance and swarming challenges, and networking and security-related challenges. Based on our review of the recent literature, we discuss open research challenges and draw high-level insights on how these challenges might be approached.},
  doi      = {10.1109/access.2019.2909530},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1113441179},
}

@Article{Motlagh2017,
  author   = {Naser Hossein Motlagh and Tarik Taleb and Osama Arouk},
  journal  = {IEEE Internet of Things Journal},
  title    = {Low-Altitude Unmanned Aerial Vehicles-Based Internet of Things Services: Comprehensive Survey and Future Perspectives},
  year     = {2017},
  number   = {6},
  pages    = {899-922},
  volume   = {3},
  abstract = {Recently, unmanned aerial vehicles (UAVs), or drones, have attracted a lot of attention, since they represent a new potential market. Along with the maturity of the technology and relevant regulations, a worldwide deployment of these UAVs is expected. Thanks to the high mobility of drones, they can be used to provide a lot of applications, such as service delivery, pollution mitigation, farming, and in the rescue operations. Due to its ubiquitous usability, the UAV will play an important role in the Internet of Things (IoT) vision, and it may become the main key enabler of this vision. While these UAVs would be deployed for specific objectives (e.g., service delivery), they can be, at the same time, used to offer new IoT value-added services when they are equipped with suitable and remotely controllable machine type communications (MTCs) devices (i.e., sensors, cameras, and actuators). However, deploying UAVs for the envisioned purposes cannot be done before overcoming the relevant challenging issues. These challenges comprise not only technical issues, such as physical collision, but also regulation issues as this nascent technology could be associated with problems like breaking the privacy of people or even use it for illegal operations like drug smuggling. Providing the communication to UAVs is another challenging issue facing the deployment of this technology. In this paper, a comprehensive survey on the UAVs and the related issues will be introduced. In addition, our envisioned UAV-based architecture for the delivery of UAV-based value-added IoT services from the sky will be introduced, and the relevant key challenges and requirements will be presented.},
  doi      = {10.1109/jiot.2016.2612119},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1061280901},
}

@Article{Floreano2015,
  author   = {Dario Floreano and Robert J. Wood},
  journal  = {Nature},
  title    = {Science, technology and the future of small autonomous drones},
  year     = {2015},
  number   = {7553},
  pages    = {460-466},
  volume   = {521},
  abstract = {We are witnessing the advent of a new era of robots — drones — that can autonomously fly in natural and man-made environments. These robots, often associated with defence applications, could have a major impact on civilian tasks, including transportation, communication, agriculture, disaster mitigation and environment preservation. Autonomous flight in confined spaces presents great scientific and technical challenges owing to the energetic cost of staying airborne and to the perceptual intelligence required to negotiate complex environments. We identify scientific and technological advances that are expected to translate, within appropriate regulatory frameworks, into pervasive use of autonomous drones for civilian applications.},
  doi      = {10.1038/nature14542},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1013849273},
}

@Article{Nex2013,
  author   = {Francesco Nex and Fabio Remondino},
  journal  = {Applied Geomatics},
  title    = {UAV for 3D mapping applications: a review},
  year     = {2013},
  number   = {1},
  pages    = {1-15},
  volume   = {6},
  abstract = {Unmanned aerial vehicle (UAV) platforms are nowadays a valuable source of data for inspection, surveillance, mapping, and 3D modeling issues. As UAVs can be considered as a low-cost alternative to the classical manned aerial photogrammetry, new applications in the short- and close-range domain are introduced. Rotary or fixed-wing UAVs, capable of performing the photogrammetric data acquisition with amateur or SLR digital cameras, can fly in manual, semiautomated, and autonomous modes. Following a typical photogrammetric workflow, 3D results like digital surface or terrain models, contours, textured 3D models, vector information, etc. can be produced, even on large areas. The paper reports the state of the art of UAV for geomatics applications, giving an overview of different UAV platforms, applications, and case studies, showing also the latest developments of UAV image processing. New perspectives are also addressed.},
  doi      = {10.1007/s12518-013-0120-x},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1010901465},
}

@Article{Zhang2012,
  author   = {Chunhua Zhang and John M. Kovacs},
  journal  = {Precision Agriculture},
  title    = {The application of small unmanned aerial systems for precision agriculture: a review},
  year     = {2012},
  number   = {6},
  pages    = {693-712},
  volume   = {13},
  abstract = {Precision agriculture (PA) is the application of geospatial techniques and sensors (e.g., geographic information systems, remote sensing, GPS) to identify variations in the field and to deal with them using alternative strategies. In particular, high-resolution satellite imagery is now more commonly used to study these variations for crop and soil conditions. However, the availability and the often prohibitive costs of such imagery would suggest an alternative product for this particular application in PA. Specifically, images taken by low altitude remote sensing platforms, or small unmanned aerial systems (UAS), are shown to be a potential alternative given their low cost of operation in environmental monitoring, high spatial and temporal resolution, and their high flexibility in image acquisition programming. Not surprisingly, there have been several recent studies in the application of UAS imagery for PA. The results of these studies would indicate that, to provide a reliable end product to farmers, advances in platform design, production, standardization of image georeferencing and mosaicing, and information extraction workflow are required. Moreover, it is suggested that such endeavors should involve the farmer, particularly in the process of field design, image acquisition, image interpretation and analysis.},
  doi      = {10.1007/s11119-012-9274-5},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1020655667},
}

@Article{Goektoǧan2009,
  author   = {Ali Haydar Göktoǧan and Salah Sukkarieh and Mitch Bryson and Jeremy Randle and Todd Lupton and Calvin Hung},
  journal  = {Journal of Intelligent & Robotic Systems},
  title    = {A Rotary-wing Unmanned Air Vehicle for Aquatic Weed Surveillance and Management},
  year     = {2009},
  number   = {1-4},
  pages    = {467},
  volume   = {57},
  abstract = {This paper addresses the novel application of an autonomous rotary-wing unmanned air vehicle (RUAV) as a cost-effective tool for the surveillance and management of aquatic weeds. A conservative estimate of the annual loss of agricultural revenue to the Australian economy due to weeds is in the order of A$4 billion, hence the reason why weed control is of national significance. The presented system locates and identifies weeds in inaccessible locations. The RUAV is equipped with low-cost sensor suites and various weed detection algorithms. In order to provide the weed control operators with the capability of autonomous or remote control spraying and treatment of the aquatic weeds the RUAV is also fitted with a spray mechanism. The system has been demonstrated over inaccessible weed infested aquatic habitats.},
  doi      = {10.1007/s10846-009-9371-5},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1028947023},
}

@Article{Aasen2015,
  author   = {Helge Aasen and Andreas Burkart and Andreas Bolten and Georg Bareth},
  journal  = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title    = {Generating 3D hyperspectral information with lightweight UAV snapshot cameras for vegetation monitoring: From camera calibration to quality assurance},
  year     = {2015},
  pages    = {245-259},
  volume   = {108},
  abstract = {This paper describes a novel method to derive 3D hyperspectral information from lightweight snapshot cameras for unmanned aerial vehicles for vegetation monitoring. Snapshot cameras record an image cube with one spectral and two spatial dimensions with every exposure. First, we describe and apply methods to radiometrically characterize and calibrate these cameras. Then, we introduce our processing chain to derive 3D hyperspectral information from the calibrated image cubes based on structure from motion. The approach includes a novel way for quality assurance of the data which is used to assess the quality of the hyperspectral data for every single pixel in the final data product. The result is a hyperspectral digital surface model as a representation of the surface in 3D space linked with the hyperspectral information emitted and reflected by the objects covered by the surface. In this study we use the hyperspectral camera Cubert UHD 185-Firefly, which collects 125 bands from 450 to 950nm. The obtained data product has a spatial resolution of approximately 1cm for the spatial and 21cm for the hyperspectral information. The radiometric calibration yields good results with less than 1% offset in reflectance compared to an ASD FieldSpec 3 for most of the spectral range. The quality assurance information shows that the radiometric precision is better than 0.13% for the derived data product. We apply the approach to data from a flight campaign in a barley experiment with different varieties during the growth stage heading (BBCH 52 – 59) to demonstrate the feasibility for vegetation monitoring in the context of precision agriculture. The plant parameters retrieved from the data product correspond to in-field measurements of a single date field campaign for plant height (R2=0.7), chlorophyll (BGI2, R2=0.52), LAI (RDVI, R2=0.32) and biomass (RDVI, R2=0.29). Our approach can also be applied for other image-frame cameras as long as the individual bands of the image cube are spatially co-registered beforehand.},
  doi      = {10.1016/j.isprsjprs.2015.08.002},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1000225286},
}

@Article{Gago2015,
  author   = {J. Gago and C. Douthe and R.E. Coopman and P.P. Gallego and M. Ribas-Carbo and J. Flexas and J. Escalona and H. Medrano},
  journal  = {Agricultural Water Management},
  title    = {UAVs challenge to assess water stress for sustainable agriculture},
  year     = {2015},
  pages    = {9-19},
  volume   = {153},
  abstract = {Unmanned aerial vehicles (UAVs) present an exciting opportunity to monitor crop fields with high spatial and temporal resolution remote sensing capable of improving water stress management in agriculture. In this study, we reviewed the application of different types of UAVs using different remote sensors and compared their performance with ground-truth plant data. Several reflectance indices, such as NDVI, TCARI/OSAVI and PRInorm obtained from UAVs have shown positive correlations related to water stress indicators such as water potential (Ψ) and stomatal conductance (gs). Nevertheless, they have performed differently in diverse crops; thus, their uses and applications are also discussed in this study. Thermal imagery is also a common remote sensing technology used to assess water stress in plants, via thermal indices (calculated using artificial surfaces as references), estimates of the difference between canopy and air temperature, and even canopy conductance estimates derived from leaf energy balance models. These indices have shown a great potential to determine field stress heterogeneity using unmanned aerial platforms. It has also been proposed that chlorophyll fluorescence could be an even better indicator of plant photosynthesis and water use efficiency under water stress. Therefore, developing systems and methodologies to easily retrieve fluorescence from UAVs should be a priority for the near future. After a decade of work with UAVs, recently emerging technologies have developed more user-friendly aerial platforms, such as the multi-copters, which offer industry, science, and society new opportunities. Their use as high-throughput phenotyping platforms for real field conditions and also for water stress management increasing temporal and resolution scales could improve our capacity to determine important crop traits such as yield or stress tolerance for breeding purposes.},
  doi      = {10.1016/j.agwat.2015.01.020},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1032694598},
}

@Article{Matese2015,
  author   = {Alessandro Matese and Piero Toscano and Salvatore Filippo Di Gennaro and Lorenzo Genesio and Francesco Primo Vaccari and Jacopo Primicerio and Claudio Belli and Alessandro Zaldei and Roberto Bianconi and Beniamino Gioli},
  journal  = {Remote Sensing},
  title    = {Intercomparison of UAV, Aircraft and Satellite Remote Sensing Platforms for Precision Viticulture},
  year     = {2015},
  note     = {https://www.mdpi.com/2072-4292/7/3/2971/pdf?version=1426238644},
  number   = {3},
  pages    = {2971-2990},
  volume   = {7},
  abstract = {Precision Viticulture is experiencing substantial growth thanks to the availability of improved and cost-effective instruments and methodologies for data acquisition and analysis, such as Unmanned Aerial Vehicles (UAV), that demonstrated to compete with traditional acquisition platforms, such as satellite and aircraft, due to low operational costs, high operational flexibility and high spatial resolution of imagery. In order to optimize the use of these technologies for precision viticulture, their technical, scientific and economic performances need to be assessed. The aim of this work is to compare NDVI surveys performed with UAV, aircraft and satellite, to assess the capability of each platform to represent the intra-vineyard vegetation spatial variability. NDVI images of two Italian vineyards were acquired simultaneously from different multi-spectral sensors onboard the three platforms, and a spatial statistical framework was used to assess their degree of similarity. Moreover, the pros and cons of each technique were also assessed performing a cost analysis as a function of the scale of application. Results indicate that the different platforms provide comparable results in vineyards characterized by coarse vegetation gradients and large vegetation clusters. On the contrary, in more heterogeneous vineyards, low-resolution images fail in representing part of the intra-vineyard variability. The cost analysis showed that the adoption of UAV platform is advantageous for small areas and that a break-even point exists above five hectares; above such threshold, airborne and then satellite have lower imagery cost.},
  doi      = {10.3390/rs70302971},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1003882381},
}

@Article{Xiang2011,
  author   = {Haitao Xiang and Lei Tian},
  journal  = {Biosystems Engineering},
  title    = {Development of a low-cost agricultural remote sensing system based on an autonomous unmanned aerial vehicle (UAV)},
  year     = {2011},
  number   = {2},
  pages    = {174-190},
  volume   = {108},
  abstract = {To provide and improved remote sensing a system based on an autonomous UAV was developed. The system was based on an easily transportable helicopter platform weighing less than 14 kg. Equipped with a multi-spectral camera and autonomous system, the UAV system was capable of acquiring multi-spectral images at the desired locations and times. An extended Kalman filter (EKF) based UAV navigation system was designed and implemented using sensor fusion techniques. A ground station was designed to be the interface between a human operator and the UAV to carry out mission planning, flight command activation, and real-time flight monitoring. Based on the navigation data, and the waypoints generated by the ground station, the UAV could be automatically navigated to the desired waypoints and hover around each waypoint to collect field image data. An experiment using the UAV system to monitor turf grass glyphosate application demonstrated the system, which indicated the UAV system provides a flexible and reliable method of sensing agricultural field with high spatial and temporal resolution of image data.},
  doi      = {10.1016/j.biosystemseng.2010.11.010},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1025608752},
}

@Article{Candiago2015,
  author   = {Sebastian Candiago and Fabio Remondino and Michaela De Giglio and Marco Dubbini and Mario Gattelli},
  journal  = {Remote Sensing},
  title    = {Evaluating Multispectral Images and Vegetation Indices for Precision Farming Applications from UAV Images},
  year     = {2015},
  note     = {https://www.mdpi.com/2072-4292/7/4/4026/pdf?version=1427979584},
  number   = {4},
  pages    = {4026-4047},
  volume   = {7},
  abstract = {Unmanned Aerial Vehicles (UAV)-based remote sensing offers great possibilities to acquire in a fast and easy way field data for precision agriculture applications. This field of study is rapidly increasing due to the benefits and advantages for farm resources management, particularly for studying crop health. This paper reports some experiences related to the analysis of cultivations (vineyards and tomatoes) with Tetracam multispectral data. The Tetracam camera was mounted on a multi-rotor hexacopter. The multispectral data were processed with a photogrammetric pipeline to create triband orthoimages of the surveyed sites. Those orthoimages were employed to extract some Vegetation Indices (VI) such as the Normalized Difference Vegetation Index (NDVI), the Green Normalized Difference Vegetation Index (GNDVI), and the Soil Adjusted Vegetation Index (SAVI), examining the vegetation vigor for each crop. The paper demonstrates the great potential of high-resolution UAV data and photogrammetric techniques applied in the agriculture framework to collect multispectral images and evaluate different VI, suggesting that these instruments represent a fast, reliable, and cost-effective resource in crop assessment for precision farming applications.},
  doi      = {10.3390/rs70404026},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1022922533},
}

@Article{Honkavaara2013,
  author   = {Eija Honkavaara and Heikki Saari and Jere Kaivosoja and Ilkka Pölönen and Teemu Hakala and Paula Litkey and Jussi Mäkynen and Liisa Pesonen},
  journal  = {Remote Sensing},
  title    = {Processing and Assessment of Spectrometric, Stereoscopic Imagery Collected Using a Lightweight UAV Spectral Camera for Precision Agriculture},
  year     = {2013},
  note     = {https://www.mdpi.com/2072-4292/5/10/5006/pdf?version=1403133501},
  number   = {10},
  pages    = {5006-5039},
  volume   = {5},
  abstract = {Imaging using lightweight, unmanned airborne vehicles (UAVs) is one of the most rapidly developing fields in remote sensing technology. The new, tunable, Fabry-Perot interferometer-based (FPI) spectral camera, which weighs less than 700 g, makes it possible to collect spectrometric image blocks with stereoscopic overlaps using light-weight UAV platforms. This new technology is highly relevant, because it opens up new possibilities for measuring and monitoring the environment, which is becoming increasingly important for many environmental challenges. Our objectives were to investigate the processing and use of this new type of image data in precision agriculture. We developed the entire processing chain from raw images up to georeferenced reflectance images, digital surface models and biomass estimates. The processing integrates photogrammetric and quantitative remote sensing approaches. We carried out an empirical assessment using FPI spectral imagery collected at an agricultural wheat test site in the summer of 2012. Poor weather conditions during the campaign complicated the data processing, but this is one of the challenges that are faced in operational applications. The results indicated that the camera performed consistently and that the data processing was consistent, as well. During the agricultural experiments, promising results were obtained for biomass estimation when the spectral data was used and when an appropriate radiometric correction was applied to the data. Our results showed that the new FPI technology has a great potential in precision agriculture and indicated many possible future research topics.},
  doi      = {10.3390/rs5105006},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1052390127},
}

@Article{Adao2017,
  author   = {Telmo Adão and Jonáš Hruška and Luís Pádua and José Bessa and Emanuel Peres and Raul Morais and Joaquim Sousa},
  journal  = {Remote Sensing},
  title    = {Hyperspectral Imaging: A Review on UAV-Based Sensors, Data Processing and Applications for Agriculture and Forestry},
  year     = {2017},
  note     = {https://www.mdpi.com/2072-4292/9/11/1110/pdf?version=1509505848},
  number   = {11},
  pages    = {1110},
  volume   = {9},
  abstract = {Traditional imagery—provided, for example, by RGB and/or NIR sensors—has proven to be useful in many agroforestry applications. However, it lacks the spectral range and precision to profile materials and organisms that only hyperspectral sensors can provide. This kind of high-resolution spectroscopy was firstly used in satellites and later in manned aircraft, which are significantly expensive platforms and extremely restrictive due to availability limitations and/or complex logistics. More recently, UAS have emerged as a very popular and cost-effective remote sensing technology, composed of aerial platforms capable of carrying small-sized and lightweight sensors. Meanwhile, hyperspectral technology developments have been consistently resulting in smaller and lighter sensors that can currently be integrated in UAS for either scientific or commercial purposes. The hyperspectral sensors’ ability for measuring hundreds of bands raises complexity when considering the sheer quantity of acquired data, whose usefulness depends on both calibration and corrective tasks occurring in pre- and post-flight stages. Further steps regarding hyperspectral data processing must be performed towards the retrieval of relevant information, which provides the true benefits for assertive interventions in agricultural crops and forested areas. Considering the aforementioned topics and the goal of providing a global view focused on hyperspectral-based remote sensing supported by UAV platforms, a survey including hyperspectral sensors, inherent data processing and applications focusing both on agriculture and forestry—wherein the combination of UAV and hyperspectral sensors plays a center role—is presented in this paper. Firstly, the advantages of hyperspectral data over RGB imagery and multispectral data are highlighted. Then, hyperspectral acquisition devices are addressed, including sensor types, acquisition modes and UAV-compatible sensors that can be used for both research and commercial purposes. Pre-flight operations and post-flight pre-processing are pointed out as necessary to ensure the usefulness of hyperspectral data for further processing towards the retrieval of conclusive information. With the goal of simplifying hyperspectral data processing—by isolating the common user from the processes’ mathematical complexity—several available toolboxes that allow a direct access to level-one hyperspectral data are presented. Moreover, research works focusing the symbiosis between UAV-hyperspectral for agriculture and forestry applications are reviewed, just before the paper’s conclusions.},
  doi      = {10.3390/rs9111110},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1092454498},
}

@Article{ZarcoTejada2014,
  author   = {P.J. Zarco-Tejada and R. Diaz-Varela and V. Angileri and P. Loudjani},
  journal  = {European Journal of Agronomy},
  title    = {Tree height quantification using very high resolution imagery acquired from an unmanned aerial vehicle (UAV) and automatic 3D photo-reconstruction methods},
  year     = {2014},
  pages    = {89-99},
  volume   = {55},
  abstract = {This study provides insight into the assessment of canopy biophysical parameter retrieval using passive sensors and specifically into the quantification of tree height in a discontinuous canopy using a low-cost camera on board an unmanned aerial vehicle (UAV). The UAV was a 2-m wingspan fixed-wing platform with 5.8kg take-off weight and 63km/h ground speed. It carried a consumer-grade RGB camera modified for color-infrared detection (CIR) and synchronized with a GPS unit. In this study, the configuration of the electric UAV carrying the camera payload enabled the acquisition of 158ha in one single flight. The camera system made it possible to acquire very high resolution (VHR) imagery (5cmpixel−1) to generate ortho-mosaics and digital surface models (DSMs) through automatic 3D reconstruction methods. The UAV followed pre-designed flight plans over each study site to ensure the acquisition of the imagery with large across- and along-track overlaps (i.e. >80%) using a grid of parallel and perpendicular flight lines. The validation method consisted of taking field measurements of the height of a total of 152 trees in two different study areas using a GPS in real-time kinematic (RTK) mode. The results of the validation assessment conducted to estimate tree height from the VHR DSMs yielded R2=0.83, an overall root mean square error (RMSE) of 35cm, and a relative root mean square error (R-RMSE) of 11.5% for trees with heights ranging between 1.16 and 4.38m. An assessment conducted on the effects of the spatial resolution of the input images acquired by the UAV on the photo-reconstruction method and DSM generation demonstrated stable relationships for pixel resolutions between 5 and 30cm that rapidly degraded for input images with pixel resolutions lower than 35cm. RMSE and R-RMSE values obtained as a function of input pixel resolution showed errors in tree quantification below 15% when 30cmpixel−1 resolution imagery was used to generate the DSMs. The study conducted in two orchards with this UAV system and the photo-reconstruction method highlighted that an inexpensive approach based on consumer-grade cameras on board a hand-launched unmanned aerial platform can provide accuracies comparable to those of the expensive and computationally more complex light detection and ranging (LIDAR) systems currently operated for agricultural and environmental applications.},
  doi      = {10.1016/j.eja.2014.01.004},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1002161636},
}

@Article{Bendig2014,
  author   = {Juliane Bendig and Andreas Bolten and Simon Bennertz and Janis Broscheit and Silas Eichfuss and Georg Bareth},
  journal  = {Remote Sensing},
  title    = {Estimating Biomass of Barley Using Crop Surface Models (CSMs) Derived from UAV-Based RGB Imaging},
  year     = {2014},
  note     = {https://www.mdpi.com/2072-4292/6/11/10395/pdf?version=1414488667},
  number   = {11},
  pages    = {10395-10412},
  volume   = {6},
  abstract = {Crop monitoring is important in precision agriculture. Estimating above-ground biomass helps to monitor crop vitality and to predict yield. In this study, we estimated fresh and dry biomass on a summer barley test site with 18 cultivars and two nitrogen (N)-treatments using the plant height (PH) from crop surface models (CSMs). The super-high resolution, multi-temporal (1 cm/pixel) CSMs were derived from red, green, blue (RGB) images captured from a small unmanned aerial vehicle (UAV). Comparison with PH reference measurements yielded an R2 of 0.92. The test site with different cultivars and treatments was monitored during “Biologische Bundesanstalt, Bundessortenamt und CHemische Industrie” (BBCH) Stages 24–89. A high correlation was found between PH from CSMs and fresh biomass (R2 = 0.81) and dry biomass (R2 = 0.82). Five models for above-ground fresh and dry biomass estimation were tested by cross-validation. Modelling biomass between different N-treatments for fresh biomass produced the best results (R2 = 0.71). The main limitation was the influence of lodging cultivars in the later growth stages, producing irregular plant heights. The method has potential for future application by non-professionals, i.e., farmers.},
  doi      = {10.3390/rs61110395},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1007471000},
}

@Article{Ma2017,
  author   = {Lei Ma and Manchun Li and Xiaoxue Ma and Liang Cheng and Peijun Du and Yongxue Liu},
  journal  = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title    = {A review of supervised object-based land-cover image classification},
  year     = {2017},
  note     = {https://doi.org/10.1016/j.isprsjprs.2017.06.001},
  pages    = {277-293},
  volume   = {130},
  abstract = {Object-based image classification for land-cover mapping purposes using remote-sensing imagery has attracted significant attention in recent years. Numerous studies conducted over the past decade have investigated a broad array of sensors, feature selection, classifiers, and other factors of interest. However, these research results have not yet been synthesized to provide coherent guidance on the effect of different supervised object-based land-cover classification processes. In this study, we first construct a database with 28 fields using qualitative and quantitative information extracted from 254 experimental cases described in 173 scientific papers. Second, the results of the meta-analysis are reported, including general characteristics of the studies (e.g., the geographic range of relevant institutes, preferred journals) and the relationships between factors of interest (e.g., spatial resolution and study area or optimal segmentation scale, accuracy and number of targeted classes), especially with respect to the classification accuracy of different sensors, segmentation scale, training set size, supervised classifiers, and land-cover types. Third, useful data on supervised object-based image classification are determined from the meta-analysis. For example, we find that supervised object-based classification is currently experiencing rapid advances, while development of the fuzzy technique is limited in the object-based framework. Furthermore, spatial resolution correlates with the optimal segmentation scale and study area, and Random Forest (RF) shows the best performance in object-based classification. The area-based accuracy assessment method can obtain stable classification performance, and indicates a strong correlation between accuracy and training set size, while the accuracy of the point-based method is likely to be unstable due to mixed objects. In addition, the overall accuracy benefits from higher spatial resolution images (e.g., unmanned aerial vehicle) or agricultural sites where it also correlates with the number of targeted classes. More than 95.6% of studies involve an area less than 300ha, and the spatial resolution of images is predominantly between 0 and 2m. Furthermore, we identify some methods that may advance supervised object-based image classification. For example, deep learning and type-2 fuzzy techniques may further improve classification accuracy. Lastly, scientists are strongly encouraged to report results of uncertainty studies to further explore the effects of varied factors on supervised object-based image classification.},
  doi      = {10.1016/j.isprsjprs.2017.06.001},
  groups   = {Drones in Agriculture},
  url      = {https://app.dimensions.ai/details/publication/pub.1086143194},
}

@Article{Nodzo2018,
  author   = {S R Nodzo and C-C Chang and K M Carroll and B T Barlow and S A Banks and D E Padgett and D J Mayman and S A Jerabek},
  journal  = {The Bone & Joint Journal},
  title    = {Intraoperative placement of total hip arthroplasty components with robotic-arm assisted technology correlates with postoperative implant position: a CT-based study.},
  year     = {2018},
  number   = {10},
  pages    = {1303-1309},
  volume   = {100-B},
  abstract = {AIMS: The aim of this study was to evaluate the accuracy of implant placement when using robotic assistance during total hip arthroplasty (THA).
PATIENTS AND METHODS: A total of 20 patients underwent a planned THA using preoperative CT scans and robotic-assisted software. There were nine men and 11 women (n = 20 hips) with a mean age of 60.8 years (sd 6.0). Pelvic and femoral bone models were constructed by segmenting both preoperative and postoperative CT scan images. The preoperative anatomical landmarks using the robotic-assisted system were matched to the postoperative 3D reconstructions of the pelvis. Acetabular and femoral component positions as measured intraoperatively and postoperatively were evaluated and compared.
RESULTS: The system reported accurate values for reconstruction of the hip when compared to those measured postoperatively using CT. The mean deviation from the executed overall hip length and offset were 1.6 mm (sd 2.9) and 0.5 mm (sd 3.0), respectively. Mean combined anteversion was similar and correlated between intraoperative measurements and postoperative CT measurements (32.5°, sd 5.9° versus 32.2°, sd 6.4°; respectively; R<sup>2</sup> = 0.65; p &lt; 0.001). There was a significant correlation between mean intraoperative (40.4°, sd 2.1°) acetabular component inclination and mean measured postoperative inclination (40.12°, sd 3.0°, R<sup>2</sup> = 0.62; p &lt; 0.001). There was a significant correlation between mean intraoperative version (23.2°, sd 2.3°), and postoperatively measured version (23.0°, sd 2.4°; R<sup>2</sup> = 0.76; p &lt; 0.001). Preoperative and postoperative femoral component anteversion were significantly correlated with one another (R<sup>2</sup> = 0.64; p &lt; 0.001). Three patients had CT scan measurements that differed substantially from the intraoperative robotic measurements when evaluating stem anteversion.
CONCLUSION: This is the first study to evaluate the success of hip reconstruction overall using robotic-assisted THA. The overall hip reconstruction obtained in the operating theatre using robotic assistance accurately correlated with the postoperative component position assessed independently using CT based 3D modelling. Clinical correlation during surgery should continue to be practiced and compared with observed intraoperative robotic values. Cite this article: Bone Joint J 2018;100-B:1303-9.},
  doi      = {10.1302/0301-620x.100b10-bjj-2018-0201.r1},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1107454237},
}

@Article{Kamara2016,
  author   = {Eli Kamara and Jonathon Robinson and Marcel A Bas and Jose A Rodriguez and Matthew S Hepinstall},
  journal  = {The Journal of Arthroplasty},
  title    = {Adoption of Robotic vs Fluoroscopic Guidance in Total Hip Arthroplasty: Is Acetabular Positioning Improved in the Learning Curve?},
  year     = {2016},
  number   = {1},
  pages    = {125-130},
  volume   = {32},
  abstract = {BACKGROUND: Acetabulum positioning affects dislocation rates, component impingement, bearing surface wear rates, and need for revision surgery. Novel techniques purport to improve the accuracy and precision of acetabular component position, but may have a significant learning curve. Our aim was to assess whether adopting robotic or fluoroscopic techniques improve acetabulum positioning compared to manual total hip arthroplasty (THA) during the learning curve.
METHODS: Three types of THAs were compared in this retrospective cohort: (1) the first 100 fluoroscopically guided direct anterior THAs (fluoroscopic anterior [FA]) done by a surgeon learning the anterior approach, (2) the first 100 robotic-assisted posterior THAs done by a surgeon learning robotic-assisted surgery (robotic posterior [RP]), and (3) the last 100 manual posterior (MP) THAs done by each surgeon (200 THAs) before adoption of novel techniques. Component position was measured on plain radiographs. Radiographic measurements were taken by 2 blinded observers. The percentage of hips within the surgeons' "target zone" (inclination, 30°-50°; anteversion, 10°-30°) was calculated, along with the percentage within the "safe zone" of Lewinnek (inclination, 30°-50°; anteversion, 5°-25°) and Callanan (inclination, 30°-45°; anteversion, 5°-25°). Relative risk (RR) and absolute risk reduction (ARR) were calculated. Variances (square of the standard deviations) were used to describe the variability of cup position.
RESULTS: Seventy-six percentage of MP THAs were within the surgeons' target zone compared with 84% of FA THAs and 97% of RP THAs. This difference was statistically significant, associated with a RR reduction of 87% (RR, 0.13 [0.04-0.40]; P < .01; ARR, 21%; number needed to treat, 5) for RP compared to MP THAs. Compared to FA THAs, RP THAs were associated with a RR reduction of 81% (RR, 0.19 [0.06-0.62]; P < .01; ARR, 13%; number needed to treat, 8). Variances were lower for acetabulum inclination and anteversion in RP THAs (14.0 and 19.5) as compared to the MP (37.5 and 56.3) and FA (24.5 and 54.6) groups. These differences were statistically significant (P < .01).
CONCLUSION: Adoption of robotic techniques delivers significant and immediate improvement in the precision of acetabular component positioning during the learning curve. While fluoroscopy has been shown to be beneficial with experience, a learning curve exists before precision improves significantly.},
  doi      = {10.1016/j.arth.2016.06.039},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1033569916},
}

@Article{Kleeblad2018,
  author   = {Laura J Kleeblad and Todd A Borus and Thomas M Coon and Jon Dounchis and Joseph T Nguyen and Andrew D Pearle},
  journal  = {The Journal of Arthroplasty},
  title    = {Midterm Survivorship and Patient Satisfaction of Robotic-Arm-Assisted Medial Unicompartmental Knee Arthroplasty: A Multicenter Study},
  year     = {2018},
  number   = {6},
  pages    = {1719-1726},
  volume   = {33},
  abstract = {BACKGROUND: Studies have showed improved accuracy of lower leg alignment, precise component position, and soft-tissue balance with robotic-assisted unicompartmental knee arthroplasty (UKA). No studies, however, have assessed the effect on midterm survivorship. Therefore, the purpose of this prospective, multicenter study was to determine midtem survivorship, modes of failure, and satisfaction of robotic-assisted medial UKA.
METHODS: A total of 473 consecutive patients (528 knees) underwent robotic-arm-assisted medial UKA surgery at 4 separate institutions between March 2009 and December 2011. All patients received a fixed-bearing, metal-backed onlay tibial component. Each patient was contacted at minimum 5-year follow-up and asked a series of questions to determine survival and satisfaction. Kaplan-Meier method was used to determine survivorship.
RESULTS: Data were collected for 384 patients (432 knees) with a mean follow-up of 5.7 years (5.0-7.7). The follow-up rate was 81.2%. In total, 13 revisions were performed, of which 11 knees were converted to total knee arthroplasty and in 2 cases 1 UKA component was revised, resulting in 97% survivorship. The mean time to revision was 2.27 years. The most common failure mode was aseptic loosening (7/13). Fourteen reoperations were reported. Of all unrevised patients, 91% was either very satisfied or satisfied with their knee function.
CONCLUSION: Robotic-arm-assisted medial UKA showed high survivorship and satisfaction at midterm follow-up in this prospective, multicenter study. However, in spite of the robotic technique, early fixation failure remains the primary cause for revision with cemented implants. Comparative studies are necessary to confirm these findings and compare to conventional implanted UKA and total knee arthroplasty.},
  doi      = {10.1016/j.arth.2018.01.036},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1100744263},
}

@Article{Kayani2019,
  author   = {Babar Kayani and Sujith Konan and Atif Ayuob and Elliot Onochie and Talal Al-Jabri and Fares S Haddad},
  journal  = {EFORT Open Reviews},
  title    = {Robotic technology in total knee arthroplasty: a systematic review.},
  year     = {2019},
  note     = {https://doi.org/10.1302/2058-5241.4.190022},
  number   = {10},
  pages    = {611-617},
  volume   = {4},
  abstract = {Robotic total knee arthroplasty (TKA) improves the accuracy of implant positioning and reduces outliers in achieving the planned limb alignment compared to conventional jig-based TKA.Robotic TKA does not have a learning curve effect for achieving the planned implant positioning. The learning curve for achieving operative times comparable to conventional jig-based TKA is 7-20 robotic TKA cases.Cadaveric studies have shown robotic TKA is associated with reduced iatrogenic injury to the periarticular soft tissue envelope compared to conventional jig-based TKA.Robotic TKA is associated with decreased postoperative pain, enhanced early functional rehabilitation, and decreased time to hospital discharge compared to conventional jig-based TKA. However, there are no differences in medium- to long-term functional outcomes between conventional jig-based TKA and robotic TKA.Limitations of robotic TKA include high installation costs, additional radiation exposure, learning curves for gaining surgical proficiency, and compatibility of the robotic technology with a limited number of implant designs.Further higher quality studies are required to compare differences in conventional TKA versus robotic TKA in relation to long-term functional outcomes, implant survivorship, time to revision surgery, and cost-effectiveness. Cite this article: <i>EFORT Open Rev</i> 2019;4:611-617. DOI: 10.1302/2058-5241.4.190022.},
  doi      = {10.1302/2058-5241.4.190022},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1121440882},
}

@Article{Hansen2014,
  author   = {Dane C Hansen and Sharat K Kusuma and Ryan M Palmer and Kira B Harris},
  journal  = {The Journal of Arthroplasty},
  title    = {Robotic Guidance Does Not Improve Component Position or Short-Term Outcome in Medial Unicompartmental Knee Arthroplasty},
  year     = {2014},
  number   = {9},
  pages    = {1784-1789},
  volume   = {29},
  abstract = {We performed a retrospective review in a matched group of patients on the use of robotic-assisted UKA implantation versus UKA performed using standard operative techniques to assess differences between procedures. While both techniques resulted in reproducible and excellent outcomes with low complication rates, the results demonstrate little to no clinical or radiographic difference in outcomes between cohorts. Average operative time differed significantly with, and average of 20 minutes greater in, the robotic-assisted UKA group (P=0.010). Our minimal clinical and radiographic differences lend to the argument that it is difficult to justify the routine use of expensive robotic techniques for standard medial UKA surgery, especially in a well-trained, high-volume surgeon. Further surgical, clinical and economical study of this technology is necessary.},
  doi      = {10.1016/j.arth.2014.04.012},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1028449260},
}

@Article{Swank2009,
  author   = {Michael L Swank and Martha Alkire and Michael Conditt and Jess H Lonner},
  journal  = {The American Journal of Orthopedics},
  title    = {Technology and cost-effectiveness in knee arthroplasty: computer navigation and robotics.},
  year     = {2009},
  number   = {2 Suppl},
  pages    = {32-6},
  volume   = {38},
  abstract = {Our aim in this article is to describe the impact that navigation technology has had on the market share of a community hospital and, specifically, to determine whether a high-volume surgeon using these technologies actually costs the hospital more than other surgeons at the same hospital and more than national means. In addition, we develop a comparable cost-effectiveness model for robotic technology in unicompartmental knee arthroplasty to demonstrate the potential cost-effectiveness at the same hospital.},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1077881457},
}

@Article{Plate2015,
  author   = {Johannes F. Plate and Marco A. Augart and Thorsten M. Seyler and Daniel N. Bracey and Aneitra Hoggard and Michael Akbar and Riyaz H. Jinnah and Gary G. Poehling},
  journal  = {Knee Surgery, Sports Traumatology, Arthroscopy},
  title    = {Obesity has no effect on outcomes following unicompartmental knee arthroplasty},
  year     = {2015},
  number   = {3},
  pages    = {645-651},
  volume   = {25},
  abstract = {PURPOSE: Although obesity has historically been described as a contraindication to UKA, improved outcomes with modern UKA implant designs have challenged this perception. The purpose of this study was to assess the influence of obesity on the outcomes of UKA with a robotic-assisted system at a minimum follow-up of 24 months with the hypothesis that obesity has no effect on robotic-assisted UKA outcomes.
METHODS: There were 746 medial robotic-assisted UKAs (672 patients) with a mean age of 64 years (SD 11) and a mean follow-up time of 34.6 months (SD 7.8). Mean overall body mass index (BMI) was 32.1 kg/m<sup>2</sup> (SD 6.5), and patients were stratified into seven weight categories according to the World Health Organization classification.
RESULTS: Patient BMI did not influence the rate of revision surgery to TKA (5.8 %) or conversion from InLay to OnLay design (1.7 %, n.s.). Mean postoperative Oxford knee score was 37 (SD 11) without correlation with BMI (n.s.). The type of prosthesis (InLay/OnLay) regardless of BMI had no influence on revision rate (n.s.). BMI did not influence 90-day readmissions (4.4 %, n.s.), but showed significant correlation with higher opioid medication requirements and a higher number of physical therapy session needed to reach discharge goals (p = 0.031).
CONCLUSION: These findings suggest that BMI does not influence clinical outcomes and readmission rates of robotic-assisted UKA at mid-term. The classic contraindication of BMI &gt;30 kg/m<sup>2</sup> may not be justified with the use of modern UKA designs or techniques.
LEVEL OF EVIDENCE: IV.},
  doi      = {10.1007/s00167-015-3597-5},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1039984345},
}

@Article{Illgen2017,
  author   = {Richard L Illgen and Brandon R Bukowski and Rasheed Abiola and Paul Anderson and Morad Chughtai and Anton Khlopas and Michael A Mont},
  journal  = {Surgical Technology Online},
  title    = {Robotic-Assisted Total Hip Arthroplasty: Outcomes at Minimum Two-Year Follow-Up.},
  year     = {2017},
  pages    = {365-372},
  volume   = {30},
  abstract = {BACKGROUND: Component malposition in total hip arthroplasty (THA) contributes to instability and early failure. Robotic-assisted total hip arthroplasty (rTHA) utilizes CT-based planning with haptically-guided bone preparation and implant insertion to optimize component position accuracy. This study compared acetabular component position and postoperative complications following manual THA (mTHA) with rTHA.
MATERIALS AND METHODS: Consecutive primary THAs performed by one surgeon at three intervals were analyzed in this retrospective cohort study: the initial 100 consecutive manual THAs (mTHA) in clinical practice (year 2000), the last consecutive 100 mTHA before rTHA introduction (year 2011), and the first consecutive 100 rTHA (year 2012). Acetabular abduction (AAB) and anteversion (AAV) angles were measured using validated software. The Lewinnek safe zone was used to define accuracy (AAB 40°±10° and AAV 15°±10°). Comparisons included operative time, estimated blood loss (EBL), infection rate, and dislocation rate.
RESULTS: The rate of acetabular component placement within Lewinnek safe zone was the highest in the rTHA cohort (77%), followed by late mTHA (45%) and early mTHA (30%) (p<0.001). Robotic-assisted THA resulted in an additional 71% improvement in accuracy in the first year of use (p<0.001). Dislocation rate was 5% with early mTHA, 3% in the late mTHA cohort, and 0% in the rTHA cohort within the first two years postoperatively. There were no statistically significant differences in the rate of infection between groups.
CONCLUSION: Robotic-assisted THA improved acetabular component accuracy and reduced dislocation rates when compared with mTHA. Further study is needed to determine if similar improvements will be noted in larger multicenter studies using alternative surgical approaches.},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1085617296},
}

@Article{Yang2017,
  author   = {Hong Yeol Yang and Jong Keun Seon and Young Joo Shin and Hong An Lim and Eun Kyoo Song},
  journal  = {Clinics in Orthopedic Surgery},
  title    = {Robotic Total Knee Arthroplasty with a Cruciate-Retaining Implant: A 10-Year Follow-up Study},
  year     = {2017},
  note     = {https://doi.org/10.4055/cios.2017.9.2.169},
  number   = {2},
  pages    = {169-176},
  volume   = {9},
  abstract = {BACKGROUND: This study compared clinical and radiological results between robotic total knee arthroplasty (TKA) and conventional TKA with a cruciate-retaining implant at 10-year follow-up. The hypothesis was that robotic TKA would allow for more accurate leg alignment and component placement, and thus enhance clinical and radiological results and long-term survival rates.
METHODS: A total of 113 primary TKAs performed using a cruciate-retaining implant in 102 patients from 2004 to 2007 were reviewed retrospectively. Of the 113 TKAs, 71 were robotic TKAs and 42 were conventional TKAs. Clinical outcomes (visual analogue scale pain score, Hospital for Special Surgery score, Western Ontario and McMaster University score, range of motion, and complications), radiological outcomes, and long-term survival rates were evaluated at a mean follow-up of 10 years.
RESULTS: Clinical outcomes and long-term survival rates were similar between the two groups. Regarding the radiological outcomes, the robotic TKA group had significantly fewer postoperative leg alignment outliers (femoral coronal inclination, tibial coronal inclination, femoral sagittal inclination, tibial sagittal inclination, and mechanical axis) and fewer radiolucent lines than the conventional TKA group.
CONCLUSIONS: Both robotic and conventional TKAs resulted in good clinical outcomes and postoperative leg alignments. Robotic TKA appeared to reduce the incidence of leg alignment outliers and radiolucent lines compared to conventional TKA.},
  doi      = {10.4055/cios.2017.9.2.169},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1085423980},
}

@Article{Nishihara2006,
  author   = {Shunsaku Nishihara and Nobuhiko Sugano and Takashi Nishii and Hidenobu Miki and Nobuo Nakamura and Hideki Yoshikawa},
  journal  = {The Journal of Arthroplasty},
  title    = {Comparison Between Hand Rasping and Robotic Milling for Stem Implantation in Cementless Total Hip Arthroplasty},
  year     = {2006},
  number   = {7},
  pages    = {957-966},
  volume   = {21},
  abstract = {We evaluated the effects of conventional hand rasping and robotic milling on the clinical and radiographic results of cementless total hip arthroplasty, with the same computed tomography (CT)-based 3-dimensional preoperative planning using a ROBODOC workstation (Integrated Surgical Systems, Davis, Calif). The robotic milling group consisted of 78 hips, and the hand-rasping group 78 hips. The radiographic findings from the preoperative planning and postoperative CT data were evaluated using the most accurate CT images reconstructed by the ROBODOC workstation. The robotic milling group showed significant superior Merle D'Aubigne hip score at 2 years. In the robotic milling group, there were no intraoperative femoral fractures, and a radiographically superior implant fit was obtained. Hand rasping had the potential to cause intraoperative femoral fractures, undersizing of the stem, unexpectedly higher vertical seating, and unexpected femoral anteversion causing inferior implant fit.},
  doi      = {10.1016/j.arth.2006.01.001},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1041693000},
}

@Article{Herry2017,
  author   = {Yannick Herry and Cécile Batailler and Timothy Lording and Elvire Servien and Philippe Neyret and Sebastien Lustig},
  journal  = {International Orthopaedics},
  title    = {Improved joint-line restitution in unicompartmental knee arthroplasty using a robotic-assisted surgical technique},
  year     = {2017},
  number   = {11},
  pages    = {2265-2271},
  volume   = {41},
  abstract = {PurposeJoint-line restitution is one objective of unicompartmental knee arthroplasty (UKA). However, the joint line is often lowered when resurfacing femoral implants are used. The aim of this study was to compare the joint-line height in UKA performed by robotic-assisted and conventional techniques.MethodsThis retrospective case–control study compared two matched groups of patients receiving a resurfacing UKA between 2013 and 2016 by either a robotic-assisted (n = 40) or conventional (n = 40) technique. Each group comprised 27 women and 13 menm wuth a mean age of 69 and 68 years, respectively. Indications for surgery were osteoarthritis (n = 35) and condylar osteonecrosis (n = 5). Two validated radiologic measurement methods were used to assess joint-line height.ResultsForty UKA (23 medial and 17 lateral) were analysed in each group. Restitution of joint-line height was significantly improved in the robotic-assisted group compared than the control group: +1.4 mm ±2.6 vs +4.7 mm ± 2.4 (p < 0.05) as assessed using method 1, and +1.5 mm ±2.3 vs +4.6 mm ±2.5 (p < 0.05) as assessed using method 2.ConclusionsRestitution of joint-line height in resurfacing UKA can be improved with robotic-assisted surgery. Improvement in clinical outcome measures must be demonstrated with long-term studies.},
  doi      = {10.1007/s00264-017-3633-9},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1091777297},
}

@Article{Kazanzides1995,
  author   = {P. Kazanzides and B.D. Mittelstadt and B.L. Musits and W.L. Bargar and J.F. Zuhars and B. Williamson and P.W. Cain and E.J. Carbone},
  journal  = {IEEE Pulse},
  title    = {An integrated system for cementless hip replacement},
  year     = {1995},
  number   = {3},
  pages    = {307-313},
  volume   = {14},
  abstract = {Describes robotics and medical imaging technology in the enhancement of precision surgery. The authors consider: cementless total hip replacement application; system overview; ORTHODOC preoperative planning workstation; ROBODOC surgical assistant; safety features; surgical protocol.<>},
  doi      = {10.1109/51.391772},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1061184595},
}

@Article{Gilmour2018,
  author   = {Alisdair Gilmour and Angus D MacLean and Philip J Rowe and Matthew S Banger and Iona Donnelly and Bryn G Jones and Mark J G Blyth},
  journal  = {The Journal of Arthroplasty},
  title    = {Robotic-Arm–Assisted vs Conventional Unicompartmental Knee Arthroplasty. The 2-Year Clinical Outcomes of a Randomized Controlled Trial},
  year     = {2018},
  note     = {https://strathprints.strath.ac.uk/63853/1/Gilmour_etal_JA2018_Robotic_arm_assisted_vs_conventional_unicompartmental_knee.pdf},
  number   = {7},
  pages    = {s109-s115},
  volume   = {33},
  abstract = {BACKGROUND: Unicompartmental knee arthroplasty (UKA) for treatment of medial compartment osteoarthritis has potential benefits over total knee arthroplasty but UKA has a higher revision rate. Robotic-assisted UKA is increasingly common and offers more accurate implant positioning and limb alignment, lower early postoperative pain but evidence of functional outcome is lacking. The aim was to assess the clinical outcomes of a single-centre, prospective, randomised controlled trial, comparing robotic-arm-assisted UKA with conventional surgery.
METHODS: A total of 139 participants were recruited and underwent robotic-arm-assisted (fixed bearing) or conventional (mobile bearing) UKA. Fifty-eight patients in the robotic-arm-assisted group and 54 in the manual group at 2 years. The main outcome measures were the Oxford Knee Score, American Knee Society Score and revision rate.
RESULTS: At 2 years, there were no significant differences for any of the outcome measures. Sub-group analysis (n = 35) of participants with a preoperative University of California Los Angeles Activity Scale >5 (more active) was performed. In this sub-group, the median Oxford Knee Score at 2 years was 46 (IQR 42.0-48.0) for robotic-arm-assisted and 41 (IQR 38.5-44.0) for the manual group (P = .036). The median American Knee Society Score was 193.5 (IQR 184.0-198.0) for the robotic-arm-assisted group and 174.0 (IQR 166.0-188.5) for the manual group (P = .017). Survivorship was 100% in robotic-arm-assisted group and 96.3% in the manual group.
CONCLUSION: Overall, participants achieved an outcome equivalent to the most widely implanted UKA in the United Kingdom. Sub-group analysis suggests that more active patients may benefit from robotic-arm- assisted surgery. Long term follow-up is required to evaluate differences in survivorship.},
  doi      = {10.1016/j.arth.2018.02.050},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1101146035},
}

@Article{Domb2015,
  author   = {Benjamin G Domb and John M Redmond and Steven S Louis and Kris J Alden and Robert J Daley and Justin M LaReau and Alexandra E Petrakos and Chengcheng Gui and Carlos Suarez-Ahedo},
  journal  = {The Journal of Arthroplasty},
  title    = {Accuracy of Component Positioning in 1980 Total Hip Arthroplasties: A Comparative Analysis by Surgical Technique and Mode of Guidance},
  year     = {2015},
  number   = {12},
  pages    = {2208-2218},
  volume   = {30},
  abstract = {The purpose of this multi-surgeon study was to assess and compare the accuracy of acetabular component placement, leg length discrepancy (LLD), and global offset difference (GOD) between six different surgical techniques and modes of guidance in total hip arthroplasty (THA). A total of 1980 THAs met inclusion criteria. Robotic- and navigation-guided techniques were more consistent than other techniques in placing the acetabular cup into Lewinnek's safe zone (P<0.005 and P<0.05, respectively). Robotic-guided surgery was more consistent than other techniques in placing the acetabular component within Callanan's safe zone (P<0.005). No statistically significant differences were found between groups in the frequency of patients with excessive LLD. Clinically significant differences between groups were not found in the frequency of patients with excessive GOD. Level of Evidence: IV.},
  doi      = {10.1016/j.arth.2015.06.059},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1036848872},
}

@Article{Kayani2019a,
  author   = {B Kayani and S Konan and J Tahmassebi and F E Rowan and F S Haddad},
  journal  = {The Bone & Joint Journal},
  title    = {An assessment of early functional rehabilitation and hospital discharge in conventional versus robotic-arm assisted unicompartmental knee arthroplasty: a prospective cohort study.},
  year     = {2019},
  number   = {1},
  pages    = {24-33},
  volume   = {101-B},
  abstract = {AIMS: The objectives of this study were to compare postoperative pain, analgesia requirements, inpatient functional rehabilitation, time to hospital discharge, and complications in patients undergoing conventional jig-based unicompartmental knee arthroplasty (UKA) versus robotic-arm assisted UKA.
PATIENTS AND METHODS: This prospective cohort study included 146 patients with symptomatic medial compartment knee osteoarthritis undergoing primary UKA performed by a single surgeon. This included 73 consecutive patients undergoing conventional jig-based mobile bearing UKA, followed by 73 consecutive patients receiving robotic-arm assisted fixed bearing UKA. All surgical procedures were performed using the standard medial parapatellar approach for UKA, and all patients underwent the same postoperative rehabilitation programme. Postoperative pain scores on the numerical rating scale and opiate analgesia consumption were recorded until discharge. Time to attainment of predefined functional rehabilitation outcomes, hospital discharge, and postoperative complications were recorded by independent observers.
RESULTS: Robotic-arm assisted UKA was associated with reduced postoperative pain (p < 0.001), decreased opiate analgesia requirements (p < 0.001), shorter time to straight leg raise (p < 0.001), decreased number of physiotherapy sessions (p < 0.001), and increased maximum knee flexion at discharge (p < 0.001) compared with conventional jig-based UKA. Mean time to hospital discharge was reduced in robotic UKA compared with conventional UKA (42.5 hours (sd 5.9) vs 71.1 hours (sd 14.6), respectively; p < 0.001). There was no difference in postoperative complications between the two groups within 90 days' follow-up.
CONCLUSION: Robotic-arm assisted UKA was associated with decreased postoperative pain, reduced opiate analgesia requirements, improved early functional rehabilitation, and shorter time to hospital discharge compared with conventional jig-based UKA.},
  doi      = {10.1302/0301-620x.101b1.bjj-2018-0564.r2},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1111052246},
}

@Article{Kayani2018,
  author   = {B Kayani and S Konan and J R T Pietrzak and S S Huq and J Tahmassebi and F S Haddad},
  journal  = {The Bone & Joint Journal},
  title    = {The learning curve associated with robotic-arm assisted unicompartmental knee arthroplasty: a prospective cohort study.},
  year     = {2018},
  number   = {8},
  pages    = {1033-1042},
  volume   = {100-B},
  abstract = {Aims: The primary aim of this study was to determine the surgical team's learning curve for introducing robotic-arm assisted unicompartmental knee arthroplasty (UKA) into routine surgical practice. The secondary objective was to compare accuracy of implant positioning in conventional jig-based UKA versus robotic-arm assisted UKA.
Patients and Methods: This prospective single-surgeon cohort study included 60 consecutive conventional jig-based UKAs compared with 60 consecutive robotic-arm assisted UKAs for medial compartment knee osteoarthritis. Patients undergoing conventional UKA and robotic-arm assisted UKA were well-matched for baseline characteristics including a mean age of 65.5 years (sd 6.8) vs 64.1 years (sd 8.7), (p = 0.31); a mean body mass index of 27.2 kg.m2 (sd 2.7) vs 28.1 kg.m2 (sd 4.5), (p = 0.25); and gender (27 males: 33 females vs 26 males: 34 females, p = 0.85). Surrogate measures of the learning curve were prospectively collected. These included operative times, the Spielberger State-Trait Anxiety Inventory (STAI) questionnaire to assess preoperative stress levels amongst the surgical team, accuracy of implant positioning, limb alignment, and postoperative complications.
Results: Robotic-arm assisted UKA was associated with a learning curve of six cases for operating time (p < 0.001) and surgical team confidence levels (p < 0.001). Cumulative robotic experience did not affect accuracy of implant positioning (p = 0.52), posterior condylar offset ratio (p = 0.71), posterior tibial slope (p = 0.68), native joint line preservation (p = 0.55), and postoperative limb alignment (p = 0.65). Robotic-arm assisted UKA improved accuracy of femoral (p < 0.001) and tibial (p < 0.001) implant positioning with no additional risk of postoperative complications compared to conventional jig-based UKA.
Conclusion: Robotic-arm assisted UKA was associated with a learning curve of six cases for operating time and surgical team confidence levels but no learning curve for accuracy of implant positioning. Cite this article: Bone Joint J 2018;100-B:1033-42.},
  doi      = {10.1302/0301-620x.100b8.bjj-2018-0040.r1},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1105917414},
}

@Article{Liow2016,
  author   = {Ming Han Lincoln Liow and Graham Seow‐Hng Goh and Merng Koon Wong and Pak Lin Chin and Darren Keng‐Jin Tay and Seng‐Jin Yeo},
  journal  = {Knee Surgery, Sports Traumatology, Arthroscopy},
  title    = {Robotic‐assisted total knee arthroplasty may lead to improvement in quality‐of‐life measures: a 2‐year follow‐up of a prospective randomized trial},
  year     = {2016},
  number   = {9},
  pages    = {2942-2951},
  volume   = {25},
  abstract = {PURPOSE: Despite reduction in radiological outliers in previous randomized trials comparing robotic-assisted versus conventional total knee arthroplasty (TKA), no differences in short-term functional outcomes were observed. The aim of this study was to determine whether there was improvement in functional outcomes and quality-of-life (QoL) measures between robotic-assisted and conventional TKA.
METHODS: All 60 knees (31 robotic-assisted; 29 conventional) from a previous randomized trial were available for analysis. Differences in range of motion, Knee Society (KSS) knee and function scores, Oxford Knee scores (OKS), SF-36 subscale and summative (physical PCS/mental component scores MCS) were analysed. In addition, patient satisfaction, fulfilment of expectations and the proportion attaining a minimum clinically important difference (MCID) in KSS, OKS and SF-36 were studied.
RESULTS: Both robotic-assisted and conventional TKA displayed significant improvements in majority of the functional outcome scores at 2 years. Despite having a higher rate of complications, the robotic-assisted group displayed a trend towards higher scores in SF-36 QoL measures, with significant differences in SF-36 vitality (p = 0.03), role emotional (p = 0.02) and a larger proportion of patients achieving SF-36 vitality MCID (48.4 vs 13.8 %, p = 0.009). No significant differences in KSS, OKS or satisfaction/expectation rates were noted.
CONCLUSION: Subtle improvements in patient QoL measures were observed in robotic-assisted TKA when compared to conventional TKA. This finding suggests that QoL measures may be more sensitive and clinically important than surgeon-driven objective scores in detecting subtle functional improvements in robotic-assisted TKA patients.
LEVEL OF EVIDENCE: II.},
  doi      = {10.1007/s00167-016-4076-3},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1053567618},
}

@Article{Sodhi2017,
  author   = {Nipun Sodhi and Anton Khlopas and Nicolas S Piuzzi and Assem A Sultan and Robert C Marchand and Arthur L Malkani and Michael A Mont},
  journal  = {The Journal of Knee Surgery},
  title    = {The Learning Curve Associated with Robotic Total Knee Arthroplasty},
  year     = {2017},
  number   = {01},
  pages    = {017-021},
  volume   = {31},
  abstract = {As with most new surgical technologies, there is an associated learning curve with robotic-assisted total knee arthroplasty (TKA) before surgeons can expect ease of use to be similar to that of manual cases. Therefore, the purpose of this study was to (1) assess robotic-assisted versus manual operative times of two joint reconstructive surgeons separately as well as (2) find an overall learning curve. A total of 240 robotic-assisted TKAs performed by two board-certified surgeons were analyzed. The cases were sequentially grouped into 20 cases and a learning curve was created based on mean operative times. For each surgeon, mean operative times for their first 20 and last 20 robotic-assisted cases were compared with 20 randomly selected manual cases performed by that surgeon as controls prior to the initiation of the robotic-assisted cases. Each of the surgeons first 20 robotic assisted, last 20 robotic assisted, and 20 controls were then combined to create 3 cohorts of 40 cases for analysis. <i>Surgeon 1</i>: First and last robotic cohort operative times were 81 and 70 minutes (<i>p</i> &lt; 0.05). Mean operative times for the first 20 robotic-assisted cases and manual cases were 81 versus 68 minutes (<i>p</i> &lt; 0.05). Mean operative times for the last 20 robotic-assisted cases and manual cases were 70 versus 68 minutes (<i>p</i> &gt; 0.05). <i>Surgeon 2</i>: First and last robotic cohort operative times were 117 and 98 minutes (<i>p</i> &lt; 0.05). Mean operative times for the first 20 robotic-assisted cases and manual cases were 117 versus 95 (<i>p</i> &lt; 0.05). Mean operative times for the last 20 robotic-cohort cases and manual cases were 98 versus 95 (<i>p</i> &gt; 0.05). A similar trend occurred when the times of two surgeons were combined. The data from this study effectively create a learning curve for the use of robotic-assisted TKA. As both surgeons completed their total cases numbers within similar time frames, these data imply that within a few months, a board-certified orthopaedic joint arthroplasty surgeon should be able to adequately perform robotic TKA without adding any operative times.},
  doi      = {10.1055/s-0037-1608809},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1092914867},
}

@Article{Plate2013,
  author   = {Johannes F. Plate and Ali Mofidi and Sandeep Mannava and Beth P. Smith and Jason E. Lang and Gary G. Poehling and Michael A. Conditt and Riyaz H. Jinnah},
  journal  = {Advances in Orthopedics},
  title    = {Achieving Accurate Ligament Balancing Using Robotic‐Assisted Unicompartmental Knee Arthroplasty},
  year     = {2013},
  note     = {https://downloads.hindawi.com/journals/aorth/2013/837167.pdf},
  number   = {1},
  pages    = {837167},
  volume   = {2013},
  abstract = {Unicompartmental knee arthroplasty (UKA) allows replacement of a single compartment in patients with limited disease. However, UKA is technically challenging and relies on accurate component positioning and restoration of natural knee kinematics. This study examined the accuracy of dynamic, real-time ligament balancing using a robotic-assisted UKA system. Surgical data obtained from the computer system were prospectively collected from 51 patients (52 knees) undergoing robotic-assisted medial UKA by a single surgeon. Dynamic ligament balancing of the knee was obtained under valgus stress prior to component implantation and then compared to final ligament balance with the components in place. Ligament balancing was accurate up to 0.53 mm compared to the preoperative plan, with 83% of cases within 1 mm at 0°, 30°, 60°, 90°, and 110° of flexion. Ligamentous laxity of 1.31 ± 0.13 mm at 30° of flexion was corrected successfully to 0.78 ± 0.17 mm (P < 0.05). Robotic-assisted UKA allows accurate and precise reproduction of a surgical balance plan using dynamic, real-time soft-tissue balancing to help restore natural knee kinematics, potentially improving implant survival and functional outcomes.},
  doi      = {10.1155/2013/837167},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1022546608},
}

@Article{Batailler2018,
  author   = {Cécile Batailler and Nathan White and Filippo Maria Ranaldi and Philippe Neyret and Elvire Servien and Sébastien Lustig},
  journal  = {Knee Surgery, Sports Traumatology, Arthroscopy},
  title    = {Improved implant position and lower revision rate with robotic‐assisted unicompartmental knee arthroplasty},
  year     = {2018},
  number   = {4},
  pages    = {1232-1240},
  volume   = {27},
  abstract = {PURPOSE: The aim of this case-control study was to compare implant position and revision rate for UKA, performed with either a robotic-assisted system or with conventional technique.
METHODS: Eighty UKA (57 medial, 23 lateral) were performed with robotic assistance (BlueBelt Navio system) between 2013 and 2017. These patients were matched with 80 patients undergoing UKA using the same prosthesis, implanted using conventional technique. The sagittal and coronal component position was assessed on postoperative radiographs. The revision rate was reported at last follow-up.
RESULTS: The mean follow-up was 19.7 months ± 9 for the robotic-assisted group, and 24.2 months ± 16 for the control group. The rate of postoperative limb alignment outliers (± 2°) was significantly higher in the control group than in the robotic-assisted group for both lateral UKA (26% in robotic group versus 61% in control group; p = 0.018) and medial UKA (16% versus 32%, resp.; p = 0.038). The coronal and sagittal tibial baseplate position had significantly less outliers (± 3°) in the robotic-assisted group, than in the control group. Revision rates were: 5% (n = 4/80) for robotic assisted UKA and 9% (n = 7/80) for conventional UKA (n.s.). The reasons for revision were different between groups, with 86% of revisions in the control group occurring in association with component malposition or limb malalignment, compared with none in the robotic-assisted group.
CONCLUSION: Robotic-assisted UKA has a lower rate of postoperative limb alignment outliers, as well as a lower revision rate, compared to conventional technique. The accuracy of implant positioning is improved by this robotic-assisted system.
LEVEL OF EVIDENCE: Level of evidence III. Retrospective case-control study CLINICAL RELEVANCE: This is the first paper comparing implant position, clinical outcome, and revision rate for UKA performed using the Navio robotic system with a control group.},
  doi      = {10.1007/s00167-018-5081-5},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1105926995},
}

@Article{Nakamura2009,
  author   = {Nobuo Nakamura and Nobuhiko Sugano and Takashi Nishii and Akihiro Kakimoto and Hidenobu Miki},
  journal  = {Clinical Orthopaedics and Related Research®},
  title    = {A Comparison between Robotic-assisted and Manual Implantation of Cementless Total Hip Arthroplasty},
  year     = {2009},
  note     = {https://europepmc.org/articles/pmc2835605?pdf=render},
  number   = {4},
  pages    = {1072-1081},
  volume   = {468},
  abstract = {BackgroundThe benefits of robotic techniques for implanting femoral components during THA are still controversial.Questions/PurposesThe purpose of this study was to prospectively compare the results and complications of robotic-assisted and hand-rasping stem implantation techniques.MethodThe minimum followup was 5 years (mean, 67 months; range, 60–85 months). One hundred forty-six primary THAs on 130 patients were included in this study. Robot-assisted primary THA was performed on 75 hips and a hand-rasping technique was used on 71 hips.ResultsAt 2 and 3 years postoperatively, the Japanese Orthopaedic Association (JOA) clinical score was slightly better in the robotic-assisted group. At 5 years followup, however, the differences were not significant. Postoperative limb lengths of the robotic-milling group had significantly less variance than the hand-rasping group. At 2 years postoperatively, there was significantly more stress shielding of the proximal femur in the hand-rasping group; this difference was more significant 5 years postoperatively.ConclusionsSubstantially more precise implant positioning seems to have led to less variance in limb-length inequality and less stress shielding of the proximal femur 5 years postoperatively.Level of Evidence Level II, therapeutic study. See Guidelines for Authors for a complete description of levels of evidence.},
  doi      = {10.1007/s11999-009-1158-2},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1023819845},
}

@Article{Moschetti2015,
  author   = {Wayne E Moschetti and Joseph F Konopka and Harry E Rubash and James W Genuario},
  journal  = {The Journal of Arthroplasty},
  title    = {Can Robot-Assisted Unicompartmental Knee Arthroplasty Be Cost-Effective? A Markov Decision Analysis},
  year     = {2015},
  number   = {4},
  pages    = {759-765},
  volume   = {31},
  abstract = {BACKGROUND: Unicompartmental knee arthroplasty (UKA) is a treatment option for single-compartment knee osteoarthritis. Robotic assistance may improve survival rates of UKA, but the cost-effectiveness of robot-assisted UKA is unknown. The purpose of this study was to delineate the revision rate, hospital volume, and robotic system costs for which this technology would be cost-effective.
METHODS: We created a Markov decision analysis to evaluate the costs, outcomes, and incremental cost-effectiveness of robot-assisted UKA in 64-year-old patients with end-stage unicompartmental knee osteoarthritis.
RESULTS: Robot-assisted UKA was more costly than traditional UKA, but offered a slightly better outcome with 0.06 additional quality-adjusted life-years at an incremental cost of $47,180 per quality-adjusted life-years, given a case volume of 100 cases annually. The system was cost-effective when case volume exceeded 94 cases per year, 2-year failure rates were below 1.2%, and total system costs were <$1.426 million.
CONCLUSION: Robot-assisted UKA is cost-effective compared with traditional UKA when annual case volume exceeds 94 cases per year. It is not cost-effective at low-volume or medium-volume arthroplasty centers.},
  doi      = {10.1016/j.arth.2015.10.018},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1047113435},
}

@Article{Marchand2017,
  author   = {Robert C Marchand and Nipun Sodhi and Anton Khlopas and Assem A Sultan and Steven F Harwin and Arthur L Malkani and Michael A Mont},
  journal  = {The Journal of Knee Surgery},
  title    = {Patient Satisfaction Outcomes after Robotic Arm-Assisted Total Knee Arthroplasty: A Short-Term Evaluation},
  year     = {2017},
  note     = {http://www.thieme-connect.de/products/ejournals/pdf/10.1055/s-0037-1607450.pdf},
  number   = {09},
  pages    = {849-853},
  volume   = {30},
  abstract = {Abstract
                  Robotic arm-assisted total knee arthroplasty (RATKA) presents a potential, new added value for orthopedic surgeons. In today's health care system, a major determinant of value can be assessed by patient satisfaction scores. Therefore, the purpose of the study was to analyze patient satisfaction outcomes between RATKA and manual total knee arthroplasty (TKA). Specifically, we used the Western Ontario and McMaster Universities Arthritis Index (WOMAC) to compare (1) pain scores, (2) physical function scores, and (3) total patient satisfaction outcomes in manual and RATKA patients at 6 months postoperatively. In this study, 28 cemented RATKAs performed by a single orthopedic surgeon at a high-volume institution were analyzed. The first 7 days were considered as an adjustment period along the learning curve. Twenty consecutive cemented RATKAs were matched and compared with 20 consecutive cemented manual TKAs performed immediately. Patients were administered a WOMAC satisfaction survey at 6 months postoperatively. Satisfaction scores between the two cohorts were compared and the data were analyzed using Student's t-tests. A p-value < 0.05 was used to determine statistical significance. The mean pain score, standard deviation (SD), and range for the manual and robotic cohorts were 5 ± 3 (range: 0–10) and 3 ± 3 (range: 0–8, p < 0.05), respectively. The mean physical function score, SD, and range for the manual and robotic cohorts were 9 ± 5 (range: 0–17) and 4 ± 5 (range, 0–14, p = 0.055), respectively. The mean total patient satisfaction score, SD, and range for the manual and robotic cohorts were 14 points (range: 0–27 points, SD: ± 8) and 7 ± 8 points (range: 0–22 points, p < 0.05), respectively. The results from this study further highlight the potential of this new surgical tool to improve short-term pain, physical function, and total satisfaction scores. Therefore, it appears that patients who undergo RATKA can expect better short-term outcomes when compared with patients who undergo manual TKA.},
  doi      = {10.1055/s-0037-1607450},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1092198359},
}

@Article{Kayani2018a,
  author   = {Babar Kayani and S. Konan and S. S. Huq and J. Tahmassebi and F. S. Haddad},
  journal  = {Knee Surgery, Sports Traumatology, Arthroscopy},
  title    = {Robotic‐arm assisted total knee arthroplasty has a learning curve of seven cases for integration into the surgical workflow but no learning curve effect for accuracy of implant positioning},
  year     = {2018},
  note     = {https://link.springer.com/content/pdf/10.1007/s00167-018-5138-5.pdf},
  number   = {4},
  pages    = {1132-1141},
  volume   = {27},
  abstract = {PURPOSE: The primary objective of this study was to determine the surgical team's learning curve for robotic-arm assisted TKA through assessments of operative times, surgical team comfort levels, accuracy of implant positioning, limb alignment, and postoperative complications. Secondary objectives were to compare accuracy of implant positioning and limb alignment in conventional jig-based TKA versus robotic-arm assisted TKA.
METHODS: This prospective cohort study included 60 consecutive conventional jig-based TKAs followed by 60 consecutive robotic-arm assisted TKAs performed by a single surgeon. Independent observers recorded surrogate markers of the learning curve including operative times, stress levels amongst the surgical team using the state-trait anxiety inventory (STAI) questionnaire, accuracy of implant positioning, limb alignment, and complications within 30 days of surgery. Cumulative summation (CUSUM) analyses were used to assess learning curves for operative time and STAI scores in robotic TKA.
RESULTS: Robotic-arm assisted TKA was associated with a learning curve of seven cases for operative times (p = 0.01) and surgical team anxiety levels (p = 0.02). Cumulative robotic experience did not affect accuracy of implant positioning (n.s.) limb alignment (n.s.) posterior condylar offset ratio (n.s.) posterior tibial slope (n.s.) and joint line restoration (n.s.). Robotic TKA improved accuracy of implant positioning (p < 0.001) and limb alignment (p < 0.001) with no additional risk of postoperative complications compared to conventional manual TKA.
CONCLUSION: Implementation of robotic-arm assisted TKA led to increased operative times and heightened levels of anxiety amongst the surgical team for the initial seven cases but there was no learning curve for achieving the planned implant positioning. Robotic-arm assisted TKA improved accuracy of implant positioning and limb alignment compared to conventional jig-based TKA. The findings of this study will enable clinicians and healthcare professionals to better understand the impact of implementing robotic TKA on the surgical workflow, assist the safe integration of this procedure into surgical practice, and facilitate theatre planning and scheduling of operative cases during the learning phase.
LEVEL OF EVIDENCE: II.},
  doi      = {10.1007/s00167-018-5138-5},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1107054942},
}

@Article{Pearle2017,
  author   = {Andrew D. Pearle and Jelle P. van der List and Lily Lee and Thomas M. Coon and Todd A. Borus and Martin W. Roche},
  journal  = {The Knee},
  title    = {Survivorship and patient satisfaction of robotic-assisted medial unicompartmental knee arthroplasty at a minimum two-year follow-up},
  year     = {2017},
  note     = {https://europepmc.org/articles/pmc5873313?pdf=render},
  number   = {2},
  pages    = {419-428},
  volume   = {24},
  abstract = {BACKGROUND: Successful clinical outcomes following unicompartmental knee arthroplasty (UKA) depend on lower limb alignment, soft tissue balance and component positioning, which can be difficult to control using manual instrumentation. Although robotic-assisted surgery more reliably controls these surgical factors, studies assessing outcomes of robotic-assisted UKA are lacking. Therefore, a prospective multicenter study was performed to assess outcomes of robotic-assisted UKA.
METHODS: A total of 1007 consecutive patients (1135 knees) underwent robotic-assisted medial UKA surgery from six surgeons at separate institutions between March 2009 and December 2011. All patients received a fixed-bearing metal-backed onlay implant as tibial component. Each patient was contacted at minimum two-year follow-up and asked a series of five questions to determine survivorship and patient satisfaction. Worst-case scenario analysis was performed whereby all patients were considered as revision when they declined participation in the study.
RESULTS: Data was collected for 797 patients (909 knees) with average follow-up of 29.6months (range: 22-52months). At 2.5-years of follow-up, 11 knees were reported as revised, which resulted in a survivorship of 98.8%. Thirty-five patients declined participation in the study yielding a worst-case survivorship of 96.0%. Of all patients without revision, 92% was either very satisfied or satisfied with their knee function.
CONCLUSION: In this multicenter study, robotic-assisted UKA was found to have high survivorship and satisfaction rate at short-term follow-up. Prospective comparison studies with longer follow-up are necessary in order to compare survivorship and satisfaction rates of robotic-assisted UKA to conventional UKA and total knee arthroplasty.},
  doi      = {10.1016/j.knee.2016.12.001},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1083695994},
}

@Article{Sugano2013,
  author   = {Nobuhiko Sugano},
  journal  = {Clinics in Orthopedic Surgery},
  title    = {Computer-Assisted Orthopaedic Surgery and Robotic Surgery in Total Hip Arthroplasty},
  year     = {2013},
  note     = {https://doi.org/10.4055/cios.2013.5.1.1},
  number   = {1},
  pages    = {1-9},
  volume   = {5},
  abstract = {Various systems of computer-assisted orthopaedic surgery (CAOS) in total hip arthroplasty (THA) were reviewed. The first clinically applied system was an active robotic system (ROBODOC), which performed femoral implant cavity preparation as programmed preoperatively. Several reports on cementless THA with ROBODOC showed better stem alignment and less variance in limb-length inequality on radiographic evaluation, less incidence of pulmonary embolic events on transesophageal cardioechogram, and less stress shielding on the dual energy X-ray absorptiometry analysis than conventional manual methods. On the other hand, some studies raise issues with active systems, including a steep learning curve, muscle and nerve damage, and technical complications, such as a procedure stop due to a bone motion during cutting, requiring re-registration and registration failure. Semi-active robotic systems, such as Acrobot and Rio, were developed for ease of surgeon acceptance. The drill bit at the tip of the robotic arm is moved by a surgeon's hand, but it does not move outside of a milling path boundary, which is defined according to three-dimensional (3D) image-based preoperative planning. However, there are still few reports on THA with these semi-active systems. Thanks to the advancements in 3D sensor technology, navigation systems were developed. Navigation is a passive system, which does not perform any actions on patients. It only provides information and guidance to the surgeon who still uses conventional tools to perform the surgery. There are three types of navigation: computed tomography (CT)-based navigation, imageless navigation, and fluoro-navigation. CT-based navigation is the most accurate, but the preoperative planning on CT images takes time that increases cost and radiation exposure. Imageless navigation does not use CT images, but its accuracy depends on the technique of landmark pointing, and it does not take into account the individual uniqueness of the anatomy. Fluoroscopic navigation is good for trauma and spine surgeries, but its benefits are limited in the hip and knee reconstruction surgeries. Several studies have shown that the cup alignment with navigation is more precise than that of the conventional mechanical instruments, and that it is useful for optimizing limb length, range of motion, and stability. Recently, patient specific templates, based on CT images, have attracted attention and some early reports on cup placement, and resurfacing showed improved accuracy of the procedures. These various CAOS systems have pros and cons. Nonetheless, CAOS is a useful tool to help surgeons perform accurately what surgeons want to do in order to better achieve their clinical objectives. Thus, it is important that the surgeon fully understands what he or she should be trying to achieve in THA for each patient.},
  doi      = {10.4055/cios.2013.5.1.1},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1045115204},
}

@Article{Dunbar2011,
  author   = {Nicholas J. Dunbar and Martin W. Roche and Brian H. Park and Sharon H. Branch and Michael A. Conditt and Scott A. Banks},
  journal  = {The Journal of Arthroplasty},
  title    = {Accuracy of Dynamic Tactile-Guided Unicompartmental Knee Arthroplasty},
  year     = {2011},
  number   = {5},
  pages    = {803-808.e1},
  volume   = {27},
  abstract = {Unicompartmental knee arthroplasty (UKA) can achieve excellent clinical and functional results for patients having single-compartment osteoarthritis. However, UKA is considered to be technically challenging to perform, and malalignment of implant components significantly contributes to UKA failures. It has been shown that surgical navigation and tactile robotics could be used to provide very accurate component placement when the bones were rigidly fixed in a stereotactic frame during preparation. The purpose of this investigation was to determine the clinically realized accuracy of UKA component placement using surgical navigation and tactile robotics when the bones are free to move. A group of 20 knees receiving medial UKA with dynamically referenced tactile-robotic assistance was studied. Implant placement errors were comparable with those achieved using tactile robotics with rigid stereotactic fixation.},
  doi      = {10.1016/j.arth.2011.09.021},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1030382394},
}

@Article{Jakopec2003,
  author   = {M. Jakopec and F. Rodriguez y Baena and S.J. Harris and P. Gomes and J. Cobb and B.L. Davies},
  journal  = {IEEE Transactions on Robotics and Automation},
  title    = {The hands-on orthopaedic robot &quot;acrobot&quot;: Early clinical trials of total knee replacement surgery},
  year     = {2003},
  number   = {5},
  pages    = {902-911},
  volume   = {19},
  abstract = {A "hands-on" robotic system for total knee replacement (TKR) surgery is presented. A computer tomography-based preoperative planning software is used to accurately plan the procedure. Intraoperatively, the surgeon guides a small special-purpose robot, called Acrobot, which is mounted on a gross positioning device. The Acrobot uses active constraint control, which constrains the motion to a predefined region, and thus allows the surgeon to safely cut the knee bones to fit a TKR prosthesis with high precision. A noninvasive anatomical registration method is described. The system has been successfully used in seven clinical trials with encouraging results.},
  doi      = {10.1109/tra.2003.817510},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1061784304},
}

@Article{Schulz2007,
  author   = {Arndt P. Schulz and Klaus Seide and Christian Queitsch and Andrea von Haugwitz and Jan Meiners and Benjamin Kienast and Mohamad Tarabolsi and Michael Kammal and Christian Jürgens},
  journal  = {International Journal of Medical Robotics and Computer Assisted Surgery},
  title    = {Results of total hip replacement using the Robodoc surgical assistant system: clinical outcome and evaluation of complications for 97 procedures},
  year     = {2007},
  number   = {4},
  pages    = {301-306},
  volume   = {3},
  abstract = {BACKGROUND: A computerized robotic surgical system was developed from 1986 by the Thomas J. Watson Research Center. In 1992 the system unit Orthodoc and the milling robot Robodoc were first used on humans. We present the results achieved with Robodoc-assisted total hip arthroplasty in 97 hips.
METHODS: Between 1997 and 2002, 143 total hip replacements (128 patients) were performed using the Robodoc system. This is a consecutive series. Complete follow-up was possible in 97 hips at a mean follow-up period of 3.8 years.
RESULTS: Technical complications directly related to the robotic device occurred in nine cases (9.3%). The pre-operative Merle d'Aubigne score was determined at 8.1 points compared to a post-operative mean score of 16.2. There was no sign of femoral stem loosening on radiographs.
CONCLUSIONS: Robotic-assisted total hip arthroplasty with the Orthodoc/ Robodoc system achieves equal results as compared to a manual technique. However, there was a high number of technical complications directly or indirectly related to the robot.},
  doi      = {10.1002/rcs.161},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1033163258},
}

@Article{Bellemans2007,
  author   = {Johan Bellemans and Hilde Vandenneucker and Johan Vanlauwe},
  journal  = {Clinical Orthopaedics and Related Research®},
  title    = {Robot-assisted Total Knee Arthroplasty},
  year     = {2007},
  number   = {&NA;},
  pages    = {111-116},
  volume   = {464},
  abstract = {Increasing evidence suggests performing total knee arthroplasty using computer navigation can lead to more accurate surgical positioning of the components and knee alignment compared to a conventional operating technique without computer assistance. The use of robotic technology could theoretically take this accuracy one level further because it uses navigation in combination with ultimate mechanical precision, which could eliminate or reduce the inevitable margin of error during mechanical preparation of the bony cuts of total knee arthroplasty by the surgeon. We prospectively followed 25 consecutive cases using an active surgical robot. The minimum followup was 5.1 years (mean, 5.5 years; range, 5.1-5.8 years). Our results demonstrate excellent implant positioning and alignment was achieved within the 1 degree error of neutral alignment in all three planes in all cases. Despite this technical precision, the excessive operating time required for the robotic implantation, the technical complexity of the system, and the extremely high operational costs have led us to abandon this procedure and direct our interest more toward smart semiactive robotic systems.
LEVEL OF EVIDENCE: Level IV, therapeutic study. See the Guidelines for Authors for a complete description of levels of evidence.},
  doi      = {10.1097/blo.0b013e318126c0c0},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1040431710},
}

@Article{Park2019,
  author   = {Kwan Kyu Park and Chang Dong Han and Ick-Hwan Yang and Woo-Suk Lee and Joo Hyung Han and Hyuck Min Kwon},
  journal  = {PLOS ONE},
  title    = {Robot-assisted unicompartmental knee arthroplasty can reduce radiologic outliers compared to conventional techniques},
  year     = {2019},
  note     = {https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0225941&type=printable},
  number   = {12},
  pages    = {e0225941},
  volume   = {14},
  abstract = {BACKGROUND: The aim of this study was to compare the clinical and radiologic outcomes of robot-assisted unicompartmental knee arthroplasty (UKA) to those of conventional UKA in Asian patients.
METHODS: Fifty-five patients underwent robot-assisted UKA and 57 patients underwent conventional UKA were assessed in this study. Preoperative and postoperative range of motion (ROM), American Knee Society (AKS) score, Western Ontario McMaster University Osteoarthritis Index scale score (WOMAC), and patellofemoral (PF) score values were compared between the two groups. The mechanical femorotibial angle (mFTA) and Kennedy zone were also measured. Coronal alignments of the femoral and tibial components and posterior slopes of the tibial component were compared. Additionally, polyethylene (PE) liner thicknesses were compared.
RESULTS: There was no significant difference between the two groups regarding postoperative ROM, AKS, WOMAC and PF score. Robot group showed fewer radiologic outliers in terms of mFTA and coronal alignment of tibial and femoral components (p = 0.022, 0.037, 0.003). The two groups showed significantly different PE liner thicknesses (8.4 ± 0.8 versus 8.8 ± 0.9, p = 0.035). Robot group was the only influencing factor for reducing radiologic outlier (postoperative mFTA) in multivariate model (odds ratio: 2.833, p = 0.037).
CONCLUSION: In this study, robot-assisted UKA had many advantages over conventional UKA, such as its ability to achieve precise implant insertion and reduce radiologic outliers. Although the clinical outcomes of robot-assisted UKA over a short-term follow-up period were not significantly different compared to those of conventional UKA, longer follow-up period is needed to determine whether the improved radiologic accuracy of the components in robotic-assisted UKA will lead to better clinical outcomes and improved long-term survival.},
  doi      = {10.1371/journal.pone.0225941},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1123098811},
}

@Article{Liow2013,
  author   = {Ming Han Lincoln Liow and Zhan Xia and Merng Koon Wong and Keng Jin Tay and Seng Jin Yeo and Pak Lin Chin},
  journal  = {The Journal of Arthroplasty},
  title    = {Robot-Assisted Total Knee Arthroplasty Accurately Restores the Joint Line and Mechanical Axis. A Prospective Randomised Study},
  year     = {2013},
  number   = {12},
  pages    = {2373-2377},
  volume   = {29},
  abstract = {Robot-assisted Total Knee Arthroplasty (TKA) improves the accuracy and precision of component implantation and mechanical axis (MA) alignment. Joint-line restoration in robot-assisted TKA is not widely described and joint-line deviation of>5mm results in mid-flexion instability and poor outcomes. We prospectively randomised 60 patients into two groups: 31 patients (robot-assisted), 29 patients (conventional). No MA outliers (>±3° from neutral) or notching was noted in the robot-assisted group as compared with 19.4% (P=0.049) and 10.3% (P=0.238) respectively in the conventional group. The robot-assisted group had 3.23% joint-line outliers (>5mm) as compared to 20.6% in the conventional group (P=0.049). Robot-assisted TKA produces similar short-term clinical outcomes when compared to conventional methods with reduction of MA alignment and joint-line deviation outliers.},
  doi      = {10.1016/j.arth.2013.12.010},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1046566936},
}

@Article{Kayani2018b,
  author   = {B. Kayani and S. Konan and J. Tahmassebi and J. R. T. Pietrzak and F. S. Haddad},
  journal  = {The Bone & Joint Journal},
  title    = {Robotic-arm assisted total knee arthroplasty is associated with improved early functional recovery and reduced time to hospital discharge compared with conventional jig-based total knee arthroplasty},
  year     = {2018},
  note     = {https://doi.org/10.1302/0301-620x.100b7.bjj-2017-1449.r1},
  number   = {7},
  pages    = {930-937},
  volume   = {100-B},
  abstract = {Aims: The objective of this study was to compare early postoperative functional outcomes and time to hospital discharge between conventional jig-based total knee arthroplasty (TKA) and robotic-arm assisted TKA.
Patients and Methods: This prospective cohort study included 40 consecutive patients undergoing conventional jig-based TKA followed by 40 consecutive patients receiving robotic-arm assisted TKA. All surgical procedures were performed by a single surgeon using the medial parapatellar approach with identical implant designs and standardized postoperative inpatient rehabilitation. Inpatient functional outcomes and time to hospital discharge were collected in all study patients.
Results: There were no systematic differences in baseline characteristics between the conventional jig-based TKA and robotic-arm assisted TKA treatment groups with respect to age (p = 0.32), gender (p = 0.50), body mass index (p = 0.17), American Society of Anesthesiologists score (p = 0.88), and preoperative haemoglobin level (p = 0.82). Robotic-arm assisted TKA was associated with reduced postoperative pain (p < 0.001), decreased analgesia requirements (p < 0.001), decreased reduction in postoperative haemoglobin levels (p < 0.001), shorter time to straight leg raise (p < 0.001), decreased number of physiotherapy sessions (p < 0.001) and improved maximum knee flexion at discharge (p < 0.001) compared with conventional jig-based TKA. Median time to hospital discharge in robotic-arm assisted TKA was 77 hours (interquartile range (IQR) 74 to 81) compared with 105 hours (IQR 98 to 126) in conventional jig-based TKA (p < 0.001).
Conclusion: Robotic-arm assisted TKA was associated with decreased pain, improved early functional recovery and reduced time to hospital discharge compared with conventional jig-based TKA. Cite this article: Bone Joint J 2018;100-B:930-7.},
  doi      = {10.1302/0301-620x.100b7.bjj-2017-1449.r1},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1105227244},
}

@Article{Park2007,
  author   = {Sang Eun Park and Chun Taek Lee},
  journal  = {The Journal of Arthroplasty},
  title    = {Comparison of Robotic-Assisted and Conventional Manual Implantation of a Primary Total Knee Arthroplasty},
  year     = {2007},
  number   = {7},
  pages    = {1054-1059},
  volume   = {22},
  abstract = {This study was aimed to compare robotic-assisted implantation of a total knee arthroplasty with conventional manual implantation. We controlled, randomized, and reviewed 72 patients for total knee arthroplasty assigned to undergo either conventional manual implantation (excluding navigation-assisted implantation cases) of a Zimmer LPS prosthesis (Zimmer, Warsaw, Ind) (30 patients: group 1) or robotic-assisted implantation of such a prosthesis (32 patients: group 2). The femoral flexion angle (gamma angle) and tibial angle (delta angle) in the lateral x-ray of group 1 were 4.19 +/- 3.28 degrees and 89.7 +/- 1.7 degrees, and those of group 2 were 0.17 +/- 0.65 degrees and 85.5 +/- 0.92 degrees. The major complications were from improper small skin incision during a constraint attempt of minimally invasive surgery and during bulk fixation frame pins insertion. Robotic-assisted technology had definite advantages in terms of preoperative planning, accuracy of the intraoperative procedure, and postoperative follow-up, especially in the femoral flexion angle (gamma angle) and tibial flexion angle (delta angle) in the lateral x-ray, and in the femoral flexion angle (alpha angle) in the anteroposterior x-ray. But a disadvantage was the high complication rate in early stage.},
  doi      = {10.1016/j.arth.2007.05.036},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1003009109},
}

@Article{Lang2011,
  author   = {J. E. Lang and S. Mannava and A. J. Floyd and M. S. Goddard and B. P. Smith and A. Mofidi and T. M. Seyler and R. H. Jinnah},
  journal  = {The Bone & Joint Journal},
  title    = {Robotic systems in orthopaedic surgery.},
  year     = {2011},
  number   = {10},
  pages    = {1296-9},
  volume   = {93},
  abstract = {Robots have been used in surgery since the late 1980s. Orthopaedic surgery began to incorporate robotic technology in 1992, with the introduction of ROBODOC, for the planning and performance of total hip replacement. The use of robotic systems has subsequently increased, with promising short-term radiological outcomes when compared with traditional orthopaedic procedures. Robotic systems can be classified into two categories: autonomous and haptic (or surgeon-guided). Passive surgery systems, which represent a third type of technology, have also been adopted recently by orthopaedic surgeons. While autonomous systems have fallen out of favour, tactile systems with technological improvements have become widely used. Specifically, the use of tactile and passive robotic systems in unicompartmental knee replacement (UKR) has addressed some of the historical mechanisms of failure of non-robotic UKR. These systems assist with increasing the accuracy of the alignment of the components and produce more consistent ligament balance. Short-term improvements in clinical and radiological outcomes have increased the popularity of robot-assisted UKR. Robot-assisted orthopaedic surgery has the potential for improving surgical outcomes. We discuss the different types of robotic systems available for use in orthopaedics and consider the indication, contraindications and limitations of these technologies.},
  doi      = {10.1302/0301-620x.93b10.27418},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1064886544},
}

@Article{Song2011,
  author   = {Eun‐Kyoo Song and Jong‐Keun Seon and Sang‐Jin Park and Woo Bin Jung and Hyeong‐Won Park and Geon Woo Lee},
  journal  = {Knee Surgery, Sports Traumatology, Arthroscopy},
  title    = {Simultaneous bilateral total knee arthroplasty with robotic and conventional techniques: a prospective, randomized study},
  year     = {2011},
  number   = {7},
  pages    = {1069-1076},
  volume   = {19},
  abstract = {PURPOSE: The authors performed this study to compare the outcomes of robotic-assisted and conventional TKA in same patient simultaneously. It was hypothesized that the robotic-assisted procedure would produce better leg alignment and component orientation, and thus, improve patient satisfaction and clinical and radiological outcomes.
METHODS: Thirty patients underwent bilateral sequential total knee replacement. One knee was replaced by robotic-assisted implantation and the other by conventional implantation.
RESULTS: Radiographic results showed significantly more postoperative leg alignment outliers of conventional sides than robotic-assisted sides (mechanical axis, coronal inclination of the femoral prosthesis, and sagittal inclination of the tibial prosthesis). Robotic-assisted sides had non-significantly better postoperative knee scores and ROMs. Robotic-assisted sides needed longer operation times (25 min, SD ± 18) and longer skin incisions. Nevertheless, postoperative bleeding was significantly less for robotic-assisted sides.
CONCLUSION: The better alignment accuracy of robotic TKA and the good clinical results achieved may favorably influence clinical and radiological outcomes.},
  doi      = {10.1007/s00167-011-1400-9},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1020705223},
}

@Article{Jakopec2001,
  author   = {M. Jakopec and S.J. Harris and F. Rodriguez y Baena and P. Gomes and J. Cobb and B.L. Davies},
  journal  = {Computer Assisted Surgery},
  title    = {The first clinical application of a “hands‐on” robotic knee surgery system},
  year     = {2001},
  number   = {6},
  pages    = {329-339},
  volume   = {6},
  abstract = {The performance of a novel "hands-on" robotic system for total knee replacement (TKR) surgery is evaluated. An integrated robotic system for accurately machining the bone surfaces in TKR surgery is described. Details of the system, comprising an "active constraint" robot, called Acrobot, a "gross positioning" robot, and patient clamps, are provided. The intraoperative protocol and the preoperative, CT-based, planning system are also described. A number of anatomical registration and cutting trials, using plastic bones, are described, followed by results from two preliminary clinical trials, which demonstrate the accuracy achieved in the anatomical registration. Finally, the first clinical trial is described, in which the results of the anatomical registration and bone cutting are seen to be of high quality. The Acrobot system has been successfully used to accurately register and cut the knee bones in TKR surgery. This demonstrates the great potential of a "hands-on" robot for improving accuracy and increasing safety in surgery.},
  doi      = {10.1002/igs.10023},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1034566767},
}

@Article{Lonner2009,
  author   = {Jess H. Lonner and Thomas K. John and Michael A. Conditt},
  journal  = {Clinical Orthopaedics and Related Research®},
  title    = {Robotic Arm‐assisted UKA Improves Tibial Component Alignment: A Pilot Study},
  year     = {2009},
  note     = {https://europepmc.org/articles/pmc2795844?pdf=render},
  number   = {1},
  pages    = {141-146},
  volume   = {468},
  abstract = {The alignment of the components of unicompartmental knee arthroplasty (UKA) reportedly influences outcomes and durability. A novel robotic arm technology has been developed with the expectation that it could improve the accuracy of bone preparation in UKA. During the study period, we compared the postoperative radiographic alignment of the tibial component with the preoperatively planned position in 31 knees in 31 consecutive patients undergoing UKA using robotic arm-assisted bone preparation and in 27 consecutive patients who underwent unilateral UKA using conventional manual instrumentation to determine the error of bone preparation and variance with each technique. Radiographically, the root mean square error of the posterior tibial slope was 3.1 degrees when using manual techniques compared with 1.9 degrees when using robotic arm assistance for bone preparation. In addition, the variance using manual instruments was 2.6 times greater than the robotically guided procedures. In the coronal plane, the average error was 2.7 degrees +/- 2.1 degrees more varus of the tibial component relative to the mechanical axis of the tibia using manual instruments compared with 0.2 degrees +/- 1.8 degrees with robotic technology, and the varus/valgus root mean square error was 3.4 degrees manually compared with 1.8 degrees robotically. Further study will be necessary to determine whether a reduction in alignment errors of these magnitudes will ultimately influence implant function or survival.
LEVEL OF EVIDENCE: Level III, therapeutic study. See Guidelines for Authors for a complete description of levels of evidence.},
  doi      = {10.1007/s11999-009-0977-5},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1014011841},
}

@Article{Domb2013,
  author   = {Benjamin G. Domb and Youssef F. El Bitar and Adam Y. Sadik and Christine E. Stake and Itamar B. Botser},
  journal  = {Clinical Orthopaedics and Related Research®},
  title    = {Comparison of Robotic-assisted and Conventional Acetabular Cup Placement in THA: A Matched-pair Controlled Study},
  year     = {2013},
  note     = {https://europepmc.org/articles/pmc3889439?pdf=render},
  number   = {1},
  pages    = {329-336},
  volume   = {472},
  abstract = {BackgroundImproper acetabular component orientation in THA has been associated with increased dislocation rates, component impingement, bearing surface wear, and a greater likelihood of revision. Therefore, any reasonable steps to improve acetabular component orientation should be considered and explored.Questions/purposesWe therefore sought to compare THA with a robotic-assisted posterior approach with manual alignment techniques through a posterior approach, using a matched-pair controlled study design, to assess whether the use of the robot made it more likely for the acetabular cup to be positioned in the safe zones described by Lewinnek et al. and Callanan et al.MethodsBetween September 2008 and September 2012, 160 THAs were performed by the senior surgeon. Sixty-two patients (38.8%) underwent THA using a conventional posterior approach, 69 (43.1%) underwent robotic-assisted THA using the posterior approach, and 29 (18.1%) underwent radiographic-guided anterior-approach THAs. From September 2008 to June 2011, all patients were offered anterior or posterior approaches regardless of BMI and anatomy. Since introduction of the robot in June 2011, all THAs were performed using the robotic technique through the posterior approach, unless a patient specifically requested otherwise. The radiographic cup positioning of the robotic-assisted THAs was compared with a matched-pair control group of conventional THAs performed by the same surgeon through the same posterior approach. The safe zone (inclination, 30°–50°; anteversion, 5°–25°) described by Lewinnek et al. and the modified safe zone (inclination, 30°–45°; anteversion, 5°–25°) of Callanan et al. were used for cup placement assessment. Matching criteria were gender, age ± 5 years, and (BMI) ± 7 units. After exclusions, a total of 50 THAs were included in each group. Strong interobserver and intraobserver correlations were found for all radiographic measurements (r > 0.82; p < 0.001).ResultsOne hundred percent (50/50) of the robotic-assisted THAs were within the safe zone described by Lewinnek et al. compared with 80% (40/50) of the conventional THAs (p = 0.001). Ninety-two percent (46/50) of robotic-assisted THAs were within the modified safe zone described by Callanan et al. compared with 62% (31/50) of conventional THAs p (p = 0.001). The odds ratios for an implanted cup out of the safe zones of Lewinnek et al. and Callanan et al. were zero and 0.142, respectively (95% CI, 0.044, 0.457).ConclusionsUse of the robot allowed for improvement in placement of the cup in both safe zones, an important parameter that plays a significant role in long-term success of THA. However, whether the radiographic improvements we observed will translate into clinical benefits for patients—such as reductions in component impingement, acetabular wear, and prosthetic dislocations, or in terms of improved longevity—remains unproven.Level of EvidenceLevel III, therapeutic study. See the Instructions for Authors for a complete description of levels of evidence.},
  doi      = {10.1007/s11999-013-3253-7},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1012736542},
}

@Article{Honl2003,
  author   = {Matthias Honl and Oliver Dierk and Christian Gauck and Volker Carrero and Frank Lampe and Sebastian Dries and Markus Quante and Karsten Schwieger and Ekkehard Hille and Michael M Morlock},
  journal  = {Journal of Bone and Joint Surgery},
  title    = {Comparison of robotic-assisted and manual implantation of a primary total hip replacement. A prospective study.},
  year     = {2003},
  number   = {8},
  pages    = {1470-8},
  volume   = {85},
  abstract = {BACKGROUND: Robotic-assisted total hip replacement has become a common method of implantation, especially in Europe. It frequently has been postulated that robotic reaming would result in an improved clinical outcome due to the better fit of the prosthesis, but that has never been demonstrated in a prospective study, to our knowledge. The purpose of this study was to compare robotic-assisted implantation of a total hip replacement with conventional manual implantation.
METHODS: One hundred and fifty-four patients scheduled for total hip replacement were randomly assigned to undergo either conventional manual implantation of an S-ROM prosthesis (eighty patients) or robotic-assisted implantation of such a prosthesis (seventy-four patients). The five-axis ROBODOC was used for the robotic-assisted procedures. Preoperatively as well as at three, six, twelve, and twenty-four months after surgery, the scores according to the Harris and Merle d'Aubigné systems and the Mayo clinical score were determined. Radiographs made at these intervals were analyzed for evidence of loosening, prosthetic alignment, and heterotopic ossification.
RESULTS: Thirteen (18%) of the seventy-four attempted robotic implantations had to be converted to manual implantations as a result of failure of the system. The duration of the robotic procedures was longer than that of the manual procedures (mean and standard deviation,107.1 +/- 29.1 compared with 82.4 +/- 23.4 minutes, p < 0.001). Limb-length equality (mean discrepancy, 0.18 +/- 0.30 compared with 0.96 +/- 0.93 cm, p < 0.001) and varus-valgus orientation of the stem (mean angle between the femur and the shaft of the prosthesis, 0.34 degrees +/- 0.67 degrees compared with 0.84 degrees +/- 1.23 degrees, p < 0.001) were better after the robotic procedures. At six months, slightly more heterotopic ossification was seen in the group treated with robotic implantation. The group treated with robotic implantation had a better Mayo clinical score at six and twelve months and a better Harris score at twelve months; however, by twenty-four months, no difference was found between the groups with regard to any of the three scores. Dislocation was more frequent in the group treated with robotic implantation: it occurred in eleven of the sixty-one patients in that group compared with three of eighty in the other group (p < 0.001). Recurrent dislocation and pronounced limping were indications for revision surgery in eight of the sixty-one patients treated with robotic implantation compared with none of the seventy-eight (excluding two with revision for infection) treated with manual insertion (p < 0.001). Rupture of the gluteus medius tendon was observed during all of the revision operations.
CONCLUSIONS: The robotic-assisted technology had advantages in terms of preoperative planning and the accuracy of the intraoperative procedure. Disadvantages were the high revision rate; the amount of muscle damage, which we believe was responsible for the higher dislocation rate; and the longer duration of surgery. This technology must be further developed before its widespread usage can be justified.},
  doi      = {10.2106/00004623-200308000-00007},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1068895745},
}

@Article{Song2013,
  author   = {EunKyoo Song and JongKeun Seon and JiHyeon Yim and Nathan A. Netravali and William L. Bargar},
  journal  = {Clinical Orthopaedics and Related Research®},
  title    = {Robotic‐assisted TKA Reduces Postoperative Alignment Outliers and Improves Gap Balance Compared to Conventional TKA},
  year     = {2013},
  note     = {https://europepmc.org/articles/pmc3528918?pdf=render},
  number   = {1},
  pages    = {118-126},
  volume   = {471},
  abstract = {BACKGROUND: Several studies have shown mechanical alignment influences the outcome of TKA. Robotic systems have been developed to improve the precision and accuracy of achieving component position and mechanical alignment.
QUESTIONS/PURPOSES: We determined whether robotic-assisted implantation for TKA (1) improved clinical outcome; (2) improved mechanical axis alignment and implant inclination in the coronal and sagittal planes; (3) improved the balance (flexion and extension gaps); and (4) reduced complications, postoperative drainage, and operative time when compared to conventionally implanted TKA over an intermediate-term (minimum 3-year) followup period.
METHODS: We prospectively randomized 100 patients who underwent unilateral TKA into one of two groups: 50 using a robotic-assisted procedure and 50 using conventional manual techniques. Outcome variables considered were postoperative ROM, WOMAC scores, Hospital for Special Surgery (HSS) knee scores, mechanical axis alignment, flexion/extension gap balance, complications, postoperative drainage, and operative time. Minimum followup was 41 months (mean, 65 months; range, 41-81 months).
RESULTS: There were no differences in postoperative ROM, WOMAC scores, and HSS knee scores. The robotic-assisted group resulted in no mechanical axis outliers (> ± 3° from neutral) compared to 24% in the conventional group. There were fewer robotic-assisted knees where the flexion gap exceeded the extension gap by 2 mm. The robotic-assisted procedures took an average of 25 minutes longer than the conventional procedures but had less postoperative blood drainage. There were no differences in complications between groups.
CONCLUSIONS: Robotic-assisted TKA appears to reduce the number of mechanical axis alignment outliers and improve the ability to achieve flexion-extension gap balance, without any differences in clinical scores or complications when compared to conventional manual techniques.},
  doi      = {10.1007/s11999-012-2407-3},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1000309245},
}

@Article{Siebert2002,
  author   = {Werner Siebert and Sabine Mai and Rudolf Kober and Peter F Heeckt},
  journal  = {The Knee},
  title    = {Technique and first clinical results of robot-assisted total knee replacement},
  year     = {2002},
  number   = {3},
  pages    = {173-180},
  volume   = {9},
  abstract = {Total knee replacement (TKR) is a common procedure for treatment of severe gonarthrosis, but the outcome may be unsatisfactory due to primary malalignment of the prosthetic components. In order to improve precision and accuracy of this surgical procedure, a commercial robotic surgical system (CASPAR) has been adapted to assist the surgeon in the preoperative planning and intraoperative execution of TKR. So far, 70 patients with idiopathic gonarthrosis were successfully treated with a robot-assisted technique in our institution. No major adverse events related to the use of the robotic system have been observed. The mean difference between preoperatively planned and postoperatively achieved tibiofemoral alignment was 0.8 degrees (0-4.1 degrees ) in the robotic group vs. 2.6 degrees (0-7 degrees ) in a manually operated historical control group of 50 patients. A clear advantage of robot-assisted TKR seems to be the ability to execute a highly precise preoperative plan based on computed tomography (CT) scans. Due to better alignment of the prosthetic components and improved bone-implant fit, implant loosening is anticipated to be diminished which may be most evident in non-cemented prostheses. Current disadvantages such as the need for placement of fiducial markers, increased operating times and higher overall costs have to be resolved in the future.},
  doi      = {10.1016/s0968-0160(02)00015-7},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1048135360},
}

@Article{Bell2016,
  author   = {Stuart W Bell and Iain Anthony and Bryn Jones and Angus MacLean and Philip Rowe and Mark Blyth},
  journal  = {Journal of Bone and Joint Surgery},
  title    = {Improved Accuracy of Component Positioning with Robotic-Assisted Unicompartmental Knee Arthroplasty},
  year     = {2016},
  note     = {https://strathprints.strath.ac.uk/56946/1/Bell_etal_JBJS_2016_Improved_accuracy_of_component_positioning_with_robotic_assisted.pdf},
  number   = {8},
  pages    = {627-635},
  volume   = {98},
  abstract = {BACKGROUND: Higher revision rates have been reported in patients who have undergone unicompartmental knee arthroplasty compared with patients who have undergone total knee arthroplasty, with poor component positioning identified as a factor in implant failure. A robotic-assisted surgical procedure has been proposed as a method of improving the accuracy of component implantation in arthroplasty. The aim of this prospective, randomized, single-blinded, controlled trial was to evaluate the accuracy of component positioning in unicompartmental knee arthroplasty comparing robotic-assisted and conventional implantation techniques.
METHODS: One hundred and thirty-nine patients were randomly assigned to treatment with either a robotic-assisted surgical procedure using the MAKO Robotic Interactive Orthopaedic Arm (RIO) system or a conventional surgical procedure using the Oxford Phase-3 unicompartmental knee replacement with traditional instrumentation. A postoperative computed tomographic scan was performed at three months to assess the accuracy of the axial, coronal, and sagittal component positioning.
RESULTS: Data were available for 120 patients, sixty-two who had undergone robotic-assisted unicompartmental knee arthroplasty and fifty-eight who had undergone conventional unicompartmental knee arthroplasty. Intraobserver agreement was good for all measured component parameters. The accuracy of component positioning was improved with the use of the robotic-assisted surgical procedure, with lower root mean square errors and significantly lower median errors in all component parameters (p < 0.01). The proportion of patients with component implantation within 2° of the target position was significantly greater in the group who underwent robotic-assisted unicompartmental knee arthroplasty compared with the group who underwent conventional unicompartmental knee arthroscopy with regard to the femoral component sagittal position (57% compared with 26%, p = 0.0008), femoral component coronal position (70% compared with 28%, p = 0.0001), femoral component axial position (53% compared with 31%, p = 0.0163), tibial component sagittal position (80% compared with 22%, p = 0.0001), and tibial component axial position (48% compared with 19%, p = 0.0009).
CONCLUSIONS: Robotic-assisted surgical procedures with the use of the MAKO RIO lead to improved accuracy of implant positioning compared with conventional unicompartmental knee arthroplasty surgical techniques.
LEVEL OF EVIDENCE: Therapeutic Level I. See Instructions for Authors for a complete description of levels of evidence.},
  doi      = {10.2106/jbjs.15.00664},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1068898169},
}

@Article{Jacofsky2016,
  author   = {David J Jacofsky and Mark Allen},
  journal  = {The Journal of Arthroplasty},
  title    = {Robotics in Arthroplasty: A Comprehensive Review},
  year     = {2016},
  number   = {10},
  pages    = {2353-2363},
  volume   = {31},
  abstract = {Robotic-assisted orthopedic surgery has been available clinically in some form for over 2 decades, claiming to improve total joint arthroplasty by enhancing the surgeon's ability to reproduce alignment and therefore better restore normal kinematics. Various current systems include a robotic arm, robotic-guided cutting jigs, and robotic milling systems with a diversity of different navigation strategies using active, semiactive, or passive control systems. Semiactive systems have become dominant, providing a haptic window through which the surgeon is able to consistently prepare an arthroplasty based on preoperative planning. A review of previous designs and clinical studies demonstrate that these robotic systems decrease variability and increase precision, primarily focusing on component positioning and alignment. Some early clinical results indicate decreased revision rates and improved patient satisfaction with robotic-assisted arthroplasty. The future design objectives include precise planning and even further improved consistent intraoperative execution. Despite this cautious optimism, many still wonder whether robotics will ultimately increase cost and operative time without objectively improving outcomes. Over the long term, every industry that has seen robotic technology be introduced, ultimately has shown an increase in production capacity, improved accuracy and precision, and lower cost. A new generation of robotic systems is now being introduced into the arthroplasty arena, and early results with unicompartmental knee arthroplasty and total hip arthroplasty have demonstrated improved accuracy of placement, improved satisfaction, and reduced complications. Further studies are needed to confirm the cost effectiveness of these technologies.},
  doi      = {10.1016/j.arth.2016.05.026},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1053428144},
}

@Article{Delp1998,
  author   = {Scott L. Delp and David S. Stulberg and Brian Davies and Frederic Picard and Francois Leitner},
  journal  = {Clinical Orthopaedics and Related Research®},
  title    = {Computer Assisted Knee Replacement},
  year     = {1998},
  number   = {&NA;},
  pages    = {49-56},
  volume   = {354},
  abstract = {Accurate alignment of knee implants is essential for the success of total knee replacement. Although mechanical alignment guides have been designed to improve alignment accuracy, there are several fundamental limitations of this technology that will inhibit additional improvements. Various computer assisted techniques have been developed to examine the potential to install knee implants more accurately and consistently than can be done with mechanical guides. For example, computer integrated instrumentation incorporates highly accurate measurement devices to locate joint centers, track surgical tools, and align prosthetic components. Image guided knee replacement provides a three-dimensional preoperative plan that guides the placement of the cutting blocks and prosthetic components. Robot assisted knee replacement allows one to machine bones accurately without the use of standard cutting blocks. The rationale for the development of computer assisted knee replacement systems is presented, the operation of several different systems is described, the advantages and disadvantages of different approaches are discussed, and areas for future research are suggested.},
  doi      = {10.1097/00003086-199809000-00007},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1060167433},
}

@Article{Cobb2006,
  author   = {J. Cobb and J. Henckel and P. Gomes and S. Harris and M. Jakopec and F. Rodriguez and A. Barrett and B. Davies},
  journal  = {The Bone & Joint Journal},
  title    = {Hands-on robotic unicompartmental knee replacement},
  year     = {2006},
  number   = {2},
  pages    = {188-197},
  volume   = {88-B},
  abstract = {We performed a prospective, randomised controlled trial of unicompartmental knee arthroplasty comparing the performance of the Acrobot system with conventional surgery. A total of 27 patients (28 knees) awaiting unicompartmental knee arthroplasty were randomly allocated to have the operation performed conventionally or with the assistance of the Acrobot. The primary outcome measurement was the angle of tibiofemoral alignment in the coronal plane, measured by CT. Other secondary parameters were evaluated and are reported. All of the Acrobot group had tibiofemoral alignment in the coronal plane within 2 degrees of the planned position, while only 40% of the conventional group achieved this level of accuracy. While the operations took longer, no adverse effects were noted, and there was a trend towards improvement in performance with increasing accuracy based on the Western Ontario and McMaster Universities Osteoarthritis Index and American Knee Society scores at six weeks and three months. The Acrobot device allows the surgeon to reproduce a pre-operative plan more reliably than is possible using conventional techniques which may have clinical advantages.},
  doi      = {10.1302/0301-620x.88b2.17220},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1051764285},
}

@Article{Bargar1998,
  author   = {William L. Bargar and André Bauer and Martin Börner},
  journal  = {Clinical Orthopaedics and Related Research®},
  title    = {Primary and Revision Total Hip Replacement Using the Robodoc® System},
  year     = {1998},
  number   = {&NA;},
  pages    = {82-91},
  volume   = {354},
  abstract = {The ROBODOC system was designed to address potential human errors in performing cementless total hip replacement. The system consists of a preoperative planning computer workstation (called ORTHODOC) and a five-axis robotic arm with a high speed milling device as an end effector. The combined experience of the United States Food and Drug Administration multicenter trial and the German postmarket use of the system are reported. The United States study is controlled and randomized with 136 hip replacements performed at three centers (65 ROBODOC and 62 control). Followup was 1 year on 127 hip replacements and 2 years on 93 hip replacements. No differences were found in the Harris hip scores or the Short Form Health Survey outcomes questionnaire. Length of stay also was not different, but the surgical time and blood loss were greater in the ROBODOC group. This was attributed to a learning curve at each center. Radiographs were evaluated by an independent bone radiologist and showed statistically better fit and positioning of the femoral component in the ROBODOC group. Complications were not different, except for three cases of intraoperative femoral fracture in the control group and none in the ROBODOC group. The German study reports on 858 patients, 42 with bilateral hip replacements and this includes 30 revision cases for a total of 900 hip replacements. The Harris hip score rose from 43.7 to 91.5. In these cases the surgical time declined quickly from 240 minutes for the first case to 90 minutes. No intraoperative femoral fractures occurred in 900 cases. Other complications were comparable with total hip replacements performed using conventional techniques. The ROBODOC system is thought to be safe and effective in producing radiographically superior implant fit and positioning while eliminating femoral fractures.},
  doi      = {10.1097/00003086-199809000-00011},
  groups   = {Robotic Arthroplasty},
  url      = {https://app.dimensions.ai/details/publication/pub.1060167437},
}

@Article{Tianmiao2017,
  author  = {Tianmiao WANG},
  journal = {Journal of Mechanical Engineering},
  title   = {Soft Robotics:Structure, Actuation, Sensing and Control},
  year    = {2017},
  note    = {http://www.cjmenet.com.cn/Jwk_jxgcxb/CN/article/downloadArticleFile.do?attachType=PDF&id=12122},
  number  = {13},
  pages   = {1},
  volume  = {53},
  doi     = {10.3901/jme.2017.13.001},
  groups  = {Soft Robotics},
  url     = {https://app.dimensions.ai/details/publication/pub.1099769159},
}

@Article{ZHANG2017,
  author  = {Jinhua ZHANG},
  journal = {Journal of Mechanical Engineering},
  title   = {Review of Soft-bodied Manipulator},
  year    = {2017},
  note    = {http://www.cjmenet.com.cn/Jwk_jxgcxb/CN/article/downloadArticleFile.do?attachType=PDF&id=12124},
  number  = {13},
  pages   = {19},
  volume  = {53},
  doi     = {10.3901/jme.2017.13.019},
  groups  = {Soft Robotics},
  url     = {https://app.dimensions.ai/details/publication/pub.1099769145},
}

@Article{Chen2017,
  author   = {Dustin Chen and Qibing Pei},
  journal  = {Chemical Reviews},
  title    = {Electronic Muscles and Skins: A Review of Soft Sensors and Actuators},
  year     = {2017},
  number   = {17},
  pages    = {11239-11268},
  volume   = {117},
  abstract = {This article reviews several classes of compliant materials that can be utilized to fabricate electronic muscles and skins. Different classes of materials range from compliant conductors, semiconductors, to dielectrics, all of which play a vital and cohesive role in the development of next generation electronics. This paper covers recent advances in the development of new materials, as well as the engineering of well-characterized materials for the repurposing in applications of flexible and stretchable electronics. In addition to compliant materials, this article further discusses the use of these materials for integrated systems to develop soft sensors and actuators. These new materials and new devices pave the way for a new generation of electronics that will change the way we see and interact with our devices for decades to come.},
  doi      = {10.1021/acs.chemrev.7b00019},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1091247330},
}

@Article{Polygerinos2017,
  author   = {Panagiotis Polygerinos and Nikolaus Correll and Stephen A. Morin and Bobak Mosadegh and Cagdas D. Onal and Kirstin Petersen and Matteo Cianchetti and Michael T. Tolley and Robert F. Shepherd},
  journal  = {Advanced Engineering Materials},
  title    = {Soft Robotics: Review of Fluid‐Driven Intrinsically Soft Devices; Manufacturing, Sensing, Control, and Applications in Human‐Robot Interaction},
  year     = {2017},
  number   = {12},
  volume   = {19},
  abstract = {The emerging field of soft robotics makes use of many classes of materials including metals, low glass transition temperature (Tg) plastics, and high Tg elastomers. Dependent on the specific design, all of these materials may result in extrinsically soft robots. Organic elastomers, however, have elastic moduli ranging from tens of megapascals down to kilopascals; robots composed of such materials are intrinsically soft − they are always compliant independent of their shape. This class of soft machines has been used to reduce control complexity and manufacturing cost of robots, while enabling sophisticated and novel functionalities often in direct contact with humans. This review focuses on a particular type of intrinsically soft, elastomeric robot − those powered via fluidic pressurization.},
  doi      = {10.1002/adem.201700016},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1085727340},
}

@Article{Lee2017,
  author   = {Chiwon Lee and Myungjoon Kim and Yoon Jae Kim and Nhayoung Hong and Seungwan Ryu and H. Jin Kim and Sungwan Kim},
  journal  = {International Journal of Control, Automation and Systems},
  title    = {Soft robot review},
  year     = {2017},
  number   = {1},
  pages    = {3-15},
  volume   = {15},
  abstract = {Soft robots are often inspired from biological systems which consist of soft materials or are actuated by electrically activated materials. There are several advantages of soft robots compared to the conventional robots; safe human-machine interaction, adaptability to wearable devices, simple gripping system, and so on. Due to the unique features and advantages, soft robots have a considerable range of applications. This article reviews state-of-the-art researches on soft robots and application areas. Actuation systems for soft robots can be categorized and analyzed into three types: variable length tendon, fluidic actuation, and electro-active polymer (EAP). The deformable property of soft robots restricts the use of many conventional rigid sensors such as encoders, strain gauges, or inertial measurement units. Thus, contactless approaches for sensing and/or sensors with low modulus are preferable for soft robots. Sensors include low modulus (< 1 MPa) elastomers with liquid-phase material filled channels and are appropriate for proprioception which is determined by the degree of curvature. In control perspective, novel control idea should be developed because the conventional control techniques may be inadequate to handle soft robots. Several innovative techniques and diverse materials & fabrication methods are described in this review article. In addition, a wide range of soft robots are characterized and analyzed based on the following sub-categories; actuation, sensing, structure, control and electronics, materials, fabrication and system, and applications.},
  doi      = {10.1007/s12555-016-0462-3},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1053869439},
}

@Article{Hughes2016,
  author   = {Josie Hughes and Utku Culha and Fabio Giardina and Fabian Guenther and Andre Rosendo and Fumiya Iida},
  journal  = {Frontiers in Robotics and AI},
  title    = {Soft Manipulators and Grippers: A Review},
  year     = {2016},
  note     = {https://www.frontiersin.org/articles/10.3389/frobt.2016.00069/pdf},
  pages    = {69},
  volume   = {3},
  abstract = {Soft robotics is a growing area of research which utilizes the compliance and adaptability of soft structures to develop highly adaptive robotics for soft interactions. One area in which soft robotics has the ability to make significant impact is in the development of soft grippers and manipulators. With an increased requirement for automation, robotics systems are required to perform task in unstructured and not well defined environments; conditions which conventional rigid robotics are not best suited. This requires a paradigm shift in the methods and materials used to develop robots such that they can adapt to and work safely in human environments. One solution to this is soft robotics, which enables soft interactions with the surroundings while maintaining the ability to apply significant force. This review paper assesses the current materials and methods, actuation methods and sensors which are used in the development of soft manipulators. The achievements and shortcomings of recent technology in these key areas are evaluated, and this paper concludes with a discussion on the potential impacts of soft manipulators on industry and society.},
  doi      = {10.3389/frobt.2016.00069},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1009708400},
}

@Article{Aguilar2016,
  author   = {Jeffrey Aguilar and Tingnan Zhang and Feifei Qian and Mark Kingsbury and Benjamin McInroe and Nicole Mazouchova and Chen Li and Ryan Maladen and Chaohui Gong and Matt Travers and Ross L Hatton and Howie Choset and Paul B Umbanhowar and Daniel I Goldman},
  journal  = {Reports on Progress in Physics},
  title    = {A review on locomotion robophysics: the study of movement at the intersection of robotics, soft matter and dynamical systems},
  year     = {2016},
  note     = {https://arxiv.org/pdf/1602.04712},
  number   = {11},
  pages    = {110001},
  volume   = {79},
  abstract = {Discovery of fundamental principles which govern and limit effective locomotion (self-propulsion) is of intellectual interest and practical importance. Human technology has created robotic moving systems that excel in movement on and within environments of societal interest: paved roads, open air and water. However, such devices cannot yet robustly and efficiently navigate (as animals do) the enormous diversity of natural environments which might be of future interest for autonomous robots; examples include vertical surfaces like trees and cliffs, heterogeneous ground like desert rubble and brush, turbulent flows found near seashores, and deformable/flowable substrates like sand, mud and soil. In this review we argue for the creation of a physics of moving systems-a 'locomotion robophysics'-which we define as the pursuit of principles of self-generated motion. Robophysics can provide an important intellectual complement to the discipline of robotics, largely the domain of researchers from engineering and computer science. The essential idea is that we must complement the study of complex robots in complex situations with systematic study of simplified robotic devices in controlled laboratory settings and in simplified theoretical models. We must thus use the methods of physics to examine both locomotor successes and failures using parameter space exploration, systematic control, and techniques from dynamical systems. Using examples from our and others' research, we will discuss how such robophysical studies have begun to aid engineers in the creation of devices that have begun to achieve life-like locomotor abilities on and within complex environments, have inspired interesting physics questions in low dimensional dynamical systems, geometric mechanics and soft matter physics, and have been useful to develop models for biological locomotion in complex terrain. The rapidly decreasing cost of constructing robot models with easy access to significant computational power bodes well for scientists and engineers to engage in a discipline which can readily integrate experiment, theory and computation.},
  doi      = {10.1088/0034-4885/79/11/110001},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1037299542},
}

@Article{Laschi2016,
  author   = {Cecilia Laschi and Barbara Mazzolai and Matteo Cianchetti},
  journal  = {Science Robotics},
  title    = {Soft robotics: Technologies and systems pushing the boundaries of robot abilities},
  year     = {2016},
  number   = {1},
  volume   = {1},
  abstract = {The proliferation of soft robotics research worldwide has brought substantial achievements in terms of principles, models, technologies, techniques, and prototypes of soft robots. Such achievements are reviewed here in terms of the abilities that they provide robots that were not possible before. An analysis of the evolution of this field shows how, after a few pioneering works in the years 2009 to 2012, breakthrough results were obtained by taking seminal technological and scientific challenges related to soft robotics from actuation and sensing to modeling and control. Further progress in soft robotics research has produced achievements that are important in terms of robot abilities-that is, from the viewpoint of what robots can do today thanks to the soft robotics approach. Abilities such as squeezing, stretching, climbing, growing, and morphing would not be possible with an approach based only on rigid links. The challenge ahead for soft robotics is to further develop the abilities for robots to grow, evolve, self-heal, develop, and biodegrade, which are the ways that robots can adapt their morphology to the environment.},
  doi      = {10.1126/scirobotics.aah3690},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1062681758},
}

@Article{Manti2016,
  author   = {Mariangela Manti and Vito Cacucciolo and Matteo Cianchetti},
  journal  = {IEEE Robotics & Automation Magazine},
  title    = {Stiffening in Soft Robotics: A Review of the State of the Art},
  year     = {2016},
  number   = {3},
  pages    = {93-106},
  volume   = {23},
  abstract = {The need for building robots with soft materials emerged recently from considerations of the limitations of service robots in negotiating natural environments, from observation of the role of compliance in animals and plants [1], and even from the role attributed to the physical body in movement control and intelligence, in the so-called embodied intelligence or morphological computation paradigm [2]-[4]. The wide spread of soft robotics relies on numerous investigations of diverse materials and technologies for actuation and sensing, and on research of control techniques, all of which can serve the purpose of building robots with high deformability and compliance. But the core challenge of soft robotics research is, in fact, the variability and controllability of such deformability and compliance.},
  doi      = {10.1109/mra.2016.2582718},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061419831},
}

@Article{Kruusamaee2015,
  author   = {Karl Kruusamäe and Andres Punning and Alvo Aabloo and Kinji Asaka},
  journal  = {Actuators},
  title    = {Self-Sensing Ionic Polymer Actuators: A Review},
  year     = {2015},
  note     = {https://www.mdpi.com/2076-0825/4/1/17/pdf?version=1425298410},
  number   = {1},
  pages    = {17-38},
  volume   = {4},
  abstract = {Ionic electromechanically active polymers (IEAP) are laminar composites that can be considered attractive candidates for soft actuators. Their outstanding properties such as low operating voltage, easy miniaturization, and noiseless operation are, however, marred by issues related to the repeatability in the production and operation of these materials. Implementing closed-loop control for IEAP actuators is a viable option for overcoming these issues. Since IEAP laminates also behave as mechanoelectrical sensors, it is advantageous to combine the actuating and sensing functionalities of a single device to create a so-called self-sensing actuator. This review article systematizes the state of the art in producing self-sensing ionic polymer actuators. The IEAPs discussed in this paper are conducting (or conjugated) polymers actuators (CPA), ionic polymer-metal composite (IPMC), and carbonaceous polymer laminates.},
  doi      = {10.3390/act4010017},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1052588601},
}

@Book{Verl2015,
  editor   = {Alexander Verl, Alin Albu-Schäffer, Oliver Brock, Annika Raatz},
  title    = {Soft Robotics, Transferring Theory to Application},
  year     = {2015},
  abstract = {The research areas as well as the knowledge gained for the practical use of robots are growing and expanding beyond manufacturing and industrial automation, making inroads in sectors such as health care and terrain sensing, as well as general assistive systems working in close interaction with humans. In a situation like this, it is necessary for future robot systems to become less stiff and more specialized by taking inspiration from the mechanical compliance and versatility found in natural materials and organisms. At present, a new discipline is emerging in this area, called »Soft Robotics«. It particularly challenges the traditional thinking of engineers, as the confluence of technologies, ranging from new materials, sensors, actuators and production techniques to new design tools, will make it possible to create new systems whose structures are almost completely made of soft materials, which bring about entirely new functions and behaviors, similar in many ways to natural systems. These Proceedings focus on four main topics: • Soft Actuators and Control • Soft Interactions • Soft Robot Assistants: Potential and Challenges • Human-centered »Soft Robotics«.},
  doi      = {10.1007/978-3-662-44506-8},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1035546790},
}

@Article{Wang2015,
  author   = {Liyu Wang and Fumiya Iida},
  journal  = {IEEE Robotics & Automation Magazine},
  title    = {Deformation in Soft-Matter Robotics: A Categorization and Quantitative Characterization},
  year     = {2015},
  number   = {3},
  pages    = {125-139},
  volume   = {22},
  abstract = {There has been an increasing interest in soft-robotics research in recent years, and this is reflected in a number of reviews of the topic from different perspectives. For example, in what was probably the first review article using the term soft robotics, [1] focused on actuators, while [2] reviewed the topic by concentrating on fabrication techniques. Three more reviews addressed control [3], biomimetics [4], and materials [5], respectively. Furthermore, in the first issue of Soft Robotics, there are seven review articles from active researchers in the field. More recently, another review [6] and an edited book [7] have tried to cover a number of topics, including design, fabrication, and control as well as sensors and actuators. All of these efforts have helped draw attention to soft robotics and summarized some of the most recent studies.},
  doi      = {10.1109/mra.2015.2448277},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061419786},
}

@Article{Rus2015,
  author   = {Daniela Rus and Michael T. Tolley},
  journal  = {Nature},
  title    = {Design, fabrication and control of soft robots},
  year     = {2015},
  number   = {7553},
  pages    = {467-475},
  volume   = {521},
  abstract = {Conventionally, engineers have employed rigid materials to fabricate precise, predictable robotic systems, which are easily modelled as rigid members connected at discrete joints. Natural systems, however, often match or exceed the performance of robotic systems with deformable bodies. Cephalopods, for example, achieve amazing feats of manipulation and locomotion without a skeleton; even vertebrates such as humans achieve dynamic gaits by storing elastic energy in their compliant bones and soft tissues. Inspired by nature, engineers have begun to explore the design and control of soft-bodied robots composed of compliant materials. This Review discusses recent developments in the emerging field of soft robotics.},
  doi      = {10.1038/nature14543},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1018093320},
}

@Article{Elango2015,
  author   = {N. Elango and A. A. M. Faudzi},
  journal  = {The International Journal of Advanced Manufacturing Technology},
  title    = {A review article: investigations on soft materials for soft robot manipulations},
  year     = {2015},
  number   = {5-8},
  pages    = {1027-1037},
  volume   = {80},
  abstract = {In recent years, exploratory research on soft materials and their mechanism has been gaining in popularity. The investigations on soft materials are mostly done for two reasons: (a) to develop an anthropomorphic/prosthetic hand or soft hand with human skin-like material to perform soft manipulations and (b) to develop soft actuators. This paper presented a comprehensive investigation into researches on soft materials for robotic applications. The primary interest of using soft materials is not to leave any marks or damage to objects during the manipulation. The other advantage would be stable grasping due to an area contact. Natural rubber, synthetic rubber, elastomer, polymer composite and nanoparticulated polymer composite are some existing soft materials. Extensive research is required to prepare a high-strength but lighter soft material for robotic soft manipulation. Human skin and its mechanical properties are initially discussed. In addition, the need of soft material for soft manipulations and observations from previous researches over the past few decades, modelling of non-linear hyperelastic/viscoelastic materials and characterization are discussed. Finally, various soft materials including the polymer-matrix composites, available fillers and their advantages, processing methods and nanoparticulated polymer matrix and its significance in robotic application are presented.},
  doi      = {10.1007/s00170-015-7085-3},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1048672075},
}

@Article{Bahramzadeh2014,
  author   = {Yousef Bahramzadeh and Mohsen Shahinpoor},
  journal  = {Soft Robotics},
  title    = {A Review of Ionic Polymeric Soft Actuators and Sensors},
  year     = {2014},
  number   = {1},
  pages    = {38-52},
  volume   = {1},
  abstract = {This article reviews properties and characteristics of soft ionic polymer–metal nanocomposites as soft biomimetic multifunctional distributed robotic materials, soft actuators, soft sensors, soft transducers and energy harvesters, and artificial muscles. After presenting some fundamental properties of biomimetic distributed nanosensing and nanoactuation of ionic polymer–metal composites, the discussion extends to some recent advances in manufacturing techniques, 3D fabrication of these soft actuators and sensors and some recent modeling and simulations, sensing and transduction, and product development. The relationships between the properties and their connection to the characteristics of soft actuators and sensors are also discussed.},
  doi      = {10.1089/soro.2013.0006},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1059311431},
}

@Article{Bauer2013,
  author   = {Siegfried Bauer and Simona Bauer‐Gogonea and Ingrid Graz and Martin Kaltenbrunner and Christoph Keplinger and Reinhard Schwödiauer},
  journal  = {Advanced Materials},
  title    = {25th Anniversary Article: A Soft Future: From Robots and Sensor Skin to Energy Harvesters},
  year     = {2013},
  note     = {https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/adma.201303349},
  number   = {1},
  pages    = {149-162},
  volume   = {26},
  abstract = {Scientists are exploring elastic and soft forms of robots, electronic skin and energy harvesters, dreaming to mimic nature and to enable novel applications in wide fields, from consumer and mobile appliances to biomedical systems, sports and healthcare. All conceivable classes of materials with a wide range of mechanical, physical and chemical properties are employed, from liquids and gels to organic and inorganic solids. Functionalities never seen before are achieved. In this review we discuss soft robots which allow actuation with several degrees of freedom. We show that different actuation mechanisms lead to similar actuators, capable of complex and smooth movements in 3d space. We introduce latest research examples in sensor skin development and discuss ultraflexible electronic circuits, light emitting diodes and solar cells as examples. Additional functionalities of sensor skin, such as visual sensors inspired by animal eyes, camouflage, self-cleaning and healing and on-skin energy storage and generation are briefly reviewed. Finally, we discuss a paradigm change in energy harvesting, away from hard energy generators to soft ones based on dielectric elastomers. Such systems are shown to work with high energy of conversion, making them potentially interesting for harvesting mechanical energy from human gait, winds and ocean waves.},
  doi      = {10.1002/adma.201303349},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1046958784},
}

@Article{Laschi2014,
  author   = {Cecilia Laschi and Matteo Cianchetti},
  journal  = {Frontiers in Bioengineering and Biotechnology},
  title    = {Soft Robotics: New Perspectives for Robot Bodyware and Control},
  year     = {2014},
  note     = {https://www.frontiersin.org/articles/10.3389/fbioe.2014.00003/pdf},
  pages    = {3},
  volume   = {2},
  abstract = {The remarkable advances of robotics in the last 50 years, which represent an incredible wealth of knowledge, are based on the fundamental assumption that robots are chains of rigid links. The use of soft materials in robotics, driven not only by new scientific paradigms (biomimetics, morphological computation, and others), but also by many applications (biomedical, service, rescue robots, and many more), is going to overcome these basic assumptions and makes the well-known theories and techniques poorly applicable, opening new perspectives for robot design and control. The current examples of soft robots represent a variety of solutions for actuation and control. Though very first steps, they have the potential for a radical technological change. Soft robotics is not just a new direction of technological development, but a novel approach to robotics, unhinging its fundamentals, with the potential to produce a new generation of robots, in the support of humans in our natural environments.},
  doi      = {10.3389/fbioe.2014.00003},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1000285042},
}

@Article{Majidi2014,
  author   = {Carmel Majidi},
  journal  = {Soft Robotics},
  title    = {Soft Robotics: A Perspective—Current Trends and Prospects for the Future},
  year     = {2014},
  number   = {1},
  pages    = {5-11},
  volume   = {1},
  abstract = {Soft robots are primarily composed of easily deformable matter such as fluids, gels, and elastomers that match the elastic and rheological properties of biological tissue and organs. Like an octopus squeezing through a narrow opening or a caterpillar rolling through uneven terrain, a soft robot must adapt its shape and locomotion strategy for a broad range of tasks, obstacles, and environmental conditions. This emerging class of elastically soft, versatile, and biologically inspired machines represents an exciting and highly interdisciplinary paradigm in engineering that could revolutionize the role of robotics in healthcare, field exploration, and cooperative human assistance.},
  doi      = {10.1089/soro.2013.0001},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1050931185},
}

@Article{Otero2012,
  author   = {T.F. Otero and J.G. Martinez and J. Arias-Pardilla},
  journal  = {Electrochimica Acta},
  title    = {Biomimetic electrochemistry from conducting polymers. A review Artificial muscles, smart membranes, smart drug delivery and computer/neuron interfaces},
  year     = {2012},
  pages    = {112-128},
  volume   = {84},
  abstract = {Films of conducting polymers in the presence of electrolytes can be oxidized or reduced by the flow of anodic or cathodic currents. Ions and solvent are exchanged during a reaction for charge and osmotic pressure balance. A reactive conducting polymer contains ions and solvent. Such variation of composition during a reaction is reminiscent of the biological processes in cells. Along changes to the composition of the material during a reaction, there are also changes to other properties, including: volume (electrochemomechanical), colour (electrochromic), stored charge (electrical storage), porosity or permselectivity (electroporosity), stored chemicals, wettability and so on. Most of those properties mimic similar property changes in organs during their functioning. These properties are being exploited to develop biomimetic reactive and soft devices: artificial muscles and polymeric actuators; supercapacitors and all organic batteries; smart membranes; electron-ion transducers; nervous interfaces and artificial synapses, or drug delivery devices. In this review we focus on the state of the art for artificial muscles, smart membranes and electron-ion transducers. The reactive nature of those devices provide them with a unique advantage related to the present days technologies: any changes in the surrounding physical or chemical variable acting on the electrochemical reaction rate will be sensed by the device while working. Working under constant current (driving signal), the evolution of the device potential or the evolution of the consumed electrical energy (sensing signals) senses and quantifies the variable increment. Driving and sensing signals are present, simultaneously, in the same two connecting wires. It is possible to prepare electrochemical devices based on conducting polymers in which there are several kinds of different sensors and one actuator embedded in one device. Examples of the tools and products, start-up companies, increasing evolution of scientific literature and patents are also presented. Scientific and technological challenges are also considered.},
  doi      = {10.1016/j.electacta.2012.03.097},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1016383527},
}

@Article{Yujun2012,
  author  = {Yujun CAO},
  journal = {Journal of Mechanical Engineering},
  title   = {Review of Soft-bodied Robots},
  year    = {2012},
  number  = {03},
  pages   = {25},
  volume  = {48},
  doi     = {10.3901/jme.2012.03.025},
  groups  = {Soft Robotics},
  url     = {https://app.dimensions.ai/details/publication/pub.1071553028},
}

@Article{Pfeifer2012,
  author   = {Rolf Pfeifer and Max Lungarella and Fumiya Iida},
  journal  = {Communications of the ACM},
  title    = {The challenges ahead for bio-inspired &#x27;soft&#x27; robotics},
  year     = {2012},
  number   = {11},
  pages    = {76-87},
  volume   = {55},
  abstract = {Soft materials may enable the automation of tasks beyond the capacities of current robotic technology.},
  doi      = {10.1145/2366316.2366335},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1022244206},
}

@Article{Ilievski2011,
  author   = {Filip Ilievski and Aaron D. Mazzeo and Robert F. Shepherd and Xin Chen and George M. Whitesides},
  journal  = {Angewandte Chemie International Edition},
  title    = {Soft Robotics for Chemists},
  year     = {2011},
  number   = {8},
  pages    = {1890-1895},
  volume   = {50},
  abstract = {Soft robots: A methodology based on embedded pneumatic networks (PneuNets) is described that enables large‐amplitude actuations in soft elastomers by pressurizing embedded channels. Examples include a structure that can change its curvature from convex to concave, and devices that act as compliant grippers for handling fragile objects (e.g., a chicken egg).},
  doi      = {10.1002/anie.201006464},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1013312043},
}

@Article{Iida2011,
  author   = {Fumiya Iida and Cecilia Laschi},
  journal  = {Procedia Computer Science},
  title    = {Soft Robotics: Challenges and Perspectives},
  year     = {2011},
  note     = {https://doi.org/10.1016/j.procs.2011.12.030},
  pages    = {99-102},
  volume   = {7},
  abstract = {There has been an increasing interest in the use of unconventional materials and morphologies in robotic systems because the underlying mechanical properties (such as body shapes, elasticity, viscosity, softness, density and stickiness) are crucial research topics for our in-depth understanding of embodied intelligence. The detailed investigations of physical system-environment interactions are particularly important for systematic development of technologies and theories of emergent adaptive behaviors. Based on the presentations and discussion in the Future Emerging Technology (fet11) conference, this article introduces the recent technological development in the field of soft robotics, and speculates about the implications and challenges in the robotics and embodied intelligence research.},
  doi      = {10.1016/j.procs.2011.12.030},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1010861669},
}

@Article{Volder2010,
  author   = {Michaël De Volder and Dominiek Reynaerts},
  journal  = {Journal of Micromechanics and Microengineering},
  title    = {Pneumatic and hydraulic microactuators: a review},
  year     = {2010},
  number   = {4},
  pages    = {043001},
  volume   = {20},
  abstract = {The development of MEMS actuators is rapidly evolving and continuously new progress in terms of efficiency, power and force output is reported. Pneumatic and hydraulic are an interesting class of microactuators that are easily overlooked. Despite the 20 years of research, and hundreds of publications on this topic, these actuators are only popular in microfluidic systems. In other MEMS applications, pneumatic and hydraulic actuators are rare in comparison with electrostatic, thermal or piezo-electric actuators. However, several studies have shown that hydraulic and pneumatic actuators deliver among the highest force and power densities at microscale. It is believed that this asset is particularly important in modern industrial and medical microsystems, and therefore, pneumatic and hydraulic actuators could start playing an increasingly important role. This paper shows an in-depth overview of the developments in this field ranging from the classic inflatable membrane actuators to more complex piston–cylinder and drag-based microdevices.},
  doi      = {10.1088/0960-1317/20/4/043001},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1034332217},
}

@Article{Greef2009,
  author   = {Aline De Greef and Pierre Lambert and Alain Delchambre},
  journal  = {Precision Engineering},
  title    = {Towards flexible medical instruments: Review of flexible fluidic actuators},
  year     = {2009},
  number   = {4},
  pages    = {311-321},
  volume   = {33},
  abstract = {Flexible instruments, i.e. instruments presenting a great number of degrees of freedom and able to perform snake-like movements when avoiding obstacles, can find a lot of applications in the medical field. On the other hand, flexible fluidic actuators, i.e. actuators having a flexible inflatable structure and actuated by fluid, present interesting features regarding medical applications. Therefore, this paper proposes to use these actuators to develop medical flexible instruments.Firstly, the advantages and drawbacks linked to the use of flexible fluidic actuators are listed and a discussion about the miniaturization of fluidic actuators peripherics (such as valves) is led. Next, a literature review of the existing flexible fluidic actuators is established. It can serve as basis to develop flexible instruments based on these actuators.},
  doi      = {10.1016/j.precisioneng.2008.10.004},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1014191409},
}

@Article{Cho2009,
  author   = {Kyu-Jin Cho and Je-Sung Koh and Sangwoo Kim and Won-Shik Chu and Yongtaek Hong and Sung-Hoon Ahn},
  journal  = {International Journal of Precision Engineering and Manufacturing},
  title    = {Review of manufacturing processes for soft biomimetic robots},
  year     = {2009},
  number   = {3},
  pages    = {171},
  volume   = {10},
  abstract = {This paper reviews various processes for manufacturing new type of robots termed “soft biomimetic robots.” Most robots are made of rigid metallic materials. But in recent years, various biomimetic robots based on soft materials and compliant parts have been developed. New manufacturing processes are required to fabricate these types of robots, and the processes include Shape Deposition Manufacturing (SDM) and Smart Composite Microstructures (SCM). Since the design of robots are limited by the available material and manufacturing processes, it is important to develop new manufacturing processes that will enable development of novel soft biomimetic robots. In this paper, various manufacturing processes which can be applied to soft robot fabrication are summarized, and features of those processes are described. Processes are divided into three categories; soft robot body fabrication, actuators for soft robots and stretchable electronics. This review provides a guideline for selecting manufacturing processes for soft robots and developing new processes that will enable new type of robots to be designed.},
  doi      = {10.1007/s12541-009-0064-6},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1032973919},
}

@Article{Trivedi2008,
  author   = {Deepak Trivedi and Christopher D. Rahn and William M. Kier and Ian D. Walker},
  journal  = {Applied Bionics and Biomechanics},
  title    = {Soft Robotics: Biological Inspiration, State of the Art, and Future Research},
  year     = {2008},
  note     = {https://downloads.hindawi.com/journals/abb/2008/520417.pdf},
  number   = {3},
  pages    = {99-117},
  volume   = {5},
  abstract = {Traditional robots have rigid underlying structures that limit their ability to interact with their environment. For example, conventional robot manipulators have rigid links and can manipulate objects using only their specialised end effectors. These robots often encounter difficulties operating in unstructured and highly congested environments. A variety of animals and plants exhibit complex movement with soft structures devoid of rigid components. Muscular hydrostats (e.g. octopus arms and elephant trunks) are almost entirely composed of muscle and connective tissue and plant cells can change shape when pressurised by osmosis. Researchers have been inspired by biology to design and build soft robots. With a soft structure and redundant degrees of freedom, these robots can be used for delicate tasks in cluttered and/or unstructured environments. This paper discusses the novel capabilities of soft robots, describes examples from nature that provide biological inspiration, surveys the state of the art and outlines existing challenges in soft robot design, modelling, fabrication and control.},
  doi      = {10.1155/2008/520417},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1063202700},
}

@InProceedings{Robinson1999,
  author    = {G. Robinson and J.B.C. Davies},
  booktitle = {Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No.99CH36288C)},
  title     = {Continuum robots - a state of the art},
  year      = {1999},
  pages     = {2849-2854 vol.4},
  abstract  = {Like the human limbs which inspired them most robots are discrete mechanisms with rigid links connected by single degree of freedom joints. In contrast, 'continuum' and 'serpentine' robot mechanisms move by bending through a series of continuous arcs producing motion which resembles that of biological tentacles or snakes. This paper provides a single reference to the expanding technology of continuum robot mechanisms. It defines the fundamental difference between discrete, serpentine and continuum robot devices, presents the 'state of the art' of continuum robots, outlines their areas of application, and introduces some control issues. Finally, some conclusions regarding the continued development of these devices are made.},
  doi       = {10.1109/robot.1999.774029},
  groups    = {Soft Robotics},
  url       = {https://app.dimensions.ai/details/publication/pub.1094524387},
}

@Article{Kim2013,
  author   = {Yoonseob Kim and Jian Zhu and Bongjun Yeom and Matthew Di Prima and Xianli Su and Jin-Gyu Kim and Seung Jo Yoo and Ctirad Uher and Nicholas A. Kotov},
  journal  = {Nature},
  title    = {Stretchable nanoparticle conductors with self-organized conductive pathways},
  year     = {2013},
  number   = {7460},
  pages    = {59-63},
  volume   = {500},
  abstract = {Stretchable conductors have many applications, from flexible electronics to medical implants; here polyurethane is filled with gold nanoparticles to give a composite with tunable viscoelastic properties arising from the dynamic self-organization of the nanoparticles under stress.},
  doi      = {10.1038/nature12401},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1047578355},
}

@Article{Kim2008,
  author   = {Sangbae Kim and Matthew Spenko and Salomon Trujillo and Barrett Heyneman and Daniel Santos and Mark R. Cutkosky},
  journal  = {IEEE Transactions on Robotics},
  title    = {Smooth Vertical Surface Climbing with Directional Adhesion},
  year     = {2008},
  note     = {https://doi.org/10.1109/tro.2007.909786},
  number   = {1},
  pages    = {65-74},
  volume   = {24},
  abstract = {Stickybot is a bioinspired robot that climbs smooth vertical surfaces such as glass, plastic, and ceramic tile at 4 cm/s. The robot employs several design principles adapted from the GECKO including a hierarchy of compliant structures, directional adhesion, and control of tangential contact forces to achieve control of adhesion. We describe the design and fabrication methods used to create underactuated, multimaterial structures that conform to surfaces over a range of length scales from centimeters to micrometers. At the finest scale, the undersides of Stickybot's toes are covered with arrays of small, angled polymer stalks. Like the directional adhesive structures used by geckos, they readily adhere when pulled tangentially from the tips of the toes toward the ankles; when pulled in the opposite direction, they release. Working in combination with the compliant structures and directional adhesion is a force control strategy that balances forces among the feet and promotes smooth attachment and detachment of the toes.},
  doi      = {10.1109/tro.2007.909786},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061784814},
}

@Article{Chen2009,
  author   = {Zheng Chen and Stephan Shatara and Xiaobo Tan},
  journal  = {IEEE/ASME Transactions on Mechatronics},
  title    = {Modeling of Biomimetic Robotic Fish Propelled by an Ionic Polymer–Metal Composite Caudal Fin},
  year     = {2009},
  number   = {3},
  pages    = {448-459},
  volume   = {15},
  abstract = {In this paper, a physics-based model is proposed for a biomimetic robotic fish propelled by an ionic polymer–metal composite (IPMC) actuator. Inspired by the biological fin structure, a passive plastic fin is further attached to the IPMC beam. The model incorporates both IPMC actuation dynamics and the hydrodynamics, and predicts the steady-state cruising speed of the robot under a given periodic actuation voltage. The interactions between the plastic fin and the IPMC actuator are also captured in the model. Experimental results have shown that the proposed model is able to predict the motion of robotic fish for different tail dimensions. Since most of the model parameters are expressed in terms of fundamental physical properties and geometric dimensions, the model is expected to be instrumental in optimal design of the robotic fish.},
  doi      = {10.1109/tmech.2009.2027812},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061692527},
}

@Article{Kim2013a,
  author   = {Sangbae Kim and Cecilia Laschi and Barry Trimmer},
  journal  = {Trends in Biotechnology},
  title    = {Soft robotics: a bioinspired evolution in robotics},
  year     = {2013},
  number   = {5},
  pages    = {287-294},
  volume   = {31},
  abstract = {Animals exploit soft structures to move effectively in complex natural environments. These capabilities have inspired robotic engineers to incorporate soft technologies into their designs. The goal is to endow robots with new, bioinspired capabilities that permit adaptive, flexible interactions with unpredictable environments. Here, we review emerging soft-bodied robotic systems, and in particular recent developments inspired by soft-bodied animals. Incorporating soft technologies can potentially reduce the mechanical and algorithmic complexity involved in robot design. Incorporating soft technologies will also expedite the evolution of robots that can safely interact with humans and natural environments. Finally, soft robotics technology can be combined with tissue engineering to create hybrid systems for medical applications.},
  doi      = {10.1016/j.tibtech.2013.03.002},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1006036928},
}

@Article{Suo2010,
  author   = {Zhigang Suo},
  journal  = {Acta Mechanica Solida Sinica},
  title    = {Theory of dielectric elastomers},
  year     = {2010},
  number   = {6},
  pages    = {549-578},
  volume   = {23},
  abstract = {In response to a stimulus, a soft material deforms, and the deformation provides a function. We call such a material a soft active material (SAM). This review focuses on one class of soft active materials: dielectric elastomers. When a membrane of a dielectric elastomer is subject to a voltage through its thickness, the membrane reduces thickness and expands area, possibly straining over 100%. The dielectric elastomers are being developed as transducers for broad applications, including soft robots, adaptive optics, Braille displays, and electric generators. This paper reviews the theory of dielectric elastomers, developed within continuum mechanics and thermodynamics, and motivated by molecular pictures and empirical observations. The theory couples large deformation and electric potential, and describes nonlinear and nonequilibrium behavior, such as electromechanical instability and viscoelasticity. The theory enables the finite element method to simulate transducers of realistic configurations, predicts the efficiency of electromechanical energy conversion, and suggests alternative routes to achieve giant voltage-induced deformation. It is hoped that the theory will aid in the creation of materials and devices.},
  doi      = {10.1016/s0894-9166(11)60004-9},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1046414848},
}

@Article{Tee2012,
  author   = {Benjamin C-K. Tee and Chao Wang and Ranulfo Allen and Zhenan Bao},
  journal  = {Nature Nanotechnology},
  title    = {An electrically and mechanically self-healing composite with pressure- and flexion-sensitive properties for electronic skin applications},
  year     = {2012},
  number   = {12},
  pages    = {825-832},
  volume   = {7},
  abstract = {Pressure sensitivity and mechanical self-healing are two vital functions of the human skin. A flexible and electrically conducting material that can sense mechanical forces and yet be able to self-heal repeatably can be of use in emerging fields such as soft robotics and biomimetic prostheses, but combining all these properties together remains a challenging task. Here, we describe a composite material composed of a supramolecular organic polymer with embedded nickel nanostructured microparticles, which shows mechanical and electrical self-healing properties at ambient conditions. We also show that our material is pressure- and flexion-sensitive, and therefore suitable for electronic skin applications. The electrical conductivity can be tuned by varying the amount of nickel particles and can reach values as high as 40 S cm−1. On rupture, the initial conductivity is repeatably restored with ∼90% efficiency after 15 s healing time, and the mechanical properties are completely restored after ∼10 min. The composite resistance varies inversely with applied flexion and tactile forces. These results demonstrate that natural skin's repeatable self-healing capability can be mimicked in conductive and piezoresistive materials, thus potentially expanding the scope of applications of current electronic skin systems.},
  doi      = {10.1038/nnano.2012.192},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1021059830},
}

@Article{Shepherd2011,
  author   = {Robert F. Shepherd and Filip Ilievski and Wonjae Choi and Stephen A. Morin and Adam A. Stokes and Aaron D. Mazzeo and Xin Chen and Michael Wang and George M. Whitesides},
  journal  = {Proceedings of the National Academy of Sciences of the United States of America},
  title    = {Multigait soft robot},
  year     = {2011},
  note     = {https://www.pnas.org/content/pnas/108/51/20400.full.pdf},
  number   = {51},
  pages    = {20400-20403},
  volume   = {108},
  abstract = {This manuscript describes a unique class of locomotive robot: A soft robot, composed exclusively of soft materials (elastomeric polymers), which is inspired by animals (e.g., squid, starfish, worms) that do not have hard internal skeletons. Soft lithography was used to fabricate a pneumatically actuated robot capable of sophisticated locomotion (e.g., fluid movement of limbs and multiple gaits). This robot is quadrupedal; it uses no sensors, only five actuators, and a simple pneumatic valving system that operates at low pressures (< 10 psi). A combination of crawling and undulation gaits allowed this robot to navigate a difficult obstacle. This demonstration illustrates an advantage of soft robotics: They are systems in which simple types of actuation produce complex motion.},
  doi      = {10.1073/pnas.1116564108},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1033757657},
}

@Article{Pfeifer2007,
  author   = {Rolf Pfeifer and Max Lungarella and Fumiya Iida},
  journal  = {Science},
  title    = {Self-Organization, Embodiment, and Biologically Inspired Robotics},
  year     = {2007},
  note     = {http://apophenia.wikidot.com/local--files/start/pfeifer_2007_self-organization_embodiment_robotics.pdf},
  number   = {5853},
  pages    = {1088-1093},
  volume   = {318},
  abstract = {Robotics researchers increasingly agree that ideas from biology and self-organization can strongly benefit the design of autonomous robots. Biological organisms have evolved to perform and survive in a world characterized by rapid changes, high uncertainty, indefinite richness, and limited availability of information. Industrial robots, in contrast, operate in highly controlled environments with no or very little uncertainty. Although many challenges remain, concepts from biologically inspired (bio-inspired) robotics will eventually enable researchers to engineer machines for the real world that possess at least some of the desirable properties of biological organisms, such as adaptivity, robustness, versatility, and agility.},
  doi      = {10.1126/science.1145803},
  groups   = {Soft Robotics},
  url      = {https://app.dimensions.ai/details/publication/pub.1053745766},
}

@Article{Belloumi2010,
  author   = {Mounir Belloumi},
  journal  = {International Journal of Tourism Research},
  title    = {The relationship between tourism receipts, real effective exchange rate and economic growth in Tunisia},
  year     = {2010},
  number   = {5},
  pages    = {550-560},
  volume   = {12},
  abstract = {Abstract The objective of this paper is to analyse the role of tourism in the Tunisian economic growth. We used a trivariate model of real gross domestic product (GDP), real international tourism receipts and real effective exchange rate to discuss the relationship between tourism and economic growth. By using annual data for Tunisia for the period of 1970–2007, our results reveal that there is a cointegrating relationship between tourism and economic growth. In addition, our results for the Granger causality test indicate that tourism has a positive impact on GDP growth unidirectionally. Copyright © 2010 John Wiley & Sons, Ltd.},
  doi      = {10.1002/jtr.774},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1000810498},
}

@Article{Chou2013,
  author   = {Ming Che Chou},
  journal  = {Economic Modelling},
  title    = {Does tourism development promote economic growth in transition countries? A panel data analysis},
  year     = {2013},
  note     = {https://doi.org/10.1016/j.econmod.2013.04.024},
  pages    = {226-232},
  volume   = {33},
  abstract = {This study examines causal relationships between tourism spending and economic growth in 10 transition countries for the period 1988–2011. Panel causality analysis, which accounts for dependency and heterogeneity across countries, is used herein. Our empirical results support the evidence on the direction of causality, and are consistent with the neutrality hypothesis for 3 of these 10 transition countries (i.e. Bulgaria, Romania and Slovenia). The growth hypothesis holds for Cyprus, Latvia and Slovakia while reverse relationships were found for the Czech Republic and Poland. The feedback hypothesis also holds for Estonia and Hungary. Our empirical findings provide important policy implications for the 10 transition countries being studied.},
  doi      = {10.1016/j.econmod.2013.04.024},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1033376226},
}

@Article{CardenasGarcia2013,
  author   = {Pablo Juan Cárdenas-García and Marcelino Sánchez-Rivero and Juan Ignacio Pulido-Fernández},
  journal  = {Journal of Travel Research},
  title    = {Does Tourism Growth Influence Economic Development?},
  year     = {2013},
  number   = {2},
  pages    = {206-221},
  volume   = {54},
  abstract = {Having recognized the importance of tourism to economic growth, most international organizations have begun to argue that tourism growth can influence, as well, the economic and sociocultural development of society. However, recently, a new approach that criticizes the relationship between both dimensions has begun to be developed; suggesting that this is not an automatic relationship. In this context, the aim of this study is to determine whether the economic growth experienced in some countries as a result of the expansion of the tourism activity over the last two decades influences an increase in the level of economic development. To that end, a sample of 144 countries has been used, which verifies that this relationship occurs, especially in more developed countries, which calls into question the conception of tourism as a driving force of economic development for the least developed countries, and even in developing countries.},
  doi      = {10.1177/0047287513514297},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1053750775},
}

@Article{Tang2012,
  author   = {Chor Foon Tang and Salah Abosedra},
  journal  = {Current Issues in Tourism},
  title    = {Small sample evidence on the tourism-led growth hypothesis in Lebanon},
  year     = {2012},
  number   = {3},
  pages    = {234-246},
  volume   = {17},
  abstract = {This paper investigates the contribution of tourism to economic growth in Lebanon for the time period of 1995–2010. The presence of long-run and causal relationships is investigated applying the bounds testing approach to cointegration and Granger causality tests. Because of the small sample (T = 16), econometric approaches and critical values used for testing receive special attention. Additionally, a number of diagnostic tests are utilised to ensure that the model is suitable and correct. Interestingly, our results reveal that tourism and economic growth are cointegrated. The Granger causality test indicates that the tourism-led growth hypothesis is valid for Lebanon. Therefore, policy initiatives promoting tourism ought to be further developed and implemented to stimulate economic growth and development for the economy of Lebanon.},
  doi      = {10.1080/13683500.2012.732044},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1034653502},
}

@Article{Chen2009a,
  author   = {Ching-Fu Chen and Song Zan Chiou-Wei},
  journal  = {Tourism Management},
  title    = {Tourism expansion, tourism uncertainty and economic growth: New evidence from Taiwan and Korea},
  year     = {2009},
  number   = {6},
  pages    = {812-818},
  volume   = {30},
  abstract = {This study examines the causal relationship between tourism expansion and economic growth in two Asian countries: Taiwan and South Korea. An EGARCH-M model with uncertainty factors is employed to examine the direction of causality between tourism expansion and economic growth, as well as the impulse impacts of uncertainty on both variables. The results indicate that the tourism-led economic growth hypothesis is supported for Taiwan while a reciprocal causal relationship is found for South Korea. The significant impacts of uncertainty on growth are also identified.},
  doi      = {10.1016/j.tourman.2008.12.013},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1007878277},
}

@Article{Antonakakis2015,
  author   = {Nikolaos Antonakakis and Mina Dragouni and George Filis},
  journal  = {Economic Modelling},
  title    = {How strong is the linkage between tourism and economic growth in Europe?},
  year     = {2015},
  note     = {https://research.wu.ac.at/files/19823969/Tourism&Growth_EcMode_Pre-Print.pdf},
  pages    = {142-155},
  volume   = {44},
  abstract = {In this study, we examine the dynamic relationship between tourism growth and economic growth, using a newly introduced spillover index approach. Based on monthly data for 10 European countries over the period 1995–2012, our analysis reveals the following empirical regularities. First, the tourism-economic growth relationship is not stable over time in terms of both magnitude and direction, indicating that the tourism-led economic growth (TLEG) and the economic-driven tourism growth (EDTG) hypotheses are time-dependent. Second, the aforementioned relationship is also highly economic event-dependent, as it is influenced by the Great Recession of 2007 and the ongoing Eurozone debt crisis that began in 2010. Finally, the impact of these economic events is more pronounced in Cyprus, Greece, Portugal and Spain, which are the European countries that have witnessed the greatest economic downturn since 2009. Plausible explanations of these results are provided and policy implications are drawn.},
  doi      = {10.1016/j.econmod.2014.10.018},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1001562704},
}

@Article{Tang2013,
  author   = {Chor Foon Tang and Eu Chye Tan},
  journal  = {Tourism Management},
  title    = {How stable is the tourism-led growth hypothesis in Malaysia? Evidence from disaggregated tourism markets},
  year     = {2013},
  note     = {https://doi.org/10.1016/j.tourman.2012.12.014},
  pages    = {52-57},
  volume   = {37},
  abstract = {We contribute to the tourism-growth literature by applying the newly developed combined cointegration test and the recursive Granger causality test to re-assess the stability of the tourism-led growth hypothesis in Malaysia with respect to 12 different tourism markets. The cointegration test results suggest that economic growth of Malaysia is cointegrated with all the 12 selected tourism markets. However, the recursive Granger causality test shows that the tourism-led growth hypothesis in Malaysia is valid and stable with respect to tourist arrivals from only 8 out of the 12 tourism markets. Almost all of them are from developed countries. Hence, not all international visitor arrivals could effectively drive the growth of the Malaysian economy. In light of this, tourism marketing policies should focus more on those tourism markets that could significantly stimulate economic growth. However, there should not be total neglect of others as they potentially contribute to the economies of scale.},
  doi      = {10.1016/j.tourman.2012.12.014},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1032490977},
}

@Article{Dogru2018,
  author   = {Tarik Dogru and Umit Bulut},
  journal  = {Tourism Management},
  title    = {Is tourism an engine for economic recovery? Theory and empirical evidence},
  year     = {2018},
  pages    = {425-434},
  volume   = {67},
  abstract = {The purpose of this study is to examine the causal relationships between tourism development and economic growth. For this purpose, the Dumitrescu and Hurlin (2012) technique was employed to analyze the casual link between tourism development and economic growth in seven European countries. The results showed that there is bidirectional causality between growth in tourism receipts and economic growth, suggesting that economic growth and tourism development are interdependent and that tourism development stimulates economic growth and vice versa in these countries. Theoretical and practical implications are discussed within the realms of growth, conservation, feedback, and neutrality hypotheses.},
  doi      = {10.1016/j.tourman.2017.06.014},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1090346357},
}

@Article{Katircioglu2009,
  author   = {Salih Katircioglu},
  journal  = {Applied Economics},
  title    = {Tourism, trade and growth: the case of Cyprus},
  year     = {2009},
  number   = {21},
  pages    = {2741-2750},
  volume   = {41},
  abstract = {Although the relationship between international trade and economic growth has found a wide application area in the literature over the years, this can not be said about tourism and growth or trade and tourism. This study employs the bounds test for cointegration and Granger causality tests to investigate a long-run equilibrium relationship between tourism, trade and real income growth, and the direction of causality among themselves for Cyprus. Results reveal that tourism, trade and real income growth are cointegrated; thus, a long-run equilibrium relationship can be inferred between these three variables. On the other hand, Granger causality test results suggest that real income growth stimulates growth in international trade (both exports and imports) and international tourist arrivals to the island. Furthermore, growth in international trade (both exports and imports) also stimulates an increase in international tourist arrivals to Cyprus. And finally, real import growth stimulate growth in real exports in the case of Cyprus.},
  doi      = {10.1080/00036840701335512},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1022720888},
}

@Article{Shahzad2017,
  author   = {Syed Jawad Hussain Shahzad and Muhammad Shahbaz and Román Ferrer and Ronald Ravinesh Kumar},
  journal  = {Tourism Management},
  title    = {Tourism-led growth hypothesis in the top ten tourist destinations: New evidence using the quantile-on-quantile approach},
  year     = {2017},
  note     = {https://mpra.ub.uni-muenchen.de/75540/1/MPRA_paper_75540.pdf},
  pages    = {223-232},
  volume   = {60},
  abstract = {This paper examines the empirical validity of the tourism-led growth hypothesis in the top ten tourist destinations in the world (China, France, Germany, Italy, Mexico, Russia, Spain, Turkey, the United Kingdom, and the United States) using the quantile-on-quantile (QQ) approach and a new index of tourism activity that combines the most commonly used tourism indicators. This methodology, recently introduced by Sim and Zhou (2015), provides an ideal framework with which to capture the overall dependence structure between tourism development and economic growth. The empirical results primarily show a positive relation between tourism and economic growth for the ten countries considered with substantial variations across countries and across quantiles within each country. The weakest links are noted for China and Germany, possibly because of the limited importance of the tourism sector relative to other major economic activities in those countries. Important country-specific policy implications may be drawn from these findings.},
  doi      = {10.1016/j.tourman.2016.12.006},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1035901982},
}

@Article{Tugcu2014,
  author   = {Can Tansel Tugcu},
  journal  = {Tourism Management},
  title    = {Tourism and economic growth nexus revisited: A panel causality analysis for the case of the Mediterranean Region},
  year     = {2014},
  pages    = {207-212},
  volume   = {42},
  abstract = {Tourism is perceived as an important source of foreign exchange that is used for financing economic growth. This study offers a modern approach to tourism-led growth and investigates the causal relationship between tourism and economic growth in the European, Asian and African countries that border the Mediterranean Sea. The study uses panel data for the period 1998–2011, and adopts a panel Granger causality analysis developed by Dumitrescu and Hurlin (2012) to assess the contribution tourism makes to economic growth in each country. The results indicate that the direction of causality between tourism and economic growth depends on the country group and tourism indicator. Furthermore, the European countries are better able to generate growth from tourism in the Mediterranean region.},
  doi      = {10.1016/j.tourman.2013.12.007},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1017098340},
}

@Article{Schubert2011,
  author   = {Stefan Franz Schubert and Juan Gabriel Brida and Wiston Adrián Risso},
  journal  = {Tourism Management},
  title    = {The impacts of international tourism demand on economic growth of small economies dependent on tourism},
  year     = {2011},
  number   = {2},
  pages    = {377-385},
  volume   = {32},
  abstract = {This paper studies the impacts on economic growth of a small tourism-driven economy caused by an increase in the growth rate of international tourism demand. We present a formal model and empirical evidence. The ingredients of the dynamic model are a large population of intertemporally optimizing agents and an AK technology representing tourism production. The model shows that an increase in the growth of tourism demand leads to transitional dynamics with gradually increasing economic growth and increasing terms of trade. In our empirical application, an econometric methodology is applied to annual data of Antigua and Barbuda from 1970 to 2008. We perform a cointegration analysis to look for the existence of a long-run relationship among variables of economic growth, international tourism earnings and the real exchange rate. The exercise confirms the theoretical findings.},
  doi      = {10.1016/j.tourman.2010.03.007},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1047663892},
}

@Article{Dogan2015,
  author   = {Eyup Dogan and Fahri Seker and Serap Bulbul},
  journal  = {Current Issues in Tourism},
  title    = {Investigating the impacts of energy consumption, real GDP, tourism and trade on CO2 emissions by accounting for cross-sectional dependence: A panel study of OECD countries},
  year     = {2015},
  number   = {16},
  pages    = {1701-1719},
  volume   = {20},
  abstract = {The objective of this study is to analyse the long-run dynamic relationship of carbon dioxide emissions, real gross domestic product (GDP), the square of real GDP, energy consumption, trade and tourism under an Environmental Kuznets Curve (EKC) model for the Organization for Economic Co-operation and Development (OECD) member countries. Since we find the presence of cross-sectional dependence within the panel time-series data, we apply second-generation unit root tests, cointegration test and causality test which can deal with cross-sectional dependence problems. The cross-sectionally augmented Dickey-Fuller (CADF) and the cross-sectionally augmented Im-Pesaran-Shin (CIPS) unit root tests indicate that the analysed variables become stationary at their first differences. The Lagrange multiplier bootstrap panel cointegration test shows the existence of a long-run relationship between the analysed variables. The dynamic ordinary least squares (DOLS) estimation technique indicates that energy consumption and tourism contribute to the levels of gas emissions, while increases in trade lead to environmental improvements. In addition, the EKC hypothesis cannot be supported as the sign of coefficients on GDP and GDP2 is negative and positive, respectively. Moreover, the Dumitrescu–Hurlin causality tests exploit a variety of causal relationship between the analysed variables. The OECD countries are suggested to invest in improving energy efficiency, regulate necessary environmental protection policies for tourism sector in specific and promote trading activities through several types of encouragement act.},
  doi      = {10.1080/13683500.2015.1119103},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1029520182},
}

@Article{Durbarry2004,
  author   = {Ramesh Durbarry},
  journal  = {Tourism Economics},
  title    = {Tourism and Economic Growth: The Case of Mauritius},
  year     = {2004},
  number   = {4},
  pages    = {389-401},
  volume   = {10},
  abstract = {Tourism is now believed to provide an impetus to the economic progress of developing nations and its importance is gaining widespread recognition. In fact, the relationship between exports and growth is the subject of ongoing debate, but the nature of exports has received little attention. Despite the continuous efforts of developing countries to increase their exports, this strategy often adds little foreign exchange to their balance of payments. For many reasons, the non-traditional exports of developing countries have too often failed to prove effective in economic development and so tourism is increasingly seen as something of a saviour. This paper focuses on the success story of a small island economy, Mauritius. Once regarded as an extreme case of a mono-crop economy, relying very predominantly on the export of sugar, Mauritius is now a reputed exporter of non-traditional goods (textiles) and services (tourism). Using cointegration and causality tests, the author's results lend support to the contention that tourism has promoted growth, and further evidence suggests that tourism has a significant positive impact on Mauritian economic development.},
  doi      = {10.5367/0000000042430962},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1072794279},
}

@Article{Seetanah2011,
  author   = {B. Seetanah},
  journal  = {Annals of Tourism Research},
  title    = {Assessing the dynamic economic impact of tourism for island economies},
  year     = {2011},
  number   = {1},
  pages    = {291-308},
  volume   = {38},
  abstract = {Using a panel data of 19 island economies for the years that span from 1990 to 2007, this study explores the potential contribution of tourism to economic growth and development within the conventional augmented Solow growth model. Since economic growth is argued to be essentially a dynamic phenomenon we employ GMM method to account for these issues. The results show that tourism significantly contributes to the economic growth of island economies. Moreover, the tourist-growth nexus is observed to be a dynamic phenomenon and granger causality analysis reveals a bi-causal relationship between tourist and growth. Comparative analysis with samples of developing and developed countries shows that tourism development on island economies may have comparatively higher growth effects.},
  doi      = {10.1016/j.annals.2010.08.009},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1003840912},
}

@Article{LokmanGunduz2005,
  author   = {Lokman Gunduz * and Abdulnasser Hatemi-J},
  journal  = {Applied Economics Letters},
  title    = {Is the tourism-led growth hypothesis valid for Turkey?},
  year     = {2005},
  number   = {8},
  pages    = {499-504},
  volume   = {12},
  abstract = {Like many developing countries, Turkey has also given priority to the development of tourism industry as a part of its economic growth strategy. This study intends to investigate whether tourism has really contributed to the economic growth in Turkey. The interaction between tourism and economic growth is investigated by making use of leveraged bootstrap causality tests. This method is robust to the existence of non-normality and ARCH effects. Special attention is given to the choice of the optimal lag order of the empirical model. It is found that the tourism-led growth hypothesis is supported empirically in the case of Turkey.},
  doi      = {10.1080/13504850500109865},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1049141736},
}

@Article{Paramati2016,
  author   = {Sudharshan Reddy Paramati and Samsul Alam and Ching-Fu Chen},
  journal  = {Journal of Travel Research},
  title    = {The Effects of Tourism on Economic Growth and CO2 Emissions: A Comparison between Developed and Developing Economies},
  year     = {2016},
  number   = {6},
  pages    = {712-724},
  volume   = {56},
  abstract = {This study empirically examines the dynamic relationships among tourism, economic growth, and CO 2 emissions and compares the effects of tourism on economic growth and CO 2 emissions between developed and developing economies. By employing robust panel econometric techniques, the results show that tourism has significant positive impacts on economic growth for both developed and developing economies, supporting the prevailing hypothesis of tourism-led economic growth. The results also reveal that the impact of tourism on CO 2 emissions is reducing much faster in developed economies than in developing economies, providing evidence of the environmental Kuznets curve (EKC) hypothesis on the link between tourism growth and CO 2 emissions. Our findings demonstrate the importance of the classification of countries by economic development level to obtain a deeper understanding of relationships among tourism, economic growth, and CO 2 emissions. Policy implications are provided and discussed.},
  doi      = {10.1177/0047287516667848},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1053802942},
}

@Article{Tang2015,
  author   = {Chor Foon Tang and Eu Chye Tan},
  journal  = {Tourism Management},
  title    = {Does tourism effectively stimulate Malaysia&#x27;s economic growth?},
  year     = {2015},
  pages    = {158-163},
  volume   = {46},
  abstract = {This study attempts to further verify the validity of the tourism-led growth hypothesis in Malaysia using a multivariate model derived from the Solow growth theory. It employs annual data from 1975 to 2011. We find that economic growth, tourism and other determinants are cointegrated. Specifically, tourism has a positive impact on Malaysia's economic growth both in the short-run and in the long-run. The Granger causality test indicates that tourism Granger-causes economic growth. All this provides the empirical support for the tourism-led growth hypothesis in Malaysia. In light of this, any policy initiative that promotes tourism could contribute to Malaysia's economic growth.},
  doi      = {10.1016/j.tourman.2014.06.020},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1007170077},
}

@Article{Lee2013,
  author   = {Jung Wan Lee and Tantatape Brahmasrene},
  journal  = {Tourism Management},
  title    = {Investigating the influence of tourism on economic growth and carbon emissions: Evidence from panel analysis of the European Union},
  year     = {2013},
  pages    = {69-76},
  volume   = {38},
  abstract = {The study investigates the influence of tourism on economic growth and CO2 emissions. In the empirical analysis, unit root and cointegration tests using panel data of European Union countries from 1988 to 2009 are performed to examine the long-run equilibrium relationship among tourism, CO2 emissions, economic growth and foreign direct investment (FDI). Results from panel cointegration techniques and fixed-effects models indicate that a long-run equilibrium relationship exists among these variables. Furthermore, tourism, CO2 emissions and FDI have high significant positive effect on economic growth. Economic growth, in turn, shows a high significant positive impact on CO2 emissions while tourism and FDI incur a high significant negative impact on CO2 emissions.},
  doi      = {10.1016/j.tourman.2013.02.016},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1004299966},
}

@Article{Dritsakis2004,
  author   = {Nikolaos Dritsakis},
  journal  = {Tourism Economics},
  title    = {Tourism as a Long-Run Economic Growth Factor: An Empirical Investigation for Greece Using Causality Analysis},
  year     = {2004},
  number   = {3},
  pages    = {305-316},
  volume   = {10},
  abstract = {This paper empirically examines the impact of tourism on the long-run economic growth of Greece by using causality analysis of real gross domestic product, real effective exchange rate and international tourism earnings. A Multivariate Auto Regressive (VAR) model is applied for the period 1960:I-2000:IV. The results of co-integration analysis suggest that there is one co-integrated vector among real gross domestic product, real effective exchange rate and international tourism earnings. Granger causality tests based on Error Correction Models (ECMs), have indicated that there is a ‘strong Granger causal’ relationship between international tourism earnings and economic growth, a ‘strong causal’ relationship between real exchange rate and economic growth, and simply ‘causal’ relationships between economic growth and international tourism earnings and between real exchange rate and international tourism earnings.},
  doi      = {10.5367/0000000041895094},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1072794260},
}

@Article{Katircioglu2009a,
  author   = {Salih T. Katircioglu},
  journal  = {Tourism Management},
  title    = {Revisiting the tourism-led-growth hypothesis for Turkey using the bounds test and Johansen approach for cointegration},
  year     = {2009},
  number   = {1},
  pages    = {17-20},
  volume   = {30},
  abstract = {This paper empirically revisits and investigates the tourism-led-growth (TLG) hypothesis in the case of Turkey by employing the bounds test and Johansen approach for cointegration using annual data from 1960–2006. Although Gunduz and Hatemi-J (2005; Is the tourism-led growth hypothesis valid for Turkey? Applied Economics Letters. 12, 499–504) support the TLG hypothesis for Turkey (suggesting unidirectional causation from tourism to economic growth) by making use of the leveraged bootstrap causality tests, and Ongan and Demiroz (2005; The contribution of tourism to the long-run Turkish economic growth. Ekonomický časopis [Journal of Economics]. 53(9), 880–894.) suggest bidirectional causality between international tourism and economic growth in Turkey, this study does not find any cointegration between international tourism and economic growth in Turkey. Therefore, unlike the findings of Gunduz and Hatemi-J (2005) and Ongan and Demiroz (2005), this study rejects the TLG hypothesis for the Turkish economy since no cointegration was found and error correction mechanisms plus causality tests cannot be run for further steps in the long term.},
  doi      = {10.1016/j.tourman.2008.04.004},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1013412296},
}

@Article{Kim2005,
  author   = {Hyun Jeong Kim and Ming-Hsiang Chen and SooCheong “Shawn” Jang},
  journal  = {Tourism Management},
  title    = {Tourism expansion and economic development: The case of Taiwan},
  year     = {2005},
  note     = {https://europepmc.org/articles/pmc7126590?pdf=render},
  number   = {5},
  pages    = {925-933},
  volume   = {27},
  abstract = {This study examines the causal relationship between tourism expansion and economic development in Taiwan. A Granger causality test is performed following the cointegration approach to reveal the direction of causality between economic growth and tourism expansion. Test results indicate a long-run equilibrium relationship and further a bi-directional causality between the two factors. In other words, in Taiwan, tourism and economic development reinforce each other. A discussion follows and managerial implications are identified based on the empirical findings.},
  doi      = {10.1016/j.tourman.2005.05.011},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1004840356},
}

@Article{Oh2005,
  author   = {Chi-Ok Oh},
  journal  = {Tourism Management},
  title    = {The contribution of tourism development to economic growth in the Korean economy},
  year     = {2005},
  number   = {1},
  pages    = {39-44},
  volume   = {26},
  abstract = {This study investigates the causal relations between tourism growth and economic expansion for the Korean economy by using Engle and Granger two-stage approach and a bivariate Vector Autoregression (VAR) model. Two principle results emerge from this study. First, the results of a cointegration test indicate that there is no long-run equilibrium relation between two series. Second, the outcomes of Granger causality test imply the one-way causal relationship of economic-driven tourism growth. The hypothesis of tourism-led economic growth is not held in the Korean economy. This consequence is supported by testing the sensitivity of causality test under different lag selections along with the optimal lag.},
  doi      = {10.1016/j.tourman.2003.09.014},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1013871876},
}

@Article{Balaguer2002,
  author   = {Jacint Balaguer and Manuel Cantavella-Jordá},
  journal  = {Applied Economics},
  title    = {Tourism as a long-run economic growth factor: the Spanish case},
  year     = {2002},
  note     = {http://www.ivie.es/downloads/docs/wpasec/wpasec-2000-10.pdf},
  number   = {7},
  pages    = {877-884},
  volume   = {34},
  abstract = {This paper examines the role of tourism in the Spanish long-run economic development. The tourism-led growth hypothesis is confirmed through cointegration and causality testing. The results indicate that, at least, during the last three decades, economic growth in Spain has been sensible to persistent expansion of international tourism. The increase of this activity has produced multiplier effects over time. External competitivity has also been proved in the model to be a fundamental variable for Spanish economic growth. From the empirical analysis it can be inferred the positive effects on income that government policy, in the adequacy of supply as well as in the promotion of tourist activity, may bring about.},
  doi      = {10.1080/00036840110058923},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1029979518},
}

@Article{Lee2008,
  author   = {Chien-Chiang Lee and Chun-Ping Chang},
  journal  = {Tourism Management},
  title    = {Tourism development and economic growth: A closer look at panels},
  year     = {2008},
  number   = {1},
  pages    = {180-192},
  volume   = {29},
  abstract = {This paper applies the new heterogeneous panel cointegration technique to re-investigate the long-run comovements and causal relationships between tourism development and economic growth for OECD and nonOECD countries (including those in Asia, Latin America and Sub-Sahara Africa) for the 1990–2002 period. On the global scale, after allowing for the heterogeneous country effect, a cointegrated relationship between GDP and tourism development is substantiated. It is also determined that tourism development has a greater impact on GDP in nonOECD countries than in OECD countries, and when the variable is tourism receipts, the greatest impact is in Sub-Sahara African countries. Additionally, the real effective exchange rate has significant effects on economic growth. Finally, in the long run, the panel causality test shows unidirectional causality relationships from tourism development to economic growth in OECD countries, bidirectional relationships in nonOECD countries, but only weak relationships in Asia. Our empirical findings have major policy implications.},
  doi      = {10.1016/j.tourman.2007.02.013},
  groups   = {Tourism Growth Nexus},
  url      = {https://app.dimensions.ai/details/publication/pub.1011854038},
}

@Article{Shrivastav2010,
  author   = {Anupama Shrivastav and Sanjiv K. Mishra and Bhumi Shethia and Imran Pancha and Deepti Jain and Sandhya Mishra},
  journal  = {International Journal of Biological Macromolecules},
  title    = {Isolation of promising bacterial strains from soil and marine environment for polyhydroxyalkanoates (PHAs) production utilizing Jatropha biodiesel byproduct},
  year     = {2010},
  number   = {2},
  pages    = {283-287},
  volume   = {47},
  abstract = {PHAs are biodegradable and environmentally friendly thermoplastics. The major contributor to PHA production cost is carbon substrate cost, therefore it is desirable to produce PHA from waste/byproducts like Jatropha biodiesel byproducts. This study was done using Jatropha biodiesel byproduct as carbon source, to decrease production cost for PHAs. Total 41 isolates from soil and marine source were able to utilize Jatropha biodiesel byproduct. Nine bacteria were selected for further studies, which were found positive for Nile red viable colony screening. Two bacterial isolates SM-P-1S and SM-P-3M isolated from soil and marine environment respectively, were found promising for PHA production. PHA accumulation for SM-P-1S and SM-P-3M was 71.8% and 75% PHA/CDW respectively and identified as Bacillus sonorensis and Halomonas hydrothermalis by MTCC. The PHA obtained from SM-P-1S and SM-P-3M was analyzed by FTIR and NMR as polyhydroxybutyrate (PHB).},
  doi      = {10.1016/j.ijbiomac.2010.04.007},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1033108925},
}

@Article{Talens2007,
  author   = {Laura Talens and Gara Villalba and Xavier Gabarrell},
  journal  = {Resources Conservation and Recycling},
  title    = {Exergy analysis applied to biodiesel production},
  year     = {2007},
  number   = {2},
  pages    = {397-407},
  volume   = {51},
  abstract = {In our aim to decrease the consumption of materials and energy and promote the use of renewable resources, such as biofuels, rises the need to measure materials and energy fluxes. This paper suggests the use of Exergy Flow Analysis (ExFA) as an environmental assessment tool to account wastes and emissions, determine the exergetic efficiency, compare substitutes and other types of energy sources: all useful in defining environmental and economical policies for resource use. In order to illustrate how ExFA is used, it is applied to the process of biodiesel production. The results show that the production process has a low exergy loss (492MJ). The exergy loss is reduced by using potassium hydroxide and sulphuric acid as process catalysts and it can be further minimised by improving the quality of the used cooking oil.},
  doi      = {10.1016/j.resconrec.2006.10.008},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1019705090},
}

@Article{Tang2013a,
  author   = {Ying Tang and Jingfang Xu and Jie Zhang and Yong Lu},
  journal  = {Journal of Cleaner Production},
  title    = {Biodiesel production from vegetable oil by using modified CaO as solid basic catalysts},
  year     = {2013},
  pages    = {198-203},
  volume   = {42},
  abstract = {A high efficient production of fatty acid methyl ester (FAME) from soybean oil and rapeseed oil was carried out using modified CaO as solid basic catalyst by connecting bromooctane to the surface of CaO chemically in a simple way. It was found that 99.5% yield of the FAME over modified CaO was obtained from soybean oil using 15:1 molar ratio of methanol to oil after 3 h at reaction temperature of 65 °C, which is much higher than the yield of 35.4% over commercial CaO at the same reaction conditions. For the transesterification between rapeseed oil and methanol, the reaction time to its highest yield, 99.8%, was shortened to 2.5 h. The physical and chemical properties of catalysts were characterized by using techniques of X-ray diffraction (XRD), scanning electron microscope (SEM), BET surface area measurement (BET), Fourier transform-infrared (FT-IR) spectroscopy and thermogravimeter (TG). The results indicated that well dispersed CaO with relatively small particle sizes and high surface areas were obtained after modification. Furthermore, the thermal stability of modified CaO is improved and the amount of Ca(OH)2 formed during the modifying process is very little. Influence of the amount of modifier and various reaction conditions, such as mass ratio of catalyst to oil, reaction temperature and molar ratio of methanol to oil, were investigated in detail. Furthermore, water-tolerance of the modified CaO was tested by adding water in the reaction system.},
  doi      = {10.1016/j.jclepro.2012.11.001},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1029649992},
}

@Article{Ghaderi2018,
  author   = {Hamid Ghaderi and Alireza Moini and Mir Saman Pishvaee},
  journal  = {Journal of Cleaner Production},
  title    = {A multi-objective robust possibilistic programming approach to sustainable switchgrass-based bioethanol supply chain network design},
  year     = {2018},
  pages    = {368-406},
  volume   = {179},
  abstract = {This paper proposes a multi-objective robust possibilistic programming model for the design of a sustainable switchgrass-based bioethanol supply chain network under the epistemic uncertainty of input data considering conflicting economic, environmental and social objectives. The newest and most effective environmental and social life cycle assessment-based methods are applied to the proposed model to measure the relevant environmental and social impacts. To deal with uncertain parameters effectively, a novel multi-objective robust possibilistic programming approach is developed which is able to maximize the mean value of supply chain performance and control the optimality as well as feasibility robustness. Computational analysis is also provided by using a real case study in Iran to show the performance and validity of the proposed model. The results show that with an increase of 2.43% in the economic objective function, a desirable level of environmental and social protection is achieved. Additionally, the mean value of supply chain performance will enjoy more desirable values if the influence of optimality robustness and feasibility robustness decreases. The results also demonstrate that the proposed robust model outperforms the deterministic model in terms of the average and standard deviation measures.},
  doi      = {10.1016/j.jclepro.2017.12.218},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1100248345},
}

@Article{Kristoufek2012,
  author   = {Ladislav Kristoufek and Karel Janda and David Zilberman},
  journal  = {Energy Economics},
  title    = {Correlations between biofuels and related commodities before and during the food crisis: A taxonomy perspective},
  year     = {2012},
  number   = {5},
  pages    = {1380-1391},
  volume   = {34},
  abstract = {In this paper, we analyze the relationships between the prices of biodiesel, ethanol and related fuels and agricultural commodities with a use of minimal spanning trees and hierarchical trees. To distinguish between short-term and medium-term effects, we construct these trees for different frequencies (weekly and monthly). We find that in short-term, both ethanol and biodiesel are very weakly connected with the other commodities. In medium-term, the biofuels network becomes more structured. The system splits into two well separated branches — a fuels part and a food part. Biodiesel tends to the fuels branch and ethanol to the food branch. When the periods before and after the food crisis of 2007/2008 are compared, the connections are much stronger for the post-crisis period. This is the first application of this methodology on the biofuel systems.},
  doi      = {10.1016/j.eneco.2012.06.016},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1004602535},
}

@Article{Lakshmikandan2020,
  author   = {M. Lakshmikandan and A.G. Murugesan and Shuang Wang and Abd El-Fatah Abomohra and P. Anjelin Jovita and S. Kiruthiga},
  journal  = {Journal of Cleaner Production},
  title    = {Sustainable biomass production under CO2 conditions and effective wet microalgae lipid extraction for biodiesel production},
  year     = {2020},
  pages    = {119398},
  volume   = {247},
  abstract = {The freshwater microalga Chlorella vulgaris MSU AGM 14 was cultured at different CO2 conditions (up to 8%) with 100 μmol photons m−2 s−1 to evaluate biomass and lipid productivity. For effective extraction of intra-cellular lipids, a novel method based on mild pressure (1–2.5 kg/cm2) with short period (5–15 min) of heat shock (50–70 °C) were studied for the wet biomass. The transesterified lipids were qualitatively and quantitatively analysed using Gas Chromatography coupled with Mass Spectrometry (GC-MS). The higher CO2 aeration (8%) significantly increased the biomass productivity (23%) when compared with control and CO2 (4%) aeration. Total lipid production (93%) acquired by conventional extraction procedure showed enhanced production simultaneously. The maximum lipid recovery (0.225 g g−1 dw) was obtained at a pressure of 2 kg/cm2 and heating for 10 min at 60 °C. The transesterified lipids showed that oleic acid (C18:1–51.62%) was the main component in both conventional and suggested lipid extraction process. The suggested extraction process showed significant increase in biodiesel yield by 26.7%. The energy outputs of biodiesel by conventional and suggested extraction process were 417.7 and 533.6 MJ ton−1, respectively. The overall results indicated that 8% of CO2 induced the biomass and lipid productivity by 94% and 54.8%, respectively, when compared with control. In addition, the suggested mild pressure with heat shock extraction process further enhanced the lipid recovery by 21% which serves as a cost-effective lipid extraction process for microalgae.},
  doi      = {10.1016/j.jclepro.2019.119398},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1122753999},
}

@Article{Kiwjaroun2009,
  author   = {Choosak Kiwjaroun and Chanporn Tubtimdee and Pornpote Piumsomboon},
  journal  = {Journal of Cleaner Production},
  title    = {LCA studies comparing biodiesel synthesized by conventional and supercritical methanol methods},
  year     = {2009},
  number   = {2},
  pages    = {143-153},
  volume   = {17},
  abstract = {Biodiesel is one of the candidates for alternative fuels since it is renewable, sustainable and produced from domestic or non-food crop resources with less detrimental environmental impact. There are several different approaches to the production process. In this paper a new production method, the supercritical methanol process, was investigated and its environmental performance was compared with the conventional alkali-catalyzed process using LCA as a tool. The supercritical process is technically a simpler production process and offers the advantages of higher yields and the production of less environmentally damaging wastes. However, it still generates a significantly higher environmental load since the methanol recovery unit requires a large amount of energy.},
  doi      = {10.1016/j.jclepro.2008.03.011},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1019376553},
}

@Article{Hayyan2014,
  author   = {Adeeb Hayyan and Mohd Ali Hashim and Maan Hayyan and Farouq S. Mjalli and Inas M. AlNashef},
  journal  = {Journal of Cleaner Production},
  title    = {A new processing route for cleaner production of biodiesel fuel using a choline chloride based deep eutectic solvent},
  year     = {2014},
  pages    = {246-251},
  volume   = {65},
  abstract = {In this study, the free fatty acids (FFA) content in acidic crude palm oil (ACPO) was converted to fatty acid methyl esters (FAME) using a choline chloride based deep eutectic solvent (ChCl-DES) for the first time. This DES is composed of a mixture of a hydrogen bond donor (i.e. p-toluenesulfonic acid monohydrate) and a salt (i.e. choline chloride). The pre-treatment esterification stage involved the use of ChCl-DES. A dosage of 0.75 mass ratio of ChCl-DES to ACPO resulted in reducing the FFA of ACPO from 9.00% to less than 1%. The molar ratio of methanol to oil was 10:1 (methanol to oil), at a temperature of 60 °C and within 30 min reaction time. Three recycling runs of ChCl-DES were achieved without significant loss in its activity. The yield of the final product after the alkaline transesterification was 92 wt% with 0.07% FFA and FAME content 96 mol%. All other properties have met the international standard specifications for biodiesel quality. The ChCl-DES can be used as a pre-treatment catalyst for a wide range of waste and acidic oils.},
  doi      = {10.1016/j.jclepro.2013.08.031},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1052968606},
}

@Article{Silalertruksa2012,
  author   = {Thapat Silalertruksa and Sébastien Bonnet and Shabbir H. Gheewala},
  journal  = {Journal of Cleaner Production},
  title    = {Life cycle costing and externalities of palm oil biodiesel in Thailand},
  year     = {2012},
  pages    = {225-232},
  volume   = {28},
  abstract = {One of the issues related to the increased use of biofuel (ethanol and biodiesel) in the transport sector concerns their higher production costs, either in pure or blended form, as compared to conventional fuels (gasoline and diesel). Based on the average cost of biodiesel, the former is not able to compete with diesel if no subsidies are provided by the government to boost its cost competitiveness. However, such a cost comparison is not a true reflection of the various potential benefits of biofuels. This study aims to evaluate the influence of externalities on the cost performance of various palm oil biodiesel blends (B5, B10 and B100) when internalized into their respective production cost for the case of Thailand. A case study of palm oil biodiesel has been assessed and compared to conventional diesel. An income elasticity of willingness to pay (WTP) was used as multiplier factor to transfer the values of selected environmental damage costs obtained from the Environmental Priority Strategies (EPS) methodology into Thai context. The three key environmental burdens considered in this work include land use, fossil energy resources depletion and air pollutants emissions i.e. CO2, CH4, N2O, CO, NOx, SO2, VOC and PM10. The results obtained indicate that environmental costs contribute to 34% of the total costs of conventional diesel. In comparison to diesel and for the same performance, the total environmental cost of biodiesel based palm methyl ester (PME) is about 3–76% lower depending on the blending levels. This is mainly due to two major advantages that biofuels present which are a reduction in the depletion of fossil energy resources and mitigation of carbon dioxide emissions. In terms of net social benefits, the promotion of biodiesel to replace diesel is economically feasible contributing a gain in welfare of about 0.01 and 0.76THBL−1 diesel equivalent for B5 and B10 respectively.},
  doi      = {10.1016/j.jclepro.2011.07.022},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1002495312},
}

@Article{Dhinesh2018,
  author   = {B. Dhinesh and M. Annamalai},
  journal  = {Journal of Cleaner Production},
  title    = {A study on performance, combustion and emission behaviour of diesel engine powered by novel nano nerium oleander biofuel},
  year     = {2018},
  pages    = {74-83},
  volume   = {196},
  abstract = {In connection with the threating environmental pollution and stringent emission norms, the present experimental research studies the effect of utilizing cerium oxide nanoparticle mixed with an emulsion of nerium oleander biofuel (ENOB) on a compression ignition (CI) direct injection (DI) diesel engine. The whole probe was persuaded using a mono-cylinder 4-stroke direct injection CI engine. A novel nerium olender biofuel was extracted and esterified. Later it was converted into the emulsion of nerium oleander biofuel, that ensued in diluted oxides of nitrogen and in reduced smoke opacity emission. However it produced a marginal penalty of CO and HC emission when equated with neat nerium oleander biofuel. Subsequently, the nano particle blended emulsion of nerium oleander biofuel depicted dramatic diminution in CO, smoke opacity, HC, and NOx emission when equated with standard fossil diesel, neat nerium oleander biofuel and an emulsion of nerium oleander biofuel at various power outputs. It was concluded that the nano emulsion of nerium oleander biofuel could be used in direct injection, compression ignition engines without any hardware alterations in the engine.},
  doi      = {10.1016/j.jclepro.2018.06.002},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1104374325},
}

@Article{Nanaki2012,
  author   = {Evanthia A. Nanaki and Christopher J. Koroneos},
  journal  = {Journal of Cleaner Production},
  title    = {Comparative LCA of the use of biodiesel, diesel and gasoline for transportation},
  year     = {2012},
  number   = {1},
  pages    = {14-19},
  volume   = {20},
  abstract = {The energy fuels used for in the Greek transport sector are made up of gasoline consumed by automobiles, diesel oil consumed by taxis, trucks, maritime transport and railroads, and jet fuel used in the aircrafts. All these fuels are hydrocarbons that emit great amounts of CO2 which has a major impact in the global warming phenomenon. The issues relating to climate change, the soaring energy prices, and the uncertainty of future oil supplies, have created a strong interest in alternative transportation fuels. During the past decade biofuels in the form of blended gasoline and biodiesel have begun to find place in energy economy. The Greek car market shows a remarkably low rate in the penetration of biodiesel compared to the average European Union market. This work compares the environmental impacts of the use of gasoline, diesel and biodiesel in Greece using as a tool for the comparison the Life Cycle Assessment (LCA) methodology. The environmental impacts taken into consideration include: organic respiratory effects, inorganic respiratory effects, fossil fuels, acidification – eutrophication, greenhouse effect, ecotoxicity and carginogenic effects. From the environmental point of view, biodiesel appears attractive since its use results in significant reductions of GHG emissions in comparison to gasoline and diesel. It also has lower well-to-wheel emissions of methane. However, the use of biodiesel as transportation fuel increases emissions of PM10, nitrous oxide, nitrogen oxides (NOx) as well as nutrients such as nitrogen and phosphorous; the latter are the main agents for eutrophication.This study can be considered as an opportunity for further research and evaluate the available options for a sustainable transportation system planning in Greece.},
  doi      = {10.1016/j.jclepro.2011.07.026},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1023206201},
}

@Article{Hall2009,
  author   = {Jeremy Hall and Stelvia Matos and Liv Severino and Napoleão Beltrão},
  journal  = {Journal of Cleaner Production},
  title    = {Brazilian biofuels and social exclusion: established and concentrated ethanol versus emerging and dispersed biodiesel},
  year     = {2009},
  pages    = {s77-s85},
  volume   = {17},
  abstract = {Increasing interest in biofuels trade between developed and developing countries has spurred worldwide discussions on issues such as subsidies and the ‘food for fuel’ crisis. One issue missing in recent discourse is the pressure exerted on developing countries to adopt large-scale mechanized farming practices to increase economic efficiencies. Such approaches often exclude small-scale farmers from participating in the emerging biofuels market, thus exacerbating poverty and social exclusion. Drawing on both qualitative and technical data, we discuss such pressures using Brazilian ethanol and biodiesel production. Pressure from international markets to become more economically efficient may contribute towards the erosion of recent schemes to encourage social benefits for small farmers in biodiesel production. We conclude with trade and policy implications.},
  doi      = {10.1016/j.jclepro.2009.01.003},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1049492388},
}

@Article{Babazadeh2017,
  author   = {Reza Babazadeh and Jafar Razmi and Mir Saman Pishvaee and Masoud Rabbani},
  journal  = {Omega},
  title    = {A sustainable second-generation biodiesel supply chain network design problem under risk},
  year     = {2017},
  note     = {http://manuscript.elsevier.com/S0305048316000177/pdf/S0305048316000177.pdf},
  pages    = {258-277},
  volume   = {66},
  abstract = {This paper presents a multi-objective possibilistic programming model to design a second-generation biodiesel supply chain network under risk. The proposed model minimizes the total costs of biodiesel supply chain from feedstock supply centers to customer centers besides minimizing the environmental impact (EI) of all involved processes under a well-to-wheel perspective. Non-edible feedstocks are considered for biodiesel production. Variable cultivation cost of non-edible feedstock is assumed to be non-linear and dependent upon the amount of cultivated area. New formulation of possibilistic programming method is developed which is able to minimize the total mean and risk values of problems with possibilistic-based uncertainty. To solve the proposed multi-objective model, a hybrid solution approach based on flexible lexicographic and augmented ɛ-constraint methods is proposed which is capable to find appropriate efficient solutions from the Pareto-optimal set. The performance of the proposed possibilistic programming method as well as the developed solution approach are evaluated and validated through conducting a real case study in Iran. The outcome of this study demonstrates that high investment cost is required for improving the environmental impact and risk of sustainable biodiesel supply chain network design under risk. Decision maker preferences are required for suitable trade-off among total costs, risk values and environmental impact.},
  doi      = {10.1016/j.omega.2015.12.010},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1044581237},
}

@Article{Ahmed2018,
  author   = {Waqas Ahmed and Biswajit Sarkar},
  journal  = {Journal of Cleaner Production},
  title    = {Impact of carbon emissions in a sustainable supply chain management for a second generation biofuel},
  year     = {2018},
  pages    = {807-820},
  volume   = {186},
  abstract = {Global warming due to excessive use of fossil fuels has driven researchers to focus on sustainable energy sources for the future. For clean production systems, biofuel is expanding the domain of renewable and sustainable energy supplies. An efficient and sustainable supply chain plays a pivotal role in ensuring this supply. In this research, crop residuals in different agricultural zones, transportation for shipment of residual biomass as well as biofuel, multiple biorefineries, and multiple market centers are considered. The expense of the resources, the yield of residual biomass in agricultural zones, and the demand of market centers are represented by fuzzy numbers as they are assumed to be uncertain. The carbon emissions cost at all stages of the supply chain was also incorporated into this model. This objective of this study is to develop a supply chain model that minimizes the total cost of a second generation biofuel supply chain and location-allocation for agricultural zones and biorefineries to meet the uncertain demand for market centers. Two numerical examples are analyzed, and the results proves that the cost of biofuel production in biorefineries contributed 52.16%, which is a major proportion of the total cost. In the entire supply chain, the transportation sector is the foremost source of carbon emissions in an environment with 88.50% of the total carbon emissions. The results confirms that the proposed model is viable for designing second generation biofuel supply chains under uncertainty. Significant managerial insights of this research are also described to better express the efficiency of the model.},
  doi      = {10.1016/j.jclepro.2018.02.289},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1101561277},
}

@Article{Aitken2014,
  author   = {Douglas Aitken and Cristian Bulboa and Alex Godoy-Faundez and Juan L. Turrion-Gomez and Blanca Antizar-Ladislao},
  journal  = {Journal of Cleaner Production},
  title    = {Life cycle assessment of macroalgae cultivation and processing for biofuel production},
  year     = {2014},
  pages    = {45-56},
  volume   = {75},
  abstract = {There has been a recent resurgence in research investigating bioenergy production from algal biomass due to the potential environmental benefits in comparison to conventional bioenergy crops and conventional fossil fuels. This life cycle assessment (LCA) considered the energy return and environmental impacts of the cultivation and processing of macroalgae (seaweed) to bioethanol and biogas with a particular focus on specific species (Gracilaria chilensis and Macrocystis pyrifera) and cultivation methods (bottom planting and long-line cultivation). The study was based mainly upon data obtained from research conducted in Chile but the results can be applied to other locations where similar cultivation is feasible. Speculative data were also included to test promising data obtained from research. The results suggested that using base case conditions the production of both bioethanol and biogas from bottom planted Gracilaria chilensis was the most sustainable option due to the low input method of cultivation. Using new advances in cultivation and processing methods of long-line cultivated Macrocystis pyrifera however resulted in a much more sustainable source of bioenergy. If these methods can be proven on a large scale, the generation of bioenergy from macroalgae could be highly competitive in terms of its sustainability compared to alternative feedstocks. Future research should bear in mind that the results of this study should however be considered highly optimistic given the early stage of research.},
  doi      = {10.1016/j.jclepro.2014.03.080},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1015524088},
}

@Article{Harding2008,
  author   = {K.G. Harding and J.S. Dennis and H. von Blottnitz and S.T.L. Harrison},
  journal  = {Journal of Cleaner Production},
  title    = {A life-cycle comparison between inorganic and biological catalysis for the production of biodiesel},
  year     = {2008},
  number   = {13},
  pages    = {1368-1378},
  volume   = {16},
  abstract = {Life cycle assessment (LCA) has been used to compare inorganic and biological catalysis for the production of biodiesel by transesterification. The inorganic route, using catalysis by sodium hydroxide, has been compared with a conceptual biological one using enzymatic catalysis by the lipase produced by Candida antarctica. Although biological catalysis has not been used for industrial production of biodiesel to date, results from laboratory experiments suggest that it could have distinct advantages over the inorganic route, particularly with regard to a simplified flowsheet for purification and concomitant energy savings. Five flowsheet options have been included in the study to investigate the alkali and enzyme catalysed production routes from rapeseed oil, use of methanol or ethanol for transesterification and the effect of efficiency of alcohol recovery. The LCA shows that the enzymatic production route is environmentally more favourable. Improvements are seen in all impact categories. Global warming, acidification, and photochemical oxidation are reduced by 5%. Certain toxicity levels have more than halved. These results are mainly due to lower steam requirements for heating in the biological process.},
  doi      = {10.1016/j.jclepro.2007.07.003},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1023297174},
}

@Article{Moazeni2019,
  author   = {Faegheh Moazeni and Yen-Chih Chen and Gaosen Zhang},
  journal  = {Journal of Cleaner Production},
  title    = {Enzymatic transesterification for biodiesel production from used cooking oil, a review},
  year     = {2019},
  pages    = {117-128},
  volume   = {216},
  abstract = {This paper reviews various aspects of the enzymatic transesterification method to convert used cooking oil to biodiesel. The goal of this paper is to provide a thorough overview from general biodiesel production processes, reaction conditions, challenges, and solutions for higher biodiesel production yield through introducing various microorganisms that are capable of producing the enzymes required to convert used cooking oil into biodiesel. The characteristics, composition, and advantages of the used cooking oil, as feedstock for biodiesel, is also discussed. In addition, the existing transesterification methods including homogeneous alkali-catalyzed, homogeneous acid-catalyzed, non-catalytic reaction under super-critical conditions, and enzyme-catalyzed reactions are explained. Furthermore, the advantages of the enzymatic method over other methods, and the enzymes, which are the key elements of such reactions, are discussed. Lipases are the most promising enzymes currently known for biodiesel conversion. The physiological and physical properties of microbial lipases, the catalytic mechanisms of the enzymes, various methods of enzyme immobilization such as adsorption, covalent and affinity binding, entrapment, and the whole-cell immobilization are also reviewed. At the end, three case studies demonstrating unique and efficient enzymatic transesterification approaches are presented.},
  doi      = {10.1016/j.jclepro.2019.01.181},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1111565453},
}

@Article{Keeney2009,
  author   = {Roman Keeney and Thomas W. Hertel},
  journal  = {American Journal of Agricultural Economics},
  title    = {The Indirect Land Use Impacts of United States Biofuel Policies: The Importance of Acreage, Yield, and Bilateral Trade Responses},
  year     = {2009},
  number   = {4},
  pages    = {895-909},
  volume   = {91},
  abstract = {Abstract Recent analysis has highlighted agricultural land conversion as a significant debit in the greenhouse gas accounting of ethanol as an alternative fuel. A controversial element of this debate is the role of crop yield growth as a means of avoiding cropland conversion in the face of biofuels growth. We find that standard assumptions of yield response are unduly restrictive. Furthermore, we identify both the acreage response and bilateral trade specifications as critical considerations for predicting global land use change. Sensitivity analysis reveals that each of these contributes importantly to parametric uncertainty.},
  doi      = {10.1111/j.1467-8276.2009.01308.x},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1061934170},
}

@Article{Gurgel2007,
  author   = {Angelo Gurgel and John M Reilly and Sergey Paltsev},
  journal  = {Journal of Agricultural & Food Industrial Organization},
  title    = {Potential Land Use Implications of a Global Biofuels Industry},
  year     = {2007},
  number   = {2},
  volume   = {5},
  abstract = {In this paper we investigate the potential production and implications of a global biofuels industry. We develop alternative approaches to consistently introduce land as an economic factor input and in physical terms into a computable general equilibrium framework. The approach allows us to parameterize biomass production consistent with agro-engineering information on yields and a "second generation" cellulosic biomass conversion technology. We explicitly model land conversion from natural areas to agricultural use in two different ways: in one approach we introduced a land supply elasticity based on observed land supply responses and in the other approach we considered only the direct cost of conversion. We estimate biofuels production at the end of the century could reach 221 to 267 EJ in a reference scenario and 319 to 368 EJ under a global effort to mitigate greenhouse gas emissions. The version with the land supply elasticity allowed much less conversion of land from natural areas, forcing intensification of production, especially on pasture and grazing land, whereas the pure conversion cost model led to significant deforestation. These different approaches emphasize the importance of somehow reflecting the non-market value of land more fully in the conversion decision. The observed land conversion response we estimate may be a short turn response that does not fully reflect the effect of long run pressure to convert land if rent differentials are sustained over 100 years.},
  doi      = {10.2202/1542-0485.1202},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1037107713},
}

@Article{Cavalett2010,
  author   = {Otávio Cavalett and Enrique Ortega},
  journal  = {Journal of Cleaner Production},
  title    = {Integrated environmental assessment of biodiesel production from soybean in Brazil},
  year     = {2010},
  number   = {1},
  pages    = {55-70},
  volume   = {18},
  abstract = {This paper presents the results of an environmental impact assessment of biodiesel production from soybean in Brazil. In order to achieve this objective, environmental impact indicators provided by Emergy Accounting (EA), Embodied Energy Analysis (EEA) and Material Flow Accounting (MFA) were used. The results showed that for one liter of biodiesel 8.8kg of topsoil are lost in erosion, besides the cost of 0.2kg of fertilizers, about 5.2m2 of crop area, 7.33kg of abiotic materials, 9.0 tons of water and 0.66kg of air and about 0.86kg of CO2 were released. About 0.27kg of crude oil equivalent is required as inputs to produce one liter of biodiesel, which means an energy return of 2.48J of biodiesel per Joule of fossil fuel invested. The transformity of biodiesel (3.90E+05seJJ−1) is higher than those calculated for fossil fuels as other biofuels, indicating a higher demand for direct and indirect environmental support. Similarly, the biodiesel emergy yield ratio (1.62) indicates that a very low net emergy is delivered to consumers, compared to alternatives. Obtained results show that when crop production and industrial conversion to fuel are supported by fossil fuels in the form of chemicals, goods, and process energy, the fraction of fuel that can actually be considered renewable is very low (around 31%).},
  doi      = {10.1016/j.jclepro.2009.09.008},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1034672998},
}

@Article{Macombe2013,
  author   = {Catherine Macombe and Pekka Leskinen and Pauline Feschet and Riina Antikainen},
  journal  = {Journal of Cleaner Production},
  title    = {Social life cycle assessment of biodiesel production at three levels: a literature review and development needs},
  year     = {2013},
  note     = {https://hal.inrae.fr/hal-02598404/file/pub00038078.pdf},
  pages    = {205-216},
  volume   = {52},
  abstract = {Assessment of social impacts of products and services has gained increasing interest in society. Life cycle assessment (LCA) is a tool developed to estimate the impacts of products and services from cradle to grave. Traditionally LCA has focused on environmental impacts, but recently approaches for social life cycle assessment (SLCA) have also been developed. Most of them fairly address social performances of business, but the aim of this paper is to analyse the possibilities and development needs in the complementary approach, which is the evaluation of social impacts in LCA. We review the field in general and take a closer look at the empirical case of biodiesel production, which is a timely topic globally in view of the climate change mitigation objectives. The analysis is carried out at three levels – company, regional, and state level. Despite active development in the field of SLCA, we conclude that in many cases it is not yet possible to carry out a comprehensive SLCA. Finally, we outline lines of research that would further improve the methodological and empirical basis of SLCA at various levels of decision-making.},
  doi      = {10.1016/j.jclepro.2013.03.026},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1052527949},
}

@Article{Borawski2019,
  author   = {Piotr Bórawski and Aneta Bełdycka-Bórawska and Elżbieta Jadwiga Szymańska and Krzysztof Józef Jankowski and Bogdan Dubis and James W. Dunn},
  journal  = {Journal of Cleaner Production},
  title    = {Development of renewable energy sources market and biofuels in The European Union},
  year     = {2019},
  pages    = {467-484},
  volume   = {228},
  abstract = {The aim of the paper is to present renewable energy market development with particular regard paid to biofuels in the EU. The analysis included data on the share of renewables in Gross Island energy consumption, changes of renewable energy in the years 2004–2016 and the amount of liquid biofuels. The authors of the paper used descriptive and statistical methods to describe the changes in bioenergy development in the European Union (EU). The biggest share of biofuels and renewable waste can be seen in Latvia (31.2%), Finland (26.7%) and Sweden (24.8%). The highest percentage of wind energy in 2015 was found in: Denmark (7.2%), Portugal (4.3%), Ireland (4.0%) and Spain (3.5%). The highest share of solar energy in 2015 was found in Cyprus (3.5%), Spain (2.6%) and Greece (2.2%). The highest contribution of geothermal energy was found in 2015 in Italy (3.5%), Portugal (0.8%) and Slovenia (0,7%). Hydropower was the biggest in 2015 in Sweden (14.2%), Austria (9.6%) and Slovenia (5.0%). The highest coefficients of variation of the share of electricity from renewable energy sources were found in the years 2004–2017 in Malta (140.3%), Cyprus (101.1%) and United Kingdom (71.9%). In addition, the highest coefficients of variation of share of renewable energy sources in heating and cooling in the years 2004–2017 were found in Malta (72.4%), United Kingdom (69.81%) and Hungary (44.91%). Moreover, the highest coefficients of variation of share of renewable energy sources in transport in the years 2004–2017 were found in Finland (113.78%), Malta (115.52%) and Belgium (96.53%). The biggest producers of ethanol and biodiesel in EU were Germany, France and Poland. Cluster analysis data show that Germany and France are of key importance in the production of biodiesel and ethanol. The biodiesel production increased in the years 2003–2017 from 719,32 million liters to 13323 million liters (increase 1852.2%). However, in the years 2014–2017 a stagnation in biofuel production was observed from 13673 million liters to 13323 million liters (−2,56%). The situation on the market and the increasing demand for green energy suggest that the production of ethanol and esters of vegetable oils will increase by 2030, which will contribute to the development of this sector.},
  doi      = {10.1016/j.jclepro.2019.04.242},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1113705995},
}

@Article{ArizaMontobbio2010,
  author   = {Pere Ariza-Montobbio and Sharachchandra Lele},
  journal  = {Ecological Economics},
  title    = {Jatropha plantations for biodiesel in Tamil Nadu, India: Viability, livelihood trade-offs, and latent conflict},
  year     = {2010},
  number   = {2},
  pages    = {189-195},
  volume   = {70},
  abstract = {Researchers, policy makers and civil society organizations have been discussing the potential of biofuels as partial substitutes for fossil fuels and thereby as a simultaneous solution for climate change and rural poverty. Research has highlighted the ambiguity of these claims across various dimensions and scales, focusing on ethanol-producing or oilseed crops in agricultural lands or Jatropha-type crops on common lands. We studied the agronomic and economic viability and livelihood impacts of Jatropha curcas plantations on private farms in Tamil Nadu, India. We found that Jatropha yields are much lower than expected and its cultivation is currently unviable, and even its potential viability is strongly determined by water access. On the whole, the crop impoverishes farmers, particularly the poorer and socially backward farmers. Jatropha cultivation therefore not only fails to alleviate poverty, but its aggressive and misguided promotion will generate conflict between the state and the farmers, between different socio-economic classes and even within households. The water demands of the crop can potentially exacerbate the conflicts and competition over water access in Tamil Nadu villages.},
  doi      = {10.1016/j.ecolecon.2010.05.011},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1026369526},
}

@Article{Hertel2010,
  author   = {Thomas W. Hertel and Wallace E. Tyner and Dileep K. Birur},
  journal  = {The Energy Journal},
  title    = {The Global Impacts of Biofuel Mandates},
  year     = {2010},
  note     = {http://web.ics.purdue.edu/~hertel/data/uploads/publications/energy-journal-hertel-tyner-birur.pdf},
  number   = {1},
  pages    = {75-100},
  volume   = {31},
  abstract = {The rise in world oil prices, coupled with heightened interest in the abatement of greenhouse gas emissions, led to a sharp increase in biofuels production around the world. Previous authors have devoted considerable attention to the impacts of these policies on a country-by-country basis. However, there are also strong interactions among these programs, as they compete in world markets for feedstocks and ultimately for a limited supply of global land. In this paper, we offer the first global assessment of biofuel programs - focusing particularly on the EU and US. We begin with an historical analysis of the period 2001-2006, which also permits us to validate the model. We then conduct an ex ante analysis of mandates in the year 2015. We find that if these mandates are indeed fulfilled the impact on global land use could be substantial, with potentially significant implications for greenhouse gas emissions.},
  doi      = {10.5547/issn0195-6574-ej-vol31-no1-4},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1072974020},
}

@Article{Morais2010,
  author   = {Sérgio Morais and Teresa M. Mata and António A. Martins and Gilberto A. Pinto and Carlos A.V. Costa},
  journal  = {Journal of Cleaner Production},
  title    = {Simulation and life cycle assessment of process design alternatives for biodiesel production from waste vegetable oils},
  year     = {2010},
  note     = {https://recipp.ipp.pt/bitstream/10400.22/3030/4/ART_TeresaMata_2010_GRAQ.pdf},
  number   = {13},
  pages    = {1251-1259},
  volume   = {18},
  abstract = {This study uses the process simulator ASPEN Plus® and Life Cycle Assessment (LCA) to compare three process design alternatives for biodiesel production from waste vegetable oils that are: the conventional alkali-catalyzed process including a free fatty acids (FFAs) pre-treatment, the acid-catalyzed process, and the supercritical methanol process using propane as co-solvent. Results show that the supercritical methanol process using propane as co-solvent is the most environmentally favorable alternative. Its smaller steam consumption in comparison with the other process design alternatives leads to a lower contribution to the potential environmental impacts (PEI’s). The acid-catalyzed process generally shows the highest PEI’s, in particular due to the high energy requirements associated with methanol recovery operations.},
  doi      = {10.1016/j.jclepro.2010.04.014},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1048939047},
}

@Article{Sanjid2014,
  author   = {A. Sanjid and H.H. Masjuki and M.A. Kalam and S.M. Ashrafur Rahman and M.J. Abedin and S.M. Palash},
  journal  = {Journal of Cleaner Production},
  title    = {Production of palm and jatropha based biodiesel and investigation of palm-jatropha combined blend properties, performance, exhaust emission and noise in an unmodified diesel engine},
  year     = {2014},
  pages    = {295-303},
  volume   = {65},
  abstract = {An ever increasing drift of energy consumption, unequal geographical distribution of natural wealth and the quest for low carbon fuel for a cleaner environment are sparking off the production and use of biodiesels in many countries around the globe. In this work, palm biodiesel and jatropha biodiesel were produced from the respective crude vegetable oils through transesterification, and the different physicochemical properties of the produced biodiesels have been presented, and found to be acceptable according to the ASTM standard of biodiesel specification. This paper presents experimental results of the research carried out to evaluate the BSFC, engine power, exhaust and noise emission characteristics of a combined palm and jatropha blend in a single-cylinder diesel engine at different engine speeds ranging from 1400 to 2200 rpm. Though the PBJB5 and PBJB10 biodiesels showed a slightly higher BSFC than diesel fuel, all the measured emission parameters and noise emission were significantly reduced, except for NO emission. CO emissions for PBJB5 and PBJB10 were 9.53% and 20.49% lower than for diesel fuel. By contrast, HC emissions for PBJB5 and PBJB10 were 3.69% and 7.81% lower than for diesel fuel. The sound levels produced by PBJB5 and PBJB10 were also reduced by 2.5% and 5% compared with diesel fuel due to their lubricity and damping characteristics.},
  doi      = {10.1016/j.jclepro.2013.09.026},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1050772383},
}

@Article{Rosegrant2008,
  author  = {Mark W. Rosegrant and Tingju Zhu and Siwa Msangi and Timothy Sulser},
  journal = {Applied Economic Perspectives and Policy},
  title   = {Global Scenarios for Biofuels: Impacts and Implications},
  year    = {2008},
  note    = {https://doi.org/10.1111/j.1467-9353.2008.00424.x},
  number  = {3},
  pages   = {495-505},
  volume  = {30},
  doi     = {10.1111/j.1467-9353.2008.00424.x},
  groups  = {Sustainable Biofuel Economy},
  url     = {https://app.dimensions.ai/details/publication/pub.1061936485},
}

@Article{Fattah2020,
  author   = {I. M. Rizwanul Fattah and H. C. Ong and T. M. I. Mahlia and M. Mofijur and A. S. Silitonga and S. M. Ashrafur Rahman and Arslan Ahmad},
  journal  = {Frontiers in Energy Research},
  title    = {State of the Art of Catalysts for Biodiesel Production},
  year     = {2020},
  note     = {https://www.frontiersin.org/articles/10.3389/fenrg.2020.00101/pdf},
  pages    = {101},
  volume   = {8},
  abstract = {Biodiesel is one of the potential alternative energy sources that can be derived from renewable and low-grade origin through different processes. One of the processes is alcoholysis or transesterification in the presence of a suitable catalyst. The catalyst can be either homogeneous or heterogeneous. This article reviews various catalysts used for biodiesel production to date, presents the state of the art of types of catalysts, and compares their suitability and associated challenges in the transesterification process. Biodiesel production using homogeneous and heterogeneous catalysis has been studied extensively, and novel heterogeneous catalysts are being continuously investigated. Homogeneous catalysts are generally efficient in converting biodiesel with low free fatty acid (FFA) and water containing single-origin feedstock. Heterogeneous catalysts, on the other hand, provide superior activity, range of selectivity, good FFA, and water adaptability. The quantity and strengths of active acid or basic sites control these properties. Some of the heterogeneous catalysts such as zirconia and zeolite-based catalysts can be used as both basic and acidic catalyst by suitable alteration. Heterogeneous catalysts from waste and biocatalysts play an essential role in attaining a sustainable alternative to traditional homogeneous catalysts for biodiesel production. Recently, high catalytic efficiency at mild operating conditions has drawn attention to nanocatalysts. This review evaluates state of the art and perspectives for catalytic biodiesel production and assesses the critical operational variables that influence biodiesel production along with the technological solutions for sustainable implementation of the process.},
  doi      = {10.3389/fenrg.2020.00101},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1128580761},
}

@Article{Gorter2009,
  author   = {Harry de Gorter and David R. Just},
  journal  = {American Journal of Agricultural Economics},
  title    = {The Economics of a Blend Mandate for Biofuels},
  year     = {2009},
  number   = {3},
  pages    = {738-750},
  volume   = {91},
  abstract = {Abstract  A biofuel blend mandate may increase or decrease consumer fuel prices with endogenous oil prices, depending on relative supply elasticities. Biofuel tax credits always reduce fuel prices. Tax credits result in lower fuel prices than under a mandate for the same level of biofuel production. If tax credits are implemented alongside mandates, then tax credits subsidize fuel consumption instead of biofuels. This contradicts energy policy goals by increasing oil dependency, CO 2 emissions, and traffic congestion, while providing little benefit to either corn or ethanol producers. These social costs will be substantial with tax credits costing taxpayers $28.7 billion annually by 2022.},
  doi      = {10.1111/j.1467-8276.2009.01275.x},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1061934137},
}

@Article{Ong2019,
  author   = {Hwai Chyuan Ong and Jassinnee Milano and Arridina Susan Silitonga and Masjuki Haji Hassan and Abd Halim Shamsuddin and Chin-Tsan Wang and Teuku Meurah Indra Mahlia and Joko Siswantoro and Fitranto Kusumo and Joko Sutrisno},
  journal  = {Journal of Cleaner Production},
  title    = {Biodiesel production from Calophyllum inophyllum-Ceiba pentandra oil mixture: Optimization and characterization},
  year     = {2019},
  note     = {http://repository.ubaya.ac.id/40907/5/Joko%20Siswantoro_Biodiesel%20production%20from%20Calophyllum%20inophyllum-Ceiba%20pentandra%20oil%20mixture%20Optimization%20and%20characterization.pdf},
  pages    = {183-198},
  volume   = {219},
  abstract = {In this study, a novel modeling approach (artificial neural networks (ANN) and ant colony optimization (ACO)) was used to optimize the process variables for alkaline-catalyzed transesterification of CI40CP60 oil mixture (40 wt% of Calophyllum inophyllum oil mixed with 60 wt% of Ceiba pentandra oil) in order to maximize the biodiesel yield. The optimum values of the methanol-to-oil molar ratio, potassium hydroxide catalyst concentration, and reaction time predicted by the ANN-ACO model are 37%, 0.78 wt%, and 153 min, respectively, at a constant reaction temperature and stirring speed of 60 °C and 1000 rpm, respectively. The ANN-ACO model was validated by performing independent experiments to produce the CI40CP60 methyl ester (CICPME) using the optimum transesterification process variables predicted by the ANN-ACO model. There is very good agreement between the average CICPME yield determined from experiments (95.18%) and the maximum CICPME yield predicted by the ANN-ACO model (95.87%) for the same optimum values of process variables, which corresponds to a difference of 0.69%. Even though the ANN-ACO model is only implemented to optimize the transesterification of process variables in this study. It is believed that the model can be used to optimize other biodiesel production processes such as seed oil extraction and acid-catalyzed esterification for various types of biodiesels and biodiesel blends.},
  doi      = {10.1016/j.jclepro.2019.02.048},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1111976854},
}

@Article{Serra2013,
  author   = {Teresa Serra and David Zilberman},
  journal  = {Energy Economics},
  title    = {Biofuel-related price transmission literature: A review},
  year     = {2013},
  pages    = {141-151},
  volume   = {37},
  abstract = {In this article, an extensive review of the rapidly growing biofuel-related time-series literature is carried out. The data used, the modeling techniques and the main findings of this literature are discussed. Providing a review of this flourishing research area is relevant as a guidepost for future research. This literature concludes that energy prices drive long-run agricultural price levels and that instability in energy markets is transferred to food markets.},
  doi      = {10.1016/j.eneco.2013.02.014},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1047356473},
}

@Article{Banerjee2009,
  author   = {A. Banerjee and R. Chakraborty},
  journal  = {Resources Conservation and Recycling},
  title    = {Parametric sensitivity in transesterification of waste cooking oil for biodiesel production—A review},
  year     = {2009},
  number   = {9},
  pages    = {490-497},
  volume   = {53},
  abstract = {Methods of pretreatment and transesterification of waste cooking oils (WCOs) to yield fatty acid alkyl esters (biodiesel) qualitatively comparable with fossil diesel fuels have been discussed. The effect of different operating and processing variables viz. reaction temperature, molar ratio of alcohol to oil, the type of alcohol used, type of catalyst used and its concentration, reaction time, presence of moisture and free fatty acids (FFA) content on transesterification and different pretreatment procedures have been thoroughly reviewed. The recent advancements involving both esterification and transesterification for enhancing the overall yield of biodiesel have been discussed.},
  doi      = {10.1016/j.resconrec.2009.04.003},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1009744782},
}

@Article{Zhong2020,
  author   = {Le Zhong and Yuxiao Feng and Gaoyang Wang and Ziyuan Wang and Muhammad Bilal and Hexin Lv and Shiru Jia and Jiandong Cui},
  journal  = {International Journal of Biological Macromolecules},
  title    = {Production and use of immobilized lipases in/on nanomaterials: A review from the waste to biodiesel production},
  year     = {2020},
  pages    = {207-222},
  volume   = {152},
  abstract = {As a highly efficient and environmentally friendly biocatalyst, immobilized lipase has received incredible interest among the biotechnology community for the production of biodiesel. Nanomaterials possess high enzyme loading, low mass transfer limitation, and good dispersibility, making them suitable biocatalytic supports for biodiesel production. In addition to traditional nanomaterials such as nano‑silicon, magnetic nanoparticles and nano metal particles, novel nanostructured forms such as nanoflowers, carbon nanotubes, nanofibers and metal-organic frameworks (MOFs) have also been studied for biodiesel production in the recent years. However, some problems still exist that need to be overcome in achieving large-scale biodiesel production using immobilized lipase on/in nanomaterials. This article mainly presents an overview of the current and state-of-the-art research on biodiesel production by immobilized lipases in/on nanomaterials. Various immobilization strategies of lipase on various advanced nanomaterial supports and its applications in biodiesel production are highlighted. Influential factors such as source of lipase, immobilization methods, feedstocks, and production process are also critically discussed. Finally, the current challenges and future directions in developing immobilized lipase-based biocatalytic systems for high-level production of biodiesel from waste resources are also recommended.},
  doi      = {10.1016/j.ijbiomac.2020.02.258},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1125099402},
}

@Article{Gorter2010,
  author   = {Harry de Gorter and David R. Just},
  journal  = {Applied Economic Perspectives and Policy},
  title    = {The Social Costs and Benefits of Biofuels: The Intersection of Environmental, Energy and Agricultural Policy},
  year     = {2010},
  number   = {1},
  pages    = {4-32},
  volume   = {32},
  abstract = {Abstract The efficacy of alternative biofuel policies in achieving energy, environmental and agricultural policy goals is assessed using economic cost‐benefit analysis. Government mandates are superior to consumption subsidies, especially with suboptimal fuel taxes and the higher costs involved with raising tax revenues. But subsidies with mandates cause adverse interaction effects; oil consumption is subsidized instead. This unique result also applies to renewable electricity that faces similar policy combinations. Ethanol policy can have a significant impact on corn prices; if not, inefficiency costs rise sharply. Ethanol policy can increase the inefficiency of farm subsidies and vice‐versa. Policies that discriminate against trade, such as production subsidies and tariffs, can more than offset any benefits of a mandate. Sustainability standards are ineffective and illegal according to the WTO, and so should be re‐designed.},
  doi      = {10.1093/aepp/ppp010},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1042117532},
}

@Article{Teo2019,
  author   = {Siow Hwa Teo and Aminul Islam and Eng Seng Chan and S.Y. Thomas Choong and Nabeel H. Alharthi and Yun Hin Taufiq-Yap and Rabiul Awual},
  journal  = {Journal of Cleaner Production},
  title    = {Efficient biodiesel production from Jatropha curcus using CaSO4/Fe2O3-SiO2 core-shell magnetic nanoparticles},
  year     = {2019},
  pages    = {816-826},
  volume   = {208},
  abstract = {Core shell nanostructures have endorsed great enhancements in the production of biodiesel with well-controlled size, shape, and surface properties. A simple and reproducible hierarchically porous core-shell CaSO4/Fe2O3-SiO2 material with controllable core morphology was developed. The materials with a well mesoporous structure were prepared by ethanol/H2O media using NaNO3 as the etchant to construct the co-valent bond in Fe2O3 framework, and the CH3COONa was an electrostatic supporter and subordinate reducing agent under mild. Solvothermal conditions. The morphology, porosity and conjugated structure of CaSO4/Fe2O3-SiO2 were measured systematically. The materials of CaSO4/Fe2O3-SiO2 showed remarkable performance in biodiesel production with crude Jatropha curcus and methanol. At suitable state, the biodiesel production reached 94%. Furthermore, the material was easily dispersed in the reaction system, quickly separated from the reaction products without using centrifugation or filtration, and satisfactory catalytic activity maintained after being recycled nine. Controlling the interaction among the active phases of core shell structure might boost structural stability of the material. In addition, the performance was compared with several forms of material in the case of biodiesel production. Therefore, the data are remarkable for offering a new juncture for the fabrication of novel and eco-friendly procedure of hierarchically porous material in the potential biodiesel production. Moreover, the easy separation of material from process fluid and the safe handling were the main impacts to imply the CaSO4/Fe2O3-SiO2 core-shell material for biodiesel production.},
  doi      = {10.1016/j.jclepro.2018.10.107},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1107587297},
}

@Article{Banse2008,
  author   = {Martin Banse and Hans van Meijl and Andrzej Tabeau and Geert Woltjer},
  journal  = {European Review of Agricultural Economics},
  title    = {Will EU biofuel policies affect global agricultural markets?},
  year     = {2008},
  number   = {2},
  pages    = {117-141},
  volume   = {35},
  abstract = {This article assesses the implications of the EU Biofuels Directive (BFD) using a computable general equilibrium framework with endogenous land supply. The results show that, without policy intervention to stimulate the use of biofuel crops, the targets of the BFD will not be met. With the BFD, the enhanced demand for biofuel crops has a strong impact on agriculture globally and within Europe, leading to an increase in land use. On the other hand, the long-term declining trend in real agricultural prices may slow down or even reverse.},
  doi      = {10.1093/erae/jbn023},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1059570180},
}

@Article{Zilberman2012,
  author  = {David Zilberman and Gal Hochman and Deepak Rajagopal and Steve Sexton and Govinda Timilsina},
  journal = {American Journal of Agricultural Economics},
  title   = {The Impact of Biofuels on Commodity Food Prices: Assessment of Findings},
  year    = {2012},
  number  = {2},
  pages   = {275-281},
  volume  = {95},
  doi     = {10.1093/ajae/aas037},
  groups  = {Sustainable Biofuel Economy},
  url     = {https://app.dimensions.ai/details/publication/pub.1059379956},
}

@Article{Saravanan2018,
  author   = {Azhaham Perumal Saravanan and Thangavel Mathimani and Garlapati Deviram and Karthik Rajendran and Arivalagan Pugazhendhi},
  journal  = {Journal of Cleaner Production},
  title    = {Biofuel policy in India: A review of policy barriers in sustainable marketing of biofuel},
  year     = {2018},
  note     = {https://cora.ucc.ie/bitstreams/d459906a-2ac7-4764-8947-f22232508d38/download},
  pages    = {734-747},
  volume   = {193},
  abstract = {Global warming issue due to the combustion of fossil fuel pushes the world to produce renewable and environmental friendly fuel from sustainable feedstock. There are several measures on different levels to reduce the global warming including clean energies from wind, solar, and biomass. There are different aspects in bringing these technologies into a reality including development of technology, economic feasibilities, environmental sustainability and finally, support from the government in the form of effective policies and public awareness. Adequate R&D efforts could overcome all the factors but only an effective policy could drive those efforts to reality. Therefore, in this connection this review initially addresses the present state of energy demand, progression of biofuel sources and the bottlenecks in microalgal biofuel production and commercialization. The biofuel policies are essential to change the world's dependence on fossil fuels for a better tomorrow. Hence, this review addresses the salient features of National Biofuel Policy of India that helps in regulating the biofuels production and their marketing. As a part of Policy implementation, government of India introduced several schemes and programs in last two-decades, which includes mandate blending of ethanol with gasoline, diesel with biodiesel, for the future clean energy vision, and incentivizing bio-based products/fuels. In addition, participation of both federal and state governments for clean energy initiatives, capital investments and tax credits were described in detail. Many policies lack easy outreach among public and industries, which needs marketing by the government that secures a clean energy future in India. Though India is in the process of evolution, it might be quite difficult to enact a dedicative legislation to deal with the challenges of biofuel marketing. Therefore, recent initiatives and scope were summarized in this review for future endeavours.},
  doi      = {10.1016/j.jclepro.2018.05.033},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1103810413},
}

@Article{Chuah2017,
  author   = {Lai Fatt Chuah and Jiří Jaromír Klemeš and Suzana Yusup and Awais Bokhari and Majid Majeed Akbar},
  journal  = {Journal of Cleaner Production},
  title    = {A review of cleaner intensification technologies in biodiesel production},
  year     = {2017},
  pages    = {181-193},
  volume   = {146},
  abstract = {Biodiesel is envisaged as environmentally benign fuel due to its unique properties, such as biodegradability, renewability and non-toxicity. Its utilisation leads to the reduction of sulphur oxide and greenhouse gas emissions as it produces quite lower amounts of carbon and sulphur based gases in comparison to conventional fossil fuels. This paper is a review of the recent achievements of the cleaner intensification technologies for biodiesel production. Merits and limitations of the cleaner intensification technologies have been discussed. Mechanical stirring via transesterification is the most common and extensively utilised for biodiesel production. It involves the conversion of oil to glycerol and acid alkyl ester while employing methanol. However, this process has an inherent drawback of mass transfer resistance resulting in a lower reaction rate and higher production cost. Intensification technologies have become more attractive to overcome these aforementioned problems. In a bid to increase the cost competitiveness of biodiesel production compared to diesel fuel, process intensification has been studied for numerous biodiesel processing technologies. Many researchers have resorted to several intensification technologies namely; microwave, ultrasonic cavitation and hydrodynamic cavitation reactor to eliminate the mass transfer resistance of immiscible reactants. Once the mass transfer resistance is reduced, it may lead to a shorter reaction time and lower energy consumptions compared to mechanical stirring. Recent studies reveal that microwave and ultrasonic cavitation techniques are not yet completely feasible for biodiesel production at industrial scale. On the other hand, it was found that hydrodynamic cavitation offers a number of advantages over other intensification technologies. It shows good performance with respect to product yield, reaction time, energy consumption and product quality. Furthermore, it exhibits a sustainable mean for energy recovery from renewable oils. It was concluded that more studies are needed to extend the existing information of hydrodynamic cavitation for the design of the plate geometry with respect to the methyl ester conversion. This will help to develop a sustainable and industrially viable route for energy recovery from renewable oils. Yield efficiency in relation to the method followed the order: hydrodynamic cavitation > microwave > ultrasonic cavitation > mechanical stirring.},
  doi      = {10.1016/j.jclepro.2016.05.017},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1011757946},
}

@Article{Chen2012,
  author   = {Chien-Wei Chen and Yueyue Fan},
  journal  = {Transportation Research Part E Logistics and Transportation Review},
  title    = {Bioethanol supply chain system planning under supply and demand uncertainties},
  year     = {2012},
  number   = {1},
  pages    = {150-164},
  volume   = {48},
  abstract = {A mixed integer stochastic programming model is established to support strategic planning of bioenergy supply chain systems and optimal feedstock resource allocation in an uncertain decision environment. The two-stage stochastic programming model, together with a Lagrange relaxation based decomposition solution algorithm, was implemented in a real-world case study in California to explore the potential of waste-based bioethanol production. The model results show that biowaste-based ethanol can be a viable part of sustainable energy solution for the future.},
  doi      = {10.1016/j.tre.2011.08.004},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1003364215},
}

@Article{Suurs2009,
  author   = {Roald A.A. Suurs and Marko P. Hekkert},
  journal  = {Technological Forecasting and Social Change},
  title    = {Cumulative causation in the formation of a technological innovation system: The case of biofuels in the Netherlands},
  year     = {2009},
  note     = {https://dspace.library.uu.nl/bitstream/handle/1874/385308/Cumulative.pdf?sequence=1&isAllowed=y},
  number   = {8},
  pages    = {1003-1020},
  volume   = {76},
  abstract = {Despite its worldwide success, the innovation systems approach is often criticised for being theoretically underdeveloped. This paper aims to contribute to the conceptual and methodological basis of the (technological) innovation systems approach. We propose an alteration that improves the analysis of dynamics, especially with respect to emerging innovation systems. We do this by expanding on the technological innovation systems and system functions literature, and by employing the method of ‘event history analysis’. By mapping events, the interactions between system functions and their development over time can be analysed. Based on this it becomes possible to identify forms of positive feedback, i.e. cumulative causation. As an illustration of the approach, we assess the biofuels innovation system in The Netherlands as it evolved from 1990 to 2007.},
  doi      = {10.1016/j.techfore.2009.03.002},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1014664202},
}

@Article{Costantini2015,
  author   = {Valeria Costantini and Francesco Crespi and Chiara Martini and Luca Pennacchio},
  journal  = {Research Policy},
  title    = {Demand-pull and technology-push public support for eco-innovation: The case of the biofuels sector},
  year     = {2015},
  number   = {3},
  pages    = {577-595},
  volume   = {44},
  abstract = {The purpose of this paper is to explore the differentiated impact of demand-pull and technology-push policies in shaping technological patterns in the biofuels sector. The empirical analysis is based on a novel and original database (BioPat) containing patents in the field of biofuels selected using appropriate keywords and classified according to the technological content of the invention. Our results generally show that technological capabilities and environmental regulation spur innovative activities in the biofuels sector. Both demand-pull and technology-push factors are found to be important drivers of innovation in the biofuels sector. However, technology exploitation activities in first generation technologies are found to be mainly driven by quantity and price-based demand-pull policies. On the contrary, the pace of technology exploration efforts in advanced generation biofuels is shown to react positively to price-based demand-pull incentives but also to technology-push policy. The clear diversity in the impact of different public support instruments provides new insights which fuel discussion on the optimal policy mix debate and offers new elements for the design of future policy strategies.},
  doi      = {10.1016/j.respol.2014.12.011},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1019951181},
}

@Article{Liew2014,
  author   = {Weng Hui Liew and Mimi H. Hassim and Denny K.S. Ng},
  journal  = {Journal of Cleaner Production},
  title    = {Review of evolution, technology and sustainability assessments of biofuel production},
  year     = {2014},
  pages    = {11-29},
  volume   = {71},
  abstract = {Biofuel is recognised as an important renewable and sustainable energy source to substitute fossil fuel. To date, biofuel has been evolved from first to fourth generation and they are mainly differed in feedstock and production technologies. Along with the evolution, various studies on different sustainability criteria have been conducted. In this review paper, the state of the art of technologies and assessment methods on economic performance, safety, health and environment (SHE) as well as social impact for biofuel production are reviewed. In addition, this review paper also evaluates the typical methodological framework used for systematic assessment during early process design phase. As conclusion, to establish the highest sustainability in biofuel production, continuous research and development on all sustainability-related aspects are very much needed.},
  doi      = {10.1016/j.jclepro.2014.01.006},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1029572917},
}

@Article{CuellarBermudez2015,
  author   = {Sara P. Cuellar-Bermudez and Jonathan S. Garcia-Perez and Bruce E. Rittmann and Roberto Parra-Saldivar},
  journal  = {Journal of Cleaner Production},
  title    = {Photosynthetic bioenergy utilizing CO2: an approach on flue gases utilization for third generation biofuels},
  year     = {2015},
  note     = {https://doi.org/10.1016/j.jclepro.2014.03.034},
  pages    = {53-65},
  volume   = {98},
  abstract = {One of the most important industrial activities related to the greenhouse gases emissions is the cement manufacturing process, which produces large amounts of carbon dioxide (CO2). Only in 2010, 8% of CO2 global emissions were due to cement industry. In this work, the use of CO2 released by the cement sector is described as potential gas for microalgae culture since their biofixation efficiency is higher than terrestrial plants. Therefore, transformation of polluting gas fluxes into new and valuable products is feasible. In addition, bulk applications such as wastewater treatment and biofuels production can be coupled. Finally, microalgae biomass can be also used for the production of valuable compounds such as pigments, food supplements for both humans and animals, and fertilizers. In this review, flue gas emissions coupled to microalgae cultures are described. In addition, since microalgae can produce energy, the biorefinery concept is also reviewed.},
  doi      = {10.1016/j.jclepro.2014.03.034},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1024929365},
}

@Article{Huang2010,
  author   = {Yongxi Huang and Chien-Wei Chen and Yueyue Fan},
  journal  = {Transportation Research Part E Logistics and Transportation Review},
  title    = {Multistage optimization of the supply chains of biofuels},
  year     = {2010},
  number   = {6},
  pages    = {820-830},
  volume   = {46},
  abstract = {In this study, a mathematical model that integrates spatial and temporal dimensions is developed for strategic planning of future bioethanol supply chain systems. The planning objective is to minimize the cost of the entire supply chain of biofuel from biowaste feedstock fields to end users over the entire planning horizon, simultaneously satisfying demand, resource, and technology constraints. This model is used to evaluate the economic potential and infrastructure requirements for bioethanol production from eight waste biomass resources in California as a case study. It is found that, through careful supply chain design, biowaste-based ethanol production can be sustained at a compatible cost around $1.1 per gallon.},
  doi      = {10.1016/j.tre.2010.03.002},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1040677299},
}

@Article{Farooq2013,
  author   = {Muhammad Farooq and Anita Ramli and Duvvuri Subbarao},
  journal  = {Journal of Cleaner Production},
  title    = {Biodiesel production from waste cooking oil using bifunctional heterogeneous solid catalysts},
  year     = {2013},
  pages    = {131-140},
  volume   = {59},
  abstract = {In the present work, bifunctional heterogeneous catalysts were studied to develop an effective catalyst for biodiesel production from waste cooking oil with improved catalytic activity and stability. The catalysts were characterized by various analytical techniques to explore their physicochemical properties. The catalytic activity was evaluated in the transesterification of waste cooking oil for low cost biodiesel production. The bifunctional heterogeneous catalysts show improved transesterification activities. Among the different catalysts tested, the Mo–Mn/γ-Al2O3-15 wt% MgO catalyst provides the maximum biodiesel yield of 91.4% in reaction time of 4 h at reaction temperature of 100 °C, methanol to oil molar ratio of 27:1 and an agitation speed of 500 rpm. Moreover, the bifunctional heterogeneous catalyst shows substantial chemical stability and could be reused for at least eight times without major loss in its catalytic activity. The physicochemical properties of the biodiesel produced from waste cooking oil were further studied and compared with the ASTM and the EN biodiesel specifications. The results show that the properties of the biodiesel produced comply with the international standard specifications.},
  doi      = {10.1016/j.jclepro.2013.06.015},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1009832103},
}

@Article{Kasteren2007,
  author   = {J.M.N. van Kasteren and A.P. Nisworo},
  journal  = {Resources Conservation and Recycling},
  title    = {A process model to estimate the cost of industrial scale biodiesel production from waste cooking oil by supercritical transesterification},
  year     = {2007},
  note     = {https://pure.tue.nl/ws/files/3981831/38840600625960.pdf},
  number   = {4},
  pages    = {442-458},
  volume   = {50},
  abstract = {This paper describes the conceptual design of a production process in which waste cooking oil is converted via supercritical transesterification with methanol to methyl esters (biodiesel).Since waste cooking oil contains water and free fatty acids, supercritical transesterification offers great advantage to eliminate the pre-treatment capital and operating cost.A supercritical transesterification process for biodiesel continuous production from waste cooking oil has been studied for three plant capacities (125,000; 80,000 and 8000tonnes biodiesel/year). It can be concluded that biodiesel by supercritical transesterification can be scaled up resulting high purity of methyl esters (99.8%) and almost pure glycerol (96.4%) attained as by-product.The economic assessment of the biodiesel plant shows that biodiesel can be sold at US$ 0.17/l (125,000tonnes/year), US$ 0.24/l (80,000tonnes/year) and US$ 0.52/l for the smallest capacity (8000tonnes/year).The sensitive key factors for the economic feasibility of the plant are: raw material price, plant capacity, glycerol price and capital cost.Overall conclusion is that the process can compete with the existing alkali and acid catalyzed processes.Especially for the conversion of waste cooking oil to biodiesel, the supercritical process is an interesting technical and economical alternative.},
  doi      = {10.1016/j.resconrec.2006.07.005},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1034625256},
}

@Article{Raheem2018,
  author   = {Abdul Raheem and Pepijn Prinsen and Arun K. Vuppaladadiyam and Ming Zhao and Rafael Luque},
  journal  = {Journal of Cleaner Production},
  title    = {A review on sustainable microalgae based biofuel and bioenergy production: Recent developments},
  year     = {2018},
  pages    = {42-59},
  volume   = {181},
  abstract = {Climate changes induced by anthropogenic greenhouse gas emissions (mainly carbon dioxide) is one of the major threats of the modern era. Primary causes are the high reliance on fossil fuels for power generation, transportation, manufacturing and the intensive land usage (deforestation). The current share of renewable biofuel production in the overall fuel demand has been found insufficient to replace fossil fuels. Microalgae can deliver a sustainable and complementary biofuel platform with some important advantages. This review aims to offer a state-of-the art review of algal biomass conversion methods into various biofuel products, including biodiesel, syngas, biogas, bioethanol. Emerging more sustainable biofuel/bioenergy production technologies are highlighted. Attention is also paid to sustainable cultivation methods, including wastewater treatment and bioremediation to capture CO2 and fix nitrogen and phosphorus, produced from industrial, agricultural and municipal sources. Finally, a light is shed on the important role of algae metabolic engineering.},
  doi      = {10.1016/j.jclepro.2018.01.125},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1100466182},
}

@Article{Boerjesson2011,
  author   = {Pål Börjesson and Linda M. Tufvesson},
  journal  = {Journal of Cleaner Production},
  title    = {Agricultural crop-based biofuels – resource efficiency and environmental performance including direct land use changes},
  year     = {2011},
  number   = {2-3},
  pages    = {108-120},
  volume   = {19},
  abstract = {This paper analyses biofuels from agricultural crops in northern Europe regarding area and energy efficiency, greenhouse gases and eutrophication. The overall findings are that direct land use changes have a significant impact on GHG balances and eutrophication for all biofuels, the choice of calculation methods when by-products are included affecting the performance of food crop-based biofuels considerably, and the technical design of production systems may in specific cases be of major importance. The presented results are essential knowledge for the development of certification systems. Indirect land use changes are recognised but not included due to current scientific and methodological deficiencies.},
  doi      = {10.1016/j.jclepro.2010.01.001},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1038072171},
}

@Article{Cherubini2009,
  author   = {Francesco Cherubini and Neil D. Bird and Annette Cowie and Gerfried Jungmeier and Bernhard Schlamadinger and Susanne Woess-Gallasch},
  journal  = {Resources Conservation and Recycling},
  title    = {Energy- and greenhouse gas-based LCA of biofuel and bioenergy systems: Key issues, ranges and recommendations},
  year     = {2009},
  number   = {8},
  pages    = {434-447},
  volume   = {53},
  abstract = {With increasing use of biomass for energy, questions arise about the validity of bioenergy as a means to reduce greenhouse gas emissions and dependence on fossil fuels. Life Cycle Assessment (LCA) is a methodology able to reveal these environmental and energy performances, but results may differ even for apparently similar bioenergy systems. Differences are due to several reasons: type and management of raw materials, conversion technologies, end-use technologies, system boundaries and reference energy system with which the bioenergy chain is compared. Based on review of published papers and elaboration of software data concerning greenhouse gas and energy balances of bioenergy, other renewable and conventional fossil systems, this paper discusses key issues in bioenergy system LCA. These issues have a strong influence on the final results but are often overlooked or mishandled in most of the studies available in literature. The article addresses the following aspects: recognition of the biomass carbon cycle, including carbon stock changes in biomass and soil over time; inclusion of nitrous oxide and methane emissions from agricultural activities; selection of the appropriate fossil reference system; homogeneity of the input parameters in Life Cycle Inventories; influence of the allocation procedure when multiple products are involved; future trends in bioenergy (i.e. second-generation biofuels and biorefineries).Because many key issues are site-specific, and many factors affect the outcome, it is not possible to give exact values for the amount of greenhouse gas emissions and fossil energy consumption saved by a certain bioenergy product, because too many uncertainties are involved. For these reasons, the results are here provided as a means of wide ranges. Despite this wide range of results, it has been possible to draw some important conclusions and devise recommendations concerning the existing bioenergy systems, and some emerging implications about the future deployment and trends of bioenergy products are pointed out.},
  doi      = {10.1016/j.resconrec.2009.03.013},
  groups   = {Sustainable Biofuel Economy},
  url      = {https://app.dimensions.ai/details/publication/pub.1046534403},
}

@Article{Chang2010,
  author   = {Yanli Chang and Sheng-Tao Yang and Jia-Hui Liu and Erya Dong and Yanwen Wang and Aoneng Cao and Yuanfang Liu and Haifang Wang},
  journal  = {Toxicology Letters},
  title    = {In vitro toxicity evaluation of graphene oxide on A549 cells},
  year     = {2010},
  number   = {3},
  pages    = {201-210},
  volume   = {200},
  abstract = {Graphene and its derivatives have attracted great research interest for their potential applications in electronics, energy, materials and biomedical areas. However, little information of their toxicity and biocompatibility is available. Herein, we performed a comprehensive study on the toxicity of graphene oxide (GO) by examining the influences of GO on the morphology, viability, mortality and membrane integrity of A549 cells. The results suggest that GO does not enter A549 cell and has no obvious cytotoxicity. But GO can cause a dose-dependent oxidative stress in cell and induce a slight loss of cell viability at high concentration. These effects are dose and size related, and should be considered in the development of bio-applications of GO. Overall, GO is a pretty safe material at cellular level, which is confirmed by the favorable cell growth on GO film.},
  doi      = {10.1016/j.toxlet.2010.11.016},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1005629396},
}

@Article{Giri2005,
  author   = {Supratim Giri and Brian G. Trewyn and Michael P. Stellmaker and Victor S.‐Y. Lin},
  journal  = {Angewandte Chemie International Edition},
  title    = {Stimuli‐Responsive Controlled‐Release Delivery System Based on Mesoporous Silica Nanorods Capped with Magnetic Nanoparticles},
  year     = {2005},
  number   = {32},
  pages    = {5038-5044},
  volume   = {44},
  abstract = {Stand and deliver! The release of pore‐encapsulated fluorescein molecules from a core/shell mesoporous silica nanorod/superparamagnetic iron oxide nanoparticle carrier in the presence of an external magnetic field and only in response to cell‐produced antioxidants indicates promise for such systems in controlled‐release drug delivery.},
  doi      = {10.1002/anie.200501819},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1031075757},
}

@Article{Paciotti2004,
  author   = {Giulio F. Paciotti and Lonnie Myer and David Weinreich and Dan Goia and Nicolae Pavel and Richard E. McLaughlin and Lawrence Tamarkin},
  journal  = {Drug Delivery},
  title    = {Colloidal Gold: A Novel Nanoparticle Vector for Tumor Directed Drug Delivery},
  year     = {2004},
  number   = {3},
  pages    = {169-183},
  volume   = {11},
  abstract = {Colloidal gold, a sol comprised of nanoparticles of Au(0), has been used as a therapeutic for the treatment of cancer as well as an indicator for immunodiagnostics. However, the use of these gold nanoparticles for in vivo drug delivery has never been described. This communication outlines the development of a colloidal gold (cAu) nanoparticle vector that targets the delivery of tumor necrosis factor (TNF) to a solid tumor growing in mice. The optimal vector, designated PT-cAu-TNF, consists of molecules of thiol-derivatized PEG (PT) and recombinant human TNF that are directly bound onto the surface of the gold nanoparticles. Following intravenous administration, PT-cAu-TNF rapidly accumulates in MC-38 colon carcinoma tumors and shows little to no accumulation in the livers, spleens (i.e., the RES) or other healthy organs of the animals. The tumor accumulation was evidenced by a marked change in the color of the tumor as it acquired the bright red/purple color of the colloidal gold sol and was coincident with the active and tumor-specific sequestration of TNF. Finally, PT-cAu-TNF was less toxic and more effective in reducing tumor burden than native TNF since maximal antitumor responses were achieved at lower doses of drug.},
  doi      = {10.1080/10717540490433895},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1038778560},
}

@Article{Singh2006,
  author   = {Ravi Singh and Davide Pantarotto and Lara Lacerda and Giorgia Pastorin and Cédric Klumpp and Maurizio Prato and Alberto Bianco and Kostas Kostarelos},
  journal  = {Proceedings of the National Academy of Sciences of the United States of America},
  title    = {Tissue biodistribution and blood clearance rates of intravenously administered carbon nanotube radiotracers},
  year     = {2006},
  note     = {https://europepmc.org/articles/pmc1413890?pdf=render},
  number   = {9},
  pages    = {3357-3362},
  volume   = {103},
  abstract = {Carbon nanotubes (CNT) are intensively being developed for biomedical applications including drug and gene delivery. Although all possible clinical applications will require compatibility of CNT with the biological milieu, their in vivo capabilities and limitations have not yet been explored. In this work, water-soluble, single-walled CNT (SWNT) have been functionalized with the chelating molecule diethylentriaminepentaacetic (DTPA) and labeled with indium ((111)In) for imaging purposes. Intravenous (i.v.) administration of these functionalized SWNT (f-SWNT) followed by radioactivity tracing using gamma scintigraphy indicated that f-SWNT are not retained in any of the reticuloendothelial system organs (liver or spleen) and are rapidly cleared from systemic blood circulation through the renal excretion route. The observed rapid blood clearance and half-life (3 h) of f-SWNT has major implications for all potential clinical uses of CNT. Moreover, urine excretion studies using both f-SWNT and functionalized multiwalled CNT followed by electron microscopy analysis of urine samples revealed that both types of nanotubes were excreted as intact nanotubes. This work describes the pharmacokinetic parameters of i.v. administered functionalized CNT relevant for various therapeutic and diagnostic applications.},
  doi      = {10.1073/pnas.0509009103},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1035387745},
}

@Article{Liu2008,
  author   = {Zhuang Liu and Corrine Davis and Weibo Cai and Lina He and Xiaoyuan Chen and Hongjie Dai},
  journal  = {Proceedings of the National Academy of Sciences of the United States of America},
  title    = {Circulation and long-term fate of functionalized, biocompatible single-walled carbon nanotubes in mice probed by Raman spectroscopy},
  year     = {2008},
  note     = {https://europepmc.org/articles/pmc2234157?pdf=render},
  number   = {5},
  pages    = {1410-1415},
  volume   = {105},
  abstract = {Carbon nanotubes are promising new materials for molecular delivery in biological systems. The long-term fate of nanotubes intravenously injected into animals in vivo is currently unknown, an issue critical to potential clinical applications of these materials. Here, using the intrinsic Raman spectroscopic signatures of single-walled carbon nanotubes (SWNTs), we measured the blood circulation of intravenously injected SWNTs and detect SWNTs in various organs and tissues of mice ex vivo over a period of three months. Functionalization of SWNTs by branched polyethylene-glycol (PEG) chains was developed, enabling thus far the longest SWNT blood circulation up to 1 day, relatively low uptake in the reticuloendothelial system (RES), and near-complete clearance from the main organs in approximately 2 months. Raman spectroscopy detected SWNT in the intestine, feces, kidney, and bladder of mice, suggesting excretion and clearance of SWNTs from mice via the biliary and renal pathways. No toxic side effect of SWNTs to mice was observed in necropsy, histology, and blood chemistry measurements. These findings pave the way to future biomedical applications of carbon nanotubes.},
  doi      = {10.1073/pnas.0707654105},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1046826509},
}

@Article{Idris2012,
  author   = {Niagara Muhammad Idris and Muthu Kumara Gnanasammandhan and Jing Zhang and Paul C Ho and Ratha Mahendran and Yong Zhang},
  journal  = {Nature Medicine},
  title    = {In vivo photodynamic therapy using upconversion nanoparticles as remote-controlled nanotransducers},
  year     = {2012},
  number   = {10},
  pages    = {1580-1585},
  volume   = {18},
  abstract = {A limitation of photodynamic therapy (PDT) is the depth of penetration of visible light needed for activation of the photosensitizers, restricting treatment to tumors on or just under the skin’s surface or those lining internal organs and cavities. Niagara Muhammad Idris and colleagues have addressed this issue by developing upconversion fluorescent nanoparticles (UNCs) that convert deeper penetrating near-infrared light to visible wavelengths without sacrificing efficacy for singlet oxygen (1O2) production. The group tested the UNCs in vivo in a subcutaneous mouse tumor model using a dual-sensitizer approach for greater PDT efficacy.},
  doi      = {10.1038/nm.2933},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1036002565},
}

@Article{Salvati2013,
  author   = {Anna Salvati and Andrzej S. Pitek and Marco P. Monopoli and Kanlaya Prapainop and Francesca Baldelli Bombelli and Delyan R. Hristov and Philip M. Kelly and Christoffer Åberg and Eugene Mahon and Kenneth A. Dawson},
  journal  = {Nature Nanotechnology},
  title    = {Transferrin-functionalized nanoparticles lose their targeting capabilities when a biomolecule corona adsorbs on the surface},
  year     = {2013},
  number   = {2},
  pages    = {137-143},
  volume   = {8},
  abstract = {Nanoparticles have been proposed as carriers for drugs, genes and therapies to treat various diseases1,2. Many strategies have been developed to target nanomaterials to specific or over-expressed receptors in diseased cells, and these typically involve functionalizing the surface of nanoparticles with proteins, antibodies or other biomolecules. Here, we show that the targeting ability of such functionalized nanoparticles may disappear when they are placed in a biological environment. Using transferrin-conjugated nanoparticles, we found that proteins in the media can shield transferrin from binding to both its targeted receptors on cells and soluble transferrin receptors. Although nanoparticles continue to enter cells, the targeting specificity of transferrin is lost. Our results suggest that when nanoparticles are placed in a complex biological environment, interaction with other proteins in the medium and the formation of a protein corona3,4 can ‘screen’ the targeting molecules on the surface of nanoparticles and cause loss of specificity in targeting.},
  doi      = {10.1038/nnano.2012.237},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1048709708},
}

@Article{Lu1999,
  author   = {Yunfeng Lu and Hongyou Fan and Aaron Stump and Timothy L. Ward and Thomas Rieker and C. Jeffrey Brinker},
  journal  = {Nature},
  title    = {Aerosol-assisted self-assembly of mesostructured spherical nanoparticles},
  year     = {1999},
  number   = {6724},
  pages    = {223-226},
  volume   = {398},
  abstract = {Particles possessing nanometre-scale pores of well-defined size and connectivity are of interest for catalysis, chromatography and controlled release of drugs, and as fillers with low dielectric constant, pigments and hosts for optically active compounds1,2. Silica containing ordered mesopores (of nanometre-scale width) can be prepared by templating of surfactant3,4 and block copolymer5 liquid-crystalline mesophases, and interfacial phenomena have been used to control the macroscopic form of these materials, providing mesoporous particles1,6, fibres7,8 and films9,10. A variety of spherical or nearly spherical particles has been reported1,6,7,11,12,13, but the degree of ordering and the range of the porous mesostructures have been limited. Here we report a rapid, aerosol-based14,15,16 process for synthesizing solid, well-ordered spherical particles with stable pore mesostructures of hexagonal and cubic topology, as well as layered (vesicular) structures. Our method relies on evaporation-induced interfacial self-assembly17 confined to spherical aerosol droplets. This simple, generalizable process can be modified for the formation of ordered mesostructured thin films.},
  doi      = {10.1038/18410},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1050276749},
}

@Article{Niidome2006,
  author   = {Takuro Niidome and Masato Yamagata and Yuri Okamoto and Yasuyuki Akiyama and Hironobu Takahashi and Takahito Kawano and Yoshiki Katayama and Yasuro Niidome},
  journal  = {Journal of Controlled Release},
  title    = {PEG-modified gold nanorods with a stealth character for in vivo applications},
  year     = {2006},
  number   = {3},
  pages    = {343-347},
  volume   = {114},
  abstract = {Gold nanorods prepared in hexadecyltrimethylammonium bromide (CTAB) solution are expected to provide novel materials for photothermal therapy and photo-controlled drug delivery systems. Since gold nanorods stabilized with CTAB show strong cytotoxicity, we developed a technique to modify these with polyethyleneglycol (PEG) for medical applications. PEG-modification was achieved by adding mPEG-SH in the CTAB solution, then, excess CTAB was removed by dialysis. PEG-modified gold nanoparticles showed a nearly neutral surface, and had little cytotoxicity in vitro. Following intravenous injection into mice, 54% of injected PEG-modified gold nanoparticles were found in blood at 0.5 h after intravenous injection, whereas most of gold was detected in the liver in the case of original gold nanorods stabilized with CTAB.},
  doi      = {10.1016/j.jconrel.2006.06.017},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1021666873},
}

@Article{Jong2008,
  author   = {Wim H. De Jong and Werner I. Hagens and Petra Krystek and Marina C. Burger and Adriënne J.A.M. Sips and Robert E. Geertsma},
  journal  = {Biomaterials},
  title    = {Particle size-dependent organ distribution of gold nanoparticles after intravenous administration},
  year     = {2008},
  number   = {12},
  pages    = {1912-1919},
  volume   = {29},
  abstract = {A kinetic study was performed to determine the influence of particle size on the in vivo tissue distribution of spherical-shaped gold nanoparticles in the rat. Gold nanoparticles were chosen as model substances as they are used in several medical applications. In addition, the detection of the presence of gold is feasible with no background levels in the body in the normal situation. Rats were intravenously injected in the tail vein with gold nanoparticles with a diameter of 10, 50, 100 and 250 nm, respectively. After 24 h, the rats were sacrificed and blood and various organs were collected for gold determination. The presence of gold was measured quantitatively with inductively coupled plasma mass spectrometry (ICP-MS). For all gold nanoparticle sizes the majority of the gold was demonstrated to be present in liver and spleen. A clear difference was observed between the distribution of the 10 nm particles and the larger particles. The 10 nm particles were present in various organ systems including blood, liver, spleen, kidney, testis, thymus, heart, lung and brain, whereas the larger particles were only detected in blood, liver and spleen. The results demonstrate that tissue distribution of gold nanoparticles is size-dependent with the smallest 10nm nanoparticles showing the most widespread organ distribution.},
  doi      = {10.1016/j.biomaterials.2007.12.037},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1028529625},
}

@Article{Cheng2006,
  author   = {Jianjun Cheng and Benjamin A. Teply and Ines Sherifi and Josephine Sung and Gaurav Luther and Frank X. Gu and Etgar Levy-Nissenbaum and Aleksandar F. Radovic-Moreno and Robert Langer and Omid C. Farokhzad},
  journal  = {Biomaterials},
  title    = {Formulation of functionalized PLGA–PEG nanoparticles for in vivo targeted drug delivery},
  year     = {2006},
  note     = {https://europepmc.org/articles/pmc2925222?pdf=render},
  number   = {5},
  pages    = {869-876},
  volume   = {28},
  abstract = {Nanoparticle (NP) size has been shown to significantly affect the biodistribution of targeted and non-targeted NPs in an organ specific manner. Herein we have developed NPs from carboxy-terminated poly(d,L-lactide-co-glycolide)-block-poly(ethylene glycol) (PLGA-b-PEG-COOH) polymer and studied the effects of altering the following formulation parameters on the size of NPs: (1) polymer concentration, (2) drug loading, (3) water miscibility of solvent, and (4) the ratio of water to solvent. We found that NP mean volumetric size correlates linearly with polymer concentration for NPs between 70 and 250 nm in diameter (linear coefficient=0.99 for NPs formulated with solvents studied). NPs with desirable size, drug loading, and polydispersity were conjugated to the A10 RNA aptamer (Apt) that binds to the prostate specific membrane antigen (PSMA), and NP and NP-Apt biodistribution was evaluated in a LNCaP (PSMA+) xenograft mouse model of prostate cancer. The surface functionalization of NPs with the A10 PSMA Apt significantly enhanced delivery of NPs to tumors vs. equivalent NPs lacking the A10 PSMA Apt (a 3.77-fold increase at 24h; NP-Apt 0.83%+/-0.21% vs. NP 0.22%+/-0.07% of injected dose per gram of tissue; mean+/-SD, n=4, p=0.002). The ability to control NP size together with targeted delivery may result in favorable biodistribution and development of clinically relevant targeted therapies.},
  doi      = {10.1016/j.biomaterials.2006.09.047},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1001343412},
}

@Article{Roy1999,
  author   = {Krishnendu Roy and Hai-Quan Mao and Shau -Ku Huang and Kam W. Leong},
  journal  = {Nature Medicine},
  title    = {Oral gene delivery with chitosan–DNA nanoparticles generates immunologic protection in a murine model of peanut allergy},
  year     = {1999},
  number   = {4},
  pages    = {387-391},
  volume   = {5},
  abstract = {Food allergy is a common and often fatal disease with no effective treatment. We describe here a new immunoprophylactic strategy using oral allergen-gene immunization to modulate peanut antigen-induced murine anaphylactic responses. Oral administration of DNA nanoparticles synthesized by complexing plasmid DNA with chitosan, a natural biocompatible polysaccharide, resulted in transduced gene expression in the intestinal epithelium. Mice receiving nanoparticles containing a dominant peanut allergen gene (pCMVArah2) produced secretory IgA and serum IgG2a. Compared with non-immunized mice or mice treated with 'naked' DNA, mice immunized with nanoparticles showed a substantial reduction in allergen-induced anaphylaxis associated with reduced levels of IgE, plasma histamine and vascular leakage. These results demonstrate that oral allergen-gene immunization with chitosan–DNA nanoparticles is effective in modulating murine anaphylactic responses, and indicate its prophylactic utility in treating food allergy.},
  doi      = {10.1038/7385},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1017225012},
}

@Article{Ballou2003,
  author   = {Byron Ballou and B. Christoffer Lagerholm and Lauren A. Ernst and Marcel P. Bruchez and Alan S. Waggoner},
  journal  = {Bioconjugate Chemistry},
  title    = {Noninvasive Imaging of Quantum Dots in Mice},
  year     = {2003},
  number   = {1},
  pages    = {79-86},
  volume   = {15},
  abstract = {Quantum dots having four different surface coatings were tested for use in in vivo imaging. Localization was successfully monitored by fluorescence imaging of living animals, by necropsy, by frozen tissue sections for optical microscopy, and by electron microscopy, on scales ranging from centimeters to nanometers, using only quantum dots for detection. Circulating half-lives were found to be less than 12 min for amphiphilic poly(acrylic acid), short-chain (750 Da) methoxy-PEG or long-chain (3400 Da) carboxy-PEG quantum dots, but approximately 70 min for long-chain (5000 Da) methoxy-PEG quantum dots. Surface coatings also determined the in vivo localization of the quantum dots. Long-term experiments demonstrated that these quantum dots remain fluorescent after at least four months in vivo.},
  doi      = {10.1021/bc034153y},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1020647724},
}

@Article{Hainfeld2006,
  author   = {J F Hainfeld and D N Slatkin and T M Focella and H M Smilowitz},
  journal  = {British Journal of Radiology},
  title    = {Gold nanoparticles: a new X-ray contrast agent},
  year     = {2006},
  number   = {939},
  pages    = {248-253},
  volume   = {79},
  abstract = {There have been few fundamental improvements in clinical X-ray contrast agents in more than 25 years, and the chemical platform of tri-iodobenzene has not changed. Current agents impose serious limitations on medical imaging: short imaging times, the need for catheterization in many cases, occasional renal toxicity, and poor contrast in large patients. This report is the first demonstration that gold nanoparticles may overcome these limitations. Gold has higher absorption than iodine with less bone and tissue interference achieving better contrast with lower X-ray dose. Nanoparticles clear the blood more slowly than iodine agents, permitting longer imaging times. Gold nanoparticles, 1.9 nm in diameter, were injected intravenously into mice and images recorded over time with a standard mammography unit. Gold biodistribution was measured by atomic absorption. Retention in liver and spleen was low with elimination by the kidneys. Organs such as kidneys and tumours were seen with unusual clarity and high spatial resolution. Blood vessels less than 100 microm in diameter were delineated, thus enabling in vivo vascular casting. Regions of increased vascularization and angiogenesis could be distinguished. With 10 mg Au ml(-1) initially in the blood, mouse behaviour was unremarkable and neither blood plasma analytes nor organ histology revealed any evidence of toxicity 11 days and 30 days after injection. Gold nanoparticles can be used as X-ray contrast agents with properties that overcome some significant limitations of iodine-based agents.},
  doi      = {10.1259/bjr/13169882},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1064567937},
}

@Article{Perrault2009,
  author   = {Steven D. Perrault and Carl Walkey and Travis Jennings and Hans C. Fischer and Warren C. W. Chan},
  journal  = {Nano Letters},
  title    = {Mediating Tumor Targeting Efficiency of Nanoparticles Through Design},
  year     = {2009},
  number   = {5},
  pages    = {1909-1915},
  volume   = {9},
  abstract = {Here we systematically examined the effect of nanoparticle size (10-100 nm) and surface chemistry (i.e., poly(ethylene glycol)) on passive targeting of tumors in vivo. We found that the physical and chemical properties of the nanoparticles influenced their pharmacokinetic behavior, which ultimately determined their tumor accumulation capacity. Interestingly, the permeation of nanoparticles within the tumor is highly dependent on the overall size of the nanoparticle, where larger nanoparticles appear to stay near the vasculature while smaller nanoparticles rapidly diffuse throughout the tumor matrix. Our results provide design parameters for engineering nanoparticles for optimized tumor targeting of contrast agents and therapeutics.},
  doi      = {10.1021/nl900031y},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1056221800},
}

@Article{Walkey2012,
  author   = {Carl D. Walkey and Jonathan B. Olsen and Hongbo Guo and Andrew Emili and Warren C. W. Chan},
  journal  = {Journal of the American Chemical Society},
  title    = {Nanoparticle Size and Surface Chemistry Determine Serum Protein Adsorption and Macrophage Uptake},
  year     = {2012},
  number   = {4},
  pages    = {2139-2147},
  volume   = {134},
  abstract = {Delivery and toxicity are critical issues facing nanomedicine research. Currently, there is limited understanding and connection between the physicochemical properties of a nanomaterial and its interactions with a physiological system. As a result, it remains unclear how to optimally synthesize and chemically modify nanomaterials for in vivo applications. It has been suggested that the physicochemical properties of a nanomaterial after synthesis, known as its "synthetic identity", are not what a cell encounters in vivo. Adsorption of blood components and interactions with phagocytes can modify the size, aggregation state, and interfacial composition of a nanomaterial, giving it a distinct "biological identity". Here, we investigate the role of size and surface chemistry in mediating serum protein adsorption to gold nanoparticles and their subsequent uptake by macrophages. Using label-free liquid chromatography tandem mass spectrometry, we find that over 70 different serum proteins are heterogeneously adsorbed to the surface of gold nanoparticles. The relative density of each of these adsorbed proteins depends on nanoparticle size and poly(ethylene glycol) grafting density. Variations in serum protein adsorption correlate with differences in the mechanism and efficiency of nanoparticle uptake by a macrophage cell line. Macrophages contribute to the poor efficiency of nanomaterial delivery into diseased tissues, redistribution of nanomaterials within the body, and potential toxicity. This study establishes principles for the rational design of clinically useful nanomaterials.},
  doi      = {10.1021/ja2084338},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1041771334},
}

@Article{Kim2008a,
  author   = {Jaeyun Kim and Hoe Suk Kim and Nohyun Lee and Taeho Kim and Hyoungsu Kim and Taekyung Yu and In Chan Song and Woo Kyung Moon and Taeghwan Hyeon},
  journal  = {Angewandte Chemie International Edition},
  title    = {Multifunctional Uniform Nanoparticles Composed of a Magnetite Nanocrystal Core and a Mesoporous Silica Shell for Magnetic Resonance and Fluorescence Imaging and for Drug Delivery},
  year     = {2008},
  number   = {44},
  pages    = {8438-8441},
  volume   = {47},
  abstract = {Magnetic, fluorescent core–shell nanoparticles consist of a single Fe3O4 nanocrystal core and a dye‐doped mesoporous silica shell with a poly(ethylene glycol) coating (see picture of TEM images and schematic depictions). These nanoparticles can be used as magnetic resonance and fluorescence imaging agents, and as drug delivery vehicles, thus making them novel candidates for simultaneous cancer diagnosis and therapy.},
  doi      = {10.1002/anie.200802469},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1015833771},
}

@Article{Weir2012,
  author   = {Alex Weir and Paul Westerhoff and Lars Fabricius and Kiril Hristovski and Natalie von Goetz},
  journal  = {Environmental Science and Technology},
  title    = {Titanium Dioxide Nanoparticles in Food and Personal Care Products},
  year     = {2012},
  note     = {https://europepmc.org/articles/pmc3288463?pdf=render},
  number   = {4},
  pages    = {2242-2250},
  volume   = {46},
  abstract = {Titanium dioxide is a common additive in many food, personal care, and other consumer products used by people, which after use can enter the sewage system and, subsequently, enter the environment as treated effluent discharged to surface waters or biosolids applied to agricultural land, incinerated wastes, or landfill solids. This study quantifies the amount of titanium in common food products, derives estimates of human exposure to dietary (nano-) TiO(2), and discusses the impact of the nanoscale fraction of TiO(2) entering the environment. The foods with the highest content of TiO(2) included candies, sweets, and chewing gums. Among personal care products, toothpastes and select sunscreens contained 1% to >10% titanium by weight. While some other crèmes contained titanium, despite being colored white, most shampoos, deodorants, and shaving creams contained the lowest levels of titanium (<0.01 μg/mg). For several high-consumption pharmaceuticals, the titanium content ranged from below the instrument detection limit (0.0001 μg Ti/mg) to a high of 0.014 μg Ti/mg. Electron microscopy and stability testing of food-grade TiO(2) (E171) suggests that approximately 36% of the particles are less than 100 nm in at least one dimension and that it readily disperses in water as fairly stable colloids. However, filtration of water solubilized consumer products and personal care products indicated that less than 5% of the titanium was able to pass through 0.45 or 0.7 μm pores. Two white paints contained 110 μg Ti/mg while three sealants (i.e., prime coat paint) contained less titanium (25 to 40 μg Ti/mg). This research showed that, while many white-colored products contained titanium, it was not a prerequisite. Although several of these product classes contained low amounts of titanium, their widespread use and disposal down the drain and eventually to wastewater treatment plants (WWTPs) deserves attention. A Monte Carlo human exposure analysis to TiO(2) through foods identified children as having the highest exposures because TiO(2) content of sweets is higher than other food products and that a typical exposure for a US adult may be on the order of 1 mg Ti per kilogram body weight per day. Thus, because of the millions of tons of titanium-based white pigment used annually, testing should focus on food-grade TiO(2) (E171) rather than that adopted in many environmental health and safety tests (i.e., P25), which is used in much lower amounts in products less likely to enter the environment (e.g., catalyst supports, photocatalytic coatings).},
  doi      = {10.1021/es204168d},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1055504074},
}

@Article{Savic2003,
  author   = {Radoslav Savić and Laibin Luo and Adi Eisenberg and Dusica Maysinger},
  journal  = {Science},
  title    = {Micellar Nanocontainers Distribute to Defined Cytoplasmic Organelles},
  year     = {2003},
  number   = {5619},
  pages    = {615-618},
  volume   = {300},
  abstract = {Block copolymer micelles are water-soluble biocompatible nanocontainers with great potential for delivering hydrophobic drugs. An understanding of their cellular distribution is essential to achieving selective delivery of drugs at the subcellular level. Triple-labeling confocal microscopy in live cells revealed the localization of micelles in several cytoplasmic organelles, including mitochondria, but not in the nucleus. Moreover, micelles change the cellular distribution of and increase the amount of the agent delivered to the cells. These micelles may thus be worth exploring for their potential to selectively deliver drugs to specified subcellular targets.},
  doi      = {10.1126/science.1078192},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1062447105},
}

@Article{Win2005,
  author   = {Khin Yin Win and Si-Shen Feng},
  journal  = {Biomaterials},
  title    = {Effects of particle size and surface coating on cellular uptake of polymeric nanoparticles for oral delivery of anticancer drugs},
  year     = {2005},
  number   = {15},
  pages    = {2713-2722},
  volume   = {26},
  abstract = {This study evaluated cellular uptake of polymeric nanoparticles by using Caco-2 cells, a human colon adenocarcinoma cell line, as an in vitro model with the aim to apply nanoparticles of biodegradable polymers for oral chemotherapy. The feasibility was demonstrated by showing the localization and quantification of the cell uptake of fluorescent polystyrene nanoparticles of standard size and poly(lactic-co-glycolic acid) (PLGA) nanoparticles coated with polyvinyl alcohol (PVA) or vitamin E TPGS. Coumarin-6 loaded PLGA nanoparticles were prepared by a modified solvent extraction/evaporation method and characterized by laser light scattering for size and size distribution, scanning electron microscopy (SEM) for surface morphology, zeta-potential for surface charge, and spectrofluorometry for fluorescent molecule release from the nanoparticles. The effects of particle size and particle surface coating on the cellular uptake of the nanoparticles were quantified by spectrofluorometric measurement. Cellular uptake of vitamin E TPGS-coated PLGA nanoparticles showed 1.4 folds higher than that of PVA-coated PLGA nanoparticles and 4-6 folds higher than that of nude polystyrene nanoparticles. Images of confocal laser scanning microscopy, cryo-SEM and transmission electron microscopy clearly evidenced the internalization of nanoparticles by the Caco-2 cells, showing that surface modification of PLGA nanoparticles with vitamin E TPGS notably improved the cellular uptake. It is highly feasible for nanoparticles of biodegradable polymers to be applied to promote oral chemotherapy.},
  doi      = {10.1016/j.biomaterials.2004.07.050},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1047132329},
}

@Article{Nasongkla2006,
  author   = {Norased Nasongkla and Erik Bey and Jimin Ren and Hua Ai and Chalermchai Khemtong and Jagadeesh Setti Guthi and Shook-Fong Chin and A. Dean Sherry and David A. Boothman and Jinming Gao},
  journal  = {Nano Letters},
  title    = {Multifunctional Polymeric Micelles as Cancer-Targeted, MRI-Ultrasensitive Drug Delivery Systems},
  year     = {2006},
  number   = {11},
  pages    = {2427-2430},
  volume   = {6},
  abstract = {We describe the development of multifunctional polymeric micelles with cancer-targeting capability via alpha(v)beta(3) integrins, controlled drug delivery, and efficient magnetic resonance imaging (MRI) contrast characteristics. Doxorubicin and a cluster of superparamagnetic iron oxide (SPIO) nanoparticles were loaded successfully inside the micelle core. The presence of cRGD on the micelle surface resulted in the cancer-targeted delivery to alpha(v)beta(3)-expressing tumor cells. In vitro MRI and cytotoxicity studies demonstrated the ultrasensitive MRI imaging and alpha(v)beta(3)-specific cytotoxic response of these multifunctional polymeric micelles.},
  doi      = {10.1021/nl061412u},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1039641194},
}

@Article{Oberdoerster2004,
  author   = {Eva Oberdörster},
  journal  = {Environmental Health Perspectives},
  title    = {Manufactured Nanomaterials (Fullerenes, C60) Induce Oxidative Stress in the Brain of Juvenile Largemouth Bass},
  year     = {2004},
  note     = {https://doi.org/10.1289/ehp.7021},
  number   = {10},
  pages    = {1058-1062},
  volume   = {112},
  abstract = {Although nanotechnology has vast potential in uses such as fuel cells, microreactors, drug delivery devices, and personal care products, it is prudent to determine possible toxicity of nanotechnology-derived products before widespread use. It is likely that nanomaterials can affect wildlife if they are accidentally released into the environment. The fullerenes are one type of manufactured nanoparticle that is being produced by tons each year, and initially uncoated fullerenes can be modified with biocompatible coatings. Fullerenes are lipophilic and localize into lipid-rich regions such as cell membranes in vitro, and they are redox active. Other nano-sized particles and soluble metals have been shown to selectively translocate into the brain via the olfactory bulb in mammals and fish. Fullerenes (C60) can form aqueous suspended colloids (nC60); the question arises of whether a redox-active, lipophilic molecule could cause oxidative damage in an aquatic species. The goal of this study was to investigate oxyradical-induced lipid and protein damage, as well as impacts on total glutathione (GSH) levels, in largemouth bass exposed to nC60. Significant lipid peroxidation was found in brains of largemouth bass after 48 hr of exposure to 0.5 ppm uncoated nC60. GSH was also marginally depleted in gills of fish, and nC60 increased water clarity, possibly due to bactericidal activity. This is the first study showing that uncoated fullerenes can cause oxidative damage and depletion of GSH in vivo in an aquatic species. Further research needs to be done to evaluate the potential toxicity of manufactured nanomaterials, especially with respect to translocation into the brain.},
  doi      = {10.1289/ehp.7021},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1034007004},
}

@Article{Monopoli2011,
  author   = {Marco P. Monopoli and Dorota Walczyk and Abigail Campbell and Giuliano Elia and Iseult Lynch and Francesca Baldelli Bombelli and Kenneth A. Dawson},
  journal  = {Journal of the American Chemical Society},
  title    = {Physical−Chemical Aspects of Protein Corona: Relevance to in Vitro and in Vivo Biological Impacts of Nanoparticles},
  year     = {2011},
  number   = {8},
  pages    = {2525-2534},
  volume   = {133},
  abstract = {It is now clearly emerging that besides size and shape, the other primary defining element of nanoscale objects in biological media is their long-lived protein ("hard") corona. This corona may be expressed as a durable, stabilizing coating of the bare surface of nanoparticle (NP) monomers, or it may be reflected in different subpopulations of particle assemblies, each presenting a durable protein coating. Using the approach and concepts of physical chemistry, we relate studies on the composition of the protein corona at different plasma concentrations with structural data on the complexes both in situ and free from excess plasma. This enables a high degree of confidence in the meaning of the hard protein corona in a biological context. Here, we present the protein adsorption for two compositionally different NPs, namely sulfonated polystyrene and silica NPs. NP-protein complexes are characterized by differential centrifugal sedimentation, dynamic light scattering, and zeta-potential both in situ and once isolated from plasma as a function of the protein/NP surface area ratio. We then introduce a semiquantitative determination of their hard corona composition using one-dimensional sodium dodecyl sulfate-polyacrylamide gel electrophoresis and electrospray liquid chromatography mass spectrometry, which allows us to follow the total binding isotherms for the particles, identifying simultaneously the nature and amount of the most relevant proteins as a function of the plasma concentration. We find that the hard corona can evolve quite significantly as one passes from protein concentrations appropriate to in vitro cell studies to those present in in vivo studies, which has deep implications for in vitro-in vivo extrapolations and will require some consideration in the future.},
  doi      = {10.1021/ja107583h},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1008013568},
}

@Article{Stoimenov2002,
  author   = {Peter K. Stoimenov and Rosalyn L. Klinger and George L. Marchin and Kenneth J. Klabunde},
  journal  = {Langmuir},
  title    = {Metal Oxide Nanoparticles as Bactericidal Agents},
  year     = {2002},
  number   = {17},
  pages    = {6679-6686},
  volume   = {18},
  abstract = {Reactive magnesium oxide nanoparticles and halogen (Cl2, Br2) adducts of these MgO particles were allowed to contact certain bacteria and spore cells. Bacteriological test data, atomic force microscopy (AFM) images, and electron microscopy (TEM) images are provided, which yield insight into the biocidal action of these nanoscale materials. The tests show that these materials are very effective against Gram-positive and Gram-negative bacteria as well as spores. ζ-Potential measurements show an attractive interaction between the MgO nanoparticles and bacteria and spore cells, which is confirmed by confocal microscopy images. The AFM studies illustrate considerable changes in the cell membranes upon treatment, resulting in the death of the cells. TEM micrographs confirm these results and supply additional information about the processes inside the cells. Overall, the results presented illustrate that dry powder nanoparticulate formulations as well as water slurries are effective. It is proposed that abrasiveness, basic character, electrostatic attraction, and oxidizing power (due to the presence of active halogen) combine to promote these biocidal properties.},
  doi      = {10.1021/la0202374},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1056143784},
}

@Article{Shukla2005,
  author   = {Ravi Shukla and Vipul Bansal and Minakshi Chaudhary and Atanu Basu and Ramesh R. Bhonde and Murali Sastry},
  journal  = {Langmuir},
  title    = {Biocompatibility of Gold Nanoparticles and Their Endocytotic Fate Inside the Cellular Compartment: A Microscopic Overview},
  year     = {2005},
  number   = {23},
  pages    = {10644-10654},
  volume   = {21},
  abstract = {Macrophages are one of the principal immune effector cells that play essential roles as secretory, phagocytic, and antigen-presenting cells in the immune system. In this study, we address the issue of cytotoxicity and immunogenic effects of gold nanoparticles on RAW264.7 macrophage cells. The cytotoxicity of gold nanoparticles has been correlated with a detailed study of their endocytotic uptake using various microscopy tools such as atomic force microscopy (AFM), confocal-laser-scanning microscopy (CFLSM), and transmission electron microscopy (TEM). Our findings suggest that Au(0) nanoparticles are not cytotoxic, reduce the production of reactive oxygen and nitrite species, and do not elicit secretion of proinflammatory cytokines TNF-alpha and IL1-beta, making them suitable candidates for nanomedicine. AFM measurements suggest that gold nanoparticles are internalized inside the cell via a mechanism involving pinocytosis, while CFLSM and TEM studies indicate their internalization in lysosomal bodies arranged in perinuclear fashion. Our studies thus underline the noncytotoxic, nonimmunogenic, and biocompatible properties of gold nanoparticles with the potential for application in nanoimmunology, nanomedicine, and nanobiotechnology.},
  doi      = {10.1021/la0513712},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1027828956},
}

@Article{Gref2000,
  author   = {R Gref and M Lück and P Quellec and M Marchand and E Dellacherie and S Harnisch and T Blunk and R.H Müller},
  journal  = {Colloids and Surfaces B Biointerfaces},
  title    = {‘Stealth’ corona-core nanoparticles surface modified by polyethylene glycol (PEG): influences of the corona (PEG chain length and surface density) and of the core composition on phagocytic uptake and plasma protein adsorption},
  year     = {2000},
  number   = {3-4},
  pages    = {301-313},
  volume   = {18},
  abstract = {Nanoparticles possessing poly(ethylene glycol) (PEG) chains on their surface have been described as blood persistent drug delivery system with potential applications for intravenous drug administration. Considering the importance of protein interactions with injected colloidal dug carriers with regard to their in vivo fate, we analysed plasma protein adsorption onto biodegradable PEG-coated poly(lactic acid) (PLA), poly(lactic-co-glycolic acid) (PLGA) and poly(varepsilon-caprolactone) (PCL) nanoparticles employing two-dimensional gel electrophoresis (2-D PAGE). A series of corona/core nanoparticles of sizes 160-270 nm were prepared from diblock PEG-PLA, PEG-PLGA and PEG-PCL and from PEG-PLA:PLA blends. The PEG Mw was varied from 2000-20000 g/mole and the particles were prepared using different PEG contents. It was thus possible to study the influence of the PEG corona thickness and density, as well as the influence of the nature of the core (PLA, PLGA or PCL), on the competitive plasma protein adsorption, zeta potential and particle uptake by polymorphonuclear (PMN) cells. 2-D PAGE studies showed that plasma protein adsorption on PEG-coated PLA nanospheres strongly depends on the PEG molecular weight (Mw) (i.e. PEG chain length at the particle surface) as well as on the PEG content in the particles (i.e. PEG chain density at the surface of the particles). Whatever the thickness or the density of the corona, the qualitative composition of the plasma protein adsorption patterns was very similar, showing that adsorption was governed by interaction with a PLA surface protected more or less by PEG chains. The main spots on the gels were albumin, fibrinogen, IgG, Ig light chains, and the apolipoproteins apoA-I and apoE. For particles made of PEG-PLA45K with different PEG Mw, a maximal reduction in protein adsorption was found for a PEG Mw of 5000 g/mole. For nanospheres differing in their PEG content from 0.5 to 20 wt %, a PEG content between 2 and 5 wt % was determined as a threshold value for optimal protein resistance. When increasing the PEG content in the nanoparticles above 5 wt % no further reduction in protein adsorption was achieved. Phagocytosis by PMN studied using chemiluminescence and zeta potential data agreed well with these findings: the same PEG surface density threshold was found to ensure simultaneously efficient steric stabilization and to avoid the uptake by PMN cells. Supposing all the PEG chains migrate to the surface, this would correspond to a distance of about 1.5 nm between two terminally attached PEG chains in the covering 'brush'. Particles from PEG5K-PLA45K, PEG5K-PLGA45K and PEG5K-PCL45K copolymers enabled to study the influence of the core on plasma protein adsorption, all other parameters (corona thickness and density) being kept constant. Adsorption patterns were in good qualitative agreement with each other. Only a few protein species were exclusively present just on one type of nanoparticle. However, the extent of proteins adsorbed differed in a large extent from one particle to another. In vivo studies could help elucidating the role of the type and amount of proteins adsorbed on the fate of the nanoparticles after intraveinous administration, as a function of the nature of their core. These results could be useful in the design of long circulating intravenously injectable biodegradable drug carriers endowed with protein resistant properties and low phagocytic uptake.},
  doi      = {10.1016/s0927-7765(99)00156-3},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1029482017},
}

@Article{Aakerman2002,
  author   = {Maria E. Åkerman and Warren C. W. Chan and Pirjo Laakkonen and Sangeeta N. Bhatia and Erkki Ruoslahti},
  journal  = {Proceedings of the National Academy of Sciences of the United States of America},
  title    = {Nanocrystal targeting in vivo},
  year     = {2002},
  note     = {https://europepmc.org/articles/pmc130509?pdf=render},
  number   = {20},
  pages    = {12617-12621},
  volume   = {99},
  abstract = {Inorganic nanostructures that interface with biological systems have recently attracted widespread interest in biology and medicine. Nanoparticles are thought to have potential as novel intravascular probes for both diagnostic (e.g., imaging) and therapeutic purposes (e.g., drug delivery). Critical issues for successful nanoparticle delivery include the ability to target specific tissues and cell types and escape from the biological particulate filter known as the reticuloendothelial system. We set out to explore the feasibility of in vivo targeting by using semiconductor quantum dots (qdots). Qdots are small (<10 nm) inorganic nanocrystals that possess unique luminescent properties; their fluorescence emission is stable and tuned by varying the particle size or composition. We show that ZnS-capped CdSe qdots coated with a lung-targeting peptide accumulate in the lungs of mice after i.v. injection, whereas two other peptides specifically direct qdots to blood vessels or lymphatic vessels in tumors. We also show that adding polyethylene glycol to the qdot coating prevents nonselective accumulation of qdots in reticuloendothelial tissues. These results encourage the construction of more complex nanostructures with capabilities such as disease sensing and drug delivery.},
  doi      = {10.1073/pnas.152463399},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1039713086},
}

@Article{Song2010,
  author   = {Yujun Song and Konggang Qu and Chao Zhao and Jinsong Ren and Xiaogang Qu},
  journal  = {Advanced Materials},
  title    = {Graphene Oxide: Intrinsic Peroxidase Catalytic Activity and Its Application to Glucose Detection},
  year     = {2010},
  number   = {19},
  pages    = {2206-2210},
  volume   = {22},
  abstract = {Carboxyl‐modified graphene oxide (GO–COOH) possesses intrinsic peroxidase‐like activity that can catalyze the reaction of peroxidase substrate 3,3,5,5‐tetramethylbenzidine (TMB) in the presence of H2O2 to produce a blue color reaction. A simple, cheap, and highly sensitive and selective colorimetric method for glucose detection has been developed and will facilitate the utilization of GOCOOH intrinsic peroxidase activity in medical diagnostics and biotechnology.},
  doi      = {10.1002/adma.200903783},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1033416172},
}

@Article{Cabral2011,
  author   = {H. Cabral and Y. Matsumoto and K. Mizuno and Q. Chen and M. Murakami and M. Kimura and Y. Terada and M. R. Kano and K. Miyazono and M. Uesaka and N. Nishiyama and K. Kataoka},
  journal  = {Nature Nanotechnology},
  title    = {Accumulation of sub-100 nm polymeric micelles in poorly permeable tumours depends on size},
  year     = {2011},
  number   = {12},
  pages    = {815-823},
  volume   = {6},
  abstract = {A major goal in cancer research is to develop carriers that can deliver drugs effectively and without side effects. Liposomal and particulate carriers with diameters of ∼100 nm have been widely used to improve the distribution and tumour accumulation of cancer drugs, but so far they have only been effective for treating highly permeable tumours. Here, we compare the accumulation and effectiveness of different sizes of long-circulating, drug-loaded polymeric micelles (with diameters of 30, 50, 70 and 100 nm) in both highly and poorly permeable tumours. All the polymer micelles penetrated highly permeable tumours in mice, but only the 30 nm micelles could penetrate poorly permeable pancreatic tumours to achieve an antitumour effect. We also showed that the penetration and efficacy of the larger micelles could be enhanced by using a transforming growth factor-β inhibitor to increase the permeability of the tumours.},
  doi      = {10.1038/nnano.2011.166},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1040077610},
}

@Article{Park2009,
  author   = {Ji-Ho Park and Luo Gu and Geoffrey von Maltzahn and Erkki Ruoslahti and Sangeeta N. Bhatia and Michael J. Sailor},
  journal  = {Nature Materials},
  title    = {Biodegradable luminescent porous silicon nanoparticles for in vivo applications},
  year     = {2009},
  note     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3058936},
  number   = {4},
  pages    = {331-336},
  volume   = {8},
  abstract = {Nanomaterials that can circulate in the body hold great potential to diagnose and treat disease, but suffer from problems such as toxicity. Porous silicon nanoparticles have now been engineered to concomitantly image tumours or organs within the body, deliver therapeutics and resorb in vivo into benign components that clear renally.},
  doi      = {10.1038/nmat2398},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1051411109},
}

@Article{He2010,
  author   = {Chunbai He and Yiping Hu and Lichen Yin and Cui Tang and Chunhua Yin},
  journal  = {Biomaterials},
  title    = {Effects of particle size and surface charge on cellular uptake and biodistribution of polymeric nanoparticles},
  year     = {2010},
  number   = {13},
  pages    = {3657-3666},
  volume   = {31},
  abstract = {To elucidate the effects of particle size and surface charge on cellular uptake and biodistribution of polymeric nanoparticles (NPs), rhodamine B (RhB) labeled carboxymethyl chitosan grafted NPs (RhB-CMCNP) and chitosan hydrochloride grafted NPs (RhB-CHNP) were developed as the model negatively and positively charged polymeric NPs, respectively. These NPs owned well defined particle sizes (150-500 nm) and Zeta potentials (-40 mV - +35 mV). FITC labeled protamine sulfate (FITC-PS) loaded RhB-CMCNP and camptothecin (CPT) loaded RhB-CHNP with high encapsulation efficiency were prepared. The fluorescence stability in plasma and towards I(-) was investigated, and the result indicated it was sufficient for qualitative and quantitative analysis. NPs with high surface charge and large particle size were phagocytized more efficiently by murine macrophage. Slight particle size and surface charge differences and different cell lines had significant implications in the cellular uptake of NPs, and various mechanisms were involved in the uptake process. In vivo biodistribution suggested that NPs with slight negative charges and particle size of 150 nm were tended to accumulate in tumor more efficiently. These results could serve as a guideline in the rational design of drug nanocarriers with maximized therapeutic efficacy and predictable in vivo properties, in which the control of particle size and surface charge was of significance.},
  doi      = {10.1016/j.biomaterials.2010.01.065},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1005792673},
}

@Article{Gradishar2005,
  author   = {William J. Gradishar and Sergei Tjulandin and Neville Davidson and Heather Shaw and Neil Desai and Paul Bhar and Michael Hawkins and Joyce O'Shaughnessy},
  journal  = {Journal of Clinical Oncology},
  title    = {Phase III Trial of Nanoparticle Albumin-Bound Paclitaxel Compared With Polyethylated Castor Oil–Based Paclitaxel in Women With Breast Cancer},
  year     = {2005},
  note     = {https://ascopubs.org/doi/pdfdirect/10.1200/JCO.2005.04.937?role=tab},
  number   = {31},
  pages    = {7794-7803},
  volume   = {23},
  abstract = {PURPOSE: ABI-007, the first biologically interactive albumin-bound paclitaxel in a nanameter particle, free of solvents, was compared with polyethylated castor oil-based standard paclitaxel in patients with metastatic breast cancer (MBC). This phase III study was performed to confirm preclinical studies demonstrating superior efficacy and reduced toxicity of ABI-007 compared with standard paclitaxel.
PATIENTS AND METHODS: Patients were randomly assigned to 3-week cycles of either ABI-007 260 mg/m(2) intravenously without premedication (n = 229) or standard paclitaxel 175 mg/m(2) intravenously with premedication (n = 225).
RESULTS: ABI-007 demonstrated significantly higher response rates compared with standard paclitaxel (33% v 19%, respectively; P = .001) and significantly longer time to tumor progression (23.0 v 16.9 weeks, respectively; hazard ratio = 0.75; P = .006). The incidence of grade 4 neutropenia was significantly lower for ABI-007 compared with standard paclitaxel (9% v 22%, respectively; P < .001) despite a 49% higher paclitaxel dose. Febrile neutropenia was uncommon (< 2%), and the incidence did not differ between the two study arms. Grade 3 sensory neuropathy was more common in the ABI-007 arm than in the standard paclitaxel arm (10% v 2%, respectively; P < .001) but was easily managed and improved rapidly (median, 22 days). No hypersensitivity reactions occurred with ABI-007 despite the absence of premedication and shorter administration time.
CONCLUSION: ABI-007 demonstrated greater efficacy and a favorable safety profile compared with standard paclitaxel in this patient population. The improved therapeutic index and elimination of corticosteroid premedication required for solvent-based taxanes make the novel albumin-bound paclitaxel ABI-007 an important advance in the treatment of MBC.},
  doi      = {10.1200/jco.2005.04.937},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1020673706},
}

@Article{Farokhzad2006,
  author   = {Omid C. Farokhzad and Jianjun Cheng and Benjamin A. Teply and Ines Sherifi and Sangyong Jon and Philip W. Kantoff and Jerome P. Richie and Robert Langer},
  journal  = {Proceedings of the National Academy of Sciences of the United States of America},
  title    = {Targeted nanoparticle-aptamer bioconjugates for cancer chemotherapy in vivo},
  year     = {2006},
  note     = {https://europepmc.org/articles/pmc1458875?pdf=render},
  number   = {16},
  pages    = {6315-6320},
  volume   = {103},
  abstract = {Targeted uptake of therapeutic nanoparticles in a cell-, tissue-, or disease-specific manner represents a potentially powerful technology. Using prostate cancer as a model, we report docetaxel (Dtxl)-encapsulated nanoparticles formulated with biocompatible and biodegradable poly(D,L-lactic-co-glycolic acid)-block-poly(ethylene glycol) (PLGA-b-PEG) copolymer and surface functionalized with the A10 2'-fluoropyrimidine RNA aptamers that recognize the extracellular domain of the prostate-specific membrane antigen (PSMA), a well characterized antigen expressed on the surface of prostate cancer cells. These Dtxl-encapsulated nanoparticle-aptamer bioconjugates (Dtxl-NP-Apt) bind to the PSMA protein expressed on the surface of LNCaP prostate epithelial cells and get taken up by these cells resulting in significantly enhanced in vitro cellular toxicity as compared with nontargeted nanoparticles that lack the PSMA aptamer (Dtxl-NP) (P < 0.0004). The Dtxl-NP-Apt bioconjugates also exhibit remarkable efficacy and reduced toxicity as measured by mean body weight loss (BWL) in vivo [body weight loss of 7.7 +/- 4% vs. 18 +/- 5% for Dtxl-NP-Apt vs. Dtxl-NP at nadir, respectively (mean +/- SD); n = 7]. After a single intratumoral injection of Dtxl-NP-Apt bioconjugates, complete tumor reduction was observed in five of seven LNCaP xenograft nude mice (initial tumor volume of approximately 300 mm3), and 100% of these animals survived our 109-day study. In contrast, two of seven mice in the Dtxl-NP group had complete tumor reduction with 109-day survivability of only 57%. Dtxl alone had a survivability of only 14%. Saline and nanoparticles without drug were similarly nonefficacious. This report demonstrates the potential utility of nanoparticle-aptamer bioconjugates for a therapeutic application.},
  doi      = {10.1073/pnas.0601755103},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1024367203},
}

@Article{Liong2008,
  author   = {Monty Liong and Jie Lu and Michael Kovochich and Tian Xia and Stefan G. Ruehm and Andre E. Nel and Fuyuhiko Tamanoi and Jeffrey I. Zink},
  journal  = {ACS Nano},
  title    = {Multifunctional Inorganic Nanoparticles for Imaging, Targeting, and Drug Delivery},
  year     = {2008},
  note     = {https://europepmc.org/articles/pmc2751731?pdf=render},
  number   = {5},
  pages    = {889-896},
  volume   = {2},
  abstract = {Drug delivery, magnetic resonance and fluorescence imaging, magnetic manipulation, and cell targeting are simultaneously possible using a multifunctional mesoporous silica nanoparticle. Superparamagnetic iron oxide nanocrystals were encapsulated inside mesostructured silica spheres that were labeled with fluorescent dye molecules and coated with hydrophilic groups to prevent aggregation. Water-insoluble anticancer drugs were delivered into human cancer cells; surface conjugation with cancer-specific targeting agents increased the uptake into cancer cells relative to that in non-cancerous fibroblasts. The highly versatile multifunctional nanoparticles could potentially be used for simultaneous imaging and therapeutic applications.},
  doi      = {10.1021/nn800072t},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1028058184},
}

@Article{Lai2003,
  author   = {Cheng-Yu Lai and Brian G. Trewyn and Dusan M. Jeftinija and Ksenija Jeftinija and Shu Xu and Srdija Jeftinija and Victor S.-Y. Lin},
  journal  = {Journal of the American Chemical Society},
  title    = {A Mesoporous Silica Nanosphere-Based Carrier System with Chemically Removable CdS Nanoparticle Caps for Stimuli-Responsive Controlled Release of Neurotransmitters and Drug Molecules},
  year     = {2003},
  number   = {15},
  pages    = {4451-4459},
  volume   = {125},
  abstract = {An MCM-41 type mesoporous silica nanosphere-based (MSN) controlled-release delivery system has been synthesized and characterized using surface-derivatized cadmium sulfide (CdS) nanocrystals as chemically removable caps to encapsulate several pharmaceutical drug molecules and neurotransmitters inside the organically functionalized MSN mesoporous framework. We studied the stimuli-responsive release profiles of vancomycin- and adenosine triphosphate (ATP)-loaded MSN delivery systems by using disulfide bond-reducing molecules, such as dithiothreitol (DTT) and mercaptoethanol (ME), as release triggers. The biocompatibility and delivery efficiency of the MSN system with neuroglial cells (astrocytes) in vitro were demonstrated. In contrast to many current delivery systems, the molecules of interest were encapsulated inside the porous framework of the MSN not by adsorption or sol-gel types of entrapment but by capping the openings of the mesoporous channels with size-defined CdS nanoparticles to physically block the drugs/neurotransmitters of certain sizes from leaching out. We envision that this new MSN system could play a significant role in developing new generations of site-selective, controlled-release delivery nanodevices.},
  doi      = {10.1021/ja028650l},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1016714991},
}

@Article{Chithrani2007,
  author   = {B. Devika Chithrani and Warren C. W. Chan},
  journal  = {Nano Letters},
  title    = {Elucidating the Mechanism of Cellular Uptake and Removal of Protein-Coated Gold Nanoparticles of Different Sizes and Shapes},
  year     = {2007},
  number   = {6},
  pages    = {1542-1550},
  volume   = {7},
  abstract = {We investigated the mechanism by which transferrin-coated gold nanoparticles (Au NP) of different sizes and shapes entered mammalian cells. We determined that transferrin-coated Au NP entered the cells via clathrin-mediated endocytosis pathway. The NPs exocytosed out of the cells in a linear relationship to size. This was different than the relationship between uptake and size. Furthermore, we developed a mathematical equation to predict the relationship of size versus exocytosis for different cell lines. These studies will provide guidelines for developing NPs for imaging and drug delivery applications, which will require "controlling" NP accumulation rate. These studies will also have implications in determining nanotoxicity.},
  doi      = {10.1021/nl070363y},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1052990623},
}

@Article{Huh2010,
  author   = {Dongeun Huh and Benjamin D. Matthews and Akiko Mammoto and Martín Montoya-Zavala and Hong Yuan Hsin and Donald E. Ingber},
  journal  = {Science},
  title    = {Reconstituting Organ-Level Lung Functions on a Chip},
  year     = {2010},
  note     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8335790},
  number   = {5986},
  pages    = {1662-1668},
  volume   = {328},
  abstract = {Here, we describe a biomimetic microsystem that reconstitutes the critical functional alveolar-capillary interface of the human lung. This bioinspired microdevice reproduces complex integrated organ-level responses to bacteria and inflammatory cytokines introduced into the alveolar space. In nanotoxicology studies, this lung mimic revealed that cyclic mechanical strain accentuates toxic and inflammatory responses of the lung to silica nanoparticles. Mechanical strain also enhances epithelial and endothelial uptake of nanoparticulates and stimulates their transport into the underlying microvascular channel. Similar effects of physiological breathing on nanoparticle absorption are observed in whole mouse lung. Mechanically active "organ-on-a-chip" microdevices that reconstitute tissue-tissue interfaces critical to organ function may therefore expand the capabilities of cell culture models and provide low-cost alternatives to animal and clinical studies for drug screening and toxicology applications.},
  doi      = {10.1126/science.1188302},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1050579616},
}

@Article{Geng2007,
  author   = {Yan Geng and Paul Dalhaimer and Shenshen Cai and Richard Tsai and Manorama Tewari and Tamara Minko and Dennis E. Discher},
  journal  = {Nature Nanotechnology},
  title    = {Shape effects of filaments versus spherical particles in flow and drug delivery},
  year     = {2007},
  note     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2740330},
  number   = {4},
  pages    = {249-255},
  volume   = {2},
  abstract = {Interaction of spherical particles with cells and within animals has been studied extensively, but the effects of shape have received little attention. Here we use highly stable, polymer micelle assemblies known as filomicelles to compare the transport and trafficking of flexible filaments with spheres of similar chemistry. In rodents, filomicelles persisted in the circulation up to one week after intravenous injection. This is about ten times longer than their spherical counterparts and is more persistent than any known synthetic nanoparticle. Under fluid flow conditions, spheres and short filomicelles are taken up by cells more readily than longer filaments because the latter are extended by the flow. Preliminary results further demonstrate that filomicelles can effectively deliver the anticancer drug paclitaxel and shrink human-derived tumours in mice. Although these findings show that long-circulating vehicles need not be nanospheres, they also lend insight into possible shape effects of natural filamentous viruses.},
  doi      = {10.1038/nnano.2007.70},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1002703153},
}

@Article{Haes2002,
  author   = {Amanda J. Haes and Richard P. Van Duyne},
  journal  = {Journal of the American Chemical Society},
  title    = {A Nanoscale Optical Biosensor: Sensitivity and Selectivity of an Approach Based on the Localized Surface Plasmon Resonance Spectroscopy of Triangular Silver Nanoparticles},
  year     = {2002},
  number   = {35},
  pages    = {10596-10604},
  volume   = {124},
  abstract = {Triangular silver nanoparticles ( approximately 100 nm wide and 50 nm high) have remarkable optical properties. In particular, the peak extinction wavelength, lambda(max) of their localized surface plasmon resonance (LSPR) spectrum is unexpectedly sensitive to nanoparticle size, shape, and local ( approximately 10-30 nm) external dielectric environment. This sensitivity of the LSPR lambda(max) to the nanoenvironment has allowed us to develop a new class of nanoscale affinity biosensors. The essential characteristics and operational principles of these LSPR nanobiosensors will be illustrated using the well-studied biotin-streptavidin system. Exposure of biotin-functionalized Ag nanotriangles to 100 nM streptavidin (SA) caused a 27.0 nm red-shift in the LSPR lambda(max). The LSPR lambda(max) shift, DeltaR/DeltaR(max), versus [SA] response curve was measured over the concentration range 10(-)(15) M < [SA] < 10(-)(6) M. Comparison of the data with the theoretical normalized response expected for 1:1 binding of a ligand to a multivalent receptor with different sites but invariant affinities yielded approximate values for the saturation response, DeltaR(max) = 26.5 nm, and the surface-confined thermodynamic binding constant K(a,surf) = 10(11) M(-)(1). At present, the limit of detection (LOD) for the LSPR nanobiosensor is found to be in the low-picomolar to high-femtomolar region. A strategy to amplify the response of the LSPR nanobiosensor using biotinylated Au colloids and thereby further improve the LOD is demonstrated. Several control experiments were performed to define the LSPR nanobiosensor's response to nonspecific binding as well as to demonstrate its response to the specific binding of another protein. These include the following: (1) electrostatic binding of SA to a nonbiotinylated surface, (2) nonspecific interactions of prebiotinylated SA to a biotinylated surface, (3) nonspecific interactions of bovine serum albumin to a biotinylated surface, and (4) specific binding of anti-biotin to a biotinylated surface. The LSPR nanobiosensor provides a pathway to ultrasensitive biodetection experiments with extremely simple, small, light, robust, low-cost instrumentation that will greatly facilitate field-portable environmental or point-of-service medical diagnostic applications.},
  doi      = {10.1021/ja020393x},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1035623214},
}

@Article{Davis2010,
  author   = {Mark E. Davis and Jonathan E. Zuckerman and Chung Hang J. Choi and David Seligson and Anthony Tolcher and Christopher A. Alabi and Yun Yen and Jeremy D. Heidel and Antoni Ribas},
  journal  = {Nature},
  title    = {Evidence of RNAi in humans from systemically administered siRNA via targeted nanoparticles},
  year     = {2010},
  note     = {https://www.nature.com/articles/nature08956.pdf},
  number   = {7291},
  pages    = {1067-1070},
  volume   = {464},
  abstract = {Human RNAi therapyThe ability to downregulate specific genes using systemically delivered short RNA molecules and the cellular mechanism known as RNA interference has been shown previously in mouse and non-human primate models. Davis et al. have now demonstrated for the first time in humans that a short interfering RNA (siRNA) molecule can be systemically delivered using nanoparticles to a solid tumour. The siRNA mediates directed cleavage of its target mRNA, thereby also reducing the protein level. This proof-of-principle study confirms the potential of this technology as a human therapeutic.},
  doi      = {10.1038/nature08956},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1035909146},
}

@Article{Qian2007,
  author   = {Ximei Qian and Xiang-Hong Peng and Dominic O Ansari and Qiqin Yin-Goen and Georgia Z Chen and Dong M Shin and Lily Yang and Andrew N Young and May D Wang and Shuming Nie},
  journal  = {Nature Biotechnology},
  title    = {In vivo tumor targeting and spectroscopic detection with surface-enhanced Raman nanoparticle tags},
  year     = {2007},
  number   = {1},
  pages    = {83-90},
  volume   = {26},
  abstract = {We describe biocompatible and nontoxic nanoparticles for in vivo tumor targeting and detection based on pegylated gold nanoparticles and surface-enhanced Raman scattering (SERS). Colloidal gold has been safely used to treat rheumatoid arthritis for 50 years, and has recently been found to amplify the efficiency of Raman scattering by 14–15 orders of magnitude. Here we show that large optical enhancements can be achieved under in vivo conditions for tumor detection in live animals. An important finding is that small-molecule Raman reporters such as organic dyes were not displaced but were stabilized by thiol-modified polyethylene glycols. These pegylated SERS nanoparticles were considerably brighter than semiconductor quantum dots with light emission in the near-infrared window. When conjugated to tumor-targeting ligands such as single-chain variable fragment (ScFv) antibodies, the conjugated nanoparticles were able to target tumor biomarkers such as epidermal growth factor receptors on human cancer cells and in xenograft tumor models.},
  doi      = {10.1038/nbt1377},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1035474832},
}

@Article{Cedervall2007,
  author   = {Tommy Cedervall and Iseult Lynch and Stina Lindman and Tord Berggård and Eva Thulin and Hanna Nilsson and Kenneth A. Dawson and Sara Linse},
  journal  = {Proceedings of the National Academy of Sciences of the United States of America},
  title    = {Understanding the nanoparticle–protein corona using methods to quantify exchange rates and affinities of proteins for nanoparticles},
  year     = {2007},
  note     = {https://europepmc.org/articles/pmc1892985?pdf=render},
  number   = {7},
  pages    = {2050-2055},
  volume   = {104},
  abstract = {Due to their small size, nanoparticles have distinct properties compared with the bulk form of the same materials. These properties are rapidly revolutionizing many areas of medicine and technology. Despite the remarkable speed of development of nanoscience, relatively little is known about the interaction of nanoscale objects with living systems. In a biological fluid, proteins associate with nanoparticles, and the amount and presentation of the proteins on the surface of the particles leads to an in vivo response. Proteins compete for the nanoparticle "surface," leading to a protein "corona" that largely defines the biological identity of the particle. Thus, knowledge of rates, affinities, and stoichiometries of protein association with, and dissociation from, nanoparticles is important for understanding the nature of the particle surface seen by the functional machinery of cells. Here we develop approaches to study these parameters and apply them to plasma and simple model systems, albumin and fibrinogen. A series of copolymer nanoparticles are used with variation of size and composition (hydrophobicity). We show that isothermal titration calorimetry is suitable for studying the affinity and stoichiometry of protein binding to nanoparticles. We determine the rates of protein association and dissociation using surface plasmon resonance technology with nanoparticles that are thiol-linked to gold, and through size exclusion chromatography of protein-nanoparticle mixtures. This method is less perturbing than centrifugation, and is developed into a systematic methodology to isolate nanoparticle-associated proteins. The kinetic and equilibrium binding properties depend on protein identity as well as particle surface characteristics and size.},
  doi      = {10.1073/pnas.0608582104},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1044670756},
}

@Article{Gratton2008,
  author   = {Stephanie E. A. Gratton and Patricia A. Ropp and Patrick D. Pohlhaus and J. Christopher Luft and Victoria J. Madden and Mary E. Napier and Joseph M. DeSimone},
  journal  = {Proceedings of the National Academy of Sciences of the United States of America},
  title    = {The effect of particle design on cellular internalization pathways},
  year     = {2008},
  note     = {https://europepmc.org/articles/pmc2575324?pdf=render},
  number   = {33},
  pages    = {11613-11618},
  volume   = {105},
  abstract = {The interaction of particles with cells is known to be strongly influenced by particle size, but little is known about the interdependent role that size, shape, and surface chemistry have on cellular internalization and intracellular trafficking. We report on the internalization of specially designed, monodisperse hydrogel particles into HeLa cells as a function of size, shape, and surface charge. We employ a top-down particle fabrication technique called PRINT that is able to generate uniform populations of organic micro- and nanoparticles with complete control of size, shape, and surface chemistry. Evidence of particle internalization was obtained by using conventional biological techniques and transmission electron microscopy. These findings suggest that HeLa cells readily internalize nonspherical particles with dimensions as large as 3 mum by using several different mechanisms of endocytosis. Moreover, it was found that rod-like particles enjoy an appreciable advantage when it comes to internalization rates, reminiscent of the advantage that many rod-like bacteria have for internalization in nonphagocytic cells.},
  doi      = {10.1073/pnas.0801763105},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1026661243},
}

@Article{Gao2007,
  author   = {Lizeng Gao and Jie Zhuang and Leng Nie and Jinbin Zhang and Yu Zhang and Ning Gu and Taihong Wang and Jing Feng and Dongling Yang and Sarah Perrett and Xiyun Yan},
  journal  = {Nature Nanotechnology},
  title    = {Intrinsic peroxidase-like activity of ferromagnetic nanoparticles},
  year     = {2007},
  number   = {9},
  pages    = {577-583},
  volume   = {2},
  abstract = {Nanoparticles containing magnetic materials, such as magnetite (Fe3O4), are particularly useful for imaging and separation techniques. As these nanoparticles are generally considered to be biologically and chemically inert, they are typically coated with metal catalysts, antibodies or enzymes to increase their functionality as separation agents. Here, we report that magnetite nanoparticles in fact possess an intrinsic enzyme mimetic activity similar to that found in natural peroxidases, which are widely used to oxidize organic substrates in the treatment of wastewater or as detection tools. Based on this finding, we have developed a novel immunoassay in which antibody-modified magnetite nanoparticles provide three functions: capture, separation and detection. The stability, ease of production and versatility of these nanoparticles makes them a powerful tool for a wide range of potential applications in medicine, biotechnology and environmental chemistry.},
  doi      = {10.1038/nnano.2007.260},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1041163416},
}

@Article{Li2010,
  author   = {Jian Feng Li and Yi Fan Huang and Yong Ding and Zhi Lin Yang and Song Bo Li and Xiao Shun Zhou and Feng Ru Fan and Wei Zhang and Zhi You Zhou and De Yin Wu and Bin Ren and Zhong Lin Wang and Zhong Qun Tian},
  journal  = {Nature},
  title    = {Shell-isolated nanoparticle-enhanced Raman spectroscopy},
  year     = {2010},
  number   = {7287},
  pages    = {392-395},
  volume   = {464},
  abstract = {Raman spectroscopy unleashedSurface-enhanced Raman scattering (SERS) spectroscopy is a powerful analytical technique able to detect substances down to single molecule level. Its applications are limited, however, because to realize a substantial Raman signal requires metal substrates that either have roughened surfaces or take the form of nanoparticles. An innovative approach is now demonstrated, where the substance under investigation, on a generic substrate, is covered by a layer of 'smart dust' consisting of gold nanoparticles coated by an ultrathin insulating shell of silica or alumina. The nanoparticles provide Raman signal amplification, and the coating keeps them separate from each other and from the probed substance. The new technique, termed SHINERS (shell-isolated nanoparticle-enhanced Raman spectroscopy), is demonstrated by probing pesticide residues on the surfaces of yeast cells and citrus fruits. It could be useful in materials science and the life sciences, as well as for the inspection of food safety, drugs, explosives and environmental pollutants.},
  doi      = {10.1038/nature08907},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1013655144},
}

@Article{Horcajada2009,
  author   = {Patricia Horcajada and Tamim Chalati and Christian Serre and Brigitte Gillet and Catherine Sebrie and Tarek Baati and Jarrod F. Eubank and Daniela Heurtaux and Pascal Clayette and Christine Kreuz and Jong-San Chang and Young Kyu Hwang and Veronique Marsaud and Phuong-Nhi Bories and Luc Cynober and Sophie Gil and Gérard Férey and Patrick Couvreur and Ruxandra Gref},
  journal  = {Nature Materials},
  title    = {Porous metal–organic-framework nanoscale carriers as a potential platform for drug delivery and imaging},
  year     = {2009},
  number   = {2},
  pages    = {172-178},
  volume   = {9},
  abstract = {In the domain of health, one important challenge is the efficient delivery of drugs in the body using non-toxic nanocarriers. Most of the existing carrier materials show poor drug loading (usually less than 5 wt% of the transported drug versus the carrier material) and/or rapid release of the proportion of the drug that is simply adsorbed (or anchored) at the external surface of the nanocarrier. In this context, porous hybrid solids, with the ability to tune their structures and porosities for better drug interactions and high loadings, are well suited to serve as nanocarriers for delivery and imaging applications. Here we show that specific non-toxic porous iron(III)-based metal–organic frameworks with engineered cores and surfaces, as well as imaging properties, function as superior nanocarriers for efficient controlled delivery of challenging antitumoural and retroviral drugs (that is, busulfan, azidothymidine triphosphate, doxorubicin or cidofovir) against cancer and AIDS. In addition to their high loadings, they also potentially associate therapeutics and diagnostics, thus opening the way for theranostics, or personalized patient treatments.},
  doi      = {10.1038/nmat2608},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1040302077},
}

@Article{Kim2007,
  author   = {Jun Sung Kim and Eunye Kuk and Kyeong Nam Yu and Jong-Ho Kim and Sung Jin Park and Hu Jang Lee and So Hyun Kim and Young Kyung Park and Yong Ho Park and Cheol-Yong Hwang and Yong-Kwon Kim and Yoon-Sik Lee and Dae Hong Jeong and Myung-Haing Cho},
  journal  = {Nanomedicine Nanotechnology Biology and Medicine},
  title    = {Antimicrobial effects of silver nanoparticles},
  year     = {2007},
  note     = {https://s-space.snu.ac.kr/bitstream/10371/8335/1/Antimicrobial%20effects%20of%20silver%20nanoparticles.pdf},
  number   = {1},
  pages    = {95-101},
  volume   = {3},
  abstract = {The antimicrobial effects of silver (Ag) ion or salts are well known, but the effects of Ag nanoparticles on microorganisms and antimicrobial mechanism have not been revealed clearly. Stable Ag nanoparticles were prepared and their shape and size distribution characterized by particle characterizer and transmission electron microscopic study. The antimicrobial activity of Ag nanoparticles was investigated against yeast, Escherichia coli, and Staphylococcus aureus. In these tests, Muller Hinton agar plates were used and Ag nanoparticles of various concentrations were supplemented in liquid systems. As results, yeast and E. coli were inhibited at the low concentration of Ag nanoparticles, whereas the growth-inhibitory effects on S. aureus were mild. The free-radical generation effect of Ag nanoparticles on microbial growth inhibition was investigated by electron spin resonance spectroscopy. These results suggest that Ag nanoparticles can be used as effective growth inhibitors in various microorganisms, making them applicable to diverse medical devices and antimicrobial control systems.},
  doi      = {10.1016/j.nano.2006.12.001},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1044430846},
}

@Article{Sondi2004,
  author   = {Ivan Sondi and Branka Salopek-Sondi},
  journal  = {Journal of Colloid and Interface Science},
  title    = {Silver nanoparticles as antimicrobial agent: a case study on E. coli as a model for Gram-negative bacteria},
  year     = {2004},
  number   = {1},
  pages    = {177-182},
  volume   = {275},
  abstract = {The antimicrobial activity of silver nanoparticles against E. coli was investigated as a model for Gram-negative bacteria. Bacteriological tests were performed in Luria-Bertani (LB) medium on solid agar plates and in liquid systems supplemented with different concentrations of nanosized silver particles. These particles were shown to be an effective bactericide. Scanning and transmission electron microscopy (SEM and TEM) were used to study the biocidal action of this nanoscale material. The results confirmed that the treated E. coli cells were damaged, showing formation of "pits" in the cell wall of the bacteria, while the silver nanoparticles were found to accumulate in the bacterial membrane. A membrane with such a morphology exhibits a significant increase in permeability, resulting in death of the cell. These nontoxic nanomaterials, which can be prepared in a simple and cost-effective manner, may be suitable for the formulation of new types of bactericidal materials.},
  doi      = {10.1016/j.jcis.2004.02.012},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1025760895},
}

@Article{Gao2004,
  author   = {Xiaohu Gao and Yuanyuan Cui and Richard M Levenson and Leland W K Chung and Shuming Nie},
  journal  = {Nature Biotechnology},
  title    = {In vivo cancer targeting and imaging with semiconductor quantum dots},
  year     = {2004},
  number   = {8},
  pages    = {969-976},
  volume   = {22},
  abstract = {We describe the development of multifunctional nanoparticle probes based on semiconductor quantum dots (QDs) for cancer targeting and imaging in living animals. The structural design involves encapsulating luminescent QDs with an ABC triblock copolymer and linking this amphiphilic polymer to tumor-targeting ligands and drug-delivery functionalities. In vivo targeting studies of human prostate cancer growing in nude mice indicate that the QD probes accumulate at tumors both by the enhanced permeability and retention of tumor sites and by antibody binding to cancer-specific cell surface biomarkers. Using both subcutaneous injection of QD-tagged cancer cells and systemic injection of multifunctional QD probes, we have achieved sensitive and multicolor fluorescence imaging of cancer cells under in vivo conditions. We have also integrated a whole-body macro-illumination system with wavelength-resolved spectral imaging for efficient background removal and precise delineation of weak spectral signatures. These results raise new possibilities for ultrasensitive and multiplexed imaging of molecular targets in vivo.},
  doi      = {10.1038/nbt994},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1039601245},
}

@Article{Huang2006,
  author   = {Xiaohua Huang and Ivan H. El-Sayed and Wei Qian and Mostafa A. El-Sayed},
  journal  = {Journal of the American Chemical Society},
  title    = {Cancer Cell Imaging and Photothermal Therapy in the Near-Infrared Region by Using Gold Nanorods},
  year     = {2006},
  number   = {6},
  pages    = {2115-2120},
  volume   = {128},
  abstract = {Due to strong electric fields at the surface, the absorption and scattering of electromagnetic radiation by noble metal nanoparticles are strongly enhanced. These unique properties provide the potential of designing novel optically active reagents for simultaneous molecular imaging and photothermal cancer therapy. It is desirable to use agents that are active in the near-infrared (NIR) region of the radiation spectrum to minimize the light extinction by intrinsic chromophores in native tissue. Gold nanorods with suitable aspect ratios (length divided by width) can absorb and scatter strongly in the NIR region (650-900 nm). In the present work, we provide an in vitro demonstration of gold nanorods as novel contrast agents for both molecular imaging and photothermal cancer therapy. Nanorods are synthesized and conjugated to anti-epidermal growth factor receptor (anti-EGFR) monoclonal antibodies and incubated in cell cultures with a nonmalignant epithelial cell line (HaCat) and two malignant oral epithelial cell lines (HOC 313 clone 8 and HSC 3). The anti-EGFR antibody-conjugated nanorods bind specifically to the surface of the malignant-type cells with a much higher affinity due to the overexpressed EGFR on the cytoplasmic membrane of the malignant cells. As a result of the strongly scattered red light from gold nanorods in dark field, observed using a laboratory microscope, the malignant cells are clearly visualized and diagnosed from the nonmalignant cells. It is found that, after exposure to continuous red laser at 800 nm, malignant cells require about half the laser energy to be photothermally destroyed than the nonmalignant cells. Thus, both efficient cancer cell diagnostics and selective photothermal therapy are realized at the same time.},
  doi      = {10.1021/ja057254a},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  url      = {https://app.dimensions.ai/details/publication/pub.1009246618},
}

@Article{Liu2023,
  author   = {Xuepeng Liu and Bin Ding and Mingyuan Han and Zhenhai Yang and Jianlin Chen and Pengju Shi and Xiangying Xue and Rahim Ghadari and Xianfu Zhang and Rui Wang and Keith Brooks and Li Tao and Sachin Kinge and Songyuan Dai and Jiang Sheng and Paul J. Dyson and Mohammad Khaja Nazeeruddin and Yong Ding},
  journal  = {Angewandte Chemie International Edition},
  title    = {Extending the π‐Conjugated System in Spiro‐Type Hole Transport Material Enhances the Efficiency and Stability of Perovskite Solar Modules},
  year     = {2023},
  note     = {https://doi.org/10.1002/anie.202304350},
  number   = {29},
  pages    = {e202304350},
  volume   = {62},
  abstract = {Hole transport materials (HTMs) are a key component of perovskite solar cells (PSCs). The small molecular 2,2',7,7'-tetrakis(N,N-di-p-methoxyphenyl)-amine-9,9'-spirobifluorene (spiro-OMeTAD, termed "Spiro") is the most successful HTM used in PSCs, but its versatility is imperfect. To improve its performance, we developed a novel spiro-type HTM (termed "DP") by substituting four anisole units on Spiro with 4-methoxybiphenyl moieties. By extending the π-conjugation of Spiro in this way, the HOMO level of the HTM matches well with the perovskite valence band, enhancing hole mobility and increasing the glass transition temperature. DP-based PSC achieves high power conversion efficiencies (PCEs) of 25.24 % for small-area (0.06 cm<sup>2</sup> ) devices and 21.86 % for modules (designated area of 27.56 cm<sup>2</sup> ), along with the certified efficiency of 21.78 % on a designated area of 27.86 cm<sup>2</sup> . The encapsulated DP-based devices maintain 95.1 % of the initial performance under ISOS-L-1 conditions after 2560 hours and 87 % at the ISOS-L-3 conditions over 600 hours.},
  doi      = {10.1002/anie.202304350},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1158048403},
}

@Article{Tian2023,
  author   = {Chuanming Tian and Bin Li and Yichuan Rui and Hao Xiong and Yu Zhao and Xuefei Han and Xinliang Zhou and Yu Qiu and Wei An and Kerui Li and Chengyi Hou and Yaogang Li and Hongzhi Wang and Qinghong Zhang},
  journal  = {Advanced Functional Materials},
  title    = {In Situ Polymerizing Internal Encapsulation Strategy Enables Stable Perovskite Solar Cells toward Lead Leakage Suppression},
  year     = {2023},
  number   = {41},
  volume   = {33},
  abstract = {Abstract  Despite the outstanding power conversion efficiency (PCE) of perovskite solar cells (PSCs) achieved over the years, unsatisfactory stability and lead toxicity remain obstacles that limit their competitiveness and large‐scale practical deployment. In this study, in situ polymerizing internal encapsulation (IPIE) is developed as a holistic approach to overcome these challenges. The uniform polymer internal package layer constructed by thermally triggered cross‐linkable monomers not only solidifies the ionic perovskite crystalline by strong electron‐withdrawing/donating chemical sites, but also acts as a water penetration and ion migration barrier to prolong shelf life under harsh environments. The optimized MAPbI 3 and FAPbI 3 devices with IPIE treatment yield impressive efficiencies of 22.29% and 24.12%, respectively, accompanied by remarkably enhanced environmental and mechanical stabilities. In addition, toxic water‐soluble lead leakage is minimized by the synergetic effect of the physical encapsulation wall and chemical chelation conferred by the IPIE. Hence, this strategy provides a feasible route for preparing efficient, stable, and eco‐friendly PSCs.},
  doi      = {10.1002/adfm.202302270},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1159957570},
}

@Article{Wang2023,
  author   = {Min Wang and Haoxuan Sun and Meng Wang and Linxing Meng and Liang Li},
  journal  = {Advanced Materials},
  title    = {Uracil Induced Simultaneously Strengthening Grain Boundaries and Interfaces Enables High‐Performance Perovskite Solar Cells with Superior Operational Stability},
  year     = {2023},
  number   = {2},
  pages    = {e2306415},
  volume   = {36},
  abstract = {The operational stability is a huge obstacle to further commercialization of perovskite solar cells. To address this critical issue, in this work, uracil is introduced as a "binder" into the perovskite film to simultaneously improve the power conversion efficiency (PCE) and operational stability. Uracil can efficiently passivate defects and strengthen grain boundaries to enhance the stability of perovskite films. Moreover, the uracil also strengthens the interface between the perovskite and the Tin oxide (SnO<sub>2</sub> ) electron transport layer to increase the binding force. The uracil-modified devices deliver a champion PCE of 24.23% (certificated 23.19%) with negligible hysteresis at active area of 0.0625 cm<sup>2</sup> . In particular, the optimal device exhibits over 90% of its initial PCE after tracking for ≈6000 h at its maximum power point under continuous light, indicating its superior operational stability. Moreover, the devices also show great reproducibility in both PCE and operational stability.},
  doi      = {10.1002/adma.202306415},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1163801313},
}

@Article{Jiang2024,
  author   = {Fangyuan Jiang and Yangwei Shi and Tanka R. Rana and Daniel Morales and Isaac E. Gould and Declan P. McCarthy and Joel A. Smith and M. Greyson Christoforo and Muammer Y. Yaman and Faiz Mandani and Tanguy Terlier and Hannah Contreras and Stephen Barlow and Aditya D. Mohite and Henry J. Snaith and Seth R. Marder and J. Devin MacKenzie and Michael D. McGehee and David S. Ginger},
  journal  = {Nature Energy},
  title    = {Improved reverse bias stability in p–i–n perovskite solar cells with optimized hole transport materials and less reactive electrodes},
  year     = {2024},
  number   = {10},
  pages    = {1275-1284},
  volume   = {9},
  abstract = {As perovskite photovoltaics stride towards commercialization, reverse bias degradation in shaded cells that must current match illuminated cells is a serious challenge. Previous research has emphasized the role of iodide and silver oxidation, and the role of hole tunnelling from the electron-transport layer into the perovskite to enable the flow of current under reverse bias in causing degradation. Here we show that device architecture engineering has a significant impact on the reverse bias behaviour of perovskite solar cells. By implementing both a ~35-nm-thick conjugated polymer hole transport layer and a more electrochemically stable back electrode, we demonstrate average breakdown voltages exceeding −15 V, comparable to those of silicon cells. Our strategy for increasing the breakdown voltage reduces the number of bypass diodes needed to protect a solar module that is partially shaded, which has been proven to be an effective strategy for silicon solar panels.},
  doi      = {10.1038/s41560-024-01600-z},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1174552302},
}

@Article{Yang2023,
  author   = {Yuanhang Yang and Siyang Cheng and Xueliang Zhu and Sheng Li and Zhuo Zheng and Kai Zhao and Liwei Ji and Ruiming Li and Yong Liu and Chang Liu and Qianqian Lin and Ning Yan and Zhiping Wang},
  journal  = {Nature Energy},
  title    = {Inverted perovskite solar cells with over 2,000 h operational stability at 85 °C using fixed charge passivation},
  year     = {2023},
  number   = {1},
  pages    = {37-46},
  volume   = {9},
  abstract = {High-quality defect passivation at perovskite/charge extraction layer heterojunctions in perovskite solar cells is critical to solar device operation. Here we report a ‘physical’ passivation method by producing fixed charges with aluminium oxide (negative fixed charges) or silicon oxide (positive fixed charges) interlayers grown by atomic layer deposition at perovskite/charge extraction layer heterojunctions. Through experimental and modelling approaches, we find that the fixed charge passivation (FCP) modifies carrier concentration distribution near the heterojunctions, which reduces interface recombination and photovoltage losses. The strong acidity of aluminium oxide simultaneously shields perovskites from being deprotonated by the nickel oxide at high temperatures. The optimized FCP device shows an efficiency of 22.5% with over 60 mV improvement in photovoltage in contrast to the control devices. Importantly, the encapsulated FCP devices, operated at the maximum power point, exhibit almost no efficiency loss after ageing under 1-sun illumination at 85 °C for 2,000 h in ambient air.},
  doi      = {10.1038/s41560-023-01377-7},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1165045432},
}

@Article{Fei2023,
  author   = {Chengbin Fei and Nengxu Li and Mengru Wang and Xiaoming Wang and Hangyu Gu and Bo Chen and Zhao Zhang and Zhenyi Ni and Haoyang Jiao and Wenzhan Xu and Zhifang Shi and Yanfa Yan and Jinsong Huang},
  journal  = {Science},
  title    = {Lead-chelating hole-transport layers for efficient and stable perovskite minimodules},
  year     = {2023},
  note     = {https://cdr.lib.unc.edu/downloads/j098zn88g},
  number   = {6647},
  pages    = {823-829},
  volume   = {380},
  abstract = {The defective bottom interfaces of perovskites and hole-transport layers (HTLs) limit the performance of p-i-n structure perovskite solar cells. We report that the addition of lead chelation molecules into HTLs can strongly interact with lead(II) ion (Pb<sup>2+</sup>), resulting in a reduced amorphous region in perovskites near HTLs and a passivated perovskite bottom surface. The minimodule with an aperture area of 26.9 square centimeters has a power conversion efficiency (PCE) of 21.8% (stabilized at 21.1%) that is certified by the National Renewable Energy Laboratory (NREL), which corresponds to a minimal small-cell efficiency of 24.6% (stabilized 24.1%) throughout the module area. Small-area cells and large-area minimodules with lead chelation molecules in HTLs had a light soaking stability of 3010 and 2130 hours, respectively, at an efficiency loss of 10% from the initial value under 1-sun illumination and open-circuit voltage conditions.},
  doi      = {10.1126/science.ade9463},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1158362233},
}

@Article{Park2023,
  author   = {So Min Park and Mingyang Wei and Jian Xu and Harindi R Atapattu and Felix T Eickemeyer and Kasra Darabi and Luke Grater and Yi Yang and Cheng Liu and Sam Teale and Bin Chen and Hao Chen and Tonghui Wang and Lewei Zeng and Aidan Maxwell and Zaiwei Wang and Keerthan R Rao and Zhuoyun Cai and Shaik M Zakeeruddin and Jonathan T Pham and Chad M Risko and Aram Amassian and Mercouri G Kanatzidis and Kenneth R Graham and Michael Grätzel and Edward H Sargent},
  journal  = {Science},
  title    = {Engineering ligand reactivity enables high-temperature operation of stable perovskite solar cells},
  year     = {2023},
  note     = {http://infoscience.epfl.ch/record/303824},
  number   = {6654},
  pages    = {209-215},
  volume   = {381},
  abstract = {Perovskite solar cells (PSCs) consisting of interfacial two- and three-dimensional heterostructures that incorporate ammonium ligand intercalation have enabled rapid progress toward the goal of uniting performance with stability. However, as the field continues to seek ever-higher durability, additional tools that avoid progressive ligand intercalation are needed to minimize degradation at high temperatures. We used ammonium ligands that are nonreactive with the bulk of perovskites and investigated a library that varies ligand molecular structure systematically. We found that fluorinated aniliniums offer interfacial passivation and simultaneously minimize reactivity with perovskites. Using this approach, we report a certified quasi-steady-state power-conversion efficiency of 24.09% for inverted-structure PSCs. In an encapsulated device operating at 85°C and 50% relative humidity, we document a 1560-hour <i>T</i><sub>85</sub> at maximum power point under 1-sun illumination.},
  doi      = {10.1126/science.adi4107},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1160686387},
}

@Article{Li2023,
  author   = {Zhen Li and Xianglang Sun and Xiaopeng Zheng and Bo Li and Danpeng Gao and Shoufeng Zhang and Xin Wu and Shuai Li and Jianqiu Gong and Joseph M Luther and Zhong'an Li and Zonglong Zhu},
  journal  = {Science},
  title    = {Stabilized hole-selective layer for high-performance inverted p-i-n perovskite solar cells},
  year     = {2023},
  number   = {6668},
  pages    = {284-289},
  volume   = {382},
  abstract = {P-i-n geometry perovskite solar cells (PSCs) offer simplified fabrication, greater amenability to charge extraction layers, and low-temperature processing over n-i-p counterparts. Self-assembled monolayers (SAMs) can enhance the performance of p-i-n PSCs but ultrathin SAMs can be thermally unstable. We report a thermally robust hole-selective layer comprised of nickel oxide (NiO<sub>x</sub>) nanoparticle film with a surface-anchored (4-(3,11-dimethoxy-7H-dibenzo[c,g]carbazol-7-yl)butyl)phosphonic acid (MeO-4PADBC) SAM that can improve and stabilize the NiO<sub>x</sub>/perovskite interface. The energetic alignment and favorable contact and binding between NiO<sub>x</sub>/MeO-4PADBC and perovskite reduced the voltage deficit of PSCs with various perovskite compositions and led to strong interface toughening effects under thermal stress. The resulting 1.53-electron-volt devices achieved 25.6% certified power conversion efficiency and maintained &gt;90% of their initial efficiency after continuously operating at 65 degrees Celsius for 1200 hours under 1-sun illumination.},
  doi      = {10.1126/science.ade9637},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1165052499},
}

@Article{Liu2023a,
  author   = {Cheng Liu and Yi Yang and Hao Chen and Jian Xu and Ao Liu and Abdulaziz S R Bati and Huihui Zhu and Luke Grater and Shreyash Sudhakar Hadke and Chuying Huang and Vinod K Sangwan and Tong Cai and Donghoon Shin and Lin X Chen and Mark C Hersam and Chad A Mirkin and Bin Chen and Mercouri G Kanatzidis and Edward H Sargent},
  journal  = {Science},
  title    = {Bimolecularly passivated interface enables efficient and stable inverted perovskite solar cells},
  year     = {2023},
  number   = {6672},
  pages    = {810-815},
  volume   = {382},
  abstract = {Compared with the n-i-p structure, inverted (p-i-n) perovskite solar cells (PSCs) promise increased operating stability, but these photovoltaic cells often exhibit lower power conversion efficiencies (PCEs) because of nonradiative recombination losses, particularly at the perovskite/C<sub>60</sub> interface. We passivated surface defects and enabled reflection of minority carriers from the interface into the bulk using two types of functional molecules. We used sulfur-modified methylthio molecules to passivate surface defects and suppress recombination through strong coordination and hydrogen bonding, along with diammonium molecules to repel minority carriers and reduce contact-induced interface recombination achieved through field-effect passivation. This approach led to a fivefold longer carrier lifetime and one-third the photoluminescence quantum yield loss and enabled a certified quasi-steady-state PCE of 25.1% for inverted PSCs with stable operation at 65°C for &gt;2000 hours in ambient air. We also fabricated monolithic all-perovskite tandem solar cells with 28.1% PCE.},
  doi      = {10.1126/science.adk1633},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1166074282},
}

@Article{Chen2023,
  author   = {Rui Chen and Jianan Wang and Zonghao Liu and Fumeng Ren and Sanwan Liu and Jing Zhou and Haixin Wang and Xin Meng and Zheng Zhang and Xinyu Guan and Wenxi Liang and Pavel A. Troshin and Yabing Qi and Liyuan Han and Wei Chen},
  journal  = {Nature Energy},
  title    = {Reduction of bulk and surface defects in inverted methylammonium- and bromide-free formamidinium perovskite solar cells},
  year     = {2023},
  number   = {8},
  pages    = {839-849},
  volume   = {8},
  abstract = {Power conversion efficiencies of inverted perovskite solar cells (PSCs) based on methylammonium- and bromide-free formamidinium lead triiodide (FAPbI3) perovskites still lag behind PSCs with a regular configuration. Here we improve the quality of both the bulk and surface of FA0.98Cs0.02PbI3 perovskite films to reduce the efficiency gap. First, we use dibutyl sulfoxide, a Lewis base additive, to improve the crystallinity and reduce the defect density and internal residual stress of the perovskite bulk. Then, we treat the surface of the perovskite film with trifluorocarbon-modified phenethylammonium iodide to optimize the energy levels, passivate defects and protect the film against moisture. The inverted PSCs simultaneously achieve 25.1% efficiency (24.5% from the reverse current–voltage scan measured by a third-party institution) and improved stability. The devices maintained 97.4% and 98.2% of their initial power conversion efficiencies after operating under continuous 1-sun air mass 1.5 G illumination for 1,800 h and under damp heat conditions (85 °C and 85% relative humidity) for 1,000 h, respectively.},
  doi      = {10.1038/s41560-023-01288-7},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1160324074},
}

@Article{Jiang2023,
  author   = {Qi Jiang and Robert Tirawat and Ross A. Kerner and E. Ashley Gaulding and Yeming Xian and Xiaoming Wang and Jimmy M. Newkirk and Yanfa Yan and Joseph J. Berry and Kai Zhu},
  journal  = {Nature},
  title    = {Towards linking lab and field lifetimes of perovskite solar cells},
  year     = {2023},
  number   = {7986},
  pages    = {313-318},
  volume   = {623},
  abstract = {Metal halide perovskite solar cells (PSCs) represent a promising low-cost thin-film photovoltaic technology, with unprecedented power conversion efficiencies obtained for both single-junction and tandem applications1–8. To push PSCs towards commercialization, it is critical, albeit challenging, to understand device reliability under real-world outdoor conditions where multiple stress factors (for example, light, heat and humidity) coexist, generating complicated degradation behaviours9–13. To quickly guide PSC development, it is necessary to identify accelerated indoor testing protocols that can correlate specific stressors with observed degradation modes in fielded devices. Here we use a state-of-the-art positive-intrinsic-negative (p–i–n) PSC stack (with power conversion efficiencies of up to approximately 25.5%) to show that indoor accelerated stability tests can predict our six-month outdoor ageing tests. Device degradation rates under illumination and at elevated temperatures are most instructive for understanding outdoor device reliability. We also find that the indium tin oxide/self-assembled monolayer-based hole transport layer/perovskite interface most strongly affects our device operation stability. Improving the ion-blocking properties of the self-assembled monolayer hole transport layer increases averaged device operational stability at 50 °C–85 °C by a factor of about 2.8, reaching over 1,000 h at 85 °C and to near 8,200 h at 50 °C, with a projected 20% degradation, which is among the best to date for high-efficiency p–i–n PSCs14–17.},
  doi      = {10.1038/s41586-023-06610-7},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1163986255},
}

@Article{Xiao2022,
  author   = {Ke Xiao and Yen-Hung Lin and Mei Zhang and Robert D J Oliver and Xi Wang and Zhou Liu and Xin Luo and Jia Li and Donny Lai and Haowen Luo and Renxing Lin and Jun Xu and Yi Hou and Henry J Snaith and Hairen Tan},
  journal  = {Science},
  title    = {Scalable processing for realizing 21.7%-efficient all-perovskite tandem solar modules},
  year     = {2022},
  note     = {https://www.science.org/doi/pdf/10.1126/science.abn7696?download=true},
  number   = {6594},
  pages    = {762-767},
  volume   = {376},
  abstract = {Challenges in fabricating all-perovskite tandem solar cells as modules rather than as single-junction configurations include growing high-quality wide-bandgap perovskites and mitigating irreversible degradation caused by halide and metal interdiffusion at the interconnecting contacts. We demonstrate efficient all-perovskite tandem solar modules using scalable fabrication techniques. By systematically tuning the cesium ratio of a methylammonium-free 1.8-electron volt mixed-halide perovskite, we improve the homogeneity of crystallization for blade-coated films over large areas. An electrically conductive conformal "diffusion barrier" is introduced between interconnecting subcells to improve the power conversion efficiency (PCE) and stability of all-perovskite tandem solar modules. Our tandem modules achieve a certified PCE of 21.7% with an aperture area of 20 square centimeters and retain 75% of their initial efficiency after 500 hours of continuous operation under simulated 1-sun illumination.},
  doi      = {10.1126/science.abn7696},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1147833193},
}

@Article{Dong2022,
  author   = {Yao Dong and Wenjian Shen and Wei Dong and Cong Bai and Juan Zhao and Yecheng Zhou and Fuzhi Huang and Yi‐Bing Cheng and Jie Zhong},
  journal  = {Advanced Energy Materials},
  title    = {Chlorobenzenesulfonic Potassium Salts as the Efficient Multifunctional Passivator for the Buried Interface in Regular Perovskite Solar Cells},
  year     = {2022},
  number   = {20},
  volume   = {12},
  abstract = {Abstract  The interfacial properties for the buried junctions of the perovskite solar cells (PSCs) play a crucial role for the further enhancement of the power conversion efficiency (PCE) and stability of devices. Delicate manipulation of the interface properties such as the defect density, energy alignment, perovskite film quality, etc., guarantees efficient extraction and transport of photogenerated carriers. Herein, chlorobenzenesulfonic potassium salts are presented as a novel multifunctional agent to modify the buried tin oxide (SnO 2 )/perovskite interface for regular PSCs. The increasing number of carbon‐chlorine bonds (CCl) in 2,4,5‐trichlorobenzenesulfonic potassium (3Cl‐BSAK) exhibit efficient interaction with uncoordinated Sn, effectively filling oxygen vacancies in the SnO 2 surface. Importantly, synergistic effects of the functional group‐rich organic anions and the potassium ion are achieved for reduced defect density, carrier recombination, and hysteresis. A champion PCE of 24.27% and the open‐circuit voltage ( V OC ) up to 1.191 V for modified devices are obtained. The unencapsulated devices maintain 80% of their initial PCE after aging at 80 °C for 800 h in the atmosphere and 95% after aging for 100 d. With 3Cl‐BSAK decoration, a high efficiency semitransparent PSC with a PCE of 12.83% and an average visible light transmittance (AVT) over 27% is also obtained.},
  doi      = {10.1002/aenm.202200417},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1146620041},
}

@Article{Luo2022,
  author   = {Chao Luo and Guanhaojie Zheng and Feng Gao and Xianjin Wang and Yao Zhao and Xingyu Gao and Qing Zhao},
  journal  = {Joule},
  title    = {Facet orientation tailoring via 2D-seed- induced growth enables highly efficient and stable perovskite solar cells},
  year     = {2022},
  note     = {http://www.cell.com/article/S2542435121005754/pdf},
  number   = {1},
  pages    = {240-257},
  volume   = {6},
  abstract = {Adding 2D perovskite to 3D perovskite has been developed to effectively enhance the intrinsic stability, but it will inevitably compromise the power conversion efficiency (PCE). Here, in a novel way, we introduced highly oriented 2D (BDA)PbI4 perovskites as seeds to optimize the growth kinetics of 3D perovskite and make its crystallization directly stride over the nucleation stage. Therefore, the seeds preferentially act as templates to epitaxially grow 3D perovskite with the desired facet orientation and stacking mode. Moreover, the 2D seeds are transformed into the grain boundary after completing the orientation-induced growth. As a result, the high-quality mixed-dimensional perovskite film delivers a superior PCE of 23.95%, accompanied by a remarkable FF of 0.847. The unencapsulated Perovskite solar cell (PSC) retains 93% of its original efficiency after 1,056 h of storage and retains 91% of its initial efficiency after 500 h of operation at the maximum power point under simulated AM1.5 illumination at 60°C.},
  doi      = {10.1016/j.joule.2021.12.006},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1144465306},
}

@Article{Tan2022,
  author   = {Shan Tan and Bingcheng Yu and Yuqi Cui and Fanqi Meng and Chunjie Huang and Yiming Li and Zijing Chen and Huijue Wu and Jiangjian Shi and Yanhong Luo and Dongmei Li and Qingbo Meng},
  journal  = {Angewandte Chemie International Edition},
  title    = {Temperature‐Reliable Low‐Dimensional Perovskites Passivated Black‐Phase CsPbI3 toward Stable and Efficient Photovoltaics},
  year     = {2022},
  number   = {23},
  pages    = {e202201300},
  volume   = {61},
  abstract = {Low-dimensional (LD) perovskites can effectively passivate and stabilize 3D perovskites for high-performance perovskite solar cells (PSCs). Regards CsPbI<sub>3</sub> -based PSCs, the influence of high-temperature annealing on the LD perovskite passivation effect has to be taken into account due to fact the black-phase CsPbI<sub>3</sub> crystallization requires high-temperature treatment, however, which has been rarely concerned so far. Here, the thermal stability of LD perovskites based on three hydrophobic organic ammonium salts and their passivation effect toward CsPbI<sub>3</sub> and the whole device performance, have been investigated. It is found that, phenyltrimethylammonium iodide (PTAI) and its corresponding LD perovskites exhibit excellent thermal stability. Further investigation reveals that PTAI-based LD perovskites are mainly distributed at grain boundaries, which not only enhances the phase stability of CsPbI<sub>3</sub> but also effectively suppresses non-radiative recombination. As a consequence, the champion PSC device based on CsPbI<sub>3</sub> exhibits a record efficiency of 21.0 % with high stability.},
  doi      = {10.1002/anie.202201300},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1146009839},
}

@Article{Li2023a,
  author   = {Guixiang Li and Zhenhuang Su and Laura Canil and Declan Hughes and Mahmoud H Aldamasy and Janardan Dagar and Sergei Trofimov and Luyao Wang and Weiwei Zuo and José J Jerónimo-Rendon and Mahdi Malekshahi Byranvand and Chenyue Wang and Rui Zhu and Zuhong Zhang and Feng Yang and Giuseppe Nasti and Boris Naydenov and Wing C Tsoi and Zhe Li and Xingyu Gao and Zhaokui Wang and Yu Jia and Eva Unger and Michael Saliba and Meng Li and Antonio Abate},
  journal  = {Science},
  title    = {Highly efficient p-i-n perovskite solar cells that endure temperature variations},
  year     = {2023},
  note     = {https://cronfa.swan.ac.uk/Record/cronfa62477/Download/62477__26444__eb1b921aa7ef44fd8aefd71b14400751.pdf},
  number   = {6630},
  pages    = {399-403},
  volume   = {379},
  abstract = {Daily temperature variations induce phase transitions and lattice strains in halide perovskites, challenging their stability in solar cells. We stabilized the perovskite black phase and improved solar cell performance using the ordered dipolar structure of β-poly(1,1-difluoroethylene) to control perovskite film crystallization and energy alignment. We demonstrated p-i-n perovskite solar cells with a record power conversion efficiency of 24.6% over 18 square millimeters and 23.1% over 1 square centimeter, which retained 96 and 88% of the efficiency after 1000 hours of 1-sun maximum power point tracking at 25° and 75°C, respectively. Devices under rapid thermal cycling between -60° and +80°C showed no sign of fatigue, demonstrating the impact of the ordered dipolar structure on the operational stability of perovskite solar cells.},
  doi      = {10.1126/science.add7331},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1154888847},
}

@Article{Zhang2022,
  author   = {Tiankai Zhang and Feng Wang and Hak-Beom Kim and In-Woo Choi and Chuanfei Wang and Eunkyung Cho and Rafal Konefal and Yuttapoom Puttisong and Kosuke Terado and Libor Kobera and Mengyun Chen and Mei Yang and Sai Bai and Bowen Yang and Jiajia Suo and Shih-Chi Yang and Xianjie Liu and Fan Fu and Hiroyuki Yoshida and Weimin M Chen and Jiri Brus and Veaceslav Coropceanu and Anders Hagfeldt and Jean-Luc Brédas and Mats Fahlman and Dong Suk Kim and Zhangjun Hu and Feng Gao},
  journal  = {Science},
  title    = {Ion-modulated radical doping of spiro-OMeTAD for more efficient and stable perovskite solar cells},
  year     = {2022},
  note     = {https://www.dora.lib4ri.ch/empa/islandora/object/empa%3A30297/datastream/PDF3/Zhang-2022-Ion-modulated_radical_doping_of_spiro-OMeTAD-%28accepted_version%29.pdf},
  number   = {6605},
  pages    = {495-501},
  volume   = {377},
  abstract = {Record power conversion efficiencies (PCEs) of perovskite solar cells (PSCs) have been obtained with the organic hole transporter 2,2',7,7'-tetrakis(<i>N</i>,<i>N</i>-di-<i>p</i>-methoxyphenyl-amine)9,9'-spirobifluorene (spiro-OMeTAD). Conventional doping of spiro-OMeTAD with hygroscopic lithium salts and volatile 4-<i>tert</i>-butylpyridine is a time-consuming process and also leads to poor device stability. We developed a new doping strategy for spiro-OMeTAD that avoids post-oxidation by using stable organic radicals as the dopant and ionic salts as the doping modulator (referred to as ion-modulated radical doping). We achieved PCEs of &gt;25% and much-improved device stability under harsh conditions. The radicals provide hole polarons that instantly increase the conductivity and work function (WF), and ionic salts further modulate the WF by affecting the energetics of the hole polarons. This organic semiconductor doping strategy, which decouples conductivity and WF tunability, could inspire further optimization in other optoelectronic devices.},
  doi      = {10.1126/science.abo2757},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1149826171},
}

@Article{Ni2021,
  author   = {Zhenyi Ni and Haoyang Jiao and Chengbin Fei and Hangyu Gu and Shuang Xu and Zhenhua Yu and Guang Yang and Yehao Deng and Qi Jiang and Ye Liu and Yanfa Yan and Jinsong Huang},
  journal  = {Nature Energy},
  title    = {Evolution of defects during the degradation of metal halide perovskite solar cells under reverse bias and illumination},
  year     = {2021},
  number   = {1},
  pages    = {65-73},
  volume   = {7},
  abstract = {The efficiency and stability of perovskite solar cells are essentially determined by defects in the perovskite layer, yet their chemical nature and linking with the degradation mechanism of devices remain unclear. Here we uncover where degradation occurs and the underlying mechanisms and defects involved in the performance degradation of p–i–n perovskite solar cells under illumination or reverse bias. Light-induced degradation starts with the generation of iodide interstitials at the interfacial region between the perovskite and both charge transport layers. While we observe trap annihilation of two types of iodide defect at the anode side, we find negatively charged iodide interstitials near the cathode side, which we show to be more detrimental to the solar cell efficiency. The reverse-bias degradation is initialized by the interaction between iodide interstitials and injected holes at the interface between the electron transport layer and the perovskite. Introducing a hole-blocking layer between the layers suppresses this interaction, improving the reverse-bias stability.},
  doi      = {10.1038/s41560-021-00949-9},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1144054488},
}

@Article{Shao2021,
  author   = {Ming Shao and Tong Bie and Lvpeng Yang and Yerun Gao and Xing Jin and Feng He and Nan Zheng and Yu Yu and Xinliang Zhang},
  journal  = {Advanced Materials},
  title    = {Over 21% Efficiency Stable 2D Perovskite Solar Cells},
  year     = {2021},
  number   = {1},
  pages    = {e2107211},
  volume   = {34},
  abstract = {Owing to their insufficient light absorption and charge transport, 2D Ruddlesden-Popper (RP) perovskites show relatively low efficiency. In this work, methylammonium (MA), formamidinum (FA), and FA/MA mixed 2D perovskite solar cells (PSCs) are fabricated. Incorporating FA cations extends the absorption range and enhances the light absorption. Optical spectroscopy shows that FA cations substantially increase the portion of 3D-like phase to 2D phases, and X-ray diffraction (XRD) studies reveal that FA-based 2D perovskite possesses an oblique crystal orientation. Nevertheless, the ultrafast interphase charge transfer results in an extremely long carrier-diffusion length (≈1.98 µm). Also, chloride additives effectively suppress the yellow δ-phase formation of pure FA-based 2D PSCs. As a result, both FA/MA mixed and pure FA-based 2D PSCs exhibit a greatly enhanced power conversion efficiency (PCE) over 20%. Specifically, the pure FA-based 2D PSCs achieve a record PCE of 21.07% (certified at 20%), which is the highest efficiency for low-dimensional PSCs (n ≤ 10) reported to date. Importantly, the FA-based 2D PSCs retain 97% of their initial efficiency at 85 °C persistent heating after 1500 h. The results unambiguously demonstrate that pure-FA-based 2D PSCs are promising for achieving comparable efficiency to 3D perovskites, along with a better device stability.},
  doi      = {10.1002/adma.202107211},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1141876155},
}

@Article{Li2023b,
  author   = {Chongwen Li and Xiaoming Wang and Enbing Bi and Fangyuan Jiang and So Min Park and You Li and Lei Chen and Zaiwei Wang and Lewei Zeng and Hao Chen and Yanjiang Liu and Corey R Grice and Abasi Abudulimu and Jaehoon Chung and Yeming Xian and Tao Zhu and Huagui Lai and Bin Chen and Randy J Ellingson and Fan Fu and David S Ginger and Zhaoning Song and Edward H Sargent and Yanfa Yan},
  journal  = {Science},
  title    = {Rational design of Lewis base molecules for stable and efficient inverted perovskite solar cells},
  year     = {2023},
  note     = {https://www.dora.lib4ri.ch/empa/islandora/object/empa%3A34293/datastream/PDF2/Li-2023-Rational_design_of_Lewis_base-%28accepted_version%29.pdf},
  number   = {6633},
  pages    = {690-694},
  volume   = {379},
  abstract = {Lewis base molecules that bind undercoordinated lead atoms at interfaces and grain boundaries (GBs) are known to enhance the durability of metal halide perovskite solar cells (PSCs). Using density functional theory calculations, we found that phosphine-containing molecules have the strongest binding energy among members of a library of Lewis base molecules studied herein. Experimentally, we found that the best inverted PSC treated with 1,3-bis(diphenylphosphino)propane (DPPP), a diphosphine Lewis base that passivates, binds, and bridges interfaces and GBs, retained a power conversion efficiency (PCE) slightly higher than its initial PCE of ~23% after continuous operation under simulated AM1.5 illumination at the maximum power point and at ~40°C for >3500 hours. DPPP-treated devices showed a similar increase in PCE after being kept under open-circuit conditions at 85°C for >1500 hours.},
  doi      = {10.1126/science.ade3970},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1155457945},
}

@Article{Liang2022,
  author   = {Jiwei Liang and Xuzhi Hu and Chen Wang and Chao Liang and Cong Chen and Meng Xiao and Jiashuai Li and Chen Tao and Guichuan Xing and Rui Yu and Weijun Ke and Guojia Fang},
  journal  = {Joule},
  title    = {Origins and influences of metallic lead in perovskite solar cells},
  year     = {2022},
  note     = {https://doi.org/10.1016/j.joule.2022.03.005},
  number   = {4},
  pages    = {816-833},
  volume   = {6},
  abstract = {Metallic lead (Pb0) impurities in metal-halide perovskites have attracted tremendous research concerns owing to their detrimental effects on perovskite solar cells (PSCs). However, the origins and influences of the Pb0 behind this issue have yet to be well understood. Herein, we show that Pb0 is hardly formed in the growth of halide perovskites but is easily postformed in the perovskite films with excess PbI2. It is found that Pb0 impurities are decomposition byproducts of residual PbI2 in perovskites under light or X-ray irradiation. Therefore, PSCs obtained using photodegraded PbI2 films show large efficiency and stability losses. By contrast, the perovskite devices without detectable Pb0 impurities have a much better efficiency and stability. This work reveals the origins and influences of Pb0 in halide perovskites and provides a strategy for avoiding the formation of detrimental Pb0 byproducts, which would drive further enhancements in device performance of halide perovskite solar cells, detectors, etc.},
  doi      = {10.1016/j.joule.2022.03.005},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1147021977},
}

@Article{Brinkmann2022,
  author   = {K. O. Brinkmann and T. Becker and F. Zimmermann and C. Kreusel and T. Gahlmann and M. Theisen and T. Haeger and S. Olthof and C. Tückmantel and M. Günster and T. Maschwitz and F. Göbelsmann and C. Koch and D. Hertel and P. Caprioglio and F. Peña-Camargo and L. Perdigón-Toro and A. Al-Ashouri and L. Merten and A. Hinderhofer and L. Gomell and S. Zhang and F. Schreiber and S. Albrecht and K. Meerholz and D. Neher and M. Stolterfoht and T. Riedl},
  journal  = {Nature},
  title    = {Perovskite–organic tandem solar cells with indium oxide interconnect},
  year     = {2022},
  number   = {7905},
  pages    = {280-286},
  volume   = {604},
  abstract = {Multijunction solar cells can overcome the fundamental efficiency limits of single-junction devices. The bandgap tunability of metal halide perovskite solar cells renders them attractive for multijunction architectures1. Combinations with silicon and copper indium gallium selenide (CIGS), as well as all-perovskite tandem cells, have been reported2–5. Meanwhile, narrow-gap non-fullerene acceptors have unlocked skyrocketing efficiencies for organic solar cells6,7. Organic and perovskite semiconductors are an attractive combination, sharing similar processing technologies. Currently, perovskite–organic tandems show subpar efficiencies and are limited by the low open-circuit voltage (Voc) of wide-gap perovskite cells8 and losses introduced by the interconnect between the subcells9,10. Here we demonstrate perovskite–organic tandem cells with an efficiency of 24.0 per cent (certified 23.1 per cent) and a high Voc of 2.15 volts. Optimized charge extraction layers afford perovskite subcells with an outstanding combination of high Voc and fill factor. The organic subcells provide a high external quantum efficiency in the near-infrared and, in contrast to paradigmatic concerns about limited photostability of non-fullerene cells11, show an outstanding operational stability if excitons are predominantly generated on the non-fullerene acceptor, which is the case in our tandems. The subcells are connected by an ultrathin (approximately 1.5 nanometres) metal-like indium oxide layer with unprecedented low optical/electrical losses. This work sets a milestone for perovskite–organic tandems, which outperform the best p–i–n perovskite single junctions12 and are on a par with perovskite–CIGS and all-perovskite multijunctions13.},
  doi      = {10.1038/s41586-022-04455-0},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1147054891},
}

@Article{Yang2022,
  author   = {Lu Yang and Jiangshan Feng and Zhike Liu and Yuwei Duan and Sheng Zhan and Shaomin Yang and Kun He and Yong Li and Yawei Zhou and Ningyi Yuan and Jianning Ding and Shengzhong Liu},
  journal  = {Advanced Materials},
  title    = {Record‐Efficiency Flexible Perovskite Solar Cells Enabled by Multifunctional Organic Ions Interface Passivation},
  year     = {2022},
  number   = {24},
  pages    = {e2201681},
  volume   = {34},
  abstract = {Flexible perovskite solar cells (f-PSCs) have attracted great attention because of their unique advantages in lightweight and portable electronics applications. However, their efficiencies are far inferior to those of their rigid counterparts. Herein, a novel histamine diiodate (HADI) is designed based on theoretical study to modify the SnO<sub>2</sub> /perovskite interface. Systematic experimental results reveal that the HADI serves effectively as a multifunctional agent mainly in three aspects: 1) surface modification to realign the SnO<sub>2</sub> conduction band upward to improve interfacial charge extraction; 2) passivating the buried perovskite surface, and 3) bridging between the SnO<sub>2</sub> and perovskite layers for effective charge transfer. Consequently, the rigid MA-free PSCs based on the HADI-SnO<sub>2</sub> electron transport layer (ETL) display not only a high champion power conversion efficiency (PCE) of 24.79% and open-circuit voltage (V<sub>OC</sub> ) of 1.20 V but also outstanding stability as demonstrated by the PSCs preserving 91% of their initial efficiencies after being exposed to ambient atmosphere for 1200 h without any encapsulation. Furthermore, the solution-processed HADI-SnO<sub>2</sub> ETL formed at low temperature (100 °C) is utilized in f-PSCs that achieve a PCE as high as 22.44%, the highest reported PCE for f-PSCs to date.},
  doi      = {10.1002/adma.202201681},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1147185686},
}

@Article{Zhang2023,
  author   = {Shuo Zhang and Fangyuan Ye and Xiaoyu Wang and Rui Chen and Huidong Zhang and Liqing Zhan and Xianyuan Jiang and Yawen Li and Xiaoyu Ji and Shuaijun Liu and Miaojie Yu and Furong Yu and Yilin Zhang and Ruihan Wu and Zonghao Liu and Zhijun Ning and Dieter Neher and Liyuan Han and Yuze Lin and He Tian and Wei Chen and Martin Stolterfoht and Lijun Zhang and Wei-Hong Zhu and Yongzhen Wu},
  journal  = {Science},
  title    = {Minimizing buried interfacial defects for efficient inverted perovskite solar cells},
  year     = {2023},
  number   = {6643},
  pages    = {404-409},
  volume   = {380},
  abstract = {Controlling the perovskite morphology and defects at the buried perovskite-substrate interface is challenging for inverted perovskite solar cells. In this work, we report an amphiphilic molecular hole transporter, (2-(4-(bis(4-methoxyphenyl)amino)phenyl)-1-cyanovinyl)phosphonic acid, that features a multifunctional cyanovinyl phosphonic acid group and forms a superwetting underlayer for perovskite deposition, which enables high-quality perovskite films with minimized defects at the buried interface. The resulting perovskite film has a photoluminescence quantum yield of 17% and a Shockley-Read-Hall lifetime of nearly 7 microseconds and achieved a certified power conversion efficiency (PCE) of 25.4% with an open-circuit voltage of 1.21 volts and a fill factor of 84.7%. In addition, 1-square centimeter cells and 10-square centimeter minimodules show PCEs of 23.4 and 22.0%, respectively. Encapsulated modules exhibited high stability under both operational and damp heat test conditions.},
  doi      = {10.1126/science.adg3755},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1157586280},
}

@Article{Tan2022a,
  author   = {Shaun Tan and Tianyi Huang and Ilhan Yavuz and Rui Wang and Tae Woong Yoon and Mingjie Xu and Qiyu Xing and Keonwoo Park and Do-Kyoung Lee and Chung-Hao Chen and Ran Zheng and Taegeun Yoon and Yepin Zhao and Hao-Cheng Wang and Dong Meng and Jingjing Xue and Young Jae Song and Xiaoqing Pan and Nam-Gyu Park and Jin-Wook Lee and Yang Yang},
  journal  = {Nature},
  title    = {Stability-limiting heterointerfaces of perovskite photovoltaics},
  year     = {2022},
  number   = {7909},
  pages    = {268-273},
  volume   = {605},
  abstract = {Optoelectronic devices consist of heterointerfaces formed between dissimilar semiconducting materials. The relative energy-level alignment between contacting semiconductors determinately affects the heterointerface charge injection and extraction dynamics. For perovskite solar cells (PSCs), the heterointerface between the top perovskite surface and a charge-transporting material is often treated for defect passivation1–4 to improve the PSC stability and performance. However, such surface treatments can also affect the heterointerface energetics1. Here we show that surface treatments may induce a negative work function shift (that is, more n-type), which activates halide migration to aggravate PSC instability. Therefore, despite the beneficial effects of surface passivation, this detrimental side effect limits the maximum stability improvement attainable for PSCs treated in this way. This trade-off between the beneficial and detrimental effects should guide further work on improving PSC stability via surface treatments.},
  doi      = {10.1038/s41586-022-04604-5},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1146288953},
}

@Article{Azmi2022,
  author   = {Randi Azmi and Esma Ugur and Akmaral Seitkhan and Faisal Aljamaan and Anand S Subbiah and Jiang Liu and George T Harrison and Mohamad I Nugraha and Mathan K Eswaran and Maxime Babics and Yuan Chen and Fuzong Xu and Thomas G Allen and Atteq Ur Rehman and Chien-Lung Wang and Thomas D Anthopoulos and Udo Schwingenschlögl and Michele De Bastiani and Erkan Aydin and Stefaan De Wolf},
  journal  = {Science},
  title    = {Damp heat–stable perovskite solar cells with tailored-dimensionality 2D/3D heterojunctions},
  year     = {2022},
  number   = {6588},
  pages    = {73-77},
  volume   = {376},
  abstract = {If perovskite solar cells (PSCs) with high power conversion efficiencies (PCEs) are to be commercialized, they must achieve long-term stability, which is usually assessed with accelerated degradation tests. One of the persistent obstacles for PSCs has been successfully passing the damp-heat test (85°C and 85% relative humidity), which is the standard for verifying the stability of commercial photovoltaic (PV) modules. We fabricated damp heat-stable PSCs by tailoring the dimensional fragments of two-dimensional perovskite layers formed at room temperature with oleylammonium iodide molecules; these layers passivate the perovskite surface at the electron-selective contact. The resulting inverted PSCs deliver a 24.3% PCE and retain >95% of their initial value after >1000 hours at damp-heat test conditions, thereby meeting one of the critical industrial stability standards for PV modules.},
  doi      = {10.1126/science.abm5784},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1145696652},
}

@Article{Li2022,
  author   = {Xiaodong Li and Wenxiao Zhang and Xuemin Guo and Chunyan Lu and Jiyao Wei and Junfeng Fang},
  journal  = {Science},
  title    = {Constructing heterojunctions by surface sulfidation for efficient inverted perovskite solar cells},
  year     = {2022},
  number   = {6579},
  pages    = {434-437},
  volume   = {375},
  abstract = {A stable perovskite heterojunction was constructed for inverted solar cells through surface sulfidation of lead (Pb)-rich perovskite films. The formed lead-sulfur (Pb-S) bonds upshifted the Fermi level at the perovskite interface and induced an extra back-surface field for electron extraction. The resulting inverted devices exhibited a power conversion efficiency (PCE) >24% with a high open-circuit voltage of 1.19 volts, corresponding to a low voltage loss of 0.36 volts. The strong Pb-S bonds could stabilize perovskite heterojunctions and strengthen underlying perovskite structures that have a similar crystal lattice. Devices with surface sulfidation retained more than 90% of the initial PCE after aging at 85°C for 2200 hours or operating at the maximum power point under continuous illumination for 1000 hours at 55° ± 5°C.},
  doi      = {10.1126/science.abl5676},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1145043743},
}

@Article{Li2022a,
  author   = {Zhen Li and Bo Li and Xin Wu and Stephanie A Sheppard and Shoufeng Zhang and Danpeng Gao and Nicholas J Long and Zonglong Zhu},
  journal  = {Science},
  title    = {Organometallic-functionalized interfaces for highly efficient inverted perovskite solar cells},
  year     = {2022},
  number   = {6591},
  pages    = {416-420},
  volume   = {376},
  abstract = {Further enhancing the performance and stability of inverted perovskite solar cells (PSCs) is crucial for their commercialization. We report that the functionalization of multication and halide perovskite interfaces with an organometallic compound, ferrocenyl-bis-thiophene-2-carboxylate (FcTc<sub>2</sub>), simultaneously enhanced the efficiency and stability of inverted PSCs. The resultant devices achieved a power conversion efficiency of 25.0% and maintained &gt;98% of their initial efficiency after continuously operating at the maximum power point for 1500 hours under simulated AM1.5 illumination. Moreover, the FcTc<sub>2</sub>-functionalized devices passed the international standards for mature photovoltaics (IEC61215:2016) and have exhibited high stability under the damp heat test (85°C and 85% relative humidity).},
  doi      = {10.1126/science.abm8566},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1147293706},
}

@Article{Kim2022,
  author   = {Minjin Kim and Jaeki Jeong and Haizhou Lu and Tae Kyung Lee and Felix T Eickemeyer and Yuhang Liu and In Woo Choi and Seung Ju Choi and Yimhyun Jo and Hak-Beom Kim and Sung-In Mo and Young-Ki Kim and Heunjeong Lee and Na Gyeong An and Shinuk Cho and Wolfgang R Tress and Shaik M Zakeeruddin and Anders Hagfeldt and Jin Young Kim and Michael Grätzel and Dong Suk Kim},
  journal  = {Science},
  title    = {Conformal quantum dot–SnO2 layers as electron transporters for efficient perovskite solar cells},
  year     = {2022},
  note     = {https://www.science.org/cms/asset/2af6a5f6-af93-41f7-8e90-1fd8b3db01b2/science.abh1885.v1.pdf},
  number   = {6578},
  pages    = {302-306},
  volume   = {375},
  abstract = {Improvements to perovskite solar cells (PSCs) have focused on increasing their power conversion efficiency (PCE) and operational stability and maintaining high performance upon scale-up to module sizes. We report that replacing the commonly used mesoporous-titanium dioxide electron transport layer (ETL) with a thin layer of polyacrylic acid-stabilized tin(IV) oxide quantum dots (paa-QD-SnO<sub>2</sub>) on the compact-titanium dioxide enhanced light capture and largely suppressed nonradiative recombination at the ETL-perovskite interface. The use of paa-QD-SnO<sub>2</sub> as electron-selective contact enabled PSCs (0.08 square centimeters) with a PCE of 25.7% (certified 25.4%) and high operational stability and facilitated the scale-up of the PSCs to larger areas. PCEs of 23.3, 21.7, and 20.6% were achieved for PSCs with active areas of 1, 20, and 64 square centimeters, respectively.},
  doi      = {10.1126/science.abh1885},
  groups   = {Perovskite Solar Cells Stability},
  url      = {https://app.dimensions.ai/details/publication/pub.1144827854},
}

@Article{Fuechtenhans2021,
  author   = {Marc Füchtenhans and Christoph H. Glock and Eric H. Grosse and Simone Zanoni},
  journal  = {International Journal of Logistics Research and Applications},
  title    = {Using smart lighting systems to reduce energy costs in warehouses: A simulation study},
  year     = {2021},
  number   = {1},
  pages    = {77-95},
  volume   = {26},
  abstract = {Despite the various technical solutions for making lighting ‘smart,’ today’s lighting systems are often kept simple, and they are frequently not adjusted to user behaviours. This is especially the case for production and logistics facilities such as warehouses, where large areas have to be illuminated, and where lighting is often fully turned on while the warehouse operates. This paper presents a simulation model developed to evaluate the cost benefits potentially resulting from using smart lighting systems in warehouses. The simulation model allows for varying warehouse design and order picking process parameters. In addition, three different operating strategies for lighting systems representing different types of smart lighting technologies are implemented and compared to a conventional lighting system. A structured simulation study provides insights into how smart lighting systems interact with system design and process parameters, and how both collectively influence warehouse operating costs. The results of the simulation model and data obtained from a practical case indicate that smart lighting systems have great potential for reducing the energy consumption in warehouses relative to conventional lighting, and that, in addition to savings in cost, they can contribute to improving the environmental footprints of warehouses.},
  doi      = {10.1080/13675567.2021.1937967},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1138791522},
}

@Article{Molleti2020,
  author   = {Sudhakar Molleti and Marianne Armstrong},
  journal  = {Intelligent Buildings International},
  title    = {Smart energy harvesting performance of photovoltaic roof assemblies in Canadian climate},
  year     = {2020},
  number   = {1},
  pages    = {70-88},
  volume   = {13},
  abstract = {In Canada, the solar electricity sector is growing rapidly. Much of this success is based on the growth of the Ontario solar market where more than 99% of Canada’s solar electricity is generated. Ontario has developed a globally recognized solar market sector. The vast surface area of existing residential roofs across Canada represents an untapped resource for capitalizing on passive and active management of impinging solar insolation. The aim of the current research study is to evaluate the new energy harvesting technologies such as a thin-film PV integrated roof system that could serve as a conventional roofing for weather protection while generating clean solar electricity, and the new generation micro inverters that have the potential to outperform string inverters under shading and snow-cover conditions. This paper has two parts that will discuss about two smart energy harvesting technologies and their performance on residential applications in Canadian climate. Part 1 of the paper focusses on field evaluation of Roof Integrated Photovoltaic (RIPV) and Part 2 talks about the energy yield performance of integrated solar tiles and new generation micro inverters. The RIPV field trial took place at the Canadian Centre for Housing Technology (CCHT) Info Centre in Ottawa, Canada. This is a novel approach adapted from a roofing system that would typically be found on low-sloped roofs such as commercial supermarkets, industrial warehouses and school buildings. Over the eight month study period, surmounting the effects of snow cover and shadows, the RIPV system generated over 1 MWh of electricity, and had a measured system efficiency of 5.3%. The study on the new generation micro inverters for residential applications addressed the shading effects on the intermittent nature of solar energy generation. Simulating the shading conditions that are experienced by typical residential rooftop, the micro inverters were found to increase production by 1–68% relative to the conventional string inverters. The research outcome of this study has demonstrated that both these energy harvesting technologies have important incremental benefits in increasing the renewables contribution to power generation in residential homes in Canadian climatic conditions.},
  doi      = {10.1080/17508975.2020.1802694},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1130180156},
}

@Article{AgyabengMensah2020,
  author   = {Yaw Agyabeng-Mensah and Esther Ahenkorah and Ebenezer Afum and Essel Dacosta and Zhongxing Tian},
  journal  = {The International Journal of Logistics Management},
  title    = {Green warehousing, logistics optimization, social values and ethics and economic performance: the role of supply chain sustainability},
  year     = {2020},
  number   = {3},
  pages    = {549-574},
  volume   = {31},
  abstract = {Purpose This study primarily explores the influence of green warehousing, logistics optimization and social values and ethics on supply chain sustainability and economic performance. The study further examines the mediating role of supply chain sustainability between economic performance and green warehousing, logistics optimization and social values and ethics.   Design/methodology/approach The study employs a quantitative research approach where survey data are collected from 200 managers of manufacturing companies in Ghana. The dataset is analyzed using partial least square structural equation modeling software (PLS-SEM) SmartPLS 3.   Findings The results show that green warehousing and logistics optimization negatively influence economic performance but improves economic performance through supply chain sustainability. It is further discovered that social values and ethics have a positive influence on supply chain sustainability and economic performance.   Originality/value This paper proposes and tests a theoretical model that explores the relationships between green warehousing, supply chain sustainability, economic performance, logistics optimization and social values and ethics through the resource dependency theory (RDT) in the manufacturing firms in Ghana.},
  doi      = {10.1108/ijlm-10-2019-0275},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1130342326},
}

@Article{Lapisa2020,
  author   = {Remon Lapisa and Arwizet Karudin and M. Martias and K. Krismadinata and A. Ambiyar and Zaid Romani and Patrick Salagnac},
  journal  = {Asian Journal of Civil Engineering},
  title    = {Effect of skylight–roof ratio on warehouse building energy balance and thermal–visual comfort in hot-humid climate area},
  year     = {2020},
  number   = {5},
  pages    = {915-923},
  volume   = {21},
  abstract = {Skylight–roofs installation on low-rise commercial/industrial building has the potentials to reduce energy consumption through the use of daylighting, yet it may also lead to an increase in room temperature and thermal discomfort for the occupants. The purpose of this numerical study is to evaluate the effect of the skylights ratios on the roof to the energy efficiency for artificial lighting, indoor daylight illuminance level and thermal discomfort in a warehousing building in the tropics. The simulations of low-rise warehouse building are performed using different simulation tools such as TRNSYS©, CONTAM©, and Velux daylight Visualizer©. Furthermore, the optimal skylight–roof ratio to this type of buildings will be figured out by considering the aspects of energy consumption and thermal comfort of occupants. The numerical simulation results show that the ratio of the area of the skylights designed between 2.5 and 5% of the whole roof can provide lighting energy efficiency to buildings in the tropics more than 50% during the day with a reasonable increase in the degree hour thermal discomfort.},
  doi      = {10.1007/s42107-020-00249-9},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1126001650},
}

@InProceedings{Sukjit2020,
  author    = {Supapich Sukjit and Assadej Vanichchinchai},
  booktitle = {2020 IEEE 7th International Conference on Industrial Engineering and Applications (ICIEA)},
  title     = {An Assessment of Motivations on Green Warehousing in Thailand},
  year      = {2020},
  pages     = {539-542},
  abstract  = {The objectives of this research are to assess the levels of green warehousing and its motivations, and to examine the impact of motivations on green warehousing in Thailand. Green warehousing and motivations frameworks were developed from literature review, validated by experts and statistical techniques. Data was collected from 261 warehouses managers. Descriptive statistics and multiple regressions were applied for data analysis. It was found that, for green warehousing, utilities for green warehousing had the highest score; while, green management had the lowest score. For motivations, social responsibility score was the highest; while, law and regulation score was the lowest. Top management commitment had a significant effect on green warehousing.},
  doi       = {10.1109/iciea49774.2020.9102035},
  groups    = {Green Warehousing},
  url       = {https://app.dimensions.ai/details/publication/pub.1127953927},
}

@Article{Minashkina2020,
  author   = {Daria Minashkina and Ari Happonen},
  journal  = {E3S Web of Conferences},
  title    = {Decarbonizing warehousing activities through digitalization and automatization with WMS integration for sustainability supporting operations},
  year     = {2020},
  note     = {https://www.e3s-conferences.org/articles/e3sconf/pdf/2020/18/e3sconf_icepp2020_03002.pdf},
  pages    = {03002},
  volume   = {158},
  abstract = {The current strong outsourcing trend dictates that the efficiency of outsourced warehousing is a big contributing impact factor for the carbon footprint from supply chains. A good warehouse management system boosts warehousing eco-friendliest by reducing space waste and unnecessary number of material movements, to ensure continuous fast materials flow and to keep the heating and cooling costs to a minimum. All this may happen on some lengths inside the traditional manual warehousing scenes, but in fact, if the highest level of efficiency is wanted/needed, the operator shall need to automate, digitalize and robotize their operations. Authors presents an example of automation in the warehousing context, where the warehouse operator is capable of turning around an order line as an outgoing packet, even in high peak load times under 3h time period. This sort of efficiency means less needed storage space as items stay less time in the warehouse and such automatization also gives the 3PLs possibility to store materials in high rising automated systems like automated storage and retrieval systems to optimize their space usage. All combined, the future warehouse operations can make a positive impact on efforts to reduce overall CO
                    ₂
                    emissions made by supply chains.},
  doi      = {10.1051/e3sconf/202015803002},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1125842670},
}

@Article{Escriva2020,
  author   = {Emilio José Sarabia Escriva and Víctor Soto Francés and Jose Manuel Pinazo Ojer},
  journal  = {Thermal Science and Engineering Progress},
  title    = {Comparison of annual cooling energy demand between conventional and inflatable dock door shelters for refrigerated and frozen food warehouses},
  year     = {2020},
  note     = {https://riunet.upv.es/bitstream/10251/140494/2/Sarabia%3bSoto%3bPinazo%20-%20Comparison%20of%20annual%20cooling%20energy%20demand%20between%20conventional%20and%20inflata....pdf},
  pages    = {100386},
  volume   = {15},
  abstract = {The aim of this study is to estimate the energy savings potential that can be achieved using inflatable dock shelters versus simple curtain dock shelters for loading/unloading activities in logistics warehouses. The article describes how these savings have been analysed and quantified in a big logistics centre of a Spanish dealer. It takes into account different refrigeration applications (i.e. for different warehouse dock temperatures), exterior conditions and daily loading/unloading schedules and their duration. We have used mean typical years for the different Spanish climatic zones, although the procedure can be easily extended to any climate. As expected, the greatest energy savings are achieved in the warmest climates with the coldest storage temperatures and for the nocturnal operations. The savings quantification allowed us to convince the owner to replace the conventional dock shelters. Finally, the implementation of the procedure in a software-tool is helping other dealers to carry out a self-evaluation of the savings potential of their facilities.},
  doi      = {10.1016/j.tsep.2019.100386},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1120098405},
}

@Article{Ali2020,
  author   = {Yousaf Ali and Talal Bin Saad and Muhammad Sabir and Noor Muhammad and Aneel Salman and Khaqan Zeb},
  journal  = {Management of Environmental Quality An International Journal},
  title    = {Integration of green supply chain management practices in construction supply chain of CPEC},
  year     = {2020},
  number   = {1},
  pages    = {185-200},
  volume   = {31},
  abstract = {Purpose China Pakistan Economic Corridor (CPEC) projects are widely spread throughout Pakistan with the potential to have a massive impact on Pakistan’s economic future. CPEC projects have, therefore, made it imperative that green practices are adapted to provide sustainability to the CPEC projects. The adoption of green supply chain management (GSCM) framework will significantly increase the value attained from CPEC projects through the increased benefits to the socio-cultural and economic conditions of Pakistan without causing harm to the environment. The purpose of this paper is to identify and rank the GSCM practices for implementation in the construction industry of Pakistan according to expert opinion.   Design/methodology/approach This study targets the experts who are employed as supply chain managers in the different construction industries of Pakistan. The opinions of these experts have been extracted through an online questionnaire that was based on six alternatives along with four criteria. The tool of multi-criteria decision making (MCDM) that is a Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) has been used to analyze the results.   Findings Six alternatives that have been used for this study are green design, green procurement, green production, green warehousing, green transportation and green recycling. The top-ranked alternative as a practice for GSCM is green warehousing followed by green production. The lowest ranked alternative in this study is green recycling. The alternatives have been ranked on the basis of “cc” values derived through TOPSIS.   Practical implications As the advancement in the construction industry will definitely going to impact the environmental sustainability of the country, the results derived through this research will assist the managers of the construction industry of Pakistan to adopt best practices among green supply chain in order to lower their impact.   Originality/value Framework using TOPSIS in order to find the best GSCM practice in Pakistan has not been reported before this study.},
  doi      = {10.1108/meq-12-2018-0211},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1121161391},
}

@Article{Carli2020,
  author   = {Raffaele Carli and Salvatore Digiesi and Mariagrazia Dotoli and Francesco Facchini},
  journal  = {Procedia Manufacturing},
  title    = {A Control Strategy for Smart Energy Charging of Warehouse Material Handling Equipment},
  year     = {2020},
  note     = {https://doi.org/10.1016/j.promfg.2020.02.041},
  pages    = {503-510},
  volume   = {42},
  abstract = {The common driver of the ‘green-warehouse’ strategy is based on the reduction of energy consumption. In warehouses with ‘picker-to-part’ operations the minimization of energy due to material handling activities can be achieved by means of different policies: by adopting smart automatic picking systems, by adopting energy-efficient material handling equipment (MHE) as well as by identifying flexible layouts. In most cases, these strategies require investments characterized by high pay-back times. In this context, management strategies focused on the adoption of available equipment allow to increase the warehouse productivity at negligible costs. With this purpose, an optimization model is proposed in order to identify an optimal control strategy for the battery charging of a fleet of electric mobile MHE (e.g., forklifts), allowing minimizing the economic and environmental impact of material handling activities in labor-intensive warehouses. The resulting scheduling problem is formalized as an integer programming (IP) problem aimed at minimizing the total cost, which is the sum of the penalty cost related to makespan over all the material handling activities and the total electricity cost for charging batteries of MHE. Numerical experiments are used to investigate and quantify the effects of integrating the scheduling of electric loads into the scheduling of material handling operations. Click here to enter text.},
  doi      = {10.1016/j.promfg.2020.02.041},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1126155979},
}

@Article{Xin2019,
  author   = {Lew Jia Xin and Kang Ching Xien and Siti Norida Wahab},
  journal  = {E3S Web of Conferences},
  title    = {A Study on the Factors Influencing Green Warehouse Practice},
  year     = {2019},
  note     = {https://www.e3s-conferences.org/articles/e3sconf/pdf/2019/62/e3sconf_icbte2019_01040.pdf},
  pages    = {01040},
  volume   = {136},
  abstract = {Going green in logistics business operation has been the main initiatives to reduce the carbon footprint while generating profit. This study described about the green practice and the drivers in the Malaysia warehousing industry. The purpose of the study is to investigate the factors of adopting green warehousing (GWH) with the main aim of exploring current green warehouse practice (GHP) in Malaysia and the factors affecting them. The relationship between government engagement, customer, supplier, manager, employee’s engagement, technology innovation and green warehouse practice has been analyzed. Data was gathered by survey methods. The survey was established for warehouse’s employees with superior or managerial position based in Malaysia. Quantitative methods used to emphasize the objective measurements and to identify how closed the relationship are. Data were analyzed using SPSS software in terms of inferential analysis. The key findings showed that the independent variables had a strong and significant relationship with the GWP.},
  doi      = {10.1051/e3sconf/201913601040},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1123270626},
}

@Article{Ozturk2019,
  author   = {Hande Mutlu Ozturk and Omer Altan Dombayci and Hakan Caliskan},
  journal  = {Journal of Environmental Engineering},
  title    = {Life-Cycle Cost, Cooling Degree Day, and Carbon Dioxide Emission Assessments of Insulation of Refrigerated Warehouses Industry in Turkey},
  year     = {2019},
  number   = {10},
  pages    = {04019062},
  volume   = {145},
  abstract = {In this study, cooling degree day (CDD) and life cycle cost (LCC) analyses are applied to refrigerated warehouses in Turkey to determine the optimum insulation thickness. The external wall of refrigerated warehouses is considered as a sandwich wall and the insulation material is taken to be expanded polystyrene. Also, the base temperatures are assumed to be −20°C, −18°C, −12°C, 0°C, and 6°C, while coefficient of performance (COP) values are 1.2, 1.5, 1.8, 2.1, and 2.5. As a result: (1) the insulation thickness is directly proportional to the insulation cost; (2) insulation thickness is inversely proportional to the electricity cost; (3) the maximum annual energy saving is found for a COP of 1.2, and the minimum rate is determined for a COP value of 2.5; (4) the maximum and minimum energy savings are found for −20°C and 6°C, respectively; (5) the maximum total cost is found for COP=1.2; (6) CO2 emissions increase with increases in CDD for the unit surface area; and (7) an increase in COP values reduces CO2 emissions.},
  doi      = {10.1061/(asce)ee.1943-7870.0001575},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1119922340},
}

@Article{Goh2019,
  author   = {Shao Hung Goh},
  journal  = {International Journal of Physical Distribution & Logistics Management},
  title    = {Barriers to low-carbon warehousing and the link to carbon abatement},
  year     = {2019},
  number   = {6},
  pages    = {679-704},
  volume   = {49},
  abstract = {Purpose
                    Warehouses are large emitters of greenhouse gases and their impact on climate change is under increasing focus. The purpose of this paper is to investigate the barriers that inhibit the adoption of low-carbon warehousing in Asia-Pacific and their links to carbon abatement performance.
                  
                  
                    Design/methodology/approach
                    An exploratory conceptual model was first developed from a literature review of the general barriers to sustainable supply chain practices and hence potentially in low-carbon warehousing. A large contract logistics services provider in the Asia-Pacific served as the subject of a case study. The perceived barriers to low-carbon warehousing were derived from an internal survey of respondents from the case company and regressed against carbon abatement outcomes at that organization’s operations across the region.
                  
                  
                    Findings
                    Results show that the case company reduced carbon emissions by 36 percent on a revenue-normalized basis between 2008 and 2014, but with relatively lower success in emerging markets vs mature markets. An Elastic Net regression analysis confirms that technology and government-related factors are the most important barriers in the case company’s efforts to “decarbonize” its local warehousing operations. However, results suggest that the customer-related barrier, which is highly correlated with the government barrier, is in part driven by the latter.
                  
                  
                    Research limitations/implications
                    This case study is based on a single multinational company in Asia-Pacific, but nonetheless serves as an impetus for more cross-sectional studies to form an industry-wide view.
                  
                  
                    Originality/value
                    An extended stewardship framework based on the natural resource-based view has been proposed, in which logistics services providers take on a proactive boundary-spanning role to lower the external barriers to low-carbon warehousing.},
  doi      = {10.1108/ijpdlm-10-2018-0354},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1120216323},
}

@Article{Bartolini2019,
  author   = {Maicol Bartolini and Eleonora Bottani and Eric H. Grosse},
  journal  = {Journal of Cleaner Production},
  title    = {Green warehousing: Systematic literature review and bibliometric analysis},
  year     = {2019},
  pages    = {242-258},
  volume   = {226},
  abstract = {Warehouses are major contributors to the rise of greenhouse gas emissions in supply chains. Thus, it is not surprising that the attention of academic research to green and sustainable warehousing has been growing in recent years. This attention has led to an increasing number of publications in this field, which is why a systematic literature review on the topic of green warehousing is proposed in the paper at hand. This work provides a comprehensive overview and classification of the existing research on green warehousing, summarizes and synthesizes the available knowledge on this topic, and identifies key trends. Based on the evaluation of the literature, promising ideas for future research are proposed. Citation and network analyses are carried out to evaluate the relationships among the topics covered. The results show an increasing interest in sustainability topics within the warehousing literature, where energy saving has been the most frequently studied objective, followed by environmental impact of warehouse buildings, and green warehouse management in general. The green warehousing literature, however, lacks case studies and empirical data. The main contribution of this paper is an exhaustive of the state of knowledge on green warehousing in terms of the macro-themes addressed, the specific topics investigated and the methodological approaches, including a comprehensive and systematic classification of the relevant literature. An outline of managerial guidelines about green warehouse management and the propositions of future research ideas contribute to the further development of this emerging research field.},
  doi      = {10.1016/j.jclepro.2019.04.055},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1113285021},
}

@InProceedings{Pamungkas2018,
  author    = {Dany Pamungkas and Manisa Pipattanasomporn and Saifur Rahman and Nanang Hariyanto and Suwarno},
  booktitle = {2018 International Conference and Utility Exhibition on Green Energy for Sustainable Development (ICUE)},
  title     = {Impacts of Solar PV, Battery Storage and HVAC Set Point Adjustments on Energy Savings and Peak Demand Reduction Potentials in Buildings},
  year      = {2018},
  pages     = {1-8},
  abstract  = {This paper discusses and compares three alternatives to reduce electrical energy consumption (kWh) and peak demand (kW) in buildings, namely deployment of rooftop solar PV, battery energy storage and HVAC set point adjustments. The building model of a warehouse located in Alexandria, VA, was developed in eQUEST, and its electrical consumption was validated with metered data. To perform the overall analysis, adjustment of HVAC set points was conducted in eQUEST, while Solar PV and battery models were developed and deployed on top of the developed eQUEST building model. Overall, the method presented here can serve as a guideline for building owners to analyze energy savings/peak demand reduction alternatives, of which benefits are varied from buildings to buildings based on building sizes, electricity tariffs, climate zones and building operation.},
  doi       = {10.23919/icue-gesd.2018.8635736},
  groups    = {Green Warehousing},
  url       = {https://app.dimensions.ai/details/publication/pub.1112019846},
}

@Article{You2018,
  author   = {Zhiyu You and Liwei Wang and Ying Han and Firuz Zare},
  journal  = {Energies},
  title    = {System Design and Energy Management for a Fuel Cell/Battery Hybrid Forklift},
  year     = {2018},
  note     = {https://www.mdpi.com/1996-1073/11/12/3440/pdf},
  number   = {12},
  pages    = {3440},
  volume   = {11},
  abstract = {Electric forklifts, dominantly powered by lead acid batteries, are widely used for material handling in factories, warehouses, and docks. The long charging time and short working time characteristics of the lead acid battery module results in the necessity of several battery modules to support one forklift. Compared with the cost and time consuming lead acid battery charging system, a fuel cell/battery hybrid power module could be more convenient for a forklift with fast hydrogen refueling and long working time. In this paper, based on the characteristics of a fuel cell and a battery, a prototype hybrid forklift with a fuel cell/battery hybrid power system is constructed, and its hardware and software are designed in detail. According to the power demand of driver cycles and the state of charge (SOC) of battery, an energy management strategy based on load current following for the hybrid forklift is proposed to improve system energy efficiency and dynamic response performance. The proposed energy management strategy will fulfill the power requirements under typical driving cycles, achieve reasonable power distribution between the fuel cell and battery and, thus, prolong its continuous working time. The proposed energy management strategy is implemented in the hybrid forklift prototype and its effectiveness is tested under different operating conditions. The results show that the forklift with the proposed hybrid powered strategy has good performance with different loads, both lifting and moving, in a smooth and steady way, and the output of the fuel cell meets the requirements of its output characteristics, its SOC of battery remaining at a reasonable level.},
  doi      = {10.3390/en11123440},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1110525171},
}

@Article{Burinskiene2018,
  author  = {A Burinskiene and A Lorenc and T Lerher},
  journal = {International Journal of Simulation Modelling},
  title   = {A Simulation Study for the Sustainability and Reduction of Waste in Warehouse Logistics},
  year    = {2018},
  note    = {https://doi.org/10.2507/ijsimm17(3)446},
  number  = {3},
  pages   = {485-497},
  volume  = {17},
  doi     = {10.2507/ijsimm17(3)446},
  groups  = {Green Warehousing},
  url     = {https://app.dimensions.ai/details/publication/pub.1106687077},
}

@Article{Perdahci2018,
  author   = {C Perdahci and H C Akin BSc and O Cekic Msc},
  journal  = {IOP Conference Series Earth and Environmental Science},
  title    = {A comparative study of fluorescent and LED lighting in industrial facilities},
  year     = {2018},
  note     = {https://doi.org/10.1088/1755-1315/154/1/012010},
  number   = {1},
  pages    = {012010},
  volume   = {154},
  abstract = {Industrial facilities have always been in search for reducing outgoings and minimizing energy consumption. Rapid developments in lighting technology require more energy efficient solutions not only for industries but also for many sectors and for households. Addition of solid-state technology has brought LED lamps into play and with LED lamp usage, efficacy level has reached its current values. Lighting systems which uses fluorescent and LED lamps have become the prior choice for many industrial facilities. This paper presents a comparative study about fluorescent and LED based indoor lighting systems for a warehouse building in an industrial facility in terms of lighting distribution values, colour rendering, power consumption, energy efficiency and visual comfort. Both scenarios have been modelled and simulated by using Relux and photometric data for the luminaires have been gathered by conducting tests and measurements in an accredited laboratory.},
  doi      = {10.1088/1755-1315/154/1/012010},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1104201531},
}

@Article{Seifhashemi2018,
  author   = {M. Seifhashemi and B.R. Capra and W. Milller and J. Bell},
  journal  = {Energy and Buildings},
  title    = {The potential for cool roofs to improve the energy efficiency of single storey warehouse-type retail buildings in Australia: A simulation case study},
  year     = {2018},
  note     = {https://eprints.qut.edu.au/114730/1/The%20potential%20for%20cool%20roofs%20to%20improve%20the%20energy%20efficiency%20of%20a%20single%20storey%20warehouse-type%20retail%20buidings%20in%20Australia%20-%20A%20simulation%20case%20study.pdf},
  pages    = {1393-1403},
  volume   = {158},
  abstract = {Australia’s commercial building stock exceeds 134 million m2 of net lettable area, with retail buildings contributing 35% to this sector’s energy use. The energy intensity of retail buildings in hotter climates is higher than the national average, as is the energy intensity of smaller buildings (under 1500m2) that are not considered ‘high-end’ commercial properties. Little attention has been paid to improving the energy efficiency of these types of buildings through regulation (for new buildings) or through market mechanisms (for retrofitting). As many of these buildings are single storey ‘warehouse’ type buildings, their predominant heat load comes through the roof, and thus are well suited to benefit from cool roof technology. Despite this, there remains a deficiency in quantifying the benefit of such technology in the context of single-storey retail buildings in Australia. This paper reports on an experimentally validated numerical study aimed at addressing this deficiency. Results show that application of cool roof technology to a warehouse type building in a subtropical environment increases the energy efficiency by shifting space temperature towards the design set point (21–23°C), and thus reducing cooling energy demand. This study also indicates an energy saving every month with the application of cool roof, with the largest saving in hotter months and no heating penalty in cooler months. Application of cool roof technology on warehouse style buildings across Australian buildings indicates energy savings can be achieved in all broad Australian climatic zones, with the greatest energy reduction associated with tropical, subtropical and desert environments.},
  doi      = {10.1016/j.enbuild.2017.11.034},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1092868344},
}

@Article{Kaur2018,
  author  = {Jasneet Kaur and Anjali Awasthi},
  journal = {International Journal of Logistics Systems and Management},
  title   = {A systematic literature review on barriers in green supply chain management},
  year    = {2018},
  number  = {3},
  pages   = {330},
  volume  = {30},
  doi     = {10.1504/ijlsm.2018.092613},
  groups  = {Green Warehousing},
  url     = {https://app.dimensions.ai/details/publication/pub.1105183849},
}

@InProceedings{Saikovski2017,
  author    = {Valeri Saikovski},
  booktitle = {2017 IEEE 58th International Scientific Conference on Power and Electrical Engineering of Riga Technical University (RTUCON)},
  title     = {Problems in the Operating and Calculation of Payback of Photovoltaic Systems in Buildings},
  year      = {2017},
  pages     = {1-4},
  abstract  = {The objects of the research are the solar photovoltaic systems (PV) that generate a significant amount of the electrical consumption in the building. This paper describes the parameters that affect the payback period of photovoltaic systems. Analysis of the first year of use of specific photovoltaic system (Building of a warehouse in Pärnu) is presented. The paper contains a analysis of the actual payback of photovoltaic systems under different scenarios of changes in electrical tariffs in the future. If calculating based on positive scenario (significant increase of tariffs) return of investment for specific PV in Pärnu is 9,5 years. If the increase in tariffs is minimal (negative scenario), the return of investment will two years later.},
  doi       = {10.1109/rtucon.2017.8125624},
  groups    = {Green Warehousing},
  url       = {https://app.dimensions.ai/details/publication/pub.1099594878},
}

@Article{Ries2016,
  author   = {Jörg M. Ries and Eric H. Grosse and Johannes Fichtinger},
  journal  = {International Journal of Production Research},
  title    = {Environmental impact of warehousing: a scenario analysis for the United States},
  year     = {2016},
  note     = {https://openaccess.city.ac.uk/id/eprint/17118/1/Environmental%20impact%20of%20warehousing%20A%20scenario%20analysis%20for%20the%20United%20States.pdf},
  number   = {21},
  pages    = {6485-6499},
  volume   = {55},
  abstract = {In recent years, there has been observed a continued growth of global carbon dioxide emissions, which are considered as a crucial factor for the greenhouse effect and associated with substantial environmental damages. Amongst others, logistic activities in global supply chains have become a major cause of industrial emissions and the progressing environmental pollution. Although a significant amount of logistic-related carbon dioxide emissions is caused by storage and material handling processes in warehouses, prior research mostly focused on the transport elements. The environmental impact of warehousing has received only little attention by research so far. Operating large and highly technological warehouses, however, causes a significant amount of energy consumption due to lighting, heating, cooling and air condition as well as fixed and mobile material handling equipment which induces considerable carbon dioxide emissions. The aim of this paper is to summarise preliminary studies of warehouse-related emissions and to discuss an integrated classification scheme enabling researchers and practitioners to systematically assess the carbon footprint of warehouse operations. Based on the systematic assessment approach containing emissions determinants and aggregates, overall warehouse emissions as well as several strategies for reducing the carbon footprint will be studied at the country level using empirical data of the United States. In addition, a factorial analysis of the warehouse-related carbon dioxide emissions in the United States enables the estimation of future developments and facilitates valuable insights for identifying effective mitigation strategies.},
  doi      = {10.1080/00207543.2016.1211342},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1035930497},
}

@Article{Nia2017,
  author   = {Ali Roozbeh Nia and Hassan Haleh and Abbas Saghaei},
  journal  = {Computers & Industrial Engineering},
  title    = {Dual command cycle dynamic sequencing method to consider GHG efficiency in unit-load multiple-rack automated storage and retrieval systems},
  year     = {2017},
  pages    = {89-108},
  volume   = {111},
  abstract = {In some AS/RS, the S/R machine executes a storage request and then a retrieval request to form a dual command (DC) cycle. This paper deals with a DC cycle “dynamic sequencing” method in unit-load multiple-rack AS/RS system. To create a kind of greenhouse gas (GHG) efficiency in the model, tax cost, penalty cost and discount of produced GHG emissions, limitations on available time of all facilities and total permitted GHG emissions produced by all equipment are considered in the model. In addition, an ant colony optimization (ACO) is employed to find a near-optimum solution of proposed mathematical model with the objective of minimizing the total cost of the GHG efficiency in AS/RS. Since no benchmark is available in the literature, a genetic algorithm (GA) is developed as well to validate the result obtained. For further validation, warehouse throughput rate in terms of time and cost is calculated for the model. At the end, twenty numerical examples are presented to demonstrate the application of the proposed methodology. Our results verified that GA was able to find better and nearer optimal solutions.},
  doi      = {10.1016/j.cie.2017.07.007},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1090587762},
}

@Article{Salhieh2016,
  author   = {L. Salhieh and I. Abushaikha},
  journal  = {South African Journal of Business Management},
  title    = {Assessing the driving forces for greening business practices: Empirical evidence from the United Arab Emirates’ logistics service industry},
  year     = {2016},
  note     = {https://sajbm.org/index.php/sajbm/article/download/75/69},
  number   = {4},
  pages    = {59-69},
  volume   = {47},
  abstract = {The uprising economic, social and political pressures on achieving a good level of environmental governance have forced companies to adopt business practices that aim to mitigate CO2 emissions. Logistics industry is considered as one of the major areas that could make a considerable improvement in reducing the environmental impact of business activities. The aim of this study is to investigate the driving forces that influence the adoption of environmentally friendly logistics activities. Following a detailed literature review, a theoretical model, which encompassed several driving forces and logistics activities, has been developed. Survey-based quantitative data were collected from logistics service providers in the United Arab Emirates (UAE) and analysed using structural equation modelling methodology and multiple regression analysis. Through answering 110 questionnaires, respondents holding different managerial levels at logistics service organizations, provided data reflecting their perception of the forces that would influence the adoption of green logistics practices. The driving forces (legislations, customers, and organizational awareness) were found to influence the adoption of green logistics practices by the logistics service industry in the areas of warehousing, packaging and transportation.},
  doi      = {10.4102/sajbm.v47i4.75},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1103540197},
}

@Article{Facchini2016,
  author   = {Francesco Facchini and Giovanni Mummolo and Giorgio Mossa and Salvatore Digiesi and Francesco Boenzi and Rossella Verriello},
  journal  = {Journal of Industrial Engineering and Management},
  title    = {Minimizing the carbon footprint of material handling equipment: Comparison of electric and LPG forklifts},
  year     = {2016},
  note     = {http://www.jiem.org/index.php/jiem/article/download/2082/792},
  number   = {5},
  pages    = {1035-1046},
  volume   = {9},
  abstract = {Purpose: The aim of this study is to identify the best Material Handling Equipment (MHE) to minimize the carbon footprint of inbound logistic activities, based on the type of the warehouse (layout, facilities and order-picking strategy) as well as the weight of the loads to be handled. Design/methodology/approach: A model to select the best environmental MHE for inbound logistic activities has been developed. Environmental performance of the MHE has been evaluated in terms of carbon Footprint (CF). The model is tested with a tool adopting a VBA macro as well as a simulation software allowing the evaluation of energy and time required by the forklift in each phase of the material handling cycle: picking, sorting and storing of the items. Findings: Nowadays, it is not possible to identify ‘a priori’ a particular engine equipped forklift performing better than others under an environmental perspective. Consistently, the application of the developed model allows to identify the best MHE tailored to each case analyzed.   Originality/value: This work gives a contribution to the disagreement between environmental performances of forklifts equipped with different engines. The developed model can be considered a valid support for decision makers to identify the best MHE minimizing the carbon footprint of inbound logistic activities.},
  doi      = {10.3926/jiem.2082},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1071706254},
}

@InBook{Boenzi2016,
  author    = {Francesco Boenzi and Salvatore Digiesi and Francesco Facchini and Giorgio Mossa and Giovanni Mummolo},
  pages     = {0980-0988},
  title     = {Greening Activities in Warehouses: A Model for Identifying Sustainable Strategies in Material Handling},
  year      = {2016},
  note      = {https://doi.org/10.2507/26th.daaam.proceedings.138},
  booktitle = {Proceedings of the 26th International DAAAM Symposium 2016},
  doi       = {10.2507/26th.daaam.proceedings.138},
  groups    = {Green Warehousing},
  url       = {https://app.dimensions.ai/details/publication/pub.1087177293},
}

@Article{Meneghetti2015,
  author  = {Antonella Meneghetti and Eleonora Dal Borgo and Luca Monti},
  journal = {International Journal of Shipping and Transport Logistics},
  title   = {Decision support optimisation models for design of sustainable automated warehouses},
  year    = {2015},
  number  = {3},
  pages   = {266},
  volume  = {7},
  doi     = {10.1504/ijstl.2015.069127},
  groups  = {Green Warehousing},
  url     = {https://app.dimensions.ai/details/publication/pub.1067495690},
}

@Article{Meneghetti2014,
  author   = {Antonella Meneghetti and Luca Monti},
  journal  = {International Journal of Production Research},
  title    = {Greening the food supply chain: an optimisation model for sustainable design of refrigerated automated warehouses},
  year     = {2014},
  number   = {21},
  pages    = {6567-6587},
  volume   = {53},
  abstract = {An optimisation model for the sustainable design of refrigerated automated storage and retrieval systems is proposed, which takes into account specific features of the food supply chain, such as temperature control. Rack configuration as well as surfaces and volumes of the cold cell are conjointly optimised in order to minimise the total yearly cost of the automated storage facility, introducing energy requirements both for refrigeration and picking operations explicitly, other than investment costs. Crane plus satellite systems are modelled in order to enable deep lane solutions and space savings, as suitable for cold storage. The model allows a deep analysis of the impact of supply chain decision variables, such as the facility location, the storage temperature and the incoming product temperature on costs, energy use and carbon dioxide emissions, so that storage facilities attributes for supply chain design models can be properly assessed to re-optimise the whole cold chain. The design problem is modelled and solved by Constraint Programming in order to easily manage non-linear functions.},
  doi      = {10.1080/00207543.2014.985449},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1002243471},
}

@Article{Fichtinger2015,
  author   = {Johannes Fichtinger and Jörg M. Ries and Eric H. Grosse and Peter Baker},
  journal  = {International Journal of Production Economics},
  title    = {Assessing the environmental impact of integrated inventory and warehouse management},
  year     = {2015},
  note     = {https://openaccess.city.ac.uk/id/eprint/17120/1/Assessing%20the%20environmental%20impact%20of%20integrated%20inventory%20and%20warehouse%20management.pdf},
  pages    = {717-729},
  volume   = {170},
  abstract = {There has been considerable research on the environmental impact of supply chains but most of this has concentrated on the transport elements. The environmental impact of warehousing has received relatively little attention except within the context of distribution networks. A high proportion of total warehouse emissions emanate from heating, cooling, air conditioning and lighting and these aspects are largely related to warehouse size. This in turn is greatly influenced by inventory management, affecting stockholding levels, and warehouse design, affecting the footprint required for holding a given amount of stock. Other emissions, such as those caused by material handling equipment, are closely related to warehouse throughput and equipment choice. There is a substantial gap in the literature regarding this interaction between inventory and warehouse management and its environmental impact. The purpose of this paper is to contribute to filling this gap. Therefore, an integrated simulation model has been built to examine this interaction and the results highlight the key effects of inventory management on warehouse-related greenhouse gas emissions. In particular, it is found that decisions on supply lead times, reorder quantities, and storage equipment all have an impact on costs and emissions and therefore this integrated approach will inform practical decision making. Additionally, it is intended that the paper provides a framework for further research in this important area.},
  doi      = {10.1016/j.ijpe.2015.06.025},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1024780163},
}

@Article{Oswiecinska2015,
  author   = {A Oswiecinska and J Hibbs and I Zajic and K J Burnham},
  journal  = {Journal of Physics Conference Series},
  title    = {Towards energy efficient operation of Heating, Ventilation and Air Conditioning systems via advanced supervisory control design},
  year     = {2015},
  note     = {https://iopscience.iop.org/article/10.1088/1742-6596/659/1/012030/pdf},
  number   = {1},
  pages    = {012030},
  volume   = {659},
  abstract = {This paper presents conceptual control solution for reliable and energy efficient operation of heating, ventilation and air conditioning (HVAC) systems used in large volume building applications, e.g. warehouse facilities or exhibition centres. Advanced two-level scalable control solution, designed to extend capabilities of the existing low-level control strategies via remote internet connection, is presented. The high-level, supervisory controller is based on Model Predictive Control (MPC) architecture, which is the state-of-the-art for indoor climate control systems. The innovative approach benefits from using passive heating and cooling control strategies for reducing the HVAC system operational costs, while ensuring that required environmental conditions are met.},
  doi      = {10.1088/1742-6596/659/1/012030},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1018121211},
}

@Article{Tappia2015,
  author   = {Elena Tappia and Gino Marchet and Marco Melacini and Sara Perotti},
  journal  = {Production Planning & Control},
  title    = {Incorporating the environmental dimension in the assessment of automated warehouses},
  year     = {2015},
  note     = {https://re.public.polimi.it/bitstream/11311/855537/4/TPPC-2013-0382_Paper%20completo.pdf},
  number   = {10},
  pages    = {824-838},
  volume   = {26},
  abstract = {In today’s competitive context, the paradigm of sustainable development is becoming more and more significant, also in warehousing. Managers are progressively considering not only purely economic aspects but also environmental concerns. Despite such consciousness, the selection of automated warehousing solutions has been mainly based on operational and economic performance in both practice and theory so far, whereas energy consumption and environmental performance have not been adequately taken into account. To fill this lack, a model is proposed to evaluate the energy consumption and environmental impact of automated warehousing solutions. The model has been used to investigate whether and how the selection of automated solutions changes depending on the dimensions involved in the analysis (i.e. only economic, only environmental or both). The analysis has been performed considering autonomous vehicle storage and retrieval systems (AVS/RSs) and its natural alternative, i.e. automated storage and retrieval systems (AS/RSs). Results confirm the importance of considering both dimensions in the assessment of automated warehouses, as depending on the scenario, the technology selection shifts from AS/RS to AVS/RS when considering not only the economic but also the environmental impact. Additionally, this study provides new insights on the suitability areas of AVS/RSs.},
  doi      = {10.1080/09537287.2014.990945},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1026334040},
}

@InProceedings{Martin2013,
  author    = {Beermann Martin and Jungmeier Gerfried and Wahlmüller Ewald and Böhme Walter and Klell Manfred and Spindlberger Mario and Fellinger Robert},
  booktitle = {2013 World Electric Vehicle Symposium and Exhibition (EVS27)},
  title     = {Hydrogen Powered Fuel Cell Forklifts — Demonstration of Green Warehouse Logistics},
  year      = {2013},
  pages     = {1-4},
  abstract  = {An Austrian consortium of industry and research partners demonstrates a system solution for renewable hydrogen powered fuel cell range extender vehicles applied in intralogistics. The core element of this development, the “HYLOG Fleet Energy Cell” by Fronius International, replaces the lead-acid battery in conventional battery-electric propulsion systems for warehouse logistics. In collaboration with Linde Material Handling a warehouse truck fleet of 10 class-3 vehicles has been adapted accordingly at an Austrian site of the global logistics company DB Schenker. Renewable hydrogen is supplied by the oil and gas company OMV using a decentral biomethane steam reformer, installed onsite at the logistic facility. The warehouse trucks are fuelled indoors for the first time in Europe. Operation experiences show that warehouse logistics is in particular attractive for hydrogen and fuel cell technologies due to increased productivity, reduced maintenance and reduced life-cycle Greenhouse gas emissions.},
  doi       = {10.1109/evs.2013.6914853},
  groups    = {Green Warehousing},
  url       = {https://app.dimensions.ai/details/publication/pub.1093378447},
}

@Article{Lerher2013,
  author   = {Tone Lerher and Milan Edl and Bojan Rosi},
  journal  = {The International Journal of Advanced Manufacturing Technology},
  title    = {Energy efficiency model for the mini-load automated storage and retrieval systems},
  year     = {2013},
  number   = {1-4},
  pages    = {97-115},
  volume   = {70},
  abstract = {In this paper, the energy efficiency model for the mini-load AS/RS is presented. As the existing models of AS/RS apply to already well-known objectives (minimum travel time, maximum throughput and minimum cost), the energy efficiency model for the mini-load AS/RS is proposed and discussed. According to global trends in material handling and warehousing, the design process should encompass not only the application of the equipment with fastest drives but should also consider the energy and environment aspect on the installed equipment. The proposed energy efficiency model for the mini-load AS/RS enables reduction of energy consumption and consequently the CO2 emission, which is good from the economic and environmental point of view. We sincerely hope that the energy and environment aspect will indubitably bring changes into planning of warehouses and will mean great challenge for those who are engaged in the planning process.},
  doi      = {10.1007/s00170-013-5253-x},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1029079666},
}

@Article{Mostafaeipour2014,
  author   = {Ali Mostafaeipour and Behnoosh Bardel and Kasra Mohammadi and Ahmad Sedaghat and Yagob Dinpashoh},
  journal  = {Renewable and Sustainable Energy Reviews},
  title    = {Economic evaluation for cooling and ventilation of medicine storage warehouses utilizing wind catchers},
  year     = {2014},
  pages    = {12-19},
  volume   = {38},
  abstract = {Renewable energy consumption has become a dominant issue in many countries because of environmental crisis, pollution, climate change and increased costs of non-renewable sources like fossil fuels. Because of global warming and energy prices designers are refocusing on the low carbon credentials of new equivalents. In this context, the main target of the proposed study is to present a new innovative method of cooling the non-refrigerator medicine storage warehouses in order to minimize energy costs and environmental hazards for the city of Yazd in Iran. For this purpose, warehouses with absorption chillers, underground warehouses, and underground warehouses including wind catchers have been analyzed. Then, the equivalent uniform annual cost (EUAC) method was applied for evaluating the costs of the three alternative systems. The results of this study show that the use of wind catcher is far more economical than the absorption chiller cooling system. Moreover, it is concluded that the construction of an underground warehouse with a wind catcher is the most economical option for the storage of medicines than the other warehouses in this case study.},
  doi      = {10.1016/j.rser.2014.05.087},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1013525464},
}

@InProceedings{Sailor2013,
  author    = {David J. Sailor and Prem Vuppuluri},
  booktitle = {Volume 4: Heat and Mass Transfer Under Extreme Conditions; Environmental Heat Transfer; Computational Heat Transfer; Visualization of Heat Transfer; Heat Transfer Education and Future Directions in Heat Transfer; Nuclear Energy},
  title     = {Energy Performance of Sustainable Roofing Systems},
  year      = {2013},
  pages     = {v004t13a002-v004t13a002},
  abstract  = {This study presents efforts to analyze how sustainable roofing technologies can contribute to the energy budget of buildings, and the resulting implications for heating and cooling energy use. The data analyzed in this study were obtained from a field experiment performed on a four story warehouse/office building in Portland, Oregon USA. The building’s roof includes a 216 panel, 45.6 kW solar photovoltaic array in combination with 576 m2 of vegetated green roofing. While most of the surface consists of green roof shaded by photovoltaic panels, the roof also has test patches of dark membrane, white membrane and un-shaded green-roofing. Interior and exterior surface temperatures were monitored over a period of two years and heat flux into the building is estimated using a finite difference conduction model. On average, the black roof membrane was the only roof that caused a net heat gain into the building in the summer. In the winter, all four roofing technologies resulted in net heat losses out of the building. Both the PV-shaded and un-shaded green-roofs indicated a net heat loss out of the interior of the building during both the summer and winter. This latter effect is largely a result of green-roof evaporative cooling — which can benefit air conditioning demand in summer but may be undesirable during heating-dominated seasons.Copyright © 2013 by ASME},
  doi       = {10.1115/ht2013-17535},
  groups    = {Green Warehousing},
  url       = {https://app.dimensions.ai/details/publication/pub.1092850655},
}

@Article{Cook2011,
  author   = {Phillip Cook and Alistair Sproul},
  journal  = {Architectural Science Review},
  title    = {Towards low-energy retail warehouse building},
  year     = {2011},
  number   = {3},
  pages    = {206-214},
  volume   = {54},
  abstract = {Retail warehouse buildings consume a growing proportion of the energy used within Australia and they have potential to be designed much more efficiently. This article outlines the results of modelling reductions in the energy consumption for a retail warehouse building in the Sydney, Australia climate. The design evaluated in this research consumes 73% less energy than does the base-case building, resulting in a significant reduction in on-going costs and in greenhouse gas emissions that are associated with the operation of this building. Techniques for reducing the total energy consumed by the building were explored using the simulation software, DesignBuilder and EnergyPlus. It was found that energy required for lighting, which is approximately 69% of the total energy consumed in the base-case model, could be dramatically reduced by introducing daylight through a sawtooth roof, in conjunction with efficient T5 fluorescent lighting and automatic daylighting controls. Other savings were made by adding insulation to the building, using natural ventilation and selective glazing to limit heat transfers into and out of the building.},
  doi      = {10.1080/00038628.2011.590055},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1015425126},
}

@Article{Rai2011,
  author   = {Deepak Rai and Behzad Sodagar and Rosi Fieldson and Xiao Hu},
  journal  = {Energy},
  title    = {Assessment of CO2 emissions reduction in a distribution warehouse},
  year     = {2011},
  number   = {4},
  pages    = {2271-2277},
  volume   = {36},
  abstract = {Building energy use accounts for almost 50% of the total CO2 emissions in the UK. Most of the research has focused on reducing the operational impact of buildings, however in recent years many studies have indicated the significance of embodied energy in different building types. This paper primarily focuses on illustrating the relative importance of operational and embodied energy in a flexible use light distribution warehouse. The building is chosen for the study as it is relatively easy to model and represents many distribution centres and industrial warehouses in Europe.A carbon footprinting study was carried out by conducting an inventory of the major installed materials with potentially significant carbon impact and material substitutions covering the building structure. Ecotect computer simulation program was used to determine the energy consumption for the 25 years design life of the building. This paper evaluates alternative design strategies for the envelope of the building and their effects on the whole life emissions by investigating both embodied and operational implications of changing the envelope characteristics. The results provide an insight to quantify the total amount of CO2 emissions saved through design optimisation by modeling embodied and operational energy.},
  doi      = {10.1016/j.energy.2010.05.006},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1015228225},
}

@InProceedings{Tan2009,
  author    = {Kah-Shien Tan and M. Daud Ahmed and David Sundaram},
  booktitle = {Proceedings of the International Workshop on Enterprises & Organizational Modeling and Simulation},
  title     = {Sustainable warehouse management},
  year      = {2009},
  note      = {http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-458/paper5.pdf},
  pages     = {1-15},
  abstract  = {Sustainable warehouse is about integrating, balancing and managing the economic, environmental and social inputs and outputs of the warehouse operations. Sustainability is a core value to many businesses but they find it hard to implement in their current business setting especially when they use third party logistics management system like warehousing and distribution in their supply chain business network. This paper explores the application of sustainability principles in the context of warehouse storage and distribution management and introduces a sustainability model for setting up of a warehouse or transformation of an existing warehouse that provides the storage services. The model in corporates the usage of third party transportation systems for supply and distribution services.},
  doi       = {10.1145/1750405.1750415},
  groups    = {Green Warehousing},
  url       = {https://app.dimensions.ai/details/publication/pub.1008996397},
}

@Article{Ciliberti2008,
  author   = {Francesco Ciliberti and Pierpaolo Pontrandolfo and Barbara Scozzi},
  journal  = {International Journal of Production Economics},
  title    = {Logistics social responsibility: Standard adoption and practices in Italian companies},
  year     = {2008},
  number   = {1},
  pages    = {88-106},
  volume   = {113},
  abstract = {This paper deals with Logistics Social Responsibility (LSR), i.e. the socially responsible management of the supply chain under a cross-functional perspective. In particular, the goal of the paper is to develop a taxonomy of the LSR practices adopted by firms. The taxonomy is built based on a literature review and an empirical analysis. In particular, the empirical analysis deals with the analysis of the non-financial reports (i.e. social, environmental, sustainability report, environmental statement, and/or SA8000 report) published by a sample of Italian companies.The taxonomy involves 47 different LSR practices classified based on five areas, namely Purchasing Social Responsibility, Sustainable Transportation, Sustainable Packaging, Sustainable Warehousing, and Reverse Logistics. The practices are further investigated to assess which and to what extent they are adopted by different categories of companies. To this aim, we use an ad hoc-defined metric, the Adoption Index, and carry out a cross-analysis. By providing an encompassing view of all the LSR issues and practices, our paper represents an initial attempt to fill a gap in the literature on LSR.},
  doi      = {10.1016/j.ijpe.2007.02.049},
  groups   = {Green Warehousing},
  url      = {https://app.dimensions.ai/details/publication/pub.1013561216},
}

@InProceedings{Cao2015,
  author    = {Yu Cao and Peng Hou and Donald Brown and Jie Wang and Songqing Chen},
  booktitle = {Proceedings of the 2015 Workshop on Mobile Big Data},
  title     = {Distributed Analytics and Edge Intelligence},
  year      = {2015},
  pages     = {43-48},
  abstract  = {Biomedical research and clinical practice are entering a data-driven era. One of the major applications of biomedical big data research is to utilize inexpensive and unobtrusive mobile biomedical sensors and cloud computing for pervasive health monitoring. However, real-world user experiences with mobile cloud-based health monitoring were poor, due to the factors such as excessive networking latency and longer response time. On the other hand, fog computing, a newly proposed computing paradigm, utilizes a collaborative multitude of end-user clients or near-user edge devices to conduct a substantial amount of computing, storage, communication, and etc. This new computing paradigm, if successfully applied for pervasive health monitoring, has great potential to accelerate the discovery of early predictors and novel biomarkers to support smart care decision making in a connected health scenarios. In this paper, we employ a real-world pervasive health monitoring application (pervasive fall detection for stroke mitigation) to demonstrate the effectiveness and efficacy of fog computing paradigm in health monitoring. Fall is a major source of morbidity and mortality among stroke patients. Hence, detecting falls automatically and in a timely manner becomes crucial for stroke mitigation in daily life. In this paper, we set to (1) investigate and develop new fall detection algorithms and (2) design and employ a real-time fall detection system employing fog computing paradigm (e.g., distributed analytics and edge intelligence), which split the detection task between the edge devices (e.g., smartphones attached to the user) and the server (e.g., servers in the cloud). Experimental results show that distributed analytics and edge intelligence, supported by fog computing paradigm, are very promising solutions for pervasive health monitoring.},
  doi       = {10.1145/2757384.2757398},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1042065361},
}

@InProceedings{Gia2015,
  author    = {Tuan Nguyen Gia and Mingzhe Jiang and Amir-Mohammad Rahmani and Tomi Westerlund and Pasi Liljeberg and Hannu Tenhunen},
  booktitle = {2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing},
  title     = {Fog Computing in Healthcare Internet of Things: A Case Study on ECG Feature Extraction},
  year      = {2015},
  pages     = {356-363},
  abstract  = {Internet of Things technology provides a competent and structured approach to improve health and wellbeing of mankind. One of the feasible ways to offer healthcare services based on loT is to monitor humans health in real-time using ubiquitous health monitoring systems which have the ability to acquire bio-signals from sensor nodes and send the data to the gateway via a particular wireless communication protocol. The real-time data is then transmitted to a remote cloud server for real-time processing, visualization, and diagnosis. In this paper, we enhance such a health monitoring system by exploiting the concept of fog computing at smart gateways providing advanced techniques and services such as embedded data mining, distributed storage, and notification service at the edge of network. Particularly, we choose Electrocardiogram (ECG) feature extraction as the case study as it plays an important role in diagnosis of many cardiac diseases. ECG signals are analyzed in smart gateways with features extracted including heart rate, P wave and T wave via a flexible template based on a lightweight wavelet transform mechanism. Our experimental results reveal that fog computing helps achieving more than 90 % bandwidth efficiency and offering low-latency real time response at the edge of the network.},
  doi       = {10.1109/cit/iucc/dasc/picom.2015.51},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1095085621},
}

@InProceedings{Monteiro2016,
  author    = {Admir Monteiro and Harishchandra Dubey and Leslie Mahler and Qing Yang and Kunal Mankodiya},
  booktitle = {2016 IEEE International Conference on Smart Computing (SMARTCOMP)},
  title     = {FIT: A Fog Computing Device for Speech Tele-Treatments},
  year      = {2016},
  note      = {https://arxiv.org/pdf/1605.06236},
  pages     = {1-3},
  abstract  = {There is an increasing demand for smart fog-computing gateways as the size of cloud data is growing. This paper presents a Fog computing interface(FIT) for processing clinical speech data. FIT builds upon our previous work on EchoWear, a wearable technology that validated the use of smartwatches for collecting clinical speech data from patients with Parkinson's disease(PD). The fog interface is a low-power embedded system that acts as a smart interface between the smartwatch and the cloud. It collects, stores, and processes the speech data before sending speech features to secure cloud storage. We developed and validated a working prototype of FIT that enabled remote processing of clinical speech data to get speech clinical features such as loudness, short-time energy, zero-crossing rate, and spectral centroid. We used speech data from six patients with PD in their homes for validating FIT. Our results showed the efficacy of FIT as a Fog interface to translate the clinical speech processing chain(CLIP) from a cloud-based backend to a fog-based smart gateway.},
  doi       = {10.1109/smartcomp.2016.7501692},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1095382211},
}

@Article{Lee2015,
  author   = {Jae Dong Lee and Tae Sik Yoon and Seung Hyun Chung and Hyo Soung Cha},
  journal  = {Healthcare Informatics Research},
  title    = {Service-Oriented Security Framework for Remote Medical Services in the Internet of Things Environment},
  year     = {2015},
  note     = {https://e-hir.org/upload/pdf/hir-21-271.pdf},
  number   = {4},
  pages    = {271-282},
  volume   = {21},
  abstract = {OBJECTIVES: Remote medical services have been expanding globally, and this is expansion is steadily increasing. It has had many positive effects, including medical access convenience, timeliness of service, and cost reduction. The speed of research and development in remote medical technology has been gradually accelerating. Therefore, it is expected to expand to enable various high-tech information and communications technology (ICT)-based remote medical services. However, the current state lacks an appropriate security framework that can resolve security issues centered on the Internet of things (IoT) environment that will be utilized significantly in telemedicine.
METHODS: This study developed a medical service-oriented frame work for secure remote medical services, possessing flexibility regarding new service and security elements through its service-oriented structure. First, the common architecture of remote medical services is defined. Next medical-oriented secu rity threats and requirements within the IoT environment are identified. Finally, we propose a "service-oriented security frame work for remote medical services" based on previous work and requirements for secure remote medical services in the IoT.
RESULTS: The proposed framework is a secure framework based on service-oriented cases in the medical environment. A com parative analysis focusing on the security elements (confidentiality, integrity, availability, privacy) was conducted, and the analysis results demonstrate the security of the proposed framework for remote medical services with IoT.
CONCLUSIONS: The proposed framework is service-oriented structure. It can support dynamic security elements in accordance with demands related to new remote medical services which will be diversely generated in the IoT environment. We anticipate that it will enable secure services to be provided that can guarantee confidentiality, integrity, and availability for all, including patients, non-patients, and medical staff.},
  doi      = {10.4258/hir.2015.21.4.271},
  groups   = {Internet of Things in Healthcare},
  url      = {https://app.dimensions.ai/details/publication/pub.1024210708},
}

@InProceedings{Doukas2012,
  author    = {Charalampos Doukas and Ilias Maglogiannis},
  booktitle = {2012 Sixth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing},
  title     = {Bringing IoT and Cloud Computing towards Pervasive Healthcare},
  year      = {2012},
  pages     = {922-926},
  abstract  = {Pervasive healthcare applications utilizing body sensor networks generate a vast amount of data that need to be managed and stored for processing and future usage. Cloud computing among with the Internet of Things (IoT) concept is a new trend for efficient managing and processing of sensor data online. This paper presents a platform based on Cloud Computing for management of mobile and wearable healthcare sensors, demonstrating this way the IoT paradigm applied on pervasive healthcare.},
  doi       = {10.1109/imis.2012.26},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1093463283},
}

@InProceedings{AlMajeed2015,
  author    = {Salah S. Al-Majeed and Intisar S. Ai-Mejibli and Jalal Karam},
  booktitle = {2015 IEEE 28th Canadian Conference on Electrical and Computer Engineering (CCECE)},
  title     = {Home Telehealth by Internet of Things (IoT)},
  year      = {2015},
  pages     = {609-613},
  abstract  = {Home based Telehealth is a combination of communications, imaging, sensing and human computer interaction technologies targeted at diagnosis, treatment and monitoring patients without disturbing the quality of lifestyle. This paper proposes development of a low cost medical sensing, communication and analytics device that is real-time monitoring internet enabled patients physiological conditions. Internet of Things (IoT) network will provide active and real-time engagement of patient, hospitals, caretaker and doctors. Massaging and synchronising the system has been the based focus in this paper, where it applies the suggested algorithm to predict the minimum time period that separates two consecutive bursts of messages and measures the minimum queue sizes for the health care personals nods, to manage the traffic and avoid the dropping of messages. NS2 simulator was employed to simulate the Telehealth environment algorithm},
  doi       = {10.1109/ccece.2015.7129344},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1095264568},
}

@InProceedings{Ullah2016,
  author    = {Kaleem Ullah and Munam Ali Shah and Sijing Zhang},
  booktitle = {2016 International Conference on Intelligent Systems Engineering (ICISE)},
  title     = {Effective Ways to Use Internet of Things in the Field of Medical and Smart Health Care},
  year      = {2016},
  pages     = {372-379},
  abstract  = {The recent advancements in technology and the availability of the Internet make it possible to connect various devices that can communicate with each other and share data. The Internet of Things (IoT) is a new concept that allows users to connect various sensors and smart devices to collect real-time data from the environment. However, it has been observed that a comprehensive platform is still missing in the e-Health and m-Health architectures to use smartphone sensors to sense and transmit important data related to a patient's health. In this paper, our contribution is twofold. Firstly, we critically evaluate the existing literature, which discusses the effective ways to deploy IoT in the field of medical and smart health care. Secondly, we propose a new semantic model for patients' e-Health. The proposed model named as ‘k-Healthcare’ makes use of 4 layers; the sensor layer, the network layer, the Internet layer and the services layer. All layers cooperate with each other effectively and efficiently to provide a platform for accessing patients' health data using smart phones.},
  doi       = {10.1109/intelse.2016.7475151},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1093507023},
}

@Article{Bhatia2016,
  author   = {Munish Bhatia and Sandeep K. Sood},
  journal  = {Journal of Medical Systems},
  title    = {Temporal Informative Analysis in Smart-ICU Monitoring: M-HealthCare Perspective},
  year     = {2016},
  number   = {8},
  pages    = {190},
  volume   = {40},
  abstract = {The rapid introduction of Internet of Things (IoT) Technology has boosted the service deliverance aspects of health sector in terms of m-health, and remote patient monitoring. IoT Technology is not only capable of sensing the acute details of sensitive events from wider perspectives, but it also provides a means to deliver services in time sensitive and efficient manner. Henceforth, IoT Technology has been efficiently adopted in different fields of the healthcare domain. In this paper, a framework for IoT based patient monitoring in Intensive Care Unit (ICU) is presented to enhance the deliverance of curative services. Though ICUs remained a center of attraction for high quality care among researchers, still number of studies have depicted the vulnerability to a patient’s life during ICU stay. The work presented in this study addresses such concerns in terms of efficient monitoring of various events (and anomalies) with temporal associations, followed by time sensitive alert generation procedure. In order to validate the system, it was deployed in 3 ICU room facilities for 30 days in which nearly 81 patients were monitored during their ICU stay. The results obtained after implementation depicts that IoT equipped ICUs are more efficient in monitoring sensitive events as compared to manual monitoring and traditional Tele-ICU monitoring. Moreover, the adopted methodology for alert generation with information presentation further enhances the utility of the system.},
  doi      = {10.1007/s10916-016-0547-9},
  groups   = {Internet of Things in Healthcare},
  url      = {https://app.dimensions.ai/details/publication/pub.1044514690},
}

@InProceedings{Anurag2014,
  author    = {Anurag Anurag and Sanaz Rahimi Moosavi and Amir-Mohammad Rahmani and Tomi Westerlund and Geng Yang and Pasi Liljeberg and Hannu Tenhunen},
  booktitle = {Proceedings of the 4th International Conference on Wireless Mobile Communication and Healthcare - "Transforming healthcare through innovations in mobile and wireless technologies"},
  title     = {Pervasive Health Monitoring Based on Internet of Things: Two Case Studies},
  year      = {2014},
  note      = {http://eudl.eu/pdf/10.4108/icst.mobihealth.2014.257395},
  doi       = {10.4108/icst.mobihealth.2014.257395},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1099281667},
}

@InProceedings{Mohammed2014,
  author    = {Junaid Mohammed and Abhinav Thakral and Adrian Filip Ocneanu and Colin Jones and Chung-Horng Lung and Andy Adler},
  booktitle = {2014 IEEE International Conference on Internet of Things(iThings), and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom)},
  title     = {Internet of Things: Remote Patient Monitoring Using Web Services and Cloud Computing},
  year      = {2014},
  pages     = {256-263},
  abstract  = {The focus on this paper is to build an Android platform based mobile application for the healthcare domain, which uses the idea of Internet of Things (IoT) and cloud computing. We have built an application called ‘ECG Android App’ which provides the end user with visualization of their Electro Cardiogram (ECG) waves and data logging functionality in the background. The logged data can be uploaded to the user's private centralized cloud or a specific medical cloud, which keeps a record of all the monitored data and can be retrieved for analysis by the medical personnel. Though the idea of building a medical application using IoT and cloud techniques is not totally new, there is a lack of empirical studies in building such a system. This paper reviews the fundamental concepts of IoT. Further, the paper presents an infrastructure for the healthcare domain, which consists of various technologies: IOIO microcontroller, signal processing, communication protocols, secure and efficient mechanisms for large file transfer, data base management system, and the centralized cloud. The paper emphasizes on the system and software architecture and design which is essential to overall IoT and cloud based medical applications. The infrastructure presented in the paper can also be applied to other healthcare domains. It concludes with recommendations and extensibilities found for the solution in the healthcare domain.},
  doi       = {10.1109/ithings.2014.45},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1094225194},
}

@Article{Gomez2016,
  author   = {Jorge Gómez and Byron Oviedo and Emilio Zhuma},
  journal  = {Procedia Computer Science},
  title    = {Patient Monitoring System Based on Internet of Things},
  year     = {2016},
  note     = {https://doi.org/10.1016/j.procs.2016.04.103},
  pages    = {90-97},
  volume   = {83},
  abstract = {The increased use of mobile technologies and smart devices in the area of health has caused great impact on the world. Health experts are increasingly taking advantage of the benefits these technologies bring, thus generating a significant improvement in health care in clinical settings and out of them. Likewise, countless ordinary users are being served from the advantages of the M-Health (Mobile Health) applications and E-Health (health care supported by ICT) to improve, help and assist their health. Applications that have had a major refuge for these users, so intuitive environment. The Internet of things is increasingly allowing to integrate devices capable of connecting to the Internet and provide information on the state of health of patients and provide information in real time to doctors who assist. It is clear that chronic diseases such as diabetes, heart and pressure among others, are remarkable in the world economic and social level problem. The aim of this article is to develop an architecture based on an ontology capable of monitoring the health and workout routine recommendations to patients with chronic diseases.},
  doi      = {10.1016/j.procs.2016.04.103},
  groups   = {Internet of Things in Healthcare},
  url      = {https://app.dimensions.ai/details/publication/pub.1044290948},
}

@Article{Lee2014,
  author  = {Kiho Lee and Yvette Gelogo and Sunguk Lee},
  journal = {International Journal of Smart Home},
  title   = {Mobile Gateway System for Ubiquitous System and Internet of Things Application},
  year    = {2014},
  number  = {5},
  pages   = {279-286},
  volume  = {8},
  doi     = {10.14257/ijsh.2014.8.5.25},
  groups  = {Internet of Things in Healthcare},
  url     = {https://app.dimensions.ai/details/publication/pub.1067238716},
}

@Article{Spano2016,
  author   = {Elisa Spano and Stefano Di Pascoli and Giuseppe Iannaccone},
  journal  = {IEEE Sensors Journal},
  title    = {Low-Power Wearable ECG Monitoring System for Multiple-Patient Remote Monitoring},
  year     = {2016},
  number   = {13},
  pages    = {5452-5462},
  volume   = {16},
  abstract = {Many devices and solutions for remote electrocardiogram (ECG) monitoring have been proposed in the literature. These solutions typically have a large marginal cost per added sensor and are not seamlessly integrated with other smart home solutions. Here, we propose an ECG remote monitoring system that is dedicated to non-technical users in need of long-term health monitoring in residential environments and is integrated in a broader Internet-of-Things (IoT) infrastructure. Our prototype consists of a complete vertical solution with a series of advantages with respect to the state of the art, considering both the prototypes with integrated front end and prototypes realized with off-the-shelf components: 1) ECG prototype sensors with record-low energy per effective number of quantized levels; 2) an architecture providing low marginal cost per added sensor/user; and 3) the possibility of seamless integration with other smart home systems through a single IoT infrastructure.},
  doi      = {10.1109/jsen.2016.2564995},
  groups   = {Internet of Things in Healthcare},
  url      = {https://app.dimensions.ai/details/publication/pub.1061324932},
}

@InProceedings{AlAdhab2016,
  author    = {Abdulmonam Al-Adhab and Hamad Altmimi and Majed Alhawashi and Hussain Alabduljabbar and Farah Harrathi and Hammeed Almubarek},
  booktitle = {2016 International Symposium on Networks, Computers and Communications (ISNCC)},
  title     = {IoT for Remote Elderly Patient Care Based on Fuzzy Logic},
  year      = {2016},
  pages     = {1-5},
  abstract  = {In this paper we propose an in-home healthcare monitoring system offering several uses. It helps the elderly person to complete the activities independently in their own lives and at the same time facilitates family members and care provider to track inhabitants. The main idea is to have several sensors that can be used at home and enables full and tightly controlled elderly patient care. Each modality is processed and analyzed by specific algorithms. A data approach based on fuzzy logic with a set of rules directed by medical recommendations.},
  doi       = {10.1109/isncc.2016.7746072},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1094830611},
}

@Article{Fan2014,
  author   = {Yuan Jie Fan and Yue Hong Yin and Li Da Xu and Yan Zeng and Fan Wu},
  journal  = {IEEE Transactions on Industrial Informatics},
  title    = {IoT-Based Smart Rehabilitation System},
  year     = {2014},
  number   = {2},
  pages    = {1568-1577},
  volume   = {10},
  abstract = {Internet of Things (IoT) makes all objects become interconnected and smart, which has been recognized as the next technological revolution. As its typical case, IoT-based smart rehabilitation systems are becoming a better way to mitigate problems associated with aging populations and shortage of health professionals. Although it has come into reality, critical problems still exist in automating design and reconfiguration of such a system enabling it to respond to the patient’s requirements rapidly. This paper presents an ontology-based automating design methodology (ADM) for smart rehabilitation systems in IoT. Ontology aids computers in further understanding the symptoms and medical resources, which helps to create a rehabilitation strategy and reconfigure medical resources according to patients’ specific requirements quickly and automatically. Meanwhile, IoT provides an effective platform to interconnect all the resources and provides immediate information interaction. Preliminary experiments and clinical trials demonstrate valuable information on the feasibility, rapidity, and effectiveness of the proposed methodology.},
  doi      = {10.1109/tii.2014.2302583},
  groups   = {Internet of Things in Healthcare},
  url      = {https://app.dimensions.ai/details/publication/pub.1061632443},
}

@InProceedings{Gelogo2015,
  author    = {Yvette E. Gelogo and Jung-Won Oh and Jin Woo Park and Haeng-Kon Kim},
  booktitle = {2015 8th International Conference on Bio-Science and Bio-Technology (BSBT)},
  title     = {Internet of Things (IOT) Driven U-Healthcare System Architecture},
  year      = {2015},
  pages     = {24-26},
  abstract  = {The IoT plays an important role in healthcare applications., from managing chronic diseases at one end of the spectrum to preventing disease at the other. loT aims to provide means to access and control all kinds of ubiquitous and uniquely identifiable devices., facilities and assets. In this paper we discussed the background of Internet of Things (loT) and its application to u-healthcare. This study aims to make mobile device gateway an integrated gateway which supports heterogeneous devices for u-healthcare convergence.},
  doi       = {10.1109/bsbt.2015.17},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1093357343},
}

@InProceedings{Pang2014,
  author    = {Zhibo Pang and Junzhe Tian and Qiang Chen},
  booktitle = {16th International Conference on Advanced Communication Technology},
  title     = {Intelligent Packaging and Intelligent Medicine Box for Medication Management towards the Internet-of-Things},
  year      = {2014},
  pages     = {352-360},
  abstract  = {The medication noncompliance problem has caused serious threat to public health as well as huge financial waste would wide. The emerging pervasive healthcare enabled by the Internet-of-Things offers promising solutions. In addition, an in-home healthcare station (IHHS) is needed to meet the rapidly increasing demands for daily monitoring and on-site diagnosis and prognosis. In this paper, a pervasive and preventive medication management solution is proposed based on intelligent and interactive packaging (I2Pack) and intelligent medicine box (iMedBox). The intelligent pharmaceutical packaging is sealed by the Controlled Delamination Material (CDM) and controlled by wireless communication. Various vital parameters can also be collected by wearable biomedical sensors through the wireless link. On-site diagnosis and prognosis of these vital parameters are supported by the high performance architecture. Additionally, friendly user interface is emphasized to ease the operation for the elderly, disabled, and patients. A prototyping system of the I2Pack and iMedBox is implemented and verified by field trials.},
  doi       = {10.1109/icact.2014.6779193},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1095490861},
}

@InProceedings{Ray2014,
  author    = {Partha P. Ray},
  booktitle = {2014 International Conference on Science Engineering and Management Research (ICSEMR)},
  title     = {Home Health Hub Internet of Things $({\rm H}^{3}{\rm IoT})$: An Architectural Framework for Monitoring Health of Elderly People},
  year      = {2014},
  note      = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7043542},
  pages     = {1-3},
  abstract  = {Internet of Things (IoT) has paved a path towards the digitization of everyday things connecting each other through internet. Due to the huge advent of IoT in recent years, researches have stared to accomplish the long cherished will of human being to make life simpler and better in many ways. Health being the most valuable wealth of human, should be given most priority. Though health related research implyingIoT has been neglected due to heterogeneity and interoperability issues. This literature presentstH3IoT a novel architectural framework for Home Health Hub Internet of Things for monitoring health of elderly people at home. The frameworkis promising in terms ofits design and future envision of usage of real lifeimplementation H3I0T.},
  doi       = {10.1109/icsemr.2014.7043542},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1094970902},
}

@Article{Hussain2015,
  author   = {Aamir Hussain and Rao Wenbi and Aristides Lopes da Silva and Muhammad Nadher and Muhammad Mudhish},
  journal  = {Journal of Systems and Software},
  title    = {Health and emergency-care platform for the elderly and disabled people in the Smart City},
  year     = {2015},
  number   = {IEEE Commun. Mag.522014},
  pages    = {253-263},
  volume   = {110},
  abstract = {Emergence of context-aware technologies and IoT devices reflect that the quality of a human life has become one of the most essential aspects in Smart Cities. With this goal health monitoring of elderly and disabled people have got plenty of attention and focus in the research. The healthcare systems rely on the components responsible for context sensing, processing, storage and inference, and response. In order to make the interoperability among the various healthcare systems, a typical standard is needed in order to uniformly access the context-aware healthcare information coming through a fundamental infrastructure. In this paper, we propose people-centric sensing framework for the healthcare of elderly and disabled people. Such platform is aimed to monitor health of the elderly and disabled person and provide them with a service oriented emergency response in case of abnormal health condition. We focus on three aspects: (a) context manipulation from the mobile device in people-centric environment; (b) emergency response using context base information; and (c) modeling mobile context sources as services. The most distinctive feature of current work is that medical resources are efficiently used to provide them real-time medical services in case of emergency simultaneously extending social network of the elderly people. The system implementation shows that the proposed people-centric sensing system is efficient and cost-effective in health and emergency care.},
  doi      = {10.1016/j.jss.2015.08.041},
  groups   = {Internet of Things in Healthcare},
  url      = {https://app.dimensions.ai/details/publication/pub.1025765207},
}

@InProceedings{Fazio2015,
  author    = {Maria Fazio and Antonio Celesti and Fermín Galán Márquez and Alex Glikson and Massimo Villari},
  booktitle = {2015 IEEE Symposium on Computers and Communication (ISCC)},
  title     = {Exploiting the FIWARE Cloud Platform to Develop a Remote Patient Monitoring System},
  year      = {2015},
  pages     = {264-270},
  abstract  = {FIWARE represents a new European Cloud platform that aims to land on the international ICT market bringing prominent novel advantages for societies. In fact, it provides new compelling and novel software components, available through APIs, able to give developers new valuable Cloud platform functionalities. The main contribution of this work consists in providing software architects an useful experience regarding the adoption of FIWARE for the design of a Cloud and Internet of Things (IoT) architecture. More specifically, we describe how can be possible to use the FIWARE Cloud platform to speed up the design of a real e-health Remote Patient Monitoring (RPM) architecture with an agile software development methodology. Our architecture aims to allow care givers to improve remote assistance to patients at home, optimizing the management of the workflow of doctors, physicians, medical assistants, and other involved hospital operators. In this paper, we specifically describe the main FIWARE components that we have adopted to design our architecture and how they have been integrated.},
  doi       = {10.1109/iscc.2015.7405526},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1094969147},
}

@Article{Mano2016,
  author   = {Leandro Y. Mano and Bruno S. Faiçal and Luis H.V. Nakamura and Pedro H. Gomes and Giampaolo L. Libralon and Rodolfo I. Meneguete and Geraldo P.R. Filho and Gabriel T. Giancristofaro and Gustavo Pessin and Bhaskar Krishnamachari and Jó Ueyama},
  journal  = {Computer Communications},
  title    = {Exploiting IoT technologies for enhancing Health Smart Homes through patient identification and emotion recognition},
  year     = {2016},
  pages    = {178-190},
  volume   = {89},
  abstract = {Currently, there is an increasing number of patients that are treated in-home, mainly in countries such as Japan, USA and Europe. As well as this, the number of elderly people has increased significantly in the last 15 years and these people are often treated in-home and at times enter into a critical situation that may require help (e.g. when facing an accident, or becoming depressed). Advances in ubiquitous computing and the Internet of Things (IoT) have provided efficient and cheap equipments that include wireless communication and cameras, such as smartphones or embedded devices like Raspberry Pi. Embedded computing enables the deployment of Health Smart Homes (HSH) that can enhance in-home medical treatment. The use of camera and image processing on IoT is still an application that has not been fully explored in the literature, especially in the context of HSH. Although use of images has been widely exploited to address issues such as safety and surveillance in the house, they have been little employed to assist patients and/or elderly people as part of the home-care systems. In our view, these images can help nurses or caregivers to assist patients in need of timely help, and the implementation of this application can be extremely easy and cheap when aided by IoT technologies. This article discusses the use of patient images and emotional detection to assist patients and elderly people within an in-home healthcare context. We also discuss the existing literature and show that most of the studies in this area do not make use of images for the purpose of monitoring patients. In addition, there are few studies that take into account the patient’s emotional state, which is crucial for them to be able to recover from a disease. Finally, we outline our prototype which runs on multiple computing platforms and show results that demonstrate the feasibility of our approach.},
  doi      = {10.1016/j.comcom.2016.03.010},
  groups   = {Internet of Things in Healthcare},
  url      = {https://app.dimensions.ai/details/publication/pub.1001751876},
}

@Article{Moosavi2016,
  author   = {Sanaz Rahimi Moosavi and Tuan Nguyen Gia and Ethiopia Nigussie and Amir M. Rahmani and Seppo Virtanen and Hannu Tenhunen and Jouni Isoaho},
  journal  = {Future Generation Computer Systems},
  title    = {End-to-end security scheme for mobility enabled healthcare Internet of Things},
  year     = {2016},
  pages    = {108-124},
  volume   = {64},
  abstract = {We propose an end-to-end security scheme for mobility enabled healthcare Internet of Things (IoT). The proposed scheme consists of (i) a secure and efficient end-user authentication and authorization architecture based on the certificate based DTLS handshake, (ii) secure end-to-end communication based on session resumption, and (iii) robust mobility based on interconnected smart gateways. The smart gateways act as an intermediate processing layer (called fog layer) between IoT devices and sensors (device layer) and cloud services (cloud layer). In our scheme, the fog layer facilitates ubiquitous mobility without requiring any reconfiguration at the device layer. The scheme is demonstrated by simulation and a full hardware/software prototype. Based on our analysis, our scheme has the most extensive set of security features in comparison to related approaches found in literature. Energy-performance evaluation results show that compared to existing approaches, our scheme reduces the communication overhead by 26% and the communication latency between smart gateways and end users by 16%. In addition, our scheme is approximately 97% faster than certificate based and 10% faster than symmetric key based DTLS. Compared to our scheme, certificate based DTLS consumes about 2.2 times more RAM and 2.9 times more ROM resources. On the other hand, the RAM and ROM requirements of our scheme are almost as low as in symmetric key-based DTLS. Analysis of our implementation revealed that the handover latency caused by mobility is low and the handover process does not incur any processing or communication overhead on the sensors.},
  doi      = {10.1016/j.future.2016.02.020},
  groups   = {Internet of Things in Healthcare},
  url      = {https://app.dimensions.ai/details/publication/pub.1034508886},
}

@InProceedings{Bazzani2012,
  author    = {Marco Bazzani and Davide Conzon and Andrea Scalera and Maurizio A. Spirito and Claudia Irene Trainito},
  booktitle = {2012 IEEE 11th International Conference on Trust, Security and Privacy in Computing and Communications},
  title     = {Enabling the IoT paradigm in e-health solutions through the VIRTUS middleware},
  year      = {2012},
  pages     = {1954-1959},
  abstract  = {In Europe, in a context of growing population and decreasing resources, ageing related diseases represent one of the most relevant challenges in terms of healthcare organization complexity, plus care levels and system financing balancing. Nowadays there are several researches that are studying the application of the IoT (Internet of Things) paradigm in the e-health field. This paper explains how a solution, built on the top of the VIRTUS IoT middleware, provides a valid alternative to current IoT solutions, which are mainly based on SOA (Service Oriented Architecture). VIRTUS leverage an Instant Messaging protocol (XMPP) to guarantee a (near) real-time, secure and reliable communication channel among heterogeneous devices. The presented development has been exploited in a healthcare case study: an implementation of a cost-savvy remote body movement monitoring system, aimed at classify daily patients' activities, designed as a modular architecture and deployed in a large scale scenario. The paper analyzes the features offered by the VIRTUS middleware, if used within a e-health solution, providing a comparison with other well-known systems.},
  doi       = {10.1109/trustcom.2012.144},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1095382199},
}

@InProceedings{Khoi2015,
  author    = {Ngo Manh Khoi and Saguna Saguna and Karan Mitra and Christer Ahlund},
  booktitle = {2015 17th International Conference on E-health Networking, Application & Services (HealthCom)},
  title     = {IREHMO: An Efficient IOT-Based Remote Health Monitoring System for Smart Regions},
  year      = {2015},
  note      = {http://ltu.diva-portal.org/smash/get/diva2:1005647/FULLTEXT01},
  pages     = {563-568},
  abstract  = {The ageing population worldwide is constantly rising, both in urban and regional areas. There is a need for IoT-based remote health monitoring systems that take care of the health of elderly people without compromising their convenience and preference of staying at home. However, such systems may generate large amounts of data. The key research challenge addressed in this paper is to efficiently transmit healthcare data within the limit of the existing network infrastructure, especially in remote areas. In this paper, we identified the key network requirements of a typical remote health monitoring system in terms of real-time event update, bandwidth requirements and data generation. Furthermore, we studied the network communication protocols such as CoAP, MQTT and HTTP to understand the needs of such a system, in particular the bandwidth requirements and the volume of generated data. Subsequently, we have proposed IReHMo - an IoT-based remote health monitoring architecture that efficiently delivers healthcare data to the servers. The CoAP-based IReHMo implementation helps to reduce up to 90 % volume of generated data for a single sensor event and up to 56 % required bandwidth for a healthcare scenario. Finally, we conducted a scalability analysis to determine the feasibility of deploying IReHMo in large numbers in regions of north Sweden.},
  doi       = {10.1109/healthcom.2015.7454565},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1095770589},
}

@InProceedings{Zgheib2015,
  author    = {Rita Zgheib and Rémi Bastide and Emmanuel Conchon},
  booktitle = {2015 International Conference on Computational Science and Computational Intelligence (CSCI)},
  title     = {A Semantic Web-of-Things Architecture for Monitoring the Risk of Bedsores},
  year      = {2015},
  pages     = {318-323},
  abstract  = {Bedsores are a common injury that mainly plagues elders and frail persons, and are a major cause of concerns in medical institutions. We present a system based on the Internet-of-Things technologies, aiming at detecting the risk of bedsores using sensor fusion. This paper mainly focuses on the software architecture of the proposed system, based on the principles of weak coupling and of semantic data exchange. We present a model of the application in terms of the Semantic Sensor Network (SSN) ontology.},
  doi       = {10.1109/csci.2015.128},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1093739978},
}

@InBook{Sinharay2016,
  author    = {Arijit Sinharay and Arpan Pal and Snehasis Banerjee and Rohan Banerjee and Soma Bandyopadhyay and Parijat Deshpande and Ranjan Dasgupta},
  pages     = {536-542},
  title     = {A Novel Approach to Unify Robotics, Sensors, and Cloud Computing Through IoT for a Smarter Healthcare Solution for Routine Checks and Fighting Epidemics},
  year      = {2016},
  abstract  = {This paper attempts to project a novel concept where medical sensors, cloud computing and robotic platform are unified to offer state-of-the-art healthcare solutions to a wide variety of scenarios. The proposed solution is most effective if there is scarcity of healthcare providers or if putting them in the field expose them into a high risk environment such as fighting epidemics. In addition, the proposed system will also benefit routine checks in quarantine wards of hospitals where human reluctance of performing routine task by the healthcare providers can be avoided. Finally, it can also assist a doctor as a decision support system by using machine’s capability of number crunching while it examines through patient’s complete history, goes through every medical test reports and then applies data mining for catching possible ailments from his/her symptoms.},
  booktitle = {Internet of Things. IoT Infrastructures},
  doi       = {10.1007/978-3-319-47063-4_59},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1031978033},
}

@Article{Gupta2016,
  author   = {P. K. Gupta and B. T. Maharaj and Reza Malekian},
  journal  = {Multimedia Tools and Applications},
  title    = {A novel and secure IoT based cloud centric architecture to perform predictive analysis of users activities in sustainable health centres},
  year     = {2016},
  number   = {18},
  pages    = {18489-18512},
  volume   = {76},
  abstract = {Diabetes, blood pressure, heart, and kidney, some of the diseases common across the world, are termed ’silent killers’. More than 50 % of the world’s population are affected by these diseases. If suitable steps are not taken during the early stages then severe complications occur from these diseases. In the work proposed, we have discussed the manner in which the Internet-of-Things based Cloud centric architecture is used for predictive analysis of physical activities of the users in sustainable health centers. The architecture proposed is based on the embedded sensors of the equipment rather than using wearable sensors or Smartphone sensors to store the value of the basic health-related parameters. Cloud centric architecture is composed of a Cloud data center, Public cloud, Private cloud, and uses the XML Web services for secure and fast communication of information. The architecture proposed here is evaluated for its adoption, prediction analysis of physical activities, efficiency, and security. From the results obtained it can be seen that the overall response between the local database server and Cloud data center remains almost constant with the rise in the number of users. For prediction analysis, If the results collected in real time for the analysis of physical activities exceed any of the parameter limits of the defined threshold value then an alert is sent to the health care personnel. Security analysis also shows the effective encryption and decryption of information. The architecture presented is effective and reduces the proliferation of information. It is also suggested, that a person suffering from any of the diseases mentioned above can defer the onset of complications by doing regular physical activities.},
  doi      = {10.1007/s11042-016-4050-6},
  groups   = {Internet of Things in Healthcare},
  url      = {https://app.dimensions.ai/details/publication/pub.1016352338},
}

@InBook{Woznowski2016,
  author    = {Przemyslaw Woznowski and Alison Burrows and Tom Diethe and Xenofon Fafoutis and Jake Hall and Sion Hannuna and Massimo Camplani and Niall Twomey and Michal Kozlowski and Bo Tan and Ni Zhu and Atis Elsts and Antonis Vafeas and Adeline Paiement and Lili Tao and Majid Mirmehdi and Tilo Burghardt and Dima Damen and Peter Flach and Robert Piechocki and Ian Craddock and George Oikonomou},
  pages     = {315-333},
  title     = {SPHERE: A Sensor Platform for Healthcare in a Residential Environment},
  year      = {2016},
  abstract  = {It can be tempting to think about smart homes like one thinks about smart cities. On the surface, smart homes and smart cities comprise coherent systems enabled by similar sensing and interactive technologies. It can also be argued that both are broadly underpinned by shared goals of sustainable development, inclusive user engagement and improved service delivery. However, the home possesses unique characteristics that must be considered in order to develop effective smart home systems that are adopted in the real world [37].},
  booktitle = {Designing, Developing, and Facilitating Smart Cities},
  doi       = {10.1007/978-3-319-44924-1_14},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1038434872},
}

@InProceedings{Ivascu2015,
  author    = {Todor Ivascu and Bogdan Manate and Viorel Negru},
  booktitle = {2015 17th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)},
  title     = {A Multi-Agent Architecture for Ontology-Based Diagnosis of Mental Disorders},
  year      = {2015},
  pages     = {423-430},
  abstract  = {This paper presents a Multi-agent system that facilitates the remote monitoring of the elderly patients which are susceptible to mental disorder diseases. In order to find early signs of health condition depreciation we have assessed four of the most common mental disorder diseases to find which kind of sensors can detect specific symptoms with the main purpose of creating an early warning system. The diagnosis component is based on an ontology that defines the relations between sensors, symptoms and diseases. Based on these relationships a specialized agent can inform the medical personnel about the detected symptoms.},
  doi       = {10.1109/synasc.2015.69},
  groups    = {Internet of Things in Healthcare},
  url       = {https://app.dimensions.ai/details/publication/pub.1095255025},
}

@Article{Chakraborty2020,
  author   = {Indranil Chakraborty and Deboleena Roy and Isha Garg and Aayush Ankit and Kaushik Roy},
  journal  = {Nature Machine Intelligence},
  title    = {Constructing energy-efficient mixed-precision neural networks through principal component analysis for edge intelligence},
  year     = {2020},
  number   = {1},
  pages    = {43-55},
  volume   = {2},
  abstract = {The ‘Internet of Things’ has brought increased demand for artificial intelligence-based edge computing in applications ranging from healthcare monitoring systems to autonomous vehicles. Quantization is a powerful tool to address the growing computational cost of such applications and yields significant compression over full-precision networks. However, quantization can result in substantial loss of performance for complex image classification tasks. To address this, we propose a principal component analysis (PCA)-driven methodology to identify the important layers of a binary network, and design mixed-precision networks. The proposed Hybrid-Net achieves a more than 10% improvement in classification accuracy over binary networks such as XNOR-Net for ResNet and VGG architectures on CIFAR-100 and ImageNet datasets, while still achieving up to 94% of the energy efficiency of XNOR-Nets. This work advances the feasibility of using highly compressed neural networks for energy-efficient neural computing in edge devices.},
  doi      = {10.1038/s42256-019-0134-0},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1124148063},
}

@Article{Ullrich2017,
  author   = {Karen Ullrich and Edward Meeds and Max Welling},
  journal  = {arXiv},
  title    = {Soft Weight-Sharing for Neural Network Compression},
  year     = {2017},
  abstract = {The success of deep learning in numerous application domains created the de-
sire to run and train them on mobile devices. This however, conflicts with
their computationally, memory and energy intense nature, leading to a growing
interest in compression. Recent work by Han et al. (2015a) propose a pipeline
that involves retraining, pruning and quantization of neural network weights,
obtaining state-of-the-art compression rates. In this paper, we show that
competitive compression rates can be achieved by using a version of soft
weight-sharing (Nowlan & Hinton, 1992). Our method achieves both quantization
and pruning in one simple (re-)training procedure. This point of view also
exposes the relation between compression and the minimum description length
(MDL) principle.},
  doi      = {10.48550/arxiv.1702.04008},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1118735394},
}

@InProceedings{Bin2018,
  author    = {Bin Bin and Richard Millham and Simon Fong},
  booktitle = {2018 Conference on Information Communications Technology and Society (ICTAS)},
  title     = {Data Stream Mining in Fog Computing Environment with Feature Selection Using Ensemble of Swarm Search Algorithms},
  year      = {2018},
  pages     = {1-6},
  abstract  = {Fog computing emerged as a contemporary strategy to process big streaming data efficiently. It is designed as a distributed computing platform for supporting the data analytics for Internet of Things (IoT) applications that pushes the data analytics from Cloud server to the far edge of a sensor network. As the name suggests, ubiquitous data which is collected from the sensors are processed locally rather than on the central servers. Fog computing helps avoid performance bottleneck at the center point and relieves raw data from overwhelming towards the center of the network. However, suitable data analysis algorithms such as those of data stream mining that are consist of learning and recognizing patterns from the incoming data streams must be fast and accurate enough for supporting Fog computing. This paper reports about a computer simulation of running data stream mining algorithms in Fog environment. Furthermore, feature selection that is powered by swarm search is used as a preprocessing method for improving the accuracy and speed of local Fog data analytics. Through the experiment, the results reveal which algorithms are the best choice to deliver edge intelligence in Fog computing environment.},
  doi       = {10.1109/ictas.2018.8368770},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1104338333},
}

@InProceedings{Sanchez2018,
  author    = {Justin Sanchez and Nasim Soltani and Ramachandra Chamarthi and Adarsh Sawant and Hamed Tabkhi},
  booktitle = {2018 IEEE High Performance extreme Computing Conference (HPEC)},
  title     = {A Novel 1D-Convolution Accelerator for Low-Power Real-time CNN processing on the Edge},
  year      = {2018},
  pages     = {1-8},
  abstract  = {With the rise of deep learning, the demand for real-time edge intelligence is greater than ever. Current algorithm and hardware realizations often focus on the cloud paradigm and maintain the assumption that the entire frames data is available in large batches. As a result, obtaining real-time AI inference at the edge has been a tough goal due to tight-latency awareness as well as streaming nature of the data. There is an inherent need for novel architectures that can realize latency-aware agile deep learning algorithms at the edge. This paper introduces a novel joint algorithm architecture approach to enable real-time low-power Convolutional Neural Network (CNN) processing on edge devices. The core of the proposed approach is utilizing 1D dimensional convolution with an architecture that can truly benefit from the algorithm optimization. On the algorithm side, we present a novel training and inference based on 1D convolution. On the architecture side, we present a novel data flow architecture with the capability of performing on-the-fly 1D convolution over the pixel stream. Our results on Xilinx Zynq-7000 FPGA for SqueezeNet demonstrates only 2% lost in accuracy while maintaining real-time processing of 60 frames per second with only 1.73W power consumption. The Dynamic power consumption is 7.3X lower than regular 2D convolution CNN for performing the same frame rate, and 4.3X less than Nvidia Jetson TX2 total power, performing only 30 frame per second.},
  doi       = {10.1109/hpec.2018.8547530},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1110363845},
}

@InProceedings{GonzalezGuerrero2019,
  author    = {Patricia Gonzalez-Guerrero and Tommy Tracy and Xinfei Guo and Mircea R. Stan},
  booktitle = {2019 Tenth International Green and Sustainable Computing Conference (IGSC)},
  title     = {Towards low-power random forest using asynchronous computing with streams},
  year      = {2019},
  pages     = {1-5},
  abstract  = {We propose a sensor architecture for the internet of things (IoT), smartdust or edge-intelligence (EI) that combines near-analog-memory (NAM) processing and asynchronous computing with streams (ACS) addressing the need for machine learning (ML) capabilities at low power budgets. In ACS an analog value is mapped to an asynchronous stream that can take one of two values (vh, vl). This stream-based data representation enables area-power efficient computing units such as the multiplier implemented as an AND gate yielding savings in power of 90% compared with digital approaches. However, a major bottleneck for computing on streams, vision sensors, and NAM approaches is the cost of analog-to-digital (ADC) and digital-to-stream-to-digital converters. Our NAM-ACS architecture, simplifies the sensor and eliminates the need for the expensive conversions. The architecture is tailored for random forest (Raf), a ML algorithm, chosen for its ability to classify using a reduced number of features. Our simulations show that using an analog-memory array of 256 512, the power consumption of the ACS-core combined with the memory interface is comparable with the consumption of an ADC based memory interface, obtaining an accuracy of 83%.},
  doi       = {10.1109/igsc48788.2019.8957193},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1124081996},
}

@InProceedings{Munir2019,
  author    = {Shirajum Munir and Sarder Fakhrul Abedin and Choong Seon Hong},
  booktitle = {2019 20th Asia-Pacific Network Operations and Management Symposium (APNOMS)},
  title     = {Artificial Intelligence-based Service Aggregation for Mobile-Agent in Edge Computing},
  year      = {2019},
  pages     = {1-6},
  abstract  = {The ongoing development of edge computing in fifth-generation (5G) networks promises to provide an artificial intelligence-as-a-service (AIaaS) for meeting the stringent requirements of everything as a service (XaaS) in the edge of the networks. Therefore, the concept of edge-artificial intelligence (edge-AI) is not only evolving but also emergent enabler toward AI service fulfillment. In this paper, we investigate an AI-based service aggregation problem for a mobile agent in AIaaS-enabled edge computing. First, we propose an optimization problem for the mobile agent and the objective is to maximize the AI service fulfillment achieved rate while satisfying the computational, memory, and delay requirements. Thus, we show that this optimization problem is NP-hard. Second, we compel the formulated problem in a community discovery problem and derive a solution by executing a data-driven approach. To do this, we incorporate density-based spatial clustering of applications with noise (DBSCAN) and flow control algorithm, and propose a low computational complexity algorithm for AI service aggregation of the mobile agent. Finally, numerical analysis shows the proposed model can perform better over other baseline methods in terms of deprived AI services, server utilization, and complexity analysis.},
  doi       = {10.23919/apnoms.2019.8892984},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1122515167},
}

@InBook{Yang2018,
  author    = {Yingyi Yang and Xiaoming Mai and Hao Wu and Ming Nie and Hui Wu},
  pages     = {508-523},
  title     = {POWER: A Parallel-Optimization-Based Framework Towards Edge Intelligent Image Recognition and a Case Study},
  year      = {2018},
  abstract  = {To improve the intelligent image recognition abilities of edge devices, a parallel-optimization-based framework called POWER is introduced in this paper. With FPGA (Field-Programmable Gate Array) as its hardware module, POWER provides well extensibility and flexible customization capability for developing intelligent firmware suitable for different types of edge devices in various scenarios. Through an actual case study, we design and implement a firmware prototype following the specification of POWER and explore its performance improvement using parallel optimization. Our experimental results show that the firmware prototype we implement exhibits good performance and is applicable to substation inspection robots, which also validate the effectiveness of our POWER framework in designing edge intelligent firmware modules indirectly.},
  booktitle = {Algorithms and Architectures for Parallel Processing},
  doi       = {10.1007/978-3-030-05051-1_35},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1110420318},
}

@InProceedings{Montino2019,
  author    = {Pietro Montino and Danilo Pau},
  booktitle = {2019 IEEE 5th International forum on Research and Technology for Society and Industry (RTSI)},
  title     = {Environmental Intelligence for Embedded Real-time Traffic Sound Classification},
  year      = {2019},
  pages     = {45-50},
  abstract  = {In this paper a prototype to classify sounds emitted by car engines to be used for urban traffic management in smart cities is presented. The solution is based on Artificial Neural Network (ANN) executed on a resource constrained low cost embedded micro controller integrated into a sensing unit very close to the microphone. The prototype operates without the need of being connected to a remote server through an always-on connectivity. The adoption of an on-the-edge artificial intelligence architecture brings a set of advantages: safety, reliability, promptness and low power consumption. The embedded intelligence is trained from a purpose-built dataset enclosing environmental car engine sound data. A pre trained ANN classifies sound events and counts cars approaching and leaving the microphone by sensing the Doppler Effects in the sound emitted by the moving vehicles. The results showed in this paper refer to the application running on the Sensortile Development Kit board from STMicroelectronics, featuring 128KB of RAM and 1MB of Flash memory. The development of the proposed solution was somehow inspired by the UN 2030 Agenda for Sustainable Development. The Agenda defines a plan of action for people, planet and prosperity. It does so by defining 17 Sustainable Development Goals (SDGs) to enable the transformation into a more peaceful and free society. Among them, the ones which interested the authors to develop the proposed solution were the following; Goal 3: “Ensure healthy lives and promote well-being for all at all ages” Goal 11: “Make cities and human settlements inclusive, safe, resilient and sustainable” Goal 13: “Take urgent action to combat climate change and its impacts”.},
  doi       = {10.1109/rtsi.2019.8895517},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1122519012},
}

@InProceedings{Chang2019,
  author    = {Yijia Chang and Xi Huang and Ziyu Shao and Yang Yang},
  booktitle = {2019 IEEE Global Communications Conference (GLOBECOM)},
  title     = {An Efficient Distributed Deep Learning Framework for Fog-based IoT Systems},
  year      = {2019},
  pages     = {1-6},
  abstract  = {Deep neural networks (DNNs) are the key techniques to enable edge/fog intelligence. By far, it remains challenging to conduct distributed deployment of DNN models onto resource-constrained fog nodes with low latency. Existing solutions adopt either model compression techniques to reduce the computation loads on fog nodes, or horizontal model partition techniques, which exploit particular communication and computation patterns to partition different layers of DNNs onto fog nodes. Nonetheless, sometimes even resource demands of particular layers can be unaffordable to fog nodes, which makes horizontal partition inadequate and calls for the joint design of vertical and horizontal model partition. Besides, model partition and compression may lead to degraded inference accuracy, but approaches to compensate such accuracy loss remain unexplored. In this paper, we propose an integrated efficient distributed deep learning (EDDL) framework to address the above challenges. Particularly, we adopt balanced incomplete block design (BIBD) methods to reduce computation loads on fog nodes by removing some data flows in DNNs in a systematic and structured manner. By leveraging grouped convolution techniques, we propose a practical scheme to conduct horizontal and vertical model partition jointly. Moreover, we integrate multi-task learning and ensemble learning techniques to further improve the inference accuracy. Simulation results verify the effectiveness of EDDL framework in achieving notable reduction in computation load and memory footprint with mild loss of inference accuracy.},
  doi       = {10.1109/globecom38437.2019.9014056},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1125166037},
}

@Article{Wu2020,
  author   = {Huaqing Wu and Feng Lyu and Conghao Zhou and Jiayin Chen and Li Wang and Xuemin Shen},
  journal  = {IEEE Journal on Selected Areas in Communications},
  title    = {Optimal UAV Caching and Trajectory in Aerial-Assisted Vehicular Networks: A Learning-Based Approach},
  year     = {2020},
  number   = {12},
  pages    = {2783-2797},
  volume   = {38},
  abstract = {In this article, we investigate the UAV-aided edge caching to assist terrestrial vehicular networks in delivering high-bandwidth content files. Aiming at maximizing the overall network throughput, we formulate a joint caching and trajectory optimization (JCTO) problem to make decisions on content placement, content delivery, and UAV trajectory simultaneously. As the decisions interact with each other and the UAV energy is limited, the formulated JCTO problem is intractable directly and timely. To this end, we propose a deep supervised learning scheme to enable intelligent edge for real-time decision-making in the highly dynamic vehicular networks. In specific, we first propose a clustering-based two-layered (CBTL) algorithm to solve the JCTO problem offline. With a given content placement strategy, we devise a time-based graph decomposition method to jointly optimize the content delivery and trajectory design, with which we then leverage the particle swarm optimization (PSO) algorithm to further optimize the content placement. We then design a deep supervised learning architecture of the convolutional neural network (CNN) to make fast decisions online. The network density and content request distribution with spatio-temporal dimensions are labeled as channeled images and input to the CNN-based model, and the results achieved by the CBTL algorithm are labeled as model outputs. With the CNN-based model, a function which maps the input network information to the output decision can be intelligently learnt to make timely inference and facilitate online decisions. We conduct extensive trace-driven experiments, and our results demonstrate both the efficiency of CBTL in solving the JCTO problem and the superior learning performance with the CNN-based model.},
  doi      = {10.1109/jsac.2020.3005469},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1128852448},
}

@Article{Liu2020,
  author   = {Jinglan Liu and Jiaxin Zhang and Yukun Ding and Xiaowei Xu and Meng Jiang and Yiyu Shi},
  journal  = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  title    = {Binarizing Weights Wisely for Edge Intelligence: Guide for Partial Binarization of Deconvolution-Based Generators},
  year     = {2020},
  note     = {https://doi.org/10.1109/tcad.2020.2983370},
  number   = {12},
  pages    = {4748-4759},
  volume   = {39},
  abstract = {This article explores the weight binarization of the deconvolution-based generator in a generative adversarial network (GAN) for memory saving and speedup of image construction on the edge. This article suggests that different from convolutional neural networks (including the discriminator) where all layers can be binarized, only some of the layers in the generator can be binarized without significant performance loss. Supported by theoretical analysis and verified by experiments, a direct metric based on the dimension of deconvolution operations is established, which can be used to quickly decide which layers in a generator can be binarized. Our results also indicate that both the generator and the discriminator should be binarized simultaneously for balanced competition and better performance during training. The experimental results on CelebA dataset with DCGAN and original loss functions suggest that directly applying state-of-the-art binarization techniques to all the layers of the generator will lead to $2.83\times $ performance loss measured by sliced Wasserstein distance compared with the original generator, while applying them to selected layers only can yield up to $25.81\times $ saving in memory consumption, and $1.96\times $ and $1.32\times $ speedup in inference and training, respectively, with little performance loss. Similar conclusions can also be drawn on other loss functions for different GANs.},
  doi      = {10.1109/tcad.2020.2983370},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1125925617},
}

@InProceedings{Xie2019,
  author    = {Feiyi Xie and Aidong Xu and Yixin Jiang and Songlin Chen and Runfa Liao and Hong Wen},
  booktitle = {2019 14th International Conference on Computer Science & Education (ICCSE)},
  title     = {Edge Intelligence based Co-training of CNN},
  year      = {2019},
  pages     = {830-834},
  abstract  = {Improving training efficiency is a long-term major topic of neural network. With the popularity of intelligent terminal devices, such as smart phones and smart household electrical appliances, edge computing is gradually developing to edge intelligence (EI), which provides support to multiple terminals and play a role of a mini cloud. By making full use of the computing power of intelligent terminals in EI, while cooperating with edge servers, the training efficiency of convolution neural network (CNN) can effectively be improved. In this paper, an EI based co-training model between the edge server and intelligent terminals is proposed. The edge server and terminals propagate part of the CNN separately to accelerate the training of the CNN. Running time is greatly cut down by the division of CNN and simultaneous propagation of multiple terminals.},
  doi       = {10.1109/iccse.2019.8845531},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1121194812},
}

@Article{Zeng2019,
  author   = {Liekang Zeng and En Li and Zhi Zhou and Xu Chen},
  journal  = {IEEE Network},
  title    = {Boomerang: On-Demand Cooperative Deep Neural Network Inference for Edge Intelligence on the Industrial Internet of Things},
  year     = {2019},
  number   = {5},
  pages    = {96-103},
  volume   = {33},
  abstract = {With the revolution of smart industry, more and more Industrial Internet of Things (IIoT) devices as well as AI algorithms are deployed to achieve industrial intelligence. While applying computation-intensive deep learning on IIoT devices, however, it is challenging to meet the critical latency requirement for industrial manufacturing. Traditional wisdom resorts to the cloud-centric paradigm but still works either inefficiently or ineffectively due to the heavy transmission latency overhead. To address this challenge, we propose Boomerang, an on-demand cooperative DNN inference framework for edge intelligence under the IIoT environment. Boomerang exploits DNN right-sizing and DNN partition to execute DNN inference tasks with low latency as well as high accuracy. DNN right-sizing reshapes the amount of DNN computation via the early-ex-it mechanism so as to reduce the total runtime of DNN inference. DNN partition adaptively segments DNN computation between the IoT devices and the edge server in order to leverage hybrid computation resources to achieve DNN inference immediacy. Combining these two keys, Boomerang carefully selects the partition point and the exit point to maximize the performance while promising the efficiency requirement. To further reduce the manual overhead of model profiling at the install phase, we develop an advanced version of Boomerang with the DRL model, achieving end-to-end automatic DNN inference plan generation. The prototype implementation and evaluations demonstrate the effectiveness of Boomerang on both versions in achieving efficient edge intelligence for IIoT.},
  doi      = {10.1109/mnet.001.1800506},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1121628094},
}

@InProceedings{Plastiras2018,
  author    = {George Plastiras and Maria Terzi and Christos Kyrkou and Theocharis Theocharidcs},
  booktitle = {2018 IEEE 29th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
  title     = {Edge Intelligence: Challenges and Opportunities of Near-Sensor Machine Learning Applications},
  year      = {2018},
  note      = {https://zenodo.org/records/1406962/files/PID5438837.pdf},
  pages     = {1-7},
  abstract  = {The number of connected IoT devices is expected to reach over 20 billion by 2020. These range from basic sensor nodes that log and report the data for cloud processing, to the ones on the edge, that are capable of processing and analyzing the incoming information and taking an action accordingly. Machine learning, and in particular deep learning, is the defacto processing paradigm for intelligently processing these immense volumes of data. However, the resource inhibited environment of edge devices, owing to their limited energy budget, and low compute capabilities, render them a challenging platform for deployment of desired data analytics, particularly in realtime applications. In this paper therefore, we argue that for a wide range of emerging applications edge intelligence is a necessary evolutionary need, and thus we provide a summary of the challenges and opportunities that arise from this need. We showcase through a case study regarding computer vision for commercial drones, how these opportunities can be taken advantage, and how some of the challenges can be potentially addressed.},
  doi       = {10.1109/asap.2018.8445118},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1106394770},
}

@Article{Zhang2019,
  author   = {Yin Zhang and Xiao Ma and Jing Zhang and M. Shamim Hossain and Ghulam Muhammad and Syed Umar Amin},
  journal  = {IEEE Network},
  title    = {Edge Intelligence in the Cognitive Internet of Things: Improving Sensitivity and Interactivity},
  year     = {2019},
  number   = {3},
  pages    = {58-64},
  volume   = {33},
  abstract = {A new network paradigm, CIoT, has been proposed by applying cognitive computing technologies, which is derived from cognitive science and artificial intelligence in combination with the data generated by connected IoT devices and the actions that these devices perform. The development of cognitive computing is very important in the above process to meet key technical challenges, such as generation of big sensory data, efficient computing/storage at the CIoT edge, and integration of multiple data sources and types. On the other hand, to evolve with the new computing and communication paradigms, the CIoT ecosystem has to update by absorbing new capabilities such as deep learning, the CIoT sensing system, data analytics, and cognitiion in providing human-like intelligence.},
  doi      = {10.1109/mnet.2019.1800344},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1116154247},
}

@InBook{Guo2020,
  author    = {Ruiqi Guo and Yingmeng Xiang and Zeyu Mao and Zhehan Yi and Xiaoying Zhao and Di Shi},
  pages     = {23-36},
  title     = {Artificial Intelligence Enabled Online Non-intrusive Load Monitoring Embedded in Smart Plugs},
  year      = {2020},
  abstract  = {Abstract
As an Internet-of-Things (IoT) device for smart homes, smart plugs have been pervasive in households, which enable users to monitor and control their electrical appliances remotely and automatically. It is promising that, the networks of smart plugs in the power system will enable autonomous demand response for optimal grid operation. This benefits power systems from several aspects, e.g., enhancing renewable penetration and reducing the peak load. In order to facilitate energy management and minimize the impact of load shedding on the customers, it is meaningful to know the type of appliances connected to the smart plugs in a real-time and non-intrusive manner. Conventionally, the online non-intrusive load monitoring (NILM) is conducted in a central server, and it requires a large number of measurements transmitted to the cloud, which can impose a huge communication burden. In this paper, using the edge-computing capability of the smart plugs, some lightweight artificial intelligence-based NILM algorithms are developed and implemented inside the smart plugs. These practical algorithms are validated using massive hardware experiments. Case studies indicate that high accuracy can be achieved for NILM with limited measurements and limited storage.},
  booktitle = {Advances in Signal Processing and Intelligent Recognition Systems},
  doi       = {10.1007/978-981-15-4828-4_3},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1127250882},
}

@Article{Lu2020,
  author   = {Sen Lu and Abhronil Sengupta},
  journal  = {Frontiers in Neuroscience},
  title    = {Exploring the Connection Between Binary and Spiking Neural Networks},
  year     = {2020},
  note     = {https://www.frontiersin.org/articles/10.3389/fnins.2020.00535/pdf},
  pages    = {535},
  volume   = {14},
  abstract = {On-chip edge intelligence has necessitated the exploration of algorithmic techniques to reduce the compute requirements of current machine learning frameworks. This work aims to bridge the recent algorithmic progress in training Binary Neural Networks and Spiking Neural Networks-both of which are driven by the same motivation and yet synergies between the two have not been fully explored. We show that training Spiking Neural Networks in the extreme quantization regime results in near full precision accuracies on large-scale datasets like CIFAR-100 and ImageNet. An important implication of this work is that Binary Spiking Neural Networks can be enabled by "In-Memory" hardware accelerators catered for Binary Neural Networks without suffering any accuracy degradation due to binarization. We utilize standard training techniques for non-spiking networks to generate our spiking networks by conversion process and also perform an extensive empirical analysis and explore simple design-time and run-time optimization techniques for reducing inference latency of spiking networks (both for binary and full-precision models) by an order of magnitude over prior work. Our implementation source code and trained models are available at https://github.com/NeuroCompLab-psu/SNN-Conversion.},
  doi      = {10.3389/fnins.2020.00535},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1128716869},
}

@Article{GomezCarmona2020,
  author   = {Oihane Gómez-Carmona and Diego Casado-Mansilla and Frank Alexander Kraemer and Diego López-de-Ipiña and Javier García-Zubia},
  journal  = {Future Generation Computer Systems},
  title    = {Exploring the computational cost of machine learning at the edge for human-centric Internet of Things},
  year     = {2020},
  note     = {https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/11250/2730784/2/2020-gomez-carmona-fgcs.pdf},
  pages    = {670-683},
  volume   = {112},
  abstract = {In response to users’ demand for privacy, trust and control over their data, executing machine learning tasks at the edge of the system has the potential to make the Internet of Things (IoT) applications and services more human-centric. This implies moving complex computation to a local stage, where edge devices must balance the computational cost of the machine learning techniques to meet the available resources. Thus, in this paper, we analyze all the factors affecting the classification process and empirically evaluate their impact in terms of performance and cost. We put the focus on Human Activity Recognition (HAR) systems, which represent a standard type of classification problems in human-centered IoT applications. We present a holistic optimization approach through input data reduction and feature engineering that aims to enhance all the stages of the classification pipeline and integrate both inference and training at the edge. The results of the conducted evaluation show that there is a highly non-linear trade-off to make between the computational cost, in terms of processing time, and the achieved classification accuracy. In the presented case of study, the computational effort can be reduced by 80% assuming a decline of the classification accuracy of only 3%. The potential impact of the optimization strategy highlights the importance of understanding the initial data and studying the most relevant characteristics of the signal to meet the cost–accuracy requirements. This would contribute to bringing embedded machine learning to the edge and, hence, creating spaces where human and machine intelligence could collaborate.},
  doi      = {10.1016/j.future.2020.06.013},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1128479095},
}

@InProceedings{Xu2019,
  author    = {Chuanhua Xu and Mianxiong Dong and Kaoru Ota and Jianhua Li and Wu Yang and Jun Wu},
  booktitle = {2019 IEEE Global Communications Conference (GLOBECOM)},
  title     = {SCEH: Smart Customized E-health Framework for Countryside Using Edge AI and Body Sensor Networks},
  year      = {2019},
  pages     = {1-6},
  abstract  = {Due to the shortage and unbalance of medical resources, it is difficult for patients in the countryside to get high-quality and timely medical services from the central medical facility. Existing researches of fog e-health has the potential of providing real-time medical services for the countryside with body sensor networks (BSN), but there are two limitations. On one hand, because of the medical services requiring not only low-latency but also high-quality, constructing an AI e-health service on resource-constrained fog with edge AI is necessary but unsolved. On the other hand, because of the regional differences in disease risk, there is a lack of an effective mechanism to provide a customized fog AI e-health service for patients in different regions. To address these issues, a smart customized e-health (SCEH) framework is proposed in this paper to provide edge-intelligent and customized medical services for the countryside. Firstly, semantics-based lightweight and meticulous load management mechanism is designed to reduce data load and involve medical semantic. Secondly, model-ensemble based fog AI collaborative analysis mechanism is proposed for load balance and knowledge integration. Thirdly, an attention-weight based customized fog AI e-health generation mechanism is devised for regional medical model reconstruction. The simulation results demonstrate the effectiveness of SCEH which ensures both the accuracy and low latency of fog e-health with limited resource.},
  doi       = {10.1109/globecom38437.2019.9014057},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1125166251},
}

@InProceedings{Zhang2019a,
  author    = {Xingzhou Zhang and Yifan Wang and Sidi Lu and Liangkai Liu and Lanyu Xu and Weisong Shi},
  booktitle = {2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)},
  title     = {OpenEI: An Open Framework for Edge Intelligence},
  year      = {2019},
  note      = {https://arxiv.org/pdf/1906.01864},
  pages     = {1840-1851},
  abstract  = {In the last five years, edge computing has attracted tremendous attention from industry and academia due to its promise to reduce latency, save bandwidth, improve availability, and protect data privacy to keep data secure. At the same time, we have witnessed the proliferation of AI algorithms and models which accelerate the successful deployment of intelligence mainly in cloud services. These two trends, combined together, have created a new horizon: Edge Intelligence (EI). The development of EI requires much attention from both the computer systems research community and the AI community to meet these demands. However, existing computing techniques used in the cloud are not applicable to edge computing directly due to the diversity of computing sources and the distribution of data sources. We envision that there missing a framework that can be rapidly deployed on edge and enable edge AI capabilities. To address this challenge, in this paper we first present the definition and a systematic review of EI. Then, we introduce an Open Framework for Edge Intelligence (OpenEI), which is a lightweight software platform to equip edges with intelligent processing and data sharing capability. We analyze four fundamental EI techniques which are used to build OpenEI and identify several open problems based on potential research directions. Finally, four typical application scenarios enabled by OpenEI are presented.},
  doi       = {10.1109/icdcs.2019.00182},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1122235047},
}

@Article{Palossi2019,
  author   = {Daniele Palossi and Antonio Loquercio and Francesco Conti and Eric Flamand and Davide Scaramuzza and Luca Benini},
  journal  = {IEEE Internet of Things Journal},
  title    = {A 64-mW DNN-Based Visual Navigation Engine for Autonomous Nano-Drones},
  year     = {2019},
  note     = {https://www.research-collection.ethz.ch/bitstream/20.500.11850/372773/3/DroNet_on_PULP_OA.pdf},
  number   = {5},
  pages    = {8357-8371},
  volume   = {6},
  abstract = {Fully miniaturized robots (e.g., drones), with artificial intelligence (AI)-based visual navigation capabilities, are extremely challenging drivers of Internet-of-Things edge intelligence capabilities. Visual navigation based on AI approaches, such as deep neural networks (DNNs) are becoming pervasive for standard-size drones, but are considered out of reach for nano-drones with a size of a few cm2. In this paper, we present the first (to the best of our knowledge) demonstration of a navigation engine for autonomous nano-drones capable of closed-loop end-to-end DNN-based visual navigation. To achieve this goal we developed a complete methodology for parallel execution of complex DNNs directly on board resource-constrained milliwatt-scale nodes. Our system is based on GAP8, a novel parallel ultralow-power computing platform, and a 27-g commercial, open-source Crazyflie 2.0 nano-quadrotor. As part of our general methodology, we discuss the software mapping techniques that enable the DroNet state-of-the-art deep convolutional neural network to be fully executed aboard within a strict 6 frame-per-second real-time constraint with no compromise in terms of flight results, while all processing is done with only 64 mW on average. Our navigation engine is flexible and can be used to span a wide performance range: at its peak performance corner, it achieves 18 frames/s while still consuming on average just 3.5% of the power envelope of the deployed nano-aircraft. To share our key findings with the embedded and robotics communities and foster further developments in autonomous nano-unmanned aerial vehicles (UAVs), we publicly release all our code, datasets, and trained networks.},
  doi      = {10.1109/jiot.2019.2917066},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1114352564},
}

@Article{Ma2019,
  author   = {Zhuo Ma and Yang Liu and Ximeng Liu and Jianfeng Ma and Kui Ren},
  journal  = {IEEE Internet of Things Journal},
  title    = {Lightweight Privacy-Preserving Ensemble Classification for Face Recognition},
  year     = {2019},
  note     = {https://ink.library.smu.edu.sg/sis_research/4405},
  number   = {3},
  pages    = {5778-5790},
  volume   = {6},
  abstract = {The development of machine learning technology and visual sensors is promoting the wider applications of face recognition into our daily life. However, if the face features in the servers are abused by the adversary, our privacy and wealth can be faced with great threat. Many security experts have pointed out that, by 3-D-printing technology, the adversary can utilize the leaked face feature data to masquerade others and break the E-bank accounts. Therefore, in this paper, we propose a lightweight privacy-preserving adaptive boosting (AdaBoost) classification framework for face recognition (POR) based on the additive secret sharing and edge computing. First, we improve the current additive secret sharing-based exponentiation and logarithm functions by expanding the effective input range. Then, by utilizing the protocols, two edge servers are deployed to cooperatively complete the ensemble classification of AdaBoost for face recognition. The application of edge computing ensures the efficiency and robustness of POR. Furthermore, we prove the correctness and security of our protocols by theoretic analysis. And experiment results show that, POR can reduce about 58% computation error compared with the existing differential privacy-based framework.},
  doi      = {10.1109/jiot.2019.2905555},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1112862699},
}

@Article{Ke2020,
  author   = {Ruimin Ke and Yifan Zhuang and Ziyuan Pu and Yinhai Wang},
  journal  = {IEEE Transactions on Intelligent Transportation Systems},
  title    = {A Smart, Efficient, and Reliable Parking Surveillance System With Edge Artificial Intelligence on IoT Devices},
  year     = {2020},
  note     = {https://arxiv.org/pdf/2001.00269},
  number   = {8},
  pages    = {4962-4974},
  volume   = {22},
  abstract = {Cloud computing has been a main-stream computing service for years. Recently, with the rapid development in urbanization, massive video surveillance data are produced at an unprecedented speed. A traditional solution to deal with the big data would require a large amount of computing and storage resources. With the advances in Internet of things (IoT), artificial intelligence, and communication technologies, edge computing offers a new solution to the problem by processing all or part of the data locally at the edge of a surveillance system. In this study, we investigate the feasibility of using edge computing for smart parking surveillance tasks, specifically, parking occupancy detection using the real-time video feed. The system processing pipeline is carefully designed with the consideration of flexibility, online surveillance, data transmission, detection accuracy, and system reliability. It enables artificial intelligence at the edge by implementing an enhanced single shot multibox detector (SSD). A few more algorithms are developed either locally at the edge of the system or on the centralized data server targeting optimal system efficiency and accuracy. Thorough field tests were conducted in the Angle Lake parking garage for three months. The experimental results are promising that the final detection method achieves over 95% accuracy in real-world scenarios with high efficiency and reliability. The proposed smart parking surveillance system is a critical component of smart cities and can be a solid foundation for future applications in intelligent transportation systems.},
  doi      = {10.1109/tits.2020.2984197},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1126481351},
}

@Article{Moon2019,
  author   = {Jaewon Moon and Seungwoo Kum and Sangwon Lee},
  journal  = {Sensors},
  title    = {A Heterogeneous IoT Data Analysis Framework with Collaboration of Edge-Cloud Computing: Focusing on Indoor PM10 and PM2.5 Status Prediction},
  year     = {2019},
  note     = {https://www.mdpi.com/1424-8220/19/14/3038/pdf?version=1562756373},
  number   = {14},
  pages    = {3038},
  volume   = {19},
  abstract = {The edge platform has evolved to become a part of a distributed computing environment. While typical edges do not have enough processing power to train machine learning models in real time, it is common to generate models in the cloud for use on the edge. The pattern of heterogeneous Internet of Things (IoT) data is dependent on individual circumstances. It is not easy to guarantee prediction performance when a monolithic model is used without considering the spatial characteristics of the space generating those data. In this paper, we propose a collaborative framework using a new method to select the best model for the edge from candidate models of cloud based on sample data correlation. This method lets the edge use the most suitable model without any training tasks on the edge side, and it also minimizes privacy issues. We apply the proposed method to predict future fine particulate matter concentration in an individual space. The results suggest that our method can provide better performance than the previous method.},
  doi      = {10.3390/s19143038},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1117920554},
}

@InProceedings{Wei2019,
  author    = {Junyong Wei and Suzhi Cao},
  booktitle = {2019 IEEE International Conference on Smart Internet of Things (SmartIoT)},
  title     = {Application of Edge Intelligent Computing in Satellite Internet of Things},
  year      = {2019},
  pages     = {85-91},
  abstract  = {With the advancement of aerospace technology and the investment of commercial satellite companies in the satellite industry, the number of satellites is increasing. Satellites become an important part of the IoT and 5G/6G communications. The sensors on the satellite will generate a large amount of data every day. However, due to the current on-board processing capability and the limitation of the inter-satellite communication rate, the data acquisition from the satellite has a higher delay and the data utilization rate is lower. In order to use the satellite Internet of Things intelligently, this paper proposes an application scheme of satellite IoT edge intelligent computing, and analyzes how edge computing and deep learning play a role in satellite IoT image data target detection. We simulated the proposed solution and experimented with the existing embedded processing board. Experiments show that the scheme can reduce the delay of acquiring images from satellites and performing target detection, and save backhaul bandwidth.},
  doi       = {10.1109/smartiot.2019.00022},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1122596363},
}

@InProceedings{Bura2018,
  author    = {Harshitha Bura and Nathan Lin and Naveen Kumar and Sangram Malekar and Sushma Nagaraj and Kaikai Liu},
  booktitle = {2018 IEEE International Conference on Cognitive Computing (ICCC)},
  title     = {An Edge Based Smart Parking Solution Using Camera Networks and Deep Learning},
  year      = {2018},
  pages     = {17-24},
  abstract  = {The smart parking industry continues to evolve as an increasing number of cities struggle with traffic congestion and inadequate parking availability. For urban dwellers, few things are more irritating than anxiously searching for a parking space. Research results show that as much as 30% of traffic is caused by drivers driving around looking for parking spaces in congested city areas. There has been considerable activity among researchers to develop smart technologies that can help drivers find a parking spot with greater ease, not only reducing traffic congestion but also the subsequent air pollution. Many existing solutions deploy sensors in every parking spot to address the automatic parking spot detection problems. However, the device and deployment costs are very high, especially for some large and old parking structures. A wide variety of other technological innovations are beginning to enable more adaptable systemsincluding license plate number detection, smart parking meter, and vision-based parking spot detection. In this paper, we propose to design a more adaptable and affordable smart parking system via distributed cameras, edge computing, data analytics, and advanced deep learning algorithms. Specifically, we deploy cameras with zoom-lens and motorized head to capture license plate numbers by tracking the vehicles when they enter or leave the parking lot; cameras with wide angle fish-eye lens will monitor the large parking lot via our custom designed deep neural network. We further optimize the algorithm and enable the real-time deep learning inference in an edge device. Through the intelligent algorithm, we can significantly reduce the cost of existing systems, while achieving a more adaptable solution. For example, our system can automatically detect when a car enters the parking space, the location of the parking spot, and precisely charge the parking fee and associate this with the license plate number.},
  doi       = {10.1109/iccc.2018.00010},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1107023934},
}

@InProceedings{Nishio2019,
  author    = {Takayuki Nishio and Ryo Yonetani},
  booktitle = {ICC 2019 - 2019 IEEE International Conference on Communications (ICC)},
  title     = {Client Selection for Federated Learning with Heterogeneous Resources in Mobile Edge},
  year      = {2019},
  note      = {https://arxiv.org/pdf/1804.08333},
  pages     = {1-7},
  abstract  = {We envision a mobile edge computing (MEC) framework for machine learning (ML) technologies, which leverages distributed client data and computation resources for training high-performance ML models while preserving client privacy. Toward this future goal, this work aims to extend Federated Learning (FL), a decentralized learning framework that enables privacy-preserving training of models, to work with heterogeneous clients in a practical cellular network. The FL protocol iteratively asks random clients to download a trainable model from a server, update it with own data, and upload the updated model to the server, while asking the server to aggregate multiple client updates to further improve the model. While clients in this protocol are free from disclosing own private data, the overall training process can become inefficient when some clients are with limited computational resources (i.e., requiring longer update time) or under poor wireless channel conditions (longer upload time). Our new FL protocol, which we refer to as FedCS, mitigates this problem and performs FL efficiently while actively managing clients based on their resource conditions. Specifically, FedCS solves a client selection problem with resource constraints, which allows the server to aggregate as many client updates as possible and to accelerate performance improvement in ML models. We conducted an experimental evaluation using publicly-available large-scale image datasets to train deep neural networks on MEC environment simulations. The experimental results show that FedCS is able to complete its training process in a significantly shorter time compared to the original FL protocol.},
  doi       = {10.1109/icc.2019.8761315},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1118023870},
}

@Article{Liu2019,
  author   = {Yi Liu and Chao Yang and Li Jiang and Shengli Xie and Yan Zhang},
  journal  = {IEEE Network},
  title    = {Intelligent Edge Computing for IoT-Based Energy Management in Smart Cities},
  year     = {2019},
  number   = {2},
  pages    = {111-117},
  volume   = {33},
  abstract = {In recent years, green energy management systems (smart grid, smart buildings, and so on) have received huge research and industrial attention with the explosive development of smart cities. By introducing Internet of Things (loT) technology, smart cities are able to achieve exquisite energy management by ubiquitous monitoring and reliable communications. However, long-term energy efficiency has become an important issue when using an loT-based network structure. In this article, we focus on designing an IoT-based energy management system based on edge computing infrastructure with deep reinforcement learning. First, an overview of IoT-based energy management in smart cities is described. Then the framework and software model of an IoT-based system with edge computing are proposed. After that, we present an efficient energy scheduling scheme with deep reinforcement learning for the proposed framework. Finally, we illustrate the effectiveness of the proposed scheme.},
  doi      = {10.1109/mnet.2019.1800254},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1113065842},
}

@Article{Liu2018,
  author   = {Chang Liu and Yu Cao and Yan Luo and Guanling Chen and Vinod Vokkarane and Ma Yunsheng and Songqing Chen and Peng Hou},
  journal  = {IEEE Transactions on Services Computing},
  title    = {A New Deep Learning-Based Food Recognition System for Dietary Assessment on An Edge Computing Service Infrastructure},
  year     = {2018},
  note     = {https://doi.org/10.1109/tsc.2017.2662008},
  number   = {2},
  pages    = {249-261},
  volume   = {11},
  abstract = {Literature has indicated that accurate dietary assessment is very important for assessing the effectiveness of weight loss interventions. However, most of the existing dietary assessment methods rely on memory. With the help of pervasive mobile devices and rich cloud services, it is now possible to develop new computer-aided food recognition system for accurate dietary assessment. However, enabling this future Internet of Things-based dietary assessment imposes several fundamental challenges on algorithm development and system design. In this paper, we set to address these issues from the following two aspects: (1) to develop novel deep learning-based visual food recognition algorithms to achieve the best-in-class recognition accuracy; (2) to design a food recognition system employing edge computing-based service computing paradigm to overcome some inherent problems of traditional mobile cloud computing paradigm, such as unacceptable system latency and low battery life of mobile devices. We have conducted extensive experiments with real-world data. Our results have shown that the proposed system achieved three objectives: (1) outperforming existing work in terms of food recognition accuracy; (2) reducing response time that is equivalent to the minimum of the existing approaches; and (3) lowering energy consumption which is close to the minimum of the state-of-the-art.},
  doi      = {10.1109/tsc.2017.2662008},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1083507587},
}

@InProceedings{Hassan2015,
  author    = {Mohammed A. Hassan and Mengbai Xiao and Qi Wei and Songqing Chen},
  booktitle = {2015 12th Annual IEEE International Conference on Sensing, Communication, and Networking - Workshops (SECON Workshops)},
  title     = {Help your Mobile Applications with Fog Computing},
  year      = {2015},
  pages     = {1-6},
  abstract  = {Cloud computing has paved a way for resource-constrained mobile devices to speed up their computing tasks and to expand their storage capacity. However, cloud computing is not necessary a panacea for all mobile applications. The high network latency to cloud data centers may not be ideal for delay-sensitive applications while storing everything on public clouds risks users' security and privacy. In this paper, we discuss two preliminary ideas, one for mobile application offloading and the other for mobile storage expansion, by leveraging the edge intelligence offered by fog computing to help mobile applications. Preliminary experiments conducted based on implemented prototypes show that fog computing can provide an effective and sometimes better alternative to help mobile applications.},
  doi       = {10.1109/seconw.2015.7328146},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1094646899},
}

@Article{Li2018,
  author   = {He Li and Kaoru Ota and Mianxiong Dong},
  journal  = {IEEE Network},
  title    = {Learning IoT in Edge: Deep Learning for the Internet of Things with Edge Computing},
  year     = {2018},
  note     = {https://muroran-it.repo.nii.ac.jp/record/9980/files/IEEENW_32_1_96_101.pdf},
  number   = {1},
  pages    = {96-101},
  volume   = {32},
  abstract = {Deep learning is a promising approach for extracting accurate information from raw sensor data from IoT devices deployed in complex environments. Because of its multilayer structure, deep learning is also appropriate for the edge computing environment. Therefore, in this article, we first introduce deep learning for IoTs into the edge computing environment. Since existing edge nodes have limited processing capability, we also design a novel offloading strategy to optimize the performance of IoT deep learning applications with edge computing. In the performance evaluation, we test the performance of executing multiple deep learning tasks in an edge computing environment with our strategy. The evaluation results show that our method outperforms other optimization solutions on deep learning for IoT.},
  doi      = {10.1109/mnet.2018.1700202},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1100673785},
}

@Article{Li2019,
  author   = {En Li and Liekang Zeng and Zhi Zhou and Xu Chen},
  journal  = {IEEE Transactions on Wireless Communications},
  title    = {Edge AI: On-Demand Accelerating Deep Neural Network Inference via Edge Computing},
  year     = {2019},
  note     = {https://arxiv.org/pdf/1910.05316},
  number   = {1},
  pages    = {447-457},
  volume   = {19},
  abstract = {As a key technology of enabling Artificial Intelligence (AI) applications in 5G era, Deep Neural Networks (DNNs) have quickly attracted widespread attention. However, it is challenging to run computation-intensive DNN-based tasks on mobile devices due to the limited computation resources. What’s worse, traditional cloud-assisted DNN inference is heavily hindered by the significant wide-area network latency, leading to poor real-time performance as well as low quality of user experience. To address these challenges, in this paper, we propose Edgent, a framework that leverages edge computing for DNN collaborative inference through device-edge synergy. Edgent exploits two design knobs: (1) DNN partitioning that adaptively partitions computation between device and edge for purpose of coordinating the powerful cloud resource and the proximal edge resource for real-time DNN inference; (2) DNN right-sizing that further reduces computing latency via early exiting inference at an appropriate intermediate DNN layer. In addition, considering the potential network fluctuation in real-world deployment, Edgent is properly design to specialize for both static and dynamic network environment. Specifically, in a static environment where the bandwidth changes slowly, Edgent derives the best configurations with the assist of regression-based prediction models, while in a dynamic environment where the bandwidth varies dramatically, Edgent generates the best execution plan through the online change point detection algorithm that maps the current bandwidth state to the optimal configuration. We implement Edgent prototype based on the Raspberry Pi and the desktop PC and the extensive experimental evaluations demonstrate Edgent’s effectiveness in enabling on-demand low-latency edge intelligence.},
  doi      = {10.1109/twc.2019.2946140},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1121932678},
}

@InProceedings{Ali2018,
  author    = {M. Ali and A. Anjum and M.U Yaseen and A.R. Zamani and D. Balouek-Thomert and O. Rana and M. Parashar},
  booktitle = {2018 IEEE 2nd International Conference on Fog and Edge Computing (ICFEC)},
  title     = {Edge Enhanced Deep Learning System for Large-scale Video Stream Analytics},
  year      = {2018},
  note      = {https://orca.cardiff.ac.uk/id/eprint/111447/1/Edge-Enhanced-Deep-Learning-ICFEC18.pdf},
  pages     = {1-10},
  abstract  = {Applying deep learning models to large-scale IoT data is a compute-intensive task and needs significant computational resources. Existing approaches transfer this big data from IoT devices to a central cloud where inference is performed using a machine learning model. However, the network connecting the data capture source and the cloud platform can become a bottleneck. We address this problem by distributing the deep learning pipeline across edge and cloudlet/fog resources. The basic processing stages and trained models are distributed towards the edge of the network and on in-transit and cloud resources. The proposed approach performs initial processing of the data close to the data source at edge and fog nodes, resulting in significant reduction in the data that is transferred and stored in the cloud. Results on an object recognition scenario show 71% efficiency gain in the throughput of the system by employing a combination of edge, in-transit and cloud resources when compared to a cloud-only approach.},
  doi       = {10.1109/cfec.2018.8358733},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1103977084},
}

@Article{Zhou2019,
  author   = {Junhao Zhou and Hong-Ning Dai and Hao Wang},
  journal  = {ACM Transactions on Intelligent Systems and Technology},
  title    = {Lightweight Convolution Neural Networks for Mobile Edge Computing in Transportation Cyber Physical Systems},
  year     = {2019},
  note     = {https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/11250/2649044/2/Lightweight%2bConvolution%2bNeural%2bNetworks%2bfor%2bMobile%2bEdge%2bComputing%2bin%2bTransportation%2bCyber%2bPhysical%2bSystems.pdf},
  number   = {6},
  pages    = {1-20},
  volume   = {10},
  abstract = {Cloud computing extends Transportation Cyber-Physical Systems (T-CPS) with provision of enhanced computing and storage capability via offloading computing tasks to remote cloud servers. However, cloud computing cannot fulfill the requirements such as low latency and context awareness in T-CPS. The appearance of Mobile Edge Computing (MEC) can overcome the limitations of cloud computing via offloading the computing tasks at edge servers in approximation to users, consequently reducing the latency and improving the context awareness. Although MEC has the potential in improving T-CPS, it is incapable of processing computational-intensive tasks such as deep learning algorithms due to the intrinsic storage and computing-capability constraints. Therefore, we design and develop a lightweight deep learning model to support MEC applications in T-CPS. In particular, we put forth a stacked convolutional neural network (CNN) consisting of factorization convolutional layers alternating with compression layers (namely, lightweight CNN-FC). Extensive experimental results show that our proposed lightweight CNN-FC can greatly decrease the number of unnecessary parameters, thereby reducing the model size while maintaining the high accuracy in contrast to conventional CNN models. In addition, we also evaluate the performance of our proposed model via conducting experiments at a realistic MEC platform. Specifically, experimental results at this MEC platform show that our model can maintain the high accuracy while preserving the portable model size.},
  doi      = {10.1145/3339308},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1122068746},
}

@InProceedings{Chen2019,
  author    = {Sifan Chen and Peng Gong and Bin Wang and Alagan Anpalagan and Mohsen Guizani and Chungang Yang},
  booktitle = {2019 IEEE 19th International Conference on Communication Technology (ICCT)},
  title     = {EDGE AI for Heterogeneous and Massive IoT Networks},
  year      = {2019},
  pages     = {350-355},
  abstract  = {By combining multiple sensing and wireless access technologies, the Internet of Things (IoT) shall exhibit features with large-scale, massive, and heterogeneous sensors and data. To integrate diverse radio access technologies, we present the architecture of heterogeneous IoT system for smart industrial parks and build an IoT experimental platform. Various sensors are installed on the IoT devices deployed on the experimental platform. To efficiently process the raw sensor data and realize edge artificial intelligence (AI), we describe four statistical features of the raw sensor data that can be effectively extracted and processed at the network edge in real time. The statistical features are calculated and fed into a back-propagation neural network (BPNN) for sensor data classification. By comparing to the k-nearest neighbor classification algorithm, we examine the BPNN-based classification method with a great amount of raw data gathered from various sensors. We evaluate the system performance according to the classification accuracy of BPNN and the performance indicators of the cloud server, which shows that the proposed approach can effectively enable the edge-AI-based heterogeneous IoT system to process the sensor data at the network edge in real time while reducing the demand for computing and network resources of the cloud.},
  doi       = {10.1109/icct46805.2019.8947193},
  groups    = {AI on Edge Devices},
  url       = {https://app.dimensions.ai/details/publication/pub.1123810093},
}

@Article{Wang2020,
  author   = {Xiaofei Wang and Yiwen Han and Victor C. M. Leung and Dusit Niyato and Xueqiang Yan and Xu Chen},
  journal  = {IEEE Communications Surveys & Tutorials},
  title    = {Convergence of Edge Computing and Deep Learning: A Comprehensive Survey},
  year     = {2020},
  note     = {https://arxiv.org/pdf/1907.08349},
  number   = {2},
  pages    = {869-904},
  volume   = {22},
  abstract = {Ubiquitous sensors and smart devices from factories and communities are generating massive amounts of data, and ever-increasing computing power is driving the core of computation and services from the cloud to the edge of the network. As an important enabler broadly changing people’s lives, from face recognition to ambitious smart factories and cities, developments of artificial intelligence (especially deep learning, DL) based applications and services are thriving. However, due to efficiency and latency issues, the current cloud computing service architecture hinders the vision of “providing artificial intelligence for every person and every organization at everywhere”. Thus, unleashing DL services using resources at the network edge near the data sources has emerged as a desirable solution. Therefore, edge intelligence, aiming to facilitate the deployment of DL services by edge computing, has received significant attention. In addition, DL, as the representative technique of artificial intelligence, can be integrated into edge computing frameworks to build intelligent edge for dynamic, adaptive edge maintenance and management. With regard to mutually beneficial edge intelligence and intelligent edge, this paper introduces and discusses: 1) the application scenarios of both; 2) the practical implementation methods and enabling technologies, namely DL training and inference in the customized edge computing framework; 3) challenges and future trends of more pervasive and fine-grained intelligence. We believe that by consolidating information scattered across the communication, networking, and DL areas, this survey can help readers to understand the connections between enabling technologies while promoting further discussions on the fusion of edge intelligence and intelligent edge, i.e., Edge DL.},
  doi      = {10.1109/comst.2020.2970550},
  groups   = {AI on Edge Devices},
  url      = {https://app.dimensions.ai/details/publication/pub.1124436340},
}

@InBook{CharoenUng2018,
  author    = {Phusanisa Charoen-Ung and Pradit Mittrapiyanuruk},
  pages     = {33-42},
  title     = {Sugarcane Yield Grade Prediction Using Random Forest with Forward Feature Selection and Hyper-parameter Tuning},
  year      = {2018},
  abstract  = {This paper presents a Random Forest (RF) based method for predicting the sugarcane yield grade of a farmer plot. The dataset used in this work is obtained from a set of sugarcane plots around a sugar mill in Thailand. The number of records in the train dataset and the test dataset are 8,765 records and 3,756 records, respectively.
We propose a forward feature selection in conjunction with hyper-parameter tuning for training the random forest classifier. The accuracy of our method is 71.88%. We compare the accuracy of our method with two non-machine-learning baselines. The first baseline is to use the actual yield of the last year as the prediction. The second baseline is that the target yield of each plot is manually predicted by human expert. The accuracies of these baselines are 51.52% and 65.50%, respectively. The results on accuracy indicate that our proposed method can be used for aiding the decision making of sugar mill operation planning.},
  booktitle = {Recent Advances in Information and Communication Technology 2018},
  doi       = {10.1007/978-3-319-93692-5_4},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1105144504},
}

@Article{Ranjan2019,
  author   = {Avinash Kumar Ranjan and Bikash Ranjan Parida},
  journal  = {Spatial Information Research},
  title    = {Paddy acreage mapping and yield prediction using sentinel-based optical and SAR data in Sahibganj district, Jharkhand (India)},
  year     = {2019},
  number   = {4},
  pages    = {399-410},
  volume   = {27},
  abstract = {Rice is an important staple food for the billions of world population. Mapping the spatial distribution of paddy and predicting yields are crucial for food security measures. Over the last three decades, remote sensing techniques have been widely used for monitoring and management of agricultural systems. This study has employed Sentinel-based both optical (Sentinel-2B) and SAR (Sentinel-1A) sensors data for paddy acreage mapping in Sahibganj district, Jharkhand during the monsoon season in 2017. A robust machine learning Random Forest (RF) classification technique was deployed for the paddy acreage mapping. A simple linear regression yield model was developed for predicting yields. The key findings showed that the paddy acreage was about 68.3–77.8 thousand hectares based on Sentinel-1A and 2B satellite data, respectively. Accordingly, the paddy production of the district was estimated as 108–126 thousand tonnes. The paddy yield was predicted as 1.60 tonnes/hectare. The spatial distribution of paddy based on RF classifier and accuracy assessment of LULC maps revealed that the SAR-based classified paddy map was more consistent than the optical data. Nevertheless, this comprehensive study concluded that the SAR data could be more pronounced in acreage mapping and yield estimation for providing timely information to decision makers.},
  doi      = {10.1007/s41324-019-00246-4},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1111503060},
}

@Article{Filippi2019,
  author   = {Patrick Filippi and Edward J. Jones and Niranjan S. Wimalathunge and Pallegedara D. S. N. Somarathna and Liana E. Pozza and Sabastine U. Ugbaje and Thomas G. Jephcott and Stacey E. Paterson and Brett M. Whelan and Thomas F. A. Bishop},
  journal  = {Precision Agriculture},
  title    = {An approach to forecast grain crop yield using multi-layered, multi-farm data sets and machine learning},
  year     = {2019},
  number   = {5},
  pages    = {1015-1029},
  volume   = {20},
  abstract = {Many broadacre farmers have a time series of crop yield monitor data for their fields which are often augmented with additional data, such as soil apparent electrical conductivity surveys and soil test results. In addition there are now readily available national and global datasets, such as rainfall and MODIS, which can be used to represent the crop-growing environment. Rather than analysing one field at a time as is typical in precision agriculture research, there is an opportunity to explore the value of combining data over multiple fields/farms and years into one dataset. Using these datasets in conjunction with machine learning approaches allows predictive models of crop yield to be built. In this study, several large farms in Western Australia were used as a case study, and yield monitor data from wheat, barley and canola crops from three different seasons (2013, 2014 and 2015) that covered ~ 11 000 to ~ 17 000 hectares in each year were used. The yield data were processed to a 10 m grid, and for each observation point associated predictor variables in space and time were collated. The data were then aggregated to a 100 m spatial resolution for modelling yield. Random forest models were used to predict crop yield of wheat, barley and canola using this dataset. Three separate models were created based on pre-sowing, mid-season and late-season conditions to explore the changes in the predictive ability of the model as more within-season information became available. These time points also coincide with points in the season when a management decision is made, such as the application of fertiliser. The models were evaluated with cross-validation using both fields and years for data splitting, and this was assessed at the field spatial resolution. Cross-validated results showed the models predicted yield relatively accurately, with a root mean square error of 0.36 to 0.42 t ha−1, and a Lin’s concordance correlation coefficient of 0.89 to 0.92 at the field resolution. The models performed better as the season progressed, largely because more information about within-season data became available (e.g. rainfall). The more years of yield data that were available for a field, the better the predictions were, and future work should use a longer time-series of yield data. The generic nature of this method makes it possible to apply to other agricultural systems where yield monitor data is available. Future work should also explore the integration of more data sources into the models, focus on predicting at finer spatial resolutions within fields, and the possibility of using the yield forecasts to guide management decisions.},
  doi      = {10.1007/s11119-018-09628-4},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1111269831},
}

@Article{Xu2019a,
  author   = {Xiangying Xu and Ping Gao and Xinkai Zhu and Wenshan Guo and Jinfeng Ding and Chunyan Li and Min Zhu and Xuanwei Wu},
  journal  = {Ecological Indicators},
  title    = {Design of an integrated climatic assessment indicator (ICAI) for wheat production: A case study in Jiangsu Province, China},
  year     = {2019},
  pages    = {943-953},
  volume   = {101},
  abstract = {Agro-meteorological condition plays a fundamental role in crop production. For a specific region, the comprehensive effects of multiple meteorological factors are important indicators for the climatic suitability of the crops. To evaluate the synthetic effects, an integrated climatic assessment indicator (ICAI) are developed in Jiangsu Province, China. A newly produced meteorological assimilation driving datasets (CMADS V1.0) combined with observation data are used in establishing the indicator. The procedure to construct the indicator involves building statistical crop models by meteorological factors and determining the indicator values by classification. In modeling, two machine learning algorithms: Random Forest (RF) and Support Vector Machine (SVM) are compared and the classification model of RF is chosen to build ICAI due to its better performance in the independent test set. To determine a reasonable division in classification, distribution detection of climatic yield is carried out and Monte Carlo simulations are applied for the Kolmogorov–Smirnov (KS) test. The generated indicator includes three values: yield loss, normal and yield increment, with the spatial and temporal prediction accuracy from 67.86% to 100% in the test set for the Northern, Central and Southern Jiangsu. The ICAI are used to estimate the past climatic suitability of winter wheat and the future suitability under global warming conditions in Jiangsu Province. The results show that the climate in 1990s has more adverse effects on wheat production than the other two sub-periods in Northern and Southern Jiangsu. The adaptability of wheat production in Southern Jiangsu has improved greatly to the local environments during the past three decades. In addition, when annual temperature accelerates upwards, both possibilities of yield loss in Northern Jiangsu and yield increment in Southern Jiangsu will increase. Therefore, more concerns should be given to the North in future warming climate, while yield potential in the South may be further improved in this circumstance.},
  doi      = {10.1016/j.ecolind.2019.01.059},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1112087374},
}

@InProceedings{Wang2018,
  author    = {Anna X. Wang and Caelin Tran and Nikhil Desai and David Lobell and Stefano Ermon},
  booktitle = {Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies},
  title     = {Deep Transfer Learning for Crop Yield Prediction with Remote Sensing Data},
  year      = {2018},
  pages     = {1-5},
  abstract  = {Accurate prediction of crop yields in developing countries in advance of harvest time is central to preventing famine, improving food security, and sustainable development of agriculture. Existing techniques are expensive and difficult to scale as they require locally collected survey data. Approaches utilizing remotely sensed data, such as satellite imagery, potentially provide a cheap, equally effective alternative. Our work shows promising results in predicting soybean crop yields in Argentina using deep learning techniques. We also achieve satisfactory results with a transfer learning approach to predict Brazil soybean harvests with a smaller amount of data. The motivation for transfer learning is that the success of deep learning models is largely dependent on abundant ground truth training data. Successful crop yield prediction with deep learning in regions with little training data relies on the ability to fine-tune pre-trained models.},
  doi       = {10.1145/3209811.3212707},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1105021375},
}

@InBook{Monga2018,
  author    = {Tanya Monga},
  pages     = {339-343},
  title     = {Estimating Vineyard Grape Yield from Images},
  year      = {2018},
  abstract  = {Agricultural yield estimation from natural images is a challenging problem to which machine learning can be applied. Convolutional Neural Networks have advanced the state of the art in many machine learning applications such as computer vision, speech recognition and natural language processing. The proposed research uses convolution neural networks to develop models that can estimate the weight of grapes on a vine using an image. Trained and tested with a dataset of 60 images of grape vines, the system manages to achieve a cross-validation yield estimation accuracy of 87%.},
  booktitle = {Advances in Artificial Intelligence},
  doi       = {10.1007/978-3-319-89656-4_37},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1103323054},
}

@InBook{Shah2018,
  author    = {Ayush Shah and Akash Dubey and Vishesh Hemnani and Divye Gala and D. R. Kalbande},
  pages     = {49-56},
  title     = {Smart Farming System: Crop Yield Prediction Using Regression Techniques},
  year      = {2018},
  abstract  = {Due to ever increasing global population, there is an ever increase in demand for food; hence, new methods need to be devised to increase the crop yield. This paper proposes an intelligent way to predict crop yield and suggest the optimal climatic factors to maximize crop yield. With the advancement in technology, the focus has now shifted to using machines and control systems to automate the processes and optimize productivity. The paper uses multivariate polynomial regression, support vector machine regression and random forest models to predict the crop yield per acre. The proposed method uses yield and weather data collected from United States Department of Agriculture. The various parameters included in the dataset are humidity, yield, temperature and rainfall. This prediction will help the farmers choose the most suitable temperature and moisture content at which the crop yield will be optimal. The paper uses RMSE, MAE, median absolute error, and R-square values to compare between multivariate polynomial regression, support vector machine regression and random forest.},
  booktitle = {Proceedings of International Conference on Wireless Communication},
  doi       = {10.1007/978-981-10-8339-6_6},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1103481417},
}

@Article{Ahmad2018,
  author   = {Ishfaq Ahmad and Umer Saeed and Muhammad Fahad and Asmat Ullah and M. Habib ur Rahman and Ashfaq Ahmad and Jasmeet Judge},
  journal  = {Journal of the Indian Society of Remote Sensing},
  title    = {Yield Forecasting of Spring Maize Using Remote Sensing and Crop Modeling in Faisalabad-Punjab Pakistan},
  year     = {2018},
  number   = {10},
  pages    = {1701-1711},
  volume   = {46},
  abstract = {Real time, accurate and reliable estimation of maize yield is valuable to policy makers in decision making. The current study was planned for yield estimation of spring maize using remote sensing and crop modeling. In crop modeling, the CERES-Maize model was calibrated and evaluated with the field experiment data and after calibration and evaluation, this model was used to forecast maize yield. A Field survey of 64 farm was also conducted in Faisalabad to collect data on initial field conditions and crop management data. These data were used to forecast maize yield using crop model at farmers’ field. While in remote sensing, peak season Landsat 8 images were classified for landcover classification using machine learning algorithm. After classification, time series normalized difference vegetation index (NDVI) and land surface temperature (LST) of the surveyed 64 farms were calculated. Principle component analysis were run to correlate the indicators with maize yield. The selected LSTs and NDVIs were used to develop yield forecasting equations using least absolute shrinkage and selection operator (LASSO) regression. Calibrated and evaluated results of CERES-Maize showed the mean absolute % error (MAPE) of 0.35–6.71% for all recorded variables. In remote sensing all machine learning algorithms showed the accuracy greater the 90%, however support vector machine (SVM-radial basis) showed the higher accuracy of 97%, that was used for classification of maize area. The accuracy of area estimated through SVM-radial basis was 91%, when validated with crop reporting service. Yield forecasting results of crop model were precise with RMSE of 255 kg ha−1, while remote sensing showed the RMSE of 397 kg ha−1. Overall strength of relationship between estimated and actual grain yields were good with R2 of 0.94 in both techniques. For regional yield forecasting remote sensing could be used due greater advantages of less input dataset and if focus is to assess specific stress, and interaction of plant genetics to soil and environmental conditions than crop model is very useful tool.},
  doi      = {10.1007/s12524-018-0825-8},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1106034026},
}

@Article{Ghazvinei2018,
  author   = {Pezhman Taherei Ghazvinei and Hossein Hassanpour Darvishi and Amir Mosavi and Khamaruzaman bin Wan Yusof and Meysam Alizamir and Shahaboddin Shamshirband and Kwok-wing Chau},
  journal  = {Engineering Applications of Computational Fluid Mechanics},
  title    = {Sugarcane growth prediction based on meteorological parameters using extreme learning machine and artificial neural network},
  year     = {2018},
  note     = {https://www.tandfonline.com/doi/pdf/10.1080/19942060.2018.1526119?needAccess=true},
  number   = {1},
  pages    = {738-749},
  volume   = {12},
  abstract = {Management strategies for sustainable sugarcane production need to deal with the increasing complexity and variability of the whole sugar system. Moreover, they need to accommodate the multiple goals of different industry sectors and the wider community. Traditional disciplinary approaches are unable to provide integrated management solutions, and an approach based on whole systems analysis is essential to bring about beneficial change to industry and the community. The application of this approach to water management, environmental management and cane supply management is outlined, where the literature indicates that the application of extreme learning machine (ELM) has never been explored in this realm. Consequently, the leading objective of the current research was set to filling this gap by applying ELM to launch swift and accurate model for crop production data-driven. The key learning has been the need for innovation both in the technical aspects of system function underpinned by modelling of sugarcane growth. Therefore, the current study is an attempt to establish an integrate model using ELM to predict the concluding growth amount of sugarcane. Prediction results were evaluated and further compared with artificial neural network (ANN) and genetic programming models. Accuracy of the ELM model is calculated using the statistics indicators of Root Means Square Error (RMSE), Pearson Coefficient (r), and Coefficient of Determination (R2) with promising results of 0.8, 0.47, and 0.89, respectively. The results also show better generalization ability in addition to faster learning curve. Thus, proficiency of the ELM for supplementary work on advancement of prediction model for sugarcane growth was approved with promising results.},
  doi      = {10.1080/19942060.2018.1526119},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1107298395},
}

@Article{Khanal2018,
  author   = {Sami Khanal and John Fulton and Andrew Klopfenstein and Nathan Douridas and Scott Shearer},
  journal  = {Computers and Electronics in Agriculture},
  title    = {Integration of high resolution remotely sensed data and machine learning techniques for spatial prediction of soil properties and corn yield},
  year     = {2018},
  pages    = {213-225},
  volume   = {153},
  abstract = {Widespread adoption of precision agriculture requires timely acquisition of low-cost, high quality soil and crop yield maps. Integration of remotely sensed data and machine learning algorithms offers cost-and time-effective approach for spatial prediction of soil properties and crop yield compared to conventional approaches. The objectives of this study were to: (i) evaluate the role of remotely sensed images; (ii) compare the performance of various machine learning algorithms; and (iii) identify the importance of remotely sensed image-derived variables, in spatial prediction of soil properties and corn yield. This study integrated field based data on five soil properties (i.e., soil organic matter (SOM), cation exchange capacity (CEC), magnesium (Mg), potassium (K), and pH) and yield monitor based corn yield data with multispectral aerial images and topographic data, both collected in 2013, from seven fields at the Molly Caren Farm near London, Ohio. Digital elevation model data, at a resolution of 1 m, was used to derive topographic properties of the fields. Multispectral images collected at bare-soil conditions, at a resolution 0.30 m, were used to derive soil and vegetation indices. Models developed for prediction of soil properties and corn yield using linear regression (LM) and five machine learning algorithms (i.e., Random Forest (RF); Neural Network (NN); Support Vector Machine (SVM) with radial and linear kernel functions; Gradient Boosting Model (GBM); and Cubist (CU)) were evaluated in terms of coefficient of determination (R2) and root mean square error (RMSE). Machine learning algorithms were found to outperform LM algorithm for most of the times with a higher R2 and lower RMSE. Based on models for seven fields, on average, NN provided the highest accuracy for SOM (R2 = 0.64, RMSE = 0.44) and CEC (R2 = 0.67, RMSE = 2.35); SVM for K (R2 = 0.21, RMSE = 0.49) and Mg (R2 = 0.22, RMSE = 4.57); and GBM for pH (R2 = 0.15, RMSE = 0.62). For corn yield, RF consistently outperformed other models and provided higher accuracy (R2 = 0.53, RMSE = 0.97). Soil and vegetation indices based on bare-soil imagery played a more significant role in demonstrating in-field variability of corn yield and soil properties than topographic variables. The accuracy of the models developed for prediction of soil properties and corn yield observed in this study suggested that the approach of integrating remotely sensed data and machine learning algorithms are promising for mapping soil properties and corn yield at a local scale, which can be useful in locating areas of potential concerns and implementing site-specific farming practices.},
  doi      = {10.1016/j.compag.2018.07.016},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1106284861},
}

@Article{Marizel2018,
  author  = {Marizel B. and Ma. Louella},
  journal = {International Journal of Advanced Computer Science and Applications},
  title   = {Bitter Melon Crop Yield Prediction using Machine Learning Algorithm},
  year    = {2018},
  note    = {http://thesai.org/Downloads/Volume9No3/Paper_1-Bitter_Melon_Crop_Yield_Prediction.pdf},
  number  = {3},
  volume  = {9},
  doi     = {10.14569/ijacsa.2018.090301},
  groups  = {Crop Yield Prediction},
  url     = {https://app.dimensions.ai/details/publication/pub.1103286049},
}

@Article{CraneDroesch2018,
  author   = {Andrew Crane-Droesch},
  journal  = {Environmental Research Letters},
  title    = {Machine learning methods for crop yield prediction and climate change impact assessment in agriculture},
  year     = {2018},
  note     = {https://iopscience.iop.org/article/10.1088/1748-9326/aae159/pdf},
  number   = {11},
  pages    = {114003},
  volume   = {13},
  abstract = {Crop yields are critically dependent on weather. A growing empirical literature models this relationship in order to project climate change impacts on the sector. We describe an approach to yield modeling that uses a semiparametric variant of a deep neural network, which can simultaneously account for complex nonlinear relationships in high-dimensional datasets, as well as known parametric structure and unobserved cross-sectional heterogeneity. Using data on corn yield from the US Midwest, we show that this approach outperforms both classical statistical methods and fully-nonparametric neural networks in predicting yields of years withheld during model training. Using scenarios from a suite of climate models, we show large negative impacts of climate change on corn yield, but less severe than impacts projected using classical statistical methods. In particular, our approach is less pessimistic in the warmest regions and the warmest scenarios.},
  doi      = {10.1088/1748-9326/aae159},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1106988165},
}

@Article{Zhong2018,
  author   = {Huaiyang Zhong and Xiaocheng Li and David Lobell and Stefano Ermon and Margaret L. Brandeau},
  journal  = {Environment Systems and Decisions},
  title    = {Hierarchical modeling of seed variety yields and decision making for future planting plans},
  year     = {2018},
  number   = {4},
  pages    = {458-470},
  volume   = {38},
  abstract = {Eradicating hunger and malnutrition is a key development goal of the twenty first century. This paper addresses the problem of optimally identifying seed varieties to reliably increase crop yield within a risk-sensitive decision making framework. Specifically, a novel hierarchical machine learning mechanism for predicting crop yield (the yield of different seed varieties of the same crop) is introduced. This prediction mechanism is then integrated with a weather forecasting model and three different approaches for decision making under uncertainty to select seed varieties for planting so as to balance yield maximization and risk. The model was applied to the problem of soybean variety selection given in the 2016 Syngenta Crop Challenge. The prediction model achieved a median absolute error of 235 kg/ha and thus provides good estimates for input into the decision models. The decision models identified the selection of soybean varieties that appropriately balance yield and risk as a function of the farmer’s risk aversion level. More generally, the models can support farmers in decision making about which seed varieties to plant.},
  doi      = {10.1007/s10669-018-9695-4},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1105035509},
}

@Article{Goldstein2017,
  author   = {Anat Goldstein and Lior Fink and Amit Meitin and Shiran Bohadana and Oscar Lutenberg and Gilad Ravid},
  journal  = {Precision Agriculture},
  title    = {Applying machine learning on sensor data for irrigation recommendations: revealing the agronomist’s tacit knowledge},
  year     = {2017},
  number   = {3},
  pages    = {421-444},
  volume   = {19},
  abstract = {Jojoba Israel is a world-leading producer of Jojoba products, whose orchards are covered with sensors that collect soil moisture data for monitoring plant needs at real-time. Based on these data, the company’s agronomist defines a weekly irrigation plan. In addition, data on weather, irrigation, and yield are recorded from other sources (e.g. meteorological station and irrigation-plan records). However, so far, there has been no attempt to use the entire set of collected data to reveal insights and interesting relationships between different variables, such as soil, weather, irrigation characteristics, and resulting yield. By integrating and utilizing data from different sources, our research aims at using the collected data not only for monitoring and controlling the crop, but also for predicting irrigation recommendations. In particular, a dataset was constructed by integrating data collected over almost two years from 22 soil-sensors spread in four major plots (which are divided into 28 subplots and eight irrigation groups), from a meteorological station, and from actual irrigation records. Different regression and classification algorithms were applied on this dataset to develop models that were able to predict the weekly irrigation plan as recommended by the agronomist. The models were developed using eight different subsets of variables to determine which variables consistently contributed to prediction accuracy. By comparing the resulting models, it was shown that the best regression model was Gradient Boosted Regression Trees, with 93% accuracy, and the best classification model was the Boosted Tree Classifier, with 95% accuracy (on the test-set). Data that were not contributing to the model prediction success rate were identified as well. The resulting model can significantly facilitate the agronomist’s irrigation planning process. In addition, the potential of applying machine learning on the company data for yield and disease prediction is discussed.},
  doi      = {10.1007/s11119-017-9527-4},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1085732641},
}

@Article{Kouadio2018,
  author   = {Louis Kouadio and Ravinesh C. Deo and Vivekananda Byrareddy and Jan F. Adamowski and Shahbaz Mushtaq and Van Phuong Nguyen},
  journal  = {Computers and Electronics in Agriculture},
  title    = {Artificial intelligence approach for the prediction of Robusta coffee yield using soil fertility properties},
  year     = {2018},
  pages    = {324-338},
  volume   = {155},
  abstract = {As a commodity for daily consumption, coffee plays a crucial role in the economy of several African, American and Asian countries; yet, the accurate prediction of coffee yield based on environmental, climatic and soil fertility conditions remains a challenge for agricultural system modellers. The ability of an Extreme Learning Machine (ELM) model to analyse soil fertility properties and to generate an accurate estimation of Robusta coffee yield was assessed in this study. The performance of 18 different ELM-based models with single and multiple combinations of the predictor variables based on the soil organic matter (SOM), available potassium, boron, sulphur, zinc, phosphorus, nitrogen, exchangeable calcium, magnesium, and pH, was evaluated. The ELM model’s performance was compared to that of existing predictive tools: Multiple Linear Regression (MLR) and Random Forest (RF). Individual model performance and inter-model performance comparisons were based on the root mean square error (RMSE), mean absolute error (MAE), Willmott’s Index (WI), Nash-Sutcliffe efficiency coefficient (ENS), and the Legates and McCabe’s Index (ELM) in the independent testing dataset. In the independent testing phase, an ELM model constructed with SOM, available potassium and available sulphur as predictor variables generated the most accurate coffee yield estimate (i.e., RMSE = 496.35 kg ha−1 or ±13.6%, and MAE = 326.40 kg ha−1 or ±7.9%). This contrasted with the less accurate MLR (RMSE = 1072.09 kg ha−1 and MAE = 797.60 kg ha−1) and RF (RMSE = 1087.35 kg ha−1 and MAE = 769.57 kg ha−1) model. Normalized metrics showed the ELM model’s ability to yield highly accurate results: WI = 0.9952, E NS = 0.406 and E LM = 0.431. In comparison to the MLR and RF models, the adoption of the ELM model as an improved class of artificial intelligence models for coffee yield prediction in smallholder farms in this study constitutes an original contribution to the agronomic sector, particularly with respect to the appropriate selection of most optimal soil properties that can be used in the prediction of optimal coffee yield. The potential utility of coupling artificial intelligence algorithms with biophysical-crop models (i.e., as a data-intelligent automation tool) in decision-support systems that implement precision agriculture, in an effort to improve yield in smallholder farms based on carefully screened soil fertility dataset was confirmed.},
  doi      = {10.1016/j.compag.2018.10.014},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1107925034},
}

@Article{Ali2016,
  author   = {Iftikhar Ali and Fiona Cawkwell and Edward Dwyer and Stuart Green},
  journal  = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  title    = {Modeling Managed Grassland Biomass Estimation by Using Multitemporal Remote Sensing DataA Machine Learning Approach},
  year     = {2016},
  note     = {https://cora.ucc.ie/bitstreams/8db458dd-b340-4f49-a1bb-f48c674ac3b0/download},
  number   = {7},
  pages    = {3254-3264},
  volume   = {10},
  abstract = {More than 80 of agricultural land in Ireland is grassland, which is a major feed source for the pasture based dairy farming and livestock industry. Many studies have been undertaken globally to estimate grassland biomass by using satellite remote sensing data, but rarely in systems like Ireland's intensively managed, but small-scale pastures, where grass is grazed as well as harvested for winter fodder. Multiple linear regression (MLR), artificial neural network (ANN) and adaptive neuro-fuzzy inference system (ANFIS) models were developed to estimate the grassland biomass (kg dry matter/ha/day) of two intensively managed grassland farms in Ireland. For the first test site (Moorepark) 12 years (20012012) and for second test site (Grange) 6 years (20012005, 2007) of in situ measurements (weekly measured biomass) were used for model development. Five vegetation indices plus two raw spectral bands (RED=red band, NIR=Near Infrared band) derived from an 8-day MODIS product (MOD09Q1) were used as an input for all three models. Model evaluation shows that the ANFIS ( $R_{{\rm{Moorepark}}}^2 = \;0.85,\;\;{\rm{RMS}}{{\rm{E}}_{{\rm{Moorepark}}}} = \;11.07$; $R_{{\rm{Grange}}}^2 = \;0.76,\;\;{\rm{RMS}}{{\rm{E}}_{{\rm{Grange}}}} = \;15.35$) has produced improved estimation of biomass as compared to the ANN and MLR. The proposed methodology will help to better explore the future inflow of remote sensing data from spaceborne sensors for the retrieval of different biophysical parameters, and with the launch of new members of satellite families (ALOS-2, Radarsat-2, Sentinel, TerraSAR-X, TanDEM-X/L) the development of tools to process large volumes of image data will become increasingly important.},
  doi      = {10.1109/jstars.2016.2561618},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061334272},
}

@InBook{Osman2016,
  author    = {Tousif Osman and Shahreen Shahjahan Psyche and MD Rafik Kamal and Fouzia Tamanna and Farzana Haque and Rashedur M. Rahman},
  pages     = {470-479},
  title     = {Predicting Early Crop Production by Analysing Prior Environment Factors},
  year      = {2016},
  abstract  = {Bangladesh has an agriculture dependent economy and hence prediction of agricultural production is of great importance to us. In this research we develop a model that considers and analyzes weather and climate prior to specific crop plantation and maps a correlation between these two. It allows us to provide information about the crop state, in quantity and quality with the possibility of early warnings so that timely interventions can be undertaken. The approach advocated in this paper is to help the people with food security and early warning system.},
  booktitle = {Advances in Information and Communication Technology},
  doi       = {10.1007/978-3-319-49073-1_51},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1084919232},
}

@Article{Fernandes2017,
  author   = {Jeferson Lobato Fernandes and Nelson Francisco Favilla Ebecken and Júlio César Dalla Mora Esquerdo},
  journal  = {International Journal of Remote Sensing},
  title    = {Sugarcane yield prediction in Brazil using NDVI time series and neural networks ensemble},
  year     = {2017},
  number   = {16},
  pages    = {4631-4644},
  volume   = {38},
  abstract = {The objective of this study is to predict the sugarcane yield in São Paulo State, Brazil, using metrics derived from normalized difference vegetation index (NDVI) time series from the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor and an ensemble model of artificial neural networks (ANNs). Sixty municipalities were selected and spectral metrics were extracted from the NDVI time series for each municipality from 2003 to 2012. A neural network wrapper with sequential backward elimination was applied to remove irrelevant and/or redundant features from the initial data set, reducing over-fitting and improving the prediction performance. Afterwards the sugarcane yield was predicted using a stacking ensemble model with ANN. At the predicted yield, the relative root mean square error (RRMSE) was 6.8% and the coefficient of determination (R2) was 0.61. The last three months were removed from the initial time-series data set to forecast the final sugarcane yield, and the process was repeated. The feature selection (FS) improved again the prediction performance and Stacking improved the FS results: RRMSE increased to 8% and R2 to 0.43. The yield was also estimated for the entire State, based on the average of the 60 selected municipalities, which were compared to the official data surveys. The Stacking method was able to estimate the sugarcane yield for São Paulo State with a smaller RMSE than the official data surveys, anticipating the crop forecast by three months before the harvest.},
  doi      = {10.1080/01431161.2017.1325531},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1085640773},
}

@Article{Bargoti2017,
  author   = {Suchet Bargoti and James P. Underwood},
  journal  = {Journal of Field Robotics},
  title    = {Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards},
  year     = {2017},
  note     = {https://arxiv.org/pdf/1610.08120},
  number   = {6},
  pages    = {1039-1060},
  volume   = {34},
  abstract = {Ground vehicles equipped with monocular vision systems are a valuable source of high‐resolution image data for precision agriculture applications in orchards. This paper presents an image processing framework for fruit detection and counting using orchard image data. A general‐purpose image segmentation approach is used, including two feature learning algorithms; multiscale multilayered perceptrons (MLP) and convolutional neural networks (CNN). These networks were extended by including contextual information about how the image data was captured (metadata), which correlates with some of the appearance variations and/or class distributions observed in the data. The pixel‐wise fruit segmentation output is processed using the watershed segmentation (WS) and circular Hough transform (CHT) algorithms to detect and count individual fruits. Experiments were conducted in a commercial apple orchard near Melbourne, Australia. The results show an improvement in fruit segmentation performance with the inclusion of metadata on the previously benchmarked MLP network. We extend this work with CNNs, bringing agrovision closer to the state‐of‐the‐art in computer vision, where although metadata had negligible influence, the best pixel‐wise F1‐score of 0.791 was achieved. The WS algorithm produced the best apple detection and counting results, with a detection F1‐score of 0.861. As a final step, image fruit counts were accumulated over multiple rows at the orchard and compared against the post‐harvest fruit counts that were obtained from a grading and counting machine. The count estimates using CNN and WS resulted in the best performance for this data set, with a squared correlation coefficient of  .},
  doi      = {10.1002/rob.21699},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1083753722},
}

@Article{Cheng2017,
  author   = {Hong Cheng and Lutz Damerow and Yurui Sun and Michael Blanke},
  journal  = {Journal of Imaging},
  title    = {Early Yield Prediction Using Image Analysis of Apple Fruit and Tree Canopy Features with Neural Networks},
  year     = {2017},
  note     = {https://www.mdpi.com/2313-433X/3/1/6/pdf?version=1484812586},
  number   = {1},
  pages    = {6},
  volume   = {3},
  abstract = {(1) Background: Since early yield prediction is relevant for resource requirements of harvesting and marketing in the whole fruit industry, this paper presents a new approach of using image analysis and tree canopy features to predict early yield with artificial neural networks (ANN); (2) Methods: Two back propagation neural network (BPNN) models were developed for the early period after natural fruit drop in June and the ripening period, respectively. Within the same periods, images of apple cv. “Gala” trees were captured from an orchard near Bonn, Germany. Two sample sets were developed to train and test models; each set included 150 samples from the 2009 and 2010 growing season. For each sample (each canopy image), pixels were segmented into fruit, foliage, and background using image segmentation. The four features extracted from the data set for the canopy were: total cross-sectional area of fruits, fruit number, total cross-section area of small fruits, and cross-sectional area of foliage, and were used as inputs. With the actual weighted yield per tree as a target, BPNN was employed to learn their mutual relationship as a prerequisite to develop the prediction; (3) Results: For the developed BPNN model of the early period after June drop, correlation coefficients (R2) between the estimated and the actual weighted yield, mean forecast error (MFE), mean absolute percentage error (MAPE), and root mean square error (RMSE) were 0.81, −0.05, 10.7%, 2.34 kg/tree, respectively. For the model of the ripening period, these measures were 0.83, −0.03, 8.9%, 2.3 kg/tree, respectively. In 2011, the two previously developed models were used to predict apple yield. The RMSE and R2 values between the estimated and harvested apple yield were 2.6 kg/tree and 0.62 for the early period (small, green fruit) and improved near harvest (red, large fruit) to 2.5 kg/tree and 0.75 for a tree with ca. 18 kg yield per tree. For further method verification, the cv. “Pinova” apple trees were used as another variety in 2012 to develop the BPNN prediction model for the early period after June drop. The model was used in 2013, which gave similar results as those found with cv. “Gala”; (4) Conclusion: Overall, the results showed in this research that the proposed estimation models performed accurately using canopy and fruit features using image analysis algorithms.},
  doi      = {10.3390/jimaging3010006},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1013313926},
}

@Article{Su2017,
  author   = {Ying-xue Su and Huan Xu and Li-jiao Yan},
  journal  = {Saudi Journal of Biological Sciences},
  title    = {Support vector machine-based open crop model (SBOCM): Case of rice production in China},
  year     = {2017},
  note     = {https://doi.org/10.1016/j.sjbs.2017.01.024},
  number   = {3},
  pages    = {537-547},
  volume   = {24},
  abstract = {Existing crop models produce unsatisfactory simulation results and are operationally complicated. The present study, however, demonstrated the unique advantages of statistical crop models for large-scale simulation. Using rice as the research crop, a support vector machine-based open crop model (SBOCM) was developed by integrating developmental stage and yield prediction models. Basic geographical information obtained by surface weather observation stations in China and the 1:1000000 soil database published by the Chinese Academy of Sciences were used. Based on the principle of scale compatibility of modeling data, an open reading frame was designed for the dynamic daily input of meteorological data and output of rice development and yield records. This was used to generate rice developmental stage and yield prediction models, which were integrated into the SBOCM system. The parameters, methods, error resources, and other factors were analyzed. Although not a crop physiology simulation model, the proposed SBOCM can be used for perennial simulation and one-year rice predictions within certain scale ranges. It is convenient for data acquisition, regionally applicable, parametrically simple, and effective for multi-scale factor integration. It has the potential for future integration with extensive social and economic factors to improve the prediction accuracy and practicability.},
  doi      = {10.1016/j.sjbs.2017.01.024},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1083415540},
}

@InProceedings{Sujatha2016,
  author    = {R. Sujatha and P. Isakki Devi},
  booktitle = {2016 International Conference on Computing Technologies and Intelligent Data Engineering (ICCTIDE'16)},
  title     = {A Study on Crop Yield Forecasting Using Classification Techniques},
  year      = {2016},
  pages     = {1-4},
  abstract  = {India is generally an agricultural country. Agriculture is the single most important contributor to the Indian economy. Agriculture crop production depends on the season, biological, and economic cause. The prognosticating of agricultural yield is a challenging and desirable task for every nation. Nowadays, Farmers are struggling to produce the yield because of unpredictable climatic changes and drastically reduce in water resource so; we are creating an agriculture data. This data could be gathered, stored and analyzed for useful information. It is used to promote new advanced methods and approaches such as data mining that can give the information of the previous results to the crop yield estimation. In this paper, we have demonstrated to estimate the crop yield, choose the most excellent crop, thereby improves the value and gain of the farming area using data mining techniques.},
  doi       = {10.1109/icctide.2016.7725357},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1094483007},
}

@InProceedings{Gandhi2016,
  author    = {Niketa Gandhi and Owaiz Petkar and Leisa J. Armstrong},
  booktitle = {2016 IEEE Technological Innovations in ICT for Agriculture and Rural Development (TIAR)},
  title     = {Rice Crop Yield Prediction Using Artificial Neural Networks},
  year      = {2016},
  pages     = {105-110},
  abstract  = {Rice crop production contributes to the food security of India, more than 40% to overall crop production. Its production is reliant on favorable climatic conditions. Variability from season to season is detrimental to the farmer's income and livelihoods. Improving the ability of farmers to predict crop productivity in under different climatic scenarios, can assist farmers and other stakeholders in making important decisions in terms of agronomy and crop choice. This study aimed to use neural networks to predict rice production yield and investigate the factors affecting the rice crop yield for various districts of Maharashtra state in India. Data were sourced from publicly available Indian Government's records for 27 districts of Maharashtra state, India. The parameters considered for the present study were precipitation, minimum temperature, average temperature, maximum temperature and reference crop evapotranspiration, area, production and yield for the Kharif season (June to November) for the years 1998 to 2002. The dataset was processed using WEKA tool. A Multilayer Perceptron Neural Network was developed. Cross validation method was used to validate the data. The results showed the accuracy of 97.5% with a sensitivity of 96.3 and specificity of 98.1. Further, mean absolute error, root mean squared error, relative absolute error and root relative squared error were calculated for the present study. The study dataset was also executed using Knowledge Flow of the WEKA tool. The performance of the classifier is visually summarized using ROC curve.},
  doi       = {10.1109/tiar.2016.7801222},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1095777829},
}

@Article{Bose2016,
  author   = {Pritam Bose and Nikola K. Kasabov and Lorenzo Bruzzone and Reggio N. Hartono},
  journal  = {IEEE Transactions on Geoscience and Remote Sensing},
  title    = {Spiking Neural Networks for Crop Yield Estimation Based on Spatiotemporal Analysis of Image Time Series},
  year     = {2016},
  number   = {11},
  pages    = {6563-6573},
  volume   = {54},
  abstract = {This paper presents spiking neural networks (SNNs) for remote sensing spatiotemporal analysis of image time series, which make use of the highly parallel and low-power-consuming neuromorphic hardware platforms possible. This paper illustrates this concept with the introduction of the first SNN computational model for crop yield estimation from normalized difference vegetation index image time series. It presents the development and testing of a methodological framework which utilizes the spatial accumulation of time series of Moderate Resolution Imaging Spectroradiometer 250-m resolution data and historical crop yield data to train an SNN to make timely prediction of crop yield. The research work also includes an analysis on the optimum number of features needed to optimize the results from our experimental data set. The proposed approach was applied to estimate the winter wheat (Triticum aestivum L.) yield in Shandong province, one of the main winter-wheat-growing regions of China. Our method was able to predict the yield around six weeks before harvest with a very high accuracy. Our methodology provided an average accuracy of 95.64%, with an average error of prediction of 0.236 t/ha and correlation coefficient of 0.801 based on a nine-feature model.},
  doi      = {10.1109/tgrs.2016.2586602},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061614459},
}

@InProceedings{Gandhi2016a,
  author    = {Niketa Gandhi and Leisa J. Armstrong and Owaiz Petkar and Amiya Kumar Tripathy},
  booktitle = {2016 13th International Joint Conference on Computer Science and Software Engineering (JCSSE)},
  title     = {Rice crop yield prediction in India using support vector machines},
  year      = {2016},
  pages     = {1-5},
  abstract  = {Food production in India is largely dependent on cereal crops including rice, wheat and various pulses. The sustainability and productivity of rice growing areas is dependent on suitable climatic conditions. Variability in seasonal climate conditions can have detrimental effect, with incidents of drought reducing production. Developing better techniques to predict crop productivity in different climatic conditions can assist farmer and other stakeholders in better decision making in terms of agronomy and crop choice. Machine learning techniques can be used to improve prediction of crop yield under different climatic scenarios. This paper presents the review on use of such machine learning technique for Indian rice cropping areas. This paper discusses the experimental results obtained by applying SMO classifier using the WEKA tool on the dataset of 27 districts of Maharashtra state, India. The dataset considered for the rice crop yield prediction was sourced from publicly available Indian Government records. The parameters considered for the study were precipitation, minimum temperature, average temperature, maximum temperature and reference crop evapotranspiration, area, production and yield for the Kharif season (June to November) for the years 1998 to 2002. For the present study the mean absolute error (MAE), root mean squared error (RMSE), relative absolute error (RAE) and root relative squared error (RRSE) were calculated. The experimental results showed that the performance of other techniques on the same dataset was much better compared to SMO.},
  doi       = {10.1109/jcsse.2016.7748856},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1093448171},
}

@Article{Everingham2016,
  author   = {Yvette Everingham and Justin Sexton and Danielle Skocaj and Geoff Inman-Bamber},
  journal  = {Agronomy for Sustainable Development},
  title    = {Accurate prediction of sugarcane yield using a random forest algorithm},
  year     = {2016},
  note     = {https://link.springer.com/content/pdf/10.1007/s13593-016-0364-z.pdf},
  number   = {2},
  pages    = {27},
  volume   = {36},
  abstract = {Foreknowledge about sugarcane crop size can help industry members make more informed decisions. There exists many different combinations of climate variables, seasonal climate prediction indices, and crop model outputs that could prove useful in explaining sugarcane crop size. A data mining method like random forests can cope with generating a prediction model when the search space of predictor variables is large. Research that has investigated the accuracy of random forests to explain annual variation in sugarcane productivity and the suitability of predictor variables generated from crop models coupled with observed climate and seasonal climate prediction indices is limited. Simulated biomass from the APSIM (Agricultural Production Systems sIMulator) sugarcane crop model, seasonal climate prediction indices and observed rainfall, maximum and minimum temperature, and radiation were supplied as inputs to a random forest classifier and a random forest regression model to explain annual variation in regional sugarcane yields at Tully, in northeastern Australia. Prediction models were generated on 1 September in the year before harvest, and then on 1 January and 1 March in the year of harvest, which typically runs from June to November. Our results indicated that in 86.36 % of years, it was possible to determine as early as September in the year before harvest if production would be above the median. This accuracy improved to 95.45 % by January in the year of harvest. The R-squared of the random forest regression model gradually improved from 66.76 to 79.21 % from September in the year before harvest through to March in the same year of harvest. All three sets of variables—(i) simulated biomass indices, (ii) observed climate, and (iii) seasonal climate prediction indices—were typically featured in the models at various stages. Better crop predictions allows farmers to improve their nitrogen management to meet the demands of the new crop, mill managers could better plan the mill’s labor requirements and maintenance scheduling activities, and marketers can more confidently manage the forward sale and storage of the crop. Hence, accurate yield forecasts can improve industry sustainability by delivering better environmental and economic outcomes.},
  doi      = {10.1007/s13593-016-0364-z},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1031921576},
}

@Article{Mola‐Yudego2016,
  author   = {Blas Mola‐Yudego and Johannes Rahlf and Rasmus Astrup and Ioannis Dimitriou},
  journal  = {GCB Bioenergy},
  title    = {Spatial yield estimates of fast‐growing willow plantations for energy based on climatic variables in northern Europe},
  year     = {2016},
  note     = {https://doi.org/10.1111/gcbb.12332},
  number   = {6},
  pages    = {1093-1105},
  volume   = {8},
  abstract = {Abstract  Spatially accurate and reliable estimates from fast‐growing plantations are a key factor for planning energy supply. This study aimed to estimate the yield of biomass from short rotation willow plantations in northern Europe. The data were based on harvesting records from 1790 commercial plantations in Sweden, grouped into three ad hoc categories: low, middle and high performance. The predictors included climatic variables, allowing the spatial extrapolation to nearby countries. The modeling and spatialization of the estimates used boosted regression trees, a method based on machine learning. The average RMSE for the final models selected was 0.33, 0.39 and 1.91 (corresponding to R 2  = 0.77, 0.88 and 0.45), for the low, medium and high performance categories, respectively. The models were then applied to obtain 1×1 km yield estimates in the rest of Sweden, as well as for Norway, Denmark, Finland, Estonia, Latvia, Lithuania and the Baltic coast of Germany and Poland. The results demonstrated a large regional variation. For the first rotation under high performance conditions, the country averages were as follows: >7 odt ha −1  yr −1 in the Baltic coast of Germany, >6 odt ha −1  yr −1 in Denmark, >5 odt ha −1  yr −1 in the Baltic coast of Poland and between 4–5 odt ha −1  yr −1 in the rest. The results of this approach indicate that they can provide faster and more accurate predictions than previous modeling approaches and can offer interesting possibilities in the field of yield modeling.},
  doi      = {10.1111/gcbb.12332},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1033490641},
}

@Article{Jeong2016,
  author   = {Jig Han Jeong and Jonathan P. Resop and Nathaniel D. Mueller and David H. Fleisher and Kyungdahm Yun and Ethan E. Butler and Dennis J. Timlin and Kyo-Moon Shim and James S. Gerber and Vangimalla R. Reddy and Soo-Hyung Kim},
  journal  = {PLOS ONE},
  title    = {Random Forests for Global and Regional Crop Yield Predictions},
  year     = {2016},
  note     = {https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0156571&type=printable},
  number   = {6},
  pages    = {e0156571},
  volume   = {11},
  abstract = {Accurate predictions of crop yield are critical for developing effective agricultural and food policies at the regional and global scales. We evaluated a machine-learning method, Random Forests (RF), for its ability to predict crop yield responses to climate and biophysical variables at global and regional scales in wheat, maize, and potato in comparison with multiple linear regressions (MLR) serving as a benchmark. We used crop yield data from various sources and regions for model training and testing: 1) gridded global wheat grain yield, 2) maize grain yield from US counties over thirty years, and 3) potato tuber and maize silage yield from the northeastern seaboard region. RF was found highly capable of predicting crop yields and outperformed MLR benchmarks in all performance statistics that were compared. For example, the root mean square errors (RMSE) ranged between 6 and 14% of the average observed yield with RF models in all test cases whereas these values ranged from 14% to 49% for MLR models. Our results show that RF is an effective and versatile machine-learning method for crop yield predictions at regional and global scales for its high accuracy and precision, ease of use, and utility in data analysis. RF may result in a loss of accuracy when predicting the extreme ends or responses beyond the boundaries of the training data.},
  doi      = {10.1371/journal.pone.0156571},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1003896548},
}

@Article{Pantazi2016,
  author   = {X.E. Pantazi and D. Moshou and T. Alexandridis and R.L. Whetton and A.M. Mouazen},
  journal  = {Computers and Electronics in Agriculture},
  title    = {Wheat yield prediction using machine learning and advanced sensing techniques},
  year     = {2016},
  pages    = {57-65},
  volume   = {121},
  abstract = {Understanding yield limiting factors requires high resolution multi-layer information about factors affecting crop growth and yield. Therefore, on-line proximal soil sensing for estimation of soil properties is required, due to the ability of these sensors to collect high resolution data (>1500 sample per ha), and subsequently reducing labor and time cost of soil sampling and analysis. The aim of this paper is to predict within field variation in wheat yield, based on on-line multi-layer soil data, and satellite imagery crop growth characteristics. Supervised self-organizing maps capable of handling existent information from different soil and crop sensors by utilizing an unsupervised learning algorithm were used. The performance of counter-propagation artificial neural networks (CP-ANNs), XY-fused Networks (XY-Fs) and Supervised Kohonen Networks (SKNs) for predicting wheat yield in a 22ha field in Bedfordshire, UK were compared for a single cropping season. The self organizing models consisted of input nodes corresponded to feature vectors formed from normalized values of on-line predicted soil parameters and the satellite normalized difference vegetation index (NDVI). The output nodes consisted of yield isofrequency classes, which were predicted from the three trained networks. Results showed that cross validation based yield prediction of the SKN model for the low yield class exceeded 91% which can be considered as highly accurate given the complex relationship between limiting factors and the yield. The medium and high yield class reached 70% and 83% respectively. The average overall accuracy for SKN was 81.65%, for CP-ANN 78.3% and for XY-F 80.92%, showing that the SKN model had the best overall performance.},
  doi      = {10.1016/j.compag.2015.11.018},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1040894579},
}

@InProceedings{Paul2015,
  author    = {Monali Paul and Santosh K. Vishwakarma and Ashok Verma},
  booktitle = {2015 International Conference on Computational Intelligence and Communication Networks (CICN)},
  title     = {Analysis of Soil Behaviour and Prediction of Crop Yield Using Data Mining Approach},
  year      = {2015},
  pages     = {766-771},
  abstract  = {Yield prediction is very popular among farmers these days, which particularly contributes to the proper selection of crops for sowing. This makes the problem of predicting the yielding of crops an interesting challenge. Earlier yield prediction was performed by considering the farmer's experience on a particular field and crop. This work presents a system, which uses data mining techniques in order to predict the category of the analyzed soil datasets. The category, thus predicted will indicate the yielding of crops. The problem of predicting the crop yield is formalized as a classification rule, where Naive Bayes and K-Nearest Neighbor methods are used.},
  doi       = {10.1109/cicn.2015.156},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1094154280},
}

@InProceedings{Ahamed2015,
  author    = {A.T.M Shakil Ahamed and Navid Tanzeem Mahmood and Nazmul Hossain and Mohammad Tanzir Kabir and Kallal Das and Faridur Rahman and Rashedur M Rahman},
  booktitle = {2015 IEEE/ACIS 16th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)},
  title     = {Applying Data Mining Techniques to Predict Annual Yield of Major Crops and Recommend Planting Different Crops in Different Districts in Bangladesh},
  year      = {2015},
  pages     = {1-6},
  abstract  = {Agricultural crop production depends on various factors such as biology, climate, economy and geography. Several factors have different impacts on agriculture, which can be quantified using appropriate statistical methodologies. Applying such methodologies and techniques on historical yield of crops, it is possible to obtain information or knowledge which can be helpful to farmers and government organizations for making better decisions and policies which lead to increased production. In this paper, our focus is on application of data mining techniques to extract knowledge from the agricultural data to estimate crop yield for major cereal crops in major districts of Bangladesh.},
  doi       = {10.1109/snpd.2015.7176185},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1094455706},
}

@Article{MATSUMURA2014,
  author   = {K. MATSUMURA and C. F. GAITAN and K. SUGIMOTO and A. J. CANNON and W. W. HSIEH},
  journal  = {The Journal of Agricultural Science},
  title    = {Maize yield forecasting by linear regression and artificial neural networks in Jilin, China},
  year     = {2014},
  note     = {https://www.cambridge.org/core/services/aop-cambridge-core/content/view/18FABBC7B735E5237CBC8D23B4AD7416/S0021859614000392a.pdf/div-class-title-maize-yield-forecasting-by-linear-regression-and-artificial-neural-networks-in-jilin-china-div.pdf},
  number   = {3},
  pages    = {399-410},
  volume   = {153},
  abstract = {SUMMARY
                  Forecasting the maize yield of China's Jilin province from 1962 to 2004, with climate conditions and fertilizer as predictors, was investigated using multiple linear regression (MLR) and non-linear artificial neural network (ANN) models. Yield was set to be a function of precipitation from July to August, precipitation in September and the amount of fertilizer used. Fertilizer emerged as the dominant predictor and was non-linearly related to yield in the ANN model. Given the difficulty of acquiring fertilizer data for maize, the current study was also tested using the previous year's yield in the place of fertilizer data. Forecast skill scores computed under both cross-validation and retroactive validation showed ANN models to significantly outperform MLR and persistence (i.e. forecast yield is identical to last year's observed yield). As the data were non-stationary, cross-validation was found to be less reliable than retroactive validation in assessing the forecast skill.},
  doi      = {10.1017/s0021859614000392},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1016061586},
}

@InBook{Kunapuli2015,
  author    = {S.S. Kunapuli and V. Rueda-Ayala and G. Benavídez-Gutiérrez and A. Córdova-Cruzatty and A. Cabrera and C. Fernández and J. Maiguashca},
  pages     = {199-206},
  title     = {Yield prediction for precision territorial management in maize using spectral data},
  year      = {2015},
  booktitle = {Precision agriculture '15},
  doi       = {10.3920/978-90-8686-814-8_24},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1087231057},
}

@InProceedings{Rahman2014,
  author    = {Mohammad Motiur Rahman and Naheena Haq and Rashedur M Rahman},
  booktitle = {2014 Annual Global Online Conference on Information and Computer Technology},
  title     = {Machine Learning Facilitated Rice Prediction in Bangladesh},
  year      = {2014},
  pages     = {1-4},
  abstract  = {The climate of a region is often determined by its landscape and amount of vegetation present in it. Environment parameters such as rainfall, wind-speed and humidity are highly influenced by these alluvial features. Bangladesh, a country situated on the banks of the Himalaya, does not have a homogeneous topography. Human settlement over the course of centuries has led to pockets of micro-regions. Each of those regions has a different micro climate. An entrepreneur involved in the food industry therefore has to carefully choose regions of land that will give him/her the desirable production. In this study a research initiative has been taken to predict the yield of crops using machine learning models. The models were at first trained on the correlation between past environmental patterns and crop production rate. Then the models are compared to measure their effectiveness on unknown climatic variables.},
  doi       = {10.1109/gocict.2014.9},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1093670904},
}

@InProceedings{Cakir2014,
  author    = {Yüksel Çakir and Mürvet Kırcı and Ece Olcay Güneş},
  booktitle = {2014 The Third International Conference on Agro-Geoinformatics},
  title     = {Yield prediction of wheat in south-east region of Turkey by using artificial neural networks},
  year      = {2014},
  pages     = {1-4},
  abstract  = {In Turkey, similarly to other grain producing countries, the prediction of wheat yield is an important problem. The objective in this study is to build an artificial neural network model that could effectively predict wheat yield by using meteorological data such as temperature and rainfall records. Multi-Layer Perceptron neural network model was chosen and the performance of the built network was tested for different input and neurons number. For defining the model parameters back propagation training technique was used. During the training of the network, various learning rates were chosen and the optimal values for these parameters were defined. For the final assessment of the obtained results a multiple parameter linear regression model was developed and tested with the same data set used for the built artificial neural network.},
  doi       = {10.1109/agro-geoinformatics.2014.6910609},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1095337245},
}

@InBook{Pantazi2014,
  author    = {Xanthoula Eirini Pantazi and Dimitrios Moshou and Abdul Mounem Mouazen and Boyan Kuang and Thomas Alexandridis},
  pages     = {556-565},
  title     = {Application of Supervised Self Organising Models for Wheat Yield Prediction},
  year      = {2014},
  note      = {https://link.springer.com/content/pdf/10.1007%2F978-3-662-44654-6_55.pdf},
  abstract  = {The management of wheat yield behavior in agricultural areas is a very important task because it influences and specifies the wheat yield production. An efficient knowledge-based approach utilizing an efficient Machine Learning algorithm for characterizing wheat yield behavior is presented in this research work. The novelty of the method is based on the use of Supervised Self Organizing Maps to handle existent sensor information by using a supervised learning algorithm so as to assess measurement data and update initial knowledge. The advent of precision farming generates data which, because of their type and complexity, are not efficiently analyzed by traditional methods. The Supervised Self Organizing Maps have been proved from the literature efficient and flexible to analyze sensor information and by using the appropriate learning algorithms can update the initial knowledge. The Self Organizing models that are developed consisted of input nodes representing the main factors in wheat crop production such as biomass indicators, Organic Carbon (OC), pH, Mg, Total N, Ca, Cation Exchange Capacity (CEC), Moisture Content (MC) and the output weights represented the class labels corresponding to the predicted wheat yield.},
  booktitle = {Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications},
  doi       = {10.1007/978-3-662-44654-6_55},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1003708746},
}

@Article{GonzalezSanchez2014,
  author   = {Alberto Gonzalez-Sanchez and Juan Frausto-Solis and Waldo Ojeda-Bustamante},
  journal  = {Spanish Journal of Agricultural Research},
  title    = {Predictive ability of machine learning methods for massive crop yield prediction},
  year     = {2014},
  note     = {https://revistas.inia.es/index.php/sjar/article/download/4439/2050},
  number   = {2},
  pages    = {313-328},
  volume   = {12},
  abstract = {An important issue for agricultural planning purposes is the accurate yield estimation for the numerous crops involved in the planning. Machine learning (ML) is an essential approach for achieving practical and effective solutions for this problem. Many comparisons of ML methods for yield prediction have been made, seeking for the most accurate technique. Generally, the number of evaluated crops and techniques is too low and does not provide enough information for agricultural planning purposes. This paper compares the predictive accuracy of ML and linear regression techniques for crop yield prediction in ten crop datasets. Multiple linear regression, M5-Prime regression trees, perceptron multilayer neural networks, support vector regression and k-nearest neighbor methods were ranked. Four accuracy metrics were used to validate the models: the root mean square error (RMS), root relative square error (RRSE), normalized mean absolute error (MAE), and correlation factor (R). Real data of an irrigation zone of Mexico were used for building the models. Models were tested with samples of two consecutive years. The results show that M5-Prime and k-nearest neighbor techniques obtain the lowest average RMSE errors (5.14 and 4.91), the lowest RRSE errors (79.46% and 79.78%), the lowest average MAE errors (18.12% and 19.42%), and the highest average correlation factors (0.41 and 0.42). Since M5-Prime achieves the largest number of crop yield models with the lowest errors, it is a very suitable tool for massive crop yield prediction in agricultural planning.},
  doi      = {10.5424/sjar/2014122-4439},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1072842102},
}

@Article{Shekoofa2014,
  author   = {Avat Shekoofa and Yahya Emam and Navid Shekoufa and Mansour Ebrahimi and Esmaeil Ebrahimie},
  journal  = {PLOS ONE},
  title    = {Determining the Most Important Physiological and Agronomic Traits Contributing to Maize Grain Yield through Machine Learning Algorithms: A New Avenue in Intelligent Agriculture},
  year     = {2014},
  note     = {https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0097288&type=printable},
  number   = {5},
  pages    = {e97288},
  volume   = {9},
  abstract = {Prediction is an attempt to accurately forecast the outcome of a specific situation while using input information obtained from a set of variables that potentially describe the situation. They can be used to project physiological and agronomic processes; regarding this fact, agronomic traits such as yield can be affected by a large number of variables. In this study, we analyzed a large number of physiological and agronomic traits by screening, clustering, and decision tree models to select the most relevant factors for the prospect of accurately increasing maize grain yield. Decision tree models (with nearly the same performance evaluation) were the most useful tools in understanding the underlying relationships in physiological and agronomic features for selecting the most important and relevant traits (sowing date-location, kernel number per ear, maximum water content, kernel weight, and season duration) corresponding to the maize grain yield. In particular, decision tree generated by C&RT algorithm was the best model for yield prediction based on physiological and agronomical traits which can be extensively employed in future breeding programs. No significant differences in the decision tree models were found when feature selection filtering on data were used, but positive feature selection effect observed in clustering models. Finally, the results showed that the proposed model techniques are useful tools for crop physiologists to search through large datasets seeking patterns for the physiological and agronomic factors, and may assist the selection of the most important traits for the individual site and field. In particular, decision tree models are method of choice with the capability of illustrating different pathways of yield increase in breeding programs, governed by their hierarchy structure of feature ranking as well as pattern discovery via various combinations of features.},
  doi      = {10.1371/journal.pone.0097288},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1001065952},
}

@InProceedings{Gunasundari2013,
  author    = {M. Gunasundari and T. Arunkumar and R. Hemavathy},
  booktitle = {2013 International Conference on Pattern Recognition, Informatics and Mobile Engineering},
  title     = {CRY — An improved crop yield prediction model using bee hive clustering approach for agricultural data sets},
  year      = {2013},
  pages     = {473-478},
  abstract  = {Agricultural researchers over the world insist on the need for an efficient mechanism to predict and improve the crop growth. The need for an integrated crop growth control with accurate predictive yield management methodology is highly felt among farming community. The complexity of predicting the crop yield is highly due to multi dimensional variable metrics and unavailability of predictive modeling approach, which leads to loss in crop yield. This research paper suggests a crop yield prediction model (CRY) which works on an adaptive cluster approach over dynamically updated historical crop data set to predict the crop yield and improve the decision making in precision agriculture. CRY uses bee hive modeling approach to analyze and classify the crop based on crop growth pattern, yield. CRY classified dataset had been tested using Clementine over existing crop domain knowledge. The results and performance shows comparison of CRY over with other cluster approaches.},
  doi       = {10.1109/icprime.2013.6496717},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1093207217},
}

@Article{Romero2013,
  author   = {José R. Romero and Pablo F. Roncallo and Pavan C. Akkiraju and Ignacio Ponzoni and Viviana C. Echenique and Jessica A. Carballido},
  journal  = {Computers and Electronics in Agriculture},
  title    = {Using classification algorithms for predicting durum wheat yield in the province of Buenos Aires},
  year     = {2013},
  note     = {https://ri.conicet.gov.ar/bitstream/11336/12720/1/CONICET_Digital_Nro.12195.pdf},
  pages    = {173-179},
  volume   = {96},
  abstract = {Wheat is one of the most important cereals worldwide for human nutrition. Tetraploid wheat (Triticum turgidum L. ssp. durum, 2n =28, genomes AABB) is mainly used to produce pasta. The main objective of durum wheat breeding programs is to develop varieties with good quality and high yields. Yield is a very complex trait, and depends on different yield components that are genetically controlled and affected by environmental constraints. In this context, machine learning constitutes an excellent alternative for the analysis of a high number of traits in order to extract the most relevant ones as confident predictors of the performance of this crop, allowing a better agricultural planning. Thus, we propose the use of machine learning algorithms for the classification of yield components and for the search of new rules to infer high yields at harvest of durum wheat. The main objective of this work was to obtain rules for predicting durum wheat yield through different machine learning algorithms, and compare them to detect the one that best fits the model. In order to achieve this goal, One-R, J48, Ibk and A priori algorithms were run with data collected by our research group of a RIL (recombinant inbreed lines) population growing in six different environments from the Province of Buenos Aires in Argentina. The results indicate that the A priori method obtains the best performance for all locations, and the classificators generated using the different algorithms share a common set of selected traits. Moreover, comparing these results with the previous ones obtained using different techniques, mainly QTL mapping, the traits indicated to be the most significant ones were the same. The analysis of the resulting rules shows the soundness in the agronomic relevance of the extracted knowledge.},
  doi      = {10.1016/j.compag.2013.05.006},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1001894773},
}

@Article{Johnson2016,
  author   = {Michael D. Johnson and William W. Hsieh and Alex J. Cannon and Andrew Davidson and Frédéric Bédard},
  journal  = {Agricultural and Forest Meteorology},
  title    = {Crop yield forecasting on the Canadian Prairies by remotely sensed vegetation indices and machine learning methods},
  year     = {2016},
  pages    = {74-84},
  volume   = {218},
  abstract = {Crop yield forecast models for barley, canola and spring wheat grown on the Canadian Prairies were developed using vegetation indices derived from satellite data and machine learning methods. Hierarchical clustering was used to group the crop yield data from 40 Census Agricultural Regions (CARs) into several larger regions for building the forecast models. The Normalized Difference Vegetation Index (NDVI) and Enhanced Vegetation Index (EVI) derived from the Moderate-resolution Imaging Spectroradiometer (MODIS), and NDVI derived from the Advanced Very High Resolution Radiometer (AVHRR) were considered as predictors for crop yields. Multiple linear regression (MLR) and two nonlinear machine learning models – Bayesian neural networks (BNN) and model-based recursive partitioning (MOB) – were used to forecast crop yields, with various combinations of MODIS-NDVI, MODIS-EVI and NOAA-NDVI as predictors. Crop yield forecasts made using predictors from July and earlier were evaluated by the cross-validated mean absolute error skill score (in reference to climatological forecasts) during 2000–2011. While MODIS-NDVI was found to be the most effective predictor for all three crops, having MODIS-EVI as an additional predictor enhanced the forecast skills. While MLR, BNN and MOB all showed significantly higher skills than climatological forecasts for all three crops, barley was the only case where the nonlinear BNN and MOB models showed slightly higher skills than MLR. The lack of skill improvement by nonlinear models over MLR is likely due to the short (12 years) record available for MODIS data, which limits our study to 2000–2011, with very low yields coming from a single severe drought year (2002).},
  doi      = {10.1016/j.agrformet.2015.11.003},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1033258084},
}

@Article{Crtomir2012,
  author   = {Rozman Črtomir and Cvelbar Urška and Tojnko Stanislav and Stajnko Denis and Pažek Karmen and Martin Pavlovič and Vračko Marjan},
  journal  = {Applied Fruit Science},
  title    = {Application of Neural Networks and Image Visualization for Early Forecast of Apple Yield},
  year     = {2012},
  number   = {2},
  pages    = {69-76},
  volume   = {54},
  abstract = {Early information on yield has a special importance in the intensive apple production. Since the majority of older forecast methods are labor, time, organization and cost intensive a hybrid model based on image analysis and neural network was developed. From the end of fruit thinning in June till harvesting digital images of 120 trees of yellow-skin ‘Golden Delicious’ (four times) and 120 trees of red-skin ‘Braeburn’ (five times) were captured from intensive orchards. Firstly, each image was processed by image analysis algorithm to receive the data on number of fruits and a yield forecast, for each sampling period separately, which served as the input information for modeling the yield with the artificial neural network (ANN). The forecast of the hybrid method showed a higher accuracy than the image analysis for both varieties, since the new procedure managed to increase the correlation between the forecasted and weighed yield from 0.73 to 0.83 for ‘Golden Delicious’ and from 0.51 to 0.78 for ‘Braeburn’. The standard deviation/image was decreased from 4.79 to 2.83 kg for ‘Golden Delicious’ and from 3.64 to 2.55 kg for ‘Braeburn’. To introduce the new method in practice, additional tests on various locations including all important apple varieties are recommended.},
  doi      = {10.1007/s10341-012-0162-y},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1041052930},
}

@InBook{Baral2011,
  author    = {Seshadri Baral and Asis Kumar Tripathy and Pritiranjan Bijayasingh},
  pages     = {315-317},
  title     = {Yield Prediction Using Artificial Neural Networks},
  year      = {2011},
  abstract  = {Artificial Neural Network (ANN’s) technology with PSO as optimization technique was used for the approximation and prediction of paddy yield at 3 different districts in different climatic zones based on 10 years of historical data sets of yields of paddy ,daily temperature(mean and maximum) and precipitation(rainfall).},
  booktitle = {Computer Networks and Information Technologies},
  doi       = {10.1007/978-3-642-19542-6_57},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1005691069},
}

@InBook{Russ2010,
  author    = {Georg Ruß and Rudolf Kruse},
  pages     = {450-463},
  title     = {Regression Models for Spatial Data: An Example from Precision Agriculture},
  year      = {2010},
  abstract  = {The term precision agriculture refers to the application of state-of-the-art GPS technology in connection with small-scale, sensor-based treatment of the crop. This data-driven approach to agriculture poses a number of data mining problems. One of those is also an obviously important task in agriculture: yield prediction. Given a precise, geographically annotated data set for a certain field, can a season’s yield be predicted?Numerous approaches have been proposed to solving this problem. In the past, classical regression models for non-spatial data have been used, like regression trees, neural networks and support vector machines. However, in a cross-validation learning approach, issues with the assumption of statistical independence of the data records appear. Therefore, the geographical location of data records should clearly be considered while employing a regression model. This paper gives a short overview about the available data, points out the issues with the classical learning approaches and presents a novel spatial cross-validation technique to overcome the problems and solve the aforementioned yield prediction task.},
  booktitle = {Advances in Data Mining. Applications and Theoretical Aspects},
  doi       = {10.1007/978-3-642-14400-4_35},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1004965338},
}

@Article{Everingham2009,
  author   = {Y.L. Everingham and C.W. Smyth and N.G. Inman-Bamber},
  journal  = {Agricultural and Forest Meteorology},
  title    = {Ensemble data mining approaches to forecast regional sugarcane crop production},
  year     = {2009},
  number   = {3-4},
  pages    = {689-696},
  volume   = {149},
  abstract = {Accurate yield forecasts are pivotal for the success of any agricultural industry that plans or sells ahead of the annual harvest. Biophysical models that integrate information about crop growing conditions can give early insight about the likely size of a crop. At a point scale, where highly detailed knowledge about environmental and management conditions are known, the performance of reputable crop modelling approaches like APSIM have been well established. However, regional growing conditions tend not to be homogenous. Heterogeneity is common in many agricultural systems, and particularly in sugarcane systems. To overcome this obstacle, hundreds of model settings (‘models’ for convenience) that represent different environmental and management conditions were created for Ayr, a major sugarcane growing region in north eastern Australia. Statistical data mining methods that used ensembles were used to select and assign weights to the best models. One technique, called a lasso approximation produced the best results. This procedure, produced a predictive correlation (rcv) of 0.71 when predicting end of season sugarcane yields some 4 months prior to the start of the harvest season, and 10 months prior to harvest completion. This continuous forecasting methodology based on statistical ensembles represents a considerable improvement upon previous research where only categorical forecast predictions had been employed.},
  doi      = {10.1016/j.agrformet.2008.10.018},
  groups   = {Crop Yield Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1028808547},
}

@InBook{Russ2008,
  author    = {Georg Ruß and Rudolf Kruse and Martin Schneider and Peter Wagner},
  pages     = {47-56},
  title     = {Data Mining with Neural Networks for Wheat Yield Prediction},
  year      = {2008},
  abstract  = {Precision agriculture (PA) and information technology (IT) are closely interwoven. The former usually refers to the application of nowadays’ technology to agriculture. Due to the use of sensors and GPS technology, in today’s agriculture many data are collected. Making use of those data via IT often leads to dramatic improvements in efficiency. For this purpose, the challenge is to change these raw data into useful information. In this paper we deal with neural networks and their usage in mining these data. Our particular focus is whether neural networks can be used for predicting wheat yield from cheaply-available in-season data. Once this prediction is possible, the industrial application is quite straightforward: use data mining with neural networks for, e.g., optimizing fertilizer usage, in economic or environmental terms.},
  booktitle = {Advances in Data Mining. Medical Applications, E-Commerce, Marketing, and Theoretical Aspects},
  doi       = {10.1007/978-3-540-70720-2_4},
  groups    = {Crop Yield Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1011532234},
}

@Article{Rejeb2022,
  author   = {Abderahman Rejeb and Alireza Abdollahi and Karim Rejeb and Horst Treiblmaier},
  journal  = {Computers and Electronics in Agriculture},
  title    = {Drones in agriculture: A review and bibliometric analysis},
  year     = {2022},
  note     = {https://doi.org/10.1016/j.compag.2022.107017},
  pages    = {107017},
  volume   = {198},
  abstract = {Drones, also called Unmanned Aerial Vehicles (UAV), have witnessed a remarkable development in recent decades. In agriculture, they have changed farming practices by offering farmers substantial cost savings, increased operational efficiency, and better profitability. Over the past decades, the topic of agricultural drones has attracted remarkable academic attention. We therefore conduct a comprehensive review based on bibliometrics to summarize and structure existing academic literature and reveal current research trends and hotspots. We apply bibliometric techniques and analyze the literature surrounding agricultural drones to summarize and assess previous research. Our analysis indicates that remote sensing, precision agriculture, deep learning, machine learning, and the Internet of Things are critical topics related to agricultural drones. The co-citation analysis reveals six broad research clusters in the literature. This study is one of the first attempts to summarize drone research in agriculture and suggest future research directions.},
  doi      = {10.1016/j.compag.2022.107017},
  groups   = {Drones in Agriculture},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1147958699},
}

@Article{Klompenburg2020,
  author   = {Thomas van Klompenburg and Ayalew Kassahun and Cagatay Catal},
  journal  = {Computers and Electronics in Agriculture},
  title    = {Crop yield prediction using machine learning: A systematic literature review},
  year     = {2020},
  note     = {https://doi.org/10.1016/j.compag.2020.105709},
  pages    = {105709},
  volume   = {177},
  abstract = {Machine learning is an important decision support tool for crop yield prediction, including supporting decisions on what crops to grow and what to do during the growing season of the crops. Several machine learning algorithms have been applied to support crop yield prediction research. In this study, we performed a Systematic Literature Review (SLR) to extract and synthesize the algorithms and features that have been used in crop yield prediction studies. Based on our search criteria, we retrieved 567 relevant studies from six electronic databases, of which we have selected 50 studies for further analysis using inclusion and exclusion criteria. We investigated these selected studies carefully, analyzed the methods and features used, and provided suggestions for further research. According to our analysis, the most used features are temperature, rainfall, and soil type, and the most applied algorithm is Artificial Neural Networks in these models. After this observation based on the analysis of machine learning-based 50 papers, we performed an additional search in electronic databases to identify deep learning-based studies, reached 30 deep learning-based papers, and extracted the applied deep learning algorithms. According to this additional analysis, Convolutional Neural Networks (CNN) is the most widely used deep learning algorithm in these studies, and the other widely used deep learning algorithms are Long-Short Term Memory (LSTM) and Deep Neural Networks (DNN).},
  doi      = {10.1016/j.compag.2020.105709},
  groups   = {Crop Yield Prediction},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1130187939},
}

@Article{Boddu2023,
  author   = {Sayi P. Boddu and M. Lane Moore and Bryeson M. Rodgers and Joseph C. Brinkman and Jens T. Verhey and Joshua S. Bingham},
  journal  = {Arthroplasty Today},
  title    = {A Bibliometric Analysis of the Top 100 Most Influential Studies on Robotic Arthroplasty},
  year     = {2023},
  note     = {http://www.arthroplastytoday.org/article/S2352344123000584/pdf},
  pages    = {101153},
  volume   = {22},
  abstract = {Background: The use of robotics in arthroplasty surgery has increased substantially in recent years. The purpose of this study was to objectively identify the 100 most influential studies in the robotic arthroplasty literature and to conduct a bibliometric analysis of these studies to describe their key characteristics.
Methods: The Clarivate Analytics Web of Knowledge database was used to gather data and metrics for robotic arthroplasty research using Boolean queries. The search list was sorted in descending order by the number of citations, and articles were included or excluded based on clinical relevance to robotic arthroplasty.
Results: The top 100 studies were cited a total of 5770 times from 1997 to 2021, with rapid growth in both citation generation and the number of articles published occurring in the past 5 years. The top 100 robotic arthroplasty articles originated from 12 countries, with the United States being responsible for almost half of the top 100. The most common study types were comparative studies (36) followed by case series (20), and the most common levels of evidence were III (23) and IV (33).
Conclusions: Research on robotic arthroplasty is rapidly growing and originates from a wide variety of countries, academic institutions, and with significant industry influence. This article serves as a reference to direct orthopaedic practitioners to the 100 most influential studies in robotic arthroplasty. We hope that these 100 studies and the analysis we provide aid healthcare professionals in efficiently assessing consensus, trends, and needs within the field.},
  doi      = {10.1016/j.artd.2023.101153},
  groups   = {Robotic Arthroplasty},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1159788042},
}

@Article{Bao2018,
  author   = {Guanjun Bao and Hui Fang and Lingfeng Chen and Yuehua Wan and Fang Xu and Qinghua Yang and Libin Zhang},
  journal  = {Soft Robotics},
  title    = {Soft Robotics: Academic Insights and Perspectives Through Bibliometric Analysis},
  year     = {2018},
  note     = {https://www.liebertpub.com/doi/pdf/10.1089/soro.2017.0135},
  number   = {3},
  pages    = {229-241},
  volume   = {5},
  abstract = {Soft robotics is of growing interest in the robot community as well as in public media, and there is an increase in the quality and quantity of publications related to this topic. To formally elaborate this growth, we have used a bibliometric analysis to evaluate the publications in the field from 1990 to 2017 based on the Science Citation Index Expanded database. We present a detailed overview and discussion based on keywords, citation, h-index, year, journal, institution, country, author, and review articles. The results show that the United States takes the leading position in this research field, followed by China and Italy. Harvard University has the most publications, high average number of citations per publication and the highest h-index. IEEE Transactions on Robotics ranks first among the top 20 academic journals publishing articles related to this field, whereas Soft Robotics holds the top position in journals categorized with "ROBOTICS." Actuator, fabrication, control, material, sensing, simulation, bionics, stiffness, modeling, power, motion, and application are the hot topics of soft robotics. Smart materials, bionics, morphological computation, and embodiment control are expected to contribute to this field in the future. Application and commercialization appear to be the initial driving force and final goal for soft robots.},
  doi      = {10.1089/soro.2017.0135},
  groups   = {Soft Robotics},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1104146079},
}

@Article{Ahmad2020,
  author   = {Nisar Ahmad and Angeliki N. Menegaki and Saeed Al‐Muharrami},
  journal  = {Journal of Economic Surveys},
  title    = {SYSTEMATIC LITERATURE REVIEW OF TOURISM GROWTH NEXUS: AN OVERVIEW OF THE LITERATURE AND A CONTENT ANALYSIS OF 100 MOST INFLUENTIAL PAPERS},
  year     = {2020},
  number   = {5},
  pages    = {1068-1110},
  volume   = {34},
  abstract = {Abstract  This study performs the first citation‐based systematic literature review of the tourism‐growth nexus . The citation analysis provides a bird's eye view of this literature, which, in turn, identifies the sources of knowledge in terms of most influential journals, authors, and articles. A detailed content analysis of 100 most influential papers has been generated on the nature of the study, variables used, country of analysis, type of analysis, the methodology, and the direction of causality. In total, 284 papers were found relevant in the Scopus database using a comprehensive list of keywords. The citation analysis reveals that Tourism Management is the leading journal with a total of 2527 citation counts, whereas Tourism Economics is the leading journal with a total of 41 publications on this topic. Juan Gabriel Brida is the most prolific author, whereas Lee and Chang (2008) is recognized as the most influential paper. The content analysis reveals that 58% have applied time series, and 38% have used panel data analysis. Tourism causing growth is the leading result of both time‐series and panel studies. International tourism receipts/earnings/expenditure and the number of international tourist arrivals are the most widely used variables to measure tourism.},
  doi      = {10.1111/joes.12386},
  groups   = {Tourism Growth Nexus},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1129627241},
}

@Article{Harito2024,
  author   = {Christian Harito and Syauqi Abdurrahman Abrori and Munawar Khalil and Brian Yuliarto and Sule Erten-Ela},
  journal  = {Current Opinion in Colloid & Interface Science},
  title    = {Current progress of perovskite solar cells stability with bibliometric study},
  year     = {2024},
  pages    = {101862},
  volume   = {74},
  abstract = {Perovskite solar cells have matched or even surpassed commercial silicone-based photovoltaics (PVs) in terms of cost effectiveness and power conversion efficiency. However, the stability is quite far behind the commercial silicone-based PV. Humidity, electrical bias, high temperature, and ultraviolet light are the determining stressors in the degradation of perovskite solar cells. This review provides the current advancement (2022 to July 31st, 2024) to the stability problem in perovskite solar cells. Equipped with bibliometric study, we deploy keyword analysis, citation analysis, and notable progress to give an overview and latest progress in perovskite solar cells stability. The importance of interface passivation is highlighted. The scalability studies of nontoxic, lead-free, stable perovskite solar cells are expected in near future.},
  doi      = {10.1016/j.cocis.2024.101862},
  groups   = {Perovskite Solar Cells Stability},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1175782890},
}

@Article{Hasan2023,
  author   = {Morshadul Hasan and Mohammad Zoynul Abedin and Mohamamd Bin Amin and Md Nekmahmud and Judit Oláh},
  journal  = {Journal of Environmental Management},
  title    = {Sustainable biofuel economy: A mapping through bibliometric research},
  year     = {2023},
  note     = {https://doi.org/10.1016/j.jenvman.2023.117644},
  pages    = {117644},
  volume   = {336},
  abstract = {Biofuels have received a lot of attention as an important source of renewable energy, with number of economic impacts. This study aims to investigate the economic potential of biofuels and then extract core aspects of how biofuels relate to a sustainable economy in order to achieve a sustainable biofuel economy. This study conducts a bibliometric analysis of publications about biofuel economic research covering 2001 to 2022 experimenting with multiple bibliometric tools, such as R Studio, Biblioshiny, and VOSviewer. Findings show that research on biofuels and biofuel production growth are positively correlated. From the analyzed publications, The United States, India, China, and Europe are the largest biofuel markets, with the USA taking the lead in publishing scientific papers, engaging country collaboration on biofuel, and has the highest social impact. Findings also show that the United Kingdom, the Netherlands, Germany, France, Sweden, and Spain are more inclined to develop sustainable biofuel economies and energy than other European countries. It also indicates that sustainable biofuel economies are still far behind those of less developed and developing countries. Besides, this study finds that biofuel linked to sustainable economy with poverty reduction, agriculture development, renewable energy production, economic growth, climate change policy, environmental protection, carbon emission reduction, green-house gas emission, land use policy, technological innovations, and development. The findings of this bibliometric research are presented using different clusters, mapping, and statistics. The discussion of this study affirms the good and effective policies for a sustainable biofuel economy.},
  doi      = {10.1016/j.jenvman.2023.117644},
  groups   = {Sustainable Biofuel Economy},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1156021846},
}

@Article{Yeung2020,
  author   = {Andy Wai Kan Yeung and Eliana B. Souto and Alessandra Durazzo and Massimo Lucarini and Ettore Novellino and Devesh Tewari and Dongdong Wang and Atanas G. Atanasov and Antonello Santini},
  journal  = {Current Research in Biotechnology},
  title    = {Big impact of nanoparticles: analysis of the most cited nanopharmaceuticals and nanonutraceuticals research},
  year     = {2020},
  note     = {https://doi.org/10.1016/j.crbiot.2020.04.002},
  pages    = {53-63},
  volume   = {2},
  abstract = {Nanopharmaceuticals and nanonutraceuticals research has been lately receiving a lot of scientific attention. We aimed to identify the top 100 most cited original articles of the scientific area, analyze their research themes, major contributors regarding authors, institutions, countries and journals. The bibliometric data was extracted from the Web of Science electronic database. Data was further processed by a bibliometric software, VOSviewer, to generate bubble maps to visualize the results. Inter-institutional and international collaboration networks were constructed to further understand the cooperation between different study centers. Results revealed that over 60% of the articles were published in the 2000s. As of November 2019, the articles were cited 576–3665 times, with 20.1–261.8 citations per year. The majority of the most prolific institutions were based in the United States. Besides the United States, China, South Korea, Canada and Germany contributed heavily to the 100 articles. Some popular themes included drug delivery, tumor, toxicity/biocompatibility and biodistribution. Regarding composition materials, gold, silver and polymeric nanoparticles were the most commonly used.},
  doi      = {10.1016/j.crbiot.2020.04.002},
  groups   = {Nanopharmaceuticals OR Nanonutraceuticals},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1127137176},
}

@Article{Perotti2023,
  author   = {Sara Perotti and Claudia Colicchia},
  journal  = {The International Journal of Logistics Management},
  title    = {Greening warehouses through energy efficiency and environmental impact reduction: a conceptual framework based on a systematic literature review},
  year     = {2023},
  note     = {https://re.public.polimi.it/bitstream/11311/1263126/1/Greening%20warehouses_postprint.pdf},
  number   = {7},
  pages    = {199-234},
  volume   = {34},
  abstract = {Purpose The purpose of this paper is to propose a framework of green strategies as a combination of energy-efficiency measures and solutions towards environmental impact reduction for improving environmental sustainability at logistics sites. Such measures are examined by discussing the related impacts, motivations and barriers that could influence the measures' adoption. Starting from the framework, directions for future research in this field are outlined.   Design/methodology/approach The proposed framework was developed starting from a systematic literature review (SLR) approach on 60 papers published from 2008 to 2022 in international peer-reviewed journals or conference proceedings.   Findings The framework identifies six main areas of intervention (“green strategies”) towards green warehousing, namely Building, Utilities, Lighting, Material Handling and Automation, Materials and Operational Practices. For each strategy, specific energy-efficiency measures and solutions towards environmental impact reduction are further pinpointed. In most cases, “green-gold” measures emerge as the most appealing, entailing environmental and economic benefits at the same time. Finally, for each measure the relationship with the measures' primary impacts is discussed.   Originality/value From an academic viewpoint, the framework fills a major gap in the scientific literature since, for the first time, this study elaborates the concept of green warehousing as a result of energy-efficiency measures and solutions towards environmental impact reduction. A classification of the main areas of intervention (“green strategies”) is proposed by adopting a holistic approach. From a managerial perspective, the paper addresses a compelling need of practitioners – e.g. logistics service providers (LSPs), manufacturers and retailers – for practices and solutions towards greener warehousing processes to increase energy efficiency and decrease the environmental impact of the practitioners' logistics facilities. In this sense, the proposed framework can provide valuable support for logistics managers that are about to approach the challenge of turning the managers' warehouses into greener nodes of the managers' supply chains.},
  doi      = {10.1108/ijlm-02-2022-0086},
  groups   = {Green Warehousing},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1163752603},
}

@Article{Ahmadi2018,
  author   = {Hossein Ahmadi and Goli Arji and Leila Shahmoradi and Reza Safdari and Mehrbakhsh Nilashi and Mojtaba Alizadeh},
  journal  = {Universal Access in the Information Society},
  title    = {The application of internet of things in healthcare: a systematic literature review and classification},
  year     = {2018},
  number   = {4},
  pages    = {837-869},
  volume   = {18},
  abstract = {The Internet of Things (IoT) is an ecosystem that integrates physical objects, software and hardware to interact with each other. Aging of population, shortage of healthcare resources, and rising medical costs make IoT-based technologies necessary to be tailored to address these challenges in healthcare. This systematic literature review has been conducted to determine the main application area of IoT in healthcare, components of IoT architecture in healthcare, most important technologies in IoT, characteristics of cloud-based architecture, security and interoperability issues in IoT architecture and effects, and challenges of IoT in healthcare. Sixty relevant papers, published between 2000 and 2016, were reviewed and analyzed. This analysis revealed that home healthcare service was one of the main application areas of IoT in healthcare. Cloud-based architecture, by providing great flexibility and scalability, has been deployed in most of the reviewed studies. Communication technologies including wireless fidelity (Wi-Fi), Bluetooth, radio-frequency identification (RFID), ZigBee, and Low-Power Wireless Personal Area Networks (LoWPAN) were frequently used in different IoT models. The studies regarding the security and interoperability issues in IoT architecture in health are still low in number. With respect to the most important effects of IoT in healthcare, these included ability of information exchange, decreasing stay of hospitalization and healthcare costs. The main challenges of IoT in healthcare were security and privacy issues.},
  doi      = {10.1007/s10209-018-0618-4},
  groups   = {Internet of Things in Healthcare},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1103814815},
}

@Article{Filho2022,
  author   = {Carlos Poncinelli Filho and Elias Marques and Victor Chang and Leonardo dos Santos and Flavia Bernardini and Paulo F. Pires and Luiz Ochi and Flavia C. Delicato},
  journal  = {Sensors},
  title    = {A Systematic Literature Review on Distributed Machine Learning in Edge Computing},
  year     = {2022},
  note     = {https://www.mdpi.com/1424-8220/22/7/2665/pdf?version=1648640517},
  number   = {7},
  pages    = {2665},
  volume   = {22},
  abstract = {Distributed edge intelligence is a disruptive research area that enables the execution of machine learning and deep learning (ML/DL) algorithms close to where data are generated. Since edge devices are more limited and heterogeneous than typical cloud devices, many hindrances have to be overcome to fully extract the potential benefits of such an approach (such as data-in-motion analytics). In this paper, we investigate the challenges of running ML/DL on edge devices in a distributed way, paying special attention to how techniques are adapted or designed to execute on these restricted devices. The techniques under discussion pervade the processes of caching, training, inference, and offloading on edge devices. We also explore the benefits and drawbacks of these strategies.},
  doi      = {10.3390/s22072665},
  groups   = {AI on Edge Devices},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1146747333},
}

@Article{Teixeira2019,
  author   = {Eldânae Nogueira Teixeira and Fellipe Araújo Aleixo and Francisco Dione de Sousa Amâncio and Edson OliveiraJr and Uirá Kulesza and Cláudia Werner},
  journal  = {Information and Software Technology},
  title    = {Software process line as an approach to support software process reuse: A systematic literature review},
  year     = {2019},
  pages    = {106175},
  volume   = {116},
  abstract = {Context Software Process Line (SPrL) aims at providing a systematic reuse technique to support reuse experiences and knowledge in the definition of software processes for new projects thus contributing to reduce effort and costs and to achieve improvements in quality. Although the research body in SPrL is expanding, it is still an immature area with results offering an overall view scattered with no consensus. Objective The goal of this work is to identify existing approaches for developing, using, managing and visualizing the evolution of SPrLs and to characterize their support, especially during the development of reusable process family artefacts, including an overview of existing SPrL supporting tools in their multiple stages; to analyse variability management and component-based aspects in SPrL; and, finally, to list practical examples and conducted evaluations. This research aims at reaching a broader and more consistent view of the research area and to provide perspectives and gaps for future research. Method We performed a systematic literature review according to well-established guidelines set. We used tools to partially support the process, which relies on a six-member research team. Results We report on 49 primary studies that deal mostly with conceptual or theoretical proposals and the domain engineering stage. Years 2014, 2015, and 2018 yielded the largest number of articles. This can indicate SPrL as a recent research theme and one that attracts ever-increasing interest. Conclusion Although this research area is growing, there is still a lack of practical experiences and approaches for actual applications or project-specific process derivations and decision-making support. The concept of an integrated reuse infrastructure is less discussed and explored; and the development of integrated tools to support all reuse stages is not fully addressed. Other topics for future research are discussed throughout the paper with gaps pointed as opportunities for improvements in the area.},
  doi      = {10.1016/j.infsof.2019.08.007},
  groups   = {Software Process Line},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1120284271},
}

@InProceedings{2010,
  booktitle = {Proceedings of the 12th International Conference on Enterprise Information Systems},
  title     = {A MODEL-DRIVEN APPROACH TO MANAGING AND CUSTOMIZING SOFTWARE PROCESS VARIABILITIES},
  year      = {2010},
  note      = {https://doi.org/10.5220/0002910300920100},
  pages     = {92-100},
  doi       = {10.5220/0002910300920100},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1099357022},
}

@InProceedings{MartinezRuiz2009,
  author    = {Tomás Martínez-Ruiz and Félix García and Mario Piattini},
  booktitle = {Proceedings of the 11th International Conference on Enterprise Information},
  title     = {PROCESS INSTITUTIONALIZATION USING SOFTWARE PROCESS LINES},
  year      = {2009},
  note      = {https://doi.org/10.5220/0001985803590362},
  pages     = {359-362},
  doi       = {10.5220/0001985803590362},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1099334650},
}

@Article{Armbrust2009,
  author   = {Ove Armbrust and Masafumi Katahira and Yuko Miyamoto and Jürgen Münch and Haruka Nakao and Alexis Ocampo},
  journal  = {Software Process Improvement and Practice},
  title    = {Scoping software process lines},
  year     = {2009},
  number   = {3},
  pages    = {181-197},
  volume   = {14},
  abstract = {Abstract Defining organization‐specific process standards by integrating, harmonizing, and standardizing heterogeneous and often implicit processes is an important task, especially for large development organizations. On the one hand, such a standard must be generic enough to cover all of the organization's development activities; on the other hand, it must be as detailed and precise as possible to support employees' daily work. Today, organizations typically maintain and advance a plethora of individual processes, each addressing specific problems. This requires enormous effort, which could be spent more efficiently. This article introduces an approach for developing a Software Process Line that, similar to a Software Product Line, promises to reduce the complexity and thus, the effort required for managing the processes of a software organization. We propose Scoping, Modeling, and Architecting the Software Process Line as major steps, and describe in detail the Scoping approach we recommend, based on an analysis of the potential products to be produced in the future, the projects expected in the future, and the respective process capabilities needed. In addition, the article sketches experience from determining the scope of space process standards for satellite software development. Finally, it discusses the approach, draws conclusions, and gives an outlook on future work. Copyright © 2009 John Wiley & Sons, Ltd.},
  doi      = {10.1002/spip.412},
  groups   = {Software Process Line},
  url      = {https://app.dimensions.ai/details/publication/pub.1049196757},
}

@InBook{Thraenert2007,
  author    = {Maik Thränert and Andrej Werner},
  pages     = {309-313},
  title     = {A Process Family Approach for the reuse of development processes},
  year      = {2007},
  abstract  = {Development processes are often newly defined for every project. However, a reuse of the process knowledge between different projects rarely takes place. In this paper, we present a concept which permits a general reuse of process knowledge on the basis of the process family approach and as well the project individual customization of processes according to a mass customization.},
  booktitle = {Innovations and Advanced Techniques in Computer and Information Sciences and Engineering},
  doi       = {10.1007/978-1-4020-6268-1_55},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1030802993},
}

@InBook{Simidchieva2007,
  author    = {Borislava I. Simidchieva and Lori A. Clarke and Leon J. Osterweil},
  pages     = {109-120},
  title     = {Representing Process Variation with a Process Family},
  year      = {2007},
  abstract  = {The formalization of process definitions has been an invaluable aid in many domains. However, noticeable variations in processes start to emerge as precise details are added to process definitions. While each such variation gives rise to a different process, these processes might more usefully be considered as variants of each other, rather than completely different processes. This paper proposes that it is beneficial to regard such an appropriately close set of process variants as a process family. The paper suggests a characterization of what might comprise a process family and introduces a formal approach to defining families based upon this characterization. To illustrate this approach, we describe a case study that demonstrates the different variations we observed in processes that define how dispute resolution is performed at the U.S. National Mediation Board. We demonstrate how our approach supports the definition of this set of process variants as a process family.},
  booktitle = {Software Process Dynamics and Agility},
  doi       = {10.1007/978-3-540-72426-1_10},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1047709746},
}

@InProceedings{Washizaki2006,
  author    = {Hironori Washizaki},
  booktitle = {2006 IEEE International Conference on Industrial Informatics},
  title     = {Deriving Project-Specific Processes from Process Line Architecture with Commonality and Variability},
  year      = {2006},
  pages     = {1301-1306},
  abstract  = {Definition and utilization of project-specific processes is important for effectively conducting industrial information system development. Process tailoring is an approach of achieving processes optimized for the characteristics of a project. However, conventional tailoring techniques such as component-based ones and generator ones lack a way to address the overall consistency or reuse process fragments. In this paper, we propose a technique for establishing process lines, which are sets of common processes in particular problem domains, and process line architectures that incorporate commonality and variability. Process line architectures are used as a basis for deriving process lines from the perspective of overall optimization. The proposed technique includes some extensions to the Software Process Engineering Metamodel for clearly expressing the commonality and variability in the process workflows when modeling the workflows as UML activity diagrams. As a result of applying the proposed technique to hardware/software co-design processes in an embedded system development domain, it is found that the proposed technique is useful for defining consistent and project-specific processes efficiently.},
  doi       = {10.1109/indin.2006.275847},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1093698709},
}

@InBook{Washizaki2006a,
  author    = {Hironori Washizaki},
  pages     = {415-421},
  title     = {Building Software Process Line Architectures from Bottom Up},
  year      = {2006},
  abstract  = {In this paper, we propose a technique for establishing process lines, which are sets of common processes in particular problem domains, and process line architectures that incorporate commonality and variability. Process line architectures are used as a basis for deriving process lines from the perspective of overall optimization. The proposed technique includes some extensions to the Software Process Engineering Metamodel for clearly expressing the commonality and variability in the process workflows described as UML activity diagrams. As a result of applying the proposed technique to hardware/software co-design processes in an embedded system domain, it is found that the proposed technique is useful for defining consistent and project-specific processes efficiently.},
  booktitle = {Product-Focused Software Process Improvement},
  doi       = {10.1007/11767718_37},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1008187633},
}

@InBook{Rombach2006,
  author    = {Dieter Rombach},
  pages     = {83-90},
  title     = {Integrated Software Process and Product Lines},
  year      = {2006},
  abstract  = {Increasing demands imposed on software-intensive systems will require more rigorous engineering and management of software artifacts and processes. Software product line engineering allows for the effective reuse of software artifacts based on the pro-active organization of similar artifacts according to similarities and variances. Software processes – although also variable across projects – are still not managed in a similar systematic way. This paper motivates the need for Software Process Lines similar to Product Lines. As a result of such organization, processes within an organization could be organized according to similarities and differences, allowing for better tailoring to specific project needs (corresponds to application engineering in product lines). The vision of SPPL (integrated product and process line) engineering is presented, where suitable artifacts and processes can be chosen based on a set of product & process requirements and project constraints. The paper concludes with some resulting challenges for research, practice, and teaching.},
  booktitle = {Unifying the Software Process Spectrum},
  doi       = {10.1007/11608035_9},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1050026341},
}

@InBook{Jaufman2005,
  author    = {Olga Jaufman and Jürgen Münch},
  pages     = {328-342},
  title     = {Acquisition of a Project-Specific Process},
  year      = {2005},
  abstract  = {Currently, proposed development processes are often considered too generic for operational use. This often leads to a misunderstanding of the project-specific processes and its refuse. One reason for non-appropriate project-specific processes is insufficient support for the tailoring of generic processes to project characteristics and context constraints. To tackle this problem, we propose a method for the acquisition of a project-specific process. This method uses a domain-specific process line for top-down process tailoring and supports bottom-up refinement of the defined generic process based on tracking process activities. The expected advantage of the method is tailoring efficiency gained by usage of a process line and higher process adherence gained by bottom-up adaptation of the process. The work described was conducted in the automotive domain. This article presents an overview of the so-called Emergent Process Acquisition method (EPAc) and sketches an initial validation study.},
  booktitle = {Product Focused Software Process Improvement},
  doi       = {10.1007/11497455_27},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1007551255},
}

@InBook{Duran2004,
  author    = {Amador Durán and David Benavides and Jesus Bermejo},
  pages     = {140-151},
  title     = {Applying System Families Concepts to Requirements Engineering Process Definition},
  year      = {2004},
  abstract  = {In this paper, some experiences gained during the definition of a unified, common software development process for several companies in Telvent are presented. Last year, Telvent made the decision of developing a unique software development process which was flexible enough to be adapted to specific practices and needs of the different companies. In this paper we focus mainly on the experiences gained during the definition of the requirements engineering process, al-though many of them are also applicable to other software development processes. One of the most interesting experiences from our point of view is that, al-though the definition process was started using a top-down approach and well-know techniques like data flow diagrams, we eventually end up applying requirements engineering techniques like glossaries, scenarios or conflict resolution for the definition of the requirements engineering process itself. On the other hand, the need of having adaptable processes for the different companies in Telvent made us adopt a process family approach, i.e. adopting an approach similar to the system families development, thus defining a core process that could be adapted to specific needs of specific companies in a predefined, controlled manner. The experiences gained in the definition of the process family were applied to the definition of requirements engineering process for product line development, which is briefly presented in this paper.},
  booktitle = {Software Product-Family Engineering},
  doi       = {10.1007/978-3-540-24667-1_11},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1044176397},
}

@InProceedings{Ternite2009,
  author    = {Thomas Ternité},
  booktitle = {2009 35th Euromicro Conference on Software Engineering and Advanced Applications},
  title     = {Process lines: a product line approach designed for process model development},
  year      = {2009},
  pages     = {173-180},
  abstract  = {Standard process models like RUP or the German V-Modell XT are developed to be used in an organizational environment. Yet introducing such a process model into an organization is no simple task. It is usually accompanied by either reorganizing internal processes to comply with a standard process model or by adaptation of the process model to organizational structures. The latter is an approach that can often be found in practice and is subject to this paper. The paper describes a generalization of the process model architecture of the V-Modell XT 1.3. Compared to the previous version 1.2.1.1, its metamodel has completely been revised with respect to a new adaptation concept. We will see how this concept allows the implementation of a so called process line. Software process lines are derived from software product lines and can be used for the adaptation of standard processes during process model development, project initialization and project runtime.},
  doi       = {10.1109/seaa.2009.48},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1095285212},
}

@InProceedings{Aleixo2010,
  author    = {Fellipe Araüjo Aleixo and Marflia Aranha Freire and Wanderson Câmara dos Santos and Uirâ Kulesza},
  booktitle = {2010 Brazilian Symposium on Software Engineering},
  title     = {An Approach to Manage and Customize Variability in Software Processes},
  year      = {2010},
  pages     = {118-127},
  abstract  = {Resumo — Este artigo apresenta uma abordagem para gerencia e customizaçâo de variabilidades em processos de software. A abordagem oferece suporte para a manipulaçâo automâtica de variaçöes ocorrendo em especificaçöes de processos, e promove a derivaçâo automâtica de customizaçöes especıficas de tais processos. De forma a validar e demonstrar os benefıcios da abordagem, o artigo apresenta uma implementaçâo da abordagem que permite a customizaçâo de processos especificados no Eclipse Processo Framework (EPF).},
  doi       = {10.1109/sbes.2010.18},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1095507755},
}

@InProceedings{Barreto2010,
  author    = {Ahilton Barreto and Elaine Nunes and Ana Regina Rocha and Leonardo Murta},
  booktitle = {2010 Seventh International Conference on the Quality of Information and Communications Technology},
  title     = {Supporting the Definition of Software Processes at Consulting Organizations via Software Process Lines},
  year      = {2010},
  pages     = {15-24},
  abstract  = {Software Process Consulting Organizations (SPCOs) support other organizations to define, deploy, and improve their software processes. Usually they are requested to define similar processes to different organizations, leading to some process reuse opportunities. Software Process Lines (SPLs) are reusable process architectures that model similarities and variabilities among processes, and could be of great value for SPCOs to derive multiple processes based on the same original SPL. This paper presents a reuse-based software process definition approach that aims at making it easier for consulting organizations to define reusable processes. We describe the main concepts related to a SPL, and how to define and use them in this context. We also present a supporting tool to define SPLs and derive defined processes. Finally, we present a usage experience of the approach in the context of a SPCO in Brazil.},
  doi       = {10.1109/quatic.2010.19},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1094407996},
}

@InProceedings{Alegria2011,
  author    = {Julio A. Hurtado Alegría and María Cecilia Bastarrica and Alcides Quispe and Sergio F. Ochoa},
  booktitle = {Proceedings of the 2011 International Conference on Software and Systems Process},
  title     = {An MDE approach to software process tailoring},
  year      = {2011},
  pages     = {43-52},
  abstract  = {Defining organizational processes is essential for enhancing maturity. However the best process depends on the particularities of each project. Typically a process engineer defines a specific process for each project in an ad-hoc fashion, which is expensive, unrepeatable and error prone. Trying to deal with this challenge we propose a model-based approach to software process tailoring that generates project specific processes based on the organizational process and the project context. The approach is systematic, repeatable and it does not depend on the people using it. The proposal has been applied for tailoring the Requirements Engineering process of a medium size company. The obtained results were validated by process engineers of the company. Processes obtained using the proposed approach matched the ones used in the company for planned contexts and also they were reasonable for non-expected situations.},
  doi       = {10.1145/1987875.1987885},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1002692059},
}

@InProceedings{MartinezRuiz2011,
  author    = {Tomás Martínez-Ruiz and Félix García and Mario Piattini and Jürgen Münch},
  booktitle = {2011 37th EUROMICRO Conference on Software Engineering and Advanced Applications},
  title     = {Applying AOSE Concepts to Model Crosscutting Variability in Variant-Rich Processes},
  year      = {2011},
  note      = {https://arxiv.org/pdf/1312.0356},
  pages     = {334-338},
  abstract  = {Software process models need to be variant-rich, in the sense that they should be systematically customizable to specific project goals and project environments. It is currently very difficult to model Variant-Rich Process (VRP) because variability mechanisms are largely missing in modern process modeling languages. Variability mechanisms from other domains, such as programming languages, might be suitable for the representation of variability and could be adapted to the modeling of software processes. Mechanisms from Software Product Line Engineering (SPLE) and concepts from Aspect-Oriented Software Engineering (AOSE) show particular promise when modeling variability. This paper presents an approach that integrates variability concepts from SPLE and AOSE in the design of a VRP approach for the systematic support of tailoring in software processes. This approach has also been implemented in SPEM, resulting in the vSPEM notation. It has been used in a pilot application, which indicates that our approach based on AOSE can make process tailoring easier and more productive.},
  doi       = {10.1109/seaa.2011.58},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1094372965},
}

@Article{MartinezRuiz2011a,
  author  = {T. Martı́nez-Ruiz and F. Garcı́a and M. Piattini and J. Münch},
  journal = {IET Software},
  title   = {Modelling software process variability: an empirical study},
  year    = {2011},
  number  = {2},
  pages   = {172},
  volume  = {5},
  doi     = {10.1049/iet-sen.2010.0020},
  groups  = {Software Process Line},
  url     = {https://app.dimensions.ai/details/publication/pub.1056837072},
}

@InProceedings{Jafarinezhad2012,
  author    = {Omid Jafarinezhad and Raman Ramsin},
  booktitle = {2012 IEEE 36th Annual Computer Software and Applications Conference},
  title     = {Development of Situational Requirements Engineering Processes: A Process Factory Approach},
  year      = {2012},
  pages     = {279-288},
  abstract  = {The Software Product Line (SPL) approach is a paradigm for systematic reuse of software products, and a Software Factory is a SPL aimed at the industrialization of software development. Based on the notion that a software/RE process can be developed via an engineering process (much akin to engineering other types of software), this research aims to provide a feature-based RE process factory to develop RE processes based on the characteristics of the project at hand (project situation). In our approach, the project situation is modeled as the problem domain through using the ${\bf i}^{\ast}$ modeling language (resulting in a situation model). A feature model can encapsulate all the features in an SPL; therefore, the abundant riches of the RE field – results of decades of research – have been explored for extracting the variations and commonalities among existing RE processes, the results of which are represented in the form of a feature model, considered as a model of the solution domain. In order to demonstrate the validity of the proposed feature model, it has been compared against RE-related activities found in prominent software development methodologies. A mapping for translating the situation model to the RE process feature model is proposed with the specific aim of promoting traceability and rationality in the selection of RE process features. The efficacy of the approach is demonstrated through an RE process development example.},
  doi       = {10.1109/compsac.2012.39},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1095083693},
}

@InProceedings{Rouille2012,
  author    = {Emmanuelle Rouille and Benoit Combemale and Olivier Barais and David Touzet and Jean-Marc Jezequel},
  booktitle = {2012 19th Asia-Pacific Software Engineering Conference},
  title     = {Leveraging CVL to Manage Variability in Software Process Lines},
  year      = {2012},
  note      = {https://hal.inria.fr/hal-00735881/file/APSEC2012_methodo_spem_cvl.pdf},
  pages     = {148-157},
  abstract  = {Variability on project requirements often implies variability on software processes. To manage such variability, Software Process Lines (SPLs) can be used to represent commonality (i.e., common practices) and variability (i.e., differences) of a set of related software processes. To this end, some Software Process Modeling Languages (SPMLs) natively integrate variability mechanisms. Nevertheless, such a coupling between the SPML and the variability mechanisms i) requires to interpret the requirements variability in terms of the processes variability, ii) limits the reuse of the requirements variability for other purposes (e.g., the development itself), and iii) is a barrier to the use of advances from the field of variability management. In this paper, we propose an approach to apply the Common Variability Language (CVL from the OMG consortium) for requirement variability modeling and its binding to the processes. This work is illustrated on a family of industrial Java development processes. Our approach enables the definition of an SPL and the automatic derivation of a process from this SPL according to the requirements of a given project. The variability is managed separately from the process model and benefits from existing tools coming from the process modeling community and CVL.},
  doi       = {10.1109/apsec.2012.82},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1093832512},
}

@Article{Hurtado2013,
  author   = {Julio Ariel Hurtado and María Cecilia Bastarrica and Sergio F. Ochoa and Jocelyn Simmonds},
  journal  = {Journal of Systems and Software},
  title    = {MDE software process lines in small companies},
  year     = {2013},
  number   = {5},
  pages    = {1153-1171},
  volume   = {86},
  abstract = {Software organizations specify their software processes so that process knowledge can be systematically reused across projects. However, different projects may require different processes. Defining a separate process for each potential project context is expensive and error-prone, since these processes must simultaneously evolve in a consistent manner. Moreover, an organization cannot envision all possible project contexts in advance because several variables may be involved, and these may also be combined in different ways. This problem is even worse in small companies since they usually cannot afford to define more than one process. Software process lines are a specific type of software product lines, in the software process domain. A benefit of software process lines is that they allow software process customization with respect to a context. In this article we propose a model-driven approach for software process lines specification and configuration. The article also presents two industrial case studies carried out at two small Chilean software development companies. Both companies have benefited from applying our approach to their processes: new projects are now developed using custom processes, process knowledge is systematically reused, and the total time required to customize a process is much shorter than before.},
  doi      = {10.1016/j.jss.2012.09.033},
  groups   = {Software Process Line},
  url      = {https://app.dimensions.ai/details/publication/pub.1021871976},
}

@InProceedings{Simmonds2013,
  author    = {Jocelyn Simmonds and María Cecilia Bastarrica and Luis Silvestre and Alcides Quispe},
  booktitle = {2013 4th International Workshop on Product LinE Approaches in Software Engineering (PLEASE)},
  title     = {Variability in Software Process Models: Requirements for Adoption in Industrial Settings},
  year      = {2013},
  pages     = {33-36},
  abstract  = {It is an increasing trend to apply Software Product Line (SPL) concepts and techniques for software process tailoring, generating a Software PRocess Line (SPrL). However, there are several aspects that must be addressed before SPrLs can be fully adopted by industry, a key aspect being how software process variability is specified and managed. In the literature, there are several general-purpose as well as domain-specific proposals for specifying process variability. In this paper, we analyze the benefits and drawbacks of two general-purpose (feature models and OVM) and two domain-specific (SPEM variability primitives and vSPEM) approaches, as well as discuss what hinders industry adoption in each case.},
  doi       = {10.1109/please.2013.6608661},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1093480635},
}

@InProceedings{Bastarrica2014,
  author    = {María Cecilia Bastarrica and Jocelyn Simmonds and Luis Silvestre},
  booktitle = {Proceedings of the 6th International Workshop on Modeling in Software Engineering},
  title     = {Using megamodeling to improve industrial adoption of complex MDE solutions},
  year      = {2014},
  pages     = {31-36},
  abstract  = {Companies formalize their software processes as a way of organizing their development projects. As each project has its own requirements and objectives, a family of processes is required in practice, in order to ensure that each project is handled appropriately. This family may be a collection of predefined processes, but can also be automatically generated by tailoring a general process to a project’s context which requires formalization and tool support to be successful. Model-driven engineering provides a formal framework for defining the models and transformations required for automated process tailoring, but various types of models must be specified and evolved, limiting the industrial adoption of this approach. To address this problem, in this paper we propose a megamodel for automated process tailoring. Megamodeling provides an integrating framework for modeling in the large, including the definition and evolution of all models and transformations required for tailoring while hiding complexity. We report the application of our approach to the software development process of Rhiscom, a small Chilean company.},
  doi       = {10.1145/2593770.2593773},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1033018638},
}

@InProceedings{Carvalho2014,
  author    = {Daniel Dias de Carvalho and Larissa Fernandes Chagas and Carla Alessandra Lima Reis},
  booktitle = {2014 XL Latin American Computing Conference (CLEI)},
  title     = {Definition of Software Process Lines for Integration of Scrum and CMMI},
  year      = {2014},
  pages     = {1-12},
  abstract  = {The adoption of Agile Methods, like Scrum, in conjunction with maturity models, like Capability Maturity Model Integration (CMMI), often leads to variation of process. Process definition is a complex and difficult task, requiring a lot of effort and experience. Software Process Line (SPrL) has been considered a suitable paradigm for software process tailoring and reuse; and there is a lack of approaches that use SPrL concepts for modeling these approaches in conjunction. This paper presents the definition of a SPrL considering Project Planning and Project Monitoring and Control process areas in processes using Scrum agile methodology together with CMMI maturity model. It is expected the presented SPrL assists software organizations to tailor processes that meet both approaches.},
  doi       = {10.1109/clei.2014.6965156},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1094703784},
}

@InProceedings{Kuhrmann2014,
  author    = {Marco Kuhrmann and Daniel Méndez Fernández and Thomas Ternité},
  booktitle = {Proceedings of the 2014 International Conference on Software and System Process},
  title     = {Realizing software process lines: insights and experiences},
  year      = {2014},
  pages     = {99-108},
  abstract  = {Software process lines provide a systematic approach to construct and manage software processes. A process line defines a reference process containing general process assets, whereas a well-defined customization approach allows process engineers to create new process variants by, e.g., extending or altering process assets. Variability operations are a powerful instrument to realize a process line. However, little is known about which variability operations are suitable in practice. In this paper, we present a study on the feasibility of variability operations to support process lines in the context of the German V-Modell XT. We analyze which variability operations were defined and used to which extent, and we provide a catalog of variability operations as an improvement proposal for other process models. Our findings show 69 variability operations defined across several metamodel versions of which 25 remain unused. Furthermore, we also find that variability operations can help process engineers to compensate process metamodel evolution.},
  doi       = {10.1145/2600821.2600833},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1043851254},
}

@Article{Lorenz2014,
  author   = {Wagner Gadêa Lorenz and Miguel Bauermann Brasil and Lisandra Manzoni Fontoura and Guilherme Vaz Pereira},
  journal  = {International Journal of Software Engineering and Knowledge Engineering},
  title    = {Activity-Based Software Process Lines Tailoring},
  year     = {2014},
  number   = {09},
  pages    = {1357-1381},
  volume   = {24},
  abstract = {Software process definition requires choosing the process elements that appropriately fulfil the tailoring requirements, such as to prevent risks or to satisfy quality goals. The selection of appropriate process elements is usually done manually, making this process complex, time-consuming and error-prone. Our main objective is to define a systematic approach to tailor software process and a support tool to simplify and to support the tailoring process by improving the selection process of reusable process elements. We developed a systematic approach to tailor software process based on software process architectures and lines. This approach selects the process elements that appropriately match the tailoring requirements. A web tool was developed to support the use of the proposed approach. We concluded that the approach aids process engineer to make decisions for selecting a set of process elements suitable to the tailoring requirements and to the project context.},
  doi      = {10.1142/s0218194014500429},
  groups   = {Software Process Line},
  url      = {https://app.dimensions.ai/details/publication/pub.1062959772},
}

@InProceedings{Blum2015,
  author    = {Fabian Rojas Blum and Jocelyn Simmonds and María Cecilia Bastarrica},
  booktitle = {Proceedings of the 2015 International Conference on Software and System Process},
  title     = {Software process line discovery},
  year      = {2015},
  pages     = {127-136},
  abstract  = {Companies define software processes for planning and guiding projects. Since process definition is expensive, and in practice, no one process "fits all" projects, the current trend is to define a Software Process Line (SPrL): a base process that represents the common process elements, along with its potential variability. Specifying a SPrL is more expensive than just specifying one process, but the SPrL can be adapted to specific project contexts, minimizing the amount of extra work carried out by employees. Mining project logs has proven to be a promising approach for discovering the process that is applied in practice. However, considering all the possible variations that may be logged, the mined process may be overly complex. Some algorithms deal with this by filtering infrequent relations between log events, but they may discard relevant relations. In this paper we propose the v-algorithm that uses two thresholds to set up a SPrL: highly frequent relations are used to build the base process, variable relations define process variability, and rare relations are discarded as noise. We applied the $v$-$algorithm$ to the project log of Mobius, a small Chilean software company. We obtained a SPrL where we identified unexpected alternative ways of performing certain activities, as well as an optional activity that was originally specified as mandatory.},
  doi       = {10.1145/2785592.2785605},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1036866528},
}

@InProceedings{Brondani2015,
  author    = {Camila Brondani and Gelson Bertuol and Lisandra Fontoura},
  booktitle = {Proceedings of the 27th International Conference on Software Engineering and Knowledge Engineering},
  title     = {Quality Evaluation of Artifacts in Tailored Software Process Lines},
  year      = {2015},
  note      = {https://doi.org/10.18293/seke2015-052},
  pages     = {223-226},
  doi       = {10.18293/seke2015-052},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1099109582},
}

@InProceedings{Garcia2015,
  author    = {Cleiton Garcia and Marco Paludo and Andreia Malucelli and Sheila Reinehr},
  booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
  title     = {A software process line for service-oriented applications},
  year      = {2015},
  pages     = {1680-1687},
  abstract  = {The management of processes and systems is a complex and time-consuming activity for organizations and also an ongoing Information Technology (IT) challenge. Among the different approaches for bringing flexibility to the business processes and systems are Service-Oriented Architecture (SOA) and Business Process Management (BPM). The SOA approach has become popular providing services and interfaces, enabling integration of heterogeneous and distributed platforms and BPM leverages the cycles of improvements, control and evaluation of business processes. BPM and SOA should work together aiming at improving business processes and evolving systems architecture. One main problem to apply BPM and SOA is the lack of established processes and this work proposes a software process line in order to simplify variability control and enable the instantiation of new development process applying BPM and SOA. It also aims at developing an environment to support the proposed software process line in order to automate the process, integrating industrial tools with one specifically developed to perform the transformation of UMA models into BPM notation. The main contribution of this work is the definition of the software process line for engineering service-oriented products. It is highly relevant to software industry since software process lines lacks experiments, practices and tools.},
  doi       = {10.1145/2695664.2695743},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1050251793},
}

@Article{Magdaleno2015,
  author   = {Andréa Magalhães Magdaleno and Marcio de Oliveira Barros and Cláudia Maria Lima Werner and Renata Mendes de Araujo and Carlos Freud Alves Batista},
  journal  = {Journal of Systems and Software},
  title    = {Collaboration optimization in software process composition},
  year     = {2015},
  pages    = {452-466},
  volume   = {103},
  abstract = {Purpose: The purpose of this paper is to describe an optimization approach to maximize collaboration in software process composition. The research question is: how to compose a process for a specific software development project context aiming to maximize collaboration among team members? The optimization approach uses heuristic search algorithms to navigate the solution space and look for acceptable solutions.Design/methodology/approach: The process composition approach was evaluated through an experimental study conducted in the context of a large oil company in Brazil. The objective was to evaluate the feasibility of composing processes for three software development projects. We have also compared genetic algorithm (GA) and hill climbing (HC) algorithms driving the optimization with a simple random search (RS) in order to determine which would be more effective in addressing the problem. In addition, human specialist point-of-view was explored to verify if the composed processes were in accordance with his/her expectations regarding size, complexity, diversity, and reasonable sequence of components.Findings: The main findings indicate that GA is more effective (best results regarding the fitness function) than HC and RS in the search of solutions for collaboration optimization in software process composition for large instances. However, all algorithms are competitive for small instances and even brute force can be a feasible alternative in such a context. These SBSE results were complemented by the feedback given by specialist, indicating his satisfaction with the correctness, diversity, adherence to the project context, and support to the project manager during the decision making in process composition.Research limitations: This work was evaluated in the context of a single company and used only three project instances. Due to confidentiality restrictions, the data describing these instances could not be disclosed to be used in other research works. The reduced size of the sample prevents generalization for other types of projects or different contexts.Implications: This research is important for practitioners who are facing challenges to handle diversity in software process definition, since it proposes an approach based on context, reuse and process composition. It also contributes to research on collaboration by presenting a collaboration management solution (COMPOOTIM) that includes both an approach to introduce collaboration in organizations through software processes and a collaboration measurement strategy. From the standpoint of software engineering looking for collaborative solutions in distributed software development, free/open source software, agile, and ecosystems initiatives, the results also indicate how to increase collaboration in software development.Originality/value: This work proposes a systematic strategy to manage collaboration in software development process composition. Moreover, it brings together a mix of computer-oriented and human-oriented studies on the search-based software engineering (SBSE) research area. Finally, this work expands the body of knowledge in SBSE to the field of software process which has not been properly explored by former research.},
  doi      = {10.1016/j.jss.2014.11.036},
  groups   = {Software Process Line},
  url      = {https://app.dimensions.ai/details/publication/pub.1000719345},
}

@InProceedings{Schramm2015,
  author    = {Joachim Schramm and Patrick Dohrmann and Marco Kuhrmann},
  booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
  title     = {Development of flexible software process lines with variability operations},
  year      = {2015},
  note      = {https://findresearcher.sdu.dk/ws/files/115257535/web_vm_lines2.pdf},
  pages     = {1-10},
  abstract  = {Context: Software processes evolve over time and several approaches were proposed to support the required flexibility. Yet, little is known whether these approaches sufficiently support the development of large software processes. A software process line helps to systematically develop and manage families of processes and, as part of this, variability operations provide means to modify and reuse pre-defined process assets. Objective: Our goal is to evaluate the feasibility of variability operations to support the development of flexible software process lines. Method: We conducted a longitudinal study in which we studied 5 variants of the V-Modell XT process line for 2 years. Results: Our results show the variability operation instrument feasible in practice. We analyzed 616 operation exemplars addressing various customization scenarios, and we found 87 different operation types contributed by 3 metamodel variants developed by different teams in different contexts. Conclusions: Although variability operations are only one instrument among others, our results suggest this instrument useful to implement variability in real-life software process lines.},
  doi       = {10.1145/2745802.2745814},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1029370861},
}

@InProceedings{Simmonds2015,
  author    = {Jocelyn Simmonds and Daniel Perovich and Maria Cecilia Bastarrica and Luis Silvestre},
  booktitle = {2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)},
  title     = {A megamodel for Software Process Line modeling and evolution},
  year      = {2015},
  pages     = {406-415},
  abstract  = {Companies formalize software processes as a way of organizing development projects. Since there are differences in project contexts, a one-size-fits-all approach does not work well in practice. Some companies use a family of a predefined processes, but this approach has a high process maintenance cost. Instead, we define Software Process Lines (SPrL), where a general process with variability is tailored to project contexts. Model- Driven Engineering (MDE) provides a formal framework for defining the models and transformations required for automated SPrL tailoring. However, this approach requires the definition and co-evolution of various types of models and tool support beyond the skills of process engineers, making the industrial adoption challenging. This paper shares our experience using a megamodeling approach to the development of the back-end of our toolset. The megamodel provides a uniform mechanism for process definition, variability, tailoring and evolution, and we hide the MDE complexity through a user-friendly front-end. We report the application of our approach at Mobius, a small Chilean software enterprise.},
  doi       = {10.1109/models.2015.7338272},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1094563585},
}

@Article{Kuhrmann2015,
  author   = {Marco Kuhrmann and Daniel Méndez Fernández and Thomas Ternité},
  journal  = {Journal of Software},
  title    = {On the use of variability operations in the V‐Modell XT software process line},
  year     = {2015},
  note     = {https://arxiv.org/pdf/1702.05724.pdf},
  number   = {4},
  pages    = {241-253},
  volume   = {28},
  abstract = {Abstract Software process lines provide a systematic approach to develop and manage software processes. It defines a reference process containing general process assets, whereas a well‐defined customization approach allows process engineers to create new process variants, for example, by extending or modifying process assets. Variability operations are an instrument to realize flexibility by explicitly declaring required modifications, which are applied to create a procedurally generated company‐specific process. However, little is known about which variability operations are suitable in practice. In this article, we present a study on the feasibility of variability operations to support the development of software process lines in the context of the V‐Modell XT. We analyze which variability operations are defined and practically used. We provide an initial catalog of variability operations as an improvement proposal for other process models. Our findings show that 69 variability operation types are defined across several metamodel versions of which, however, 25 remain unused. The found variability operations allow for systematically modifying the content of process model elements and the process documentation, and they allow for altering the structure of a process model and its description. Furthermore, we also find that variability operations can help process engineers to compensate process metamodel evolution. Copyright © 2015 John Wiley & Sons, Ltd.},
  doi      = {10.1002/smr.1751},
  groups   = {Software Process Line},
  url      = {https://app.dimensions.ai/details/publication/pub.1038922413},
}

@InProceedings{Costa2018,
  author    = {Diogo M. Costa and Eldânae N. Teixeira and Claudia M. L. Werner},
  booktitle = {Proceedings of the XVII Brazilian Symposium on Software Quality},
  title     = {Odyssey-ProcessCase},
  year      = {2018},
  pages     = {170-179},
  abstract  = {Software processes have been the focus of discussion in the literature, but defining a software process that meets project-specific needs remains a challenge. The Software Process Line (SPrL) technique offers a systematic to identify processes' similarities and variability to support software process reuse. Based on a literature review analysis, a concentration of SPrL approaches that use mapping/rules techniques to support project-specific software process definition was observed. However, the knowledge acquisition process required by this kind of technique is not trivial, due to the unavailability of experts and overhead in domain engineering. This paper presents an incremental learning approach for SPrL, called Odyssey-ProcessCase, focused on the decision-making support to solve SPrL variability during the project-specific software process definition. The approach applies techniques such as Case-Based Reasoning (CBR) and Rule-Based System to offer complementary mechanisms to support the decision-making task aiming at the software process definition from reusable artifacts of SPrL.},
  doi       = {10.1145/3275245.3275263},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1107809783},
}

@InProceedings{Nesi2018,
  author    = {Paolo Nesi and Mchela Paolucci},
  booktitle = {Proceedings of the 24th International DMS Conference on Visualization and Visual Languages},
  title     = {Supporting Living Lab with Life Cycle and Tools for Smart City Environments (S)},
  year      = {2018},
  note      = {https://doi.org/10.18293/seke2018-019},
  pages     = {36-43},
  doi       = {10.18293/seke2018-019},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1107974935},
}

@InBook{Gallina2018,
  author    = {Barbara Gallina and Shankar Iyer},
  pages     = {469-479},
  title     = {Towards Quantitative Evaluation of Reuse Within Safety-Oriented Process Lines},
  year      = {2018},
  abstract  = {Recently, Safety-oriented Process Line Engineering (SoPLE) has been proposed as a sound solution to systematize reuse in the context of safety-oriented processes described within safety-related standards. Currently, however, no metrics have been used to measure the actual gain in terms of reuse that the application of this engineering method entails. To overcome this lack of quantitative evidence, we adopt the GQM$$^{+}$$ Strategies model, an extension of the Goal/Question/Metric (GQM) paradigm, for measurements. After having defined our specific measurement goals, we build on top of existing metrics, defined for measuring product-related reuse, and we translate them in our semantic space to evaluate our goals. We then apply our GQM$$^{+}$$ Strategies model on a ECSS-compliant SoPL to illustrate and assess its usefulness.},
  booktitle = {Systems, Software and Services Process Improvement},
  doi       = {10.1007/978-3-319-97925-0_40},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1106070091},
}

@Article{Pazin2018,
  author   = {Maicon G. Pazin and Ana P. Allian and Edson OliveiraJr},
  journal  = {IET Software},
  title    = {Empirical study on software process variability modelling with SMartySPEM and vSPEM},
  year     = {2018},
  number   = {6},
  pages    = {536-546},
  volume   = {12},
  abstract = {With the continuous improvement of software processes, it is possible to increase quality, to address different application domains and accelerate the development of software products. However, existing process modelling notations like the SPEM meta‐model typically do not have appropriate constructs for expressing process variability. Thus, SPEM‐based approaches such as SMartySPEM and vSPEM provide mechanisms for representing variabilities to address characteristics of different projects. Here, the authors empirically compared SMartySPEM and for variability representation in software processes models, aiming to analyze correction, time, and efficiency. The performed experiment provides preliminary evidence based on 11 participants and sample size N = 44. Participants took longer to comprehend diagrams in SMartySPEM. However, the correctness of SMartySPEM diagrams had a superior result and higher efficiency based on the median values and hypothesis tests. With regard to variability mechanisms, the diagrams modelled with SMartySPEM had a slightly lower efficiency and it took longer for modifying them, thus the correctness of the SMartySPEM diagrams had a superior result. Therefore, an initial body of knowledge indicated a positive efficiency of SMartySPEM for variability representation in process models, as well as potential improvements as the reduction in SMartySPEM diagrams complexity, which may contribute to its evolution.},
  doi      = {10.1049/iet-sen.2017.0061},
  groups   = {Software Process Line},
  url      = {https://app.dimensions.ai/details/publication/pub.1104018773},
}

@InProceedings{Ruiz2018,
  author    = {Pablo H. Ruiz and Cecilia Camacho and Julio A. Hurtado},
  booktitle = {2018 ICAI Workshops (ICAIW)},
  title     = {A Comparative Study for Scoping a Software Process Line},
  year      = {2018},
  pages     = {1-6},
  abstract  = {Software process lines (SPrL) is an approach for facilitating the adaptation and evolution of a set of related software processes within a software organization. In order to correctly establish the processes and process assets to be reused as well as to typify the situations under which the processes will be used (the process demands) an implicit or explicit scoping activity that is normally performed. There are some methods oriented to support this activity. In this article we explore the definition of the scope of a SPrL of a software company focused on the software testing service through two methods: SCOPE and CASPER Scoping. The main goal of this work is to explore, through an empirical study, the advantages and disadvantages of scope determination in an industrial context by using two methods reported in the literature: SCOPE and CASPER. The developed study allows us to evidence some limitations of these methods in terms of (1) defining a suitable scope granularity level, (2) clarity to systematically conduct the methods (3) lack of an acceptable definition of what the scope of a SPrL is. We consider that it is very feasible to make a proposal that integrates these approaches so that the definition of scope in real contexts is more appropriate},
  doi       = {10.1109/icaiw.2018.8554998},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1110365318},
}

@InProceedings{Teixeira2018,
  author    = {Eldânae Nogueira Teixeira and Aline Vasconcelos and Cláudia Werner},
  booktitle = {Proceedings of the 20th International Conference on Enterprise Information Systems},
  title     = {OdysseyProcessReuse - A Component-based Software Process Line Approach},
  year      = {2018},
  note      = {https://doi.org/10.5220/0006672902310238},
  pages     = {231-238},
  doi       = {10.5220/0006672902310238},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1101815920},
}

@InProceedings{Jaufman2005a,
  author    = {Olga Jaufman},
  booktitle = {Proceedings of the 27th international conference on Software engineering - ICSE '05},
  title     = {Emergent process design},
  year      = {2005},
  pages     = {653-653},
  doi       = {10.1145/1062455.1062587},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1035088883},
}

@InProceedings{Sutton1998,
  author    = {S.M. Sutton and L.J. Osterweil},
  booktitle = {Proceedings 10th International Software Process Workshop},
  title     = {Product families and process families},
  year      = {1998},
  pages     = {109-111},
  abstract  = {Product lines or families represent an important way of organizing software products. Product families might include, for example, successive revisions of a single application, versions of an application for different host platforms, or versions with varying features, e.g., different levels of security. A software product family can thus be viewed as a collection of products that are similar in some important respects yet systematically different in others. The family viewpoint emphasizes both the commonality among family members and the differences between them, and it draws attention to their interrelationships. Our work in software processes has lead us to conclude that software processes also can be usefully viewed in terms of families. As software process engineers, we also naturally view products in terms of the processes that create them and, conversely, we view processes in terms of the products they create. This suggests that product families and process families should be closely interrelated. Consequently, both products and processes may benefit from analysis in terms of families and family relationships. We begin to sketch out such an analysis and some of the issues that it raises.},
  doi       = {10.1109/ispw.1996.654385},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1094827284},
}

@InProceedings{Hollenbach1996,
  author    = {C. Hollenbach and W. Frakes},
  booktitle = {Proceedings of Fourth IEEE International Conference on Software Reuse},
  title     = {Software process reuse in an industrial setting},
  year      = {1996},
  pages     = {22-30},
  abstract  = {The paper describes a method for creating reusable processes and the authors' experience using them in an industrial environment. A notation and process for creating and tailoring reusable processes is described and applied to the building of a 120 process library at PRC Inc. Initial data collected on use of the library indicates large potential payoffs from process reuse such as a 10 to 1 improvement in the time to develop a project specific process.},
  doi       = {10.1109/icsr.1996.496110},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1093611620},
}

@InProceedings{2014,
  booktitle = {Proceedings of the 2nd International Conference on Model-Driven Engineering and Software Development},
  title     = {A Model-based Tool for Generating Software Process Model Tailoring Transformations},
  year      = {2014},
  note      = {https://doi.org/10.5220/0004715805330540},
  pages     = {533-540},
  doi       = {10.5220/0004715805330540},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1099319497},
}

@InBook{Silvestre2015,
  author    = {Luis Silvestre and María Cecilia Bastarrica and Sergio F. Ochoa},
  pages     = {171-182},
  title     = {Reducing Complexity of Process Tailoring Transformations Generation},
  year      = {2015},
  abstract  = {Tailoring software processes to particular contexts applying model transformations has proved to be appropriate and technically feasible. However, the use of this approach can become awkward for most process engineers, because it requires knowledge about the process and its tailoring needs, and also about building model transformations. In a previous work we have proposed a tool based on model-driven engineering (MDE) for automatically generating software process model tailoring transformations. This paper presents an improved user interface of the tool and proposes a process for guiding its application for tailoring processes. We illustrate its use by applying it for tailoring the process of Rhiscom, a Chilean small software company. The tool and the process balance the formally required by MDE with the usability needed by the process engineers.},
  booktitle = {Model-Driven Engineering and Software Development},
  doi       = {10.1007/978-3-319-25156-1_11},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1085997260},
}

@InBook{Varkoi2017,
  author    = {Timo Varkoi and Timo Mäkinen and Barbara Gallina and Frank Cameron and Risto Nevalainen},
  pages     = {83-95},
  title     = {Towards Systematic Compliance Evaluation Using Safety-Oriented Process Lines and Evidence Mapping},
  year      = {2017},
  abstract  = {The role of software is growing in safety related systems. This underlines the need for software process assessment in many safety-critical domains. For example, the nuclear power industry has strict safety requirements for control systems and many methods are applied to evaluate compliance to domain specific standards and requirements. This paper discusses the needs of the nuclear domain and presents alternatives to develop a process assessment method that takes into account domain specific requirements. The aim is to provide an approach that facilitates the use of assessment findings in evaluating compliance with the domain requirements and supports other assurance needs. Safety-oriented Process Line Engineering (SoPLE) is studied as a method for mapping assessment criteria to domain specific requirements. A binary distance metric is used to evaluate, how far a process mapping based method would solve problems found in compliance evaluation. Based on the results, SoPLE is applicable in this case, but process mapping is not adequate to facilitate compliance evaluation.},
  booktitle = {Systems, Software and Services Process Improvement},
  doi       = {10.1007/978-3-319-64218-5_7},
  groups    = {Software Process Line},
  url       = {https://app.dimensions.ai/details/publication/pub.1091160240},
}

@Article{Qin2019,
  author   = {Qin, Cui and Eichelberger, Holger and Schmid, Klaus},
  journal  = {Information and Software Technology},
  title    = {Enactment of adaptation in data stream processing with latency implications—A systematic literature review},
  year     = {2019},
  pages    = {1-21},
  volume   = {111},
  abstract = {Context Stream processing is a popular paradigm to continuously process huge amounts of data. Runtime adaptation plays a significant role in supporting the optimization of data processing tasks. In recent years runtime adaptation has received significant interest in scientific literature. However, so far no categorization of the enactment approaches for runtime adaptation in stream processing has been established. Objective This paper identifies and characterizes different approaches towards the enactment of runtime adaptation in stream processing with a main focus on latency as quality dimension. Method We performed a systematic literature review (SLR) targeting five main research questions. An automated search, resulting in 244 papers, was conducted. 75 papers published between 2006 and 2018 were finally included. From the selected papers, we extracted data like processing problems, adaptation goals, enactment approaches of adaptation, enactment techniques, evaluation metrics as well as evaluation parameters used to trigger the enactment of adaptation in their evaluation. Results We identified 17 different enactment approaches and categorized them into a taxonomy. For each, we extracted the underlying technique used to implement this enactment approach. Further, we identified 9 categories of processing problems, 6 adaptation goals, 9 evaluation metrics and 12 evaluation parameters according to the extracted data properties. Conclusion We observed that the research interest on enactment approaches to the adaptation of stream processing has significantly increased in recent years. The most commonly applied enactment approaches are parameter adaptation to tune parameters or settings of the processing, load balancing used to re-distribute workloads, and processing scaling to dynamically scale up and down the processing. In addition to latency, most adaptations also address resource fluctuation / bottleneck problems. For presenting a dynamic environment to evaluate enactment approaches, researchers often change input rates or processing workloads.},
  doi      = {10.1016/j.infsof.2019.03.006},
  groups   = {Data Stream Processing Latency},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1112685903},
}

@InProceedings{Hamza2016,
  author    = {Hamza, Ahmed and Hefeeda, Mohamed},
  booktitle = {Proceedings of the 7th International Conference on Multimedia Systems},
  title     = {Adaptive streaming of interactive free viewpoint videos to heterogeneous clients},
  year      = {2016},
  pages     = {1-12},
  abstract  = {Recent advances in video capturing and rendering technologies have paved the way for new video streaming applications. Free-viewpoint video (FVV) streaming is one such application where users are able to interact with the scene by navigating to different viewpoints. Free-viewpoint videos are composed of multiple streams representing the captured scene and its geometry from different vantage points. Rendering non-captured views at the client requires transmitting multiple views with associated depth map streams, thereby increasing the network traffic requirements for such systems. Adding to the complexity of these systems is the fact that different component streams contribute differently to the quality of the final rendered view. In this paper, we present a free-viewpoint video streaming system based on HTTP adaptive streaming and the multi-view-plus-depth (MVD) representation. We propose a novel quality-aware rate adaptation method for FVV streaming based on a virtual view distortion model. This view distortion model represents the relation between the distortion of the texture and depth components of reference views and a target virtual view and enables the streaming client to find the best set of representations to request from the server. We have implemented the proposed rate adaptation method in a prototype FVV DASH-based streaming system and performed objective and subjective evaluation experiments. Our experimental results show that the proposed FVV streaming rate adaptation method improves the user's quality-of-experience and increases the visual quality of rendered virtual views by up to 4 dB for some video sequences. Moreover, users have rated the quality of videos streamed using our proposed method higher than videos streamed using other rate adaptation methods in the literature.},
  doi       = {10.1145/2910017.2910610},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1023915518},
}

@Article{Cardellini2018,
  author   = {Cardellini, Valeria and Presti, Francesco Lo and Nardelli, Matteo and Russo, Gabriele Russo},
  journal  = {Future Generation Computer Systems},
  title    = {Decentralized self-adaptation for elastic Data Stream Processing},
  year     = {2018},
  pages    = {171-185},
  volume   = {87},
  abstract = {Data Stream Processing (DSP) applications are widely used to develop new pervasive services, which require to seamlessly process huge amounts of data in a near real-time fashion. To keep up with the high volume of daily produced data, these applications need to dynamically scale their execution on multiple computing nodes, so to process the incoming data flow in parallel. In this paper, we present a hierarchical distributed architecture for the autonomous control of elastic DSP applications. It consists of a two-layered hierarchical solution, where a centralized per-application component coordinates the run-time adaptation of subordinated distributed components, which, in turn, locally control the adaptation of single DSP operators. Thanks to its features, the proposed solution can efficiently run in large-scale Fog computing environments. Exploiting this framework, we design several distributed self-adaptation policies, including a popular threshold-based approach and two reinforcement learning solutions. We integrate the hierarchical architecture and the devised self-adaptation policies in Apache Storm, a popular open-source DSP framework. Relying on the DEBS 2015 Grand Challenge as a benchmark application, we show the benefits of the presented self-adaptation policies, and discuss the strengths of reinforcement learning based approaches, which autonomously learn from experience how to optimize the application performance.},
  doi      = {10.1016/j.future.2018.05.025},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1103991503},
}

@Article{Zhao2017,
  author   = {Zhao, Mincheng and Gong, Xiangyang and Liang, Jie and Wang, Wendong and Que, Xirong and Guo, Yihua and Cheng, Shiduan},
  journal  = {Signal Processing Image Communication},
  title    = {QoE-driven optimization for cloud-assisted DASH-based scalable interactive multiview video streaming over wireless network},
  year     = {2017},
  pages    = {157-172},
  volume   = {57},
  abstract = {In interactive multiview video streaming (IMVS), the viewers can periodically switch viewpoints. If the captured view is not available at the desired viewpoint, virtual views can be rendered from neighboring coded views using view synthesis techniques. Dynamic adaptive streaming over HTTP (DASH) is a new standard that allows to adjust the quality of video streaming based on the network condition. In this paper, an improved DASH-based IMVS framework is proposed. It has the following characteristics. First, virtual views could be adaptively generated at either the cloud-based server or the client in our scheme, depending on the network condition and the cost of the cloud. Second, scalable video coding (SVC) is used to improve the flexibility in our system. To optimize the Quality of Experience (QoE) of multiple clients in wireless scenario, we develop a cross-layer optimization scheme. We first propose a new cache management method to selectively store the video data according to SVC structure and the clients’ requesting condition. Next, a cross-layer scheduling scheme is proposed by considering the video rate adaptation and the wireless resource allocation. The optimization problem is shown to be equivalent to the Multiple Choice Knapsack (MCKP) problem. A dynamic programming method and a low-cost greedy method are developed to solve the problem. Simulations with the NS3 tool demonstrate the advantage of our proposed scheme over the existing approach that always uses client-based view synthesis and single-layer video coding.},
  doi      = {10.1016/j.image.2017.05.015},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1086071207},
}

@Article{Tudoran2016,
  author   = {Tudoran, Radu and Costan, Alexandru and Nano, Olivier and Santos, Ivo and Soncu, Hakan and Antoniu, Gabriel},
  journal  = {Future Generation Computer Systems},
  title    = {JetStream: Enabling high throughput live event streaming on multi-site clouds},
  year     = {2016},
  note     = {https://hal.inria.fr/hal-01239124/file/fgcs.pdf},
  pages    = {274-291},
  volume   = {54},
  abstract = {Scientific and commercial applications operate nowadays on tens of cloud datacenters around the globe, following similar patterns: they aggregate monitoring or sensor data, assess the QoS or run global data mining queries based on inter-site event stream processing. Enabling fast data transfers across geographically distributed sites allows such applications to manage the continuous streams of events in real time and quickly react to changes. However, traditional event processing engines often consider data resources as second-class citizens and support access to data only as a side-effect of computation (i.e. they are not concerned by the transfer of events from their source to the processing site). This is an efficient approach as long as the processing is executed in a single cluster where nodes are interconnected by low latency networks. In a distributed environment, consisting of multiple datacenters, with orders of magnitude differences in capabilities and connected by a WAN, this will undoubtedly lead to significant latency and performance variations. This is namely the challenge we address in this paper, by proposing JetStream, a high performance batch-based streaming middleware for efficient transfers of events between cloud datacenters. JetStream is able to self-adapt to the streaming conditions by modeling and monitoring a set of context parameters. It further aggregates the available bandwidth by enabling multi-route streaming across cloud sites, while at the same time optimizing resource utilization and increasing cost efficiency. The prototype was validated on tens of nodes from US and Europe datacenters of the Windows Azure cloud with synthetic benchmarks and a real-life application monitoring the ALICE experiment at CERN. The results show a 3× increase of the transfer rate using the adaptive multi-route streaming, compared to state of the art solutions.},
  doi      = {10.1016/j.future.2015.01.016},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1042603016},
}

@Article{DeMatteis2017,
  author   = {De Matteis, Tiziano and Mencagli, Gabriele},
  journal  = {Journal of Systems and Software},
  title    = {Proactive elasticity and energy awareness in data stream processing},
  year     = {2017},
  note     = {https://arpi.unipi.it/bitstream/11568/798536/2/preprint-jss-2017.pdf},
  pages    = {302-319},
  volume   = {127},
  abstract = {Data stream processing applications have a long running nature (24 hr/7 d) with workload conditions that may exhibit wide variations at run-time. Elasticity is the term coined to describe the capability of applications to change dynamically their resource usage in response to workload fluctuations. This paper focuses on strategies for elastic data stream processing targeting multicore systems. The key idea is to exploit Model Predictive Control, a control-theoretic method that takes into account the system behavior over a future time horizon in order to decide the best reconfiguration to execute. We design a set of energy-aware proactive strategies, optimized for throughput and latency QoS requirements, which regulate the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) support offered by modern multicore CPUs. We evaluate our strategies in a high-frequency trading application fed by synthetic and real-world workload traces. We introduce specific properties to effectively compare different elastic approaches, and the results show that our strategies are able to achieve the best outcome.},
  doi      = {10.1016/j.jss.2016.08.037},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1018598520},
}

@InBook{Zhou2008,
  author    = {Zhou, Yongluan and Aberer, Karl and Tan, Kian-Lee},
  pages     = {326-345},
  title     = {Toward Massive Query Optimization in Large-Scale Distributed Stream Systems},
  year      = {2008},
  note      = {https://link.springer.com/content/pdf/10.1007/978-3-540-89856-6_17.pdf},
  abstract  = {Existing distributed stream systems adopt a tightly-coupled communication paradigm and focus on fine-tuning of operator placements to achieve communication efficiency. This kind of approach is hard to scale (both to the nodes in the network and the users). In this paper, we propose a fundamentally different approach and present the design of a middleware for optimizing massive queries. Our approach takes the advantages of existing Publish/Subscribe systems (Pub/Sub) to achieve loosely-coupled communication and to “intelligently” exploit the sharing of communication among different queries. To fully exploit the capability of a Pub/Sub, we present a new query distribution algorithm, which can adaptively and rapidly (re)distribute the streaming queries at runtime to achieve both load balancing and low communication cost. Both the simulation studies and the prototype experiments executed on PlanetLab show the effectiveness of our techniques.},
  booktitle = {Middleware 2008},
  doi       = {10.1007/978-3-540-89856-6_17},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1012606071},
}

@InBook{Zacheilas2016,
  author    = {Zacheilas, Nikos and Zygouras, Nikolas and Panagiotou, Nikolaos and Kalogeraki, Vana and Gunopulos, Dimitrios},
  pages     = {174-188},
  title     = {Dynamic Load Balancing Techniques for Distributed Complex Event Processing Systems},
  year      = {2016},
  note      = {https://doi.org/10.1007/978-3-319-39577-7_14},
  abstract  = {Applying real-time, cost-effective Complex Event processing (CEP) in the cloud has been an important goal in recent years. Distributed Stream Processing Systems (DSPS) have been widely adopted by major computing companies such as Facebook and Twitter for performing scalable event processing in streaming data. However, dynamically balancing the load of the DSPS’ components can be particularly challenging due to the high volume of data, the components’ state management needs, and the low latency processing requirements. Systems should be able to cope with these challenges and adapt to dynamic and unpredictable load changes in real-time. Our approach makes the following contributions: (i) we formulate the load balancing problem in distributed CEP systems as an instance of the job-shop scheduling problem, and (ii) we present a novel framework that dynamically balances the load of CEP engines in real-time and adapts to sudden changes in the volume of streaming data by exploiting two balancing policies. Our detailed experimental evaluation using data from the Twitter social network indicates the benefits of our approach in the system’s throughput.},
  booktitle = {Distributed Applications and Interoperable Systems},
  doi       = {10.1007/978-3-319-39577-7_14},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1044962254},
}

@InProceedings{Yin2009,
  author    = {Yin, Hao and Liu, Xuening and Zhan, Tongyu and Sekar, Vyas and Qiu, Feng and Lin, Chuang and Zhang, Hui and Li, Bo},
  booktitle = {Proceedings of the 17th ACM international conference on Multimedia},
  title     = {Design and deployment of a hybrid CDN-P2P system for live video streaming},
  year      = {2009},
  pages     = {25-34},
  abstract  = {We present our design and deployment experiences with LiveSky, a commercially deployed hybrid CDN-P2P live streaming system. CDNs and P2P systems are the common techniques used for live streaming, each having its own set of advantages and disadvantages. LiveSky inherits the best of both worlds: the quality control and reliability of a CDN and the inherent scalability of a P2P system. We address several key challenges in the system design and implementation including (a) dynamic resource scaling while guaranteeing stream quality, (b) providing low startup latency, (c) ease of integration with existing CDN infrastructure, and (d) ensuring network-friendliness and upload fairness in the P2P operation. LiveSky has been commercially deployed and used for several large-scale live streaming events serving more than ten million users in China. We evaluate the performance of LiveSky using data from these real-world deployments. Our results indicate that such a hybrid CDN-P2P system provides quality and user performance comparable to a CDN and effectively scales the system capacity when the user volume exceeds the CDN capacity.},
  doi       = {10.1145/1631272.1631279},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1043395206},
}

@InProceedings{Qian2013,
  author    = {Qian, Zhengping and He, Yong and Su, Chunzhi and Wu, Zhuojie and Zhu, Hongyu and Zhang, Taizhi and Zhou, Lidong and Yu, Yuan and Zhang, Zheng},
  booktitle = {Proceedings of the 8th ACM European Conference on Computer Systems},
  title     = {TimeStream},
  year      = {2013},
  pages     = {1-14},
  abstract  = {TimeStream is a distributed system designed specifically for low-latency continuous processing of big streaming data on a large cluster of commodity machines. The unique characteristics of this emerging application domain have led to a significantly different design from the popular MapReduce-style batch data processing. In particular, we advocate a powerful new abstraction called resilient substitution that caters to the specific needs in this new computation model to handle failure recovery and dynamic reconfiguration in response to load changes. Several real-world applications running on our prototype have been shown to scale robustly with low latency while at the same time maintaining the simple and concise declarative programming model. TimeStream handles an on-line advertising aggregation pipeline at a rate of 700,000 URLs per second with a 2-second delay, while performing sentiment analysis of Twitter data at a peak rate close to 10,000 tweets per second, with approximately 2-second delay.},
  doi       = {10.1145/2465351.2465353},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1015991543},
}

@InBook{Lakshmanan2008,
  author    = {Lakshmanan, Geetika T. and Strom, Robert E.},
  pages     = {223-242},
  title     = {Biologically-Inspired Distributed Middleware Management for Stream Processing Systems},
  year      = {2008},
  note      = {https://link.springer.com/content/pdf/10.1007/978-3-540-89856-6_12.pdf},
  abstract  = {We present a decentralized and dynamic biologically-inspired algorithm for placing dataflow graphs composed of stream processing tasks onto a distributed network of machines, while minimizing the end-to-end latency. Our algorithm responds on-the-fly to placement requests of new flow graphs or to modifications of an already running stream processing flow graph, and dynamically adapts to changes in performance characteristics such as message rates or service times as well as to changes in processor availability or link performance during runtime. Our algorithm is derived by analogy to pheromone-based cooperation between ants to fulfill goals such as food discovery. We have conducted extensive simulation experiments to show the scalability and adaptability of our algorithm.},
  booktitle = {Middleware 2008},
  doi       = {10.1007/978-3-540-89856-6_12},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1005124287},
}

@Article{JacquesSilva2018,
  author   = {Jacques-Silva, Gabriela and Lei, Ran and Cheng, Luwei and Chen, Guoqiang Jerry and Ching, Kuen and Hu, Tanji and Mei, Yuan and Wilfong, Kevin and Shetty, Rithin and Yilmaz, Serhat and Banerjee, Anirban and Heintz, Benjamin and Iyer, Shridar and Jaiswal, Anshul},
  journal  = {Proceedings of the VLDB Endowment},
  title    = {Providing streaming joins as a service at Facebook},
  year     = {2018},
  number   = {12},
  pages    = {1809-1821},
  volume   = {11},
  abstract = {Stream processing applications reduce the latency of batch data pipelines and enable engineers to quickly identify production issues. Many times, a service can log data to distinct streams, even if they relate to the same real-world event (e.g., a search on Facebook's search bar). Furthermore, the logging of related events can appear on the server side with different delay, causing one stream to be significantly behind the other in terms of logged event times for a given log entry. To be able to stitch this information together with low latency , we need to be able to join two different streams where each stream may have its own characteristics regarding the degree in which its data is out-of-order . Doing so in a streaming fashion is challenging as a join operator consumes lots of memory, especially with significant data volumes. This paper describes an end-to-end streaming join service that addresses the challenges above through a streaming join operator that uses an adaptive stream synchronization algorithm that is able to handle the different distributions we observe in real-world streams regarding their event times. This synchronization scheme paces the parsing of new data and reduces overall operator memory footprint while still providing high accuracy. We have integrated this into a streaming SQL system and have successfully reduced the latency of several batch pipelines using this approach.},
  doi      = {10.14778/3229863.3229869},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1106923111},
}

@InProceedings{Wang2009,
  author    = {Wang, Song and Rundensteiner, Elke},
  booktitle = {Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology},
  title     = {Scalable stream join processing with expensive predicates},
  year      = {2009},
  note      = {https://dl.acm.org/doi/pdf/10.1145/1516360.1516396},
  pages     = {299-310},
  abstract  = {Multi-way stream joins with expensive join predicates lead to great challenge for real-time (or close to real-time) stream processing. Given the memory- and CPU-intensive nature of such stream join queries, scalable processing on a cluster must be employed. This paper proposes a novel scheme for distributed processing of generic multi-way joins with window constraints, called Pipelined State Partitioning (PSP). We target generic joins with arbitrarily join conditions, which are used in non-trivial stream applications such as image matching and biometric recognizing. The PSP scheme partitions the states into disjoint slices in the time domain, and then distributes the fine-grained states in the cluster, forming a virtual computation ring. Compared to replication-based distribution of non-equi-joins, PSP scheme is superior since: (1) zero state duplication and thus no repeated computations, (2) pipelined processing of every input tuple on multiple nodes to achieve low response time, and (3) cost-based adaptive workload distribution. We have implemented the proposed PSP schemes within the CAPE DSMS. Our experimental study demonstrates the significant performance improvements compared to the state-of-the-art generic distributed stream join algorithms.},
  doi       = {10.1145/1516360.1516396},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1040187607},
}

@InProceedings{Venkataraman2017,
  author    = {Venkataraman, Shivaram and Panda, Aurojit and Ousterhout, Kay and Armbrust, Michael and Ghodsi, Ali and Franklin, Michael J. and Recht, Benjamin and Stoica, Ion},
  booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
  title     = {Drizzle},
  year      = {2017},
  pages     = {374-389},
  abstract  = {Large scale streaming systems aim to provide high throughput and low latency. They are often used to run mission-critical applications, and must be available 24x7. Thus such systems need to adapt to failures and inherent changes in workloads, with minimal impact on latency and throughput. Unfortunately, existing solutions require operators to choose between achieving low latency during normal operation and incurring minimal impact during adaptation. Continuous operator streaming systems, such as Naiad and Flink, provide low latency during normal execution but incur high overheads during adaptation (e.g., recovery), while micro-batch systems, such as Spark Streaming and FlumeJava, adapt rapidly at the cost of high latency during normal operations. Our key observation is that while streaming workloads require millisecond-level processing, workload and cluster properties change less frequently. Based on this, we develop Drizzle, a system that decouples the processing interval from the coordination interval used for fault tolerance and adaptability. Our experiments on a 128 node EC2 cluster show that on the Yahoo Streaming Benchmark, Drizzle can achieve end-to-end record processing latencies of less than 100ms and can get 2-3x lower latency than Spark. Drizzle also exhibits better adaptability, and can recover from failures 4x faster than Flink while having up to 13x lower latency during recovery.},
  doi       = {10.1145/3132747.3132750},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1092569607},
}

@InProceedings{Zhang2018,
  author    = {Zhang, Ben and Jin, Xin and Ratnasamy, Sylvia and Wawrzynek, John and Lee, Edward A.},
  booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
  title     = {AWStream},
  year      = {2018},
  note      = {https://dl.acm.org/doi/pdf/10.1145/3230543.3230554},
  pages     = {236-252},
  abstract  = {The emerging class of wide-area streaming analytics faces the challenge of scarce and variable WAN bandwidth. Non-adaptive applications built with TCP or UDP suffer from increased latency or degraded accuracy. State-of-the-art approaches that adapt to network changes require developer writing sub-optimal manual policies or are limited to application-specific optimizations. We present AWStream, a stream processing system that simultaneously achieves low latency and high accuracy in the wide area, requiring minimal developer efforts. To realize this, AWStream uses three ideas: (i) it integrates application adaptation as a first-class programming abstraction in the stream processing model; (ii) with a combination of offline and online profiling, it automatically learns an accurate profile that models accuracy and bandwidth trade-off; and (iii) at runtime, it carefully adjusts the application data rate to match the available bandwidth while maximizing the achievable accuracy. We evaluate AWStream with three real-world applications: augmented reality, pedestrian detection, and monitoring log analysis. Our experiments show that AWStream achieves sub-second latency with only nominal accuracy drop (2-6%).},
  doi       = {10.1145/3230543.3230554},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1106072382},
}

@InBook{Babazadeh2015,
  author    = {Babazadeh, Masiar and Gallidabino, Andrea and Pautasso, Cesare},
  pages     = {24-33},
  title     = {Liquid Stream Processing Across Web Browsers and Web Servers},
  year      = {2015},
  note      = {https://link.springer.com/content/pdf/10.1007/978-3-319-19890-3_3.pdf},
  abstract  = {The recently proposed API definition WebRTC introduced peer-to-peer real time communication between Web browsers, allowing streaming systems to be deployed on browsers in addition to traditional server-side execution environments. While streaming applications can be adapted to run on Web browsers, it remains difficult to deal with temporary disconnections, energy consumption on mobile devices and a potentially very large number of heterogeneous peers that join and leave the execution environment affecting the quality of the stream. In this paper we present the decentralized control approach followed by the Web Liquid Streams (WLS) framework, a novel framework for streaming applications running on Web browsers, Web servers and smart devices. Given the heterogeneity of the deployment environment and the volatility of Web browsers, we implemented a control infrastructure which is able to take operator migration decisions keeping into account the deployment constraints and the unpredictable workload.},
  booktitle = {Engineering the Web in the Big Data Era},
  doi       = {10.1007/978-3-319-19890-3_3},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1047880318},
}

@InProceedings{Guo2006,
  author    = {Guo, Lei and Tan, Enhua and Chen, Songqing and Xiao, Zhen and Spatscheck, Oliver and Zhang, Xiaodong},
  booktitle = {Proceedings of the 6th ACM SIGCOMM conference on Internet measurement},
  title     = {Delving into internet streaming media delivery},
  year      = {2006},
  pages     = {217-230},
  abstract  = {Modern Internet streaming services have utilized various techniques to improve the quality of streaming media delivery. Despite the characterization of media access patterns and user behaviors in many measurement studies, few studies have focused on the streaming techniques themselves, particularly on the quality of streaming experiences they offer end users and on the resources of the media systems that they consume. In order to gain insights into current streaming services techniques and thus provide guidance on designing resource-efficient and high quality streaming media systems, we have collected a large streaming media workload from thousands of broadband home users and business users hosted by a major ISP, and analyzed the most commonly used streaming techniques such as automatic protocol switch, Fast Streaming, MBR encoding and rate adaptation. Our measurement and analysis results show that with these techniques, current streaming systems these techniques tend to over-utilize CPU and bandwidth resources to provide better services to end users, which may not be a desirable and effective is not necessary the best way to improve the quality of streaming media delivery. Motivated by these results, we propose and evaluate a coordination mechanism that effectively takes advantage of both Fast Streaming and rate adaptation to better utilize the server and Internet resources for streaming quality improvement.},
  doi       = {10.1145/1177080.1177108},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1053223966},
}

@InProceedings{Hamza2014,
  author    = {Hamza, Ahmed and Hefeeda, Mohamed},
  booktitle = {Proceedings of Network and Operating System Support on Digital Audio and Video Workshop},
  title     = {A DASH-based Free Viewpoint Video Streaming System},
  year      = {2014},
  pages     = {55-60},
  abstract  = {We present an interactive free-viewpoint video (FVV) streaming system that is based on the dynamic adaptive streaming over HTTP (DASH) standard. The system uses standard HTTP Web servers to achieve scalability with a large number of users and performs view synthesis and rate adaptation at the client-side to achieve high response time. We propose a rate adaptation logic based on sampled rate-distortion (R-D) values, which relate the distortion of synthesized view to the bit rates of the texture and depth components of the reference views, to maximize the quality of rendered virtual views. Initial results indicate that the proposed R-D-based rate adaptation strategy outperforms equal bit rate allocation among the reference streams components.},
  doi       = {10.1145/2597176.2578276},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1098870698},
}

@Article{Ottenwaelder2014,
  author   = {Ottenwälder, Beate and Koldehofe, Boris and Rothermel, Kurt and Hong, Kirak and Lillethun, David and Ramachandran, Umakishore},
  journal  = {ACM Transactions on Internet Technology},
  title    = {MCEP},
  year     = {2014},
  number   = {1},
  pages    = {1-24},
  volume   = {14},
  abstract = {With the proliferation of mobile devices and sensors, complex event proceesing (CEP) is becoming increasingly important to scalably detect situations in real time. Current CEP systems are not capable of dealing efficiently with highly dynamic mobile consumers whose interests change with their location. We introduce the distributed mobile CEP (MCEP) system which automatically adapts the processing of events according to a consumer's location. MCEP significantly reduces latency, network utilization, and processing overhead by providing on-demand and opportunistic adaptation algorithms to dynamically assign event streams and computing resources to operators of the MCEP system.},
  doi      = {10.1145/2633688},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1007287400},
}

@InProceedings{Backman2012,
  author    = {Backman, Nathan and Pattabiraman, Karthik and Fonseca, Rodrigo and Cetintemel, Ugur},
  booktitle = {Proceedings of third international workshop on MapReduce and its Applications Date},
  title     = {C-MR},
  year      = {2012},
  pages     = {1-8},
  abstract  = {The widespread appeal of MapReduce is due, in part, to its simple programming model. Programmers provide only application logic while the MapReduce framework handles the logistics of data distribution and parallel task management. We present the Continuous-MapReduce (C-MR) framework which implements a modified MapReduce processing model to continuously execute workflows of MapReduce jobs on unbounded data streams. In keeping with the philosophy of MapReduce, C-MR abstracts away the complexities of parallel stream processing and workflow scheduling while providing the simple and familiar MapReduce programming interface with the addition of stream window semantics. Modifying the MapReduce processing model allowed us to: (1) maintain correct stream order and execution semantics in the presence of parallel and asynchronous processing elements; (2) implement an operator scheduler framework to facilitate latency-oriented scheduling policies for executing complex workflows of MapReduce jobs; and (3) leverage much of the work that has gone into the last decade of stream processing research including: pipelined parallelism, incremental processing for both Map and Reduce operations, minimizing redundant computations, sharing of sub-queries, and adaptive query processing. C-MR was developed for use on a multiprocessor architecture, where we demonstrate its effectiveness at supporting high-performance stream processing even in the presence of load spikes and external workloads.},
  doi       = {10.1145/2287016.2287018},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1004391931},
}

@InProceedings{Das2014,
  author    = {Das, Tathagata and Zhong, Yuan and Stoica, Ion and Shenker, Scott},
  booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
  title     = {Adaptive Stream Processing using Dynamic Batch Sizing},
  year      = {2014},
  pages     = {1-13},
  abstract  = {The need for real-time processing of "big data" has led to the development of frameworks for distributed stream processing in clusters. It is important for such frameworks to be robust against variable operating conditions such as server failures, changes in data ingestion rates, and workload characteristics. To provide fault tolerance and efficient stream processing at scale, recent stream processing frameworks have proposed to treat streaming workloads as a series of batch jobs on small batches of streaming data. However, the robustness of such frameworks against variable operating conditions has not been explored. In this paper, we explore the effects of the batch size on the performance of streaming workloads. The throughput and end-to-end latency of the system can have complicated relationships with batch sizes, data ingestion rates, variations in available resources, workload characteristics, etc. We propose a simple yet robust control algorithm that automatically adapts the batch size as the situation necessitates. We show through extensive experiments that it can ensure system stability and low latency for a wide range of workloads, despite large variations in data rates and operating conditions.},
  doi       = {10.1145/2670979.2670995},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1022138468},
}

@InProceedings{Madsen2015,
  author    = {Madsen, Kasper Grud Skat and Zhou, Yongluan},
  booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
  title     = {Dynamic Resource Management In a Massively Parallel Stream Processing Engine},
  year      = {2015},
  pages     = {13-22},
  abstract  = {The emerging interest in Massively Parallel Stream Processing Engines (MPSPEs), which are able to process long-standing computations over data streams with ever-growing velocity at a large-scale cluster, calls for efficient dynamic resource management techniques to avoid any waste of resources and/or excessive processing latency. In this paper, we propose an approach to integrate dynamic resource management with passive fault-tolerance mechanisms in a MPSPE so that we can harvest the checkpoints prepared for failure recovery to enhance the efficiency of dynamic load migrations. To maximize the opportunity of reusing checkpoints for fast load migration, we formally define a checkpoint allocation problem and provide a pragmatic algorithm to solve it. We implement all the proposed techniques on top of Apache Storm, an open-source MPSPE, and conduct extensive experiments using a real dataset to examine various aspects of our techniques. The results show that our techniques can greatly improve the efficiency of dynamic resource reconfiguration without imposing significant overhead or latency to the normal job execution.},
  doi       = {10.1145/2806416.2806449},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1047717752},
}

@InProceedings{Chatzistergiou2014,
  author    = {Chatzistergiou, Andreas and Viglas, Stratis D.},
  booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
  title     = {Fast Heuristics for Near-Optimal Task Allocation in Data Stream Processing over Clusters},
  year      = {2014},
  pages     = {1579-1588},
  abstract  = {We study provisioning and job reconfiguration techniques for adapting to execution environment changes when processing data streams on cluster-based deployments. By monitoring the performance of an executing job, we identify computation and communication bottlenecks. In such cases we reconfigure the job by reallocating its tasks to minimize the communication cost. Our work targets data-intensive applications where the inter-node transfer latency is significant. We aim to minimize the transfer latency while keeping the nodes below some computational load threshold. We propose a scalable centralized scheme that employs fast allocation heuristics. Our techniques are based on a general group-based job representation that is commonly found in many distributed data stream processing frameworks. Using this representation we devise linear-time task allocation algorithms that improve existing quadratic-time solutions in practical cases. We have implemented and evaluated our proposals using both synthetic and real-world scenarios. Our results show that our algorithms: (a) exhibit significant allocation throughput while producing near-optimal allocations, and (b) significantly improve existing task-level approaches.},
  doi       = {10.1145/2661829.2661882},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1011754334},
}

@Article{Higashino2016,
  author   = {Higashino, Wilson A. and Eichler, Cédric and Capretz, Miriam A. M. and Bittencourt, Luiz F. and Monteil, Thierry},
  journal  = {ACM Transactions on Autonomous and Adaptive Systems},
  title    = {Attributed Graph Rewriting for Complex Event Processing Self-Management},
  year     = {2016},
  note     = {http://dl.acm.org/ft_gateway.cfm?id=2967499&type=pdf},
  number   = {3},
  pages    = {1-39},
  volume   = {11},
  abstract = {The use of Complex Event Processing (CEP) and Stream Processing (SP) systems to process high-volume, high-velocity Big Data has renewed interest in procedures for managing these systems. In particular, self-management and adaptation of runtime platforms have been common research themes, as most of these systems run under dynamic conditions. Nevertheless, the research landscape in this area is still young and fragmented. Most research is performed in the context of specific systems, and it is difficult to generalize the results obtained to other contexts. To enable generic and reusable CEP/SP system management procedures and self-management policies, this research introduces the Attributed Graph Rewriting for Complex Event Processing Management (
                    AGeCEP
                    ) formalism.
                    AGeCEP
                    represents queries in a language- and technology-agnostic fashion using attributed graphs. Query reconfiguration capabilities are expressed through standardized attributes, which are defined based on a novel classification of CEP query operators. By leveraging this representation,
                    AGeCEP
                    also proposes graph rewriting rules to define consistent reconfigurations of queries. To demonstrate
                    AGeCEP
                    feasibility, this research has used it to design an autonomic manager and to define a selected set of self-management policies. Finally, experiments demonstrate that
                    AGeCEP
                    can indeed be used to develop algorithms that can be integrated into diverse CEP systems.},
  doi      = {10.1145/2967499},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1028394848},
}

@Article{Wang2016,
  author   = {Wang, Feng and Liu, Jiangchuan and Chen, Minghua and Wang, Haiyang},
  journal  = {IEEE/ACM Transactions on Networking},
  title    = {Migration Towards Cloud-Assisted Live Media Streaming},
  year     = {2016},
  number   = {1},
  pages    = {272-282},
  volume   = {24},
  abstract = {Live media streaming has become one of the most popular applications over the Internet. We have witnessed the successful deployment of commercial systems with content delivery network (CDN)- or peer-to-peer-based engines. While each being effective in certain aspects, having an all-round scalable, reliable, responsive, and cost-effective solution remains an illusive goal. Moreover, today's live streaming services have become highly globalized, with subscribers from all over the world. Such a globalization makes user behaviors and demands even more diverse and dynamic, further challenging state-of-the-art system designs. The emergence of cloud computing, however, sheds new light into this dilemma. Leveraging the elastic resource provisioning from the cloud, we present Cloud-Assisted Live Media Streaming (CALMS), a generic framework that facilitates a migration to the cloud. CALMS adaptively leases and adjusts cloud server resources in a fine granularity to accommodate temporal and spatial dynamics of demands from live streaming users. We present optimal solutions to deal with cloud servers with diverse capacities and lease prices, as well as the potential latencies in initiating and terminating leases in real-world cloud platforms. Our solution well accommodates location heterogeneity, mitigating the impact from user globalization. It also enables seamless migration for existing streaming systems, e.g., peer-to-peer, and fully explores their potentials. Simulations with data traces from both cloud service providers (Amazon EC2 and SpotCloud) and a live streaming service provider (PPTV) demonstrate that CALMS effectively mitigates the overall system deployment costs and yet provides users with satisfactory streaming latency and rate.},
  doi      = {10.1109/tnet.2014.2362541},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1061716021},
}

@InProceedings{Kalim2018,
  author    = {Kalim, Faria and Xu, Le and Bathey, Sharanya and Meherwal, Richa and Gupta, Indranil},
  booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
  title     = {Henge},
  year      = {2018},
  pages     = {249-262},
  abstract  = {We present Henge, a system to support intent-based multi-tenancy in modern distributed stream processing systems. Henge supports multi-tenancy as a first-class citizen: everyone in an organization can now submit their stream processing jobs to a single, shared, consolidated cluster. Secondly, Henge allows each job to specify its own intents (i.e., requirements) as a Service Level Objective (SLO) that captures latency and/or throughput needs. In such an intent-driven multi-tenant cluster, the Henge scheduler adapts continually to meet jobs' respective SLOs in spite of limited cluster resources, and under dynamically varying workloads. SLOs are soft and are based on utility functions. Henge's overall goal is to maximize the total system utility achieved by all jobs in the system. Henge is integrated into Apache Storm and we present experimental results using both production jobs from Yahoo! and real datasets from Twitter.},
  doi       = {10.1145/3267809.3267832},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1107307625},
}

@InProceedings{Kuang2011,
  author    = {Kuang, Jilong and Bhuyan, Laxmi and Xie, Haiyong and Guo, Danhua},
  booktitle = {2011 ACM/IEEE Seventh Symposium on Architectures for Networking and Communications Systems},
  title     = {E-AHRW: An Energy-Efficient Adaptive Hash Scheduler for Stream Processing on Multi-core Servers},
  year      = {2011},
  pages     = {45-56},
  abstract  = {We study a streaming network application -- video transcoding to be executed on a multi-core server. It is important for the scheduler to minimize the total processing time and preserve good video quality in an energy-efficient manner. However, the performance of existing scheduling schemes is largely limited by ineffective use of the multi-core architecture characteristic and undifferentiated transcoding cost in terms of energy consumption. In this paper, we identify three key factors that collectively play important roles in affecting transcoding performance: memory access (M), core/cache topology (C) and transcoding format cost (C), or MC^2 for short. Based on MC^2, we propose E-AHRW, an Energy-efficient Adaptive Highest Random Weight hash scheduler by extending the HRW scheduler proposed for packet scheduling on a homogeneous multiprocessor. E-AHRW achieves stream locality and load balancing at both stream and packet (frame) level by adaptively adjusting the hashing decision according to real-time weighted queue length of each processing unit (PU). Based on E-AHRW, we also design, implement and evaluate a hash-tree scheduler to further reduce the computation cost and achieve more effective load balancing on multi-core architectures. Through implementation on an Intel Xeon server and evaluations on realistic workload, we demonstrate that E-AHRW improves throughput, energy efficiency and video quality due to better load balancing, lower L2 cache miss rate and negligible scheduling overhead.},
  doi       = {10.1109/ancs.2011.15},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1095129896},
}

@InBook{Claypool2008,
  author    = {Claypool, Kajal and Claypool, Mark},
  pages     = {220-234},
  title     = {Teddies: Trained Eddies for Reactive Stream Processing},
  year      = {2008},
  abstract  = {In this paper, we present an adaptive stream query processor, Teddies, that combines the key advantages of the Eddies system with the scalability of the more traditional dataflow model. In particular, we introduce the notion of adaptive packetization of tuples to overcome the large memory requirements of the Eddies system. The Teddies optimizer groups tuples with the same history into data packets which are then scheduled on a per packet basis through the query tree. Corresponding to the introduction of this second dimension – the packet granularity – we propose an adaptive scheduler that can react to not only the varying statistics of the input streams and the selectivity of the operators, but also to the fluctuations in the internal packet sizes. The scheduler degrades to the Eddies scheduler in the worst case scenario. We present experimental results that compare both the reaction time as well as the scalability of the Teddies system with the Eddies and the data flow systems, and classify the conditions under which the Teddies’ simple packet optimizer strategy outperforms the per-tuple Eddies optimizer strategy.},
  booktitle = {Database Systems for Advanced Applications},
  doi       = {10.1007/978-3-540-78568-2_18},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1051953792},
}

@InProceedings{Simmhan2011,
  author    = {Simmhan, Yogesh and Cao, Baohua and Giakkoupis, Michail and Prasanna, Viktor K.},
  booktitle = {Proceedings of the 2nd international workshop on Scientific cloud computing},
  title     = {Adaptive rate stream processing for smart grid applications on clouds},
  year      = {2011},
  pages     = {33-38},
  abstract  = {Pervasive smart meters that continuously measure power usage by consumers within a smart (power) grid are providing utilities and power systems researchers with unprecedented volumes of information through streams that need to be processed and analyzed in near realtime. We introduce the use of Cloud platforms to perform scalable, latency sensitive stream processing for eEngineering applications in the smart grid domain. One unique aspect of our work is the use of adaptive rate control to throttle the rate of generation of power events by smart meters, which meets accuracy requirements of smart grid applications while consuming 50% lesser bandwidth resources in the Cloud.},
  doi       = {10.1145/1996109.1996116},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1027552609},
}

@InProceedings{Ravindra2017,
  author    = {Ravindra, Sajith and Dayarathna, Miyuru and Jayasena, Sanath},
  booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
  title     = {Latency Aware Elastic Switching-based Stream Processing Over Compressed Data Streams},
  year      = {2017},
  pages     = {91-102},
  abstract  = {Elastic scaling of event stream processing systems has gained significant attention recently due to the prevalence of cloud computing technologies. We investigate on the complexities associated with elastic scaling of an event processing system in a private/public cloud scenario. We develop an Elastic Switching Mechanism (ESM) which reduces the overall average latency of event processing jobs by significant amount considering the cost of operating the system. ESM is augmented with adaptive compressing of upstream data. The ESM conducts one of the two types of switching where either part of the data is sent to the public cloud (data switching) or a selected query is sent to the public cloud (query switching) based on the characteristics of the query. We model the operation of the ESM as the function of two binary switching functions. We show that our elastic switching mechanism with compression is capable of handling out-of-order events more efficiently compared to techniques which does not involve compression. We used two application benchmarks called EmailProcessor and a Social Networking Benchmark (SNB2016) to conduct multiple experiments to evaluate the effectiveness of our approach. In a single query deployment with EmailProcessor benchmark we observed that our elastic switching mechanism provides 1.24 seconds average latency improvement per processed event which is 16.70% improvement compared to private cloud only deployment. When presented the option of scaling EmailProcessor with four public cloud VMs ESM further reduced the average latency by 37.55% compared to the single public cloud VM. In a multi-query deployment with both EmailProcessor and SNB2016 we obtained a reduction of average latency of both the queries by 39.61 seconds which is a decrease of 7% of overall latency. These performance figures indicate that our elastic switching mechanism with compressed data streams can effectively reduce the average elapsed time of stream processing happening in private/public clouds.},
  doi       = {10.1145/3030207.3030227},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1085044268},
}

@InProceedings{Balkesen2013,
  author    = {Balkesen, Cagri and Tatbul, Nesime and Özsu, M. Tamer},
  booktitle = {Proceedings of the 7th ACM international conference on Distributed event-based systems},
  title     = {Adaptive input admission and management for parallel stream processing},
  year      = {2013},
  note      = {https://cs.uwaterloo.ca/~tozsu/publications/stream/debs016f-balkesen.pdf},
  pages     = {15-26},
  abstract  = {In this paper, we propose a framework for adaptive admission control and management of a large number of dynamic input streams in parallel stream processing engines. The framework takes as input any available information about input stream behaviors and the requirements of the query processing layer, and adaptively decides how to adjust the entry points of streams to the system. As the optimization decisions propagate early from input management layer to the query processing layer, the size of the cluster is minimized, the load balance is maintained, and latency bounds of queries are met in a more effective and timely manner. Declarative integration of external meta-data about data sources makes the system more robust and resource-efficient. Additionally, exploiting knowledge about queries moves data partitioning to the input management layer, where better load balance for query processing can be achieved. We implemented these techniques as a part of the Borealis stream processing system and conducted experiments showing the performance benefits of our framework.},
  doi       = {10.1145/2488222.2488258},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1003923740},
}

@InProceedings{Lohrmann2012,
  author    = {Lohrmann, Björn and Warneke, Daniel and Kao, Odej},
  booktitle = {Proceedings of the 21st international symposium on High-Performance Parallel and Distributed Computing},
  title     = {Massively-parallel stream processing under QoS constraints with Nephele},
  year      = {2012},
  pages     = {271-282},
  abstract  = {Today, a growing number of commodity devices, like mobile phones or smart meters, is equipped with rich sensors and capable of producing continuous data streams. The sheer amount of these devices and the resulting overall data volumes of the streams raise new challenges with respect to the scalability of existing stream processing systems. At the same time, massively-parallel data processing systems like MapReduce have proven that they scale to large numbers of nodes and efficiently organize data transfers between them. Many of these systems also provide streaming capabilities. However, unlike traditional stream processors, these systems have disregarded QoS requirements of prospective stream processing applications so far. In this paper we address this gap. First, we analyze common design principles of today's parallel data processing frameworks and identify those principles that provide degrees of freedom in trading off the QoS goals latency and throughput. Second, we propose a scheme which allows these frameworks to detect violations of user-defined latency constraints and optimize the job execution without manual interaction in order to meet these constraints while keeping the throughput as high as possible. As a proof of concept, we implemented our approach for our parallel data processing framework Nephele and evaluated its effectiveness through a comparison with Hadoop Online. For a multimedia streaming application we can demonstrate an improved processing latency by factor of at least 15 while preserving high data throughput when needed.},
  doi       = {10.1145/2287076.2287117},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1019780188},
}

@InProceedings{Heinze2015,
  author    = {Heinze, Thomas and Zia, Mariam and Krahn, Robert and Jerzak, Zbigniew and Fetzer, Christof},
  booktitle = {Proceedings of the 9th ACM International Conference on Distributed Event-Based Systems},
  title     = {An adaptive replication scheme for elastic data stream processing systems},
  year      = {2015},
  pages     = {150-161},
  abstract  = {A major challenge for cloud-based systems is to be fault tolerant to cope with an increasing probability of faults in cloud environments. This is especially true for in-memory computing solutions like data stream processing systems, where a single host failure might result in an unrecoverable information loss. In state of the art data streaming systems either active replication or upstream backup are applied to ensure fault tolerance, which have a high resource overhead or a high recovery time respectively. This paper combines these two fault tolerance mechanisms in one system to minimize the number of violations of a user-defined recovery time threshold and to reduce the overall resource consumption compared to active replication. The system switches for individual operators between both replication techniques dynamically based on the current workload characteristics. Our approach is implemented as an extension of an elastic data stream processing engine, which is able to reduce the number of used hosts due to the smaller replication overhead. Based on a real-world evaluation we show that our system is able to reduce the resource usage by up to 19% compared to an active replication scheme.},
  doi       = {10.1145/2675743.2771831},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1053737749},
}

@InProceedings{Liu2011,
  author    = {Liu, Yu and Meier, René},
  booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
  title     = {AdaptStream},
  year      = {2011},
  pages     = {217-223},
  abstract  = {Stream-based systems are frequently subject to changes in their operational environments due to fluctuations in the available computation and communication resources. Dynamic adaptation is a mechanism to improve the fitness of such systems. However, adaptation can block one or more streams thus inadvertently affecting the timeliness properties of streams. This paper describes AdaptStream, an adaptation framework that provides timeliness support for stream-based adaptations. We introduce the concept of fluidity to measure the temporal alignment of stream synchronization during adaptation. We present a scheduling algorithm that calculates the time-bounded schedule of adaptation actions on multiple streams to achieve the fluidity requirement that is traded off against available resources and the smoothness requirement of individual streams.},
  doi       = {10.1145/1982185.1982235},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1017589856},
}

@InProceedings{Madsen2016,
  author    = {Madsen, Kasper Grud Skat and Zhou, Yongluan and Su, Li},
  booktitle = {Proceedings of the 10th ACM International Conference on Distributed and Event-based Systems},
  title     = {Enorm},
  year      = {2016},
  pages     = {37-48},
  abstract  = {Modern distributed stream processing systems (DSPS), such as Storm, typically provide a flexible programming model, where computation is specified as complicated UDFs and data is opaque to the system. While such a programming framework provides very high flexibility to the developers, it does not provide much semantic information to the system and hence it is hard to perform optimizations that has already been proved very effective in conventional stream systems. Examples include sharing computation among overlapping windows, co-partitioning operators to save communication overhead and efficient state migration during load balancing. In lieu of these challenges, we propose a new framework, which is designed to expose sufficient semantic information of the applications to enable the aforementioned effective optimizations, while on the other hand, maintaining the flexibility of Storm's original programming framework. Furthermore, we present new optimization algorithms to minimize the communication cost and state migration overhead for dynamic load balancing. We implement our framework on top of Storm and run an extensive experimental study to verify its effectiveness.},
  doi       = {10.1145/2933267.2933315},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1051363944},
}

@InProceedings{Lin2015,
  author    = {Lin, Qian and Ooi, Beng Chin and Wang, Zhengkui and Yu, Cui},
  booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
  title     = {Scalable Distributed Stream Join Processing},
  year      = {2015},
  pages     = {811-825},
  abstract  = {Efficient and scalable stream joins play an important role in performing real-time analytics for many cloud applications. However, like in conventional database processing, online theta-joins over data streams are computationally expensive and moreover, being memory-based processing, they impose high memory requirement on the system. In this paper, we propose a novel stream join model, called join-biclique, which organizes a large cluster as a complete bipartite graph. Join-biclique has several strengths over state-of-the-art techniques, including memory-efficiency, elasticity and scalability. These features are essential for building efficient and scalable streaming systems. Based on join-biclique, we develop a scalable distributed stream join system, BiStream, over a large-scale commodity cluster. Specifically, BiStream is designed to support efficient full-history joins, window-based joins and online data aggregation. BiStream also supports adaptive resource management to dynamically scale out and down the system according to its application workloads. We provide both theoretical cost analysis and extensive experimental evaluations to evaluate the efficiency, elasticity and scalability of BiStream.},
  doi       = {10.1145/2723372.2746485},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1026868921},
}

@Article{Lee2015a,
  author   = {Lee, Suk Kyu and Yoo, Seungho and Jung, Jongtack and Kim, Hwangnam and Ryoo, Jihoon},
  journal  = {ACM Transactions on Multimedia Computing Communications and Applications},
  title    = {Link-Aware Reconfigurable Point-to-Point Video Streaming for Mobile Devices},
  year     = {2015},
  number   = {1},
  pages    = {1-25},
  volume   = {12},
  abstract = {Even though people of all social standings use current mobile devices in the wide spectrum of purpose from entertainment tools to communication means, some issues with real-time video streaming in hostile wireless environment still exist. In this article, we introduce CoSA , a link-aware real-time video streaming system for mobile devices. The proposed system utilizes a 3D camera to distinguish the region of importance (ROI) and non-ROI region within the video frame. Based on the link-state feedback from the receiver, the proposed system allocates a higher bandwidth for the region that is classified as ROI and a lower bandwidth for non-ROI in the video stream by reducing the video's bit rate. We implemented CoSA in a real test-bed where the IEEE 802.11 is employed as a medium for wireless networking. Furthermore, we verified the effectiveness of the proposed system by conducting a thorough empirical study. The results indicate that the proposed system enables real-time video streaming while maintaining a consistent visual quality by dynamically reconfiguring video coding parameters according to the link quality.},
  doi      = {10.1145/2771438},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1020294488},
}

@Article{Huang2016,
  author  = {Huang, Rui and Sun, Dawei},
  journal = {International Journal of Wireless and Mobile Computing},
  title   = {Analysing and evaluating topology structure of online application in Big Data stream computing environment},
  year    = {2016},
  number  = {4},
  pages   = {317},
  volume  = {10},
  doi     = {10.1504/ijwmc.2016.078204},
  groups  = {Data Stream Processing Latency},
  url     = {https://app.dimensions.ai/details/publication/pub.1067504856},
}

@InProceedings{Esmaili2011,
  author    = {Esmaili, Kyumars Sheykh and Sanamrad, Tahmineh and Fischer, Peter M. and Tatbul, Nesime},
  booktitle = {Proceedings of the 2011 ACM SIGMOD International Conference on Management of data},
  title     = {Changing flights in mid-air},
  year      = {2011},
  pages     = {613-624},
  abstract  = {Continuous queries can run for unpredictably long periods of time. During their lifetime, these queries may need to be adapted either due to changes in application semantics (e.g., the implementation of a new alert detection policy), or due to changes in the system's behavior (e.g., adapting performance to a changing load). While in previous works query modification has been implicitly utilized to serve specific purposes (e.g., load management), to date no research has been done that defines a general-purpose, reliable, and efficiently implementable model for modifying continuous queries at run-time. In this paper, we introduce a punctuation-based framework that can formally express arbitrary lifecycle operations on the basis of input-output mappings and basic control elements such as start or stop of queries. On top of this foundation, we derive all possible query change methods, each providing different levels of correctness guarantees and performance. We further show how these models can be efficiently realized in a state-of-the-art stream processing engine; we also provide experimental results demonstrating the key performance tradeoffs of the change methods.},
  doi       = {10.1145/1989323.1989388},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1028489924},
}

@Article{Wu2009,
  author   = {Wu, Ji and Tan, Kian-Lee and Zhou, Yongluan},
  journal  = {Information Systems},
  title    = {Data-driven memory management for stream join},
  year     = {2009},
  number   = {4-5},
  pages    = {454-467},
  volume   = {34},
  abstract = {Memory management is a critical issue in stream processing involving stateful operators such as join. Traditionally, the memory requirement for a stream join is query-driven: a query has to explicitly define a window for each (potentially unbounded) input. The window essentially bounds the size of the buffer allocated for that stream. However, output produced this way may not be desirable (if the window size is not part of the intended query semantic) due to the volatile input characteristics. We discover that when streams are ordered or partially ordered, it is possible to use a data-driven memory management scheme to improve the performance. In this work, we present a novel data-driven memory management scheme, called Window-Oblivious Join (WO-Join), which adaptively adjusts the state buffer size according to the input characteristics. Our performance study shows that, compared to traditional Window-Join (W-Join), WO-Join is more robust with respect to the dynamic input and therefore produces higher quality results with lower memory costs.},
  doi      = {10.1016/j.is.2009.02.001},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1017108908},
}

@Article{Budhkar2017,
  author   = {Budhkar, Shilpa and Tamarapalli, Venkatesh},
  journal  = {Journal of Network and Systems Management},
  title    = {Delay Management in Mesh-Based P2P Live Streaming Using a Three-Stage Peer Selection Strategy},
  year     = {2017},
  number   = {2},
  pages    = {401-425},
  volume   = {26},
  abstract = {Peer-to-peer (P2P) live streaming systems have gained popularity due to the self-scalability property of the P2P overlay networks. In P2P live streaming, peers retrieve stream content from other peers in the system. Therefore, peer selection strategy is a fundamental element to build an overlay which manages the playback delay and startup delay experienced by the peers. In this paper, we propose a peer selection strategy which manages to build a minimum delay overlay using three different stages of overlay construction. In the first stage, the tracker suggests some peers as prospective partners to a new peer. In the second stage, the peer selects its partners out of these peers such that delay is minimized. The third stage is the topology adaptation phase of peers, where peers reposition themselves in the overlay to maintain minimum delay during peer churn. In the proposed peer selection strategy, peers are selected in all the stages based on parameters such as propagation delay, upload capacity, buffering duration and buffering level. The proposed strategy is compared with two existing strategies in the literature: Fast-Mesh (Ren et al. in IEEE Trans Multimed 11: 1446, 2009) and Hybrid live p2p streaming protocol (Hammami et al., 2014) using simulations. Our results show that playback delay and startup delay are reduced significantly with the help of proposed strategy. We demonstrate that the stability of the system also improves during peer churn.},
  doi      = {10.1007/s10922-017-9420-5},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1091198101},
}

@InProceedings{Luthra2018,
  author    = {Luthra, Manisha and Koldehofe, Boris and Weisenburger, Pascal and Salvaneschi, Guido and Arif, Raheel},
  booktitle = {Proceedings of the 12th ACM International Conference on Distributed and Event-based Systems},
  title     = {TCEP},
  year      = {2018},
  note      = {https://pure.rug.nl/ws/files/124804170/3210284.3210292.pdf},
  pages     = {136-147},
  abstract  = {Operator placement has a profound impact on the performance of a distributed complex event processing system (DCEP). Since the behavior of a placement mechanism strongly depends on its environment; a single placement mechanism is often not enough to fulfill stringent performance requirements under environmental changes. In this paper, we show how DCEP can benefit from the adaptive use of multiple placement mechanisms. We propose Tcep, a DCEP system to integrate multiple placement mechanisms. By enabling transitions, Tcep can seamlessly exchange distinct operator mechanisms at runtime. We make two main contributions that are highly important for a cost-efficient transition: i) a transition strategy for efficiently scheduling state migrations and ii) a lightweight learning algorithm to adaptively select an appropriate placement mechanism as a consequence of a transition. Our evaluations for important decentralized placement mechanisms in the context of an IoT scenario show that transitions can better fulfill QoS demands in a dynamic environment. Thereby efficient scheduling of state migrations can help to faster complete transitions by up to 94 %.},
  doi       = {10.1145/3210284.3210292},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1105021384},
}

@InProceedings{Kulkarni2008,
  author    = {Kulkarni, Dhananjay and Ravishankar, Chinya V. and Cherniack, Mitch},
  booktitle = {Proceedings of the second international conference on Distributed event-based systems},
  title     = {Real-time, load-adaptive processing of continuous queries over data streams},
  year      = {2008},
  note      = {http://www.cs.ucr.edu/~ravi/Papers/DBConf/DEBS2008_LRM.pdf},
  pages     = {277-288},
  abstract  = {We introduce a new type of query, called a real-time continuous query (RCQ) that captures the real-time requirements of processing data streams. We develop techniques to efficiently process the RCQs in the presence of fluctuating query load and data load. We show that Rate-Monotonic scheduling is applicable to this problem domain, and show how to make this method adaptive to varying load conditions. When a set of queries becomes unschedulable due to load variations, we perform controlled input load shedding by dropping tuples using a novel feedback-based approach to decide which tuples to drop. Our work shows how to provide response time guarantees for processing RCQs, and enables making the appropriate trade-off between penalty due to missed deadlines and result accuracy. Our experiments show that our approach works very well and is usable in practice.},
  doi       = {10.1145/1385989.1386024},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1038952871},
}

@Article{Su2014,
  author   = {Su, Yan and Shi, Feng and Talpur, Shahnawaz and Wang, Yizhuo and Hu, Sensen and Wei, Jin},
  journal  = {Cluster Computing},
  title    = {Achieving self-aware parallelism in stream programs},
  year     = {2014},
  number   = {2},
  pages    = {949-962},
  volume   = {18},
  abstract = {The age of big data open the door to a new approach in data exploration and utilization. With the increasing complexities and dynamics of modern IT systems and services, it has become a challenge to effectively exploit parallelism on multicore platforms in computing systems that are heterogeneous, dynamic and decentralised. Self-aware software is a response to these demands in dealing with distributed applications in changing environments. It is a closed-loop system with a series of optimization strategies to adjust itself dynamicly during data processing. We focus on incorporating adaptation mechanisms into the stream programs for exposing distributed parallelism. In the traditional stream programming models, changing data and status normally require human supervision to adjust the stream graph for performance. As one-time optimization strategy, the reconfiguration and maintenance lead to costly and time-consuming procedures during the operating phase. To address these problems, we propose a self-aware stream programming model called StreamAware. A key property of this model is that exposing self-aware parallelism in the message driven execution paradigm, which provides dynamic and reconfigurable stream graph in adapting to the data flow changes. The model defines the self-awareness loop based on finite state machine for stream applications to adjust their own stream graph with continuous optimization strategy. This paper presents three different self-aware systems built using StreamAware. The empirical evaluation demonstrate how these systems can exploit self-aware parallelism using the Parsec benchmark problems, optimize performance per Watt, and respond to significant changes in stream processing.},
  doi      = {10.1007/s10586-014-0412-x},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1046421221},
}

@InProceedings{Zhang2014,
  author    = {Zhang, Yanwei and Liu, Qing and Klasky, Scott and Wolf, Matthew and Schwan, Karsten and Eisenhauer, Greg and Choi, Jong and Podhorszki, Norbert},
  booktitle = {Proceedings of the first workshop on Parallel programming for analytics applications},
  title     = {Active workflow system for near real-time extreme-scale science},
  year      = {2014},
  pages     = {53-62},
  abstract  = {In recent years, streaming-based data processing has been gaining substantial traction for dealing with overwhelming data generated by real-time applications, from both enterprise sources and scientific computing. In this work, however, we look at an emerging class of scientific data with Near Real-Time (NRT) requirement, in which data is typically generated in a bursty fashion with the near real-time constraints being applied primarily between bursts, rather than within a stream. A key challenge for this types of data sources is that the processing time per data element is not uniform, and not always feasible to predict. Given the observations on the increasing unpredictability of compute load and system dynamics, this work looks to adapt streaming-based approach to the context of this new class of large experiments and simulations that have complex run-time control and analysis issues. In particular, we deploy a novel two-tier scheme for handling the increasing unpredictability of runtime behaviors: Instead of relying on determining what and where to run the scientific workflows beforehand or partial dynamically, the decision will also be adaptively enhanced online according to system runtime status. This is enabled by embedding workflow along with data streams. Specifically, we break data outputs generated from experiments or simulations into multiple self-describing "chunks", which we call active data objects. As such, if there is a transient hotspot observed, a data object with unfinished workflow pipeline can break its previous schedule and search for a least loaded location to continue the execution. Our preliminary experiment results based on synthetic workloads demonstrate the proposed active workflow system as a very promising solution by outperforming the state-of-the-art semi-dynamic workflow schedulers with an improved workflow completion time, as well as a good scalability.},
  doi       = {10.1145/2567634.2567637},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1022055976},
}

@Article{Lim2012,
  author   = {Lim, Lipyeow and Misra, Archan and Mo, Tianli},
  journal  = {Distributed and Parallel Databases},
  title    = {Adaptive data acquisition strategies for energy-efficient, smartphone-based, continuous processing of sensor streams},
  year     = {2012},
  number   = {2},
  pages    = {321-351},
  volume   = {31},
  abstract = {There is a growing interest in applications that utilize continuous sensing of individual activity or context, via sensors embedded or associated with personal mobile devices (e.g., smartphones). Reducing the energy overheads of sensor data acquisition and processing is essential to ensure the successful continuous operation of such applications, especially on battery-limited mobile devices. To achieve this goal, this paper presents a framework, called ACQUA, for ‘acquisition-cost’ aware continuous query processing. ACQUA replaces the current paradigm, where the data is typically streamed (pushed) from the sensors to the one or more smartphones, with a pull-based asynchronous model, where a smartphone retrieves appropriate blocks of relevant sensor data from individual sensors, as an integral part of the query evaluation process. We describe algorithms that dynamically optimize the sequence (for complex stream queries with conjunctive and disjunctive predicates) in which such sensor data streams are retrieved by the query evaluation component, based on a combination of (a) the communication cost & selectivity properties of individual sensor streams, and (b) the occurrence of the stream predicates in multiple concurrently executing queries. We also show how a transformation of a group of stream queries into a disjunctive normal form provides us with significantly greater degrees of freedom in choosing this sequence, in which individual sensor streams are retrieved and evaluated. While the algorithms can apply to a broad category of sensor-based applications, we specifically demonstrate their application to a scenario where multiple stream processing queries execute on a single smartphone, with the sensors transferring their data over an appropriate PAN technology, such as Bluetooth or IEEE 802.11. Extensive simulation experiments indicate that ACQUA’s intelligent batch-oriented data acquisition process can result in as much as 80 % reduction in the energy overhead of continuous query processing, without any loss in the fidelity of the processing logic.},
  doi      = {10.1007/s10619-012-7093-3},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1015987089},
}

@Article{Li2014,
  author   = {Li, Liqun and Xing, Guoliang and Sun, Limin and Liu, Yan},
  journal  = {ACM Transactions on Sensor Networks},
  title    = {A Quality-Aware Voice Streaming System for Wireless Sensor Networks},
  year     = {2014},
  number   = {4},
  pages    = {1-25},
  volume   = {10},
  abstract = {Recent years have witnessed the pilot deployments of audio or low-rate video wireless sensor networks for a class of mission-critical applications including search-and-rescue, security surveillance, and disaster management. In this article, we report the design and implementation of Quality-aware Voice Streaming (QVS) for wireless sensor networks. QVS is built upon SenEar, a new sensor hardware platform we developed for high-bandwidth wireless audio communication. QVS comprises several novel components, which include an empirical model for online voice-quality evaluation and control, dynamic voice compression/duplication adaptation for lossy wireless links, and distributed stream admission control that exploits network capacity for rate allocation. We have extensively tested QVS on a 20-node network deployment. Our experimental results show that QVS delivers satisfactory voice quality under a range of realistic settings while achieving high network capacity utilization.},
  doi      = {10.1145/2594775},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1020410305},
}

@InProceedings{Eskandari2016,
  author    = {Eskandari, Leila and Huang, Zhiyi and Eyers, David},
  booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
  title     = {P-Scheduler},
  year      = {2016},
  pages     = {1-10},
  abstract  = {With ever-accelerating data creation rates in Big Data applications, there is a need for efficient stream processing engines. Apache Storm has been of interest in both academia and industry because of its real-time, distributed, scalable and reliable framework for stream processing. In this paper, we propose an adaptive hierarchical scheduler for the Storm framework, to allocate the resources more efficiently and improve performance. In our method, we consider the data transfer rate and traffic pattern between Storm's tasks and assign highly-communicating task pairs to the same computing node by dynamically employing two phases of graph partitioning. We also calculate the number of required computing nodes in the cluster based on the overall load of the application and use this information to reduce inter-node traffic. Our performance evaluation shows a significant improvement compared to the default scheduler provided by Storm, which evenly distributes the tasks across the cluster, ignoring the communication patterns between them.},
  doi       = {10.1145/2843043.2843056},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1000213201},
}

@Article{Floratou2017,
  author   = {Floratou, Avrilia and Agrawal, Ashvin and Graham, Bill and Rao, Sriram and Ramasamy, Karthik},
  journal  = {Proceedings of the VLDB Endowment},
  title    = {Dhalion},
  year     = {2017},
  number   = {12},
  pages    = {1825-1836},
  volume   = {10},
  abstract = {In recent years, there has been an explosion of large-scale real-time analytics needs and a plethora of streaming systems have been developed to support such applications. These systems are able to continue stream processing even when faced with hardware and software failures. However, these systems do not address some crucial challenges facing their operators: the manual, time-consuming and error-prone tasks of tuning various configuration knobs to achieve service level objectives (SLO) as well as the maintenance of SLOs in the face of sudden, unpredictable load variation and hardware or software performance degradation.  In this paper, we introduce the notion of self-regulating streaming systems and the key properties that they must satisfy. We then present the design and evaluation of Dhalion, a system that provides self-regulation capabilities to underlying streaming systems. We describe our implementation of the Dhalion framework on top of Twitter Heron, as well as a number of policies that automatically reconfigure Heron topologies to meet throughput SLOs, scaling resource consumption up and down as needed. We experimentally evaluate our Dhalion policies in a cloud environment and demonstrate their effectiveness. We are in the process of open-sourcing our Dhalion policies as part of the Heron project.},
  doi      = {10.14778/3137765.3137786},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1091641317},
}

@Article{Mencagli2018,
  author   = {Mencagli, Gabriele and Torquati, Massimo and Danelutto, Marco},
  journal  = {Future Generation Computer Systems},
  title    = {Elastic-PPQ: A two-level autonomic system for spatial preference query processing over dynamic data streams},
  year     = {2018},
  note     = {https://arpi.unipi.it/bitstream/11568/875747/2/preprint-fgcs-2017-2.pdf},
  pages    = {862-877},
  volume   = {79},
  abstract = {Paradigms like Internet of Things and the most recent Internet of Everything are shifting the attention towards systems able to process unbounded sequences of items in the form of data streams. In the real world, data streams may be highly variable, exhibiting burstiness in the arrival rate and non-stationarities such as trends and cyclic behaviors. Furthermore, input items may be not ordered according to timestamps. This raises the complexity of stream processing systems, which must support elastic resource management and autonomic QoS control through sophisticated strategies and run-time mechanisms. In this paper we present Elastic-PPQ, a system for processing spatial preference queries over dynamic data streams. The key aspect of the system design is the existence of two adaptation levels handling workload variations at different time-scales. To address fast time-scale variations we design a fine regulatory mechanism of load balancing supported by a control-theoretic approach. The logic of the second adaptation level, targeting slower time-scale variations, is incorporated in a Fuzzy Logic Controller that makes scale in/out decisions of the system parallelism degree. The approach has been successfully evaluated under synthetic and real-world datasets.},
  doi      = {10.1016/j.future.2017.09.004},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1091508847},
}

@InBook{LePhuoc2011,
  author    = {Le-Phuoc, Danh and Dao-Tran, Minh and Xavier Parreira, Josiane and Hauswirth, Manfred},
  pages     = {370-388},
  title     = {A Native and Adaptive Approach for Unified Processing of Linked Streams and Linked Data},
  year      = {2011},
  note      = {https://link.springer.com/content/pdf/10.1007/978-3-642-25073-6_24.pdf},
  abstract  = {In this paper we address the problem of scalable, native and adaptive query processing over Linked Stream Data integrated with Linked Data. Linked Stream Data consists of data generated by stream sources, e.g., sensors, enriched with semantic descriptions, following the standards proposed for Linked Data. This enables the integration of stream data with Linked Data collections and facilitates a wide range of novel applications. Currently available systems use a “black box” approach which delegates the processing to other engines such as stream/event processing engines and SPARQL query processors by translating to their provided languages. As the experimental results described in this paper show, the need for query translation and data transformation, as well as the lack of full control over the query execution, pose major drawbacks in terms of efficiency. To remedy these drawbacks, we present CQELS (Continuous Query Evaluation over Linked Streams), a native and adaptive query processor for unified query processing over Linked Stream Data and Linked Data. In contrast to the existing systems, CQELS uses a “white box” approach and implements the required query operators natively to avoid the overhead and limitations of closed system regimes. CQELS provides a flexible query execution framework with the query processor dynamically adapting to the changes in the input data. During query execution, it continuously reorders operators according to some heuristics to achieve improved query execution in terms of delay and complexity. Moreover, external disk access on large Linked Data collections is reduced with the use of data encoding and caching of intermediate query results. To demonstrate the efficiency of our approach, we present extensive experimental performance evaluations in terms of query execution time, under varied query types, dataset sizes, and number of parallel queries. These results show that CQELS outperforms related approaches by orders of magnitude.},
  booktitle = {The Semantic Web – ISWC 2011},
  doi       = {10.1007/978-3-642-25073-6_24},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1011592440},
}

@InProceedings{Caneill2016,
  author    = {Caneill, Matthieu and Rheddane, Ahmed El and Leroy, Vincent and De Palma, Noël},
  booktitle = {Proceedings of the 17th International Middleware Conference},
  title     = {Locality-Aware Routing in Stateful Streaming Applications},
  year      = {2016},
  note      = {https://hal.inria.fr/hal-01407457/file/storm-locality-middleware-2016.pdf},
  pages     = {1-13},
  abstract  = {Distributed stream processing engines continuously execute series of operators on data streams. Horizontal scaling is achieved by deploying multiple instances of each operator in order to process data tuples in parallel. As the application is distributed on an increasingly high number of servers, the likelihood that the stream is sent to a different server for each operator increases. This is particularly important in the case of stateful applications that rely on keys to deterministically route messages to a specific instance of an operator. Since network is a bottleneck for many stream applications, this behavior significantly degrades their performance. Our objective is to improve stream locality for stateful stream processing applications. We propose to analyse traces of the application to uncover correlations between the keys used in successive routing operations. By assigning correlated keys to instances hosted on the same server, we significantly reduce network consumption and increase performance while preserving load balance. Furthermore, this approach is executed online, so that the assignment can automatically adapt to changes in the characteristics of the data. Data migration is handled seamlessly with each routing configuration update. We implemented and evaluated our protocol using Apache Storm, with a real workload consisting of geo-tagged Flickr pictures as well as Twitter publications. Our results show a significant improvement in throughput.},
  doi       = {10.1145/2988336.2988340},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1027575106},
}

@InProceedings{Chen2014,
  author    = {Chen, Shuo and Li, Xiaoming},
  booktitle = {Proceedings of the 28th ACM international conference on Supercomputing},
  title     = {Input-adaptive parallel sparse fast fourier transform for stream processing},
  year      = {2014},
  pages     = {93-102},
  abstract  = {Fast Fourier Transform (FFT) is frequently invoked in stream processing, e.g., calculating the spectral representation of audio/video frames, and in many cases the inputs are sparse, i.e., most of the inputs' Fourier coefficients being zero. Many sparse FFT algorithms have been proposed to improve FFT's efficiency when inputs are known to be sparse. However, like their "dense" counterparts, existing sparse FFT implementations are input oblivious in the sense that how the algorithms work is not affected by the value of input. The sparse FFT computation on one frame is exactly the same as the computation on the next frame. This paper improves upon existing sparse FFT algorithms by simultaneously exploiting the input sparsity and the similarity between adjacent inputs in stream processing. Our algorithm detects and takes advantage of the similarity between input samples to automatically design and customize sparse filters that lead to better parallelism and performance. More specifically, we develop an efficient heuristic to detect the similarity between the current input to its predecessor in stream processing, and when it is found to be similar, we novelly use the spectral representation of the predecessor to accelerate the sparse FFT computation on the current input. Given a sparse signal that has only $k$ non-zero Fourier coefficients, our algorithm utilizes sparse approximation by tuning several adaptive filters to efficiently package the non-zero Fourier coefficients into a small number of bins which can then be estimated accurately. Therefore, our algorithm has runtime sub-linear to the input size and gets rid of recursive coefficient estimation, both of which improve parallelism and performance. Furthermore, the new heuristic can detect the discontinuities inside the streams and resumes the input adaptation very quickly. We evaluate our input-adaptive sparse FFT implementation on Intel i7 CPU and three NVIDIA GPUs, i.e., NVIDIA GeForce GTX480, Tesla C2070 and Tesla C2075. Our algorithm is faster than previous FFT implementations both in theory and implementation. For inputs with size N=2^{24}, our parallel implementation outperforms FFTW for k up to 2^{18}, which is an order of magnitude higher than prior sparse algorithms. Furthermore, our input adaptive sparse FFT on Tesla C2075 GPU achieves up to 77.2x and 29.3x speedups over 1-thread and 4-thread FFTW, 10.7x, 6.4x, 5.2x speedups against sFFT 1.0, sFFT 2.0, CUFFT, and 6.9x speedup over our sequential CPU performance, respectively.},
  doi       = {10.1145/2597652.2597669},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1019059277},
}

@Article{Mencagli2016,
  author   = {Mencagli, Gabriele},
  journal  = {ACM Transactions on Autonomous and Adaptive Systems},
  title    = {A Game-Theoretic Approach for Elastic Distributed Data Stream Processing},
  year     = {2016},
  note     = {https://arpi.unipi.it/bitstream/11568/794461/6/Preprint-TAAS-2016.pdf},
  number   = {2},
  pages    = {1-34},
  volume   = {11},
  abstract = {Distributed data stream processing applications are structured as graphs of interconnected modules able to ingest high-speed data and to transform them in order to generate results of interest. Elasticity is one of the most appealing features of stream processing applications. It makes it possible to scale up/down the allocated computing resources on demand in response to fluctuations of the workload. On clouds, this represents a necessary feature to keep the operating cost at affordable levels while accommodating user-defined QoS requirements. In this article, we study this problem from a game-theoretic perspective. The control logic driving elasticity is distributed among local control agents capable of choosing the right amount of resources to use by each module. In a first step, we model the problem as a noncooperative game in which agents pursue their self-interest. We identify the Nash equilibria and we design a distributed procedure to reach the best equilibrium in the Pareto sense. As a second step, we extend the noncooperative formulation with a decentralized incentive-based mechanism in order to promote cooperation by moving the agreement point closer to the system optimum. Simulations confirm the results of our theoretical analysis and the quality of our strategies.},
  doi      = {10.1145/2903146},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1002873060},
}

@InProceedings{Nehme2009,
  author    = {Nehme, Rimma V. and Rundensteiner, Elke A. and Bertino, Elisa},
  booktitle = {Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology},
  title     = {Self-tuning query mesh for adaptive multi-route query processing},
  year      = {2009},
  note      = {https://dl.acm.org/doi/pdf/10.1145/1516360.1516452},
  pages     = {803-814},
  abstract  = {In real-life applications, different subsets of data may have distinct statistical properties, e.g., various websites may have diverse visitation rates, different categories of stocks may have dissimilar price fluctuation patterns. For such applications, it can be fruitful to eliminate the commonly made single execution plan assumption and instead execute a query using several plans, each optimally serving a subset of data with particular statistical properties. Furthermore, in dynamic environments, data properties may change continuously, thus calling for adaptivity. The intriguing question is: can we have an execution strategy that (1) is plan-based to leverage on all the benefits of traditional plan-based systems, (2) supports multiple plans each customized for different subset of data, and yet (3) is as adaptive as "plan-less" systems like Eddies? While the recently proposed Query Mesh (QM) approach provides a foundation for such an execution paradigm, it does not address the question of adaptivity required for highly dynamic environments. In this work, we fill this gap by proposing a Self-Tuning Query Mesh (ST-QM) --- an adaptive solution for content-based multi-plan execution engines. ST-QM addresses adaptive query processing by abstracting it as a concept drift problem --- a well-known subject in machine learning. Such abstraction allows to discard adaptivity candidates (i.e., the cases indicating a change in the environment) early in the process if they are insignificant or not "worthwhile" to adapt to, and thus minimize the adaptivity overhead. A unique feature of our aproach is that all logical transformations to the execution strategy get translated into a single inexpensive physical operation --- the classifier change. Our experimental evaluation using a continuous query engine shows the performance benefits of ST-QM approach over the alternatives, namely the non-adaptive and the Eddies-based solutions.},
  doi       = {10.1145/1516360.1516452},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1003323638},
}

@Article{Pham2015,
  author   = {Pham, Thao N. and Chrysanthis, Panos K. and Labrinidis, Alexandros},
  journal  = {The VLDB Journal},
  title    = {Avoiding class warfare: managing continuous queries with differentiated classes of service},
  year     = {2015},
  number   = {2},
  pages    = {197-221},
  volume   = {25},
  abstract = {Data stream management systems (DSMSs) offer the most effective solution for processing data streams by efficiently executing continuous queries (CQs) over the incoming data. CQs inherently have different levels of criticality and hence different levels of expected quality of service (QoS) and quality of data (QoD). Adhering to such expected QoS/QoD metrics is even more important in cases of multi-tenant data stream management services. In this work, we propose DILoS, a framework that, through priority-based scheduling and load shedding, supports differentiated QoS and QoD for multiple classes of CQs. Unlike existing works that consider scheduling and load shedding separately, DILoS is a novel unified framework that exploits the synergy between scheduling and load shedding. We also propose ALoMa, a general, adaptive load manager that DILoS is built upon. By its design, ALoMa performs better than the state-of-the-art alternatives in three dimensions: (1) it automatically tunes the headroom factor, (2) it honors the delay target, (3) it is applicable to complex query networks with shared operators. We implemented DILoS and ALoMa in our real DSMS prototype system (AQSIOS) and evaluate their performance for a variety of real and synthetic workloads. Our experimental evaluation of ALoMa verified its clear superiority over the state-of-the-art approaches. Our experimental evaluation of the DILoS framework showed that it (a) allows the scheduler and load shedder to consistently honor CQs’ priorities, (b) significantly increases system capacity utilization by exploiting batch processing, and (c) enables operator sharing among query classes of different priorities while avoiding priority inversion, i.e., a lower-priority class never blocks a higher-priority one.},
  doi      = {10.1007/s00778-015-0411-4},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1009920573},
}

@InProceedings{Schneider2016,
  author    = {Schneider, Scott and Wolf, Joel and Hildrum, Kirsten and Khandekar, Rohit and Wu, Kun-Lung},
  booktitle = {Proceedings of the 17th International Middleware Conference},
  title     = {Dynamic Load Balancing for Ordered Data-Parallel Regions in Distributed Streaming Systems},
  year      = {2016},
  pages     = {1-14},
  abstract  = {Distributed stream computing has emerged as a technology that can satisfy the low latency, high throughput demands of big data. Stream computing naturally exposes pipeline, task and data parallelism. Meeting the throughput and latency demands of online big data requires exploiting such parallelism across heterogeneous clusters. When a single job is running on a homogeneous cluster, load balancing is important. When multiple jobs are running across a heterogeneous cluster, load balancing becomes critical. The data parallel regions of distributed streaming applications are particularly sensitive to load imbalance, as their overall speed is gated by the slowest performer. We propose a dynamic load balancing technique based on a system artifact: the TCP blocking rate per connection. We build a function for each connection based on this blocking rate, and obtain a balanced load distribution by modeling the problem as a minimax separable resource allocation problem. In other words, we minimize the maximum value of these functions. Our model achieves local load balancing that does not require any global information. We test our model in a real streaming system, and demonstrate that it is able to detect differences in node capacities, determine the correct load distribution for those capacities and dynamically adapt to changes in the system.},
  doi       = {10.1145/2988336.2990475},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1023581144},
}

@Article{Zhang2016,
  author   = {Zhang, Weishan and Duan, Pengcheng and Xie, Xiaodan and Xia, Feng and Lu, Qinghua and Liu, Xin and Zhou, Jiehan},
  journal  = {Personal and Ubiquitous Computing},
  title    = {QoS4IVSaaS: a QoS management framework for intelligent video surveillance as a service},
  year     = {2016},
  number   = {5},
  pages    = {795-808},
  volume   = {20},
  abstract = {Quality of service (QoS) is critical for real-time intelligent video surveillance as a service (IVSaaS) platform, which is both computation intensive and data intensive by nature. However, there is scarce work on a QoS framework for IVSaaS platform. In this paper, we propose QoS for intelligent video surveillance as a service, a QoS framework to make computing resources highly available. In the framework, multiple metrics such as throughput, loads of CPU/GPU, memory and IO are taken into account with different time series models to enhance the adaptivity of different video services. A model selection algorithm is proposed to choose the model that achieves the best performance under various error indicators. At the same time, a resource abnormality detection algorithm is designed to detect anomalies when a service is underperformed. Evaluation results show that the proposed QoS framework can successfully ensure QoS by dynamically scheduling computing resources.},
  doi      = {10.1007/s00779-016-0945-5},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1021765870},
}

@Article{Birke2012,
  author   = {Birke, Robert and Kiraly, Csaba and Leonardi, Emilio and Mellia, Marco and Meo, Michela and Traverso, Stefano},
  journal  = {Computer Communications},
  title    = {A delay-based aggregate rate control for P2P streaming systems},
  year     = {2012},
  note     = {https://iris.polito.it/bitstream/11583/2502292/2/HRC-COMCOM12.pdf},
  number   = {18},
  pages    = {2237-2244},
  volume   = {35},
  abstract = {In this paper we consider mesh based P2P streaming systems focusing on the problem of regulating peer transmission rate to match the system demand while not overloading each peer upload link capacity. We propose Hose Rate Control (HRC), a novel scheme to control the speed at which peers offer chunks to other peers, ultimately controlling peer uplink capacity utilization. This is of critical importance for heterogeneous scenarios like the one faced in the Internet, where peer upload capacity is unknown and varies widely.HRC nicely adapts to the actual peer available upload bandwidth and system demand, so that Quality of Experience is greatly enhanced. To support our claims we present both simulations and actual experiments involving more than 1000 peers to assess performance in real scenarios. Results show that HRC consistently outperforms the Quality of Experience achieved by non-adaptive schemes.},
  doi      = {10.1016/j.comcom.2012.07.005},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1027453968},
}

@Article{Atzori2012,
  author   = {Atzori, Luigi and Floris, Alessandro and Ginesu, Giaime and Giusto, Daniele},
  journal  = {Signal Processing Image Communication},
  title    = {Streaming video over wireless channels: Exploiting reduced-reference quality estimation at the user-side},
  year     = {2012},
  number   = {10},
  pages    = {1049-1065},
  volume   = {27},
  abstract = {We propose a source rate control scheme for streaming video sequences over wireless channels by resorting on a reduced-reference (RR) quality estimation approach. It works as follows: the server extracts important features of the original video, which are coded and sent through the channel along with the video sequence and then exploited at the decoder to compute the actual quality; the observed quality is analyzed to obtain information on the impact of the source rate at the given system configuration; at the receiver, decisions are taken on the optimal source rate to be applied next at the encoder to maximize the quality as perceived at the user-side. The rate is adjusted on a per-window basis to compensate low-throughput periods with high-throughput periods so as to avoid abrupt video quality changes, which can be caused by sudden variations in the channel throughput. The use of the RR quality estimation represents the main novelty of the proposed work. This has the advantage of allowing the rate control to optimize the user-perceived video quality after all the streaming system impairments have affected the signal, including actual channel errors, playback buffer starvation occurrences and error concealment. This approach is new in this context, since in the past proposals video models are used to predict the relationships of the quality with the coding rate, channel errors and starvation occurrences. Numerical simulations show how the proposed approach is able to achieve results similar to those obtained with model-based approaches, but with the significant benefit of not requiring any knowledge on the signal and channel characteristics.},
  doi      = {10.1016/j.image.2012.09.005},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1019477087},
}

@Article{Hidalgo2017,
  author   = {Hidalgo, Nicolas and Wladdimiro, Daniel and Rosas, Erika},
  journal  = {Journal of Systems and Software},
  title    = {Self-adaptive processing graph with operator fission for elastic stream processing},
  year     = {2017},
  pages    = {205-216},
  volume   = {127},
  abstract = {Nowadays, information generated by the Internet interactions is growing exponentially, creating massive and continuous flows of events from the most diverse sources. These interactions contain valuable information for domains such as government, commerce, and banks, among others. Extracting information in near real-time from such data requires powerful processing tools to cope with the high-velocity and the high-volume stream of events. Specially designed distributed processing engines build a graph-based topology of a static number of processing operators creating bottlenecks and load balance problems when processing dynamic flows of events. In this work we propose a self-adaptive processing graph that provides elasticity and scalability by automatically increasing or decreasing the number of processing operators to improve performance and resource utilization of the system. Our solution uses a model that monitors, analyzes and changes the graph topology with a control algorithm that is both reactive and proactive to the flow of events. We have evaluated our solution with three stream processing applications and results show that our model can adapt the graph topology when receiving events at high rate with sudden peaks, producing very low costs of memory and CPU usage.},
  doi      = {10.1016/j.jss.2016.06.010},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1009990370},
}

@Article{Li2010a,
  author   = {Li, Zhenhua and Cao, Jiannong and Chen, Guihai and Liu, Yan},
  journal  = {Journal of Parallel and Distributed Computing},
  title    = {On the source switching problem of Peer-to-Peer streaming},
  year     = {2010},
  number   = {5},
  pages    = {537-546},
  volume   = {70},
  abstract = {Peer-to-Peer(P2P) streaming has been proved a popular and efficient paradigm of Internet media streaming. In some applications, such as an Internet video distance education system, there are multiple media sources which work alternately. A fundamental problem in designing such kind of P2P streaming system is how to achieve fast source switching so that the startup delay of the new source can be minimized. In this paper, we propose an efficient solution to this problem. We model the source switch process, formulate it into an optimization problem and derive its theoretical optimal solution. Then we propose a practical greedy algorithm, named fast source switch algorithm, which approximates the optimal solution by properly interleaving the data delivery of different media sources. The algorithm can adapt to the dynamics and heterogeneity of real Internet environments. We have carried out extensive simulations on various real-trace P2P overlay topologies to demonstrate the effectiveness of our model and algorithm. The simulation results show that our proposed algorithm outperforms the normal source switch algorithm by reducing the source switch time by 20%–30% without bringing extra communication overhead. The reduction in source switching time is more obvious as the network scale increases.},
  doi      = {10.1016/j.jpdc.2010.01.005},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1002953010},
}

@Article{Liu2006,
  author   = {Liu, Yunqiang and Yu, Songyu and Zhou, Jun},
  journal  = {Computer Communications},
  title    = {Adaptive segment-based patching scheme for video streaming delivery system},
  year     = {2006},
  number   = {11},
  pages    = {1889-1895},
  volume   = {29},
  abstract = {In on-demand video streaming system, periodic broadcast technique scheme has been shown to be very effective for serving a popular video in reducing the demand on server bandwidth. On the contrary, reactive server transmission approach is more suitable for the video that is not popular enough. However, the level of demand on a video may change by time. In this paper, we propose a segment-based patching scheme which allocates adaptively transmission resources according to the varying client request rate. Our technique smoothly adjusts itself to cope with the changing workloads. The scheme tries to dynamically search the optimal number of channels assigned to the video by the newly updated request rate so as to minimize the bandwidth requirement. We also show how to seamlessly perform the transition of changing the number of channels with the guarantee that the clients viewing this video will not experience any disruption. Simulation results indicate that the scheme adapts very well to the changing client request rate and improves the system performance significantly in terms of the total server bandwidth requirement.},
  doi      = {10.1016/j.comcom.2005.10.036},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1050319886},
}

@Article{Moon2016,
  author   = {Moon, Seonghoon and Yoo, Juwan and Kim, Songkuk},
  journal  = {Future Generation Computer Systems},
  title    = {Adaptive interface selection over cloud-based split-layer video streaming via multi-wireless networks},
  year     = {2016},
  pages    = {664-674},
  volume   = {56},
  abstract = {As mobile devices such as tablet PCs and smartphones proliferate, the online video consumption over a wireless network has been accelerated. From this phenomenon, there are several challenges to provide the video streaming service more efficiently and stably in the heterogeneous mobile environment. In order to guarantee the QoS of real-time HD video services, the steady and reliable wireless mesh is necessary. Furthermore, the video service providers have to maintain the QoS by provisioning streaming servers to respond the clients’ request of different video resolution. In this paper, we propose a reliable cloud-based video delivery scheme with the split-layer SVC encoding and real-time adaptive multi-interface selection over LTE and WiFi links. A split-layer video streaming can effectively scale to manage the required channels on each layer of various client connections. Moreover, split-layer SVC model brings streaming service providers a remarkable opportunity to stream video over multiple interfaces (e.g. WiFi, LTE, etc.) with a separate controlling based on their network status. Through the adaptive interface selection, the proposed system aims to ensure the maximizing video quality which the bandwidth of LTE/WiFi accommodates. In addition, the system offers cost-effective streaming to mobile clients by saving the LTE data consumption. In our system, an adaptive interface selection is developed with two different algorithms, such as INSTANT and EWMA methods. We implemented a prototype of mobile client based on iOS particularly by using iPhone5S. Moreover, we also employ the split-layer SVC encodes in streaming server-side as the add-on module to SVC reference encoding tool in a virtualized environment of KVM hypervisor. We evaluated the proposed system in an emulated and a real-world heterogeneous wireless network environments. The results show that the proposed system not only achieves to guarantee the highest quality of video frames via WiFi and LTE simultaneous connection, but also efficiently saves LTE bandwidth consumption for cost-effectiveness to client-side. Our proposed method provides the highest video quality without deadline misses, while it consumes 50.6% LTE bandwidth of ‘LTE-only’ method and 72.8% of the conventional (non-split) SVC streaming over a real-world mobile environment.},
  doi      = {10.1016/j.future.2015.09.022},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1045321562},
}

@Article{Pozueco2013,
  author   = {Pozueco, Laura and Pañeda, Xabiel García and García, Roberto and Melendi, David and Cabrero, Sergio},
  journal  = {Computers & Electrical Engineering},
  title    = {Adaptable system based on Scalable Video Coding for high-quality video service},
  year     = {2013},
  number   = {3},
  pages    = {775-789},
  volume   = {39},
  abstract = {Content adaptation to a heterogeneous environment like the Internet is a key process for improving the perceived quality of the user. This paper presents an adaptive streaming system using Scalable Video Coding (SVC) technology. Using feedback information from clients about the transmission status, the server is able to select the most suitable combination of SVC layers for the available bandwidth. The estimation of the available bandwidth is carried out with non-intrusive methods, based on classic metrics such as packet loss, jitter and novel metrics like the linearity of reception times of RTP packets. The system is implemented in real equipment and the results show the correct operation and the accuracy of the system when adapting to different variations of the available bandwidth. We also study the scalability of the system when several clients access the service simultaneously, demonstrating that our system is as scalable as a non-adaptive system with SVC.},
  doi      = {10.1016/j.compeleceng.2013.01.015},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1043780503},
}

@Article{Works2015,
  author   = {Works, Karen and Rundensteiner, Elke A.},
  journal  = {Big Data Research},
  title    = {Practical Identification of Dynamic Precedence Criteria to Produce Critical Results from Big Data Streams},
  year     = {2015},
  number   = {4},
  pages    = {127-144},
  volume   = {2},
  abstract = {During periods of high volume, big data stream applications may not have enough resources to process all incoming tuples. To maximize the production of the most critical results under such resource shortages, a recent solution, PR (short for Preferential Result), utilizes both static criteria (defined at compile-time) and dynamic criteria (identified online at run-time) to prioritize the processing of tuples throughout the query pipeline. Unfortunately, locating the optimal criteria placement (i.e., where in the query pipeline to evaluate each prioritization criteria) is extremely compute-intensive and runs in exponential time. This makes PR impractical for complex big data stream systems. Our proposed criteria selection and placement approach, PR-Prune (short for Preferential Result-Pruning), is practical. PR-Prune prunes ineffective dynamic criteria and combines multiple criteria along the same pipeline. To achieve this, PR-Prune seeks to expand the duration in the query pipeline that tuples identified as critical are pulled forward. Our experiments use a real data stream from the S&P 500 stocks, synthetic data streams, and a diverse set of queries. The results substantiate that PR-Prune increases the production of the most critical results compared to the state-of-the-art approaches. In addition, PR-Prune significantly lowers the optimization search time compared to PR.},
  doi      = {10.1016/j.bdr.2015.09.001},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1047534361},
}

@Article{Yuan2013,
  author   = {Yuan, Xiaoqun and Min, Geyong and Ding, Yi and Liu, Qiong and Liu, Jinhong and Yin, Hao and Fang, Qing},
  journal  = {Future Generation Computer Systems},
  title    = {Adaptive resource management for P2P live streaming systems},
  year     = {2013},
  number   = {6},
  pages    = {1573-1582},
  volume   = {29},
  abstract = {Peer-to-Peer (P2P) has become a popular live streaming delivery technology owing to its scalability and low cost. P2P streaming systems often employ multi-channels to deliver streaming to users simultaneously, which leads to a great challenge of allocating server resources among these channels appropriately. Most existing P2P systems resort to over-allocating server resources to different channels, which results in low-efficiency and high-cost. To allocate server resources to different channels efficiently, we propose a dynamic resource allocation algorithm based on a streaming quality model for P2P live streaming systems. This algorithm can improve the channel streaming quality for multi-channel P2P live streaming system and also guarantees the streaming quality of the channels under extreme Internet conditions. In an experiment, the proposed algorithm is validated by the trace data.},
  doi      = {10.1016/j.future.2012.09.002},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1009745429},
}

@Article{Huang2016a,
  author   = {Huang, Qun and Lee, Patrick P. C.},
  journal  = {Proceedings of the VLDB Endowment},
  title    = {Toward high-performance distributed stream processing via approximate fault tolerance},
  year     = {2016},
  number   = {3},
  pages    = {73-84},
  volume   = {10},
  abstract = {Fault tolerance is critical for distributed stream processing systems, yet achieving error-free fault tolerance often incurs substantial performance overhead. We present
                    AF-Stream
                    , a distributed stream processing system that addresses the trade-off between performance and accuracy in fault tolerance. AF-Stream builds on a notion called
                    approximate fault tolerance
                    , whose idea is to mitigate backup overhead by adaptively issuing backups, while ensuring that the errors upon failures are bounded with theoretical guarantees. Our AF-Stream design provides an extensible programming model for incorporating general streaming algorithms, and also exports only few threshold parameters for configuring approximation fault tolerance. Experiments on Amazon EC2 show that AF-Stream maintains high performance (compared to no fault tolerance) and high accuracy after multiple failures (compared to no failures) under various streaming algorithms.},
  doi      = {10.14778/3021924.3021925},
  groups   = {Data Stream Processing Latency},
  url      = {https://app.dimensions.ai/details/publication/pub.1087132733},
}

@InProceedings{Fang2017,
  author    = {Fang, Junhua and Zhang, Rong and Fu, Tom Z.J. and Zhang, Zhenjie and Zhou, Aoying and Zhu, Junhua},
  booktitle = {Proceedings of the 26th International Symposium on High-Performance Parallel and Distributed Computing},
  title     = {Parallel Stream Processing Against Workload Skewness and Variance},
  year      = {2017},
  note      = {https://arxiv.org/pdf/1610.05121},
  pages     = {15-26},
  abstract  = {Key-based workload partitioning is a common strategy used in parallel stream processing engines, enabling effective key-value tuple distribution over worker threads in a logical operator. It is likely to generate poor balancing performance when workload variance occurs on the incoming data stream. This paper presents a new key-based workload partitioning framework, with practical algorithms to support dynamic workload assignment for stateful operators. The framework combines hash-based and explicit key-based routing strategies for workload distribution, which specifies the destination worker threads for a handful of keys and assigns the other keys with the hash function. When short-term distribution fluctuations occur to the incoming data stream, the system adaptively updates the routing table containing the chosen keys, in order to rebalance the workload with minimal migration overhead within the stateful operator. We formulate the rebalance operation as an optimization problem, with multiple objectives on minimizing state migration costs, controlling the size of the routing table and breaking workload imbalance among worker threads. Despite of the NP-hardness nature behind the optimization formulation, we carefully investigate and justify the heuristics behind key (re)routing and state migration, to facilitate fast response to workload variance with ignorable cost to the normal processing in the distributed system. Empirical studies on synthetic data and real-world stream applications validate the usefulness of our proposals.},
  doi       = {10.1145/3078597.3078613},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1090317520},
}

@InProceedings{Hosseini2018,
  author    = {Hosseini, Mohammad and Timmerer, Christian},
  booktitle = {Proceedings of the 23rd Packet Video Workshop},
  title     = {Dynamic Adaptive Point Cloud Streaming},
  year      = {2018},
  pages     = {25-30},
  abstract  = {High-quality point clouds have recently gained interest as an emerging form of representing immersive 3D graphics. Unfortunately, these 3D media are bulky and severely bandwidth intensive, which makes it difficult for streaming to resource-limited and mobile devices. This has called researchers to propose efficient and adaptive approaches for streaming of high-quality point clouds. In this paper, we run a pilot study towards dynamic adaptive point cloud streaming, and extend the concept of dynamic adaptive streaming over HTTP (DASH) towards DASH-PC, a dynamic adaptive bandwidth-efficient and view-aware point cloud streaming system. DASH-PC can tackle the huge bandwidth demands of dense point cloud streaming while at the same time can semantically link to human visual acuity to maintain high visual quality when needed. In order to describe the various quality representations, we propose multiple thinning approaches to spatially sub-sample point clouds in the 3D space, and design a DASH Media Presentation Description manifest speci.c for point cloud streaming. Our initial evaluations show that we can achieve signi.cant bandwidth and performance improvement on dense point cloud streaming with minor negative quality impacts compared to the baseline scenario when no adaptations is applied.},
  doi       = {10.1145/3210424.3210429},
  groups    = {Data Stream Processing Latency},
  url       = {https://app.dimensions.ai/details/publication/pub.1105056307},
}

@Article{Adamo2021,
  author   = {Adamo, Greta and Ghidini, Chiara and Di Francescomarino, Chiara},
  journal  = {Software and Systems Modeling},
  title    = {What is a process model composed of?},
  year     = {2021},
  number   = {4},
  pages    = {1215-1243},
  volume   = {20},
  abstract = {Business process modelling languages typically enable the representation of business process models by employing (graphical) symbols. These symbols can vary depending upon the verbosity of the language, the modelling paradigm, the focus of the language and so on. To make explicit different constructs and rules employed by a specific language, as well as bridge the gap across different languages, meta-models have been proposed in the literature. These meta-models are a crucial source of knowledge on what state-of-the-art literature considers relevant to describe business processes. The goal of this work is to provide the first extensive systematic literature review (SLR) of business process meta-models. This SLR aims to answer research questions concerning: (1) the kind of meta-models proposed in the literature, (2) the recurring constructs they contain, (3) their purposes and (4) their evaluations. The SRL was performed manually considering papers automatically retrieved from reference paper repositories as well as proceedings of the main conferences in the Business Process Management research area. Sixty-five papers were selected and evaluated against four research questions. The results indicate the existence of a reasonable body of work conducted in this specific area, but not a full maturity. In particular, in answering the research questions several challenges have (re-)emerged for the Business Process Community, concerning: (1) the type of elements that constitute a Business Process and their meaning, (2) the absence of a (or several) reference meta-model(s) for the community, (3) the purpose for which meta-models are introduced in the literature and (4) a framework for the evaluation of the meta-models themselves. Moreover, the classification framework devised to answer the four research questions can provide a reference structure for future descriptive categorizations.},
  doi      = {10.1007/s10270-020-00847-w},
  groups   = {Business Process Meta Models},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1134463877},
}

@InProceedings{Bernardi2012,
  author    = {Bernardi, Mario Luca and Cimitile, Marta and Di Lucca, Giuseppe Antonio and Maggi, Fabrizio M.},
  booktitle = {Proceedings of the twelfth international workshop on Web information and data management},
  title     = {M3D},
  year      = {2012},
  pages     = {73-80},
  abstract  = {Nowadays, Web Applications (WAs) are complex software systems, used by multiple users with different roles and often developed to support and manage business processes. Due to the changing nature of the supported processes, WAs need to be easily and quickly modified, to adapt and align them to the processes they support. In recent years, Model Driven Engineering (MDE) approaches have been proposed and used to develop and evolve WAs. However, the definition of appropriate MDE approaches for the development of flexible process-centric WAs is still limited. In particular, (flexible) workflow models have never been integrated with the models (e.g., presentation, information models) used in MDE approaches to develop this type of applications. In this paper, we present M3D (Model Driven Development with Declare), a tool for developing WAs that integrates three MDE metamodels used to represent the main components of a WA with the metamodel of Declare, a declarative language to model business processes. The tool exploits and combines the declarative nature of Declare and the advantages of MDE to get an efficient roundtrip engineering support to develop and evolve flexible process-centric WAs.},
  doi       = {10.1145/2389936.2389951},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1006491994},
}

@Article{Kwan2003,
  author   = {Kwan, M. Millie and Balasubramanian, P.},
  journal  = {Decision Support Systems},
  title    = {KnowledgeScope: managing knowledge in context},
  year     = {2003},
  number   = {4},
  pages    = {467-486},
  volume   = {35},
  abstract = {Knowledge repositories have been implemented in many organizations, but they often suffer from non-use. This research considers two key design factors that cause non-use: the extra burden on users to document knowledge in the repository, and the lack of a standard knowledge structure that facilitates knowledge sharing among users with different perspectives. We propose a design of a knowledge management system called KnowledgeScope that addresses these problems through (1) an integrated workflow support capability that captures and retrieves knowledge as an organizational process proceeds, i.e., within the context in which it is created and used, and (2) a process meta-model that organizes that knowledge and context in a knowledge repository. In this paper, we describe this design and report the results from implementing it in a real-life organization.},
  doi      = {10.1016/s0167-9236(02)00126-4},
  groups   = {Business Process Meta Models},
  url      = {https://app.dimensions.ai/details/publication/pub.1052895230},
}

@Article{Amjad2018,
  author   = {Amjad, Anam and Azam, Farooque and Anwar, Muhammad Waseem and Butt, Wasi Haider and Rashid, Muhammad and Naeem, Aamir},
  journal  = {IEEE Access},
  title    = {UMLPACE for Modeling and Verification of Complex Business Requirements in Event-Driven Process Chain (EPC)},
  year     = {2018},
  note     = {https://doi.org/10.1109/access.2018.2883610},
  pages    = {76198-76216},
  volume   = {6},
  abstract = {Business processes (BPs) are often modeled to elaborate process-related business requirements (BRs). This leads to verify the complex BRs in early automation stages. Among various BP languages, event-driven process chain (EPC) is a well-known semi-formal modeling language, which is verifiable after transforming it into any other formal language, such as, timed automata or Petri nets. However, full potential of EPC cannot be exploited as yet because existing EPC tools can only model or verify the simple patterns and they lack the modeling/verification of complex patterns. Moreover, only the proprietary tools are available, which limit its applicability toward overwhelming utilization amongst widespread practitioners and research community. This research work is the first attempt to make EPC more expressive in terms of modeling complex patterns for real time systems. Particularly, the UMLPACE (Unified Modeling Language Profile for Atomic and Complex events in EPC) has been developed, which adapts the concepts of UML activity diagram for representing both simple as well as complex patterns in EPC. As a part of research, a complete open source transformation engine is developed to transform UMLPACE source models into timed automata target models for the verification of complex BPs. The implementation of transformation engine is carried out in JAVA language and Acceleo tool through model-to-text transformation approach. Finally, the broader applications of UMLPACE are demonstrated through two benchmark case studies.},
  doi      = {10.1109/access.2018.2883610},
  groups   = {Business Process Meta Models},
  url      = {https://app.dimensions.ai/details/publication/pub.1110301891},
}

@InProceedings{Yahya2016,
  author    = {Yahya, Fadwa and Boukadi, Khouloud and Maamar, Zakaria and Ben-Abdallah, Hanêne},
  booktitle = {Proceedings of the 18th International Conference on Information Integration and Web-based Applications and Services},
  title     = {Towards a meta-modeling approach for social business process requirements capture},
  year      = {2016},
  pages     = {345-354},
  abstract  = {A Social Business Process (SBP) is the result of blending social computing (a.k.a. Web 2.0) with business process (BP). Despite the benefits of SBP to enterprises, several limitations continue to undermine them. In this paper, we address two specific limitations, namely the difficulty of capturing SBP's requirements and the lack of a definition for SBP. Thus, meta-modeling is used to capture requirements from organizational, technological, and management perspectives. In addition, we introduce a definition for SBP by enriching an existing BP meta-model with social concepts. To annotate the SBP model with its requirements, a BPMN extension is proposed. The proposed meta-models are evaluated in terms of completeness and clarity using the Bunge-Wand-Weber ontology.},
  doi       = {10.1145/3011141.3011170},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1084731217},
}

@Article{Mertens2017,
  author   = {Mertens, Steven and Gailly, Frederik and Poels, Geert},
  journal  = {Expert Systems with Applications},
  title    = {Towards a decision-aware declarative process modeling language for knowledge-intensive processes},
  year     = {2017},
  note     = {https://biblio.ugent.be/publication/8532812/file/8532813.pdf},
  pages    = {316-334},
  volume   = {87},
  abstract = {Modeling loosely framed and knowledge-intensive business processes with the currently available process modeling languages is very challenging. Some lack the flexibility to model this type of processes, while others are missing one or more perspectives needed to add the necessary level of detail to the models. In this paper we have composed a list of requirements that a modeling language should fulfil in order to adequately support the modeling of this type of processes. Based on these requirements, a metamodel for a new modeling language was developed that satisfies them all. The new language, called DeciClare, incorporates parts of several existing modeling languages, integrating them with new solutions to requirements that had not yet been met. Deciclare is a declarative modeling language at its core, and therefore, can inherently deal with the flexibility required to model loosely framed processes. The complementary resource and data perspectives add the capability to reason about, respectively, resources and data values. The latter makes it possible to encapsulate the knowledge that governs the process flow by offering support for decision modeling. The abstract syntax of DeciClare has been implemented in the form of an Ecore model. Based on this implementation, the language-domain appropriateness of the language was validated by domain experts using the arm fracture case as application scenario.},
  doi      = {10.1016/j.eswa.2017.06.024},
  groups   = {Business Process Meta Models},
  url      = {https://app.dimensions.ai/details/publication/pub.1086073267},
}

@InBook{Russell2005,
  author    = {Russell, Nick and van der Aalst, Wil M. P. and ter Hofstede, Arthur H. M. and Edmond, David},
  pages     = {216-232},
  title     = {Workflow Resource Patterns: Identification, Representation and Tool Support},
  year      = {2005},
  note      = {https://link.springer.com/content/pdf/10.1007/11431855_16.pdf},
  abstract  = {In the main, the attention of workflow researchers and workflow developers has focussed on the process perspective, i.e., control-flow. As a result, issues associated with the resource perspective, i.e., the people and machines actually doing the work, have been largely neglected. Although the process perspective is of most significance, appropriate consideration of the resource perspective is essential for successful implementation of workflow technology. Previous work has identified recurring, generic constructs in the control-flow and data perspectives, and presented them in the form of control-flow and data patterns. The next logical step is to describe workflow resource patterns that capture the various ways in which resources are represented and utilised in workflows. These patterns include a number of distinct groupings such as push patterns (“the system pushes work to a worker”) and pull patterns (“the worker pulls work from the system”) to describe the many ways in which work can be distributed. By delineating these patterns in a form that is independent of specific workflow technologies and modelling languages, we are able to provide a comprehensive treatment of the resource perspective and we subsequently use these patterns as the basis for a detailed comparison of a number of commercially available workflow management systems.},
  booktitle = {Active Flow and Combustion Control 2018},
  doi       = {10.1007/11431855_16},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1025884987},
}

@InProceedings{2013,
  booktitle = {Proceedings of the 15th International Conference on Enterprise Information Systems},
  title     = {Change Management of BPM-based Software Applications},
  year      = {2013},
  note      = {https://doi.org/10.5220/0004441400370045},
  pages     = {37-45},
  doi       = {10.5220/0004441400370045},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1099323931},
}

@InProceedings{Weiss2011,
  author    = {Weiß, Burkhard and Winkelmann, Axel},
  booktitle = {2011 44th Hawaii International Conference on System Sciences},
  title     = {A Metamodel Based Perspective on the Adaptation of a Semantic Business Process Modeling Language to the Financial Sector},
  year      = {2011},
  pages     = {1-10},
  abstract  = {Process modeling is an important prerequisite to process reorganization and management. As a result, many companies have spent much effort on process documentation, while hardly gaining equivalent benefits in the analysis and usage of the resulting process models. To balance the cost-benefit-ratio of process modeling projects with respect to their later usage, especially regarding automatic process model analysis (i.e. for optimization purposes etc.), new domain-specific and thus semantic business process modeling languages (SBPML) have been proposed for selected domains. In this paper we investigate the adaptability of SBPML from the public sector to the banking sector from the perspective of the language's metamodel, since banks are currently highly involved in modeling initiatives to industrialize and optimize their process landscapes and are unsatisfied with existing modeling approaches regarding the cost-benefit-ratio of modeling. While taking a metamodel perspective on the language artifact itself, we derive requirements for process modeling from the domain of financial institutions and present findings on the adaptation of SBPML, giving a complete conceptual model of the SBPML method for banks.},
  doi       = {10.1109/hicss.2011.16},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1093214085},
}

@InProceedings{Bruening2011,
  author    = {Brüning, Jens and Gogolla, Martin},
  booktitle = {2011 IEEE 15th International Enterprise Distributed Object Computing Conference},
  title     = {UML Metamodel-based Workflow Modeling and Execution},
  year      = {2011},
  note      = {http://www.db.informatik.uni-bremen.de/publications/Bruening_2011_EDOC.pdf},
  pages     = {97-106},
  abstract  = {In this paper, we present a UML metamodel-based approach for creating and executing workflow models. The workflow modeling language is introduced through its abstract syntax, and an evaluation shows how this language supports known workflow patterns. Some patterns can be expressed easier compared to established languages like EPCs or BPMN. Organizational and data aspects in workflow models can be described on the basis of the presented metamodel. The workflow models can be instantiated and executed with a tool realizing parts of the UML action semantics. At an early stage of design, our workflow models can be evaluated by testing scenarios with the used tool in combination with the developed workflow plugin. Employing the tool, dynamic aspects of the workflow process models together with data and organizational aspects can be evaluated. During execution of the workflow scenarios, the workflow models can be adaptively changed, and data can be captured and evaluated by formulating process mining queries with UML's OCL (Object Constraint Language).},
  doi       = {10.1109/edoc.2011.31},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1095475458},
}

@InBook{Natschlaeger2011,
  author    = {Natschläger, Christine},
  pages     = {1-15},
  title     = {Towards a BPMN 2.0 Ontology},
  year      = {2011},
  abstract  = {The Business Process Model and Notation (BPMN) is a widely used standard for business process modelling and maintained by the Object Management Group (OMG). However, the BPMN 2.0 specification is quite comprehensive and spans more than 500 pages. The definition of an element is distributed across different sections and sometimes conflicting. In addition, the structure of the elements and their relationships are described within the metamodel, however, further syntactical rules are defined within the natural text. Therefore, this paper defines an ontology that formally represents the BPMN specification. This ontology is called the BPMN 2.0 Ontology and can be used as a knowledge base. The description of an element is combined within the corresponding class and further explanations are provided in annotations. This allows a much faster understanding of BPMN. In addition, the ontology is used as a syntax checker to validate concrete BPMN models.},
  booktitle = {Business Process Model and Notation},
  doi       = {10.1007/978-3-642-25160-3_1},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1047113209},
}

@InBook{Momotko2004,
  author    = {Momotko, Mariusz and Subieta, Kazimierz},
  pages     = {306-321},
  title     = {Process Query Language: A Way to Make Workflow Processes More Flexible},
  year      = {2004},
  abstract  = {Many requirements for a business process depend on the workflow execution data that includes common data for all the population of processes, state of resources, state of processes, etc. The natural way to specify and implement such requirements is to put them into the process definition. In order to do it, we need: (1) a generalised workflow metamodel that includes data on the workflow environment, process definitions, and process execution; (2) a powerful and flexible query language addressing the metamodel; (3) integration of a query language with a business process definition language. In this paper the mentioned workflow metamodel together with the business process query language BPQL is presented. BPQL is integrated with the XML Process Definition Language (XPDL) increasing significantly its expressiveness and flexibility. We also present practical results for application of the proposed language in the OfficeObjects® WorkFlow system.},
  booktitle = {Advances in Databases and Information Systems},
  doi       = {10.1007/978-3-540-30204-9_21},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1008876262},
}

@Article{Strembeck2011,
  author   = {Strembeck, Mark and Mendling, Jan},
  journal  = {Information and Software Technology},
  title    = {Modeling process-related RBAC models with extended UML activity models},
  year     = {2011},
  number   = {5},
  pages    = {456-483},
  volume   = {53},
  abstract = {ContextBusiness processes are an important source for the engineering of customized software systems and are constantly gaining attention in the area of software engineering as well as in the area of information and system security. While the need to integrate processes and role-based access control (RBAC) models has been repeatedly identified in research and practice, standard process modeling languages do not provide corresponding language elements.ObjectiveIn this paper, we are concerned with the definition of an integrated approach for modeling processes and process-related RBAC models – including roles, role hierarchies, statically and dynamically mutual exclusive tasks, as well as binding of duty constraints on tasks.MethodWe specify a formal metamodel for process-related RBAC models. Based on this formal model, we define a domain-specific extension for a standard modeling language.ResultsOur formal metamodel is generic and can be used to extend arbitrary process modeling languages. To demonstrate our approach, we present a corresponding extension for UML2 activity models. The name of our extension is Business Activities. Moreover, we implemented a library and runtime engine that can manage Business Activity runtime models and enforce the different policies and constraints in a software system.ConclusionThe definition of process-related RBAC models at the modeling-level is an important prerequisite for the thorough implementation and enforcement of corresponding policies and constraints in a software system. We identified the need for modeling support of process-related RBAC models from our experience in real-world role engineering projects and case studies. The Business Activities approach presented in this paper is successfully applied in role engineering projects.},
  doi      = {10.1016/j.infsof.2010.11.015},
  groups   = {Business Process Meta Models},
  url      = {https://app.dimensions.ai/details/publication/pub.1036461743},
}

@Article{Papavassiliou2003,
  author   = {Papavassiliou, Giorgos and Mentzas, Gregoris},
  journal  = {Journal of Knowledge Management},
  title    = {Knowledge modelling in weakly‐structured business processes},
  year     = {2003},
  number   = {2},
  pages    = {18-33},
  volume   = {7},
  abstract = {In this paper we present a new approach for integrating knowledge management and business process management. We focus on the modelling of weakly‐structured knowledge‐intensive business processes. We develop a framework for modelling this type of processes that explicitly considers knowledge‐related tasks and knowledge objects and present a workflow tool that is an implementation of our theoretical meta‐model. As an example, we sketch one case study, the process for granting full old age pension as it is performed in the Greek Social Security Institution. Finally we briefly describe some related approaches and compare them to our work and draw the main conclusions and further research directions.},
  doi      = {10.1108/13673270310477261},
  groups   = {Business Process Meta Models},
  url      = {https://app.dimensions.ai/details/publication/pub.1010483144},
}

@InBook{Ruiz2014,
  author    = {Ruiz, Marcela and Costal, Dolors and España, Sergio and Franch, Xavier and Pastor, Óscar},
  pages     = {332-346},
  title     = {Integrating the Goal and Business Process Perspectives in Information System Analysis},
  year      = {2014},
  abstract  = {There are several motivations to promote investment and scientific effort in the integration of intentional and operational perspectives: organisational reengineering, continuous improvement of business processes, alignment among complementary analysis perspectives, information traceability, etc. In this paper we propose the integration of two modelling languages that support the creation of goal and business process models: the i* goal-oriented modelling method and Communication Analysis, a communication-oriented business process modelling method. We describe the methodological integration of the two modelling methods with the aim of fulfilling several criteria: i) to rely on appropriate theories; ii) to provide abstract and concrete syntaxes; iii) to provide scenarios of application; and iv) to develop tool support. We provide guidelines for using the two modelling methods in a top-down analysis scenario. We also present an illustrative case that demonstrates the feasibility of the approach.},
  booktitle = {Advanced Information Systems Engineering},
  doi       = {10.1007/978-3-319-07881-6_23},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1006163752},
}

@Article{Farrell2006,
  author   = {Farrell, Andrew D. H. and Sergot, Marek J. and Bartolini, Claudio},
  journal  = {Group Decision and Negotiation},
  title    = {Formalising Workflow: A CCS-inspired Characterisation of the YAWL Workflow Patterns},
  year     = {2006},
  note     = {http://spiral.imperial.ac.uk/bitstream/10044/1/518/1/Formalising%20workflow%20-%20%20A%20CCS-inspired.pdf},
  number   = {3},
  pages    = {213-254},
  volume   = {16},
  abstract = {We present work concerning the formal specification of business processes. It is of substantial benefit to be able to pin down the meaning of business processes precisely. This is an end in itself, but we are also concerned to do so in order that we might prove properties about the business processes that are being specified. It is a notable characteristic of most languages for representing business processes that they lack a robust semantics, and a notable characteristic of most commercial Business Process Management products that they have no support for verification of business process models. We define a high-level meta-model, called Liesbet, for representing business processes. The ontological commitments for Liesbet are sourced from the YAWL workflow patterns, which have been defined from studies into the behavioural nature of business processes. A formal characterisation of Liesbet is provided using Milner’s Calculus of Communicating Systems (CCS). In this article, we omit some of the technical details of this characterisation and instead present the essential features by means of an abstract machine language, called LCCS. We also explain how we have facilitated the verification of certain properties of business processes specified in Liesbet, and discuss how Liesbet supports the YAWL workflow patterns. We include a simple three-part example of using Liesbet.},
  doi      = {10.1007/s10726-006-9064-4},
  groups   = {Business Process Meta Models},
  url      = {https://app.dimensions.ai/details/publication/pub.1016839878},
}

@Article{Rosemann2008,
  author  = {Rosemann, Michael and Recker, Jan and Flender, Christian},
  journal = {International Journal of Business Process Integration and Management},
  title   = {Contextualisation of business processes},
  year    = {2008},
  note    = {https://eprints.qut.edu.au/14016/1/14016.pdf},
  number  = {1},
  pages   = {47},
  volume  = {3},
  doi     = {10.1504/ijbpim.2008.019347},
  groups  = {Business Process Meta Models},
  url     = {https://app.dimensions.ai/details/publication/pub.1067438521},
}

@InBook{LaRosa2008,
  author    = {La Rosa, Marcello and Dumas, Marlon and ter Hofstede, Arthur H. M. and Mendling, Jan and Gottschalk, Florian},
  pages     = {199-215},
  title     = {Beyond Control-Flow: Extending Business Process Configuration to Roles and Objects},
  year      = {2008},
  abstract  = {A configurable process model is an integrated representation of multiple variants of a business process. It is designed to be individualized to meet a particular set of requirements. As such, configurable process models promote systematic reuse of proven or common practices. Existing notations for configurable process modeling focus on capturing tasks and control-flow dependencies, neglecting equally important aspects of business processes such as data flow, material flow and resource management. This paper fills this gap by proposing an integrated meta-model for configurable processes with advanced features for capturing resources involved in the performance of tasks (through task-role associations) as well as flow of data and physical artifacts (through task-object associations). Although embodied as an extension of a popular process modeling notation, namely EPC, the meta-model is defined in an abstract and formal manner to make it applicable to other notations.},
  booktitle = {Conceptual Modeling - ER 2008},
  doi       = {10.1007/978-3-540-87877-3_16},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1008663385},
}

@InBook{DeNicola2010,
  author    = {De Nicola, Antonio and Missikoff, Michele and Proietti, Maurizio and Smith, Fabrizio},
  pages     = {76-90},
  title     = {An Open Platform for Business Process Modeling and Verification},
  year      = {2010},
  abstract  = {In this paper we present the BPAL platform that includes a logicbased language for business process (BP) modeling and a reasoning mechanism providing support for several tasks. Firstly, the definition of a BP meta-model (MM) consisting of a set of rules that guide the BP designers in their work. Secondly, given a BP, the BPAL platform allows for the automatic verification of the compliance (well-formedness) of a given BP w.r.t. the defined MM. Finally, the execution semantics of a BP is given in term of its instances (referred to as traces) to provide services for i) checking if the actual execution of a BP has been carried out in accordance with the corresponding definition, ii) simulating executions by trace generation. The proposed platform is open since it can easily be enhanced by adding other logic-based modeling, reasoning, and querying functionalities.},
  booktitle = {Database and Expert Systems Applications},
  doi       = {10.1007/978-3-642-15364-8_6},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1013367507},
}

@InProceedings{List2006,
  author    = {List, Beate and Korherr, Birgit},
  booktitle = {Proceedings of the 2006 ACM symposium on Applied computing},
  title     = {An evaluation of conceptual business process modelling languages},
  year      = {2006},
  pages     = {1532-1539},
  abstract  = {Conceptual Business Process Modelling Languages (BPMLs) express certain aspects of processes (e.g. activities, roles, interactions, data, etc.) and address different application areas. To evaluate BPMLs, a general framework is required. Although a lot of BPMLs are available in research and industry, an established evaluation framework as well as a comprehensive evaluation of BPMLs is missing. To bridge this gap, we propose a generic meta-model that captures a wide range of process concepts and evaluate seven BPMLs based on this meta-model.},
  doi       = {10.1145/1141277.1141633},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1053416319},
}

@Article{Axenath2007,
  author  = {Axenath, Bjorn and Kindler, Ekkart and Rubin, Vladimir},
  journal = {International Journal of Business Process Integration and Management},
  title   = {AMFIBIA: a meta-model for integrating business process modelling aspects},
  year    = {2007},
  number  = {2},
  pages   = {120},
  volume  = {2},
  doi     = {10.1504/ijbpim.2007.015136},
  groups  = {Business Process Meta Models},
  url     = {https://app.dimensions.ai/details/publication/pub.1067438500},
}

@Article{Arevalo2016,
  author   = {Arevalo, C. and Escalona, M.J. and Ramos, I. and Domínguez-Muñoz, M.},
  journal  = {Information and Software Technology},
  title    = {A metamodel to integrate business processes time perspective in BPMN 2.0},
  year     = {2016},
  note     = {https://idus.us.es/bitstream/11441/58862/1/A%20metamodel%20to%20integrate.pdf},
  pages    = {17-33},
  volume   = {77},
  abstract = {ContextBusiness Process Management (BPM) is becoming a strategic advantage for organizations to streamline their operations. Most business experts are betting for OMG Business Process Model and Notation (BPMN) as de-facto standard (ISO/IEC 19510:2013) and selected technology to model processes. The temporal dimension underlies in any kind of process however, technicians need to shape this perspective that must also coexist with task control flow aspects, as well as resource and case perspectives. BPMN poorly gathers temporary rules. This is why there are contributions that extend the standard to cover such dimension. BPMN is mainly an imperative language. There are research contributions showing time constraints in BPMN, such as (i) BPMN patterns to express each rule with a combination of artifacts, thus these approaches increase the use of imperative BPMN style, and (ii) new decorators to capture time rules semantics giving clearer and simpler comprehensible specifications. Nevertheless, these extensions cannot yet be found in the present standard.ObjectiveTo define a time rule taxonomy easily found in most business processes and look for an approach that applies each rule with current BPMN 2.0 standard in a declarative way.MethodA model-driven approach is used to propose a BPMN metamodel extension to address time-perspective.ResultsWe look at a declarative approach where new time specifications may overlie the main control flow of a BPMN process. This proposal is totally supported with current BPMN standard, giving a BPMN metamodel extension with OCL constraints. We also use AQUA-WS as a software project case study which is planned and managed with MS Project. We illustrate business process extraction from project plans.ConclusionThis paper suggests to handle business temporal rules with current BPMN standard, along with other business perspectives like resources and cases. This approach can be applied to reverse engineering processes from legacy databases.},
  doi      = {10.1016/j.infsof.2016.05.004},
  groups   = {Business Process Meta Models},
  url      = {https://app.dimensions.ai/details/publication/pub.1009455055},
}

@InProceedings{Heidari2013,
  author    = {Heidari, Farideh and Loucopoulos, Pericles and Brazier, Frances and Barjis, Joseph},
  booktitle = {2013 IEEE 15th Conference on Business Informatics},
  title     = {A Meta-Meta-Model For Seven Business Process Modeling Languages},
  year      = {2013},
  pages     = {216-221},
  abstract  = {Many different business process modelling languages (BPMLs) have been designed in recent years. In cross-organizational business processes and heterogeneous organizations where multiple BPMLs are deployed there is a need for a unified view to ease communication and foster understandability. This paper proposes a language independent abstraction of seven mainstream BPMLs' concepts, in a unified meta-meta model based on an analysis of these modelling languages. Generic concepts are identified and a unified meta-model is developed. An ontological analysis of the representational capability of this meta-model is examined in relation to the Bunge-Wand-Weber ontology and applicability of the approach is demonstrated via an Example.},
  doi       = {10.1109/cbi.2013.38},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1095792619},
}

@InProceedings{Friedenstab2012,
  author    = {Friedenstab, Jan-Philipp and Janiesch, Christian and Matzner, Martin and Müller, Oliver},
  booktitle = {2012 45th Hawaii International Conference on System Sciences},
  title     = {Extending BPMN for Business Activity Monitoring},
  year      = {2012},
  pages     = {4158-4167},
  abstract  = {Real-time access to key performance indicators is necessary to ensure timeliness and effectiveness of operational business processes. The concept of Business Activity Monitoring (BAM) refers to the observation, analysis, and presentation of real-time information about business activities across systems and companies' borders. Designing and maintaining BAM applications is challenging, as the involved concepts (e.g., business processes, audit logs, performance measures) though being strongly interrelated— are developed by different communities of practice. Also, they reside on different levels of abstraction, and are handled by different IT systems. Hence, we developed a conceptual modeling language which extends the widely accepted Business Process Modeling Notation (BPMN) by BAM-relevant concepts. The main results presented in this paper are: (1) a meta-model which formally describes the conceptual aspects of the developed BPMN extension (abstract syntax); (2) graphical symbols as an exemplary representation of this abstract syntax (concrete syntax); (3) a demo scenario that illustrates the application of the language in a fictitious scenario.},
  doi       = {10.1109/hicss.2012.276},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1095376931},
}

@InProceedings{Albert2005,
  author    = {Albert, Patrick and Henocque, Laurent and Kleiner, Mathias},
  booktitle = {IEEE International Conference on Web Services (ICWS'05)},
  title     = {Configuration Based Workflow Composition},
  year      = {2005},
  pages     = {285-292 vol.1},
  abstract  = {Automatic or assisted workflow composition is a field of intense research for applications to the world wide web or to business process modeling. Workflow composition is traditionally addressed in various ways, generally via theorem proving techniques. The originality of this research stems from the observation that building a composite workflow bears strong relationships with finite model search, and that some workflow languages can be defined as constrained object metamodels [1], [2]. This leads to consider the viability of applying configuration techniques to this problem. Our main contribution is to prove the feasibility of such an approach, with some advantages and drawbacks compared to logical based techniques. We present a constrained object model for workflow composition, based upon a metamodel for workflows and ontologies for processes and data flows. Experimental results are listed for a working implementation that generates complex interleaving composite workflows involving transformations, synchronization and branching constructs.},
  doi       = {10.1109/icws.2005.38},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1093446551},
}

@Article{Rittgen2006,
  author   = {Rittgen, Peter},
  journal  = {European Journal of Information Systems},
  title    = {A language-mapping approach to action-oriented development of information systems},
  year     = {2006},
  number   = {1},
  pages    = {70-81},
  volume   = {15},
  abstract = {Two important views in the development of information systems are the action view and the reaction view which govern the areas of business process modelling and information systems modelling, respectively. We suggest an approach to mediate between these partially conflicting views by specifying a language-mapping framework. In other words, we specify a transition from a process model with human actors to an IS model with inanimate agents.},
  doi      = {10.1057/palgrave.ejis.3000597},
  groups   = {Business Process Meta Models},
  url      = {https://app.dimensions.ai/details/publication/pub.1051544866},
}

@InProceedings{2013a,
  booktitle = {Proceedings of the 15th International Conference on Enterprise Information Systems},
  title     = {A Generic Workflow Metamodel to Support Resource-aware Decision Making},
  year      = {2013},
  note      = {https://doi.org/10.5220/0004417002430250},
  pages     = {243-250},
  doi       = {10.5220/0004417002430250},
  groups    = {Business Process Meta Models},
  url       = {https://app.dimensions.ai/details/publication/pub.1099323867},
}

@InProceedings{Frank2017,
  author    = {Frank, Markus and Hilbrich, Marcus and Lehrig, Sebastian and Becker, Steffen},
  booktitle = {2017 IEEE 7th International Symposium on Cloud and Service Computing (SC2)},
  title     = {Parallelization, Modeling, and Performance Prediction in the Multi-/Many Core Area: A Systematic Literature Review},
  year      = {2017},
  pages     = {48-55},
  abstract  = {Context: Software developers face complex, connected, and large software projects. The development of such systems involves design decisions that directly impact the quality of the software. For an early decision making, software developers can use model-based prediction approaches for (non-)functional quality properties. Unfortunately, the accuracy of these approaches is challenged by newly introduced hardware features like multiple cores within a single CPU (multicores) and their dependence on shared memory and other shared resources. Objectives: Our goal is to understand whether and how existing model-based performance prediction approaches face this challenge. We plan to use gained insights as foundation for enriching existing prediction approaches with capabilities to predict systems running on multicores. Methods: We perform a Systematic Literature Review (SLR) to identify current model-based prediction approaches in the context of multicores. Results: Our SLR covers the software engineering, embedded systems, High Performance Computing, and Software Performance Engineering domains for which we examined 34 sources in detail. We found various performance prediction approaches which tries to increase prediction accuracy for multicore systems by including shared memory designs to the prediction models. Conclusion: However, our results show that the memory designs models are only in an initial phase. Further research has to be done to improve cache, memory, and memory bandwidth model as well as to include auto tuner support.},
  doi       = {10.1109/sc2.2017.15},
  groups    = {Multicore Performance Prediction},
  priority  = {prio1},
  url       = {https://app.dimensions.ai/details/publication/pub.1101547396},
}

@InProceedings{Saad2019,
  author    = {Saad, Abdallah and El-Mahdy, Ahmed and El-Shishiny, Hisham},
  booktitle = {Proceedings of the Rapid Simulation and Performance Evaluation: Methods and Tools},
  title     = {Performance Modeling of MPI-based Applications on Cloud Multicore Servers},
  year      = {2019},
  pages     = {1-6},
  abstract  = {While cloud computing is widely adopted in many application domains, it is not yet the case for the high performance computing (HPC) domain. HPC traditionally runs on homogeneous, high-cost servers with fast networking providing for predictable performance; while bare-metal cloud offerings is promising, the underlying hardware is heterogeneous, with slower network connection, making it difficult to predict performance and hence tune applications. In this paper we consider performance modelling message passing interface (MPI)-based applications, being a major class of HPC applications. In particular, we present a queueing network performance model to account for computation and communication contentions on the underlying heterogeneous, relatively slow-interconnect architecture of the cloud bare-metal servers. The proposed model uses a non-linear problem solver to enhance the parameters acquired by profiling. We utilise our model to conduct an initial study of the performance of two benchmarks from SPECMPI-2007 suite and two NASA Parallel kernels, executing on a small cluster with varying number of multicore servers ranging from 2 to 8. Comparing the predicted and actual execution times of workloads with different number of processes shows 86% average accuracy for the benchmarks used.},
  doi       = {10.1145/3300189.3300194},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1116305779},
}

@InBook{Ipek2005,
  author    = {Ipek, Engin and de Supinski, Bronis R. and Schulz, Martin and McKee, Sally A.},
  pages     = {196-205},
  title     = {An Approach to Performance Prediction for Parallel Applications},
  year      = {2005},
  note      = {https://link.springer.com/content/pdf/10.1007/11549468_24.pdf},
  abstract  = {Accurately modeling and predicting performance for large-scale applications becomes increasingly difficult as system complexity scales dramatically. Analytic predictive models are useful, but are difficult to construct, usually limited in scope, and often fail to capture subtle interactions between architecture and software. In contrast, we employ multilayer neural networks trained on input data from executions on the target platform. This approach is useful for predicting many aspects of performance, and it captures full system complexity. Our models are developed automatically from the training input set, avoiding the difficult and potentially error-prone process required to develop analytic models. This study focuses on the high-performance, parallel application SMG2000, a much studied code whose variations in execution times are still not well understood. Our model predicts performance on two large-scale parallel platforms within 5%-7% error across a large, multi-dimensional parameter space.},
  booktitle = {Euro-Par 2005 Parallel Processing},
  doi       = {10.1007/11549468_24},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1029836641},
}

@InProceedings{Pllana2005,
  author    = {Pllana, Sabri and Fahringer, Thomas},
  booktitle = {2005 International Conference on Parallel Processing Workshops (ICPPW'05)},
  title     = {Performance Prophet: A Performance Modeling and Prediction Tool for Parallel and Distributed Programs},
  year      = {2005},
  pages     = {509-516},
  abstract  = {High-performance computing is essential for solving large problems and for reducing the time to solution for a single problem. Current top high-performance computing systems contain 1000's of processors. Therefore, new tools are needed to support the program development that will exploit high degrees of parallelism. The issue of model-based performance evaluation of real world programs on large scale systems is addressed in this paper. We present the PerformanceProphet, which is a performance modeling and prediction tool for parallel and distributed programs. One of the main contributions of this paper is our methodology for reducing the time needed to evaluate the model. In addition, we describe our method for automatic performance model generation. We have implemented Performan-ceProphet in Java and C++. We illustrate our approach by modeling and simulating a real-world material science parallel program.},
  doi       = {10.1109/icppw.2005.72},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1094447170},
}

@Article{Pankratius2009,
  author   = {Pankratius, V. and Jannesari, A. and Tichy, W.F.},
  journal  = {IEEE Software},
  title    = {Parallelizing Bzip2: A Case Study in Multicore Software Engineering},
  year     = {2009},
  note     = {https://publikationen.bibliothek.kit.edu/1000009973/614524},
  number   = {6},
  pages    = {70-77},
  volume   = {26},
  abstract = {We conducted a case study of parallelizing a real program for multicore computers using currently available libraries and tools. We selected the sequential Bzip2 compression program for the study because it's a computing-intensive, widely used, and relevant application in everyday life. Its source code is available, and its algorithm is well documented. In addition, the algorithm is non-trivial, but, with 8,000 LOC, the application is small enough to manage in a course.},
  doi      = {10.1109/ms.2009.183},
  groups   = {Multicore Performance Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061421075},
}

@Article{Hwu2008,
  author   = {Hwu, Wen-mei and Keutzer, K. and Mattson, T.G.},
  journal  = {IEEE Design and Test},
  title    = {The Concurrency Challenge},
  year     = {2008},
  number   = {4},
  pages    = {312-320},
  volume   = {25},
  abstract = {The evolutionary path of microprocessor design includes both multicore and many-core architectures. Harnessing the most computing throughput from these architectures requires concurrent or parallel execution of instructions. The authors describe the challenges facing the industry as parallel-computing platforms become even more widely available.},
  doi      = {10.1109/mdt.2008.110},
  groups   = {Multicore Performance Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061399768},
}

@Article{Haller2009,
  author   = {Haller, Philipp and Odersky, Martin},
  journal  = {Theoretical Computer Science},
  title    = {Scala Actors: Unifying thread-based and event-based programming},
  year     = {2009},
  note     = {https://doi.org/10.1016/j.tcs.2008.09.019},
  number   = {2-3},
  pages    = {202-220},
  volume   = {410},
  abstract = {There is an impedance mismatch between message-passing concurrency and virtual machines, such as the JVM. VMs usually map their threads to heavyweight OS processes. Without a lightweight process abstraction, users are often forced to write parts of concurrent applications in an event-driven style which obscures control flow, and increases the burden on the programmer.In this paper we show how thread-based and event-based programming can be unified under a single actor abstraction. Using advanced abstraction mechanisms of the Scala programming language, we implement our approach on unmodified JVMs. Our programming model integrates well with the threading model of the underlying VM.},
  doi      = {10.1016/j.tcs.2008.09.019},
  groups   = {Multicore Performance Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1036049428},
}

@Article{Cerotti2015,
  author   = {Cerotti, D. and Gribaudo, M. and Iacono, M. and Piazzolla, P.},
  journal  = {Concurrency and Computation Practice and Experience},
  title    = {Modeling and analysis of performances for concurrent multithread applications on multicore and graphics processing unit systems},
  year     = {2015},
  number   = {2},
  pages    = {438-452},
  volume   = {28},
  abstract = {Summary The capabilities of multicore processors lead them to be widely adopted in systems at any scale, since their are able to provide more computing power at a lower consumption and dissipation cost. System designers are challenged to a deeper understanding of multicore functioning in order to fully exploit them while keeping the optimal balance between cores utilization and optimal throughput, response time and energy usage. Besides the advancement of general purpose CPUs, the same technological evolution leads to the rise of GPUs, dramatic evolution of graphical coprocessors, that are now affordable, efficient, dedicated computing units, capable of parallel computing and equipped with facilities that make them suited for supporting the main CPU of a system in running ordinary applications. The availability of commercial off‐the‐shelf (COTS) multicore computers, eventually equipped with one or more GPUs, makes them the basic building block of data centers devoted to cloud applications or scientific computing. The way to optimal exploitation of such a wide amount of computing power passes through the ability of matching the best scheduling of hardware resources with the software characteristics of the applications. This requires appropriate models and evaluation methods. Simulation and analytical techniques are essential tools to support the design and the management process of such architectures, but a sound characterization of the workloads is required. Typical workloads consist in multithreaded applications, with different characteristics, that dynamically span over the cores of multiple machines, connected by fast networks. In this paper we propose several parametric performance models for different configurations of multicore machines, with or without GPU support, running multiple class multithreaded applications, aiming to supply a detailed modeling help for complex data centers. Copyright © 2015 John Wiley & Sons, Ltd.},
  doi      = {10.1002/cpe.3504},
  groups   = {Multicore Performance Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1004373119},
}

@InBook{Aldinucci2017,
  author    = {Aldinucci, Marco and Danelutto, Marco and Kilpatrick, Peter and Torquati, Massimo},
  pages     = {261-280},
  title     = {Fastflow: High‐Level and Efficient Streaming on Multicore},
  year      = {2017},
  note      = {https://iris.unito.it/bitstream/2318/1518646/1/2012_ff_wileybook_chap13.pdf},
  abstract  = {This chapter first outlines FastFlow design and then shows sample use of the FastFlow programming environment together with performance results achieved on various state‐of‐the‐art multicore architectures. The FastFlow framework has been designed according to four foundational principles: layered design; efficiency in base mechanisms; support for stream parallelism; and a programming model based on design pattern/algorithmic skeleton concepts. The core of the FastFlow framework provides an efficient implementation of single‐producer‐single‐consumer (SPSC) first in‐first out (FIFO) queues. The next tier up extends from one‐to‐one queues to one‐to‐many, many‐to‐one, and many‐to‐many synchronizations and data flows, which are implemented using only SPSC queues and arbiter threads, thus providing lock‐free and wait‐free arbitrary dataflow graphs. When designing and implementing new parallel applications using FastFlow, programmers instantiate patterns provided by FastFlow to adapt them to the specific needs of the application at hand. The chapter demonstrates how the principal FastFlow patterns may be used in a parallel application.},
  booktitle = {Programming multi‐core and many‐core computing systems},
  doi       = {10.1002/9781119332015.ch13},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1074245157},
}

@InProceedings{Escobar2016,
  author    = {Escobar, Rodrigo and Boppana, Rajendra V.},
  booktitle = {2016 IEEE 23rd International Conference on High Performance Computing (HiPC)},
  title     = {Performance Prediction of Parallel Applications Based on Small-Scale Executions},
  year      = {2016},
  pages     = {362-371},
  abstract  = {Predicting the execution time of parallel applications in High Performance Computing (HPC) clusters has served different objectives, including helping developers to find relevant areas of code that require fine tuning, designing better job schedulers to increase clusters' utilization, and detecting system bottlenecks. We present a statistical approach to predict parallel application execution times using empirical analyses of the application execution times for small input sizes and the time spent on various phases of execution. We model the execution time of each phase an application by selecting a suitable kernel from a collection of well known benchmark kernels. To predict the application execution time for a larger input, the matching kernels are used to estimate the execution times for the major phases of the application, and a regression approach is then used to estimate the overall execution time. Prior approaches required determination of application's characteristics by extracting instruction traces, instrumenting the application code for time stamps, static code analysis, or creation of accurate simulation models. In contrast, our approach requires a few short executions (each taking less than 50 seconds) of the application to collect runtime profile data that are used to match application phases to kernels using statistical analyses and produce accurate execution time predictions for parallel scientific applications. We evaluate our methodology using three well known parallel scientific applications: SMG2000, SNAP and HPCG. Our prediction errors range from 1% to 15%.},
  doi       = {10.1109/hipc.2016.049},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1094223098},
}

@Article{Massingill2006,
  author   = {Massingill, Berna L. and Mattson, Timothy G. and Sanders, Beverly A.},
  journal  = {Concurrency and Computation Practice and Experience},
  title    = {Reengineering for Parallelism: an entry point into PLPP for legacy applications},
  year     = {2006},
  number   = {4},
  pages    = {503-529},
  volume   = {19},
  abstract = {Abstract  Many parallel programs begin as legacy sequential code that is later reengineered to take advantage of parallel hardware. This paper presents a pattern called Reengineering for Parallelism to help with this task. The new pattern is intended to be used in conjunction with PLPP (Pattern Language for Parallel Programming), described in our book (Mattson TG, Sanders BA, Massingill BL. Patterns for Parallel Programming . Addison‐Wesley: Reading, MA, 2004). PLPP contains a structured collection of patterns and embodies a methodology for developing parallel programs in which the programmer starts with a good understanding of the problem, works through a sequence of patterns, and finally ends up with the code. Most of the patterns in PLPP are also applicable when reengineering legacy code, but it is not always clear how to get started. Reengineering for Parallelism provides an alternate point of entry into PLPP and addresses particular issues that arise when dealing with legacy code. Copyright © 2006 John Wiley & Sons, Ltd.},
  doi      = {10.1002/cpe.1147},
  groups   = {Multicore Performance Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1021257745},
}

@InProceedings{Pankratius2008,
  author    = {Pankratius, Victor and Schaefer, Christoph and Jannesari, Ali and Tichy, Walter F.},
  booktitle = {Proceedings of the 1st international workshop on Multicore software engineering},
  title     = {Software engineering for multicore systems},
  year      = {2008},
  pages     = {53-60},
  abstract  = {The emergence of inexpensive parallel computers powered by multicore chips combined with stagnating clock rates raises new challenges for software engineering. As future performance improvements will not come "for free" from increased clock rates, performance critical applications will need to be parallelized. However, little is known about the engineering principles for parallel general-purpose applications. This paper presents an experience report with four diverse case studies on multicore software development for general-purpose applications. They were programmed in different languages and benchmarked on several multicore computers. Empirical findings include: 1) Multicore computers deliver: Real speedups are achievable, albeit with significant programming effort and speedups that are typically lower than the number of cores employed; 2) Massive refactoring of sequential programs is required, sometimes at several levels. Special tools for parallelization refactorings appear to be an important area of research; 3) Autotuning is indispensable, as manually tuning thread assignment, number of pipeline stages, size of data partitions and other parameters is difficult and error prone; 4) Architectures that encompass several parallel components are poorly understood. Tuneable architectural patterns with parallelism at several levels need to be discovered.},
  doi       = {10.1145/1370082.1370096},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1041687543},
}

@Article{Asanovic2009,
  author   = {Asanovic, Krste and Bodik, Rastislav and Demmel, James and Keaveny, Tony and Keutzer, Kurt and Kubiatowicz, John and Morgan, Nelson and Patterson, David and Sen, Koushik and Wawrzynek, John and Wessel, David and Yelick, Katherine},
  journal  = {Communications of the ACM},
  title    = {A view of the parallel computing landscape},
  year     = {2009},
  note     = {https://dl.acm.org/doi/pdf/10.1145/1562764.1562783},
  number   = {10},
  pages    = {56-67},
  volume   = {52},
  abstract = {Writing programs that scale with increasing numbers of cores should be as easy as writing programs for sequential computers.},
  doi      = {10.1145/1562764.1562783},
  groups   = {Multicore Performance Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1027518140},
}

@Article{Mehrara2009,
  author   = {Mehrara, M. and Jablin, T. and Upton, D. and August, D. and Hazelwood, K. and Mahlke, S.},
  journal  = {IEEE Signal Processing Magazine},
  title    = {Multicore compilation strategies and challenges},
  year     = {2009},
  number   = {6},
  pages    = {55-63},
  volume   = {26},
  abstract = {To overcome challenges stemming from high power densities and thermal hot spots in microprocessors, multicore computing platforms have emerged as the ubiquitous computing platform from servers down through embedded systems. Unfortunately, providing multiple cores does not directly translate into increased performance or better energy efficiency for most applications. The burden is placed on software developers and tools to find and exploit coarse-grain parallelism to effectively make use of the abundance of computing resources provided by these systems. Concurrent applications are much more complex to develop than their single-threaded ancestors, thus software development tools will be critical to help programmers create both high performance and correct software. This article provides an overview of parallelism and compiler technology to help the community understand the software development challenges and opportunities for multicore signal processors.},
  doi      = {10.1109/msp.2009.934117},
  groups   = {Multicore Performance Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061423303},
}

@InProceedings{Otto2009,
  author    = {Otto, Frank and Pankratius, Victor and Tichy, Walter F.},
  booktitle = {2009 31st International Conference on Software Engineering - Companion Volume},
  title     = {High-Level Multicore Programming with XJava},
  year      = {2009},
  pages     = {319-322},
  abstract  = {Multicore chips are becoming mainstream, but programming them is difficult because the prevalent thread-based programming model is error-prone and does not scale well. To address this problem, we designed XJava, an extension of Java that permits the direct expression of producer/consumer, pipeline, master/slave, and data parallelism. The central concept of the extension is the task, a parallel activity similar to a filter in Unix. Tasks can be combined with new operators to create arbitrary nestings of parallel activities. Preliminary experience with XJava and its compiler suggests that the extensions lead to code savings and reduce the potential for synchronization defects, while preserving the advantages of object-orientation and type-safety. The proposed extensions provide intuitive “what-you-see-is-what-you-gety” parallelism. They also enable other software tools, such as auto-tuning and accurate static analysis for race detection.},
  doi       = {10.1109/icse-companion.2009.5071011},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1094231277},
}

@InBook{Stuermer2009,
  author    = {Stürmer, Markus and Wellein, Gerhard and Hager, Georg and Köstler, Harald and Rüde, Ulrich},
  pages     = {551-566},
  title     = {Challenges and Potentials of Emerging Multicore Architectures},
  year      = {2009},
  abstract  = {We present performance results on two current multicore architectures, a STI (Sony, Toshiba, and IBM) Cell processor included in the new Playstation™ 3 and a Sun UltraSPARC T2 (“Niagara 2”) machine. On the Niagara 2 we analyze typical performance patterns that emerge from the peculiar way the memory controllers are activated on this chip using the standard STREAM benchmark and a shared-memory parallel lattice Boltzmann code. On the Cell processor we measure the memory bandwidth and run performance tests for LBM simulations. Additionally, we show results for an application in image processing on the Cell processor, where it is required to solve nonlinear anisotropic PDEs.},
  booktitle = {High Performance Computing in Science and Engineering, Garching/Munich 2007},
  doi       = {10.1007/978-3-540-69182-2_43},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1053016373},
}

@InProceedings{Brown2010,
  author    = {Brown, Richard and Shoop, Elizabeth and Adams, Joel and Clifton, Curtis and Gardner, Mark and Haupt, Michael and Hinsbeeck, Peter},
  booktitle = {Proceedings of the 2010 ITiCSE working group reports},
  title     = {Strategies for preparing computer science students for the multicore world},
  year      = {2010},
  note      = {http://synergy.cs.vt.edu/pubs/papers/brown-iticse10-cs-education-multicore.pdf},
  pages     = {97-115},
  abstract  = {Multicore computers have become standard, and the number of cores per computer is rising rapidly. How does the new demand for understanding of parallel computing impact computer science education? In this paper, we examine several aspects of this question: (i) What parallelism body of knowledge do today's students need to learn? (ii) How might these concepts and practices be incorporated into the computer science curriculum? (iii) What resources will support computer science educators, including non-specialists, to teach parallel computing? (iv) What systemic obstacles impede this change, and how might they be overcome? We address these concerns as an initial framework for responding to the urgent challenge of injecting parallelism into computer science curricula},
  doi       = {10.1145/1971681.1971689},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1016245318},
}

@InProceedings{Eyerman2010,
  author    = {Eyerman, Stijn and Eeckhout, Lieven},
  booktitle = {Proceedings of the 37th annual international symposium on Computer architecture},
  title     = {Modeling critical sections in Amdahl&#x27;s law and its implications for multicore design},
  year      = {2010},
  pages     = {362-370},
  abstract  = {This paper presents a fundamental law for parallel performance: it shows that parallel performance is not only limited by sequential code (as suggested by Amdahl's law) but is also fundamentally limited by synchronization through critical sections. Extending Amdahl's software model to include critical sections, we derive the surprising result that the impact of critical sections on parallel performance can be modeled as a completely sequential part and a completely parallel part. The sequential part is determined by the probability for entering a critical section and the contention probability (i.e., multiple threads wanting to enter the same critical section). This fundamental result reveals at least three important insights for multicore design. (i) Asymmetric multicore processors deliver less performance benefits relative to symmetric processors than suggested by Amdahl's law, and in some cases even worse performance. (ii) Amdahl's law suggests many tiny cores for optimum performance in asymmetric processors, however, we find that fewer but larger small cores can yield substantially better performance. (iii) Executing critical sections on the big core can yield substantial speedups, however, performance is sensitive to the accuracy of the critical section contention predictor.},
  doi       = {10.1145/1815961.1816011},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1031284348},
}

@InProceedings{Schaefer2010,
  author    = {Schaefer, Christoph A. and Pankratius, Victor and Tichy, Walter F.},
  booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1},
  title     = {Engineering parallel applications with tunable architectures},
  year      = {2010},
  pages     = {405-414},
  abstract  = {Current multicore computers differ in many hardware characteristics. Software developers thus hand-tune their parallel programs for a specific platform to achieve the best performance; this is tedious and leads to non-portable code. Although the software architecture also requires adaptation to achieve best performance, it is rarely modified because of the additional implementation effort. The Tunable Architectures approach proposed in this paper automates the architecture adaptation of parallel programs and uses an auto-tuner to find the best-performing software architecture for a particular machine. We introduce a new architecture description language based on parallel patterns and a framework to express architecture variants in a generic way. Several case studies demonstrate significant performance improvements due to architecture tuning and show the applicability of our approach to industrial applications. Software developers are exposed to less parallel programming complexity, thus making the approach attractive for experts as well as inexperienced parallel programmers.},
  doi       = {10.1145/1806799.1806859},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1037936517},
}

@InBook{Karcher2011,
  author    = {Karcher, Thomas and Pankratius, Victor},
  pages     = {3-14},
  title     = {Run-Time Automatic Performance Tuning for Multicore Applications},
  year      = {2011},
  note      = {https://link.springer.com/content/pdf/10.1007/978-3-642-23400-2_2.pdf},
  abstract  = {Multicore hardware and system software have become complex and differ from platform to platform. Parallel application performance optimization and portability are now a real challenge. In practice, the effects of tuning parameters are hard to predict. Programmers face even more difficulties when several applications run in parallel and influence each other indirectly. We tackle these problems with Perpetuum, a novel operating-system-based auto-tuner that is capable of tuning applications while they are running. We go beyond tuning one application in isolation and are the first to employ OS-based auto-tuning to improve system-wide application performance. Our fully functional auto-tuner extends the Linux kernel, and the application tuning process does not require any user involvement. General multicore applications are automatically re-tuned on new platforms while they are executing, which makes portability easy. Extensive case studies with real applications demonstrate the feasibility and efficiency of our approach. Perpetuum realizes a first milestone in our vision to make every performance-critical multicore application auto-tuned by default.},
  booktitle = {Euro-Par 2011 Parallel Processing},
  doi       = {10.1007/978-3-642-23400-2_2},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1039682850},
}

@InProceedings{Pankratius2011,
  author    = {Pankratius, Victor and Heneka, Martin},
  booktitle = {2011 International Conference on Parallel Processing},
  title     = {Moving Database Systems to Multicore: An Auto- Tuning Approach},
  year      = {2011},
  pages     = {582-591},
  abstract  = {In the multicore era, database systems are facing new challenges to exploit parallelism and scale query performance on new processors. Taking advantage of multicore, however, is not trivial and goes far beyond inserting parallel constructs into available database system code. Varying hardware characteristics require different query parallelization strategies on each multicore platform. Query optimizers at the heart of each database system have to be reengineered, but the problem is that these optimizers are complex. In addition, optimization best practices evolved during a long-term process of research and experimentation. This paper presents a successful modular technique that does not require a major rewrite of database code from scratch. We discuss the implementation details of new fine-granular parallelism approach that can be used as an add-on to existing systems and other query optimizations. We start with query execution plans that are generated by sequential optimizers. Using multithreading, we exploit parallelism within queries and within join operators, which leverages the new performance opportunities in modern multicore hardware. Our query performance optimization is adaptive and employs QJetpack, a feedback-directed auto-tuner, in a novel way. It iteratively partitions query execution plans by detecting performance patterns that are pre-benchmarked on each platform. Then, the auto-tuner steers the application of parallel transformations based on query run-time feedback. This paper focuses on difficult scenarios with I/O-intensive join queries and shows that we can speed up query execution despite significant I/O limitations. The performance of all benchmarked queries could be improved, with low tuning overhead, on all of our multicore platforms.},
  doi       = {10.1109/icpp.2011.24},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1095570190},
}

@InProceedings{Pankratius2011a,
  author    = {Pankratius, Victor},
  booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
  title     = {Automated usability evaluation of parallel programming constructs (NIER track)},
  year      = {2011},
  pages     = {936-939},
  abstract  = {Multicore computers are ubiquitous, and proposals to extend existing languages with parallel constructs mushroom. While everyone claims to make parallel programming easier and less error-prone, empirical language usability evaluations are rarely done in-the-field with many users and real programs. Key obstacles are costs and a lack of appropriate environments to gather enough data for representative conclusions. This paper discusses the idea of automating the usability evaluation of parallel language constructs by gathering subjective and objective data directly in every software engineer's IDE. The paper presents an Eclipse prototype suite that can aggregate such data from potentially hundreds of thousands of programmers. Mismatch detection in subjective and objective feedback as well as construct usage mining can improve language design at an early stage, thus reducing the risk of developing and maintaining inappropriate constructs. New research directions arising from this idea are outlined for software repository mining, debugging, and software economics.},
  doi       = {10.1145/1985793.1985951},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1027165238},
}

@InProceedings{Zwinkau2012,
  author    = {Zwinkau, Andreas and Pankratius, Victor},
  booktitle = {2012 IEEE 18th International Conference on Parallel and Distributed Systems},
  title     = {AutoTunium: An Evolutionary Tuner for General-Purpose Multicore Applications},
  year      = {2012},
  pages     = {392-399},
  abstract  = {Today's increasing diversity in multicore hardware challenges programmers when it comes to software performance optimization and portability. As multicore processors are in almost every PC and server, programmers now have to parallelize a larger spectrum of applications, many of which are non-numerical. To obtain good performance, programmers typically try out different software tuning parameter configurations on each platform. However, this manual approach to finding good configurations in the search space is impractical due to combinatorial explosion, but yet it is common practice due to lack of alternatives for general programs. This paper presents a smarter way to tackle this problem algorithmically for a variety of multicore applications, including non-numerical ones. Our work introduces AutoTunium, a novel feedback-directed optimizer that automates the application tuning process with evolutionary search strategies. The software infrastructure is easy to use and integrated in the popular Eclipse environment. It collects run-time information to predict parameter configurations that are likely to lead to good performance in future runs, and configures programs for production runs in the best possible way. We quantify the effectiveness of various tuning strategies on a diverse set of real applications and multicore platforms. The evaluation shows that AutoTunium's evolutionary strategies work well despite the broad scope of applications and perform better in this context than other simplex-based search algorithms. Our insights are derived from model-based analyses as well as from performance analyses with real programs in the PARSEC benchmark suite.},
  doi       = {10.1109/icpads.2012.61},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1094511177},
}

@Article{Lin2011,
  author   = {Lin, Chao-Sheng and Lu, Chun-Hsien and Lin, Shang-Wei and Chen, Yean-Ru and Hsiung, Pao-Ann},
  journal  = {Journal of Computer Science and Technology},
  title    = {VERTAF/Multi-Core: A SysML-Based Application Framework for Multi-Core Embedded Software Development},
  year     = {2011},
  number   = {3},
  pages    = {448-462},
  volume   = {26},
  abstract = {Multi-core processors are becoming prevalent rapidly in personal computing and embedded systems. Nevertheless, the programming environment for multi-core processor-based systems is still quite immature and lacks efficient tools. In this work, we present a new VERTAF/Multi-Core framework and show how software code can be automatically generated from SysML models of multi-core embedded systems. We illustrate how model-driven design based on SysML can be seamlessly integrated with Intel’s threading building blocks (TBB) and the quantum framework (QF) middleware. We use a digital video recording system to illustrate the benefits of the framework. Our experiments show how SysML/QF/TBB help in making multi-core embedded system programming model-driven, easy, and efficient.},
  doi      = {10.1007/s11390-011-1146-3},
  groups   = {Multicore Performance Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1047632364},
}

@Article{Rodrigues2011,
  author   = {Rodrigues, Antonio Wendell De Oliveira and Guyomarc'H, Frédéric and Dekeyser, Jean-Luc},
  journal  = {arXiv},
  title    = {A Modeling Approach based on UML/MARTE for GPU Architecture},
  year     = {2011},
  abstract = {Nowadays, the High Performance Computing is part of the context of embedded
systems. Graphics Processing Units (GPUs) are more and more used in
acceleration of the most part of algorithms and applications. Over the past
years, not many efforts have been done to describe abstractions of applications
in relation to their target architectures. Thus, when developers need to
associate applications and GPUs, for example, they find difficulty and prefer
using API for these architectures. This paper presents a metamodel extension
for MARTE profile and a model for GPU architectures. The main goal is to
specify the task and data allocation in the memory hierarchy of these
architectures. The results show that this approach will help to generate code
for GPUs based on model transformations using Model Driven Engineering (MDE).},
  doi      = {10.48550/arxiv.1105.4424},
  groups   = {Multicore Performance Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1119355454},
}

@Article{Rodrigues2011a,
  author   = {Rodrigues, Wendell and Guyomarc'h, Frédéric and Dekeyser, Jean-Luc},
  journal  = {arXiv},
  title    = {Programming Massively Parallel Architectures using MARTE: a Case Study},
  year     = {2011},
  abstract = {Nowadays, several industrial applications are being ported to parallel
architectures. These applications take advantage of the potential parallelism
provided by multiple core processors. Many-core processors, especially the
GPUs(Graphics Processing Unit), have led the race of floating-point performance
since 2003. While the performance improvement of general- purpose
microprocessors has slowed significantly, the GPUs have continued to improve
relentlessly. As of 2009, the ratio between many-core GPUs and multicore CPUs
for peak floating-point calculation throughput is about 10 times. However, as
parallel programming requires a non-trivial distribution of tasks and data,
developers find it hard to implement their applications effectively. Aiming to
improve the use of many-core processors, this work presents an case-study using
UML and MARTE profile to specify and generate OpenCL code for intensive signal
processing applications. Benchmark results show us the viability of the use of
MDE approaches to generate GPU applications.},
  doi      = {10.48550/arxiv.1103.4881},
  groups   = {Multicore Performance Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1119340740},
}

@InProceedings{Luebke2008,
  author    = {Luebke, David},
  booktitle = {2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro},
  title     = {CUDA: SCALABLE PARALLEL PROGRAMMING FOR HIGH-PERFORMANCE SCIENTIFIC COMPUTING},
  year      = {2008},
  pages     = {836-838},
  abstract  = {Graphics processing units (GPUs) originally designed for computer video cards have emerged as the most powerful chip in a high-performance workstation. Unlike multicore CPU architectures, which currently ship with two or four cores, GPU architectures are “manycore” with hundreds of cores capable of running thousands of threads in parallel. NVIDIA's CUDA is a co-evolved hardware-software architecture that enables high-performance computing developers to harness the tremendous computational power and memory bandwidth of the GPU in a familiar programming environment - the C programming language. We describe the CUDA programming model and motivate its use in the biomedical imaging community.},
  doi       = {10.1109/isbi.2008.4541126},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1095265173},
}

@Article{Hadjidoukas2009,
  author   = {Hadjidoukas, P.E. and Philos, G.Ch. and Dimakopoulos, V.V.},
  journal  = {Scientific Programming},
  title    = {Exploiting Fine-Grain Thread Parallelism on Multicore Architectures},
  year     = {2009},
  note     = {https://downloads.hindawi.com/journals/sp/2009/249651.pdf},
  number   = {4},
  pages    = {309-323},
  volume   = {17},
  abstract = {In this work we present a runtime threading system which provides an efficient substrate for fine-grain parallelism, suitable for deployment in multicore platforms. Its architecture encompasses a number of optimizations that make it particularly effective in managing a large number of threads and with low overheads. The runtime system has been integrated into an OpenMP implementation to allow for transparent usage under a high level programming paradigm. We evaluate our implementation on two multicore systems using synthetic microbenchmarks and a real-time face detection application.},
  doi      = {10.1155/2009/249651},
  groups   = {Multicore Performance Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1063202943},
}

@InProceedings{Rabenseifner2009,
  author    = {Rabenseifner, R. and Hager, G. and Jost, G.},
  booktitle = {2009 17th Euromicro International Conference on Parallel, Distributed and Network-based Processing},
  title     = {Hybrid MPI/OpenMP Parallel Programming on Clusters of Multi-Core SMP Nodes},
  year      = {2009},
  pages     = {427-436},
  abstract  = {Today most systems in high-performance computing (HPC) feature a hierarchical hardware design: Shared memory nodes with several multi-core CPUs are connected via a network infrastructure. Parallel programming must combine distributed memory parallelization on the node interconnect with shared memory parallelization inside each node. We describe potentials and challenges of the dominant programming models on hierarchically structured hardware: Pure MPI (Message Passing Interface), pure OpenMP (with distributed shared memory extensions) and hybrid MPI+OpenMP in several flavors. We pinpoint cases where a hybrid programming model can indeed be the superior solution because of reduced communication needs and memory consumption, or improved load balance. Furthermore we show that machine topology has a significant impact on performance for all parallelization strategies and that topology awareness should be built into all applications in the future. Finally we give an outlook on possible standardization goals and extensions that could make hybrid programming easier to do with performance in mind.},
  doi       = {10.1109/pdp.2009.43},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1093861837},
}

@InProceedings{Martinez2011,
  author    = {Martinez, Gabriel and Gardner, Mark and Feng, Wu-chun},
  booktitle = {2011 IEEE 17th International Conference on Parallel and Distributed Systems},
  title     = {CU2CL: A CUDA-to-OpenCL Translator for Multi- and Many-core Architectures},
  year      = {2011},
  note      = {https://vtechworks.lib.vt.edu/bitstreams/511c5d1a-75bd-4f13-bd6f-64dc98c2cb45/download},
  pages     = {300-307},
  abstract  = {The use of graphics processing units (GPUs) in high-performance parallel computing continues to become more prevalent, often as part of a heterogeneous system. For years, CUDA has been the de facto programming environment for nearly all general-purpose GPU (GPGPU) applications. In spite of this, the framework is available only on NVIDIA GPUs, traditionally requiring reimplementation in other frameworks in order to utilize additional multi- or many-core devices. On the other hand, OpenCL provides an open and vendor-neutral programming environment and runtime system. With implementations available for CPUs, GPUs, and other types of accelerators, OpenCL therefore holds the promise of a “write once, run anywhere” ecosystem for heterogeneous computing. Given the many similarities between CUDA and OpenCL, manually porting a CUDA application to OpenCL is typically straightforward, albeit tedious and error-prone. In response to this issue, we created CU2CL, an automated CUDA-to-OpenCL source-to-source translator that possesses a novel design and clever reuse of the Clang compiler framework. Currently, the CU2CL translator covers the primary constructs found in CUDA runtime API, and we have successfully translated many applications from the CUDA SDK and Rodinia benchmark suite. The performance of the automatically translated applications via CU2CL is on par with their manually ported counterparts.},
  doi       = {10.1109/icpads.2011.48},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1094182579},
}

@Article{Diaz2012,
  author   = {Diaz, J. and Munoz-Caro, C. and Nino, A.},
  journal  = {IEEE Transactions on Parallel and Distributed Systems},
  title    = {A Survey of Parallel Programming Models and Tools in the Multi and Many-Core Era},
  year     = {2012},
  number   = {8},
  pages    = {1369-1386},
  volume   = {23},
  abstract = {In this work, we present a survey of the different parallel programming models and tools available today with special consideration to their suitability for high-performance computing. Thus, we review the shared and distributed memory approaches, as well as the current heterogeneous parallel programming model. In addition, we analyze how the partitioned global address space (PGAS) and hybrid parallel programming models are used to combine the advantages of shared and distributed memory systems. The work is completed by considering languages with specific parallel support and the distributed programming paradigm. In all cases, we present characteristics, strengths, and weaknesses. The study shows that the availability of multi-core CPUs has given new impulse to the shared memory parallel programming approach. In addition, we find that hybrid parallel programming is the current way of harnessing the capabilities of computer clusters with multi-core nodes. On the other hand, heterogeneous programming is found to be an increasingly popular paradigm, as a consequence of the availability of multi-core CPUs+GPUs systems. The use of open industry standards like OpenMP, MPI, or OpenCL, as opposed to proprietary solutions, seems to be the way to uniformize and extend the use of parallel programming models.},
  doi      = {10.1109/tpds.2011.308},
  groups   = {Multicore Performance Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061753868},
}

@InBook{Iwainsky2015,
  author    = {Iwainsky, Christian and Shudler, Sergei and Calotoiu, Alexandru and Strube, Alexandre and Knobloch, Michael and Bischof, Christian and Wolf, Felix},
  pages     = {451-463},
  title     = {How Many Threads will be too Many? On the Scalability of OpenMP Implementations},
  year      = {2015},
  note      = {https://link.springer.com/content/pdf/10.1007/978-3-662-48096-0_35.pdf},
  abstract  = {Exascale systems will exhibit much higher degrees of parallelism both in terms of the number of nodes and the number of cores per node. OpenMP is a widely used standard for exploiting parallelism on the level of individual nodes. Although successfully used on today’s systems, it is unclear how well OpenMP implementations will scale to much higher numbers of threads. In this work, we apply automated performance modeling to examine the scalability of OpenMP constructs across different compilers and platforms. We ran tests on Intel Xeon multi-board, Intel Xeon Phi, and Blue Gene with compilers from GNU, IBM, Intel, and PGI. The resulting models reveal a number of scalability issues in implementations of OpenMP constructs and show unexpected differences between compilers.},
  booktitle = {Euro-Par 2015: Parallel Processing},
  doi       = {10.1007/978-3-662-48096-0_35},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1052167219},
}

@InProceedings{Gray2012,
  author    = {Gray, Ian and Audsley, Neil C.},
  booktitle = {2012 23rd IEEE International Symposium on Rapid System Prototyping (RSP)},
  title     = {Challenges in Software Development for Multicore System-on-Chip Development},
  year      = {2012},
  pages     = {115-121},
  abstract  = {Multiprocessor Systems-on-Chip (MPSoC)-based platforms are becoming more common in the embedded domain. Such systems are a significant deviation from the homogeneous, uniprocessor architectures that have been traditionally employed by embedded designers, thereby making the software development process to effectively target the platform more challenging. Low-resource embedded systems rely on efficient implementations that are not well supported by traditional solutions based on architecture virtualisation or middleware. Within this paper we examine these challenges and discuss ways in which they can be mitigated. In particular, we focus on the contributions made by two recent approaches based on Model-Driven Engineering (MDE). We also discuss challenges for future research.},
  doi       = {10.1109/rsp.2012.6380699},
  groups    = {Multicore Performance Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1094355382},
}

@Article{Bini2011,
  author   = {Bini, E and Buttazzo, G and Eker, J and Schorr, S and Guerra, R and Fohler, G and Arzen, Karl-Erik and Romero, V and Scordino, C},
  journal  = {IEEE Micro},
  title    = {Resource Management on Multicore Systems: The ACTORS Approach},
  year     = {2011},
  number   = {3},
  pages    = {72-81},
  volume   = {31},
  abstract = {High-performance embedded systems require the execution of many applications on multicore platforms and are subject to stringent restrictions and constraints. The ACTORS project approach provides temporal isolation through resource reservation over a multicore platform, adapting the available resources on the basis of the overall quality requirements. The architecture is fully operational on both ARM MPCore and x86 multicore platforms.},
  doi      = {10.1109/mm.2011.1},
  groups   = {Multicore Performance Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061408760},
}

@Article{Jamshidi2014,
  author   = {Jamshidi, Pooyan and Ahmad, Aakash and Pahl, Claus},
  journal  = {IEEE Transactions on Cloud Computing},
  title    = {Cloud Migration Research: A Systematic Review},
  year     = {2014},
  note     = {https://doras.dcu.ie/19636/1/TCC-AuthorsVersion.pdf},
  number   = {2},
  pages    = {142-157},
  volume   = {1},
  abstract = {Background--By leveraging cloud services, organizations can deploy their software systems over a pool of resources. However, organizations heavily depend on their business-critical systems, which have been developed over long periods. These legacy applications are usually deployed on-premise. In recent years, research in cloud migration has been carried out. However, there is no secondary study to consolidate this research. Objective--This paper aims to identify, taxonomically classify, and systematically compare existing research on cloud migration. Method--We conducted a systematic literature review (SLR) of 23 selected studies, published from 2010 to 2013. We classified and compared the selected studies based on a characterization framework that we also introduce in this paper. Results--The research synthesis results in a knowledge base of current solutions for legacy-to-cloud migration. This review also identifies research gaps and directions for future research. Conclusion--This review reveals that cloud migration research is still in early stages of maturity, but is advancing. It identifies the needs for a migration framework to help improving the maturity level and consequently trust into cloud migration. This review shows a lack of tool support to automate migration tasks. This study also identifies needs for architectural adaptation and self-adaptive cloud-enabled systems.},
  doi      = {10.1109/tcc.2013.10},
  groups   = {Cloud Migration},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1061541753},
}

@InBook{Renger2008,
  author    = {Renger, Michiel and Kolfschoten, Gwendolyn L. and de Vreede, Gert-Jan},
  pages     = {61-77},
  title     = {Challenges in Collaborative Modeling: A Literature Review},
  year      = {2008},
  abstract  = {Modeling is a key activity in conceptual design and system design. Users as well as stakeholders, experts and entrepreneurs need to be able to create shared understanding about a system representation. In this paper we conducted a literature review to provide an overview of studies in which collaborative modeling efforts have been conducted to give first insights in the challenges of collaborative modeling, specifically with respect to group composition, collaboration & participation methods, modeling methods and quality in collaborative modeling. We found a critical challenge in dealing with the lack of modeling skills, such as having a modeler to support the group, or create the model for the group versus training to empower participants to actively participate in the modeling effort, and another critical challenge in resolving conflicting (parts of) models and integration of submodels or models from different perspectives. The overview of challenges presented in this paper will inspire the design of methods and support systems that will ultimately advance the efficiency and effectiveness of collaborative modeling tasks.},
  booktitle = {Advances in Enterprise Engineering I},
  doi       = {10.1007/978-3-540-68644-6_5},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1043355881},
}

@InProceedings{Mair2005,
  author    = {Mair, Carolyn and Shepperd, Martin and Jørgensen, Magne},
  booktitle = {Proceedings of the 2005 workshop on Predictor models in software engineering},
  title     = {An analysis of data sets used to train and validate cost prediction systems},
  year      = {2005},
  note      = {http://promise.site.uottawa.ca/proceedings/pdf/8.pdf},
  pages     = {1-6},
  abstract  = {OBJECTIVE - to build up a picture of the nature and type of data sets being used to develop and evaluate different software project effort prediction systems. We believe this to be important since there is a growing body of published work that seeks to assess different prediction approaches.METHOD - we performed an exhaustive search from 1980 onwards from three software engineering journals for research papers that used project data sets to compare cost prediction systems.RESULTS - this identified a total of 50 papers that used, one or more times, a total of 71 unique project data sets. We observed that some of the better known and easily accessible data sets were used repeatedly making them potentially disproportionately influential. Such data sets also tend to be amongst the oldest with potential problems of obsolescence. We also note that only about 60% of all data sets are in the public domain. Finally, extracting relevant information from research papers has been time consuming due to different styles of presentation and levels of contextural information.CONCLUSIONS - first, the community needs to consider the quality and appropriateness of the data set being utilised; not all data sets are equal. Second, we need to assess the way results are presented in order to facilitate meta-analysis and whether a standard protocol would be appropriate.},
  doi       = {10.1145/1083165.1083166},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1036043271},
}

@Article{Kagdi2007,
  author   = {Kagdi, Huzefa and Collard, Michael L. and Maletic, Jonathan I.},
  journal  = {Journal of Software Maintenance and Evolution Research and Practice},
  title    = {A survey and taxonomy of approaches for mining software repositories in the context of software evolution},
  year     = {2007},
  note     = {http://www.cs.kent.edu/~jmaletic/papers/JSME07-survey.pdf},
  number   = {2},
  pages    = {77-131},
  volume   = {19},
  abstract = {Abstract A comprehensive literature survey on approaches for mining software repositories (MSR) in the context of software evolution is presented. In particular, this survey deals with those investigations that examine multiple versions of software artifacts or other temporal information. A taxonomy is derived from the analysis of this literature and presents the work via four dimensions: the type of software repositories mined (what), the purpose (why), the adopted/invented methodology used (how), and the evaluation method (quality). The taxonomy is demonstrated to be expressive (i.e., capable of representing a wide spectrum of MSR investigations) and effective (i.e., facilitates similarities and comparisons of MSR investigations). Lastly, a number of open research issues in MSR that require further investigation are identified. Copyright © 2007 John Wiley & Sons, Ltd.},
  doi      = {10.1002/smr.344},
  groups   = {Cloud Migration},
  url      = {https://app.dimensions.ai/details/publication/pub.1012689387},
}

@InBook{Davis2007,
  author    = {Davis, Alan and Hickey, Ann and Dieste, Oscar and Juristo, Natalia and Moreno, Ana},
  pages     = {129-143},
  title     = {A Quantitative Assessment of Requirements Engineering Publications – 1963–2006},
  year      = {2007},
  abstract  = {Requirements engineering research has been conducted for over 40 years. It is important to recognize the plethora of results accumulated to date to: (a) improve researchers’ understanding of the historical roots of our field in the real-world and the problems that they are trying to solve, (b) expose researchers to the breadth and depth of solutions that have been proposed, (c) provide a synergistic basis for improving those solutions or building new ones to solve real-world problems facing the industry today, and d) increase practitioner awareness of available solutions. A detailed meta-analysis of the requirements engineering literature will provide an objective overview of the advances and current state of the discipline. This paper represents the first step in a planned multi-year analysis. It presents the results of a demographic analysis by date, type, outlet, author, and author affiliation for an existing database of over 4,000 requirements engineering publications.},
  booktitle = {Requirements Engineering: Foundation for Software Quality},
  doi       = {10.1007/978-3-540-73031-6_10},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1019803350},
}

@InProceedings{Yalaho2006,
  author    = {Yalaho, Anicet},
  booktitle = {2006 10th IEEE International Enterprise Distributed Object Computing Conference Workshops (EDOCW'06)},
  title     = {A Conceptual Model of ICT-Supported Unified Process of International Outsourcing of Software Production},
  year      = {2006},
  pages     = {47-47},
  abstract  = {This is an ongoing research in international outsourcing software production. This research examines how Software production through the ICT-supported unified process of international outsourcing could be executed and managed effectively. To address this research question, the results of an in-depth literature review in the areas of outsourcing, international outsourcing, information technology, and international software production is presented. This study proposes the information communication technologies' (ICT) - supported unified process model of international outsourcing of software production (SUPMIOSP). ICT-SUPMIOSP provides a detailed guideline on how to manage the entire process of international outsourcing by integrating a number of key issues such as relationship and risks management. Both theoretical and practical aspects of ICT-SUPMIOSP are presented. At the theoretical level, the model can be used as a basis for further research, while at the practical level, it helps managers and other stakeholders to understand the multiple activities involved in offshore outsourcing, improve, systematize, and execute the ICT-SUPIOSP more effectively and efficiently.},
  doi       = {10.1109/edocw.2006.5},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1095674802},
}

@Article{Mohagheghi2007,
  author   = {Mohagheghi, Parastoo and Conradi, Reidar},
  journal  = {Empirical Software Engineering},
  title    = {Quality, productivity and economic benefits of software reuse: a review of industrial studies},
  year     = {2007},
  number   = {5},
  pages    = {471-516},
  volume   = {12},
  abstract = {Systematic software reuse is proposed to increase productivity and software quality and lead to economic benefits. Reports of successful software reuse programs in industry have been published. However, there has been little effort to organize the evidence systematically and appraise it. This review aims to assess the effects of software reuse in industrial contexts. Journals and major conferences between 1994 and 2005 were searched to find observational studies and experiments conducted in industry, returning eleven papers of observational type. Systematic software reuse is significantly related to lower problem (defect, fault or error) density in five studies and to decreased effort spent on correcting problems in three studies. The review found evidence for significant gains in apparent productivity in three studies. Other significant benefits of software reuse were reported in single studies or the results were inconsistent. Evidence from industry is sparse and combining results was done by vote-counting. Researchers should pay more attention to using comparable metrics, performing longitudinal studies, and explaining the results and impact on industry. For industry, evaluating reuse of COTS or OSS components, integrating reuse activities in software processes, better data collection and evaluating return on investment are major challenges.},
  doi      = {10.1007/s10664-007-9040-x},
  groups   = {Cloud Migration},
  url      = {https://app.dimensions.ai/details/publication/pub.1052578288},
}

@InBook{Hosbond2005,
  author    = {Hosbond, Jens Henrik and Nielsen, Peter Axel},
  pages     = {215-232},
  title     = {Mobile Systems Development: A Literature Review},
  year      = {2005},
  note      = {https://link.springer.com/content/pdf/10.1007/0-387-28918-6_17.pdf},
  abstract  = {This article reviews 105 representative contributions to the literature on mobile systems development. The contributions are categorized according to a simple conceptual framework. The framework comprises four perspectives: the requirements perspective, the technology perspective, the application perspective, and the business perspective. Our literature review shows that mobile systems development is overlooked in the current debate. From the review, we extend the traditional view on systems development to encompass mobile systems and, based on the identified perspectives, we propose core characteristics for mobile systems. We also extend the traditional focus found in systems development on processes in a development project to encompass the whole of the development company as well as interorganizational linkage between development companies. Finally, we point at research directions emerging from the review that are relevant to the field of mobile systems development.},
  booktitle = {Designing Ubiquitous Information Environments: Socio-Technical Issues and Challenges},
  doi       = {10.1007/0-387-28918-6_17},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1031618271},
}

@Article{BELLINI2008,
  author   = {BELLINI, CARLO GABRIEL PORTO and PEREIRA, RITA DE CÁSSIA DE FARIA and BECKER, JOÃO LUIZ},
  journal  = {International Journal of Software Engineering and Knowledge Engineering},
  title    = {MEASUREMENT IN SOFTWARE ENGINEERING: FROM THE ROADMAP TO THE CROSSROADS},
  year     = {2008},
  number   = {01},
  pages    = {37-64},
  volume   = {18},
  abstract = {Research on software measurement can be organized around five key conceptual and methodological issues: how to apply measurement theory to software, how to frame software metrics, how to develop metrics, how to collect core measures, and how to analyze measures. The subject is of special concern for the industry, which is interested in improving practices — mainly in developing countries, where the software industry represents an opportunity for growth and usually receives institutional support for matching international quality standards. Academics are also in need of understanding and developing more effective methods for managing the software process and assessing the success of products and services, as a result of an enhanced awareness about the emergency of aligning business processes and information systems. This paper unveils the fundamentals of measurement in software engineering and discusses current issues and foreseeable trends for the subject. A literature review was performed within major academic publications in the last decade, and findings suggest a sensible shift of measurement interests towards managing the software process as a whole — without losing from sight the customary focus on hard issues like algorithm efficiency and worker productivity.},
  doi      = {10.1142/s021819400800357x},
  groups   = {Cloud Migration},
  url      = {https://app.dimensions.ai/details/publication/pub.1062959392},
}

@InProceedings{Boer2008,
  author    = {de Boer, Remco C. and Farenhorst, Rik},
  booktitle = {Proceedings of the 3rd international workshop on Sharing and reusing architectural knowledge},
  title     = {In search of `architectural knowledge&#x27;},
  year      = {2008},
  pages     = {71-78},
  abstract  = {The software architecture community puts more and more emphasis on 'architectural knowledge'. However, there appears to be no commonly accepted definition of what architectural knowledge entails, which makes it a fuzzy concept. In order to obtain a better understanding of how different authors view 'architectural knowledge', we have conducted a systematic review to examine how architectural knowledge is defined and how the different definitions in use are related. From this review it became clear that many authors do not provide a concrete definition of what they think architectural knowledge entails. What is more intriguing, though, is that those who do give a definition seem to agree that architectural knowledge spans from problem domain through decision making to solution; an agreement that is not obvious from the definitions themselves, but which is only brought to light after careful systematic comparison of the different studies.},
  doi       = {10.1145/1370062.1370080},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1014654869},
}

@Article{Neto2008,
  author   = {Neto, Arilo Dias and Subramanyan, Rajesh and Vieira, Marlon and Travassos, Guilherme Horta and Shull, Forrest},
  journal  = {IEEE Software},
  title    = {Improving Evidence about Software Technologies: A Look at Model-Based Testing},
  year     = {2008},
  number   = {3},
  pages    = {10-13},
  volume   = {25},
  abstract = {A rich body of experiences hasn't yet been published on all the software development techniques researchers have proposed. In fact, by some estimates, the techniques for which we do have substantial experience are few and far between. When we started looking at the evidence on model-based testing (MBT), we thought we'd come across some strong studies that showed this approach's capabilities compared to conventional testing techniques-this wasn't the case. However, we can still extract some useful knowledge and also discuss some issues that are relevant to other software technologies with similar types of evidence.},
  doi      = {10.1109/ms.2008.64},
  groups   = {Cloud Migration},
  url      = {https://app.dimensions.ai/details/publication/pub.1061420973},
}

@InBook{Harjumaa2008,
  author    = {Harjumaa, Lasse and Markkula, Jouni and Oivo, Markku},
  pages     = {230-243},
  title     = {How Does a Measurement Programme Evolve in Software Organizations?},
  year      = {2008},
  abstract  = {Establishing a software measurement programme within an organization is not a straightforward task. Previous literature surveys have focused on software process improvement in general and software measurement has been analysed in case studies. This literature survey collects the data from separate cases and presents the critical success factors that are specific to software measurement programmes. We present a categorization of the success factors based on organizational roles that are involved in measurement. Furthermore, the most essential elements of success in different phases of the life cycle of the measurement programme are analysed. It seems that the role of upper management is crucial when starting measurement and the individual developers’ impact increases in the later phases. Utilization of the measurement data and improvement of the measurement and development processes requires active management support again.},
  booktitle = {Product-Focused Software Process Improvement},
  doi       = {10.1007/978-3-540-69566-0_20},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1012774582},
}

@InProceedings{HA¶st2005,
  author    = {HÃ¶st, Martin and Wohlin, Claes and Thelin, Thomas},
  booktitle = {Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.},
  title     = {Experimental Context Classification: Incentives and Experience of Subjects},
  year      = {2005},
  pages     = {470-478},
  abstract  = {There is a need to identify factors that affect the result of empirical studies in software engineering research. It is still the case that seemingly identical replications of controlled experiments result in different conclusions due to the fact that all factors describing the experiment context are not clearly defined and hence controlled. In this article, a scheme for describing the participants of controlled experiments is proposed and evaluated. It consists of two main factors, the incentives for participants in the experiment and theexperience of the participants. The scheme has been evaluated by classifying a set of previously conducted experiments from literature. It can be concluded that the scheme was easy to use and understand. It is also found that experiments that are classified in the same way to a large extent point at the same results, which indicates that the scheme addresses relevant factors.},
  doi       = {10.1109/icse.2005.1553590},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1093218858},
}

@Article{Jorgensen2005,
  author   = {Jorgensen, M.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Evidence-based guidelines for assessment of software development cost uncertainty},
  year     = {2005},
  number   = {11},
  pages    = {942-954},
  volume   = {31},
  abstract = {Several studies suggest that uncertainty assessments of software development costs are strongly biased toward overconfidence, i.e., that software cost estimates typically are believed to be more accurate than they really are. This overconfidence may lead to poor project planning. As a means of improving cost uncertainty assessments, we provide evidence-based guidelines for how to assess software development cost uncertainty, based on results from relevant empirical studies. The general guidelines provided are: 1) Do not rely solely on unaided, intuition-based uncertainty assessment processes, 2) do not replace expert judgment with formal uncertainty assessment models, 3) apply structured and explicit judgment-based processes, 4) apply strategies based on an outside view of the project, 5) combine uncertainty assessments from different sources through group work, not through mechanical combination, 6) use motivational mechanisms with care and only if greater effort is likely to lead to improved assessments, and 7) frame the assessment problem to fit the structure of the relevant uncertainty information and the assessment process. These guidelines are preliminary and should be updated in response to new evidence.},
  doi      = {10.1109/tse.2005.128},
  groups   = {Cloud Migration},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788449},
}

@InProceedings{Davis2006,
  author    = {Davis, Alan and Dieste, Oscar and Hickey, Ann and Juristo, Natalia and Moreno, Ana M.},
  booktitle = {14th IEEE International Requirements Engineering Conference (RE'06)},
  title     = {Effectiveness of Requirements Elicitation Techniques: Empirical Results Derived from a Systematic Review},
  year      = {2006},
  pages     = {179-188},
  abstract  = {This paper reports a systematic review of empirical studies concerning the effectiveness of elicitation techniques, and the subsequent aggregation of empirical evidence gathered from those studies. The most significant results of the aggregation process are as follows: (1) Interviews, preferentially structured, appear to be one of the most effective elicitation techniques; (2) Many techniques often cited in the literature, like card sorting, ranking or thinking aloud, tend to be less effective than interviews; (3) Analyst experience does not appear to be a relevant factor; and (4) The studies conducted have not found the use of intermediate representations during elicitation to have significant positive effects. It should be noted that, as a general rule, the studies from which these results were aggregated have not been replicated, and therefore the above claims cannot be said to be absolutely certain. However, they can be used by researchers as pieces of knowledge to be further investigated and by practitioners in development projects, always taking into account that they are preliminary findings.},
  doi       = {10.1109/re.2006.17},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1095289700},
}

@InBook{Feller2006,
  author    = {Feller, Joseph and Finnegan, Patrick and Kelly, David and MacNamara, Maurice},
  pages     = {261-278},
  title     = {Developing Open Source Software: A Community-Based Analysis of Research},
  year      = {2006},
  note      = {https://link.springer.com/content/pdf/10.1007/0-387-34588-4_18.pdf},
  abstract  = {Open source software (OSS) creates the potential for the inclusion of large and diverse communities in every aspect of the software development and consumption life cycle. However, despite 6 years of effort by an ever growing research community, we still don’t know exactly what we do and don’t know about OSS, nor do we have a clear idea about the basis for our knowledge. This paper presents an analysis of 155 research artefacts in the area of open source software. The purpose of the study is to identify the kinds of open source project communities that have been researched, the kinds of research questions that have been asked, and the methodologies used by researchers. Emerging from the study is a clearer understanding of what we do and don’t know about open source software, and recommendations for future research efforts},
  booktitle = {Social Inclusion: Societal and Organizational Implications for Information Systems},
  doi       = {10.1007/0-387-34588-4_18},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1033313185},
}

@InProceedings{Liebchen2008,
  author    = {Liebchen, Gernot A. and Shepperd, Martin},
  booktitle = {Proceedings of the 4th international workshop on Predictor models in software engineering},
  title     = {Data sets and data quality in software engineering},
  year      = {2008},
  note      = {http://bura.brunel.ac.uk/bitstream/2438/1852/1/PROMISE2008_v16.pdf},
  pages     = {39-44},
  abstract  = {OBJECTIVE - to assess the extent and types of techniques used to manage quality within software engineering data sets. We consider this a particularly interesting question in the context of initiatives to promote sharing and secondary analysis of data sets. METHOD - we perform a systematic review of available empirical software engineering studies. RESULTS - only 23 out of the many hundreds of studies assessed, explicitly considered data quality. CONCLUSIONS - first, the community needs to consider the quality and appropriateness of the data set being utilised; not all data sets are equal. Second, we need more research into means of identifying, and ideally repairing, noisy cases. Third, it should become routine to use sensitivity analysis to assess conclusion stability with respect to the assumptions that must be made concerning noise levels.},
  doi       = {10.1145/1370788.1370799},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1023990088},
}

@Article{Wicks2007,
  author   = {Wicks, M.N. and Dewar, R.G.},
  journal  = {Journal of Systems and Software},
  title    = {A new research agenda for tool integration},
  year     = {2007},
  number   = {9},
  pages    = {1569-1585},
  volume   = {80},
  abstract = {This article highlights tool integration within software engineering environments. Tool integration concerns the techniques used to form coalitions of tools that provide an environment supporting some, or all, activities within a software engineering process. These techniques have been used to create environments that attempt to address aspects of software development, with varying success. This article provides a timely analysis and review of many of the significant projects in the field and, combined with evidence collected from industry, concludes by proposing an empirical manifesto for future research, where we see the need for work to justify tool integration efforts in terms of relevant socio-economic indicators.},
  doi      = {10.1016/j.jss.2007.03.089},
  groups   = {Cloud Migration},
  url      = {https://app.dimensions.ai/details/publication/pub.1008918541},
}

@InBook{Mohagheghi2008,
  author    = {Mohagheghi, Parastoo and Dehlen, Vegard},
  pages     = {432-443},
  title     = {Where Is the Proof? - A Review of Experiences from Applying MDE in Industry},
  year      = {2008},
  abstract  = {Model-Driven Engineering (MDE) has been promoted as a solution to handle the complexity of software development by raising the abstraction level and automating labor-intensive and error-prone tasks. However, few efforts have been made at collecting evidence to evaluate its benefits and limitations, which is the subject of this review. We searched several publication channels in the period 2000 to June 2007 for empirical studies on applying MDE in industry, which produced 25 papers for the review. Our findings include industry motivations for investigating MDE and the different domains it has been applied to. In most cases the maturity of third-party tool environments is still perceived as unsatisfactory for large-scale industrial adoption. We found reports of improvements in software quality and of both productivity gains and losses, but these reports were mainly from small-scale studies. There are a few reports on advantages of applying MDE in larger projects, however, more empirical studies and detailed data are needed to strengthen the evidence. We conclude that there is too little evidence to allow generalization of the results at this stage.},
  booktitle = {Model Driven Architecture – Foundations and Applications},
  doi       = {10.1007/978-3-540-69100-6_31},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1030075621},
}

@InProceedings{Segal2005,
  author    = {Segal, Judith and Grinyer, Antony and Sharp, Helen},
  booktitle = {Proceedings of the 2005 workshop on Realising evidence-based software engineering - REBSE '05},
  title     = {The type of evidence produced by empirical software engineers},
  year      = {2005},
  pages     = {1-4},
  abstract  = {This paper reports on the research published between the years 1997 and 2003 inclusive in the journal of Empirical Software Engineering, drawing on the taxonomy developed by Glass et al. in [3]. We found that the research was somewhat narrow in topic with about half the papers focusing on measurement/metrics, review and inspection; that researchers were almost as interested in formulating as in evaluating; that hypothesis testing and laboratory experiments dominated evaluations; that research was not very likely to focus on people and extremely unlikely to refer to other disciplines. We discuss our findings in the context of making empirical software engineering more relevant to practitioners.},
  doi       = {10.1145/1083174.1083176},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1004881437},
}

@Article{Hannay2008,
  author   = {Hannay, J.E. and Jorgensen, M.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {The Role of Deliberate Artificial Design Elements in Software Engineering Experiments},
  year     = {2008},
  number   = {2},
  pages    = {242-259},
  volume   = {34},
  abstract = {Increased realism in software engineering experiments is often promoted as an important means of increasing generalizability and industrial relevance. In this context, artificiality, e.g., the use of constructed tasks in place of realistic tasks, is seen as a threat. In this paper, we examine the opposite view that deliberately introduced artificial design elements may increase knowledge gain and enhance both generalizability and relevance. In the first part of this paper, we identify and evaluate arguments and examples in favor of and against deliberately introducing artificiality into software engineering experiments. We find that there are good arguments in favor of deliberately introducing artificial design elements to 1) isolate basic mechanisms, 2) establish the existence of phenomena, 3) enable generalization from particularly unfavorable to more favorable conditions (persistence of phenomena), and 4) relate experiments to theory. In the second part of this paper, we summarize a content analysis of articles that report software engineering experiments published over a 10-year period from 1993 to 2002. The analysis reveals a striving for realism and external validity, but little awareness of for what and when various degrees of artificiality and realism are appropriate. Furthermore, much of the focus on realism seems to be based on a narrow understanding of the nature of generalization. We conclude that an increased awareness and deliberation as to when and for what purposes both artificial and realistic design elements are applied is valuable for better knowledge gain and quality in empirical software engineering experiments. We also conclude that time spent on studies that have obvious threats to validity that are due to artificiality might be better spent on studies that investigate research questions for which artificiality is a strength rather than a weakness. However, arguments in favor of artificial design elements should not be used to justify studies that are badly designed or that have research questions of low relevance.},
  doi      = {10.1109/tse.2008.13},
  groups   = {Cloud Migration},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788671},
}

@InProceedings{Freire2007,
  author    = {Freire, Andre Pimenta and Goularte, Rudinei and de Mattos Fortes, Renata Pontin},
  booktitle = {Proceedings of the 25th annual ACM international conference on Design of communication},
  title     = {Techniques for developing more accessible web applications},
  year      = {2007},
  pages     = {162-169},
  abstract  = {The Web has become one of the most important communication media, since it is spread all over the world. In order to enable everyone to access this medium, Web accessibility has become an emerging topic, and many techniques have been evolved to support the development of accessible Web content. This paper presents a survey on techniques for Web accessibility and proposes a classification into the processes of ISO/IEC 12207 standard. The survey was carried out applying systematic review principles during the literature review. The results include analysis obtained from the synthesis of 53 studies, selected from an initial set of 844. Although the survey results indicate a growth in research on techniques for design and evaluation of Web applications, they also indicate that several development activities have been poorly addressed by scientific research efforts.},
  doi       = {10.1145/1297144.1297177},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1049407720},
}

@InBook{Hanssen2007,
  author    = {Hanssen, Geir Kjetil and Bjørnson, Finn Olav and Westerheim, Hans},
  pages     = {7-18},
  title     = {Tailoring and Introduction of the Rational Unified Process},
  year      = {2007},
  abstract  = {RUP is a comprehensive software development process framework that has gained a lot of interest by the industry. One major challenge of taking RUP into use is to tailor it to specific needs and then to introduce it into a development organization. This study presents a review and a systematic assembly of existing studies on the tailoring and introduction of RUP. From a systematic search for study reports on this topic we found that most research is anecdotal and focus on the effects of RUP itself. Only a few number of studies address tailoring and introduction. We have found that tailoring RUP is a considerable challenge by itself and that it must be closely related to existing best practices. We see a tendency of turning from large complete process frameworks towards smaller and more light-weight processes which may impose a smoother transition from process model to process in use.},
  booktitle = {Software Process Improvement},
  doi       = {10.1007/978-3-540-75381-0_2},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1011427038},
}

@Article{Kampenes2007,
  author   = {Kampenes, Vigdis By and Dybå, Tore and Hannay, Jo E. and Sjøberg, Dag I.K.},
  journal  = {Information and Software Technology},
  title    = {A systematic review of effect size in software engineering experiments},
  year     = {2007},
  number   = {11-12},
  pages    = {1073-1086},
  volume   = {49},
  abstract = {An effect size quantifies the effects of an experimental treatment. Conclusions drawn from hypothesis testing results might be erroneous if effect sizes are not judged in addition to statistical significance. This paper reports a systematic review of 92 controlled experiments published in 12 major software engineering journals and conference proceedings in the decade 1993–2002. The review investigates the practice of effect size reporting, summarizes standardized effect sizes detected in the experiments, discusses the results and gives advice for improvements. Standardized and/or unstandardized effect sizes were reported in 29% of the experiments. Interpretations of the effect sizes in terms of practical importance were not discussed beyond references to standard conventions. The standardized effect sizes computed from the reviewed experiments were equal to observations in psychology studies and slightly larger than standard conventions in behavioral science.},
  doi      = {10.1016/j.infsof.2007.02.015},
  groups   = {Cloud Migration},
  url      = {https://app.dimensions.ai/details/publication/pub.1043650304},
}

@Article{Staples2008,
  author   = {Staples, Mark and Niazi, Mahmood},
  journal  = {Information and Software Technology},
  title    = {Systematic review of organizational motivations for adopting CMM-based SPI},
  year     = {2008},
  number   = {7-8},
  pages    = {605-620},
  volume   = {50},
  abstract = {Background: Software Process Improvement (SPI) is intended to improve software engineering, but can only be effective if used. To improve SPI’s uptake, we should understand why organizations adopt SPI. CMM-based SPI approaches are widely known and studied. Objective: We investigated why organizations adopt CMM-based SPI approaches, and how these motivations relate to organizations’ size. Method: We performed a systematic review, examining reasons reported in more than forty primary studies. Results: Reasons usually related to product quality and project performance, and less commonly, to process. Organizations reported customer reasons infrequently and employee reasons very rarely. We could not show that reasons related to size. Conclusion: Despite its origins in helping to address customer-related issues for the USAF, CMM-based SPI has mostly been adopted to help organizations improve project performance and product quality issues. This reinforces a view that the goal of SPI is not to improve process per se, but instead to provide business benefits.},
  doi      = {10.1016/j.infsof.2007.07.003},
  groups   = {Cloud Migration},
  url      = {https://app.dimensions.ai/details/publication/pub.1017554634},
}

@InBook{Hoefer2007,
  author    = {Höfer, Andreas and Tichy, Walter F.},
  pages     = {10-19},
  title     = {Status of Empirical Research in Software Engineering},
  year      = {2007},
  abstract  = {We provide an assessment of the status of empirical software research by analyzing all refereed articles that appeared in the Journal of Empirical Software Engineering from its first issue in January 1996 through June 2006. The journal publishes empirical software research exclusively and it is the only journal to do so. The main findings are: 1. The dominant empirical methods are experiments and case studies. Other methods (correlational studies, meta analysis, surveys, descriptive approaches, ex post facto studies) occur infrequently; long-term studies are missing. About a quarter of the experiments are replications. 2. Professionals are used somewhat more frequently than students as subjects. 3. The dominant topics studied are measurement/metrics and tools/methods/frameworks. Metrics research is dominated by correlational and case studies without any experiments. 4. Important topics are underrepresented or absent, for example: programming languages, model driven development, formal methods, and others. The narrow focus on a few empirically researched topics is in contrast to the broad scope of software research.},
  booktitle = {Empirical Software Engineering Issues. Critical Assessment and Future Directions},
  doi       = {10.1007/978-3-540-71301-2_3},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1000653945},
}

@InProceedings{Shepperd2007,
  author    = {Shepperd, Martin},
  booktitle = {Future of Software Engineering (FOSE '07)},
  title     = {Software Project Economics: A Roadmap},
  year      = {2007},
  note      = {http://bura.brunel.ac.uk/bitstream/2438/1077/1/Shepperd_ProjectEconomics.pdf},
  pages     = {304-315},
  abstract  = {The objective of this paper is to consider research progress in the field of software project economics with a view to identifying important challenges and promising research directions. I argue that this is an important sub-discipline since this will underpin any cost-benefit analysis used to justify the resourcing, or otherwise, of a software project. To accomplish this I conducted a bibliometric analysis of peer reviewed research articles to identify major areas of activity. My results indicate that the primary goal of more accurate cost prediction systems remains largely unachieved. However, there are a number of new and promising avenues of research including: how we can combine results from primary studies, integration of multiple predictions and applying greater emphasis upon the human aspects of prediction tasks. I conclude that the field is likely to remain very challenging due to the people-centric nature of software engineering, since it is in essence a design task. Nevertheless the need for good economic models will grow rather than diminish as software becomes increasingly ubiquitous.},
  doi       = {10.1109/fose.2007.23},
  groups    = {Cloud Migration},
  url       = {https://app.dimensions.ai/details/publication/pub.1094400008},
}

@Article{Pino2007,
  author   = {Pino, Francisco J. and García, Félix and Piattini, Mario},
  journal  = {Software Quality Journal},
  title    = {Software process improvement in small and medium software enterprises: a systematic review},
  year     = {2007},
  number   = {2},
  pages    = {237-261},
  volume   = {16},
  abstract = {Small and medium enterprises are a very important cog in the gears of the world economy. The software industry in most countries is composed of an industrial scheme that is made up mainly of small and medium software enterprises—SMEs. To strengthen these types of organizations, efficient Software Engineering practices are needed—practices which have been adapted to their size and type of business. Over the last two decades, the Software Engineering community has expressed special interest in software process improvement (SPI) in an effort to increase software product quality, as well as the productivity of software development. However, there is a widespread tendency to make a point of stressing that the success of SPI is only possible for large companies. In this article, a systematic review of published case studies on the SPI efforts carried out in SMEs is presented. Its objective is to analyse the existing approaches towards SPI which focus on SMEs and which report a case study carried out in industry. A further objective is that of discussing the significant issues related to this area of knowledge, and to provide an up-to-date state of the art, from which innovative research activities can be thought of and planned.},
  doi      = {10.1007/s11219-007-9038-z},
  groups   = {Cloud Migration},
  url      = {https://app.dimensions.ai/details/publication/pub.1013552649},
}

@InProceedings{Abreu1996,
  author    = {Abreu, F. Brito e and Melo, W.},
  booktitle = {Proceedings of the 3rd International Software Metrics Symposium},
  title     = {Evaluating the impact of object-oriented design on software quality},
  year      = {1996},
  note      = {http://ctp.di.fct.unl.pt/QUASAR/Resources/Papers/Metrics/Metrics96.pdf},
  pages     = {90-99},
  abstract  = {Describes the results of a study where the impact of object-oriented (OO) design on software quality characteristics is experimentally evaluated. A suite of Metrics for OO Design (MOOD) was adopted to measure the use of OO design mechanisms. Data collected on the development of eight small-sized information management systems based on identical requirements were used to assess the referred impact. Data obtained in this experiment show how OO design mechanisms such as inheritance, polymorphism, information hiding and coupling, can influence quality characteristics like reliability or maintainability. Some predictive models based on OO design metrics are also presented.},
  doi       = {10.1109/metric.1996.492446},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1095624517},
}

@Article{Basili1996,
  author   = {Basili, V.R. and Briand, L.C. and Melo, W.L.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {A validation of object-oriented design metrics as quality indicators},
  year     = {1996},
  note     = {https://drum.lib.umd.edu/bitstreams/d038e3cc-d184-4dae-823b-3f39a6a400cb/download},
  number   = {10},
  pages    = {751-761},
  volume   = {22},
  abstract = {This paper presents the results of a study in which we empirically investigated the suite of object-oriented (OO) design metrics introduced in (Chidamber and Kemerer, 1994). More specifically, our goal is to assess these metrics as predictors of fault-prone classes and, therefore, determine whether they can be used as early quality indicators. This study is complementary to the work described in (Li and Henry, 1993) where the same suite of metrics had been used to assess frequencies of maintenance changes to classes. To perform our validation accurately, we collected data on the development of eight medium-sized information management systems based on identical requirements. All eight projects were developed using a sequential life cycle model, a well-known OO analysis/design method and the C++ programming language. Based on empirical and quantitative analysis, the advantages and drawbacks of these OO metrics are discussed. Several of Chidamber and Kemerer's OO metrics appear to be useful to predict class fault-proneness during the early phases of the life-cycle. Also, on our data set, they are better predictors than "traditional" code metrics, which can only be collected at a later phase of the software development processes.},
  doi      = {10.1109/32.544352},
  groups   = {Software Fault Prediction Metrics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061154182},
}

@Article{Ohlsson1996,
  author   = {Ohlsson, N. and Alberg, H.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Predicting fault-prone software modules in telephone switches},
  year     = {1996},
  number   = {12},
  pages    = {886-894},
  volume   = {22},
  abstract = {An empirical study was carried out at Ericsson Telecom AB to investigate the relationship between several design metrics and the number of function test failure reports associated with software modules. A tool, ERIMET, was developed to analyze the design documents automatically. Preliminary results from the study of 130 modules showed that: based on fault and design data one can satisfactorily build, before coding has started, a prediction model for identifying the most fault-prone modules. The data analyzed show that 20 percent of the most fault-prone modules account for 60 percent of all faults. The prediction model built in this paper would have identified 20 percent of the modules accounting for 47 percent of all faults. At least four design measures can alternatively be used as predictors with equivalent performance. The size (with respect to the number of lines of code) used in a previous prediction model was not significantly better than these four measures. The Alberg diagram introduced in this paper offers a way of assessing a predictor based on historical data, which is a valuable complement to linear regression when prediction data is ordinal. Applying the method described in this paper makes it possible to use measures at the design phase to predict the most fault-prone modules.},
  doi      = {10.1109/32.553637},
  groups   = {Software Fault Prediction Metrics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061154197},
}

@InProceedings{Binkley1998,
  author    = {Binkley, A.B. and Schach, S.R.},
  booktitle = {Proceedings of the 20th International Conference on Software Engineering},
  title     = {Validation of the coupling dependency metric as a predictor of run-time failures and maintenance measures},
  year      = {1998},
  pages     = {452-455},
  abstract  = {The coupling dependency metric (CDM) is a successful design quality metric. Here we apply it to four case studies: run-time failure data for a COBOL registration system; maintenance data for a C text-processing utility; maintenance data for a C++ patient collaborative care system; and maintenance data for a Java electronic file transfer facility. CDM outperformed a wide variety of competing metrics in predicting run-time failures and a number of different maintenance measures. These results imply that coupling metrics may be good predictors of levels of interaction within a software product.},
  doi       = {10.1109/icse.1998.671604},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1094566044},
}

@InProceedings{Briand1998,
  author    = {Briand, L.C. and Daly, J. and Porter, V. and Wust, J.},
  booktitle = {Proceedings Ninth International Symposium on Software Reliability Engineering (Cat. No.98TB100257)},
  title     = {Predicting fault-prone classes with design measures in object-oriented systems},
  year      = {1998},
  pages     = {334-343},
  abstract  = {The paper aims at empirically exploring the relationships between existing object oriented coupling, cohesion, and inheritance measures and the probability of fault detection in system classes during testing. The underlying goal of such a study is to better understand the relationship between existing product measurement in OO systems and the quality of the software developed. It is shown that by using a subset of existing measures, accurate models can be built to predict in which classes most of the faults are likely to lie in. By inspecting 48% of the classes, it is possible to find 95% of the faults. Besides the size of classes, the frequency of method invocations and the depth of inheritance hierarchies seem to be the main driving factors of fault proneness.},
  doi       = {10.1109/issre.1998.730898},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1094661240},
}

@InProceedings{Briand1998a,
  author    = {Briand, L.C. and Daly, J. and Porter, V. and Wust, J.},
  booktitle = {Proceedings Fifth International Software Metrics Symposium. Metrics (Cat. No.98TB100262)},
  title     = {A comprehensive empirical validation of design measures for object-oriented systems},
  year      = {1998},
  pages     = {246-257},
  abstract  = {This paper aims at empirically exploring the relationships between existing object-oriented coupling, cohesion, and inheritance measures and the probability of fault detection in system classes during testing. The underlying goal of such a study is to better understand the relationship between existing design measurement in OO systems and the quality of the software developed. Results show that many of the measures capture similar dimensions in the data set, thus reflecting the fact that many of them are based on similar principles and hypotheses. Besides the size of classes, the frequency of method invocations and the depth of inheritance hierarchies seem to be the main driving factors of fault-proneness.},
  doi       = {10.1109/metric.1998.731251},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1093776808},
}

@InProceedings{Munson1998,
  author    = {Munson, J.C. and Elbaum, S.G.},
  booktitle = {Proceedings. International Conference on Software Maintenance (Cat. No. 98CB36272)},
  title     = {Code churn: a measure for estimating the impact of code change},
  year      = {1998},
  pages     = {24-31},
  abstract  = {This study presents a methodology that will produce a viable fault surrogate. The focus of the effort is on the precise measurement of software development process and product outcomes. Tools and processes for the static measurement of the source code have been installed and made operational in a large embedded software system. Source code measurements have been gathered unobtrusively for each build in the software evolution process. The measurements are synthesized to obtain the fault surrogate. The complexity of sequential builds is compared and a new measure, code churn, is calculated. This paper demonstrates the effectiveness of code complexity churn by validating it against the testing problem reports.},
  doi       = {10.1109/icsm.1998.738486},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1094303700},
}

@InProceedings{Kamiya1999,
  author    = {Kamiya, T. and Kusumoto, S. and Inoue, K.},
  booktitle = {Proceedings 2nd IEEE International Symposium on Object-Oriented Real-Time Distributed Computing (ISORC'99) (Cat. No.99-61702)},
  title     = {Prediction of fault-proneness at early phase in object-oriented development},
  year      = {1999},
  note      = {http://iip-lab.ics.es.osaka-u.ac.jp/~lab-db/betuzuri/archive/271/271.pdf},
  pages     = {253-258},
  abstract  = {To analyse the complexity of object-oriented software, several metrics have been proposed. Among them, Chidamber and Kemerer's (1994) metrics are well-known object-oriented metrics. Also, their effectiveness has been empirically evaluated from the viewpoint of estimating the fault-proneness of object-oriented software. In the evaluations, these metrics were applied, not to the design specification but to the source code, because some of them measure the inner complexity of a class, and such information cannot be obtained until the algorithm and the class structure are determined at the end of the design phase. However, the estimation of the fault-proneness should be done in the early phase so as to effectively allocate effort for fixing the faults. This paper proposes a new method to estimate the fault-proneness of an object class in the early phase, using several complexity metrics for object-oriented software. In the proposed method, we introduce four checkpoints into the analysis/design/implementation phase, and we estimate the fault-prone classes using applicable metrics at each checkpoint.},
  doi       = {10.1109/isorc.1999.776386},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1094349395},
}

@InProceedings{Tang1999,
  author    = {Tang, Mei-Huei and Kao, Ming-Hung and Chen, Mei-Hwa},
  booktitle = {Proceedings Sixth International Software Metrics Symposium (Cat. No.PR00403)},
  title     = {An empirical study on object-oriented metrics},
  year      = {1999},
  pages     = {242-249},
  abstract  = {The objective of this study is the investigation of the correlation between object-oriented design metrics and the likelihood of the occurrence of object oriented faults. Such a relationship, if identified, can be utilized to select effective testing techniques that take the characteristics of the program under test into account. Our empirical study was conducted on three industrial real-time systems that contain a number of natural faults reported for the past three years. The faults found in these three systems are classified into three types: object-oriented faults, object management faults and traditional faults. The object-oriented design metrics suite proposed by Chidamber and Kemerer (1994) is validated using these faults. Moreover, we propose a set of new metrics that can serve as an indicator of how strongly object-oriented a program is, so that the decision to adopt object oriented testing techniques can be made, to achieve more reliable testing and yet minimize redundant testing efforts.},
  doi       = {10.1109/metric.1999.809745},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1093506486},
}

@Article{Graves2000,
  author   = {Graves, T.L. and Karr, A.F. and Marron, J.S. and Siy, H.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Predicting fault incidence using software change history},
  year     = {2000},
  number   = {7},
  pages    = {653-661},
  volume   = {26},
  abstract = {This paper is an attempt to understand the processes by which software ages. We define code to be aged or decayed if its structure makes it unnecessarily difficult to understand or change and we measure the extent of decay by counting the number of faults in code in a period of time. Using change management data from a very large, long-lived software system, we explore the extent to which measurements from the change history are successful in predicting the distribution over modules of these incidences of faults. In general, process measures based on the change history are more useful in predicting fault rates than product metrics of the code: For instance, the number of times code has been changed is a better indication of how many faults it will contain than is its length. We also compare the fault rates of code of various ages, finding that if a module is, on the average, a year older than an otherwise similar module, the older module will have roughly a third fewer faults. Our most successful model measures the fault potential of a module as the sum of contributions from all of the times the module has been changed, with large, recent changes receiving the most weight.},
  doi      = {10.1109/32.859533},
  groups   = {Software Fault Prediction Metrics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061154618},
}

@InProceedings{Yuan2000,
  author    = {Yuan, X. and Khoshgoftaar, T.M. and Allen, E.B. and Ganesan, K.},
  booktitle = {Proceedings 3rd IEEE Symposium on Application-Specific Systems and Software Engineering Technology},
  title     = {An application of fuzzy clustering to software quality prediction},
  year      = {2000},
  pages     = {85-90},
  abstract  = {The ever increasing demand for high software reliability requires more robust modeling techniques for software quality prediction. The paper presents a modeling technique that integrates fuzzy subtractive clustering with module-order modeling for software quality prediction. First fuzzy subtractive clustering is used to predict the number of faults, then module-order modeling is used to predict whether modules are fault-prone or not. Note that multiple linear regression is a special case of fuzzy subtractive clustering. We conducted a case study of a large legacy telecommunication system to predict whether each module will be considered fault-prone. The case study found that using fuzzy subtractive clustering and module-order modeling, one can classify modules which will likely have faults discovered by customers with useful accuracy prior to release.},
  doi       = {10.1109/asset.2000.888052},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1095336786},
}

@InProceedings{Khoshgoftaar2000,
  author    = {Khoshgoftaar, T.M. and Shan, R. and Allen, E.B.},
  booktitle = {Proceedings. Fifth IEEE International Symposium on High Assurance Systems Engineering (HASE 2000)},
  title     = {Using product, process, and execution metrics to predict fault-prone software modules with classification trees},
  year      = {2000},
  pages     = {301-310},
  abstract  = {Software-quality classification models can make predictions to guide improvement efforts to those modules that need it the most. Based on software metrics, a model can predict which modules will be considered fault-prone, or not. We consider a module fault-prone if any faults were discovered by customers. Useful predictions are contingent on the availability of candidate predictors that are actually related to faults discovered by customers. With a diverse set of candidate predictors in hand, classification-tree modeling is a robust technique for building such software quality models. This paper presents an empirical case study of four releases of a very large telecommunications system. The case study used the regression-tree algorithm in the S-Plus package and then applied our general decision rule to classify modules. Results showed that in addition to product metrics, process metrics and execution metrics were significant predictors of faults discovered by customers.},
  doi       = {10.1109/hase.2000.895475},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1094791278},
}

@InProceedings{Fioravanti2001,
  author    = {Fioravanti, F. and Nesi, P.},
  booktitle = {Proceedings Fifth European Conference on Software Maintenance and Reengineering},
  title     = {A study on fault-proneness detection of object-oriented systems},
  year      = {2001},
  pages     = {121-130},
  abstract  = {Fault-proneness detection in object-oriented systems is an interesting area for software companies and researchers. Several hundred metrics have been defined with the aim of measuring the different aspects of object-oriented systems. Only a few of them have been validated for fault detection, and several interesting works with this view have been considered. This paper reports a research study starting from the analysis of more than 200 different object-oriented metrics extracted from the literature with the aim of identifying suitable models for the detection of the fault-proneness of classes. Such a large number of metrics allows the extraction of a subset of them in order to obtain models that can be adopted for fault-proneness detection. To this end, the whole set of metrics has been classified on the basis of the measured aspect in order to reduce them to a manageable number; then, statistical techniques were employed to produce a hybrid model comprised of 12 metrics. The work has focused on identifying models that can detect as many faulty classes as possible and, at the same time, that are based on a manageably small set of metrics. A compromise between these aspects and the classification correctness of faulty and non-faulty classes was the main challenge of the research. As a result, two models for fault-proneness class detection have been obtained and validated.},
  doi       = {10.1109/csmr.2001.914976},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1093396093},
}

@Article{Emam2001,
  author   = {Emam, K. El and Benlarbi, S. and Goel, N. and Rai, S.N.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {The confounding effect of class size on the validity of object-oriented metrics},
  year     = {2001},
  number   = {7},
  pages    = {630-650},
  volume   = {27},
  abstract = {Much effort has been devoted to the development and empirical validation of object-oriented metrics. The empirical validations performed thus far would suggest that a core set of validated metrics is close to being identified. However, none of these studies allow for the potentially confounding effect of class size. We demonstrate a strong size confounding effect and question the results of previous object-oriented metrics validation studies. We first investigated whether there is a confounding effect of class size in validation studies of object-oriented metrics and show that, based on previous work, there is reason to believe that such an effect exists. We then describe a detailed empirical methodology for identifying those effects. Finally, we perform a study on a large C++ telecommunications framework to examine if size is really a confounder. This study considered the Chidamber and Kemerer metrics and a subset of the Lorenz and Kidd metrics. The dependent variable was the incidence of a fault attributable to a field failure (fault-proneness of a class). Our findings indicate that, before controlling for size, the results are very similar to previous studies. The metrics that are expected to be validated are indeed associated with fault-proneness.},
  doi      = {10.1109/32.935855},
  groups   = {Software Fault Prediction Metrics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061154750},
}

@Article{Briand2002,
  author   = {Briand, L.C. and Melo, W.L. and Wust, J.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Assessing the applicability of fault-proneness models across object-oriented software projects},
  year     = {2002},
  number   = {7},
  pages    = {706-720},
  volume   = {28},
  abstract = {A number of papers have investigated the relationships between design metrics and the detection of faults in object-oriented software. Several of these studies have shown that such models can be accurate in predicting faulty classes within one particular software product. In practice, however, prediction models are built on certain products to be used on subsequent software development projects. How accurate can these models be, considering the inevitable differences that may exist across projects and systems? Organizations typically learn and change. From a more general standpoint, can we obtain any evidence that such models are economically viable tools to focus validation and verification effort? This paper attempts to answer these questions by devising a general but tailorable cost-benefit model and by using fault and design data collected on two mid-size Java systems developed in the same environment. Another contribution of the paper is the use of a novel exploratory analysis technique - MARS (multivariate adaptive regression splines) to build such fault-proneness models, whose functional form is a-priori unknown. The results indicate that a model built on one system can be accurately used to rank classes within another system according to their fault proneness. The downside, however, is that, because of system differences, the predicted fault probabilities are not representative of the system predicted. However, our cost-benefit model demonstrates that the MARS fault-proneness model is potentially viable, from an economical standpoint. The linear model is not nearly as good, thus suggesting a more complex model is required.},
  doi      = {10.1109/tse.2002.1019484},
  groups   = {Software Fault Prediction Metrics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788213},
}

@InProceedings{Menzies2002,
  author    = {Menzies, Tim and Di Stefano, Justin S. and Chapman, Mike and McGill, Ken},
  booktitle = {27th Annual NASA Goddard/IEEE Software Engineering Workshop, 2002. Proceedings.},
  title     = {Metrics That Matter},
  year      = {2002},
  pages     = {51-57},
  abstract  = {Within NASA, there is an increasing awareness that software is of growing importance to the success of missions. Much data has been collected, and many theories have been advanced on how to reduce or eliminate errors in code. However, learning requires experience. This article documents a new NASA initiative to build a centralized repository of software defect data; in particular, it documents one specific case study on software metrics. Software metrics are used as a basis for prediction of errors in code modules, but there are many different metrics available. McCabe is one of the more popular tools used to produce metrics, but, as will be shown in this paper, other metrics can be more significant.},
  doi       = {10.1109/sew.2002.1199449},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1094902607},
}

@InProceedings{Yu2002,
  author    = {Yu, Ping and Systa, T. and Muller, H.},
  booktitle = {Proceedings of the Sixth European Conference on Software Maintenance and Reengineering},
  title     = {Predicting fault-proneness using OO metrics. An industrial case study},
  year      = {2002},
  pages     = {99-107},
  abstract  = {Software quality is an important external software attribute that is difficult to measure objectively. In this case study, we empirically validate a set of object-oriented metrics in terms of their usefulness in predicting fault-proneness, an important software quality indicator We use a set of ten software product metrics that relate to the following software attributes: the size of the software, coupling, cohesion, inheritance, and reuse. Eight hypotheses on the correlations of the metrics with fault-proneness are given. These hypotheses are empirically tested in a case study, in which the client side of a large network service management system is studied. The subject system is written in Java and it consists of 123 classes. The validation is carried out using two data analysis techniques: regression analysis and discriminant analysis.},
  doi       = {10.1109/csmr.2002.995794},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1093519125},
}

@InProceedings{Menzies2004,
  author    = {Menzies, Tim and Di Stefano, Justin S.},
  booktitle = {Eighth IEEE International Symposium on High Assurance Systems Engineering, 2004. Proceedings.},
  title     = {How Good is Your Blind Spot Sampling Policy?},
  year      = {2004},
  note      = {http://menzies.us/pdf/03blind.pdf},
  pages     = {129-138},
  abstract  = {Assessing software costs money and better assessment costs exponentially more money. Given finite budgets, assessment resources are typically skewed towards areas that are believed to be mission critical. This leaves blind spots: portions of the system that may contain defects which may be missed. Therefore, in addition to rigorously assessing mission critical areas, a parallel activity should sample the blind spots. This paper assesses defect detectors based on static code measures as a blind spot sampling method. In contrast to previous results, we find that such defect detectors yield results that are stable across many applications. Further, these detectors are inexpensive to use and can be tuned to the specifics of the current business situations.},
  doi       = {10.1109/hase.2004.1281737},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1094933957},
}

@InProceedings{Li2005,
  author    = {Li, Paul Luo and Herbsleb, Jim and Shaw, Mary},
  booktitle = {11th IEEE International Software Metrics Symposium (METRICS'05)},
  title     = {Finding Predictors of Field Defects for Open Source Software Systems in Commonly Available Data Sources: a Case Study of OpenBSD},
  year      = {2005},
  note      = {http://reports-archive.adm.cs.cmu.edu/anon/isri2005/CMU-ISRI-05-121.pdf},
  pages     = {1-10},
  abstract  = {Open source software systems are important components of many business software applications. Field defect predictions for open source software systems may allow organizations to make informed decisions regarding open source software components. In this paper, we remotely measure and analyze predictors (metrics available before release) mined from established data sources (the code repository and the request tracking system) as well as a novel source of data (mailing list archives) for nine releases of OpenBSD. First, we attempt to predict field defects by extending a software reliability model fitted to development defects. We find this approach to be infeasible, which motivates examining metrics-based field defect prediction. Then, we evaluate 139 predictors using established statistical methods: Kendall's rank correlation, Pearson's rank correlation, and forward AIC model selection. The metrics we collect include product metrics, development metrics, deployment and usage metrics, and software and hardware configurations metrics. We find the number of messages to the technical discussion mailing list during the development period (a deployment and usage metric captured from mailing list archives) to be the best predictor of field defects. Our work identifies predictors of field defects in commonly available data sources for open source software systems and is a step towards metrics-based field defect prediction for quantitatively-based decision making regarding open source software components.},
  doi       = {10.1109/metrics.2005.26},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1094572706},
}

@Article{Gyimothy2005,
  author   = {Gyimothy, T. and Ferenc, R. and Siket, I.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Empirical validation of object-oriented metrics on open source software for fault prediction},
  year     = {2005},
  number   = {10},
  pages    = {897-910},
  volume   = {31},
  abstract = {Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database - called Bugzilla - using regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle.},
  doi      = {10.1109/tse.2005.112},
  groups   = {Software Fault Prediction Metrics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788438},
}

@InProceedings{Nagappan2005,
  author    = {Nagappan, Nachiappan and Ball, Thomas},
  booktitle = {Proceedings of the 27th international conference on Software engineering - ICSE '05},
  title     = {Use of relative code churn measures to predict system defect density},
  year      = {2005},
  pages     = {284-292},
  abstract  = {Software systems evolve over time due to changes in requirements, optimization of code, fixes for security and reliability bugs etc. Code churn, which measures the changes made to a component over a period of time, quantifies the extent of this change. We present a technique for early prediction of system defect density using a set of relative code churn measures that relate the amount of churn to other variables such as component size and the temporal extent of churn.Using statistical regression models, we show that while absolute measures of code churn are poor predictors of defect density, our set of relative measures of code churn is highly predictive of defect density. A case study performed on Windows Server 2003 indicates the validity of the relative code churn measures as early indicators of system defect density. Furthermore, our code churn metric suite is able to discriminate between fault and not fault-prone binaries with an accuracy of 89.0 percent.},
  doi       = {10.1145/1062455.1062514},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1005717752},
}

@InProceedings{Abubakar2006,
  author    = {Abubakar, Adam and AlGhamdi, Jarallah and Ahmed, Moataz},
  booktitle = {IEEE International Conference on Computer Systems and Applications, 2006.},
  title     = {Can Cohesion Predict Fault Density?},
  year      = {2006},
  pages     = {890-893},
  abstract  = {Cohesion is an internal software attribute which depicts how well the components of a software module are connected. It is thought of as having effect on the quality of the software system. This paper presents the results of an empirical investigation of whether cohesion of object-oriented systems plays a role on software reliability. The paper presents a study of the correlation between cohesion measures and the number of defects in software systems, using seven open source projects.},
  doi       = {10.1109/aiccsa.2006.205193},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1095448393},
}

@Article{Zhou2006,
  author   = {Zhou, Yuming and Leung, Hareton},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Empirical Analysis of Object-Oriented Design Metrics for Predicting High and Low Severity Faults},
  year     = {2006},
  number   = {10},
  pages    = {771-789},
  volume   = {32},
  abstract = {In the last decade, empirical studies on object-oriented design metrics have shown some of them to be useful for predicting the fault-proneness of classes in object-oriented software systems. This research did not, however, distinguish among faults according to the severity of impact. It would be valuable to know how object-oriented design metrics and class fault-proneness are related when fault severity is taken into account. In this paper, we use logistic regression and machine learning methods to empirically investigate the usefulness of object-oriented design metrics, specifically, a subset of the Chidamber and Kemerer suite, in predicting fault-proneness when taking fault severity into account. Our results, based on a public domain NASA data set, indicate that 1) most of these design metrics are statistically related to fault-proneness of classes across fault severity, and 2) the prediction capabilities of the investigated metrics greatly depend on the severity of faults. More specifically, these design metrics are able to predict low severity faults in fault-prone classes better than high severity faults in fault-prone classes},
  doi      = {10.1109/tse.2006.102},
  groups   = {Software Fault Prediction Metrics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788513},
}

@InProceedings{Nagappan2006,
  author    = {Nagappan, Nachiappan and Ball, Thomas and Murphy, Brendan},
  booktitle = {2006 17th International Symposium on Software Reliability Engineering},
  title     = {Using Historical In-Process and Product Metrics for Early Estimation of Software Failures},
  year      = {2006},
  pages     = {62-74},
  abstract  = {The benefits that a software organization obtains from estimates of product quality are dependent upon how early in the product cycle that these estimates are available. Early estimation of software quality can help organizations make informed decisions about corrective actions. To provide such early estimates we present an empirical case study of two large scale commercial operating systems, Windows XP and Windows Server 2003. In particular, we leverage various historical in-process and product metrics from Windows XP binaries to create statistical predictors to estimate the post-release failures/failure-proneness of Windows Server 2003 binaries. These models estimate the failures and failure-proneness of Windows Server 2003 binaries at statistically significant levels. Our study is unique in showing that historical predictors for a software product line can be useful, even at the very large scale of the Windows operating system.},
  doi       = {10.1109/issre.2006.50},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1093266454},
}

@Article{Olague2007,
  author   = {Olague, H.M. and Etzkorn, L.H. and Gholston, S. and Quattlebaum, S.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Empirical Validation of Three Software Metrics Suites to Predict Fault-Proneness of Object-Oriented Classes Developed Using Highly Iterative or Agile Software Development Processes},
  year     = {2007},
  number   = {6},
  pages    = {402-419},
  volume   = {33},
  abstract = {Empirical validation of software metrics suites to predict fault proneness in object-oriented (OO) components is essential to ensure their practical use in industrial settings. In this paper, we empirically validate three OO metrics suites for their ability to predict software quality in terms of fault-proneness: the Chidamber and Kemerer (CK) metrics, Abreu's Metrics for Object-Oriented Design (MOOD), and Bansiya and Davis' Quality Metrics for Object-Oriented Design (QMOOD). Some CK class metrics have previously been shown to be good predictors of initial OO software quality. However, the other two suites have not been heavily validated except by their original proposers. Here, we explore the ability of these three metrics suites to predict fault-prone classes using defect data for six versions of Rhino, an open-source implementation of JavaScript written in Java. We conclude that the CK and QMOOD suites contain similar components and produce statistical models that are effective in detecting error-prone classes. We also conclude that the class components in the MOOD metrics suite are not good class fault-proneness predictors. Analyzing multivariate binary logistic regression models across six Rhino versions indicates these models may be useful in assessing quality in OO classes produced using modern highly iterative or agile software development processes.},
  doi      = {10.1109/tse.2007.1015},
  groups   = {Software Fault Prediction Metrics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788591},
}

@InProceedings{Zimmermann2007,
  author    = {Zimmermann, Thomas and Premraj, Rahul and Zeller, Andreas},
  booktitle = {Third International Workshop on Predictor Models in Software Engineering (PROMISE'07: ICSE Workshops 2007)},
  title     = {Predicting Defects for Eclipse},
  year      = {2007},
  pages     = {9},
  abstract  = {We have mapped defects from the bug database of Eclipse (one of the largest open-source projects) to source code locations. The resulting data set lists the number of pre- and post-release defects for every package and file in the Eclipse releases 2.0, 2.1, and 3.0. We additionally annotated the data with common complexity metrics. All data is publicly available and can serve as a benchmark for defect prediction models.},
  doi       = {10.1109/promise.2007.10},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1095418072},
}

@Article{Pai2007,
  author   = {Pai, Ganesh J. and Dugan, Joanne Bechta},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Empirical Analysis of Software Fault Content and Fault Proneness Using Bayesian Methods},
  year     = {2007},
  number   = {10},
  pages    = {675-686},
  volume   = {33},
  abstract = {We present a methodology for Bayesian analysis of software quality. We cast our research in the broader context of constructing a causal framework that can include process, product, and other diverse sources of information regarding fault introduction during the software development process. In this paper, we discuss the aspect of relating internal product metrics to external quality metrics. Specifically, we build a Bayesian network (BN) model to relate object-oriented software metrics to software fault content and fault proneness. Assuming that the relationship can be described as a generalized linear model, we derive parametric functional forms for the target node conditional distributions in the BN. These functional forms are shown to be able to represent linear, Poisson, and binomial logistic regression. The models are empirically evaluated using a public domain data set from a software subsystem. The results show that our approach produces statistically significant estimations and that our overall modeling method performs no worse than existing techniques.},
  doi      = {10.1109/tse.2007.70722},
  groups   = {Software Fault Prediction Metrics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788629},
}

@InProceedings{Nagappan2007,
  author    = {Nagappan, Nachiappan and Ball, Thomas},
  booktitle = {First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007)},
  title     = {Using Software Dependencies and Churn Metrics to Predict Field Failures: An Empirical Case Study},
  year      = {2007},
  pages     = {364-373},
  abstract  = {Commercial software development is a complex task that requires a thorough understanding of the architecture of the software system. We analyze the Windows Server 2003 operating system in order to assess the relationship between its software dependencies, churn measures and post-release failures. Our analysis indicates the ability of software dependencies and churn measures to be efficient predictors of post-release failures. Further, we investigate the relationship between the software dependencies and churn measures and their ability to assess failure-proneness probabilities at statistically significant levels.},
  doi       = {10.1109/esem.2007.13},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1093292255},
}

@InProceedings{Jiang2007,
  author    = {Jiang, Yue and Cukic, Bojan and Menzies, Tim},
  booktitle = {The 18th IEEE International Symposium on Software Reliability (ISSRE '07)},
  title     = {Fault Prediction using Early Lifecycle Data},
  year      = {2007},
  pages     = {237-246},
  abstract  = {The prediction of fault-prone modules in a software project has been the topic of many studies. In this paper, we investigate whether metrics available early in the development lifecycle can be used to identify fault-prone software modules. More precisely, we build predictive models using the metrics that characterize textual requirements. We compare the performance of requirements-based models against the performance of code-based models and models that combine requirement and code metrics. Using a range of modeling techniques and the data from three NASA projects, our study indicates that the early lifecycle metrics can play an important role in project management, either by pointing to the need for increased quality monitoring during the development or by using the models to assign verification and validation activities.},
  doi       = {10.1109/issre.2007.24},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1095355101},
}

@Article{Menzies2007,
  author   = {Menzies, T. and Greenwald, J. and Frank, A.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Data Mining Static Code Attributes to Learn Defect Predictors},
  year     = {2007},
  number   = {1},
  pages    = {2-13},
  volume   = {33},
  abstract = {The value of using static code attributes to learn defect predictors has been widely debated. Prior work has explored issues like the merits of "McCabes versus Halstead versus lines of code counts" for generating defect predictors. We show here that such debates are irrelevant since how the attributes are used to build predictors is much more important than which particular attributes are used. Also, contrary to prior pessimism, we show that such defect predictors are demonstrably useful and, on the data studied here, yield predictors with a mean probability of detection of 71 percent and mean false alarms rates of 25 percent. These predictors would be useful for prioritizing a resource-bound exploration of code that has yet to be inspected},
  doi      = {10.1109/tse.2007.256941},
  groups   = {Software Fault Prediction Metrics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788604},
}

@Article{Marcus2008,
  author   = {Marcus, A. and Poshyvanyk, D. and Ferenc, R.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Using the Conceptual Cohesion of Classes for Fault Prediction in Object-Oriented Systems},
  year     = {2008},
  number   = {2},
  pages    = {287-300},
  volume   = {34},
  abstract = {High cohesion is a desirable property of software as it positively impacts understanding, reuse, and maintenance. Currently proposed measures for cohesion in Object-Oriented (OO) software reflect particular interpretations of cohesion and capture different aspects of it. Existing approaches are largely based on using the structural information from the source code, such as attribute references, in methods to measure cohesion. This paper proposes a new measure for the cohesion of classes in OO software systems based on the analysis of the unstructured information embedded in the source code, such as comments and identifiers. The measure, named the Conceptual Cohesion of Classes (C3), is inspired by the mechanisms used to measure textual coherence in cognitive psychology and computational linguistics. This paper presents the principles and the technology that stand behind the C3 measure. A large case study on three open source software systems is presented which compares the new measure with an extensive set of existing metrics and uses them to construct models that predict software faults. The case study shows that the novel measure captures different aspects of class cohesion compared to any of the existing cohesion measures. In addition, combining C3 with existing structural cohesion metrics proves to be a better predictor of faulty classes when compared to different combinations of structural cohesion metrics.},
  doi      = {10.1109/tse.2007.70768},
  groups   = {Software Fault Prediction Metrics},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788656},
}

@InProceedings{Wahyudin2008,
  author    = {Wahyudin, Dindin and Schatten, Alexander and Winkler, Dietmar and Tjoa, A Min and Biffl, Stefan},
  booktitle = {2008 34th Euromicro Conference Software Engineering and Advanced Applications},
  title     = {Defect Prediction Using Combined Product and Project Metrics a Case Study from the Open Source “Apache” MyFaces Project Family},
  year      = {2008},
  note      = {http://publik.tuwien.ac.at/files/PubDat_169478.pdf},
  pages     = {207-215},
  abstract  = {The quality evaluation of open source software (OSS) products, e.g., defect estimation and prediction approaches of individual releases, gains importance with increasing OSS adoption in industry applications. Most empirical studies on the accuracy of defect prediction and software maintenance focus on product metrics as predictors that are available only when the product is finished. Only few prediction models consider information on the development process (project metrics) that seems relevant to quality improvement of the software product. In this paper, we investigate defect prediction with data from a family of widely used OSS projects based both on product and project metrics as well as on combinations of these metrics. Main results of data analysis are (a) a set of project metrics prior to product release that had strong correlation to potential defect growth between releases and (b) a combination of product and project metrics enables a more accurate defect prediction than the application of one single type of measurement. Thus, the combined application of project and product metries can (a) improve the accuracy of defect prediction, (b) enable a better guidance of the release process from project management point of view, and (c) help identifying areas for product and process improvement.},
  doi       = {10.1109/seaa.2008.36},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1094238526},
}

@InProceedings{Moser2008,
  author    = {Moser, Raimund and Pedrycz, Witold and Succi, Giancarlo},
  booktitle = {Proceedings of the 13th international conference on Software engineering - ICSE '08},
  title     = {A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction},
  year      = {2008},
  note      = {http://hiper.cis.udel.edu/lp/lib/exe/fetch.php/courses/icse08-moser-defectpredict.pdf},
  pages     = {181-190},
  abstract  = {In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, Naïve Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: >75% percentage of correctly classified files, a recall of >80%, and a false positive rate <30%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.},
  doi       = {10.1145/1368088.1368114},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1031519727},
}

@InProceedings{Nagappan2008,
  author    = {Nagappan, Nachiappan and Murphy, Brendan and Basili, Victor},
  booktitle = {Proceedings of the 13th international conference on Software engineering - ICSE '08},
  title     = {The influence of organizational structure on software quality},
  year      = {2008},
  pages     = {521-530},
  abstract  = {Often software systems are developed by organizations consisting of many teams of individuals working together. Brooks states in the Mythical Man Month book that product quality is strongly affected by organization structure. Unfortunately there has been little empirical evidence to date to substantiate this assertion. In this paper we present a metric scheme to quantify organizational complexity, in relation to the product development process to identify if the metrics impact failure-proneness. In our case study, the organizational metrics when applied to data from Windows Vista were statistically significant predictors of failure-proneness. The precision and recall measures for identifying failure-prone binaries, using the organizational metrics, was significantly higher than using traditional metrics like churn, complexity, coverage, dependencies, and pre-release bug measures that have been used to date to predict failure-proneness. Our results provide empirical evidence that the organizational metrics are related to, and are effective predictors of failure-proneness.},
  doi       = {10.1145/1368088.1368160},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1048522318},
}

@InProceedings{Zimmermann2008,
  author    = {Zimmermann, Thomas and Nagappan, Nachiappan},
  booktitle = {Proceedings of the 13th international conference on Software engineering - ICSE '08},
  title     = {Predicting defects using network analysis on dependency graphs},
  year      = {2008},
  pages     = {531-540},
  abstract  = {In software development, resources for quality assurance are limited by time and by cost. In order to allocate resources effectively, managers need to rely on their experience backed by code complexity metrics. But often dependencies exist between various pieces of code over which managers may have little knowledge. These dependencies can be construed as a low level graph of the entire system. In this paper, we propose to use network analysis on these dependency graphs. This allows managers to identify central program units that are more likely to face defects. In our evaluation on Windows Server 2003, we found that the recall for models built from network measures is by 10% points higher than for models built from complexity metrics. In addition, network measures could identify 60% of the binaries that the Windows developers considered as critical-twice as many as identified by complexity metrics.},
  doi       = {10.1145/1368088.1368161},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1022710599},
}

@InProceedings{Kpodjedo2009,
  author    = {Kpodjedo, Segla and Ricca, Filippo and Antoniol, Giuliano and Galinier, Philippe},
  booktitle = {2009 1st International Symposium on Search Based Software Engineering},
  title     = {Evolution and Search Based Metrics to Improve Defects Prediction},
  year      = {2009},
  pages     = {23-32},
  abstract  = {Testing activity is the most widely adopted practice to ensure software quality. Testing effort should be focused on defect prone and critical resources i.e., on resources highly coupled with other entities of the software application. In this paper, we used search based techniques to define software metrics accounting for the role a class plays in the class diagram and for its evolution over time. We applied Chidamber and Kemerer and the newly defined metrics to Rhino, a Java ECMA script interpreter, to predict version 1.6R5 defect prone classes. Preliminary results show that the new metrics favorably compare with traditional object oriented metrics.},
  doi       = {10.1109/ssbse.2009.24},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1095559062},
}

@InProceedings{Hassan2009,
  author    = {Hassan, Ahmed E.},
  booktitle = {2009 IEEE 31st International Conference on Software Engineering},
  title     = {Predicting faults using the complexity of code changes},
  year      = {2009},
  pages     = {78-88},
  abstract  = {Predicting the incidence of faults in code has been commonly associated with measuring complexity. In this paper, we propose complexity metrics that are based on the code change process instead of on the code. We conjecture that a complex code change process negatively affects its product, i.e., the software system. We validate our hypothesis empirically through a case study using data derived from the change history for six large open source projects. Our case study shows that our change complexity metrics are better predictors of fault potential in comparison to other well-known historical predictors of faults, i.e., prior modifications and prior faults.},
  doi       = {10.1109/icse.2009.5070510},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1095317546},
}

@InProceedings{Holschuh2009,
  author    = {Holschuh, Tilman and Päuser, Markus and Herzig, Kim and Zimmermann, Thomas and Premraj, Rahul and Zeller, Andreas},
  booktitle = {2009 31st International Conference on Software Engineering - Companion Volume},
  title     = {Predicting Defects in SAP Java Code: An Experience Report},
  year      = {2009},
  note      = {http://thomas-zimmermann.com/publications/files/holschuh-icse-2009.pdf},
  pages     = {172-181},
  abstract  = {Which components of a large software system are the most defect-prone? In a study on a large SAP Java system, we evaluated and compared a number of defect predictors, based on code features such as complexity metrics, static error detectors, change frequency, or component imports, thus replicating a number of earlier case studies in an industrial context. We found the overall predictive power to be lower than expected; still, the resulting regression models successfully predicted 50–60% of the 20% most defect-prone components.},
  doi       = {10.1109/icse-companion.2009.5070975},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1095132962},
}

@InProceedings{Caglayan2009,
  author    = {Caglayan, Bora and Bener, Ayse and Koch, Stefan},
  booktitle = {2009 ICSE Workshop on Emerging Trends in Free/Libre/Open Source Software Research and Development},
  title     = {Merits of using repository metrics in defect prediction for open source projects},
  year      = {2009},
  pages     = {31-36},
  abstract  = {Many corporate code developers are the beta testers of open source software.They continue testing until they are sure that they have a stable version to build their code on. In this respect defect predictors play a critical role to identify defective parts of the software. Performance of a defect predictor is determined by correctly finding defective parts of the software without giving any false alarms. Having high false alarms means testers/ developers would inspect bug free code unnecessarily. Therefore in this research we focused on decreasing the false alarm rates by using repository metrics. We conducted experiments on the data sets of Eclipse project. Our results showed that repository metrics decreased the false alarm rates on the average to 23% from 32% corresponding up to 907 less files to inspect.},
  doi       = {10.1109/floss.2009.5071357},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1093362345},
}

@InProceedings{Zhang2009a,
  author    = {Zhang, Hongyu},
  booktitle = {2009 IEEE International Conference on Software Maintenance},
  title     = {An Investigation of the Relationships between Lines of Code and Defects},
  year      = {2009},
  pages     = {274-283},
  abstract  = {It is always desirable to understand the quality of a software system based on static code metrics. In this paper, we analyze the relationships between Lines of Code (LOC) and defects (including both pre-release and post-release defects). We confirm the ranking ability of (LOC) discovered by Fenton and Ohlsson. Furthermore, we find that the ranking ability of LOC can be formally described using Weibull functions. We can use defect density values calculated from a small percentage of largest modules to predict the number of total defects accurately. We also find that, given LOC we can predict the number of defective components reasonably well using typical classification techniques. We perform an extensive experiment using the public Eclipse dataset, and replicate the study using the NASA dataset. Our results confirm that simple static code attributes such as LOC can be useful predictors of software quality.},
  doi       = {10.1109/icsm.2009.5306304},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1094242278},
}

@InProceedings{Rana2009,
  author    = {Rana, Zeeshan Ali and Shamail, Shafay and Awais, Mian Muhammad},
  booktitle = {2009 WRI World Congress on Software Engineering},
  title     = {Ineffectiveness of Use of Software Science Metrics as Predictors of Defects in Object Oriented Software},
  year      = {2009},
  pages     = {3-7},
  abstract  = {Software science metrics (SSM) have been widely used as predictors of software defects. The usage of SSM is an effect of correlation of size and complexity metrics with number of defects. The SSM have been proposed keeping in view the procedural paradigm and structural nature of the programs. There has been a shift in software development paradigm from procedural to object oriented (OO) and SSM have been used as defect predictors of OO software as well. However, the effectiveness of SSM in OO software needs to be established. This paper investigates the effectiveness of use of SSM for: a) classification of defect prone modules in OO software b) prediction of number of defects. Various binary and numeric classification models have been applied on dataset kcl with class level data to study the role of SSM. The results show that the removal of SSM from the set of independent variables does not significantly affect the classification of modules as defect prone and the prediction of number of defects. In most of the cases the accuracy and mean absolute error has improved when SSM were removed from the set of independent variables. The results thus highlight the ineffectiveness of use of SSM in defect prediction in OO software.},
  doi       = {10.1109/wcse.2009.92},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1094738621},
}

@InProceedings{DAmbros2009,
  author    = {D'Ambros, Marco and Lanza, Michele and Robbes, Romain},
  booktitle = {2009 16th Working Conference on Reverse Engineering},
  title     = {On the Relationship between Change Coupling and Software Defects},
  year      = {2009},
  pages     = {135-144},
  abstract  = {Change coupling is the implicit relationship between two or more software artifacts that have been observed to frequently change together during the evolution of a software system. Researchers have studied this dependency and have observed that it points to design issues such as architectural decay. It is still unknown whether change coupling correlates with a tangible effect of design issues, i.e., software defects. In this paper we analyze the relationship between change coupling and software defects on three large software systems. We investigate whether change coupling correlates with defects, and if the performance of bug prediction models based on software metrics can be improved with change coupling information.},
  doi       = {10.1109/wcre.2009.19},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1093856281},
}

@InProceedings{DAmbros2010,
  author    = {D'Ambros, Marco and Lanza, Michele and Robbes, Romain},
  booktitle = {2010 7th IEEE Working Conference on Mining Software Repositories (MSR 2010)},
  title     = {An Extensive Comparison of Bug Prediction Approaches},
  year      = {2010},
  note      = {https://bia.unibz.it/view/delivery/39UBZ_INST/12235345170001241/13235356670001241},
  pages     = {31-41},
  abstract  = {Reliably predicting software defects is one of software engineering's holy grails. Researchers have devised and implemented a plethora of bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available data set consisting of several software systems, and provide an extensive comparison of the explanative and predictive power of well-known bug prediction approaches, together with novel approaches we devised. Based on the results, we discuss the performance and stability of the approaches with respect to our benchmark and deduce a number of insights on bug prediction models.},
  doi       = {10.1109/msr.2010.5463279},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1095496474},
}

@InProceedings{Nugroho2010,
  author    = {Nugroho, Ariadi and Chaudron, Michel R. V. and Arisholm, Erik},
  booktitle = {2010 7th IEEE Working Conference on Mining Software Repositories (MSR 2010)},
  title     = {Assessing UML Design Metrics for Predicting Fault-prone Classes in a Java System},
  year      = {2010},
  pages     = {21-30},
  abstract  = {Identifying and fixing software problems before implementation are believed to be much cheaper than after implementation. Hence, it follows that predicting fault-proneness of software modules based on early software artifacts like software design is beneficial as it allows software engineers to perform early predictions to anticipate and avoid faults early enough. Taking this motivation into consideration, in this paper we evaluate the usefulness of UML design metrics to predict fault-proneness of Java classes. We use historical data of a significant industrial Java system to build and validate a UML-based prediction model. Based on the case study we have found that level of detail of messages and import coupling —both measured from sequence diagrams, are significant predictors of class fault-proneness. We also learn that the prediction model built exclusively using the UML design metrics demonstrates a better accuracy than the one built exclusively using code metrics.},
  doi       = {10.1109/msr.2010.5463285},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1094464848},
}

@InProceedings{Kaur2010,
  author    = {Kaur, Arashdeep and Brar, Amanpreet Singh and Sandhu, Parvinder S.},
  booktitle = {2010 5th International Conference on Industrial and Information Systems},
  title     = {An Empirical Approach for Software Fault Prediction},
  year      = {2010},
  pages     = {261-265},
  abstract  = {Measuring software quality in terms of fault proneness of data can help the tomorrow's programmers to predict the fault prone areas in the projects before development. Knowing the faulty areas early from previous developed projects can be used to allocate experienced professionals for development of fault prone modules. Experienced persons can emphasize the faulty areas and can get the solutions in minimum time and budget that in turn increases software quality and customer satisfaction. We have used Fuzzy C Means clustering technique for the prediction of faulty/non-faulty modules in the project. The datasets used for training and testing modules available from NASA projects namely CM1, PC1 and JM1 include requirement and code metrics which are then combined to get a combination metric model. These three models are then compared with each other and the results show that combination metric model is found to be the best prediction model among three. Also, this approach is compared with others in the literature and is proved to be more accurate. This approach has been implemented in MATLAB7.9.},
  doi       = {10.1109/iciinfs.2010.5578698},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1093508390},
}

@InProceedings{Ujhazi2010,
  author    = {Újházi, Béla and Ferenc, RudoIf and Poshyvanyk, Denys and Gyimóthy, Tibor},
  booktitle = {2010 10th IEEE Working Conference on Source Code Analysis and Manipulation},
  title     = {New Conceptual Coupling and Cohesion Metrics for Object-Oriented Systems},
  year      = {2010},
  note      = {http://www.cs.wm.edu/~denys/pubs/SCAM'10-CohesionCouplingMetrics.pdf},
  pages     = {33-42},
  abstract  = {The paper presents two novel conceptual metrics for measuring coupling and cohesion in software systems. Our first metric, Conceptual Coupling between Object classes (CCBO), is based on the well-known CBO coupling metric, while the other metric, Conceptual Lack of Cohesion on Methods (CLCOM5), is based on the LCOM5 cohesion metric. One advantage of the proposed conceptual metrics is that they can be computed in a simpler (and in many cases, programming language independent) way as compared to some of the structural metrics. We empirically studied CCBO and CLCOM5 for predicting fault-proneness of classes in a large open-source system and compared these metrics with a host of existing structural and conceptual metrics for the same task. As the result, we found that the proposed conceptual metrics, when used in conjunction, can predict bugs nearly as precisely as the 58 structural metrics available in the Columbus source code quality framework and can be effectively combined with these metrics to improve bug prediction.},
  doi       = {10.1109/scam.2010.14},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1093246157},
}

@InProceedings{Nagappan2010,
  author    = {Nagappan, Nachiappan and Zeller, Andreas and Zimmermann, Thomas and Herzig, Kim and Murphy, Brendan},
  booktitle = {2010 IEEE 21st International Symposium on Software Reliability Engineering},
  title     = {Change Bursts as Defect Predictors},
  year      = {2010},
  pages     = {309-318},
  abstract  = {In software development, every change induces a risk. What happens if code changes again and again in some period of time? In an empirical study on Windows Vista, we found that the features of such change bursts have the highest predictive power for defect-prone components. With precision and recall values well above 90%, change bursts significantly improve upon earlier predictors such as complexity metrics, code churn, or organizational structure. As they only rely on version history and a controlled change process, change bursts are straight-forward to detect and deploy.},
  doi       = {10.1109/issre.2010.25},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1094870261},
}

@InProceedings{Eski2011,
  author    = {Eski, Sinan and Buzluca, Feza},
  booktitle = {2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops},
  title     = {An Empirical Study on Object-Oriented Metrics and Software Evolution in order to Reduce Testing Costs by Predicting Change-Prone Classes},
  year      = {2011},
  pages     = {566-571},
  abstract  = {Software maintenance cost is typically more than fifty percent of the cost of the total software life cycle and software testing plays a critical role in reducing it. Determining the critical parts of a software system is an important issue, because they are the best place to start testing in order to reduce cost and duration of tests. Software quality is an important key factor to determine critical parts since high quality parts of software are less error prone and easy to maintain. As object oriented software metrics give important evidence about design quality, they can help software engineers to choose critical parts, which should be tested firstly and intensely. In this paper, we present an empirical study about the relation between object oriented metrics and changes in software. In order to obtain the results, we analyze modifications in software across the historical sequence of open source projects. Empirical results of the study indicate that the low level quality parts of a software change frequently during the development and management process. Using this relation we propose a method that can be used to estimate change-prone classes and to determine parts which should be tested first and more deeply.},
  doi       = {10.1109/icstw.2011.43},
  groups    = {Software Fault Prediction Metrics},
  url       = {https://app.dimensions.ai/details/publication/pub.1093463018},
}

@Article{Radjenovic2013,
  author   = {Radjenović, Danijel and Heričko, Marjan and Torkar, Richard and Živkovič, Aleš},
  journal  = {Information and Software Technology},
  title    = {Software fault prediction metrics: A systematic literature review},
  year     = {2013},
  number   = {8},
  pages    = {1397-1418},
  volume   = {55},
  abstract = {ContextSoftware metrics may be used in fault prediction models to improve software quality by predicting fault location.ObjectiveThis paper aims to identify software metrics and to assess their applicability in software fault prediction. We investigated the influence of context on metrics’ selection and performance.MethodThis systematic literature review includes 106 papers published between 1991 and 2011. The selected papers are classified according to metrics and context properties.ResultsObject-oriented metrics (49%) were used nearly twice as often compared to traditional source code metrics (27%) or process metrics (24%). Chidamber and Kemerer’s (CK) object-oriented metrics were most frequently used. According to the selected studies there are significant differences between the metrics used in fault prediction performance. Object-oriented and process metrics have been reported to be more successful in finding faults compared to traditional size and complexity metrics. Process metrics seem to be better at predicting post-release faults compared to any static code metrics.ConclusionMore studies should be performed on large industrial software systems to find metrics more relevant for the industry and to answer the question as to which metrics should be used in a given context.},
  doi      = {10.1016/j.infsof.2013.02.009},
  groups   = {Software Fault Prediction Metrics},
  priority = {prio1},
  url      = {https://app.dimensions.ai/details/publication/pub.1038168456},
}

@Article{Radjenovic2013a,
  author   = {Radjenović, Danijel and Heričko, Marjan and Torkar, Richard and Živkovič, Aleš},
  journal  = {Information and Software Technology},
  title    = {Software fault prediction metrics: A systematic literature review},
  year     = {2013},
  number   = {8},
  pages    = {1397-1418},
  volume   = {55},
  abstract = {ContextSoftware metrics may be used in fault prediction models to improve software quality by predicting fault location.ObjectiveThis paper aims to identify software metrics and to assess their applicability in software fault prediction. We investigated the influence of context on metrics’ selection and performance.MethodThis systematic literature review includes 106 papers published between 1991 and 2011. The selected papers are classified according to metrics and context properties.ResultsObject-oriented metrics (49%) were used nearly twice as often compared to traditional source code metrics (27%) or process metrics (24%). Chidamber and Kemerer’s (CK) object-oriented metrics were most frequently used. According to the selected studies there are significant differences between the metrics used in fault prediction performance. Object-oriented and process metrics have been reported to be more successful in finding faults compared to traditional size and complexity metrics. Process metrics seem to be better at predicting post-release faults compared to any static code metrics.ConclusionMore studies should be performed on large industrial software systems to find metrics more relevant for the industry and to answer the question as to which metrics should be used in a given context.},
  doi      = {10.1016/j.infsof.2013.02.009},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1038168456},
}

@Article{Bibi2008a,
  author   = {Bibi, S. and Tsoumakas, G. and Stamelos, I. and Vlahavas, I.},
  journal  = {Expert Systems with Applications},
  title    = {Regression via Classification applied on software defect estimation},
  year     = {2008},
  number   = {3},
  pages    = {2091-2101},
  volume   = {34},
  abstract = {In this paper we apply Regression via Classification (RvC) to the problem of estimating the number of software defects. This approach apart from a certain number of faults, it also outputs an associated interval of values, within which this estimate lies with a certain confidence. RvC also allows the production of comprehensible models of software defects exploiting symbolic learning algorithms. To evaluate this approach we perform an extensive comparative experimental study of the effectiveness of several machine learning algorithms in two software data sets. RvC manages to get better regression error than the standard regression approaches on both datasets.},
  doi      = {10.1016/j.eswa.2007.02.012},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1016640320},
}

@Article{Vandecruys2008,
  author   = {Vandecruys, Olivier and Martens, David and Baesens, Bart and Mues, Christophe and De Backer, Manu and Haesen, Raf},
  journal  = {Journal of Systems and Software},
  title    = {Mining software repositories for comprehensible software fault prediction models},
  year     = {2008},
  number   = {5},
  pages    = {823-839},
  volume   = {81},
  abstract = {Software managers are routinely confronted with software projects that contain errors or inconsistencies and exceed budget and time limits. By mining software repositories with comprehensible data mining techniques, predictive models can be induced that offer software managers the insights they need to tackle these quality and budgeting problems in an efficient way. This paper deals with the role that the Ant Colony Optimization (ACO)-based classification technique AntMiner+ can play as a comprehensible data mining technique to predict erroneous software modules. In an empirical comparison on three real-world public datasets, the rule-based models produced by AntMiner+ are shown to achieve a predictive accuracy that is competitive to that of the models induced by several other included classification techniques, such as C4.5, logistic regression and support vector machines. In addition, we will argue that the intuitiveness and comprehensibility of the AntMiner+ models can be considered superior to the latter models.},
  doi      = {10.1016/j.jss.2007.07.034},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1044795300},
}

@Article{Catal2009,
  author   = {Catal, Cagatay and Diri, Banu},
  journal  = {Expert Systems with Applications},
  title    = {A systematic review of software fault prediction studies},
  year     = {2009},
  number   = {4},
  pages    = {7346-7354},
  volume   = {36},
  abstract = {This paper provides a systematic review of previous software fault prediction studies with a specific focus on metrics, methods, and datasets. The review uses 74 software fault prediction papers in 11 journals and several conference proceedings. According to the review results, the usage percentage of public datasets increased significantly and the usage percentage of machine learning algorithms increased slightly since 2005. In addition, method-level metrics are still the most dominant metrics in fault prediction research area and machine learning algorithms are still the most popular methods for fault prediction. Researchers working on software fault prediction area should continue to use public datasets and machine learning algorithms to build better fault predictors. The usage percentage of class-level is beyond acceptable levels and they should be used much more than they are now in order to predict the faults earlier in design phase of software life cycle.},
  doi      = {10.1016/j.eswa.2008.10.027},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1049597330},
}

@Article{Gondra2008,
  author   = {Gondra, Iker},
  journal  = {Journal of Systems and Software},
  title    = {Applying machine learning to software fault-proneness prediction},
  year     = {2008},
  number   = {2},
  pages    = {186-195},
  volume   = {81},
  abstract = {The importance of software testing to quality assurance cannot be overemphasized. The estimation of a module’s fault-proneness is important for minimizing cost and improving the effectiveness of the software testing process. Unfortunately, no general technique for estimating software fault-proneness is available. The observed correlation between some software metrics and fault-proneness has resulted in a variety of predictive models based on multiple metrics. Much work has concentrated on how to select the software metrics that are most likely to indicate fault-proneness. In this paper, we propose the use of machine learning for this purpose. Specifically, given historical data on software metric values and number of reported errors, an Artificial Neural Network (ANN) is trained. Then, in order to determine the importance of each software metric in predicting fault-proneness, a sensitivity analysis is performed on the trained ANN. The software metrics that are deemed to be the most critical are then used as the basis of an ANN-based predictive model of a continuous measure of fault-proneness. We also view fault-proneness prediction as a binary classification task (i.e., a module can either contain errors or be error-free) and use Support Vector Machines (SVM) as a state-of-the-art classification method. We perform a comparative experimental study of the effectiveness of ANNs and SVMs on a data set obtained from NASA’s Metrics Data Program data repository.},
  doi      = {10.1016/j.jss.2007.05.035},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1030311613},
}

@Article{Elish2008,
  author   = {Elish, Karim O. and Elish, Mahmoud O.},
  journal  = {Journal of Systems and Software},
  title    = {Predicting defect-prone software modules using support vector machines},
  year     = {2008},
  number   = {5},
  pages    = {649-660},
  volume   = {81},
  abstract = {Effective prediction of defect-prone software modules can enable software developers to focus quality assurance activities and allocate effort and resources more efficiently. Support vector machines (SVM) have been successfully applied for solving both classification and regression problems in many applications. This paper evaluates the capability of SVM in predicting defect-prone software modules and compares its prediction performance against eight statistical and machine learning models in the context of four NASA datasets. The results indicate that the prediction performance of SVM is generally better than, or at least, is competitive against the compared models.},
  doi      = {10.1016/j.jss.2007.07.040},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1030916580},
}

@Article{Khoshgoftaar2009,
  author   = {Khoshgoftaar, Taghi M. and Van Hulse, Jason},
  journal  = {IEEE Transactions on Human-Machine Systems},
  title    = {Empirical Case Studies in Attribute Noise Detection},
  year     = {2009},
  number   = {4},
  pages    = {379-388},
  volume   = {39},
  abstract = {The quality of data is an important issue in any domain-specific data mining and knowledge discovery initiative. The validity of solutions produced by data-driven algorithms can be diminished if the data being analyzed are of low quality. The quality of data is often realized in terms of data noise present in the given dataset and can include noisy attributes or labeling errors. Hence, tools for improving the quality of data are important to the data mining analyst. We present a comprehensive empirical investigation of our new and innovative technique for ranking attributes in a given dataset from most to least noisy. Upon identifying the noisy attributes, specific treatments can be applied depending on how the data are to be used. in a classification setting, for example, if the class label is determined to contain the most noise, processes to cleanse this important attribute may be undertaken. Independent variables or predictors that have a low correlation to the class attribute and appear noisy may be eliminated from the analysis. Several case studies using both real-world and synthetic datasets are presented in this study. The noise detection performance is evaluated by injecting noise into multiple attributes at different noise levels. The empirical results demonstrate conclusively that our technique provides a very accurate and useful ranking of noisy attributes in a given dataset.},
  doi      = {10.1109/tsmcc.2009.2013815},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061798146},
}

@InProceedings{Khoshgoftaar2009a,
  author    = {Khoshgoftaar, Taghi M. and Gao, Kehan},
  booktitle = {2009 International Conference on Machine Learning and Applications},
  title     = {Feature Selection with Imbalanced Data for Software Defect Prediction},
  year      = {2009},
  pages     = {235-240},
  abstract  = {In this paper, we study the learning impact of data sampling followed by attribute selection on the classification models built with binary class imbalanced data within the scenario of software quality engineering. We use a wrapper-based attribute ranking technique to select a subset of attributes, and the random undersampling technique (RUS) on the majority class to alleviate the negative effects of imbalanced data on the prediction models. The datasets used in the empirical study were collected from numerous software projects. Five data preprocessing scenarios were explored in these experiments, including: (1) training on the original, unaltered fit dataset, (2) training on a sampled version of the fit dataset, (3) training on an unsampled version of the fit dataset using only the attributes chosen by feature selection based on the unsampled fit dataset, (4) training on an unsampled version of the fit dataset using only the attributes chosen by feature selection based on a sampled version of the fit dataset, and (5) training on a sampled version of the fit dataset using only the attributes chosen by feature selection based on the sampled version of the fit dataset. We compared the performances of the classification models constructed over these five different scenarios. The results demonstrate that the classification models constructed on the sampled fit data with or without feature selection (case 2 and case 5) significantly outperformed the classification models built with the other cases (unsampled fit data). Moreover, the two scenarios using sampled data (case 2 and case 5) showed very similar performances, but the subset of attributes (case 5) is only around 15% or 30% of the complete set of attributes (case 2).},
  doi       = {10.1109/icmla.2009.18},
  groups    = {Software Defect Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1095749524},
}

@Article{Catal2009a,
  author   = {Catal, Cagatay and Diri, Banu},
  journal  = {Information Sciences},
  title    = {Investigating the effect of dataset size, metrics sets, and feature selection techniques on software fault prediction problem},
  year     = {2009},
  number   = {8},
  pages    = {1040-1058},
  volume   = {179},
  abstract = {Software quality engineering comprises of several quality assurance activities such as testing, formal verification, inspection, fault tolerance, and software fault prediction. Until now, many researchers developed and validated several fault prediction models by using machine learning and statistical techniques. There have been used different kinds of software metrics and diverse feature reduction techniques in order to improve the models’ performance. However, these studies did not investigate the effect of dataset size, metrics set, and feature selection techniques for software fault prediction. This study is focused on the high-performance fault predictors based on machine learning such as Random Forests and the algorithms based on a new computational intelligence approach called Artificial Immune Systems. We used public NASA datasets from the PROMISE repository to make our predictive models repeatable, refutable, and verifiable. The research questions were based on the effects of dataset size, metrics set, and feature selection techniques. In order to answer these questions, there were defined seven test groups. Additionally, nine classifiers were examined for each of the five public NASA datasets. According to this study, Random Forests provides the best prediction performance for large datasets and Naive Bayes is the best prediction algorithm for small datasets in terms of the Area Under Receiver Operating Characteristics Curve (AUC) evaluation parameter. The parallel implementation of Artificial Immune Recognition Systems (AIRS2Parallel) algorithm is the best Artificial Immune Systems paradigm-based algorithm when the method-level metrics are used.},
  doi      = {10.1016/j.ins.2008.12.001},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1040067116},
}

@Article{Turhan2009,
  author   = {Turhan, Burak and Menzies, Tim and Bener, Ayşe B. and Di Stefano, Justin},
  journal  = {Empirical Software Engineering},
  title    = {On the relative value of cross-company and within-company data for defect prediction},
  year     = {2009},
  number   = {5},
  pages    = {540-578},
  volume   = {14},
  abstract = {We propose a practical defect prediction approach for companies that do not track defect related data. Specifically, we investigate the applicability of cross-company (CC) data for building localized defect predictors using static code features. Firstly, we analyze the conditions, where CC data can be used as is. These conditions turn out to be quite few. Then we apply principles of analogy-based learning (i.e. nearest neighbor (NN) filtering) to CC data, in order to fine tune these models for localization. We compare the performance of these models with that of defect predictors learned from within-company (WC) data. As expected, we observe that defect predictors learned from WC data outperform the ones learned from CC data. However, our analyses also yield defect predictors learned from NN-filtered CC data, with performance close to, but still not better than, WC data. Therefore, we perform a final analysis for determining the minimum number of local defect reports in order to learn WC defect predictors. We demonstrate in this paper that the minimum number of data samples required to build effective defect predictors can be quite small and can be collected quickly within a few months. Hence, for companies with no local defect data, we recommend a two-phase approach that allows them to employ the defect prediction process instantaneously. In phase one, companies should use NN-filtered CC data to initiate the defect prediction process and simultaneously start collecting WC (local) data. Once enough WC data is collected (i.e. after a few months), organizations should switch to phase two and use predictors learned from WC data.},
  doi      = {10.1007/s10664-008-9103-7},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1016659627},
}

@Article{Turhan2009a,
  author   = {Turhan, Burak and Kocak, Gozde and Bener, Ayse},
  journal  = {Expert Systems with Applications},
  title    = {Data mining source code for locating software bugs: A case study in telecommunication industry},
  year     = {2009},
  number   = {6},
  pages    = {9986-9990},
  volume   = {36},
  abstract = {In a large software system knowing which files are most likely to be fault-prone is valuable information for project managers. They can use such information in prioritizing software testing and allocating resources accordingly. However, our experience shows that it is difficult to collect and analyze fine-grained test defects in a large and complex software system. On the other hand, previous research has shown that companies can safely use cross-company data with nearest neighbor sampling to predict their defects in case they are unable to collect local data. In this study we analyzed 25 projects of a large telecommunication system. To predict defect proneness of modules we trained models on publicly available Nasa MDP data. In our experiments we used static call graph based ranking (CGBR) as well as nearest neighbor sampling for constructing method level defect predictors. Our results suggest that, for the analyzed projects, at least 70% of the defects can be detected by inspecting only (i) 6% of the code using a Naïve Bayes model, (ii) 3% of the code using CGBR framework.},
  doi      = {10.1016/j.eswa.2008.12.028},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1032338646},
}

@Article{Liu2010,
  author   = {Liu, Yi and Khoshgoftaar, Taghi M and Seliya, Naeem},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Evolutionary Optimization of Software Quality Modeling with Multiple Repositories},
  year     = {2010},
  number   = {6},
  pages    = {852-864},
  volume   = {36},
  abstract = {A novel search-based approach to software quality modeling with multiple software project repositories is presented. Training a software quality model with only one software measurement and defect data set may not effectively encapsulate quality trends of the development organization. The inclusion of additional software projects during the training process can provide a cross-project perspective on software quality modeling and prediction. The genetic-programming-based approach includes three strategies for modeling with multiple software projects: Baseline Classifier, Validation Classifier, and Validation-and-Voting Classifier. The latter is shown to provide better generalization and more robust software quality models. This is based on a case study of software metrics and defect data from seven real-world systems. A second case study considers 17 different (nonevolutionary) machine learners for modeling with multiple software data sets. Both case studies use a similar majority-voting approach for predicting fault-proneness class of program modules. It is shown that the total cost of misclassification of the search-based software quality models is consistently lower than those of the non-search-based models. This study provides clear guidance to practitioners interested in exploiting their organization's software measurement data repositories for improved software quality modeling.},
  doi      = {10.1109/tse.2010.51},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788822},
}

@Article{Zheng2010,
  author   = {Zheng, Jun},
  journal  = {Expert Systems with Applications},
  title    = {Cost-sensitive boosting neural networks for software defect prediction},
  year     = {2010},
  number   = {6},
  pages    = {4537-4543},
  volume   = {37},
  abstract = {Software defect predictors which classify the software modules into defect-prone and not-defect-prone classes are effective tools to maintain the high quality of software products. The early prediction of defect-proneness of the modules can allow software developers to allocate the limited resources on those defect-prone modules such that high quality software can be produced on time and within budget. In the process of software defect prediction, the misclassification of defect-prone modules generally incurs much higher cost than the misclassification of not-defect-prone ones. Most of the previously developed predication models do not consider this cost issue. In this paper, three cost-sensitive boosting algorithms are studied to boost neural networks for software defect prediction. The first algorithm based on threshold-moving tries to move the classification threshold towards the not-fault-prone modules such that more fault-prone modules can be classified correctly. The other two weight-updating based algorithms incorporate the misclassification costs into the weight-update rule of boosting procedure such that the algorithms boost more weights on the samples associated with misclassified defect-prone modules. The performances of the three algorithms are evaluated by using four datasets from NASA projects in terms of a singular measure, the Normalized Expected Cost of Misclassification (NECM). The experimental results suggest that threshold-moving is the best choice to build cost-sensitive software defect prediction models with boosted neural networks among the three algorithms studied, especially for the datasets from projects developed by object-oriented language.},
  doi      = {10.1016/j.eswa.2009.12.056},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1009020017},
}

@Article{Menzies2010,
  author   = {Menzies, Tim and Milton, Zach and Turhan, Burak and Cukic, Bojan and Jiang, Yue and Bener, Ayşe},
  journal  = {Automated Software Engineering},
  title    = {Defect prediction from static code features: current results, limitations, new approaches},
  year     = {2010},
  number   = {4},
  pages    = {375-407},
  volume   = {17},
  abstract = {Building quality software is expensive and software quality assurance (QA) budgets are limited. Data miners can learn defect predictors from static code features which can be used to control QA resources; e.g. to focus on the parts of the code predicted to be more defective.Recent results show that better data mining technology is not leading to better defect predictors. We hypothesize that we have reached the limits of the standard learning goal of maximizing area under the curve (AUC) of the probability of false alarms and probability of detection “AUC(pd, pf)”; i.e. the area under the curve of a probability of false alarm versus probability of detection.Accordingly, we explore changing the standard goal. Learners that maximize “AUC(effort, pd)” find the smallest set of modules that contain the most errors. WHICH is a meta-learner framework that can be quickly customized to different goals. When customized to AUC(effort, pd), WHICH out-performs all the data mining methods studied here. More importantly, measured in terms of this new goal, certain widely used learners perform much worse than simple manual methods.Hence, we advise against the indiscriminate use of learners. Learners must be chosen and customized to the goal at hand. With the right architecture (e.g. WHICH), tuning a learner to specific local business goals can be a simple task.},
  doi      = {10.1007/s10515-010-0069-5},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1006992827},
}

@Article{Arisholm2010,
  author   = {Arisholm, Erik and Briand, Lionel C. and Johannessen, Eivind B.},
  journal  = {Journal of Systems and Software},
  title    = {A systematic and comprehensive investigation of methods to build and evaluate fault prediction models},
  year     = {2010},
  number   = {1},
  pages    = {2-17},
  volume   = {83},
  abstract = {This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and would like to devote extra resources to faulty system parts. The main research focus of this paper is to systematically assess three aspects on how to build and evaluate fault-proneness models in the context of this large Java legacy system development project: (1) compare many data mining and machine learning techniques to build fault-proneness models, (2) assess the impact of using different metric sets such as source code structural measures and change/fault history (process measures), and (3) compare several alternative ways of assessing the performance of the models, in terms of (i) confusion matrix criteria such as accuracy and precision/recall, (ii) ranking ability, using the receiver operating characteristic area (ROC), and (iii) our proposed cost-effectiveness measure (CE).The results of the study indicate that the choice of fault-proneness modeling technique has limited impact on the resulting classification accuracy or cost-effectiveness. There is however large differences between the individual metric sets in terms of cost-effectiveness, and although the process measures are among the most expensive ones to collect, including them as candidate measures significantly improves the prediction models compared with models that only include structural measures and/or their deltas between releases – both in terms of ROC area and in terms of CE. Further, we observe that what is considered the best model is highly dependent on the criteria that are used to evaluate and compare the models. And the regular confusion matrix criteria, although popular, are not clearly related to the problem at hand, namely the cost-effectiveness of using fault-proneness prediction models to focus verification efforts to deliver software with less faults at less cost.},
  doi      = {10.1016/j.jss.2009.06.055},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1029996552},
}

@InProceedings{Wang2010,
  author    = {Wang, Huanjing and Khoshgoftaar, Taghi M. and Napolitano, Amri},
  booktitle = {2010 Ninth International Conference on Machine Learning and Applications},
  title     = {A Comparative Study of Ensemble Feature Selection Techniques for Software Defect Prediction},
  year      = {2010},
  pages     = {135-140},
  abstract  = {Feature selection has become the essential step in many data mining applications. Using a single feature subset selection method may generate local optima. Ensembles of feature selection methods attempt to combine multiple feature selection methods instead of using a single one. We present a comprehensive empirical study examining 17 different ensembles of feature ranking techniques (rankers) including six commonly-used feature ranking techniques, the signal-to-noise filter technique, and 11 threshold-based feature ranking techniques. This study utilized 16 real-world software measurement data sets of different sizes and built 13,600 classification models. Experimental results indicate that ensembles of very few rankers are very effective and even better than ensembles of many or all rankers.},
  doi       = {10.1109/icmla.2010.27},
  groups    = {Software Defect Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1093583251},
}

@Article{Song2011a,
  author   = {Song, Qinbao and Jia, Zihan and Shepperd, M and Ying, Shi and Liu, Jin},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {A General Software Defect-Proneness Prediction Framework},
  year     = {2011},
  note     = {http://bura.brunel.ac.uk/bitstream/2438/8781/2/Fulltext.pdf},
  number   = {3},
  pages    = {356-370},
  volume   = {37},
  abstract = {BACKGROUND - Predicting defect-prone software components is an economically important activity and so has received a good deal of attention. However, making sense of the many, and sometimes seemingly inconsistent, results is difficult. OBJECTIVE - We propose and evaluate a general framework for software defect prediction that supports 1) unbiased and 2) comprehensive comparison between competing prediction systems. METHOD - The framework is comprised of 1) scheme evaluation and 2) defect prediction components. The scheme evaluation analyzes the prediction performance of competing learning schemes for given historical data sets. The defect predictor builds models according to the evaluated learning scheme and predicts software defects with new data according to the constructed model. In order to demonstrate the performance of the proposed framework, we use both simulation and publicly available software defect data sets. RESULTS - The results show that we should choose different learning schemes for different data sets (i.e., no scheme dominates), that small details in conducting how evaluations are conducted can completely reverse findings, and last, that our proposed framework is more effective and less prone to bias than previous approaches. CONCLUSIONS - Failure to properly or fully evaluate a learning scheme can be misleading; however, these problems may be overcome by our proposed framework.},
  doi      = {10.1109/tse.2010.90},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788851},
}

@Article{Catal2011,
  author   = {Catal, Cagatay and Sevim, Ugur and Diri, Banu},
  journal  = {Expert Systems with Applications},
  title    = {Practical development of an Eclipse-based software fault prediction tool using Naive Bayes algorithm},
  year     = {2011},
  number   = {3},
  pages    = {2347-2353},
  volume   = {38},
  abstract = {Despite the amount of effort software engineers have been putting into developing fault prediction models, software fault prediction still poses great challenges. This research using machine learning and statistical techniques has been ongoing for 15years, and yet we still have not had a breakthrough. Unfortunately, none of these prediction models have achieved widespread applicability in the software industry due to a lack of software tools to automate this prediction process. Historical project data, including software faults and a robust software fault prediction tool, can enable quality managers to focus on fault-prone modules. Thus, they can improve the testing process. We developed an Eclipse-based software fault prediction tool for Java programs to simplify the fault prediction process. We also integrated a machine learning algorithm called Naive Bayes into the plug-in because of its proven high-performance for this problem. This article presents a practical view to software fault prediction problem, and it shows how we managed to combine software metrics with software fault data to apply Naive Bayes technique inside an open source platform.},
  doi      = {10.1016/j.eswa.2010.08.022},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1050027319},
}

@Article{Khoshgoftaar2010,
  author   = {Khoshgoftaar, Taghi M. and Van Hulse, Jason and Napolitano, Amri},
  journal  = {IEEE Transactions on Systems Man and Cybernetics Systems},
  title    = {Comparing Boosting and Bagging Techniques with Noisy and Imbalanced Data},
  year     = {2010},
  number   = {3},
  pages    = {552-568},
  volume   = {41},
  abstract = {This paper compares the performance of several boosting and bagging techniques in the context of learning from imbalanced and noisy binary-class data. Noise and class imbalance are two well-established data characteristics encountered in a wide range of data mining and machine learning initiatives. The learning algorithms studied in this paper, which include SMOTEBoost, RUSBoost, Exactly Balanced Bagging, and Roughly Balanced Bagging, combine boosting or bagging with data sampling to make them more effective when data are imbalanced. These techniques are evaluated in a comprehensive suite of experiments, for which nearly four million classification models were trained. All classifiers are assessed using seven different performance metrics, providing a complete perspective on the performance of these techniques, and results are tested for statistical significance via analysis-of-variance modeling. The experiments show that the bagging techniques generally outperform boosting, and hence in noisy data environments, bagging is the preferred method for handling class imbalance.},
  doi      = {10.1109/tsmca.2010.2084081},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061795697},
}

@Article{Misirli2011,
  author   = {Mısırlı, Ayşe Tosun and Bener, Ayşe Başar and Turhan, Burak},
  journal  = {Software Quality Journal},
  title    = {An industrial case study of classifier ensembles for locating software defects},
  year     = {2011},
  number   = {3},
  pages    = {515-536},
  volume   = {19},
  abstract = {As the application layer in embedded systems dominates over the hardware, ensuring software quality becomes a real challenge. Software testing is the most time-consuming and costly project phase, specifically in the embedded software domain. Misclassifying a safe code as defective increases the cost of projects, and hence leads to low margins. In this research, we present a defect prediction model based on an ensemble of classifiers. We have collaborated with an industrial partner from the embedded systems domain. We use our generic defect prediction models with data coming from embedded projects. The embedded systems domain is similar to mission critical software so that the goal is to catch as many defects as possible. Therefore, the expectation from a predictor is to get very high probability of detection (pd). On the other hand, most embedded systems in practice are commercial products, and companies would like to lower their costs to remain competitive in their market by keeping their false alarm (pf) rates as low as possible and improving their precision rates. In our experiments, we used data collected from our industry partners as well as publicly available data. Our results reveal that ensemble of classifiers significantly decreases pf down to 15% while increasing precision by 43% and hence, keeping balance rates at 74%. The cost-benefit analysis of the proposed model shows that it is enough to inspect 23% of the code on local datasets to detect around 70% of defects.},
  doi      = {10.1007/s11219-010-9128-1},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1026182535},
}

@Article{Azar2011,
  author   = {Azar, D. and Vybihal, J.},
  journal  = {Information and Software Technology},
  title    = {An ant colony optimization algorithm to improve software quality prediction models: Case of class stability},
  year     = {2011},
  number   = {4},
  pages    = {388-393},
  volume   = {53},
  abstract = {ContextAssessing software quality at the early stages of the design and development process is very difficult since most of the software quality characteristics are not directly measurable. Nonetheless, they can be derived from other measurable attributes. For this purpose, software quality prediction models have been extensively used. However, building accurate prediction models is hard due to the lack of data in the domain of software engineering. As a result, the prediction models built on one data set show a significant deterioration of their accuracy when they are used to classify new, unseen data.ObjectiveThe objective of this paper is to present an approach that optimizes the accuracy of software quality predictive models when used to classify new data.MethodThis paper presents an adaptive approach that takes already built predictive models and adapts them (one at a time) to new data. We use an ant colony optimization algorithm in the adaptation process. The approach is validated on stability of classes in object-oriented software systems and can easily be used for any other software quality characteristic. It can also be easily extended to work with software quality predictive problems involving more than two classification labels.ResultsResults show that our approach out-performs the machine learning algorithm C4.5 as well as random guessing. It also preserves the expressiveness of the models which provide not only the classification label but also guidelines to attain it.ConclusionOur approach is an adaptive one that can be seen as taking predictive models that have already been built from common domain data and adapting them to context-specific data. This is suitable for the domain of software quality since the data is very scarce and hence predictive models built from one data set is hard to generalize and reuse on new data.},
  doi      = {10.1016/j.infsof.2010.11.013},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1014268670},
}

@Article{Bishnu2012,
  author   = {Bishnu, P. S. and Bhattacherjee, V.},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  title    = {Software Fault Prediction Using Quad Tree-Based K-Means Clustering Algorithm},
  year     = {2012},
  number   = {6},
  pages    = {1146-1150},
  volume   = {24},
  abstract = {Unsupervised techniques like clustering may be used for fault prediction in software modules, more so in those cases where fault labels are not available. In this paper a Quad Tree-based K-Means algorithm has been applied for predicting faults in program modules. The aims of this paper are twofold. First, Quad Trees are applied for finding the initial cluster centers to be input to the A'-Means Algorithm. An input threshold parameter δ governs the number of initial cluster centers and by varying δ the user can generate desired initial cluster centers. The concept of clustering gain has been used to determine the quality of clusters for evaluation of the Quad Tree-based initialization algorithm as compared to other initialization techniques. The clusters obtained by Quad Tree-based algorithm were found to have maximum gain values. Second, the Quad Tree- based algorithm is applied for predicting faults in program modules. The overall error rates of this prediction approach are compared to other existing algorithms and are found to be better in most of the cases.},
  doi      = {10.1109/tkde.2011.163},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061662354},
}

@Article{Pelayo2012,
  author   = {Pelayo, Lourdes and Dick, Scott},
  journal  = {IEEE Transactions on Reliability},
  title    = {Evaluating Stratification Alternatives to Improve Software Defect Prediction},
  year     = {2012},
  number   = {2},
  pages    = {516-525},
  volume   = {61},
  abstract = {Numerous studies have applied machine learning to the software defect prediction problem, i.e. predicting which modules will experience a failure during operation based on software metrics. However, skewness in defect-prediction datasets can mean that the resulting classifiers often predict the faulty (minority) class less accurately. This problem is well known in machine learning, and is often referred to as “learning from imbalanced datasets.” One common approach for mitigating skewness is to use stratification to homogenize class distributions; however, it is unclear what stratification techniques are most effective, both generally and specifically in software defect prediction. In this article, we investigate two major stratification alternatives (under-, and over-sampling) for software defect prediction using Analysis of Variance. Our analysis covers several modern software defect prediction datasets using a factorial design. We find that the main effect of under-sampling is significant at $\alpha=0.05$, as is the interaction between under- and over-sampling. However, the main effect of over-sampling is not significant.},
  doi      = {10.1109/tr.2012.2183912},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061783611},
}

@Article{Jin2012,
  author  = {Jin, C. and Jin, S.-W. and Ye, J.-M.},
  journal = {IET Software},
  title   = {Artificial neural network-based metric selection for software fault-prone prediction model},
  year    = {2012},
  number  = {6},
  pages   = {479},
  volume  = {6},
  doi     = {10.1049/iet-sen.2011.0138},
  groups  = {Software Defect Prediction},
  url     = {https://app.dimensions.ai/details/publication/pub.1056837147},
}

@Article{Sun2012,
  author   = {Sun, Zhongbin and Song, Qinbao and Zhu, Xiaoyan},
  journal  = {IEEE Transactions on Human-Machine Systems},
  title    = {Using Coding-Based Ensemble Learning to Improve Software Defect Prediction},
  year     = {2012},
  number   = {6},
  pages    = {1806-1817},
  volume   = {42},
  abstract = {Using classification methods to predict software defect proneness with static code attributes has attracted a great deal of attention. The class-imbalance characteristic of software defect data makes the prediction much difficult; thus, a number of methods have been employed to address this problem. However, these conventional methods, such as sampling, cost-sensitive learning, Bagging, and Boosting, could suffer from the loss of important information, unexpected mistakes, and overfitting because they alter the original data distribution. This paper presents a novel method that first converts the imbalanced binary-class data into balanced multiclass data and then builds a defect predictor on the multiclass data with a specific coding scheme. A thorough experiment with four different types of classification algorithms, three data coding schemes, and six conventional imbalance data-handling methods was conducted over the 14 NASA datasets. The experimental results show that the proposed method with a one-against-one coding scheme is averagely superior to the conventional methods.},
  doi      = {10.1109/tsmcc.2012.2226152},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061798471},
}

@Article{Gray2012a,
  author  = {Gray, D. and Bowes, D. and Davey, N. and Sun, Y. and Christianson, B.},
  journal = {IET Software},
  title   = {Reflections on the NASA MDP data sets},
  year    = {2012},
  number  = {6},
  pages   = {549},
  volume  = {6},
  doi     = {10.1049/iet-sen.2011.0132},
  groups  = {Software Defect Prediction},
  url     = {https://app.dimensions.ai/details/publication/pub.1056837144},
}

@Article{Ma2012,
  author   = {Ma, Ying and Luo, Guangchun and Zeng, Xue and Chen, Aiguo},
  journal  = {Information and Software Technology},
  title    = {Transfer learning for cross-company software defect prediction},
  year     = {2012},
  number   = {3},
  pages    = {248-256},
  volume   = {54},
  abstract = {ContextSoftware defect prediction studies usually built models using within-company data, but very few focused on the prediction models trained with cross-company data. It is difficult to employ these models which are built on the within-company data in practice, because of the lack of these local data repositories. Recently, transfer learning has attracted more and more attention for building classifier in target domain using the data from related source domain. It is very useful in cases when distributions of training and test instances differ, but is it appropriate for cross-company software defect prediction?ObjectiveIn this paper, we consider the cross-company defect prediction scenario where source and target data are drawn from different companies. In order to harness cross company data, we try to exploit the transfer learning method to build faster and highly effective prediction model.MethodUnlike the prior works selecting training data which are similar from the test data, we proposed a novel algorithm called Transfer Naive Bayes (TNB), by using the information of all the proper features in training data. Our solution estimates the distribution of the test data, and transfers cross-company data information into the weights of the training data. On these weighted data, the defect prediction model is built.ResultsThis article presents a theoretical analysis for the comparative methods, and shows the experiment results on the data sets from different organizations. It indicates that TNB is more accurate in terms of AUC (The area under the receiver operating characteristic curve), within less runtime than the state of the art methods.ConclusionIt is concluded that when there are too few local training data to train good classifiers, the useful knowledge from different-distribution training data on feature level may help. We are optimistic that our transfer learning method can guide optimal resource allocation strategies, which may reduce software testing cost and increase effectiveness of software testing process.},
  doi      = {10.1016/j.infsof.2011.09.007},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1041664266},
}

@Article{Peng2012,
  author   = {Peng, Yi and Wang, Guoxun and Wang, Honggang},
  journal  = {Information Sciences},
  title    = {User preferences based software defect detection algorithms selection using MCDM},
  year     = {2012},
  note     = {https://doi.org/10.1016/j.ins.2010.04.019},
  pages    = {3-13},
  volume   = {191},
  abstract = {A variety of classification algorithms for software defect detection have been developed over the years. How to select an appropriate classifier for a given task is an important issue in Data mining and knowledge discovery (DMKD). Many studies have compared different types of classification algorithms and the performances of these algorithms may vary using different performance measures and under different circumstances. Since the algorithm selection task needs to examine several criteria, such as accuracy, computational time, and misclassification rate, it can be modeled as a multiple criteria decision making (MCDM) problem. The goal of this paper is to use a set of MCDM methods to rank classification algorithms, with empirical results based on the software defect detection datasets. Since the preferences of the decision maker (DM) play an important role in algorithm evaluation and selection, this paper involved the DM during the ranking procedure by assigning user weights to the performance measures. Four MCDM methods are examined using 38 classification algorithms and 13 evaluation criteria over 10 public-domain software defect datasets. The results indicate that the boosting of CART and the boosting of C4.5 decision tree are ranked as the most appropriate algorithms for software defect datasets. Though the MCDM methods provide some conflicting results for the selected software defect datasets, they agree on most top-ranked classification algorithms.},
  doi      = {10.1016/j.ins.2010.04.019},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1027402804},
}

@Article{Wong2011,
  author   = {Wong, W. Eric and Debroy, Vidroha and Golden, Richard and Xu, Xiaofeng and Thuraisingham, Bhavani},
  journal  = {IEEE Transactions on Reliability},
  title    = {Effective Software Fault Localization Using an RBF Neural Network},
  year     = {2011},
  number   = {1},
  pages    = {149-169},
  volume   = {61},
  abstract = {We propose the application of a modified radial basis function neural network in the context of software fault localization, to assist programmers in locating bugs effectively. This neural network is trained to learn the relationship between the statement coverage information of a test case and its corresponding execution result, success or failure. The trained network is then given as input a set of virtual test cases, each covering a single statement. The output of the network, for each virtual test case, is considered to be the suspiciousness of the corresponding covered statement. A statement with a higher suspiciousness has a higher likelihood of containing a bug, and thus statements can be ranked in descending order of their suspiciousness. The ranking can then be examined one by one, starting from the top, until a bug is located. Case studies on 15 different programs were conducted, and the results clearly show that our proposed technique is more effective than several other popular, state of the art fault localization techniques. Further studies investigate the robustness of the proposed technique, and illustrate how it can easily be applied to programs with multiple bugs as well.},
  doi      = {10.1109/tr.2011.2172031},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061783588},
}

@InProceedings{Zhang2012a,
  author    = {Zhang, Peng and Chang, Yu-tong},
  booktitle = {2012 8th International Conference on Natural Computation},
  title     = {Software Fault Prediction Based on Grey Neural Network},
  year      = {2012},
  pages     = {466-469},
  abstract  = {Considering determining the number of software fault is an uncertain non-linear problem with only small sample, a novel software fault prediction method based on grey neural network is put forward. Firstly, constructing the grey neural network topological structure according the small sample sequence is necessary, and then the network learning algorithm is discussed. Finally, the grey neural network prediction model based on the grey theory and artificial neural network is proposed. The sample fault sequences of some software project are used to verify the precision of this method. Comparison with GM(l,l), the proposed model can reduce the prediction relative error effectively.},
  doi       = {10.1109/icnc.2012.6234505},
  groups    = {Software Defect Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1093218381},
}

@Article{Dejaeger2013,
  author   = {Dejaeger, K. and Verbraken, T. and Baesens, B.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Toward Comprehensible Software Fault Prediction Models Using Bayesian Network Classifiers},
  year     = {2013},
  number   = {2},
  pages    = {237-257},
  volume   = {39},
  abstract = {Software testing is a crucial activity during software development and fault prediction models assist practitioners herein by providing an upfront identification of faulty software code by drawing upon the machine learning literature. While especially the Naive Bayes classifier is often applied in this regard, citing predictive performance and comprehensibility as its major strengths, a number of alternative Bayesian algorithms that boost the possibility of constructing simpler networks with fewer nodes and arcs remain unexplored. This study contributes to the literature by considering 15 different Bayesian Network (BN) classifiers and comparing them to other popular machine learning techniques. Furthermore, the applicability of the Markov blanket principle for feature selection, which is a natural extension to BN theory, is investigated. The results, both in terms of the AUC and the recently introduced H-measure, are rigorously tested using the statistical framework of Demšar. It is concluded that simple and comprehensible networks with less nodes can be constructed using BN classifiers other than the Naive Bayes classifier. Furthermore, it is found that the aspects of comprehensibility and predictive performance need to be balanced out, and also the development context is an item which should be taken into account during model selection.},
  doi      = {10.1109/tse.2012.20},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788960},
}

@Article{Shepperd2013,
  author   = {Shepperd, M. and Song, Qinbao and Sun, Zhongbin and Mair, C.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Data Quality: Some Comments on the NASA Software Defect Datasets},
  year     = {2013},
  note     = {https://ualresearchonline.arts.ac.uk/id/eprint/6099/1/Data%20Quality%20Some%20Comments%20on%20the%20NASA%20Software%20Defect%20Data%20Sets.pdf},
  number   = {9},
  pages    = {1208-1215},
  volume   = {39},
  abstract = {Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset and generally insufficient detail documented on dataset preprocessing. Conclusions--It is recommended that researchers 1) indicate the provenance of the datasets they use, 2) report any preprocessing in sufficient detail to enable meaningful replication, and 3) invest effort in understanding the data prior to applying machine learners.},
  doi      = {10.1109/tse.2013.11},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061789009},
}

@Article{Wang2013,
  author   = {Wang, Shuo and Yao, Xin},
  journal  = {IEEE Transactions on Reliability},
  title    = {Using Class Imbalance Learning for Software Defect Prediction},
  year     = {2013},
  note     = {http://www.cs.bham.ac.uk/~xin/papers/TR2012-103-Final.pdf},
  number   = {2},
  pages    = {434-443},
  volume   = {62},
  abstract = {To facilitate software testing, and save testing costs, a wide range of machine learning methods have been studied to predict defects in software modules. Unfortunately, the imbalanced nature of this type of data increases the learning difficulty of such a task. Class imbalance learning specializes in tackling classification problems with imbalanced distributions, which could be helpful for defect prediction, but has not been investigated in depth so far. in this paper, we study the issue of if and how class imbalance learning methods can benefit software defect prediction with the aim of finding better solutions. We investigate different types of class imbalance learning methods, including resampling techniques, threshold moving, and ensemble algorithms. Among those methods we studied, AdaBoost. NC shows the best overall performance in terms of the measures including balance, G-mean, and Area Under the Curve (AUC). To further improve the performance of the algorithm, and facilitate its use in software defect prediction, we propose a dynamic version of AdaBoost. NC, which adjusts its parameter automatically during training. Without the need to pre-define any parameters, it is shown to be more effective and efficient than the original AdaBoost. NC.},
  doi      = {10.1109/tr.2013.2259203},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061783724},
}

@Article{Park2013,
  author   = {Park, Byoung-Jun and Oh, Sung-Kwun and Pedrycz, Witold},
  journal  = {Information Sciences},
  title    = {The design of polynomial function-based neural network predictors for detection of software defects},
  year     = {2013},
  pages    = {40-57},
  volume   = {229},
  abstract = {In this study, we introduce a design methodology of polynomial function-based Neural Network (pf-NN) classifiers (predictors). The essential design components include Fuzzy C-Means (FCM) regarded as a generic clustering algorithm and polynomials providing all required nonlinear capabilities of the model. The learning method uses a weighted cost function (objective function) while to analyze the performance of the system we engage a standard receiver operating characteristics (ROC) analysis. The proposed networks are used to detect software defects. From the conceptual standpoint, the classifier of this form can be expressed as a collection of “if-then” rules. Fuzzy clustering (Fuzzy C-Means, FCM) is aimed at the development of premise layer of the rules while the corresponding consequences of the rules are formed by some local polynomials. A detailed learning algorithm for the pf-NNs is presented with particular provisions made for dealing with imbalanced classes encountered quite commonly in software quality problems. The use of simple measures such as accuracy of classification becomes questionable. In the assessment of quality of classifiers, we confine ourselves to the use of the area under curve (AUC) in the receiver operating characteristics (ROCs) analysis. AUC comes as a sound classifier metric capturing a tradeoff between the high true positive rate (TP) and the low false positive rate (FP). The performance of the proposed classifier is contrasted with the results produced by some “standard” Radial Basis Function (RBF) neural networks.},
  doi      = {10.1016/j.ins.2011.01.026},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1014545963},
}

@Article{Peters2013,
  author   = {Peters, F. and Menzies, T. and Gong, L. and Zhang, H.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Balancing Privacy and Utility in Cross-Company Defect Prediction},
  year     = {2013},
  number   = {8},
  pages    = {1054-1068},
  volume   = {39},
  abstract = {Background: Cross-company defect prediction (CCDP) is a field of study where an organization lacking enough local data can use data from other organizations for building defect predictors. To support CCDP, data must be shared. Such shared data must be privatized, but that privatization could severely damage the utility of the data. Aim: To enable effective defect prediction from shared data while preserving privacy. Method: We explore privatization algorithms that maintain class boundaries in a dataset. CLIFF is an instance pruner that deletes irrelevant examples. MORPH is a data mutator that moves the data a random distance, taking care not to cross class boundaries. CLIFF+MORPH are tested in a CCDP study among 10 defect datasets from the PROMISE data repository. Results: We find: 1) The CLIFFed+MORPHed algorithms provide more privacy than the state-of-the-art privacy algorithms; 2) in terms of utility measured by defect prediction, we find that CLIFF+MORPH performs significantly better. Conclusions: For the OO defect data studied here, data can be privatized and shared without a significant degradation in utility. To the best of our knowledge, this is the first published result where privatization does not compromise defect prediction.},
  doi      = {10.1109/tse.2013.6},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061789060},
}

@Article{Koru2003,
  author   = {Koru, A. Güneş and Tian, Jeff},
  journal  = {Journal of Systems and Software},
  title    = {An empirical comparison and characterization of high defect and high complexity modules},
  year     = {2003},
  number   = {3},
  pages    = {153-163},
  volume   = {67},
  abstract = {We analyzed a large set of complexity metrics and defect data collected from six large-scale software products, two from IBM and four from Nortel Networks, to compare and characterize the similarities and differences between the high defect (HD) and high complexity modules. We observed that the most complex modules often have an acceptable quality and HD modules are not typically the most complex ones. This observation was statistically validated through hypothesis testing. Our analyses also indicated that the clusters of modules with the highest defects are usually those whose complexity rankings are slightly below the most complex ones. These results should help us better understand the complexity behavior of HD modules and guide future software development and research efforts.},
  doi      = {10.1016/s0164-1212(02)00126-7},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1028857888},
}

@InProceedings{Wang2004,
  author    = {Wang, Qi and Yu, Bo and Zhu, Jie},
  booktitle = {16th IEEE International Conference on Tools with Artificial Intelligence},
  title     = {Extract Rules from Software Quality Prediction Model Based on Neural Network},
  year      = {2004},
  pages     = {191-195},
  abstract  = {To get a highly reliable software product to the market on schedule, software engineers must allocate resources on the fault-prone software modules across the development effort. Software quality models based upon data mining from past projects can identify fault-prone modules in current similar development efforts. So that resources can be focused on fault-prone modules to improve quality prior to release. Many researchers have applied the neural networks approach to predict software quality. Although neural networks have shown their strengths in solving complex problems, their shortcoming of being 'black boxes' models has prevented them from being accepted as a common practice for fault-prone software modules prediction. That is a significant weakness, for without the ability to produce comprehensible decisions, it is hard to trust the reliability of neural networks that address real-world problems. In this paper, we introduce an interpretable neural network model for software quality prediction. First, a three-layer feed-forward neural network with the sigmoid function in hidden units and the identity junction in output unit was trained. The data used to train the neural network is collected from an earlier release of a telecommunications software system. Then use clustering genetic algorithm (CGA) to extract comprehensible rules from the trained neural network. We use the rule set extracted from the trained neural network to detect the fault-prone software modules of the later release and compare the predicting results with the neural network predicting results. The comparison shows that although the rule set's predicting accuracy is a little less than the trained neural network, it is more comprehensible.},
  doi       = {10.1109/ictai.2004.62},
  groups    = {Software Defect Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1093454342},
}

@Article{Kanmani2004,
  author   = {Kanmani, S. and Uthariaraj, V. Rhymend and Sankaranarayanan, V. and Thambidurai, P.},
  journal  = {ACM SIGSOFT Software Engineering Notes},
  title    = {Object oriented software quality prediction using general regression neural networks},
  year     = {2004},
  number   = {5},
  pages    = {1-6},
  volume   = {29},
  abstract = {This paper discusses the application of General Regression Neural Network (GRNN) for predicting the software quality attribute -- fault ratio. This study is carried out using static Object-Oriented (OO) measures (64 in total) as the independent variables and fault ratio as the dependent variable. Software metrics used include those concerning inheritance, size, cohesion and coupling. Prediction models are designed using 15 possible combinations of the four categories of the measures. We also tested the goodness of fit of the neural network model with the standard parameters. Our study is conducted in an academic institution with the software developed by students of Undergraduate/Graduate courses.},
  doi      = {10.1145/1022494.1022515},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1042524665},
}

@Article{Ostrand2005,
  author   = {Ostrand, T.J. and Weyuker, E.J. and Bell, R.M.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Predicting the location and number of faults in large software systems},
  year     = {2005},
  number   = {4},
  pages    = {340-355},
  volume   = {31},
  abstract = {Advance knowledge of which files in the next release of a large software system are most likely to contain the largest numbers of faults can be a very valuable asset. To accomplish this, a negative binomial regression model has been developed and used to predict the expected number of faults in each file of the next release of a system. The predictions are based on the code of the file in the current release, and fault and modification history of the file from previous releases. The model has been applied to two large industrial systems, one with a history of 17 consecutive quarterly releases over 4 years, and the other with nine releases over 2 years. The predictions were quite accurate: for each release of the two systems, the 20 percent of the files with the highest predicted number of faults contained between 71 percent and 92 percent of the faults that were actually detected, with the overall average being 83 percent. The same model was also used to predict which files of the first system were likely to have the highest fault densities (faults per KLOC). In this case, the 20 percent of the files with the highest predicted fault densities contained an average of 62 percent of the system's detected faults. However, the identified files contained a much smaller percentage of the code mass than the files selected to maximize the numbers of faults. The model was also used to make predictions from a much smaller input set that only contained fault data from integration testing and later. The prediction was again very accurate, identifying files that contained from 71 percent to 93 percent of the faults, with the average being 84 percent. Finally, a highly simplified version of the predictor selected files containing, on average, 73 percent and 74 percent of the faults for the two systems.},
  doi      = {10.1109/tse.2005.49},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788479},
}

@InProceedings{Xing2005,
  author    = {Xing, Fei and Guo, Ping and Lyu, Michael R.},
  booktitle = {16th IEEE International Symposium on Software Reliability Engineering (ISSRE'05)},
  title     = {A Novel Method for Early Software Quality Prediction Based on Support Vector Machine},
  year      = {2005},
  pages     = {1-10},
  abstract  = {The software development process imposes major impacts on the quality of software at every development stage; therefore, a common goal of each software development phase concerns how to improve software quality. Software quality prediction thus aims to evaluate software quality level periodically and to indicate software quality problems early. In this paper, we propose a novel technique to predict software quality by adopting Support Vector Machine (SVM) in the classification of software modules based on complexity metrics. Because only limited information of software complexity metrics is available in early software life cycle, ordinary software quality models cannot make good predictions generally. It is well known that SVM generalizes well even in high dimensional spaces under small training sample conditions. We consequently propose a SVM-based software classification model, whose characteristic is appropriate for early software quality predictions when only a small number of sample data are available. Experimental results with a Medical Imaging System software metrics data show that our SVM prediction model achieves better software quality prediction than some commonly used software quality prediction models.},
  doi       = {10.1109/issre.2005.6},
  groups    = {Software Defect Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1094284227},
}

@InProceedings{Challagulla2005,
  author    = {Challagulla, Venkata U.B. and Bastani, Farokh B. and Yen, I-Ling and Paul, Raymond A.},
  booktitle = {10th IEEE International Workshop on Object-Oriented Real-Time Dependable Systems},
  title     = {Empirical Assessment of Machine Learning based Software Defect Prediction Techniques},
  year      = {2005},
  pages     = {263-270},
  abstract  = {The wide-variety of real-time software systems, including telecontrolltelepresence systems, robotic systems, and mission planning systems, can entail dynamic code synthesis based on runtime mission-specific requirements and operating conditions. This necessitates the need for dynamic dependability assessment to ensure that these systems will perform as specified and will not fail in catastrophic ways. One approach in achieving this is to dynamically assess the modules in the synthesized code using software defect prediction techniques. Statistical models, such as Stepwise Multi-linear Regression models and multivariate models, and machine learning approaches, such as Artificial Neural Networks, Instance-based Reasoning, Bayesian-Belief Networks, Decision Trees, and Rule Inductions, have been investigated for predicting software quality. However, there is still no consensus about the best predictor model for software defects. In this paper, we evaluate different predictor models on four different real-time software defect data sets. The results show that a combination of lR and Instance-based Learning along with the Consistency-based Subset Evaluation technique provides a relatively better consistency in accuracy prediction compared to other models. The results also show that “size” and “complexity” metrics are not sufficient for accurately predicting real-time software defects.},
  doi       = {10.1109/words.2005.32},
  groups    = {Software Defect Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1093778985},
}

@InProceedings{Koru2005,
  author    = {Koru, A. Günes and Liu, Hongfang},
  booktitle = {Proceedings of the 2005 workshop on Predictor models in software engineering},
  title     = {An investigation of the effect of module size on defect prediction using static measures},
  year      = {2005},
  pages     = {1-5},
  abstract  = {We used several machine learning algorithms to predict the defective modules in five NASA products, namely, CM1, JM1, KC1, KC2, and PC1. A set of static measures were employed as predictor variables. While doing so, we observed that a large portion of the modules were small, as measured by lines of code (LOC). When we experimented on the data subsets created by partitioning according to module size, we obtained higher prediction performance for the subsets that include larger modules. We also performed defect prediction using class-level data for KC1 rather than the method-level data. In this case, the use of class-level data resulted in improved prediction performance compared to using method-level data. These findings suggest that quality assurance activities can be guided even better if defect prediction is performed by using data that belong to larger modules.},
  doi       = {10.1145/1083165.1083172},
  groups    = {Software Defect Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1011990728},
}

@Article{Khoshgoftaar2005,
  author   = {Khoshgoftaar, Taghi M. and Seliya, Naeem and Gao, Kehan},
  journal  = {Empirical Software Engineering},
  title    = {Assessment of a New Three-Group Software Quality Classification Technique: An Empirical Case Study},
  year     = {2005},
  number   = {2},
  pages    = {183-218},
  volume   = {10},
  abstract = {The primary aim of risk-based software quality classification models is to detect, prior to testing or operations, components that are most-likely to be of high-risk. Their practical usage as quality assurance tools is gauged by the prediction-accuracy and cost-effective aspects of the models. Classifying modules into two risk groups is the more commonly practiced trend. Such models assume that all modules predicted as high-risk will be subjected to quality improvements. Due to the always-limited reliability improvement resources and the variability of the quality risk-factor, a more focused classification model may be desired to achieve cost-effective software quality assurance goals. In such cases, calibrating a three-group (high-risk, medium-risk, and low-risk) classification model is more rewarding. We present an innovative method that circumvents the complexities, computational overhead, and difficulties involved in calibrating pure or direct three-group classification models. With the application of the proposed method, practitioners can utilize an existing two-group classification algorithm thrice in order to yield the three risk-based classes. An empirical approach is taken to investigate the effectiveness and validity of the proposed technique. Some commonly used classification techniques are studied to demonstrate the proposed methodology. They include, the C4.5 decision tree algorithm, discriminant analysis, and case-based reasoning. For the first two, we compare the three-group model calibrated using the respective techniques with the one built by applying the proposed method. Any two-group classification technique can be employed by the proposed method, including those that do not provide a direct three-group classification model, e.x., logistic regression and certain binary classification trees, such as CART. Based on a case study of a large-scale industrial software system, it is observed that the proposed method yielded promising results. For a given classification technique, the expected cost of misclassification of the proposed three-group models were significantly better (generally) when compared to the technique’s direct three-group model. In addition, the proposed method is also evaluated against an alternate indirect three-group classification method.},
  doi      = {10.1007/s10664-004-6191-x},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1048561887},
}

@Article{Song2006,
  author   = {Song, Qinbao and Shepperd, M. and Cartwright, M. and Mair, C.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Software defect association mining and defect correction effort prediction},
  year     = {2006},
  note     = {http://bura.brunel.ac.uk/bitstream/2438/1185/3/Software Defect 2006.pdf},
  number   = {2},
  pages    = {69-82},
  volume   = {32},
  abstract = {Much current software defect prediction work focuses on the number of defects remaining in a software system. In this paper, we present association rule mining based methods to predict defect associations and defect correction effort. This is to help developers detect software defects and assist project managers in allocating testing resources more effectively. We applied the proposed methods to the SEL defect data consisting of more than 200 projects over more than 15 years. The results show that, for defect association prediction, the accuracy is very high and the false-negative rate is very low. Likewise, for the defect correction effort prediction, the accuracy for both defect isolation effort prediction and defect correction effort prediction are also high. We compared the defect correction effort prediction method with other types of methods - PART, C4.5, and Naive Bayes - and show that accuracy has been improved by at least 23 percent. We also evaluated the impact of support and confidence levels on prediction accuracy, false-negative rate, false-positive rate, and the number of rules. We found that higher support and confidence levels may not result in higher prediction accuracy, and a sufficient number of rules is a precondition for high prediction accuracy.},
  doi      = {10.1109/tse.2006.1599417},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788527},
}

@Article{Zhou2006a,
  author   = {Zhou, Yuming and Leung, Hareton},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Empirical Analysis of Object-Oriented Design Metrics for Predicting High and Low Severity Faults},
  year     = {2006},
  number   = {10},
  pages    = {771-789},
  volume   = {32},
  abstract = {In the last decade, empirical studies on object-oriented design metrics have shown some of them to be useful for predicting the fault-proneness of classes in object-oriented software systems. This research did not, however, distinguish among faults according to the severity of impact. It would be valuable to know how object-oriented design metrics and class fault-proneness are related when fault severity is taken into account. In this paper, we use logistic regression and machine learning methods to empirically investigate the usefulness of object-oriented design metrics, specifically, a subset of the Chidamber and Kemerer suite, in predicting fault-proneness when taking fault severity into account. Our results, based on a public domain NASA data set, indicate that 1) most of these design metrics are statistically related to fault-proneness of classes across fault severity, and 2) the prediction capabilities of the investigated metrics greatly depend on the severity of faults. More specifically, these design metrics are able to predict low severity faults in fault-prone classes better than high severity faults in fault-prone classes},
  doi      = {10.1109/tse.2006.102},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788513},
}

@InProceedings{Challagulla2006,
  author    = {Challagulla, Venkata U.B. and Bastani, Farokh B. and Yen, I-Ling},
  booktitle = {2006 18th IEEE International Conference on Tools with Artificial Intelligence (ICTAI'06)},
  title     = {A Unified Framework for Defect Data Analysis Using the MBR Technique},
  year      = {2006},
  pages     = {39-46},
  abstract  = {Failures of mission-critical software systems can have catastrophic consequences and, hence, there is strong need for scientifically rigorous methods for assuring high system reliability. To reduce the V&V cost for achieving high confidence levels, quantitatively based software defect prediction techniques can be used to effectively estimate defects from prior data. Better prediction models facilitate better project planning and risk/cost estimation. Memory Based Reasoning (MBR) is one such classifier that quantitatively solves new cases by reusing knowledge gained from past experiences. However, it can have different configurations by varying its input parameters, giving potentially different predictions. To overcome this problem, we develop a framework that derives the optimal configuration of an MBR classifier for software defect data, by logical variation of its configuration parameters. We observe that this adaptive MBR technique provides a flexible and effective environment for accurate prediction of mission-critical software defect data.},
  doi       = {10.1109/ictai.2006.23},
  groups    = {Software Defect Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1094706654},
}

@Article{Khoshgoftaar2006,
  author   = {Khoshgoftaar, Taghi M. and Seliya, Naeem and Sundaresh, Nandini},
  journal  = {Software Quality Journal},
  title    = {An empirical study of predicting software faults with case-based reasoning},
  year     = {2006},
  number   = {2},
  pages    = {85-111},
  volume   = {14},
  abstract = {The resources allocated for software quality assurance and improvement have not increased with the ever-increasing need for better software quality. A targeted software quality inspection can detect faulty modules and reduce the number of faults occurring during operations. We present a software fault prediction modeling approach with case-based reasoning (CBR), a part of the computational intelligence field focusing on automated reasoning processes. A CBR system functions as a software fault prediction model by quantifying, for a module under development, the expected number of faults based on similar modules that were previously developed. Such a system is composed of a similarity function, the number of nearest neighbor cases used for fault prediction, and a solution algorithm. The selection of a particular similarity function and solution algorithm may affect the performance accuracy of a CBR-based software fault prediction system. This paper presents an empirical study investigating the effects of using three different similarity functions and two different solution algorithms on the prediction accuracy of our CBR system. The influence of varying the number of nearest neighbor cases on the performance accuracy is also explored. Moreover, the benefits of using metric-selection procedures for our CBR system is also evaluated. Case studies of a large legacy telecommunications system are used for our analysis. It is observed that the CBR system using the Mahalanobis distance similarity function and the inverse distance weighted solution algorithm yielded the best fault prediction. In addition, the CBR models have better performance than models based on multiple linear regression.},
  doi      = {10.1007/s11219-006-7597-z},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1031601770},
}

@Article{Menzies2007a,
  author   = {Menzies, T. and Greenwald, J. and Frank, A.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Data Mining Static Code Attributes to Learn Defect Predictors},
  year     = {2007},
  number   = {1},
  pages    = {2-13},
  volume   = {33},
  abstract = {The value of using static code attributes to learn defect predictors has been widely debated. Prior work has explored issues like the merits of "McCabes versus Halstead versus lines of code counts" for generating defect predictors. We show here that such debates are irrelevant since how the attributes are used to build predictors is much more important than which particular attributes are used. Also, contrary to prior pessimism, we show that such defect predictors are demonstrably useful and, on the data studied here, yield predictors with a mean probability of detection of 71 percent and mean false alarms rates of 25 percent. These predictors would be useful for prioritizing a resource-bound exploration of code that has yet to be inspected},
  doi      = {10.1109/tse.2007.256941},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788604},
}

@Article{Seliya2007,
  author   = {Seliya, Naeem and Khoshgoftaar, Taghi M.},
  journal  = {IEEE Transactions on Systems Man and Cybernetics Systems},
  title    = {Software Quality Analysis of Unlabeled Program Modules with Semisupervised Clustering},
  year     = {2007},
  number   = {2},
  pages    = {201-211},
  volume   = {37},
  abstract = {Software quality assurance is a vital component of software project development. A software quality estimation model is trained using software measurement and defect (software quality) data of a previously developed release or similar project. Such an approach assumes that the development organization has experience with systems similar to the current project and that defect data are available for all modules in the training data. In software engineering practice, however, various practical issues limit the availability of defect data for modules in the training data. In addition, the organization may not have experience developing a similar system. In such cases, the task of software quality estimation or labeling modules as fault prone or not fault prone falls on the expert. We propose a semisupervised clustering scheme for software quality analysis of program modules with no defect data or quality-based class labels. It is a constraint-based semisupervised clustering scheme that uses $k$-means as the underlying clustering algorithm. Software measurement data sets obtained from multiple National Aeronautics and Space Administration software projects are used in our empirical investigation. The proposed technique is shown to aid the expert in making better estimations as compared to predictions made when the expert labels the clusters formed by an unsupervised learning algorithm. In addition, the software quality knowledge learnt during the semisupervised process provided good generalization performance for multiple test data sets. An analysis of program modules that remain unlabeled subsequent to our semisupervised clustering scheme provided useful insight into the characteristics of their software attributes.},
  doi      = {10.1109/tsmca.2006.889473},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061795230},
}

@InProceedings{Li2007,
  author    = {Li, Zhan and Reformat, Marek},
  booktitle = {2007 IEEE International Conference on Information Reuse and Integration},
  title     = {A practical method for the software fault-prediction},
  year      = {2007},
  pages     = {659-666},
  abstract  = {In the paper, a novel machine learning method, SimBoost, is proposed to handle the software fault-prediction problem when highly skewed datasets are used. Although the method, proved by empirical results, can make the datasets much more balanced, the accuracy of the prediction is still not satisfactory. Therefore, a fuzzy-based representation of the software module fault state has been presented instead of the original faulty/non-faulty one. Several experiments were conducted using datasets from NASA Metrics Data Program. The discussion of the results of experiments is provided.},
  doi       = {10.1109/iri.2007.4296695},
  groups    = {Software Defect Prediction},
  url       = {https://app.dimensions.ai/details/publication/pub.1095092354},
}

@Article{Fenton2007,
  author   = {Fenton, Norman and Neil, Martin and Marsh, William and Hearty, Peter and Marquez, David and Krause, Paul and Mishra, Rajat},
  journal  = {Information and Software Technology},
  title    = {Predicting software defects in varying development lifecycles using Bayesian nets},
  year     = {2007},
  note     = {http://www.dcs.qmw.ac.uk/~norman/papers/ist_fenton.pdf},
  number   = {1},
  pages    = {32-43},
  volume   = {49},
  abstract = {An important decision in software projects is when to stop testing. Decision support tools for this have been built using causal models represented by Bayesian Networks (BNs), incorporating empirical data and expert judgement. Previously, this required a custom BN for each development lifecycle. We describe a more general approach that allows causal models to be applied to any lifecycle. The approach evolved through collaborative projects and captures significant commercial input. For projects within the range of the models, defect predictions are very accurate. This approach enables decision-makers to reason in a way that is not possible with regression-based models.},
  doi      = {10.1016/j.infsof.2006.09.001},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1053284993},
}

@Article{Koru2007,
  author   = {Koru, A. Güneş and Liu, Hongfang},
  journal  = {Journal of Systems and Software},
  title    = {Identifying and characterizing change-prone classes in two large-scale open-source products},
  year     = {2007},
  number   = {1},
  pages    = {63-73},
  volume   = {80},
  abstract = {Developing and maintaining open-source software has become an important source of profit for many companies. Change-prone classes in open-source products increase project costs by requiring developers to spend effort and time. Identifying and characterizing change-prone classes can enable developers to focus timely preventive actions, for example, peer-reviews and inspections, on the classes with similar characteristics in the future releases or products. In this study, we collected a set of static metrics and change data at class level from two open-source projects, KOffice and Mozilla. Using these data, we first tested and validated Pareto’s Law which implies that a great majority (around 80%) of change is rooted in a small proportion (around 20%) of classes. Then, we identified and characterized the change-prone classes in the two products by producing tree-based models. In addition, using tree-based models, we suggested a prioritization strategy to use project resources for focused preventive actions in an efficient manner. Our empirical results showed that this strategy was effective for prioritization purposes. This study should provide useful guidance to practitioners involved in development and maintenance of large-scale open-source products.},
  doi      = {10.1016/j.jss.2006.05.017},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1012487871},
}

@Article{Pai2007a,
  author   = {Pai, Ganesh J. and Dugan, Joanne Bechta},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Empirical Analysis of Software Fault Content and Fault Proneness Using Bayesian Methods},
  year     = {2007},
  number   = {10},
  pages    = {675-686},
  volume   = {33},
  abstract = {We present a methodology for Bayesian analysis of software quality. We cast our research in the broader context of constructing a causal framework that can include process, product, and other diverse sources of information regarding fault introduction during the software development process. In this paper, we discuss the aspect of relating internal product metrics to external quality metrics. Specifically, we build a Bayesian network (BN) model to relate object-oriented software metrics to software fault content and fault proneness. Assuming that the relationship can be described as a generalized linear model, we derive parametric functional forms for the target node conditional distributions in the BN. These functional forms are shown to be able to represent linear, Poisson, and binomial logistic regression. The models are empirically evaluated using a public domain data set from a software subsystem. The results show that our approach produces statistically significant estimations and that our overall modeling method performs no worse than existing techniques.},
  doi      = {10.1109/tse.2007.70722},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788629},
}

@Article{Lessmann2008,
  author   = {Lessmann, S. and Baesens, B. and Mues, C. and Pietsch, S.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings},
  year     = {2008},
  note     = {http://hiper.cis.udel.edu/lp/lib/exe/fetch.php/courses/tse08-classifyingdefpred.pdf},
  number   = {4},
  pages    = {485-496},
  volume   = {34},
  abstract = {Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.},
  doi      = {10.1109/tse.2008.35},
  groups   = {Software Defect Prediction},
  url      = {https://app.dimensions.ai/details/publication/pub.1061788688},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Artificial Intelligence\;0\;1\;\;\;\;;
2 StaticGroup:AI on Edge Devices\;0\;1\;\;\;\;;
1 StaticGroup:Biology\;0\;1\;\;\;\;;
2 StaticGroup:Synthetic Biology\;0\;0\;\;\;\;;
1 StaticGroup:Management\;0\;1\;\;\;\;;
2 StaticGroup:Resilience in Business and management\;0\;0\;\;\;\;;
1 StaticGroup:Medicine\;0\;1\;\;\;\;;
2 StaticGroup:Cervical Myelopathy\;0\;1\;\;\;\;;
1 StaticGroup:Agriculture\;0\;1\;\;\;\;;
2 StaticGroup:Drones in Agriculture\;0\;1\;\;\;\;;
2 StaticGroup:Crop Yield Prediction\;0\;1\;\;\;\;;
1 StaticGroup:Robots\;0\;1\;\;\;\;;
2 StaticGroup:Robotic Arthroplasty\;0\;1\;\;\;\;;
2 StaticGroup:Soft Robotics\;0\;1\;\;\;\;;
1 StaticGroup:Economics\;0\;1\;\;\;\;;
2 StaticGroup:Tourism Growth Nexus\;0\;1\;\;\;\;;
1 StaticGroup:Renewable Energy\;0\;1\;\;\;\;;
2 StaticGroup:Perovskite Solar Cells Stability\;0\;1\;\;\;\;;
2 StaticGroup:Sustainable Biofuel Economy\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Nanotechnology\;0\;1\;\;\;\;;
2 StaticGroup:Nanopharmaceuticals OR Nanonutraceuticals\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Climate Science\;0\;1\;\;\;\;;
2 StaticGroup:Green Warehousing\;0\;1\;\;\;\;;
1 StaticGroup:Internet of Things\;0\;1\;\;\;\;;
2 StaticGroup:Internet of Things in Healthcare\;0\;1\;\;\;\;;
1 StaticGroup:SLR\;0\;1\;\;\;\;;
2 StaticGroup:Software Process Line\;0\;1\;\;\;\;;
2 StaticGroup:Data Stream Processing Latency\;0\;1\;\;\;\;;
2 StaticGroup:Business Process Meta Models\;0\;1\;\;\;\;;
2 StaticGroup:Multicore Performance Prediction\;0\;1\;\;\;\;;
2 StaticGroup:Cloud Migration\;0\;1\;\;\;\;;
2 StaticGroup:Software Fault Prediction Metrics\;0\;1\;\;\;\;;
2 StaticGroup:Software Defect Prediction\;0\;1\;\;\;\;;
}
