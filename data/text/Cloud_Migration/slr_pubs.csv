id,title,abstract
pub.1148638160,"Pangea: An MLOps Tool for Automatically Generating Infrastructure and Deploying Analytic Pipelines in Edge, Fog and Cloud Layers","Development and operations (DevOps), artificial intelligence (AI), big data and edge-fog-cloud are disruptive technologies that may produce a radical transformation of the industry. Nevertheless, there are still major challenges to efficiently applying them in order to optimise productivity. Some of them are addressed in this article, concretely, with respect to the adequate management of information technology (IT) infrastructures for automated analysis processes in critical fields such as the mining industry. In this area, this paper presents a tool called Pangea aimed at automatically generating suitable execution environments for deploying analytic pipelines. These pipelines are decomposed into various steps to execute each one in the most suitable environment (edge, fog, cloud or on-premise) minimising latency and optimising the use of both hardware and software resources. Pangea is focused in three distinct objectives: (1) generating the required infrastructure if it does not previously exist; (2) provisioning it with the necessary requirements to run the pipelines (i.e., configuring each host operative system and software, install dependencies and download the code to execute); and (3) deploying the pipelines. In order to facilitate the use of the architecture, a representational state transfer application programming interface (REST API) is defined to interact with it. Therefore, in turn, a web client is proposed. Finally, it is worth noting that in addition to the production mode, a local development environment can be generated for testing and benchmarking purposes."
pub.1152597201,Design and Implementation of a Cloud PACS Architecture,"The limitations of the classic PACS (picture archiving and communication system), such as the backward-compatible DICOM network architecture and poor security and maintenance, are well-known. They are challenged by various existing solutions employing cloud-related patterns and services. However, a full-scale cloud-native PACS has not yet been demonstrated. The paper introduces a vendor-neutral cloud PACS architecture. It is divided into two main components: a cloud platform and an access device. The cloud platform is responsible for nearline (long-term) image archive, data flow, and backend management. It operates in multi-tenant mode. The access device is responsible for the local DICOM (Digital Imaging and Communications in Medicine) interface and serves as a gateway to cloud services. The cloud PACS was first implemented in an Amazon Web Services environment. It employs a number of general-purpose services designed or adapted for a cloud environment, including Kafka, OpenSearch, and Memcached. Custom services, such as a central PACS node, queue manager, or flow worker, also developed as cloud microservices, bring DICOM support, external integration, and a management layer. The PACS was verified using image traffic from, among others, computed tomography (CT), magnetic resonance (MR), and computed radiography (CR) modalities. During the test, the system was reliably storing and accessing image data. In following tests, scaling behavior differences between the monolithic Dcm4chee server and the proposed solution are shown. The growing number of parallel connections did not influence the monolithic server's overall throughput, whereas the performance of cloud PACS noticeably increased. In the final test, different retrieval patterns were evaluated to assess performance under different scenarios. The current production environment stores over 450 TB of image data and handles over 4000 DICOM nodes."
pub.1125047520,"Software-Defined Vehicular Cloud Networks: Architecture, Applications and Virtual Machine Migration","Cloud computing supports many unprecedented cloud-based vehicular applications. To improve connectivity and bandwidth through programmable networking architectures, Software-Defined (SD) Vehicular Network (SDVN) is introduced. SDVN architecture enables vehicles to be equipped with SDN OpenFlow switch on which the routing rules are updated from a SDN OpenFlow controller. From SDVN, new vehicular architectures are introduced, for instance SD Vehicular Cloud (SDVC). In SDVC, vehicles are SDN devices that host virtualization technology for enabling deployment of cloud-based vehicular applications. In addition, the migration of Virtual Machines (VM) over SDVC challenges the performance of cloud-based vehicular applications due the highly mobility of vehicles. However, the current literature that discusses VM migration in SDVC is very limited. In this paper, we first analyze the evolution of computation and networking technologies of SDVC with a focus on its architecture within the cloud-based vehicular environment. Then, we discuss the potential cloud-based vehicular applications assisted by the SDVC along with its ability to manage several VM migration scenarios. Lastly, we provide a detailed comparison of existing frameworks in SDVC that integrate the VM migration approach and different emulators or simulators network used to evaluate VM frameworks' use cases."
pub.1149417691,Dynamic Collaboration Model of Production Network Based on Cloud Service Bus,"The IT system of manufacturing enterprises usually has many problems, such as complex industrial software, different development languages, diverse communication protocols, and complex operation environment. Cloud service bus (CSB) technology based on service model encapsulates various applications existing in enterprises by means of the integration of cloud service bus and micro services, which can realize the rapid cloud migration and deployment of heterogeneous industrial application software. Theoretically, after the production system is connected to CSB, it can be arranged arbitrarily by service choreography technology to produce any possible products. However, in the context of industrial Internet, the production system connected to CSB corresponds to the equipment, materials, personnel, and other resources on the production line one by one. These production nodes need to consider the production capacity and cost of production service nodes and the output value of the whole production network, and cannot be combined arbitrarily. Therefore, when integrating production service nodes, we should not only consider the technical integration, but also consider whether the production conditions support this integration. To solve the problem of production node integration in CSB, a dynamic collaboration model of production network based on cloud service bus is proposed in this paper. The model takes the capacity, cost, and production relationship of production nodes as constraints, and the overall efficiency of production network as the optimization goal. The model can calculate the new creation, modification, deletion, and other scheduling operations of production line services in real time and give the production plan with the highest resource utilization and the greatest value in the current production network. The model can improve the rationality and economy of service choreography and give full play to the value of production network. Taking an enterprise with 11 production nodes and 5 production lines as an example, this paper discusses in detail how to use this model to calculate the optimal production organization scheme and the maximum output value of the enterprise."
pub.1149384143,[Retracted] Computer Security Issues and Legal System Based on Cloud Computing,"To effectively improve the security and accuracy of computer information storage, a computer security problem and legal system based on cloud computing are proposed. Firstly, this article details the evolution of cloud computing, its characteristics, architecture, and application status of cloud computing in detail. Second, we discussed security strategies to ensure the confidentiality and integrity of cloud computing information, focuses on the data encryption technology of cloud data security, and designs and implements the data backup and recovery system based on the cloud platform. The core layers of the system are the system layer and data operation layer. The system uses multithreading technology based on epoll and thread pool to improve the efficiency of data transmission. At the same time, the basic visual page is realized, and users can use the page to create a convenient operating system. Finally, the system is built in the laboratory environment and tested as a whole. The test results show that through the performance comparison with the current commonly used systems, it is found that the system in this paper has a certain improvement in data transmission rate, but the utilization rate of node CPU is as high as 40%, which leads to certain requirements for node CPU performance. Therefore, the system meets the functional requirements proposed in the design. Compared to the existing system, its performance has been found to meet the actual requirements of use, proving that the system is accessible and efficient."
pub.1143610639,Migrating a research data warehouse to a public cloud: challenges and opportunities,"OBJECTIVE: Clinical research data warehouses (RDWs) linked to genomic pipelines and open data archives are being created to support innovative, complex data-driven discoveries. The computing and storage needs of these research environments may quickly exceed the capacity of on-premises systems. New RDWs are migrating to cloud platforms for the scalability and flexibility needed to meet these challenges. We describe our experience in migrating a multi-institutional RDW to a public cloud.
MATERIALS AND METHODS: This study is descriptive. Primary materials included internal and public presentations before and after the transition, analysis documents, and actual billing records. Findings were aggregated into topical categories.
RESULTS: Eight categories of migration issues were identified. Unanticipated challenges included legacy system limitations; network, computing, and storage architectures that realize performance and cost benefits in the face of hyper-innovation, complex security reviews and approvals, and limited cloud consulting expertise.
DISCUSSION: Cloud architectures enable previously unavailable capabilities, but numerous pitfalls can impede realizing the full benefits of a cloud environment. Rapid changes in cloud capabilities can quickly obsolete existing architectures and associated institutional policies. Touchpoints with on-premise networks and systems can add unforeseen complexity. Governance, resource management, and cost oversight are critical to allow rapid innovation while minimizing wasted resources and unnecessary costs.
CONCLUSIONS: Migrating our RDW to the cloud has enabled capabilities and innovations that would not have been possible with an on-premises environment. Notwithstanding the challenges of managing cloud resources, the resulting RDW capabilities have been highly positive to our institution, research community, and partners."
pub.1149651969,Where Is Research in the Era of Electronic Health Records?,"IntroductionData extraction from electronic health records (EHRs) for use in clinical research continues to be labor-intensive and to offer little benefit over traditional paper chart reviews. This is largely due to poor integration of EHR systems with hospital process flow, which still relies heavily on traditional paperwork as a means of documentation. DiscussionNew methods in data collection through mobile applications have streamlined data entry through better data standardization and improved overall data quality. However, mobile applications address only a portion of the problem. Data entry errors and legacy integration will continue to be an issue when there are changes between practitioners with different EHR systems. The combination of a mobile application with the cloud platform has been applied in multiple specialties to monitor recovery and patient-reported outcomes. ConclusionMobile applications along with a virtual cloud environment to host data provide a reasonable solution for consolidating patient data and can accelerate population research."
pub.1034421591,Tavaxy: Integrating Taverna and Galaxy workflows with cloud computing support,"BackgroundOver the past decade the workflow system paradigm has evolved as an efficient and user-friendly approach for developing complex bioinformatics applications. Two popular workflow systems that have gained acceptance by the bioinformatics community are Taverna and Galaxy. Each system has a large user-base and supports an ever-growing repository of application workflows. However, workflows developed for one system cannot be imported and executed easily on the other. The lack of interoperability is due to differences in the models of computation, workflow languages, and architectures of both systems. This lack of interoperability limits sharing of workflows between the user communities and leads to duplication of development efforts.ResultsIn this paper, we present Tavaxy, a stand-alone system for creating and executing workflows based on using an extensible set of re-usable workflow patterns. Tavaxy offers a set of new features that simplify and enhance the development of sequence analysis applications: It allows the integration of existing Taverna and Galaxy workflows in a single environment, and supports the use of cloud computing capabilities. The integration of existing Taverna and Galaxy workflows is supported seamlessly at both run-time and design-time levels, based on the concepts of hierarchical workflows and workflow patterns. The use of cloud computing in Tavaxy is flexible, where the users can either instantiate the whole system on the cloud, or delegate the execution of certain sub-workflows to the cloud infrastructure.ConclusionsTavaxy reduces the workflow development cycle by introducing the use of workflow patterns to simplify workflow creation. It enables the re-use and integration of existing (sub-) workflows from Taverna and Galaxy, and allows the creation of hybrid workflows. Its additional features exploit recent advances in high performance cloud computing to cope with the increasing data size and complexity of analysis.The system can be accessed either through a cloud-enabled web-interface or downloaded and installed to run within the user's local environment. All resources related to Tavaxy are available at http://www.tavaxy.org."
pub.1164654734,Integrating digital solutions into national health data systems through public–private collaboration: An early experience of the SPICE platform in Kenya,"Public-private collaborative efforts to address healthcare challenges in low- and middle-income countries have been the focus of digital initiatives to improve both access and quality of health services. We report the early feasibility, experience, and learnings of migrating healthcare data generated from a proprietary, privately owned cloud-based environment into an on-premises National Health Data Center (NHDC) in compliance with Kenya's data management legislation. In 2018, Medtronic LABS entered into a partnership with the Kenya Ministry of Health and other stakeholders to improve access to quality services and data availability for non-communicable diseases (diabetes and hypertension), anchored on the SPICE digital health platform. Data migration from SPICE to the NHDC necessitated the establishment of multi-stakeholder coordination structures, alignment on system configuration requirements, provisioning of on-premises servers, data replication and monitoring. The data replication process showed consistency in format and content with no evidence of data loss. The monitoring of the server uptime and availability, however, exposed overall downtime of 15% of the total time tracked between April and December 2022 caused by Internet Protocol address configuration issues, power outages, firewall rule changes, and unscheduled system maintenance. Monthly tracked downtime however reduced from a high of 28% in April 2022 to 5% in December 2022. Our early experience shows that data migration from proprietary host environments to public ""one-stop-shop"" national data warehouses are feasible provided investments are made in the requisite infrastructure, software and human resource capacity to ensure long-term sustainability, maintenance, and scale to match cloud-based data hosting. Further, digital health solutions developed in collaboration with non-state actors can be integrated into national data systems, saving Governments the cost and efforts of building similar tools while leveraging private sector capacity."
pub.1110454564,PhenoMeNal: processing and analysis of metabolomics data in the cloud,"BACKGROUND: Metabolomics is the comprehensive study of a multitude of small molecules to gain insight into an organism's metabolism. The research field is dynamic and expanding with applications across biomedical, biotechnological, and many other applied biological domains. Its computationally intensive nature has driven requirements for open data formats, data repositories, and data analysis tools. However, the rapid progress has resulted in a mosaic of independent, and sometimes incompatible, analysis methods that are difficult to connect into a useful and complete data analysis solution.
FINDINGS: PhenoMeNal (Phenome and Metabolome aNalysis) is an advanced and complete solution to set up Infrastructure-as-a-Service (IaaS) that brings workflow-oriented, interoperable metabolomics data analysis platforms into the cloud. PhenoMeNal seamlessly integrates a wide array of existing open-source tools that are tested and packaged as Docker containers through the project's continuous integration process and deployed based on a kubernetes orchestration framework. It also provides a number of standardized, automated, and published analysis workflows in the user interfaces Galaxy, Jupyter, Luigi, and Pachyderm.
CONCLUSIONS: PhenoMeNal constitutes a keystone solution in cloud e-infrastructures available for metabolomics. PhenoMeNal is a unique and complete solution for setting up cloud e-infrastructures through easy-to-use web interfaces that can be scaled to any custom public and private cloud environment. By harmonizing and automating software installation and configuration and through ready-to-use scientific workflow user interfaces, PhenoMeNal has succeeded in providing scientists with workflow-driven, reproducible, and shareable metabolomics data analysis platforms that are interfaced through standard data formats, representative datasets, versioned, and have been tested for reproducibility and interoperability. The elastic implementation of PhenoMeNal further allows easy adaptation of the infrastructure to other application areas and 'omics research domains."
pub.1130428131,A model-driven framework for data-driven applications in serverless cloud computing,"In a serverless cloud computing environment, the cloud provider dynamically manages the allocation of resources whereas the developers purely focus on their applications. The data-driven applications in serverless cloud computing mainly address the web as well as other distributed scenarios, and therefore, it is essential to offer a consistent user experience across different connection types. In order to address the issues of data-driven application in a real-time distributed environment, the use of GraphQL (Graph Query Language) is getting more and more popularity in state-of-the-art cloud computing approaches. However, the existing solutions target the low level implementation of GraphQL, for the development of a complex data-driven application, which may lead to several errors and involve a significant amount of development efforts due to various users' requirements in real-time. Therefore, it is critical to simplify the development process of data-driven applications in a serverless cloud computing environment. Consequently, this research introduces UMLPDA (Unified Modeling Language Profile for Data-driven Applications), which adopts the concepts of UML-based Model-driven Architectures to model the frontend as well as the backend requirements for data-driven applications developed at a higher abstraction level. Particularly, a modeling approach is proposed to resolve the development complexities such as data communication and synchronization. Subsequently, a complete open source transformation engine is developed using a Model-to-Text approach to automatically generate the frontend as well as backend low level implementations of Angular2 and GraphQL respectively. The validation of proposed work is performed with three different case studies, deployed on Amazon Web Services platform. The results show that the proposed framework enables to develop the data-driven applications with simplicity."
pub.1138787627,"Contactless Technologies for Smart Cities: Big Data, IoT, and Cloud Infrastructures","Intelligent systems are enhancing city environments and improving their overall performance in all possible aspects. Innovations in the field of information and communication technologies (ICT) and the proliferation of big data, internet-of-things (IoT), and cloud (BIC) infrastructures revolutionize the existing agile city ecosystems while effectively addressing customers and citizens needs. In this paper, we address the technology-driven applications that are capable of influencing the existing city infrastructures during their transformation towards smart cities with contactless technologies. We present applications, design principles, technology standards, and cost-effective techniques that leverage BIC for contactless applications and discuss user interfaces deployed in smart city environments. We further discuss state-of-the-art sensing methods and smart applications that support cities with smart contactless features. Finally, a case study is reported on how BIC can assist in efficiently handling and managing emergency situations such as the COVID-19 pandemic."
pub.1147566849,MaxQuant and MSstats in Galaxy Enable Reproducible Cloud-Based Analysis of Quantitative Proteomics Experiments for Everyone,"Quantitative mass spectrometry-based proteomics has become a high-throughput technology for the identification and quantification of thousands of proteins in complex biological samples. Two frequently used tools, MaxQuant and MSstats, allow for the analysis of raw data and finding proteins with differential abundance between conditions of interest. To enable accessible and reproducible quantitative proteomics analyses in a cloud environment, we have integrated MaxQuant (including TMTpro 16/18plex), Proteomics Quality Control (PTXQC), MSstats, and MSstatsTMT into the open-source Galaxy framework. This enables the web-based analysis of label-free and isobaric labeling proteomics experiments via Galaxy's graphical user interface on public clouds. MaxQuant and MSstats in Galaxy can be applied in conjunction with thousands of existing Galaxy tools and integrated into standardized, sharable workflows. Galaxy tracks all metadata and intermediate results in analysis histories, which can be shared privately for collaborations or publicly, allowing full reproducibility and transparency of published analysis. To further increase accessibility, we provide detailed hands-on training materials. The integration of MaxQuant and MSstats into the Galaxy framework enables their usage in a reproducible way on accessible large computational infrastructures, hence realizing the foundation for high-throughput proteomics data science for everyone."
pub.1071312238,Secure Secondary Use of Clinical Data with Cloud-based NLP Services,"OBJECTIVES: The secondary use of clinical data provides large opportunities for clinical and translational research as well as quality assurance projects. For such purposes, it is necessary to provide a flexible and scalable infrastructure that is compliant with privacy requirements. The major goals of the cloud4health project are to define such an architecture, to implement a technical prototype that fulfills these requirements and to evaluate it with three use cases.
METHODS: The architecture provides components for multiple data provider sites such as hospitals to extract free text as well as structured data from local sources and de-identify such data for further anonymous or pseudonymous processing. Free text documentation is analyzed and transformed into structured information by text-mining services, which are provided within a cloud-computing environment. Thus, newly gained annotations can be integrated along with the already available structured data items and the resulting data sets can be uploaded to a central study portal for further analysis.
RESULTS: Based on the architecture design, a prototype has been implemented and is under evaluation in three clinical use cases. Data from several hundred patients provided by a University Hospital and a private hospital chain have already been processed.
CONCLUSIONS: Cloud4health has shown how existing components for secondary use of structured data can be complemented with text-mining in a privacy compliant manner. The cloud-computing paradigm allows a flexible and dynamically adaptable service provision that facilitates the adoption of services by data providers without own investments in respective hardware resources and software tools."
pub.1170189742,Compact rover surveying and laser scanning for BIM development,"This paper presents a custom made small rover based surveying, mapping and building information modeling solution. Majority of the commercially available mobile surveying systems are larger in size which restricts their maneuverability in the targeted indoor vicinities. Furthermore their functional cost is unaffordable for low budget projects belonging to developing markets. Keeping in view these challenges, an economical indigenous rover based scanning and mapping system has developed using orthogonal integration of two low cost RPLidar A1 laser scanners. All the instrumentation of the rover has been interfaced with Robot Operating System (ROS) for online processing and recording of all sensorial data. The ROS based pose and map estimations of the rover have performed using Simultaneous Localization and Mapping (SLAM) technique. The perceived class 1 laser scans data belonging to distinct vicinities with variable reflective properties have been successfully tested and validated for required structural modeling. Systematically the recorded scans have been used in offline mode to generate the 3D point cloud map of the surveyed environment. Later the structural planes extraction from the point cloud data has been done using Random Sampling and Consensus (RANSAC) technique. Finally the 2D floor plan and 3D building model have been developed using point cloud processing in appropriate software. Multiple interiors of existing buildings and under construction indoor sites have been scanned, mapped and modelled as presented in this paper. In addition, the validation of the as-built models have been performed by comparing with the actual architecture design of the surveyed buildings. In comparison to available surveying solutions present in the local market, the developed system has been found faster, accurate and user friendly to produce more enhanced structural results with minute details."
pub.1175550690,Weld seam object detection system based on the fusion of 2D images and 3D point clouds using interpretable neural networks,"This study introduces a novel approach that addresses the limitations of existing methods by integrating 2D image processing with 3D point cloud analysis, enhanced by interpretable neural networks. Unlike traditional methods that rely on either 2D or 3D data alone, our approach leverages the complementary strengths of both data types to improve detection accuracy in environments adversely affected by welding spatter and smoke. Our system employs an improved Faster R-CNN model with a ResNet50 backbone for 2D image analysis, coupled with an innovative orthogonal plane intersection line extraction algorithm for 3D point cloud processing. By incorporating explainable components such as visualizable feature maps and a transparent region proposal network, we address the “black box” issue common in deep learning models.This architecture enables a more transparent decision-making process, providing technicians with necessary insights to understand and trust the system’s outputs. The Faster-RCNN structure is designed to break down the object detection process into distinct, understandable steps, from initial feature extraction to final bounding box refinement. This fusion of 2D-3D data analysis and interpretability not only improves detection performance but also sets a new standard for transparency and reliability in automated welding systems, facilitating wider adoption in industrial applications."
pub.1025140984,Smart Learning Services Based on Smart Cloud Computing,"Context-aware technologies can make e-learning services smarter and more efficient since context-aware services are based on the user's behavior. To add those technologies into existing e-learning services, a service architecture model is needed to transform the existing e-learning environment, which is situation-aware, into the environment that understands context as well. The context-awareness in e-learning may include the awareness of user profile and terminal context. In this paper, we propose a new notion of service that provides context-awareness to smart learning content in a cloud computing environment. We suggest the elastic four smarts (E4S)--smart pull, smart prospect, smart content, and smart push--concept to the cloud services so smart learning services are possible. The E4S focuses on meeting the users' needs by collecting and analyzing users' behavior, prospecting future services, building corresponding contents, and delivering the contents through cloud computing environment. Users' behavior can be collected through mobile devices such as smart phones that have built-in sensors. As results, the proposed smart e-learning model in cloud computing environment provides personalized and customized learning services to its users."
pub.1136325695,Kubernetes Cluster for Automating Software Production Environment,"Microservices, Continuous Integration and Delivery, Docker, DevOps, Infrastructure as Code-these are the current trends and buzzwords in the technological world of 2020. A popular tool which can facilitate the deployment and maintenance of microservices is Kubernetes. Kubernetes is a platform for running containerized applications, for example microservices. There are two main questions which answer was important for us: how to deploy Kubernetes itself and how to ensure that the deployment fulfils the needs of a production environment. Our research concentrates on the analysis and evaluation of Kubernetes cluster as the software production environment. However, firstly it is necessary to determine and evaluate the requirements of production environment. The paper presents the determination and analysis of such requirements and their evaluation in the case of Kubernetes cluster. Next, the paper compares two methods of deploying a Kubernetes cluster: kops and eksctl. Both of the methods concern the AWS cloud, which was chosen mainly because of its wide popularity and the range of provided services. Besides the two chosen methods of deployment, there are many more, including the DIY method and deploying on-premises."
pub.1163231164,A Harris Hawk Optimisation system for energy and resource efficient virtual machine placement in cloud data centers,"Virtualisation is a major technology in cloud computing for optimising the cloud data centre's power usage. In the current scenario, most of the services are migrated to the cloud, putting more load on the cloud data centres. As a result, the data center's size expands resulting in increased energy usage. To address this problem, a resource allocation optimisation method that is both efficient and effective is necessary. The optimal utilisation of cloud infrastructure and optimisation algorithms plays a vital role. The cloud resources rely on the allocation policy of the virtual machine on cloud resources. A virtual machine placement technique, based on the Harris Hawk Optimisation (HHO) model for the cloud data centre is presented in this paper. The proposed HHO model aims to find the best place for virtual machines on suitable hosts with the least load and power consumption. PlanetLab's real-time workload traces are used for performance evaluation with existing PSO (Particle Swarm Optimisation) and PABFD (Best Fit Decreasing). The performance evaluation of the proposed method is done using power consumption, SLA, CPU utilisation, RAM utilisation, Execution time (ms) and the number of VM migrations. The performance evaluation is done using two simulation scenarios with scaling workload in scenario 1 and increasing resources for the virtual machine to study the performance in underloaded and overloaded conditions. Experimental results show that the proposed HHO algorithm improved execution time(ms) by 4%, had a 27% reduction in power consumption, a 16% reduction in SLA violation and an increase in resource utilisation by 17%. The HHO algorithm is also effective in handling dynamic and uncertain environments, making it suitable for real-world cloud infrastructures."
pub.1156414425,QuantImage v2: a comprehensive and integrated physician-centered cloud platform for radiomics and machine learning research,"BackgroundRadiomics, the field of image-based computational medical biomarker research, has experienced rapid growth over the past decade due to its potential to revolutionize the development of personalized decision support models. However, despite its research momentum and important advances toward methodological standardization, the translation of radiomics prediction models into clinical practice only progresses slowly. The lack of physicians leading the development of radiomics models and insufficient integration of radiomics tools in the clinical workflow contributes to this slow uptake.MethodsWe propose a physician-centered vision of radiomics research and derive minimal functional requirements for radiomics research software to support this vision. Free-to-access radiomics tools and frameworks were reviewed to identify best practices and reveal the shortcomings of existing software solutions to optimally support physician-driven radiomics research in a clinical environment.ResultsSupport for user-friendly development and evaluation of radiomics prediction models via machine learning was found to be missing in most tools. QuantImage v2 (QI2) was designed and implemented to address these shortcomings. QI2 relies on well-established existing tools and open-source libraries to realize and concretely demonstrate the potential of a one-stop tool for physician-driven radiomics research. It provides web-based access to cohort management, feature extraction, and visualization and supports “no-code” development and evaluation of machine learning models against patient-specific outcome data.ConclusionsQI2 fills a gap in the radiomics software landscape by enabling “no-code” radiomics research, including model validation, in a clinical environment. Further information about QI2, a public instance of the system, and its source code is available at https://medgift.github.io/quantimage-v2-info/.Key pointsAs domain experts, physicians play a key role in the development of radiomics models.Existing software solutions do not support physician-driven research optimally.QuantImage v2 implements a physician-centered vision for radiomics research.QuantImage v2 is a web-based, “no-code” radiomics research platform."
pub.1153591959,Feature Consistent Point Cloud Registration in Building Information Modeling,"Point Cloud Registration contributes a lot to measuring, monitoring, and simulating in building information modeling (BIM). In BIM applications, the robustness and generalization of point cloud features are particularly important due to the huge differences in sampling environments. We notice two possible factors that may lead to poor generalization, the normal ambiguity of boundaries on hard edges leading to less accuracy in transformation; and the fact that existing methods focus on spatial transformation accuracy, leaving the advantages of feature matching unaddressed. In this work, we propose a boundary-encouraging local frame reference, the PyramidFeature(PMD), consisting of point-level, line-level, and mesh-level information to extract a more generalizing and continuous point cloud feature to encourage the knowledge of boundaries to overcome the normal ambiguity. Furthermore, instead of registration guided by spatial transformation accuracy alone, we suggest another supervision to extract consistent hybrid features. A large number of experiments have demonstrated the superiority of our PyramidNet (PMDNet), especially when the training (ModelNet40) and testing (BIM) sets are very different, PMDNet still achieves very high scalability."
pub.1034700829,Scalable Computing for Evolutionary Genomics,"Genomic data analysis in evolutionary biology is becoming so computationally intensive that analysis of multiple hypotheses and scenarios takes too long on a single desktop computer. In this chapter, we discuss techniques for scaling computations through parallelization of calculations, after giving a quick overview of advanced programming techniques. Unfortunately, parallel programming is difficult and requires special software design. The alternative, especially attractive for legacy software, is to introduce poor man’s parallelization by running whole programs in parallel as separate processes, using job schedulers. Such pipelines are often deployed on bioinformatics computer clusters. Recent advances in PC virtualization have made it possible to run a full computer operating system, with all of its installed software, on top of another operating system, inside a “box,” or virtual machine (VM). Such a VM can flexibly be deployed on multiple computers, in a local network, e.g., on existing desktop PCs, and even in the Cloud, to create a “virtual” computer cluster. Many bioinformatics applications in evolutionary biology can be run in parallel, running processes in one or more VMs. Here, we show how a ready-made bioinformatics VM image, named BioNode, effectively creates a computing cluster, and pipeline, in a few steps. This allows researchers to scale-up computations from their desktop, using available hardware, anytime it is required. BioNode is based on Debian Linux and can run on networked PCs and in the Cloud. Over 200 bioinformatics and statistical software packages, of interest to evolutionary biology, are included, such as PAML, Muscle, MAFFT, MrBayes, and BLAST. Most of these software packages are maintained through the Debian Med project. In addition, BioNode contains convenient configuration scripts for parallelizing bioinformatics software. Where Debian Med encourages packaging free and open source bioinformatics software through one central project, BioNode encourages creating free and open source VM images, for multiple targets, through one central project. BioNode can be deployed on Windows, OSX, Linux, and in the Cloud. Next to the downloadable BioNode images, we provide tutorials online, which empower bioinformaticians to install and run BioNode in different environments, as well as information for future initiatives, on creating and building such images."
pub.1001463773,A Smart City Lighting Case Study on an OpenStack-Powered Infrastructure,"The adoption of embedded systems, mobile devices and other smart devices keeps rising globally, and the scope of their involvement broadens, for instance, in smart city-like scenarios. In light of this, a pressing need emerges to tame such complexity and reuse as much tooling as possible without resorting to vertical ad hoc solutions, while at the same time taking into account valid options with regard to infrastructure management and other more advanced functionalities. Existing solutions mainly focus on core mechanisms and do not allow one to scale by leveraging infrastructure or adapt to a variety of scenarios, especially if actuators are involved in the loop. A new, more flexible, cloud-based approach, able to provide device-focused workflows, is required. In this sense, a widely-used and competitive framework for infrastructure as a service, such as OpenStack, with its breadth in terms of feature coverage and expanded scope, looks to fit the bill, replacing current application-specific approaches with an innovative application-agnostic one. This work thus describes the rationale, efforts and results so far achieved for an integration of IoT paradigms and resource ecosystems with such a kind of cloud-oriented device-centric environment, by focusing on a smart city scenario, namely a park smart lighting example, and featuring data collection, data visualization, event detection and coordinated reaction, as example use cases of such integration. "
pub.1165661697,Fault tolerant trust based task scheduler using Harris Hawks optimization and deep reinforcement learning in multi cloud environment,"Cloud Computing model provides on demand delivery of seamless services to customers around the world yet single point of failures occurs in cloud model due to improper assignment of tasks to precise virtual machines which leads to increase in rate of failures which effects SLA based trust parameters (Availability, success rate, turnaround efficiency) upon which impacts trust on cloud provider. In this paper, we proposed a task scheduling algorithm which captures priorities of all tasks, virtual resources from task manager which comes onto cloud application console are fed to task scheduler which takes scheduling decisions based on hybridization of both Harris hawk optimization and ML based reinforcement algorithms to enhance the scheduling process. Task scheduling in this research performed in two phases i.e. Task selection and task mapping phases. In task selection phase, all incoming priorities of tasks, VMs are captured and generates schedules using Harris hawks optimization. In task mapping phase, generated schedules are optimized using a DQN model which is based on deep reinforcement learning. In this research, we used multi cloud environment to tackle availability of VMs if there is an increase in upcoming tasks dynamically and migrate tasks to one cloud to another to mitigate migration time. Extensive simulations are conducted in Cloudsim and workload generated by fabricated datasets and realtime synthetic workloads from NASA, HPC2N are used to check efficacy of our proposed scheduler (FTTHDRL). It compared against existing task schedulers i.e. MOABCQ, RATS-HM, AINN-BPSO approaches and our proposed FTTHDRL outperforms existing mechanisms by minimizing rate of failures, resource cost, improved SLA based trust parameters."
pub.1120929754,Integration of the Rosetta suite with the python software stack via reproducible packaging and core programming interfaces for distributed simulation,"The Rosetta software suite for macromolecular modeling is a powerful computational toolbox for protein design, structure prediction, and protein structure analysis. The development of novel Rosetta-based scientific tools requires two orthogonal skill sets: deep domain-specific expertise in protein biochemistry and technical expertise in development, deployment, and analysis of molecular simulations. Furthermore, the computational demands of molecular simulation necessitate large scale cluster-based or distributed solutions for nearly all scientifically relevant tasks. To reduce the technical barriers to entry for new development, we integrated Rosetta with modern, widely adopted computational infrastructure. This allows simplified deployment in large-scale cluster and cloud computing environments, and effective reuse of common libraries for simulation execution and data analysis. To achieve this, we integrated Rosetta with the Conda package manager; this simplifies installation into existing computational environments and packaging as docker images for cloud deployment. Then, we developed programming interfaces to integrate Rosetta with the PyData stack for analysis and distributed computing, including the popular tools Jupyter, Pandas, and Dask. We demonstrate the utility of these components by generating a library of a thousand de novo disulfide-rich miniproteins in a hybrid simulation that included cluster-based design and interactive notebook-based analyses. Our new tools enable users, who would otherwise not have access to the necessary computational infrastructure, to perform state-of-the-art molecular simulation and design with Rosetta."
pub.1156330687,VAI-B: a multicenter platform for the external validation of artificial intelligence algorithms in breast imaging,"Purpose: Multiple vendors are currently offering artificial intelligence (AI) computer-aided systems for triage detection, diagnosis, and risk prediction of breast cancer based on screening mammography. There is an imminent need to establish validation platforms that enable fair and transparent testing of these systems against external data.
Approach: We developed validation of artificial intelligence for breast imaging (VAI-B), a platform for independent validation of AI algorithms in breast imaging. The platform is a hybrid solution, with one part implemented in the cloud and another in an on-premises environment at Karolinska Institute. Cloud services provide the flexibility of scaling the computing power during inference time, while secure on-premises clinical data storage preserves their privacy. A MongoDB database and a python package were developed to store and manage the data on-premises. VAI-B requires four data components: radiological images, AI inferences, radiologist assessments, and cancer outcomes.
Results: To pilot test VAI-B, we defined a case-control population based on 8080 patients diagnosed with breast cancer and 36,339 healthy women based on the Swedish national quality registry for breast cancer. Images and radiological assessments from more than 100,000 mammography examinations were extracted from hospitals in three regions of Sweden. The images were processed by AI systems from three vendors in a virtual private cloud to produce abnormality scores related to signs of cancer in the images. A total of 105,706 examinations have been processed and stored in the database.
Conclusions: We have created a platform that will allow downstream evaluation of AI systems for breast cancer detection, which enables faster development cycles for participating vendors and safer AI adoption for participating hospitals. The platform was designed to be scalable and ready to be expanded should a new vendor want to evaluate their system or should a new hospital wish to obtain an evaluation of different AI systems on their images."
pub.1163878853,Personalizable AI platform for universal access to research and diagnosis in digital pathology,"BACKGROUND AND MOTIVATION: Digital pathology has been evolving over the last years, proposing significant workflow advantages that have fostered its adoption in professional environments. Patient clinical and image data are readily available in remote data banks that can be consumed efficiently over standard communication technologies. The appearance of new imaging techniques and advanced artificial intelligence algorithms has significantly reduced the burden on medical professionals by speeding up the screening process. Despite these advancements, the usage of digital pathology in professional environments has been slowed down by poor interoperability between services resulting from a lack of standard interfaces and integrative solutions. This work addresses this issue by proposing a cloud-based digital pathology platform built on standard and open interfaces.
METHODS: The work proposes and describes a vendor-neutral platform that provides interfaces for managing digital slides, and medical reports, and integrating digital image analysis services compatible with existing standards. The solution integrates the open-source plugin-based Dicoogle PACS for interoperability and extensibility, which grants the proposed solution great feature customization.
RESULTS: The solution was developed in collaboration with iPATH research project partners, including the validation by medical pathologists. The result is a pure Web collaborative framework that supports both research and production environments. A total of 566 digital slides from different pathologies were successfully uploaded to the platform. Using the integration interfaces, a mitosis detection algorithm was successfully installed into the platform, and it was trained with 2400 annotations collected from breast carcinoma images.
CONCLUSION: Interoperability is a key factor when discussing digital pathology solutions, as it facilitates their integration into existing institutions' information systems. Moreover, it improves data sharing and integration of third-party services such as image analysis services, which have become relevant in today's digital pathology workflow. The proposed solution fully embraces the DICOM standard for digital pathology, presenting an interoperable cloud-based solution that provides great feature customization thanks to its extensible architecture."
pub.1174757814,An enhanced round robin using dynamic time quantum for real-time asymmetric burst length processes in cloud computing environment,"Cloud computing is a popular, flexible, scalable, and cost-effective technology in the modern world that provides on-demand services dynamically. The dynamic execution of user requests and resource-sharing facilities require proper task scheduling among the available virtual machines, which is a significant issue and plays a crucial role in developing an optimal cloud computing environment. Round Robin is a prevalent scheduling algorithm for fair distribution of resources with a balanced contribution in minimized response time and turnaround time. This paper introduced a new enhanced round-robin approach for task scheduling in cloud computing systems. The proposed algorithm generates and keeps updating a dynamic quantum time for process execution, considering the available number of process in the system and their burst length. Since our method dynamically runs processes, it is appropriate for a real-time environment like cloud computing. The notable part of this approach is the capability of scheduling tasks with asymmetric distribution of burst time, avoiding the convoy effect. The experimental result indicates that the proposed algorithm has outperformed the existing improved round-robin task scheduling approaches in terms of minimized average waiting time, average turnaround time, and number of context switches. Comparing the method against five other enhanced round robin approaches, it reduced average waiting times by 15.77% and context switching by 20.68% on average. After executing the experiment and comparative study, it can be concluded that the proposed enhanced round-robin scheduling algorithm is optimal, acceptable, and relatively better suited for cloud computing environments."
pub.1127952068,Adoption of image surface parameters under moving edge computing in the construction of mountain fire warning method,"In order to cope with the problems of high frequency and multiple causes of mountain fires, it is very important to adopt appropriate technologies to monitor and warn mountain fires through a few surface parameters. At the same time, the existing mobile terminal equipment is insufficient in image processing and storage capacity, and the energy consumption is high in the data transmission process, which requires calculation unloading. For this circumstance, first, a hierarchical discriminant analysis algorithm based on image feature extraction is introduced, and the image acquisition software in the mobile edge computing environment in the android system is designed and installed. Based on the remote sensing data, the land surface parameters of mountain fire are obtained, and the application of image recognition optimization algorithm in the mobile edge computing (MEC) environment is realized to solve the problem of transmission delay caused by traditional mobile cloud computing (MCC). Then, according to the forest fire sensitivity index, a forest fire early warning model based on MEC is designed. Finally, the image recognition response time and bandwidth consumption of the algorithm are studied, and the occurrence probability of mountain fire in Muli county, Liangshan prefecture, Sichuan is predicted. The results show that, compared with the MCC architecture, the algorithm presented in this study has shorter recognition and response time to different images in WiFi network environment; compared with MCC, MEC architecture can identify close users and transmit less data, which can effectively reduce the bandwidth pressure of the network. In most areas of Muli county, Liangshan prefecture, the probability of mountain fire is relatively low, the probability of mountain fire caused by non-surface environment is about 8 times that of the surface environment, and the influence of non-surface environment in the period of high incidence of mountain fire is lower than that in the period of low incidence. In conclusion, the surface parameters of MEC can be used to effectively predict the mountain fire and provide preventive measures in time."
pub.1175854807,"DECOMICS, a shiny application for unsupervised cell type deconvolution and biological interpretation of bulk omic data","Summary: Unsupervised deconvolution algorithms are often used to estimate cell composition from bulk tissue samples. However, applying cell-type deconvolution and interpreting the results remain a challenge, even more without prior training in bioinformatics. Here, we propose a tool for estimating and identifying cell type composition from bulk transcriptomes or methylomes. DECOMICS is a shiny-web application dedicated to unsupervised deconvolution approaches of bulk omic data. It provides (i) a variety of existing algorithms to perform deconvolution on the gene expression or methylation-level matrix, (ii) an enrichment analysis module to aid biological interpretation of the deconvolved components, based on enrichment analysis, and (iii) some visualization tools. Input data can be downloaded in csv format and preprocessed in the web application (normalization, transformation, and feature selection). The results of the deconvolution, enrichment, and visualization processes can be downloaded.
Availability and implementation: DECOMICS is an R-shiny web application that can be launched (i) directly from a local R session using the R package available here: https://gitlab.in2p3.fr/Magali.Richard/decomics (either by installing it locally or via a virtual machine and a Docker image that we provide); or (ii) in the Biosphere-IFB Clouds Federation for Life Science, a multi-cloud environment scalable for high-performance computing: https://biosphere.france-bioinformatique.fr/catalogue/appliance/193/."
pub.1085173651,Public Auditing with Privacy Protection in a Multi-User Model of Cloud-Assisted Body Sensor Networks,"Wireless Body Sensor Networks (WBSNs) are gaining importance in the era of the Internet of Things (IoT). The modern medical system is a particular area where the WBSN techniques are being increasingly adopted for various fundamental operations. Despite such increasing deployments of WBSNs, issues such as the infancy in the size, capabilities and limited data processing capacities of the sensor devices restrain their adoption in resource-demanding applications. Though providing computing and storage supplements from cloud servers can potentially enrich the capabilities of the WBSNs devices, data security is one of the prevailing issues that affects the reliability of cloud-assisted services. Sensitive applications such as modern medical systems demand assurance of the privacy of the users' medical records stored in distant cloud servers. Since it is economically impossible to set up private cloud servers for every client, auditing data security managed in the remote servers has necessarily become an integral requirement of WBSNs' applications relying on public cloud servers. To this end, this paper proposes a novel certificateless public auditing scheme with integrated privacy protection. The multi-user model in our scheme supports groups of users to store and share data, thus exhibiting the potential for WBSNs' deployments within community environments. Furthermore, our scheme enriches user experiences by offering public verifiability, forward security mechanisms and revocation of illegal group members. Experimental evaluations demonstrate the security effectiveness of our proposed scheme under the Random Oracle Model (ROM) by outperforming existing cloud-assisted WBSN models."
pub.1142562671,Fault-Tolerant and Data-Intensive Resource Scheduling and Management for Scientific Applications in Cloud Computing,"Cloud computing is a fully fledged, matured and flexible computing paradigm that provides services to scientific and business applications in a subscription-based environment. Scientific applications such as Montage and CyberShake are organized scientific workflows with data and compute-intensive tasks and also have some special characteristics. These characteristics include the tasks of scientific workflows that are executed in terms of integration, disintegration, pipeline, and parallelism, and thus require special attention to task management and data-oriented resource scheduling and management. The tasks executed during pipeline are considered as bottleneck executions, the failure of which result in the wholly futile execution, which requires a fault-tolerant-aware execution. The tasks executed during parallelism require similar instances of cloud resources, and thus, cluster-based execution may upgrade the system performance in terms of make-span and execution cost. Therefore, this research work presents a cluster-based, fault-tolerant and data-intensive (CFD) scheduling for scientific applications in cloud environments. The CFD strategy addresses the data intensiveness of tasks of scientific workflows with cluster-based, fault-tolerant mechanisms. The Montage scientific workflow is considered as a simulation and the results of the CFD strategy were compared with three well-known heuristic scheduling policies: (a) MCT, (b) Max-min, and (c) Min-min. The simulation results showed that the CFD strategy reduced the make-span by 14.28%, 20.37%, and 11.77%, respectively, as compared with the existing three policies. Similarly, the CFD reduces the execution cost by 1.27%, 5.3%, and 2.21%, respectively, as compared with the existing three policies. In case of the CFD strategy, the SLA is not violated with regard to time and cost constraints, whereas it is violated by the existing policies numerous times."
pub.1165073948,Health Status Classification for Cows Using Machine Learning and Data Management on AWS Cloud,"The health and welfare of livestock are significant for ensuring the sustainability and profitability of the agricultural industry. Addressing efficient ways to monitor and report the health status of individual cows is critical to prevent outbreaks and maintain herd productivity. The purpose of the study is to develop a machine learning (ML) model to classify the health status of milk cows into three categories. In this research, data are collected from existing non-invasive IoT devices and tools in a dairy farm, monitoring the micro- and macroenvironment of the cow in combination with particular information on age, days in milk, lactation, and more. A workflow of various data-processing methods is systematized and presented to create a complete, efficient, and reusable roadmap for data processing, modeling, and real-world integration. Following the proposed workflow, the data were treated, and five different ML algorithms were trained and tested to select the most descriptive one to monitor the health status of individual cows. The highest result for health status assessment is obtained by random forest classifier (RFC) with an accuracy of 0.959, recall of 0.954, and precision of 0.97. To increase the security, speed, and reliability of the work process, a cloud architecture of services is presented to integrate the trained model as an additional functionality in the Amazon Web Services (AWS) environment. The classification results of the ML model are visualized in a newly created interface in the client application."
pub.1150713448,Cloud Data-Driven Intelligent Monitoring System for Interactive Smart Farming,"Smart farms, as a part of high-tech agriculture, collect a huge amount of data from IoT devices about the conditions of animals, plants, and the environment. These data are most often stored locally and are not used in intelligent monitoring systems to provide opportunities for extracting meaningful knowledge for the farmers. This often leads to a sense of missed transparency, fairness, and accountability, and a lack of motivation for the majority of farmers to invest in sensor-based intelligent systems to support and improve the technological development of their farm and the decision-making process. In this paper, a data-driven intelligent monitoring system in a cloud environment is proposed. The designed architecture enables a comprehensive solution for interaction between data extraction from IoT devices, preprocessing, storage, feature engineering, modelling, and visualization. Streaming data from IoT devices to interactive live reports along with built machine learning (ML) models are included. As a result of the proposed intelligent monitoring system, the collected data and ML modelling outcomes are visualized using a powerful dynamic dashboard. The dashboard allows users to monitor various parameters across the farm and provides an accessible way to view trends, deviations, and patterns in the data. ML models are trained on the collected data and are updated periodically. The data-driven visualization enables farmers to examine, organize, and represent collected farm's data with the goal of better serving their needs. Performance and durability tests of the system are provided. The proposed solution is a technological bridge with which farmers can easily, affordably, and understandably monitor and track the progress of their farms with easy integration into an existing IoT system."
pub.1158683518,Load Balancing Using Artificial Intelligence for Cloud-Enabled Internet of Everything in Healthcare Domain,"The emergence of the Internet of Things (IoT) and its subsequent evolution into the Internet of Everything (IoE) is a result of the rapid growth of information and communication technologies (ICT). However, implementing these technologies comes with certain obstacles, such as the limited availability of energy resources and processing power. Consequently, there is a need for energy-efficient and intelligent load-balancing models, particularly in healthcare, where real-time applications generate large volumes of data. This paper proposes a novel, energy-aware artificial intelligence (AI)-based load balancing model that employs the Chaotic Horse Ride Optimization Algorithm (CHROA) and big data analytics (BDA) for cloud-enabled IoT environments. The CHROA technique enhances the optimization capacity of the Horse Ride Optimization Algorithm (HROA) using chaotic principles. The proposed CHROA model balances the load, optimizes available energy resources using AI techniques, and is evaluated using various metrics. Experimental results show that the CHROA model outperforms existing models. For instance, while the Artificial Bee Colony (ABC), Gravitational Search Algorithm (GSA), and Whale Defense Algorithm with Firefly Algorithm (WD-FA) techniques attain average throughputs of 58.247 Kbps, 59.957 Kbps, and 60.819 Kbps, respectively, the CHROA model achieves an average throughput of 70.122 Kbps. The proposed CHROA-based model presents an innovative approach to intelligent load balancing and energy optimization in cloud-enabled IoT environments. The results highlight its potential to address critical challenges and contribute to developing efficient and sustainable IoT/IoE solutions."
pub.1171217141,A Performance Analysis of Security Protocols for Distributed Measurement Systems Based on Internet of Things with Constrained Hardware and Open Source Infrastructures,"The widespread adoption of Internet of Things (IoT) devices in home, industrial, and business environments has made available the deployment of innovative distributed measurement systems (DMS). This paper takes into account constrained hardware and a security-oriented virtual local area network (VLAN) approach that utilizes local message queuing telemetry transport (MQTT) brokers, transport layer security (TLS) tunnels for local sensor data, and secure socket layer (SSL) tunnels to transmit TLS-encrypted data to a cloud-based central broker. On the other hand, the recent literature has shown a correlated exponential increase in cyber attacks, mainly devoted to destroying critical infrastructure and creating hazards or retrieving sensitive data about individuals, industrial or business companies, and many other entities. Much progress has been made to develop security protocols and guarantee quality of service (QoS), but they are prone to reducing the network throughput. From a measurement science perspective, lower throughput can lead to a reduced frequency with which the phenomena can be observed, generating, again, misevaluation. This paper does not give a new approach to protect measurement data but tests the network performance of the typically used ones that can run on constrained hardware. This is a more general scenario typical for IoT-based DMS. The proposal takes into account a security-oriented VLAN approach for hardware-constrained solutions. Since it is a worst-case scenario, this permits the generalization of the achieved results. In particular, in the paper, all OpenSSL cipher suites are considered for compatibility with the Mosquitto server. The most used key metrics are evaluated for each cipher suite and QoS level, such as the total ratio, total runtime, average runtime, message time, average bandwidth, and total bandwidth. Numerical and experimental results confirm the proposal's effectiveness in foreseeing the minimum network throughput concerning the selected QoS and security. Operating systems yield diverse performance metric values based on various configurations. The primary objective is identifying algorithms to ensure suitable data transmission and encryption ratios. Another aim is to explore algorithms that ensure wider compatibility with existing infrastructures supporting MQTT technology, facilitating secure connections for geographically dispersed DMS IoT networks, particularly in challenging environments like suburban or rural areas. Additionally, leveraging open firmware on constrained devices compatible with various MQTT protocols enables the customization of the software components, a crucial necessity for DMS."
pub.1110426892,Cloud Deployment of High-Resolution Medical Image Analysis With TOMAAT,"BACKGROUND: Deep learning has been recently applied to a multitude of computer vision and medical image analysis problems. Although recent research efforts have improved the state of the art, most of the methods cannot be easily accessed, compared or used by other researchers or clinicians. Even if developers publish their code and pre-trained models on the internet, integration in stand-alone applications and existing workflows is often not straightforward, especially for clinical research partners. In this paper, we propose an open-source framework to provide AI-enabled medical image analysis through the network.
METHODS: TOMAAT provides a cloud environment for general medical image analysis, composed of three basic components: (i) an announcement service, maintaining a public registry of (ii) multiple distributed server nodes offering various medical image analysis solutions, and (iii) client software offering simple interfaces for users. Deployment is realized through HTTP-based communication, along with an API and wrappers for common image manipulations during pre- and post-processing.
RESULTS: We demonstrate the utility and versatility of TOMAAT on several hallmark medical image analysis tasks: segmentation, diffeomorphic deformable atlas registration, landmark localization, and workflow integration. Through TOMAAT, the high hardware demands, setup and model complexity of demonstrated approaches are transparent to users, who are provided with simple client interfaces. We present example clients in three-dimensional Slicer, in the web browser, on iOS devices and in a commercially available, certified medical image analysis suite.
CONCLUSION: TOMAAT enables deployment of state-of-the-art image segmentation in the cloud, fostering interaction among deep learning researchers and medical collaborators in the clinic. Currently, a public announcement service is hosted by the authors, and several ready-to-use services are registered and enlisted at http://tomaat.cloud."
pub.1122544116,An analytical model to minimize the latency in healthcare internet-of-things in fog computing environment,"Fog computing (FC) is an evolving computing technology that operates in a distributed environment. FC aims to bring cloud computing features close to edge devices. The approach is expected to fulfill the minimum latency requirement for healthcare Internet-of-Things (IoT) devices. Healthcare IoT devices generate various volumes of healthcare data. This large volume of data results in high data traffic that causes network congestion and high latency. An increase in round-trip time delay owing to large data transmission and large hop counts between IoTs and cloud servers render healthcare data meaningless and inadequate for end-users. Time-sensitive healthcare applications require real-time data. Traditional cloud servers cannot fulfill the minimum latency demands of healthcare IoT devices and end-users. Therefore, communication latency, computation latency, and network latency must be reduced for IoT data transmission. FC affords the storage, processing, and analysis of data from cloud computing to a network edge to reduce high latency. A novel solution for the abovementioned problem is proposed herein. It includes an analytical model and a hybrid fuzzy-based reinforcement learning algorithm in an FC environment. The aim is to reduce high latency among healthcare IoTs, end-users, and cloud servers. The proposed intelligent FC analytical model and algorithm use a fuzzy inference system combined with reinforcement learning and neural network evolution strategies for data packet allocation and selection in an IoT-FC environment. The approach is tested on simulators iFogSim (Net-Beans) and Spyder (Python). The obtained results indicated the better performance of the proposed approach compared with existing methods."
pub.1156046530,"A Systematic Literature Review on Service Composition for People with Disabilities: Taxonomies, Solutions, and Open Research Challenges","Integrating smart heterogeneous objects, IoT devices, data sources, and software services to produce new business processes and functionalities continues to attract considerable attention from the research community due to its unraveled advantages, including reusability, adaptation, distribution, and pervasiveness. However, the exploitation of service-oriented computing technologies (e.g., SOC, SOA, and microservice architectures) by people with special needs is underexplored and often overlooked. Furthermore, the existing challenges in this area are yet to be identified clearly. This research study presents a rigorous literature survey of the recent advances in service-oriented composition approaches and solutions for disabled people, their domains of application, and the major challenges, covering studies published between January 2010 and October 2022. To this end, we applied the systematic literature review (SLR) methodology to retrieve and collate only the articles presenting and discussing service composition solutions tailored to produce digitally accessible services for consumption by people who suffer from an impairment or loss of some physical or mental functions. We searched six renowned bibliographic databases, particularly IEEE Xplore, Web of Science, Springer Link, ACM Library, ScienceDirect, and Google Scholar, to synthesize a final pool of 38 related articles. Our survey contributes a comprehensive taxonomy of service composition solutions, techniques, and practices that are utilized to create assistive technologies and services. The seven-facet taxonomy helps researchers and practitioners to quickly understand and analyze the fundamental conceptualizations and characteristics of accessible service composition for people with disabilities. Key findings showed that services are fused to assist disabled persons to carry out their daily activities, mainly in smart homes and ambient intelligent environments. Despite the emergence of immersive technologies (e.g., wearable computing), user-service interactions are enabled primarily through tactile and speech modalities. Service descriptions mainly incorporate functional features (e.g., performance, latency, and cost) of service quality, largely ignoring accessibility features. Moreover, the outstanding research problems revolve around (1) the unavailability of assistive services datasets, (2) the underspecification of accessibility aspects of disabilities, (3) the weak adoption of accessible and universal design practices, (4) the abstraction of service composition approaches, and (5) the rare experimental testing of composition approaches with disabled users. We conclude our survey with a set of guidelines to realize effective assistive service composition in IoT and cloud environments. Researchers and practitioners are advised to create assistive services that support the social relationships of disabled users and model their accessibility needs as part of the quality of service (QoS)"
pub.1150316818,A Versatile and Scalable Platform That Streamlines Data Collection for Patient-Centered Studies: Usability and Feasibility Study,"BACKGROUND: The Food and Drug Administration Center for Biologics Evaluation and Research (CBER) established the Biologics Effectiveness and Safety (BEST) Initiative with several objectives, including the expansion and enhancement of CBER's access to fit-for-purpose data sources, analytics, tools, and infrastructures to improve the understanding of patient experiences with conditions related to CBER-regulated products. Owing to existing challenges in data collection, especially for rare disease research, CBER recognized the need for a comprehensive platform where study coordinators can engage with study participants and design and deploy studies while patients or caregivers could enroll, consent, and securely participate as well.
OBJECTIVE: This study aimed to increase awareness and describe the design, development, and novelty of the Survey of Health and Patient Experience (SHAPE) platform, its functionality and application, quality improvement efforts, open-source availability, and plans for enhancement.
METHODS: SHAPE is hosted in a Google Cloud environment and comprises 3 parts: the administrator application, participant app, and application programming interface. The administrator can build a study comprising a set of questionnaires and self-report entries through the app. Once the study is deployed, the participant can access the app, consent to the study, and complete its components. To build SHAPE to be scalable and flexible, we leveraged the open-source software development kit, Ionic Framework. This enabled the building and deploying of apps across platforms, including iOS, Android, and progressive web applications, from a single codebase by using standardized web technologies. SHAPE has been integrated with a leading Health Level 7 (HL7®) Fast Healthcare Interoperability Resources (FHIR®) application programming interface platform, 1upHealth, which allows participants to consent to 1-time data pull of their electronic health records. We used an agile-based process that engaged multiple stakeholders in SHAPE's design and development.
RESULTS: SHAPE allows study coordinators to plan, develop, and deploy questionnaires to obtain important end points directly from patients or caregivers. Electronic health record integration enables access to patient health records, which can validate and enhance the accuracy of data-capture methods. The administrator can then download the study data into HL7® FHIR®-formatted JSON files. In this paper, we illustrate how study coordinators can use SHAPE to design patient-centered studies. We demonstrate its broad applicability through a hypothetical type 1 diabetes cohort study and an ongoing pilot study on metachromatic leukodystrophy to implement best practices for designing a regulatory-grade natural history study for rare diseases.
CONCLUSIONS: SHAPE is an intuitive and comprehensive data-collection tool for a variety of clinical studies. Further customization of this versatile and scalable platform allo"
pub.1165562177,The impact of digital transformation on green total factor productivity of heavily polluting enterprises,"Introduction: Digital transformation has become an important engine for economic high-quality development and environment high-level protection. However, green total factor productivity (GTFP), as an indicator that comprehensively reflects economic and environmental benefits, there is a lack of studies that analyze the effect of digital transformation on heavily polluting enterprises' GTFP from a micro perspective, and its impact mechanism is still unclear. Therefore, we aim to study the impact of digital transformation on heavily polluting enterprises' GTFP and its mechanism, and explore the heterogeneity of its impact.
Methods: We use Chinese A-share listed enterprises in the heavily polluting industry data from 2007 to 2019, measure enterprise digital transformation indicator using text analysis, and measure enterprise GTFP indicator using the GML index based on SBM directional distance function, to investigate the impact of digital transformation on heavily polluting enterprises' GTFP.
Results: Digital transformation can significantly enhance heavily polluting enterprises' GTFP, and this finding still holds after considering the endogenous problem and conducting robustness tests. Digital transformation can enhance heavily polluting enterprises' GTFP by promoting green innovation, improving management efficiency, and reducing external transaction costs. The improvement role of digital transformation on heavily polluting enterprises' GTFP is more obvious in the samples of non-state-owned enterprises, non-high-tech industries, and the eastern region. Compared with blockchain technology, artificial intelligence technology, cloud computing technology, big data technology, and digital technology application can significantly improve heavily polluting enterprises' GTFP.
Discussion: Our paper breaks through the limitations of existing research, which not only theoretically enriches the literature related to digital transformation and GTFP, but also practically provides policy implications for continuously promoting heavily polluting enterprises' digital transformation and facilitating their high-quality development."
pub.1167923358,HIVseqDB: a portable resource for NGS and sample metadata integration for HIV-1 drug resistance analysis,"Summary: Human immunodeficiency virus (HIV) remains a public health threat, with drug resistance being a major concern in HIV treatment. Next-generation sequencing (NGS) is a powerful tool for identifying low-abundance drug resistance mutations (LA-DRMs) that conventional Sanger sequencing cannot reliably detect. To fully understand the significance of LA-DRMs, it is necessary to integrate NGS data with clinical and demographic data. However, freely available tools for NGS-based HIV-1 drug resistance analysis do not integrate these data. This poses a challenge in interpretation of the impact of LA-DRMs, mainly for resource-limited settings due to the shortage of bioinformatics expertise. To address this challenge, we present HIVseqDB, a portable, secure, and user-friendly resource for integrating NGS data with associated clinical and demographic data for analysis of HIV drug resistance. HIVseqDB currently supports uploading of NGS data and associated sample data, HIV-1 drug resistance data analysis, browsing of uploaded data, and browsing and visualizing of analysis results. Each function of HIVseqDB corresponds to an individual Django application. This ensures efficient incorporation of additional features with minimal effort. HIVseqDB can be deployed on various computing environments, such as on-premises high-performance computing facilities and cloud-based platforms.
Availability and implementation: HIVseqDB is available at https://github.com/AlfredUg/HIVseqDB. A deployed instance of HIVseqDB is available at https://hivseqdb.org."
pub.1169997343,PPPCT: Privacy-Preserving framework for Parallel Clustering Transcriptomics data,"Single-cell transcriptomics data provides crucial insights into patients' health, yet poses significant privacy concerns. Genomic data privacy attacks can have deep implications, encompassing not only the patients' health information but also extending widely to compromise their families'. Moreover, the permanence of leaked data exacerbates the challenges, making retraction an impossibility. While extensive efforts have been directed towards clustering single-cell transcriptomics data, addressing critical challenges, especially in the realm of privacy, remains pivotal. This paper introduces an efficient, fast, privacy-preserving approach for clustering single-cell RNA-sequencing (scRNA-seq) datasets. The key contributions include ensuring data privacy, achieving high-quality clustering, accommodating the high dimensionality inherent in the datasets, and maintaining reasonable computation time for big-scale datasets. Our proposed approach utilizes the map-reduce scheme to parallelize clustering, addressing intensive calculation challenges. Intel Software Guard eXtension (SGX) processors are used to ensure the security of sensitive code and data during processing. Additionally, the approach incorporates a logarithm transformation as a preprocessing step, employs non-negative matrix factorization for dimensionality reduction, and utilizes parallel k-means for clustering. The approach fully leverages the computing capabilities of all processing resources within a secure private cloud environment. Experimental results demonstrate the efficacy of our approach in preserving patient privacy while surpassing state-of-the-art methods in both clustering quality and computation time. Our method consistently achieves a minimum of 7% higher Adjusted Rand Index (ARI) than existing approaches, contingent on dataset size. Additionally, due to parallel computations and dimensionality reduction, our approach exhibits efficiency, converging to very good results in less than 10 seconds for a scRNA-seq dataset with 5000 genes and 6000 cells when prioritizing privacy and under two seconds without privacy considerations. Availability and implementation Code and datasets availability: https://github.com/University-of-Windsor/PPPCT."
pub.1172674065,Medical Extended Reality for Radiology Education and Training,"Medical extended reality (MXR), encompassing augmented reality, virtual reality, and mixed reality (MR), presents a novel paradigm in radiology training by offering immersive, interactive, and realistic learning experiences in health care. Although traditional educational tools in the field of radiology are essential, it is necessary to capitalize on the innovative and emerging educational applications of extended reality (XR) technologies. At the most basic level of learning anatomy, XR has been extensively used with an emphasis on its superiority over conventional learning methods, especially in spatial understanding and recall. For imaging interpretation, XR has fostered the concepts of virtual reading rooms by enabling collaborative learning environments and enhancing image analysis and understanding. Moreover, image-guided interventions in interventional radiology have witnessed an uptick in XR utilization, illustrating its effectiveness in procedural training and skill acquisition for medical students and residents in a safe and risk-free environment. However, there remain several challenges and limitations for XR in radiology education, including technological, economic, and ergonomic challenges and and integration into existing curricula. This review explores the transformative potential of MXR in radiology education and training along with insights on the future of XR in radiology education, forecasting advancements in immersive simulations, artificial intelligence integration for personalized learning, and the potential of cloud-based XR platforms for remote and collaborative training. In summation, MXR's burgeoning role in reshaping radiology education offers a safer, scalable, and more efficient training model that aligns with the dynamic healthcare landscape."
pub.1141910947,ROA: A Rapid Learning Scheme for In-Situ Memristor Networks,"Memristors show great promise in neuromorphic computing owing to their high-density integration, fast computing and low-energy consumption. However, the non-ideal update of synaptic weight in memristor devices, including nonlinearity, asymmetry and device variation, still poses challenges to the <i>in-situ</i> learning of memristors, thereby limiting their broad applications. Although the existing offline learning schemes can avoid this problem by transferring the weight optimization process into cloud, it is difficult to adapt to unseen tasks and uncertain environments. Here, we propose a bi-level meta-learning scheme that can alleviate the non-ideal update problem, and achieve fast adaptation and high accuracy, named Rapid One-step Adaption (ROA). By introducing a special regularization constraint and a dynamic learning rate strategy for <i>in-situ</i> learning, the ROA method effectively combines offline pre-training and online rapid one-step adaption. Furthermore, we implemented it on memristor-based neural networks to solve few-shot learning tasks, proving its superiority over the pure offline and online schemes under noisy conditions. This method can solve <i>in-situ</i> learning in non-ideal memristor networks, providing potential applications of on-chip neuromorphic learning and edge computing."
pub.1151335859,[Retracted] Legal Guarantee of Smart City Pilot and Green and Low‐Carbon Development,"Green and smart cities are based on clean energy and rely on information technology. They are the guarantee for the realization of efficient and intelligent urban development and green ecological transformation, the basis for sustainable social and economic development, and the inevitable trend of urban development. Therefore, the evaluation of the development level of green and smart cities is of great significance to the development of Chinese cities. This paper has aimed to study the issue of smart city pilots and legal guarantees for green and low-carbon development and introduced the concept of smart city line management, as well as the related theory of entropy weight method, cloud model, and support vector machine algorithm. Based on the sustainable development index system, this paper has combined the low-carbon concept to construct the low-carbon city evaluation index system and carried out an empirical analysis. The sustainable development index system, the research results of low-carbon city, and the current situation and characteristics of low-carbon city construction are studied and analyzed. On this premise, a low-carbon city assessment framework in view of reasonable improvement is built, including low-carbon economy, low-carbon society, low-carbon climate, and low-carbon component. The experimental results of this paper show that the low-carbon environment subsystem has the best coordinated development among the four subsystems, and the current state is the best. By 2021, the coordination degree value has reached 0.6656, which is in a relatively coordinated state."
pub.1005988539,Impact constraints on the environment for chemical evolution and the continuity of life,"The Moon and the Earth were bombarded heavily by planetesimals and asteroids that were capable of interfering with chemical evolution and the origin of life. In this paper, we explore the frequency of giant terrestrial impacts able to stop prebiotic chemistry in the probable regions of chemical evolution. The limited time available between impacts disruptive to prebiotic chemistry at the time of the oldest evidence of life suggests the need for a rapid process for chemical evolution of life. The classical hypothesis for the origin of life through the slow accumulation of prebiotic reactants in the primordial soup in the entire ocean may not be consistent with constraints imposed by the impact history of Earth. On the other hand, rapid chemical evolution in cloud systems and lakes or other shallow evaporating water bodies would have been possible because reactants could have been concentrated and polymerized rapidly in this environment. Thus, life probably could have originated near the surface between frequent surface sterilizing impacts. There may not have been continuity of life depending on sunlight because there is evidence that life, existing as early as 3.8 Gyr ago, may have been destroyed by giant impacts. The first such organisms on Earth where probably not the ancestors of present life."
pub.1062217156,MO‐C‐BRCD‐03: The Role of Informatics in Medical Physics and Vice Versa,"Like Medical Physics, Imaging Informatics encompasses concepts touching every aspect of the imaging chain from image creation, acquisition, management and archival, to image processing, analysis, display and interpretation. The two disciplines are in fact quite complementary, with similar goals to improve the quality of care provided to patients using an evidence-based approach, to assure safety in the clinical and research environments, to facilitate efficiency in the workplace, and to accelerate knowledge discovery. Use-cases describing several areas of informatics activity will be given to illustrate current limitations that would benefit from medical physicist participation, and conversely areas in which informaticists may contribute to the solution. Topics to be discussed include radiation dose monitoring, process management and quality control, display technologies, business analytics techniques, and quantitative imaging. Quantitative imaging is increasingly becoming an essential part of biomedicalresearch as well as being incorporated into clinical diagnostic activities. Referring clinicians are asking for more objective information to be gleaned from the imaging tests that they order so that they may make the best clinical management decisions for their patients. Medical Physicists may be called upon to identify existing issues as well as develop, validate and implement new approaches and technologies to help move the field further toward quantitative imaging methods for the future. Biomedical imaging informatics tools and techniques such as standards, integration, data mining, cloud computing and new systems architectures, ontologies and lexicons, data visualization and navigation tools, and business analytics applications can be used to overcome some of the existing limitations.
LEARNING OBJECTIVES: 1. Describe what is meant by Medical Imaging Informatics and understand why the medical physicist should care. 2. Identify existing limitations in information technologies with respect to Medical Physics, and conversely see how Informatics may assist the medical physicist in filling some of the current gaps in their activities. 3. Understand general informatics concepts and areas of investigation including imaging and workflow standards, systems integration, computing architectures, ontologies, data mining and business analytics, data visualization and human-computer interface tools, and the importance of quantitative imaging for the future of Medical Physics and Imaging Informatics. 4. Become familiar with on-going efforts to address current challenges facing future research into and clinical implementation of quantitative imaging applications."
pub.1154411016,mHealth App to Facilitate Remote Care for Patients With COVID-19: Rapid Development of the DrCovid+ App,"BACKGROUND: The 2019 novel COVID-19 has severely burdened the health care system through its rapid transmission. Mobile health (mHealth) is a viable solution to facilitate remote monitoring and continuity of care for patients with COVID-19 in a home environment. However, the conceptualization and development of mHealth apps are often time and labor-intensive and are laden with concerns relating to data security and privacy. Implementing mHealth apps is also a challenging feat as language-related barriers limit adoption, whereas its perceived lack of benefits affects sustained use. The rapid development of an mHealth app that is cost-effective, secure, and user-friendly will be a timely enabler.
OBJECTIVE: This project aimed to develop an mHealth app, DrCovid+, to facilitate remote monitoring and continuity of care for patients with COVID-19 by using the rapid development approach. It also aimed to address the challenges of mHealth app adoption and sustained use.
METHODS: The Rapid Application Development approach was adopted. Stakeholders including decision makers, physicians, nurses, health care administrators, and research engineers were engaged. The process began with requirements gathering to define and finalize the project scope, followed by an iterative process of developing a working prototype, conducting User Acceptance Tests, and improving the prototype before implementation. Co-designing principles were applied to ensure equal collaborative efforts and collective agreement among stakeholders.
RESULTS: DrCovid+ was developed on Telegram Messenger and hosted on a cloud server. It features a secure patient enrollment and data interface, a multilingual communication channel, and both automatic and personalized push messaging. A back-end dashboard was also developed to collect patients' vital signs for remote monitoring and continuity of care. To date, 400 patients have been enrolled into the system, amounting to 2822 hospital bed-days saved.
CONCLUSIONS: The rapid development and implementation of DrCovid+ allowed for timely clinical care management for patients with COVID-19. It facilitated early patient hospital discharge and continuity of care while addressing issues relating to data security and labor-, time-, and cost-effectiveness. The use case for DrCovid+ may be extended to other medical conditions to advance patient care and empowerment within the community, thereby meeting existing and rising population health challenges."
pub.1142591753,A cloud-based toolbox for the versatile environmental annotation of biodiversity data,"A vast range of research applications in biodiversity sciences requires integrating primary species, genetic, or ecosystem data with other environmental data. This integration requires a consideration of the spatial and temporal scale appropriate for the data and processes in question. But a versatile and scale flexible environmental annotation of biodiversity data remains constrained by technical hurdles. Existing tools have streamlined the intersection of occurrence records with gridded environmental data but have remained limited in their ability to address a range of spatial and temporal grains, especially for large datasets. We present the Spatiotemporal Observation Annotation Tool (STOAT), a cloud-based toolbox for flexible biodiversity-environment annotations. STOAT is optimized for large biodiversity datasets and allows user-specified spatial and temporal resolution and buffering in support of environmental characterizations that account for the uncertainty and scale of data and of relevant processes. The tool offers these services for a growing set of near global, remotely sensed, or modeled environmental data, including Landsat, MODIS, EarthEnv, and CHELSA. STOAT includes a user-friendly, web-based dashboard that provides tools for annotation task management and result visualization, linked to Map of Life, and a dedicated R package (rstoat) for programmatic access. We demonstrate STOAT functionality with several examples that illustrate phenological variation and spatial and temporal scale dependence of environmental characteristics of birds at a continental scale. We expect STOAT to facilitate broader exploration and assessment of the scale dependence of observations and processes in ecology."
pub.1150289552,"Fault Tolerance Structures in Wireless Sensor Networks (WSNs): Survey, Classification, and Future Directions","The Industrial Revolution 4.0 (IR 4.0) has drastically impacted how the world operates. The Internet of Things (IoT), encompassed significantly by the Wireless Sensor Networks (WSNs), is an important subsection component of the IR 4.0. WSNs are a good demonstration of an ambient intelligence vision, in which the environment becomes intelligent and aware of its surroundings. WSN has unique features which create its own distinct network attributes and is deployed widely for critical real-time applications that require stringent prerequisites when dealing with faults to ensure the avoidance and tolerance management of catastrophic outcomes. Thus, the respective underlying Fault Tolerance (FT) structure is a critical requirement that needs to be considered when designing any algorithm in WSNs. Moreover, with the exponential evolution of IoT systems, substantial enhancements of current FT mechanisms will ensure that the system constantly provides high network reliability and integrity. Fault tolerance structures contain three fundamental stages: error detection, error diagnosis, and error recovery. The emergence of analytics and the depth of harnessing it has led to the development of new fault-tolerant structures and strategies based on artificial intelligence and cloud-based. This survey provides an elaborate classification and analysis of fault tolerance structures and their essential components and categorizes errors from several perspectives. Subsequently, an extensive analysis of existing fault tolerance techniques based on eight constraints is presented. Many prior studies have provided classifications for fault tolerance systems. However, this research has enhanced these reviews by proposing an extensively enhanced categorization that depends on the new and additional metrics which include the number of sensor nodes engaged, the overall fault-tolerant approach performance, and the placement of the principal algorithm responsible for eliminating network errors. A new taxonomy of comparison that also extensively reviews previous surveys and state-of-the-art scientific articles based on different factors is discussed and provides the basis for the proposed open issues."
pub.1109766275,Research of the three-dimensional tracking and registration method based on multiobjective constraints in an AR system.,"To match the virtual image and actual environment in an augmented reality (AR) system, it is necessary to complete the task of three-dimensional (3D) tracking registration. This paper proposes a new method for 3D tracking registration. Previous methods extract feature points in images to realize tracking registration. In this paper, the objects are extracted from the deep convolution neural network in the scene, and the camera pose is estimated by establishing the constraint relation of the objects. Then, 3D tracking and registration of the virtual object are realized. We design an improved single-shot multibox detector semantic segmentation network to identify and segment the scene and extract the pixel classification results of the objects in the scene. The effect of classification with this method is better. The depth of the extracted object is estimated based on the data from the left and right cameras, and the 2D image is converted into a 3D point cloud. A camera pose estimation method, combined with multiobjective information, is proposed. The camera transformation matrix is directly estimated by establishing a mathematical model. This method avoids the effect on the accuracy of the camera pose estimation when the feature points are not sufficient. Moreover, by assigning different weights to the point clouds of different objects, errors caused by the model can be reduced. The experimental results showed that the 3D registration method proposed in this paper is less than 2.5 pixels in the application scene of an augmented reality head-up display. This method had a better effect compared with that of existing methods and also improved driving safety."
pub.1122506486,Chemical Imaging of Atmospheric Particles,"Airborne particles are very dynamic and highly reactive components of the Earth's atmosphere. Their high surface area and water content provide a unique reaction environment for multiphase chemistry that continually modifies particle composition and properties that consequently impact air quality as well as concentrations of gas-phase species. By absorbing and scattering solar and terrestrial radiation, particles directly influence the planet's radiative balance. Their indirect effects include modifying the nucleation, lifetime, and physical properties of clouds. Due to the sensitivity of the atmospheric environment to all these variables, fundamental studies of chemical transformations of atmospheric particles, their sources, continuously evolving composition, and physical properties are of highest research priority. Accurate descriptions of particles and their effects in the atmosphere require comprehensive information not only on the particle-type populations and their size distributions and concentrations, but also on the diversity and the spatial heterogeneity of chemical components within individual particles. Developments and applications of modern chemical imaging approaches for off-line characterization of atmospheric particles have been at the forefront of modern experimental studies and have resulted in a transformative impact in atmospheric chemistry and physics. This Account presents a synopsis of recent advances in chemical imaging of atmospheric particles collected on substrates during field and laboratory experiments. The unique advantage of chemical imaging methods is that they simultaneously provide two analytical measurements: imaging of particles to assess variability in their individual sizes and morphology, as well as particle-specific speciation of their composition and spatial heterogeneity of different chemical components within individual particles. We also highlight analytical chemistry approaches that enable chemical imaging of particles with different levels of elemental and molecular specificity, including applications of multimodal methodologies where the same or similar groups of particles are probed by two or more complementary techniques. These approaches provide unique experimental insights on the nature and sources of particles, understanding their physical properties, atmospheric reactivity, and transformations. Chemical imaging data provide unique experimental input for atmospheric models that simulate aging and changes in particle-type populations, internal composition, and their associated optical and cloud forming properties. We highlight applications of chemical imaging in selected recent studies, discuss their existing limitations, and forecast future research directions for this area."
pub.1065129700,Modeling of intensity-modulated continuous-wave laser absorption spectrometer systems for atmospheric CO(2) column measurements.,"The focus of this study is to model and validate the performance of intensity-modulated continuous-wave (IM-CW) CO(2) laser absorption spectrometer (LAS) systems and their CO(2) column measurements from airborne and satellite platforms. The model accounts for all fundamental physics of the instruments and their related CO(2) measurement environments, and the modeling results are presented statistically from simulation ensembles that include noise sources and uncertainties related to the LAS instruments and the measurement environments. The characteristics of simulated LAS systems are based on existing technologies and their implementation in existing systems. The modeled instruments are specifically assumed to be IM-CW LAS systems such as the Exelis' airborne multifunctional fiber laser lidar (MFLL) operating in the 1.57 μm CO(2) absorption band. Atmospheric effects due to variations in CO(2), solar radiation, and thin clouds, are also included in the model. Model results are shown to agree well with LAS atmospheric CO(2) measurement performance. For example, the relative bias errors of both MFLL simulated and measured CO(2) differential optical depths were found to agree to within a few tenths of a percent when compared to the in situ observations from the flight of 3 August 2011 over Railroad Valley (RRV), Nevada, during the summer 2011 flight campaign. In addition, the horizontal variations in the model CO(2) differential optical depths were also found to be consistent with those from MFLL measurements. In general, the modeled and measured signal-to-noise ratios (SNRs) of the CO(2) column differential optical depths (τd) agreed to within about 30%. Model simulations of a spaceborne IM-CW LAS system in a 390 km dawn/dusk orbit for CO(2) column measurements showed that with a total of 42 W of transmitted power for one offline and two different sideline channels (placed at different locations on the side of the CO(2) absorption line), the accuracy of the τd measurements for surfaces similar to the playa of RRV, Nevada, will be better than 0.1% for 10 s averages. For other types of surfaces such as low-reflectivity snow and ice surfaces, the precision and bias errors will be within 0.23% and 0.1%, respectively. Including thin clouds with optical depths up to 1, the SNR of the τd measurements with 0.1 s integration period for surfaces similar to the playa of RRV, Nevada, will be greater than 94 and 65 for sideline positions placed +3 and +10  pm, respectively, from the CO(2) line center at 1571.112 nm. The CO(2) column bias errors introduced by the thin clouds are ≤0.1% for cloud optical depth ≤0.4, but they could reach ∼0.5% for more optically thick clouds with optical depths up to 1. When the cloud and surface altitudes and scattering amplitudes are obtained from matched filter analysis, the cloud bias errors can be further reduced. These results indicate that the IM-CW LAS instrument approach when implemented in a dawn/dusk orbit can make accura"
pub.1136576418,New insights into ice multiplication using remote-sensing observations of slightly supercooled mixed-phase clouds in the Arctic,"Secondary ice production (SIP) can significantly enhance ice particle number concentrations in mixed-phase clouds, resulting in a substantial impact on ice mass flux and evolution of cold cloud systems. SIP is especially important at temperatures warmer than -[Formula: see text]C, for which primary ice nucleation lacks a significant number of efficient ice nucleating particles. However, determining the climatological significance of SIP has proved difficult using existing observational methods. Here we quantify the long-term occurrence of secondary ice events and their multiplication factors in slightly supercooled clouds using a multisensor, remote-sensing technique applied to 6 y of ground-based radar measurements in the Arctic. Further, we assess the potential contribution of the underlying mechanisms of rime splintering and freezing fragmentation. Our results show that the occurrence frequency of secondary ice events averages to <10% over the entire period. Although infrequent, the events can have a significant impact in a local region when they do occur, with up to a 1,000-fold enhancement in ice number concentration. We show that freezing fragmentation, which appears to be enhanced by updrafts, is more efficient for SIP than the better-known rime-splintering process. Our field observations are consistent with laboratory findings while shedding light on the phenomenon and its contributing factors in a natural environment. This study provides critical insights needed to advance parameterization of SIP in numerical simulations and to design future laboratory experiments."
pub.1174903606,Process evaluation of an mHealth-based school education program to reduce salt intake scaling up in China (EduSaltS): a mixed methods study using the RE-AIM framework,"BackgroundAn mHealth-based school health education platform (EduSaltS) was promoted in real-world China to reduce salt intake among children and their families. This progress evaluation explores its implementation process and influencing factors using mixed methods.MethodsThe mixed-methods process evaluation employed the RE-AIM framework. Quantitative data were collected from a management website monitoring 54,435 third-grade students across two cities. Questionnaire surveys (n = 27,542) assessed pre- and post-education effectiveness. Mixed-effects models were used to control cluster effects. Qualitative interviews (23 individuals and 8 focus groups) identified program performance, facilitators, and barriers. Findings were triangulated using the RE-AIM framework.ResultsThe program achieved 100% participation among all the third-grade classes of the 208 invited primary schools, with a 97.7% registration rate among all the 54,435 families, indicating high ""Reach."" Qualitative interviews revealed positive engagement from children and parents through the ""small hands leading big hands"" strategy. The high completion rate of 84.9% for each health cloud lesson and the significant improvement in salt reduction knowledge and behaviors scores from 75.0 (95%CI: 74.7–75.3) to 80.9 (95%CI: 80.6–81.2) out of 100 demonstrated the ""Effect"" of EduSaltS. The program's ""Adoption"" and ""Implementation"" were supported by attractive materials, reduced workload via auto-delivered lessons/activities and performance evaluation, and high fidelity to recommended activities, with medians 3.0 (IQR: 2.0–8.0)/class and 9.0 (IQR: 5.0–14.0)/school. Stable course completion rates (79.4%-93.4%) over one year indicated promising ""Maintenance."" Apart from the facilitating features praised by the interviewees, government support was the basis for the scaling up of EduSaltS. Barriers included the lack of smartphone skills among some parents and competing priorities for schools. Unhealthy off-campus environments, such as excessive use of salt in pre-packaged and restaurant foods, also hindered salt reduction efforts. The program's scalability was evident through its integration into existing health education, engagement of local governments and adaptation across various mobile devices.ConclusionsThe mHealth-based school health education program is scalable and effective for public salt reduction in China. Identified barriers and facilitators can inform future health program scale-ups. The program's successful implementation demonstrates its potential for broader application in public health initiatives aimed at reducing dietary salt intake."
pub.1059415022,Isomorphic semantic mapping of variant call format (VCF2RDF),"Summary: The move of computational genomics workflows to Cloud Computing platforms is associated with a new level of integration and interoperability that challenges existing data representation formats. The Variant Calling Format (VCF) is in a particularly sensitive position in that regard, with both clinical and consumer-facing analysis tools relying on this self-contained description of genomic variation in Next Generation Sequencing (NGS) results. In this report we identify an isomorphic map between VCF and the reference Resource Description Framework. RDF is advanced by the World Wide Web Consortium (W3C) to enable representations of linked data that are both distributed and discoverable. The resulting ability to decompose VCF reports of genomic variation without loss of context addresses the need to modularize and govern NGS pipelines for Precision Medicine. Specifically, it provides the flexibility (i.e. the indexing) needed to support the wide variety of clinical scenarios and patient-facing governance where only part of the VCF data is fitting.
Availability and Implementation: Software libraries with a claim to be both domain-facing and consumer-facing have to pass the test of portability across the variety of devices that those consumers in fact adopt. That is, ideally the implementation should itself take place within the space defined by web technologies. Consequently, the isomorphic mapping function was implemented in JavaScript, and was tested in a variety of environments and devices, client and server side alike. These range from web browsers in mobile phones to the most popular micro service platform, NodeJS. The code is publicly available at https://github.com/ibl/VCFr , with a live deployment at: http://ibl.github.io/VCFr/ .
Contact: jonas.almeida@stonybrookmedicine.edu."
pub.1067225945,The Hayes principles: learning from the national pilot of information technology and core generalisable theory in informatics.,"BACKGROUND: There has been much criticism of the NHS national programme for information technology (IT); it has been an expensive programme and some elements appear to have achieved little. The Hayes report was written as an independent review of health and social care IT in England.
OBJECTIVE: To identify key principles for health IT implementation which may have relevance beyond the critique of NHS IT.
OUTCOME: We elicit ten principles from the Hayes report, which if followed may result in more effective IT implementation in health care. They divide into patient-centred, subsidiarity and strategic principles. The patient-centred principles are: 1) the patient must be at the centre of all information systems; 2) the provision of patient-level operational data should form the foundation - avoid the dataset mentality; 3) store health data as close to the patient as possible; 4) enable the patient to take a more active role with their health data within a trusted doctor-patient relationship. The subsidiarity principles set out to balance the local and health-system-wide needs: 5) standardise centrally - patients must be able to benefit from interoperability; 6) provide a standard procurement package and an approved process that ensures safety standards and provision of interoperable systems; 7) authorise a range of local suppliers so that health providers can select the system best meeting local needs; 8) allow local migration from legacy systems, as and when improved functionality for patients is available. And finally the strategic principles: 9) evaluate health IT systems in terms of measureable benefits to patients; 10) strategic planning of systems should reflect strategic goals for the health of patients/the population.
CONCLUSIONS: Had the Hayes principles been embedded within our approach to health IT, and in particular to medical record implementation, we might have avoided many of the costly mistakes with the UK national programme. However, these principles need application within the modern IT environment. Closeness to the patient must not be interpreted as physical but instead as a virtual patient-centred space; data will be secure within the cloud and we should dump the vault and infrastructure mentality. Health IT should be developed as an adaptive ecosystem."
pub.1111308956,Integration of a Mobile Node into a Hybrid Wireless Sensor Network for Urban Environments †,"Robots, or in general, intelligent vehicles, require large amounts of data to adapt their behavior to the environment and achieve their goals. When their missions take place in large areas, using additional information to that gathered by the onboard sensors frequently offers a more efficient solution of the problem. The emergence of Cyber-Physical Systems and Cloud computing allows this approach, but integration of sensory information, and its effective availability for the robots or vehicles is challenging. This paper addresses the development and implementation of a modular mobile node of a Wireless Sensor Network (WSN), designed to be mounted onboard vehicles, and capable of using different sensors according to mission needs. The mobile node is integrated with an existing static network, transforming it into a Hybrid Wireless Sensor Network (H-WSN), and adding flexibility and range to it. The integration is achieved without the need for multi-hop routing. A database holds the data acquired by both mobile and static nodes, allowing access in real-time to the gathered information. A Human⁻Machine Interface (HMI) presents this information to users. Finally, the system is tested in real urban scenarios in a use-case of measurement of gas levels."
pub.1143467420,"Lake ecosystem health assessment using a novel hybrid decision-making framework in the Nam Co, Qinghai-Tibet Plateau","Lake health assessment (LHA), a powerful tool for lake ecological protection, provides the foundation for sustainable water environment management. However, existing methods have not yet considered the effects of fuzziness and randomness on LHA. In addition, most of the current studies on LHA focus on the plain areas, lack of quantitative studies in mountain areas, such as the Qinghai-Tibet Plateau. The Pythagorean fuzzy cloud (PFC) integration algorithm drawing on the advantages of Pythagorean fuzzy sets (PFS) and cloud model was proposed. A novel hybrid decision-making framework combining PFC integration algorithm and TOPSIS model was developed to determine the lake health levels with fuzziness and randomness. An indicator system incorporating ecosystem integrity (physical habitat, water quantity and quality, aquatic life) and non-ecological performance (social services) was established. To comprehensively investigate the lake health level in the Qinghai-Tibet Plateau, the Nam Co was selected as study area. Our results confirm that the developed framework in this study can overcome the shortcomings of existing methods and provide a more effective approach for LHA with fuzziness and randomness. In Nam Co, the non-ecological performance was significantly better than the ecosystem integrity. Health levels exhibited a remarkable spatial variation influenced by tourism and grazing, with decreasing health status from the northwestern to southeastern Nam Co. Approximately 85% of the sampling sites were at excellent or healthy levels, 15% were subhealthy, and no sampling sites were unhealthy and sick. Our results highlight that tourism has affected health levels at Nam Co, and effective measures are needed to minimize the impact in ecological fragile areas."
pub.1141737181,"HEAD Metamodel: Hierarchical, Extensible, Advanced, and Dynamic Access Control Metamodel for Dynamic and Heterogeneous Structures","The substantial advancements in information technologies have brought unprecedented concepts and challenges to provide solutions and integrate advanced and self-ruling systems in critical and heterogeneous structures. The new generation of networking environments (e.g., the Internet of Things (IoT), cloud computing, etc.) are dynamic and ever-evolving environments. They are composed of various private and public networks, where all resources are distributed and accessed from everywhere. Protecting resources by controlling access to them is a complicated task, especially with the presence of cybercriminals and cyberattacks. What makes this reality also challenging is the diversity and the heterogeneity of access control (AC) models, which are implemented and integrated with a countless number of information systems. The evolution of ubiquitous computing, especially the concept of Industry 4.0 and IoT applications, imposes the need to enhance AC methods since the traditional methods are not able to answer the increasing demand for privacy and security standards. To address this issue, we propose a Hierarchical, Extensible, Advanced, and Dynamic (HEAD) AC metamodel for dynamic and heterogeneous structures that is able to encompass the heterogeneity of the existing AC models. Various AC models can be derived, and different static and dynamic AC policies can be generated using its components. We use Eclipse (xtext) to define the grammar of our AC metamodel. We illustrate our approach with several successful instantiations for various models and hybrid models. Additionally, we provide some examples to show how some of the derived models can be implemented to generate AC policies."
pub.1146624359,It takes two to tango: technological and non-technological factors of Industry 4.0 implementation in manufacturing firms,"It is commonly held that new technologies improve the productivity of organizations. However, technology acceptance does not happen instantaneously—it depends on complementary, non-technological changes in organizational behaviour. The lack of the latter may present a barrier to technology implementation and could even result in adverse effects on productivity. This is often the case in emerging economies that are deeply embedded in mature technological frameworks and with limited readiness for the adoption of new technologies. Using data from organizations in the manufacturing sector of an emerging European economy, we empirically tested the effects of technological and non-technological factors of the organizational implementation of Industry 4.0 principles on productivity. The results of the investigation, based on structural equation modelling, reveal the positive effects of technology-related Industry 4.0 factors—such as the Internet of Things, cyber-physical systems, and cloud computing—on productivity. The findings also reveal that these effects are enhanced by the mediating effect of non-technological changes to business models, organizational structures and cultures, strategies, and shifts in focus regarding customers, products, and services. This study adds to the existing body of knowledge in this area by revealing the relevance of the individual channels through which transitions towards Industry 4.0 can be enhanced, using traditional manufacturing environments often neglected in studies within this research field."
pub.1173304042,Enhancing security in smart healthcare systems: Using intelligent edge computing with a novel Salp Swarm Optimization and radial basis neural network algorithm,"A smart healthcare system (SHS) is a health service system that employs advanced technologies such as wearable devices, the Internet of Things (IoT), and mobile internet to dynamically access information and connect people and institutions related to healthcare, thereby actively managing and responding to medical ecosystem needs. Edge computing (EC) plays a significant role in SHS as it enables real-time data processing and analysis at the data source, which reduces latency and improves medical intervention speed. However, the integration of patient information, including electronic health records (EHRs), into the SHS framework induces security and privacy concerns. To address these issues, an intelligent EC framework was proposed in this study. The objective of this study is to accurately identify security threats and ensure secure data transmission in the SHS environment. The proposed EC framework leverages the effectiveness of Salp Swarm Optimization and Radial Basis Functional Neural Network (SS-RBFN) for enhancing security and data privacy. The proposed methodology commences with the collection of healthcare information, which is then pre-processed to ensure the consistency and quality of the database for further analysis. Subsequently, the SS-RBFN algorithm was trained using the pre-processed database to distinguish between normal and malicious data streams accurately, offering continuous monitoring in the SHS environment. Additionally, a Rivest-Shamir-Adelman (RSA) approach was applied to safeguard data against security threats during transmission to cloud storage. The proposed model was trained and validated using the IoT-based healthcare database available at Kaggle, and the experimental results demonstrated that it achieved 99.87 % accuracy, 99.76 % precision, 99.49 % f-measure, 98.99 % recall, 97.37 % throughput, and 1.2s latency. Furthermore, the results achieved by the proposed model were compared with the existing models to validate its effectiveness in enhancing security."
pub.1136165255,Bioinformatics tools developed to support BioCompute Objects,"Developments in high-throughput sequencing (HTS) result in an exponential increase in the amount of data generated by sequencing experiments, an increase in the complexity of bioinformatics analysis reporting and an increase in the types of data generated. These increases in volume, diversity and complexity of the data generated and their analysis expose the necessity of a structured and standardized reporting template. BioCompute Objects (BCOs) provide the requisite support for communication of HTS data analysis that includes support for workflow, as well as data, curation, accessibility and reproducibility of communication. BCOs standardize how researchers report provenance and the established verification and validation protocols used in workflows while also being robust enough to convey content integration or curation in knowledge bases. BCOs that encapsulate tools, platforms, datasets and workflows are FAIR (findable, accessible, interoperable and reusable) compliant. Providing operational workflow and data information facilitates interoperability between platforms and incorporation of future dataset within an HTS analysis for use within industrial, academic and regulatory settings. Cloud-based platforms, including High-performance Integrated Virtual Environment (HIVE), Cancer Genomics Cloud (CGC) and Galaxy, support BCO generation for users. Given the 100K+ userbase between these platforms, BioCompute can be leveraged for workflow documentation. In this paper, we report the availability of platform-dependent and platform-independent BCO tools: HIVE BCO App, CGC BCO App, Galaxy BCO API Extension and BCO Portal. Community engagement was utilized to evaluate tool efficacy. We demonstrate that these tools further advance BCO creation from text editing approaches used in earlier releases of the standard. Moreover, we demonstrate that integrating BCO generation within existing analysis platforms greatly streamlines BCO creation while capturing granular workflow details. We also demonstrate that the BCO tools described in the paper provide an approach to solve the long-standing challenge of standardizing workflow descriptions that are both human and machine readable while accommodating manual and automated curation with evidence tagging. Database URL:  https://www.biocomputeobject.org/resources."
pub.1140375592,Three-dimensional simulation of clouds of multi-disperse evaporating saliva droplets in a train cabin,"In line with recent ongoing efforts to collect crucial information about the mechanisms of virus diffusion and put them in relation to the effective complexity of the several natural or artificial environments where human beings leave and operate, the present study deals with the dispersion of evaporating saliva droplets in the cabin of an interregional train. A relevant physical model is constructed taking into account the state of the art in terms of existing paradigms and their ability to represent some fundamental aspects related to the evolution in time of a cloud of multi-disperse droplets. Conveniently, such a theoretical framework is turned into a computational one that relies on low Mach-number asymptotics and can therefore take advantage of the typical benefits (relatively low computational cost) associated with pressure-based methods. Numerical simulations are used to predict the flow established in the cabin as a result of the ventilation systems and related settings dictated by considerations on passenger comfort. The solution of two-way coupled Lagrangian evolution equations is used to capture the associated dynamics of the dispersed phase and predict its transport in conjunction with the peculiar topology of the considered flow and morphology of solid surfaces, which bound it (including the human beings). Typical physiological processes such as talking or coughing are considered. An analysis on the impact of the multiplicity of droplet sources is also conducted, thereby providing some indications in terms of potential risks for the cabin occupants."
pub.1153654347,Use of Lt Systems in Large Class Lab Delivery,"The attainment of laboratory-based skills is essential for the development of all students who study the Life Sciences. When the COVID-19 pandemic hit and Universities worldwide closed campuses, there was great uncertainty around how long people would have to work from home, leading to a real risk that students could miss out on obtaining laboratory practice for one or perhaps 2 years. It, therefore, became critical that processes were put in place that would allow students the opportunity to gain insight into what the laboratory environment is like, and to also gain experience in data collection, interpretation, and analysis. The adoption of Lt systems (a cloud-based learning platform for the Life Sciences) to create laboratory-based teaching allowed for these issues to be addressed. Detailed planning, teamwork and production of online lab sessions allowed for the creation of bespoke lessons that replicated as best as possible the ‘face-to-face’ experience. This was achieved by taking aspects of pre-existing labs, filming new material and utilising interactive data analysis tools in order to create online Lt labs that gave students a feel of what it would be like to be in a laboratory on campus. Lt systems also doubled as a revision tool to enhance student learning and ultimately allowed intended learning outcomes to be successfully met. The labs were well received by students and the online material meant that they could access content at a time convenient to them. This ‘online anytime’ possibility was crucial for such a large class (n = 378) who were working from home in many different countries worldwide during a pandemic. Going forward, the online lessons built during the pandemic can be integrated with future face-to-face sessions to create a more enhanced learning experience for the student."
pub.1091342294,The DIY Digital Medical Centre,"Healthcare systems worldwide are confronted with major economic, organizational and logistical challenges. Historic evolution of health care has led to significant healthcare sector fragmentation, resulting in systemic inefficiencies and suboptimal resource exploitation. To attain a sustainable healthcare model, fundamental, system-wide improvements that effectively network, and ensure fulfilment of potential synergies between sectors, and include and facilitate coherent strategic planning and organisation of healthcare infrastructure are needed. Critically, they must be specifically designed to sustainably achieve peak performance within the current policy environment for cost-control, and efficiency and quality improvement for service delivery. We propose creation of a new healthcare cluster, to be embedded in existing healthcare systems. It consists of (i) local 24/7 walk-in virtually autonomous do-it-yourself Digital Medical Centres performing routine diagnosis, monitoring, prevention, treatment and standardized documentation and health outcome assessment/reporting, which are online interfaced with (ii) regional 24/7 eClinician Centres providing on-demand clinical supervision/assistance to Digital Medical Centre patients. Both of these are, in turn, online interfaced with (iii) the National Clinical Informatics Centre, which houses the national patient data centre (cloud) and data analysis units that conduct patient- and population-level, personalized and predictive(-medicine) intervention optimization analyses. The National Clinical Informatics Centre also interfaces with biomedical research and prioritizes and accelerates the translation of new discoveries into clinical practice. The associated Health Policy Innovation and Evaluation Centre rapidly integrates new findings with health policy/regulatory discussions. This new cluster would synergistically link all health system components in a circular format, enable not only access by all arms of the health service to latest patient data, but also automatic algorithm analysis and prediction of clinical development of individual patients, reduce bureaucratic burden on medical professionals by enabling a greater level of focus of their expertise on non-routine medical tasks, lead to automatic translation of aggregate patient data/new knowledge into medical practice, and orient future evolution of health systems towards greater cohesion/integration and hence efficiency. A central plank of the proposed concept is increased emphasis on reduction of disease incidence and severity, to diminish both patient suffering and treatment costs. This will be achieved at the individual and population levels, through (i) significantly improved access to medical services, (ii) stronger focus on primary and secondary prevention and early treatment measures, and disease susceptibility prediction via personalized medicine, involving inter alia genome analysis at birth and periodic analysis of microbiomes and bioma"
pub.1079239294,COHeRE: Cross-Ontology Hierarchical Relation Examination for Ontology Quality Assurance.,"Biomedical ontologies play a vital role in healthcare information management, data integration, and decision support. Ontology quality assurance (OQA) is an indispensable part of the ontology engineering cycle. Most existing OQA methods are based on the knowledge provided within the targeted ontology. This paper proposes a novel cross-ontology analysis method, Cross-Ontology Hierarchical Relation Examination (COHeRE), to detect inconsistencies and possible errors in hierarchical relations across multiple ontologies. COHeRE leverages the Unified Medical Language System (UMLS) knowledge source and the MapReduce cloud computing technique for systematic, large-scale ontology quality assurance work. COHeRE consists of three main steps with the UMLS concepts and relations as the input. First, the relations claimed in source vocabularies are filtered and aggregated for each pair of concepts. Second, inconsistent relations are detected if a concept pair is related by different types of relations in different source vocabularies. Finally, the uncovered inconsistent relations are voted according to their number of occurrences across different source vocabularies. The voting result together with the inconsistent relations serve as the output of COHeRE for possible ontological change. The highest votes provide initial suggestion on how such inconsistencies might be fixed. In UMLS, 138,987 concept pairs were found to have inconsistent relationships across multiple source vocabularies. 40 inconsistent concept pairs involving hierarchical relationships were randomly selected and manually reviewed by a human expert. 95.8% of the inconsistent relations involved in these concept pairs indeed exist in their source vocabularies rather than being introduced by mistake in the UMLS integration process. 73.7% of the concept pairs with suggested relationship were agreed by the human expert. The effectiveness of COHeRE indicates that UMLS provides a promising environment to enhance qualities of biomedical ontologies by performing cross-ontology examination. "
pub.1132077807,A Framework for Off-Line Operation of Smart and Traditional Devices of IoT Services,"Recently, with the continuous evolution of information technology, various products such as Building Information, Internet of Things (IoT), Big Data, Cloud Computing and Machine Learning have been developed and have created a lifestyle change. A smart Internet of Things (IoT) system is formed by combining the communication capabilities of the internet with control, monitoring and identification services to integrate people, things and objects. However, in some IoT environments that have a weak signal, such as remote areas, warehouses or basements, the network may become unstable, meaning that the IoT system is unable to provide efficient services. This paper therefore presents a framework that ensures the reliability of IoT system services so that even if the IoT system cannot connect to the network, the system can provide the services offline. To avoid increasing the installation cost or replacing existing traditional devices with modern smart devices, this framework can also be used to control traditional devices. The system operation is convenient because users can operate all their smart and traditional devices under the IoT system through voice commands and/or a handheld microcontroller, thus reducing the manual operation of the user. The framework proposed in this paper can be applied to various smart scenarios, including smart warehouses, smart restaurants, smart homes, smart farms and smart factories, to improve people's quality of life and convenience, and create a humane and comfortable smart living environment."
pub.1137942532,"Trends, Technologies, and Key Challenges in Smart and Connected Healthcare","Cardio Vascular Diseases (CVD) is the leading cause of death globally and is increasing at an alarming rate, according to the American Heart Association's Heart Attack and Stroke Statistics-2021. This increase has been further exacerbated because of the current coronavirus (COVID-19) pandemic, thereby increasing the pressure on existing healthcare resources. Smart and Connected Health (SCH) is a viable solution for the prevalent healthcare challenges. It can reshape the course of healthcare to be more strategic, preventive, and custom-designed, making it more effective with value-added services. This research endeavors to classify state-of-the-art SCH technologies via a thorough literature review and analysis to comprehensively define SCH features and identify the enabling technology-related challenges in SCH adoption. We also propose an architectural model that captures the technological aspect of the SCH solution, its environment, and its primary involved stakeholders. It serves as a reference model for SCH acceptance and implementation. We reflected the COVID-19 case study illustrating how some countries have tackled the pandemic differently in terms of leveraging the power of different SCH technologies, such as big data, cloud computing, Internet of Things, artificial intelligence, robotics, blockchain, and mobile applications. In combating the pandemic, SCH has been used efficiently at different stages such as disease diagnosis, virus detection, individual monitoring, tracking, controlling, and resource allocation. Furthermore, this review highlights the challenges to SCH acceptance, as well as the potential research directions for better patient-centric healthcare."
pub.1022437771,Migrating to Cloud-Native Architectures Using Microservices: An Experience Report,"Migration to the cloud has been a popular topic in industry and academia in recent years. Despite many benefits that the cloud presents, such as high availability and scalability, most of the on-premise application architectures are not ready to fully exploit the benefits of this environment, and adapting them to this environment is a non-trivial task. Microservices have appeared recently as novel architectural styles that are native to the cloud. These cloud-native architectures can facilitate migrating on-premise architectures to fully benefit from the cloud environments because non-functional attributes, like scalability, are inherent in this style. The existing approaches on cloud migration does not mostly consider cloud-native architectures as their first-class citizens. As a result, the final product may not meet its primary drivers for migration. In this paper, we intend to report our experience and lessons learned in an ongoing project on migrating a monolithic on-premise software architecture to microservices. We concluded that microservices is not a one-fit-all solution as it introduces new complexities to the system, and many factors, such as distribution complexities, should be considered before adopting this style. However, if adopted in a context that needs high flexibility in terms of scalability and availability, it can deliver its promised benefits."
pub.1014529889,Implications of Integration and Interoperability for Enterprise Cloud-Based Applications,"Enterprise’s adoption of cloud-based solutions is often hindered by problems associated with the integration of the cloud environment with on-premise systems. Currently, each cloud provider creates its proprietary application programming interfaces (APIs), which will complicate integration efforts for companies as they struggle to understand and manage these unique application interfaces in an interoperable way. This paper aims to address this challenge by providing recommendations to enterprises. The presented work is based on a quantitative study of 114 companies, which discuss current issues and future trends of integration and interoperability requirements for enterprise cloud application adoption and migration. The outcome of the discussion provides a guideline applicable to support decision makers, software architects and developers when considering to design and develop interoperable applications in order to avoid lock-in and integrate seamlessly into other cloud and on-premise systems."
pub.1061277269,Low-Power Wearable Systems for Continuous Monitoring of Environment and Health for Chronic Respiratory Disease,"We present our efforts toward enabling a wearable sensor system that allows for the correlation of individual environmental exposures with physiologic and subsequent adverse health responses. This system will permit a better understanding of the impact of increased ozone levels and other pollutants on chronic asthma conditions. We discuss the inefficiency of existing commercial off-the-shelf components to achieve continuous monitoring and our system-level and nano-enabled efforts toward improving the wearability and power consumption. Our system consists of a wristband, a chest patch, and a handheld spirometer. We describe our preliminary efforts to achieve a submilliwatt system ultimately powered by the energy harvested from thermal radiation and motion of the body with the primary contributions being an ultralow-power ozone sensor, an volatile organic compounds sensor, spirometer, and the integration of these and other sensors in a multimodal sensing platform. The measured environmental parameters include ambient ozone concentration, temperature, and relative humidity. Our array of sensors also assesses heart rate via photoplethysmography and electrocardiography, respiratory rate via photoplethysmography, skin impedance, three-axis acceleration, wheezing via a microphone, and expiratory airflow. The sensors on the wristband, chest patch, and spirometer consume 0.83, 0.96, and 0.01 mW, respectively. The data from each sensor are continually streamed to a peripheral data aggregation device and are subsequently transferred to a dedicated server for cloud storage. Future work includes reducing the power consumption of the system-on-chip including radio to reduce the entirety of each described system in the submilliwatt range."
pub.1095087232,Towards a Cloud Migration Framework,"Cloud computing paradigm is becoming more and more grown, which provides services model based on virtualization technologies. Despite outdating of legacy system, it is still used considerably after modern technology has come out, either because the organization has already invested in term of time and money in it, or because the legacy system has important data. In order to exploit the benefits of cloud computing, some of the organization wrote or rewrote software specially to run in the cloud. The numbers of companies that are expected to migrate their applications to cloud environment is increasing. Moving to the cloud is still a big challenge for many organizations. Effectively, how to draw a plan about moving a legacy style application to the cloud arise various challenges. The complexity and size of such project represent a big challenge especially for small and medium organizations to take the full advantages of cloud. As any software development project, migration projects should be planed carefully and have a good methodology to guarantee successful execution. The aim of this paper is to propose a methodology to ensure a migration process based on Architecture Driven Modernization (ADM) approach to cloud environment, this methodology focuses on understanding an on-premise application then establishing its Platform-Independent Model (PIM) to transform it into Platform Specific Model (PSM) cloud."
pub.1090276402,Integrating multisensor satellite data merging and image reconstruction in support of machine learning for better water quality management,"Monitoring water quality changes in lakes, reservoirs, estuaries, and coastal waters is critical in response to the needs for sustainable development. This study develops a remote sensing-based multiscale modeling system by integrating multi-sensor satellite data merging and image reconstruction algorithms in support of feature extraction with machine learning leading to automate continuous water quality monitoring in environmentally sensitive regions. This new Earth observation platform, termed ""cross-mission data merging and image reconstruction with machine learning"" (CDMIM), is capable of merging multiple satellite imageries to provide daily water quality monitoring through a series of image processing, enhancement, reconstruction, and data mining/machine learning techniques. Two existing key algorithms, including Spectral Information Adaptation and Synthesis Scheme (SIASS) and SMart Information Reconstruction (SMIR), are highlighted to support feature extraction and content-based mapping. Whereas SIASS can support various data merging efforts to merge images collected from cross-mission satellite sensors, SMIR can overcome data gaps by reconstructing the information of value-missing pixels due to impacts such as cloud obstruction. Practical implementation of CDMIM was assessed by predicting the water quality over seasons in terms of the concentrations of nutrients and chlorophyll-a, as well as water clarity in Lake Nicaragua, providing synergistic efforts to better monitor the aquatic environment and offer insightful lake watershed management strategies."
pub.1111097004,LimeSeg: a coarse-grained lipid membrane simulation for 3D image segmentation,"Background3D segmentation is often a prerequisite for 3D object display and quantitative measurements. Yet existing voxel-based methods do not directly give information on the object surface or topology. As for spatially continuous approaches such as level-set, active contours and meshes, although providing surfaces and concise shape description, they are generally not suitable for multiple object segmentation and/or for objects with an irregular shape, which can hamper their adoption by bioimage analysts.ResultsWe developed LimeSeg, a computationally efficient and spatially continuous 3D segmentation method. LimeSeg is easy-to-use and can process many and/or highly convoluted objects. Based on the concept of SURFace ELements (“Surfels”), LimeSeg resembles a highly coarse-grained simulation of a lipid membrane in which a set of particles, analogous to lipid molecules, are attracted to local image maxima. The particles are self-generating and self-destructing thus providing the ability for the membrane to evolve towards the contour of the objects of interest.The capabilities of LimeSeg: simultaneous segmentation of numerous non overlapping objects, segmentation of highly convoluted objects and robustness for big datasets are demonstrated on experimental use cases (epithelial cells, brain MRI and FIB-SEM dataset of cellular membrane system respectively).ConclusionIn conclusion, we implemented a new and efficient 3D surface reconstruction plugin adapted for various sources of images, which is deployed in the user-friendly and well-known ImageJ environment."
pub.1094500179,Model Architecture for Automatic Translation and Migration of Legacy Applications to Cloud Computing Environments,"On-demand computing, Software-as-a-Service, Platform-as-a-Service, and in general Cloud Computing is currently the main approach by which both academic and commercial domains are delivering systems and content. Nevertheless there still remains a huge segment of legacy systems and application ranging from accounting and management information systems to scientific software based on classic desktop or simple client-server architectures. Although in the past years more and more companies and organizations have invested important budgets in translating legacy apps to online cloud-enabled environment there still remains an important segment of applications that for various reasons (budget related in most cases) have not been translated. This paper proposes an innovative pipeline model architecture for automated translation and migration of legacy application to cloud-enabled environment with a minimal software development costs."
pub.1095106689,Cloud Integration - Strategy to Connect Applications to Cloud,"Cloud Computing is being widely hailed path by enterprises to realize benefits without compromising control. However, enterprises concern is the integration of applications hosted both on premise, Cloud and partner environments. In spite of enterprise's strategic imperatives to meet their business goals by building integration services between these environments, new integration challenges are posed with the advent of areas like Cloud, Social, and Mobile that are augmenting to the existing complexity. The key challenges being Performance and Usability of these services which cut across Legacy, On Premise, Cloud applications and SaaS applications. Enterprises are spending highly on these integrations as these are crucial for their business process to be executed seamlessly. Key Patterns w.r.t Enterprises Integration considering the widely adopted Cloud scenario are: • On-Premise Application to Cloud Application Integration • Cloud Application to On-Premise Application Integration • Cloud to Cloud Application Integration • B2B Integration • Web API Publishing On-Premise Application to Cloud Application Integration Cloud Application to On-Premise Application Integration Cloud to Cloud Application Integration B2B Integration Web API Publishing This paper helps in adoption of an efficient integration strategy to connect applications on cloud with effective cost benefits and lowered total cost of ownership (TCO)."
pub.1118429280,Migrating to Cloud-Native Architectures Using Microservices: An Experience Report,"Migration to the cloud has been a popular topic in industry and academia in
recent years. Despite many benefits that the cloud presents, such as high
availability and scalability, most of the on-premise application architectures
are not ready to fully exploit the benefits of this environment, and adapting
them to this environment is a non-trivial task. Microservices have appeared
recently as novel architectural styles that are native to the cloud. These
cloud-native architectures can facilitate migrating on-premise architectures to
fully benefit from the cloud environments because non-functional attributes,
like scalability, are inherent in this style. The existing approaches on cloud
migration does not mostly consider cloud-native architectures as their
first-class citizens. As a result, the final product may not meet its primary
drivers for migration. In this paper, we intend to report our experience and
lessons learned in an ongoing project on migrating a monolithic on-premise
software architecture to microservices. We concluded that microservices is not
a one-fit-all solution as it introduces new complexities to the system, and
many factors, such as distribution complexities, should be considered before
adopting this style. However, if adopted in a context that needs high
flexibility in terms of scalability and availability, it can deliver its
promised benefits."
pub.1094068276,ARTIST Methodology and Framework: A Novel Approach for the Migration of Legacy Software on the Cloud,"Nowadays Cloud Computing is considered as the ideal environment for engineering, hosting and provisioning applications. A continuously increasing set of cloud-based solutions is available to application owners and developers to tailor their applications exploiting the advanced features of this paradigm for elasticity, high availability and performance. Although these offerings provide many benefits to new applications, they also incorporate constrains to the modernization and migration of legacy applications by obliging the use of specific technologies and explicit architectural design approaches. The modernization and adaptation of legacy applications to cloud environments is a great challenge for all involved stakeholders, not only from the technical perspective, but also in business level with the need to adapt the business processes and models of the modernized application that will be offered from now on, as a service. In this paper we present a novel model-driven approach for the migration of legacy applications in modern cloud environments which covers all aspects and phases of the migration process, as well as an integrated framework that supports all migration process."
pub.1150433845,VirTEE,"Modern security architectures provide Trusted Execution Environments (TEEs) to protect critical data and applications against malicious privileged software in so-called enclaves. However, the seamless integration of existing TEEs into the cloud is hindered, as they require substantial adaptation of the software executing inside an enclave as well as the cloud management software to handle enclaved workloads. We tackle these challenges by presenting VirTEE, the first TEE architecture that allows strongly isolated execution of unmodified virtual machines (VMs) in enclaves, as well as secure live migration of VM enclaves between VirTEE-enabled servers. Combined with its secure I/O capabilities, VirTEE enables the integration of enclaved computing in today's complex cloud infrastructure. We thoroughly evaluate our RISC-V-based prototype, and show its effectiveness and efficiency."
pub.1151744880,Migrating on-premise application workloads to a hybrid cloud architecture,"Organizations are moving towards adapting cloud strategies for various reasons. The important benefits realized with the adaption of cloud computing are - scalability, availability, performance and cost. With the benefits, come the challenges such as - security, privacy, regulatory compliances, trust, etc. With adoption of cloud, organizations are relieved from hardware infrastructures up to a large extent as well as overheads of managing, operating and physical security of infrastructure hardware. Having legacy applications on-premise, enterprises are looking forward to modernize the applications in terms of planning to strategize the future vision, need for scaling those applications, be competitive in the market with an ability as well as capability to cope and adapt the new demand and changes with ease, flexibility and cost effectiveness. One of the immediate adaptions with the cloud that the enterprises are looking forward is migrating their on-premise workloads to the cloud infrastructure, so that they can realize the benefit of cost-effective solutions along with minimizing operational and management overheads. In this paper we are proposing a hybrid cloud architecture that an enterprise may adapt while considering migrating one of its legacy applications’ workload to the cloud environment; with minimal changes and migration effort."
pub.1096021370,A Legacy Application Meta-model for Modernization,"Cloud computing provides many advantages to enterprises such as rapid elasticity, reduced operational costs, no upfront investment, resource pooling, so on. To take advantage of cloud benefits, enterprise attempt to move their legacy application to the cloud environment. However, many factors influence this decision. First one involves a full understanding of the legacy application architectural model from different view models such as business (e.g., defining application functionalities), implementation and data (e.g., the architecture of the application, the language used) and infrastructure (e.g., Hardware resources). Second, it requires choosing, in an efficiently way, the most suitable cloud services responding to users' needs. Finally, each cloud service adopted defines a scenario modernization based on Architecture Driven Modernization (ADM) which support a transformation of the legacy application to a cloud environment to satisfy new users demands. These concerns motivate the need for legacy application meta-model. This meta-model captures the high level of a legacy application regardless of technical details and represents three viewpoints (business viewpoints, implementation and data viewpoints and infrastructure viewpoints). Each viewpoint focuses on the application stakeholder perspectives. This meta-model helps also helps to make a decision about which view of the legacy application should be modernized and which cloud service (e.g., IaaS, PaaS, and SaaS) model is appropriate to be moved."
pub.1157303468,Legacy systems to cloud migration: A review from the architectural perspective,"Legacy systems are business-critical systems that hold the organization’s core business functions developed in a traditional way using monolith architecture and usually deployed on-premises. Through time, this system is exposed to improvement changes, increasing its size and number of functionalities, thus increasing its complexity, and maintaining it becomes a disadvantage to the organization. Migration to the cloud environment becomes the primary option to improve legacy application agility, maintainability, and flexibility. However, to take advantage of the cloud environment, monolith legacy application needs to be rearchitected as microservice architecture to fully benefit from cloud advantages. This paper aims to understand the motivation for cloud migration, investigate existing cloud migration frameworks, identify the target architecture for the cloud, and establish any empirical quality issues in cloud migration from the implementation point of view. To achieve those objectives, we conducted a systematic literature review (SLR) of 47 selected studies from the most relevant scientific digital libraries covering pre-migration, migration, and post-migration stages. The SLR outcome provided us with the primary motivation for the cloud migration, existing cloud migration frameworks, targeted migration architecture patterns, and migration challenges. The results also highlight areas where more research is needed and suggest future research in this field. Furthermore, our analysis shows that current migration approaches lack quality consideration, thus contributing to post-migration quality concerns."
pub.1125521601,Statistical Comparison of Architecture Driven Modernization with other Cloud Migration Frameworks and Formation of Clusters,"Corporations are migrating their legacy software systems towards the cloud environment for amelioration, to avail benefits of the cloud. Long term success of modernizing a legacy software depends on the characteristics of the chosen cloud migration approach. Organizations must think over how strategically imperative is the chosen cloud migration framework to their business? Thus, the Object Management Group (OMG) has defined standards for the modernization process based on Architecture Driven Modernization (ADM) framework. ADM serves as a vehicle for facilitating the arrangement of information technology with business stratagem and its architecture. Until now, it seems that there is no systematic mapping among ADM and other cloud migration frameworks, highlighting the demanding features. This research aims to give an in-depth study of similar cloud migration frameworks. Thus, the researchers introduced the clusters containing cloud migration frameworks having similar features to ADM. This systematic mapping can be seen as a valuable asset for those who are interested in choosing the best migration framework from the pool of cloud modernization frameworks, according to their legacy software requirements. The clustering technique is used to appraise and compare ADM with some of the other cloud migration frameworks for highlighting the similarities and key differences. The quality of clusters is evaluated by the Rand index and Silhouette measurements. The study distills the record and yields a sound and healthy catalog for essential events and concerns that are communal in cloud migration frameworks. This research offers the one-stop-shop convenience that the industry desperately desires. "
pub.1101647580,Model Driven Modernization and Cloud Migration Framework with Smart Use Case,"Abstract
With the fast evolving cloud market, many enterprises have attempted to move their legacy application in this new environment to take its full advantage. However, this move is strongly associated with the fact of packaging the legacy application in an image or in a container encapsulated into a virtual machine and deployed in a single instance.So, the need of making the legacy application more agile and flexible is becoming a must on moving to a cloud environment, yet to fully benefit from it. It is important to change the architecture of the legacy application before deploying it in a cloud environment. In consequence, there is a need for an approach that assure two important things: how the architecture of legacy application has to be changed to transform it into a cloud native application architecture and then to be deployed it in a cloud environment.In this work, we propose a modernization iterative and incremental process to overcome the listed issues above. This process is based on the concept of smart use case combining with the architecture driven modernization (ADM) approach. The last consists on followed the steps: Reverse Engineering, Transformation/Upgrade, and Forward Engineering. This process aims to not only raise the IT agility, but moreover its business agility."
pub.1129597933,A Review of Possible Planetary Atmospheres in the TRAPPIST-1 System,"TRAPPIST-1 is a fantastic nearby (∼39.14 light years) planetary system made of at least seven transiting terrestrial-size, terrestrial-mass planets all receiving a moderate amount of irradiation. To date, this is the most observationally favourable system of potentially habitable planets known to exist. Since the announcement of the discovery of the TRAPPIST-1 planetary system in 2016, a growing number of techniques and approaches have been used and proposed to characterize its true nature. Here we have compiled a state-of-the-art overview of all the observational and theoretical constraints that have been obtained so far using these techniques and approaches. The goal is to get a better understanding of whether or not TRAPPIST-1 planets can have atmospheres, and if so, what they are made of. For this, we surveyed the literature on TRAPPIST-1 about topics as broad as irradiation environment, planet formation and migration, orbital stability, effects of tides and Transit Timing Variations, transit observations, stellar contamination, density measurements, and numerical climate and escape models. Each of these topics adds a brick to our understanding of the likely—or on the contrary unlikely—atmospheres of the seven known planets of the system. We show that (i) Hubble Space Telescope transit observations, (ii) bulk density measurements comparison with H2-rich planets mass-radius relationships, (iii) atmospheric escape modelling, and (iv) gas accretion modelling altogether offer solid evidence against the presence of hydrogen-dominated—cloud-free and cloudy—atmospheres around TRAPPIST-1 planets. This means that the planets are likely to have either (i) a high molecular weight atmosphere or (ii) no atmosphere at all. There are several key challenges ahead to characterize the bulk composition(s) of the atmospheres (if present) of TRAPPIST-1 planets. The main one so far is characterizing and correcting for the effects of stellar contamination. Fortunately, a new wave of observations with the James Webb Space Telescope and near-infrared high-resolution ground-based spectrographs on existing very large and forthcoming extremely large telescopes will bring significant advances in the coming decade."
pub.1143522861,What Matters Most in Life? A German Cohort Study on the Sources of Meaning and Their Neurobiological Foundations in Four Age Groups,"Existing work in the field of positive psychology suggests that people can draw meaning from a variety of sources. The present study aimed to identify the most important sources of meaning and to explore the role of age and neural adaptation processes in this context. As part of a large German cohort study, 1,587 individuals between 12 and 94 years were asked to provide a maximum of five responses to the question ""What matters most to you in life?"" We divided the study population into four age groups and analyzed the obtained answers qualitatively and quantitatively using (1) word clouds and (2) frequency comparisons based on a summarizing content analysis. A chi-squared test was used to test the observed differences between age groups. Identified sources of meaning could be clustered into 16 main and 76 subcategories, with <i>relationships</i> (by 90% of respondents) and <i>health and well-being</i> (by 65% of respondents) being the most frequently named main categories, followed by a <i>good living environment</i> (by 28%), <i>(leisure) time</i> (by 26%), and <i>work</i> (by 24%). The study revealed some remarkable age-related patterns. While the importance of <i>partnership</i> increased with age, <i>social networks</i> were less important to older individuals. We also found that, for example, the importance of <i>self-realization</i>, <i>success and career</i> decreased with age, while the opposite was true for <i>life satisfaction</i> and <i>peace and harmony</i>. <i>Security</i> was most important to individuals in the two middle age groups between 30 and 69 years. The study advances our understanding of meaning across various ages by showing that individuals of different ages perceive different things as meaningful to them. Interpreting our results in the light of a neurobiological model of motivation systems, we argue that neural adaptation processes may play an important role in the (changing) perceptions of meaning throughout life."
pub.1150832215,Architect: A Framework for the Migration to Microservices,"The migration from a monolithic to a microservice architecture is a recurring step in many software projects. With the increasing distributed nature of the transformed system, new challenges for data consistency and deployment arise, which can be counteracted by the integration of microservices patterns. However, the use of such patterns is complex and time-consuming. In this paper, we describe a case study of a migration process of the learning management system Artemis which consists of two phases. The first phase shows the transformation from a monolithic architecture to a microservice architecture with a shared database. It sets as a goal the identification of microservice boundaries, the decomposition of the monolith application into multiple distributed entities, as well as their orchestration in a cloud-ready environment. The second phase migrates the shared database into multiple databases based on the database-per-microservice pattern. While analyzing the current Artemis architecture, we describe a gradual refactoring of an existing application to decompose Artemis into multiple subsystems. We developed Architect, a framework which is based on a domain-specific language for building dependable distributed systems as a template to ensure the data consistency of the distributed transactions using the Saga pattern. We decomposed Artemis into 3 microservices and provided the migration concept from shared-database to database-per-microservice using Architect. The framework helped to reduce the complexity of using the Saga pattern. It introduced the eventual consistency in a distributed database system and decreased the coupling of the data storage. The migration to a microservice architecture solves many problems of a monolith application, but introduces new challenges and increases the complexity of the overall system. Architect focuses on greenfield project, but currently does not provide a software evolution approach. We will add support for reengineering projects, which can facilitate the migration process of existing system."
pub.1129399088,Microservices mobility in datacenters supported by Software Defined Networking,"The growing adoption of software architectures based on microservices, and consequently the proliferation of the use of containers in environments with hybrid infrastructures (local data centers and public cloud providers), poses several challenges in terms of integrating this new paradigm into already existing solutions. In this context, this article initially focuses on the challenges of integrating a container orchestration platform (Kubernetes) with existing solutions in the data center, namely: network infrastructure (Cisco ACI – Application Centric Infrastructure), load balancing (F5) and storage (NFS). In a second stage, this paper also addresses the issue of integration with orchestration platforms hosted in a public cloud (Microsoft Azure). This integration will allow the movement of containers from the local data center to the cloud without having to reconFigure the network infrastructure or the current security policy."
pub.1164606524,Cloud migration framework clustering method for social decision support in modernizing the legacy system,"Abstract Cloud solutions accelerate the large‐scale acceptance of IoT projects. By diminishing the need for maintaining on‐premises infrastructure, the cloud has enabled corporations to surpass the traditional applications of IoT (e.g., in‐home appliances) and opened the doors for large‐scale deployment of IoT applications on the cloud. However, shifting legacy systems to the cloud environment can be considerably difficult. Accordingly, this article proposes a method that may support organizations in deciding to modernize their legacy systems. The main concept of this study is to discuss the modernization strategies in detail and to support organizations in selecting the most accurate and appropriate cloud migration strategy, based on their requirements of the legacy system. This article introduces a novel research process, called the K‐means cosine cloud clustering method (K3CM). K3CM is a statistical knowledge‐based method for identifying and clustering the most relevant and similar cloud migration strategies. The quality of a cluster is evaluated by measuring intra‐cohesiveness. Simulation experiments statistically analyzed, evaluated, and verified the quality of K3CM clusters. Correspondence analysis explored the similarity and relationship among cloud migration frameworks and validated the proposed technique. The statistical and simulation results of this study focus on the analytics and decision support system implementation that provides a reliable, valid, and robust clustering method for modernizing the legacy system."
pub.1140757148,Legacy Digital Transformation: TCO and ROI Analysis,"Legacy Digital Transformation is modernizing or migrating systems from non-digital or older digital technology to newer digital technologies. Digitalization is essential for information reading, processing, transforming, and storing. Social media, Cloud, and analytics are the major technologies in today's digital world. Digitalization (business process) and Digital Transformation (the effect) are the core elements of newer global policies and processes. Recent COVID pandemic situation, Organizations are willing to digitalize their environment without losing business. Digital technologies help to improve their capabilities to transform processes that intern promote new business models. Applications cannot remain static and should modernize to meet the evolving business and technology needs. Business needs time to market, Agility, and reduce technical debt. Technology needs consist of APIs, better Security, Portability, Scalability, Cloud support, Deployment, Automation, and Integration. This paper elaborates different transformation/modernization approaches for Legacy systems written in very long or End of Life (EOL) systems to newer digital technologies to serve the business needs. EOL impacts application production, supportability, compliance, and security. Organizations spend money and resources on Digital Transformation for considering Investment versus Return on Investment, Agility of the System, and improved business processes. Migration and Modernization are critical for any Legacy Digital Transformation. Management takes decisions to proceed with Digital Transformation for considering Total Cost Ownership (TCO) and Return on Investment (ROI) of the program. The paper also includes a TCO-ROI calculator for Transformation from Legacy / Monolithic to new architectures like Microservices."
pub.1070324010,The potential impacts of climate variability and change on air pollution-related health effects in the United States.,"Climate change may affect exposures to air pollutants by affecting weather, anthropogenic emissions, and biogenic emissions and by changing the distribution and types of airborne allergens. Local temperature, precipitation, clouds, atmospheric water vapor, wind speed, and wind direction influence atmospheric chemical processes, and interactions occur between local and global-scale environments. If the climate becomes warmer and more variable, air quality is likely to be affected. However, the specific types of change (i.e., local, regional, or global), the direction of change in a particular location (i.e., positive or negative), and the magnitude of change in air quality that may be attributable to climate change are a matter of speculation, based on extrapolating present understanding to future scenarios. There is already extensive evidence on the health effects of air pollution. Ground-level ozone can exacerbate chronic respiratory diseases and cause short-term reductions in lung function. Exposure to particulate matter can aggravate chronic respiratory and cardiovascular diseases, alter host defenses, damage lung tissue, lead to premature death, and possibly contribute to cancer. Health effects of exposures to carbon monoxide, sulfur dioxide, and nitrogen dioxide can include reduced work capacity, aggravation of existing cardiovascular diseases, effects on pulmonary function, respiratory illnesses, lung irritation, and alterations in the lung's defense systems. Adaptations to climate change should include ensuring responsiveness of air quality protection programs to changing pollution levels. Research needs include basic atmospheric science work on the association between weather and air pollutants; improving air pollution models and their linkage with climate change scenarios; and closing gaps in the understanding of exposure patterns and health effects."
pub.1174904091,Insights on Microservice Architecture Through the Eyes of Industry Practitioners,"The adoption of microservice architecture has seen a considerable upswing in
recent years, mainly driven by the need to modernize legacy systems and address
their limitations. Legacy systems, typically designed as monolithic
applications, often struggle with maintenance, scalability, and deployment
inefficiencies. This study investigates the motivations, activities, and
challenges associated with migrating from monolithic legacy systems to
microservices, aiming to shed light on common practices and challenges from a
practitioner's point of view. We conducted a comprehensive study with 53
software practitioners who use microservices, expanding upon previous research
by incorporating diverse international perspectives. Our mixed-methods approach
includes quantitative and qualitative analyses, focusing on four main aspects:
(i) the driving forces behind migration, (ii) the activities to conduct the
migration, (iii) strategies for managing data consistency, and (iv) the
prevalent challenges. Thus, our results reveal diverse practices and challenges
practitioners face when migrating to microservices. Companies are interested in
technical benefits, enhancing maintenance, scalability, and deployment
processes. Testing in microservice environments remains complex, and extensive
monitoring is crucial to managing the dynamic nature of microservices. Database
management remains challenging. While most participants prefer decentralized
databases for autonomy and scalability, challenges persist in ensuring data
consistency. Additionally, many companies leverage modern cloud technologies to
mitigate network overhead, showcasing the importance of cloud infrastructure in
facilitating efficient microservice communication."
pub.1095620342,Futuristic Assimilation of Cloud Computing Platforms and Its Services,"Cloud computing is one of today's most exciting technologies due to its ability to reduce costs associated with computing while increasing flexibility and scalability for computer processes. Cloud computing is Internet-based computing, whereby shared resources, software and information, are provided to computers and devices on-demand, like the electricity grid. It aims to construct a perfect system with powerful computing capability through a large number of relatively low-cost computing entities, and using the advanced business models. The construction of on-premises application platform into cloud environment with limited foundations, infrastructure and application services. The integration of cloud platform architecture and on-premises platform together is an enormous reform on building a huge environment on today's cloud computing environment. SOA is a set of services and advocate the principles of component reuse and well defined relationship between a service provider and service consumer. This article introduces the background, services and service model of cloud computing and overlapping of SOA and cloud computing."
pub.1001895850,Impact of Cloud Adoption on Agile Software Development,"Cloud computing provides a wide range of core infrastructure services such as compute and storage, along with building blocks which can be consumed from both on-premise environments and the Internet to develop cloud-based applications. It offers Platform as a Service capability which allows applications to be built, hosted and run from within managed data centres using programmable APIs or interoperable services exposed by the platform. The objective of this chapter is to study the effects of cloud adoption on software development projects that use agile methodologies. Agile methodologies involve iterative and incremental approaches to software development. The ubiquitous nature of cloud computing makes it an enabler of agile software development. This chapter highlights various aspects of cloud provision that can catalyse agile software development. The chapter provides directions for agile teams that are keen on exploiting the potential of cloud to alleviate the challenges currently faced by them. A case study of an agile development team which adopted cloud is discussed to articulate the real-time benefits and challenges in adopting the cloud environment."
pub.1051595582,A proposed novel enterprise cloud development application model,"Currently, there is limited guidance on generic cloud software architecture model for designing and building cloud applications and each platform provider has different standards that influences the way applications are developed and written. In this paper we propose a cloud application development model based on developing services (typically REST API services). We see clear benefits in applying this model to service oriented architecture (SOA) and business process management (BPM) design in order to develop enterprise applications for cloud environments in a systematic, rapid manner. This conceptual model described in this paper is drawn from the core ideas and practices behind REST API, BPM, and SOA. In this paper we focus on the proposed software development model for enterprise cloud applications and services. Throughout this paper we use the term cloud application and cloud service interchangeably. The model described in this paper can be applied to both. Further research on a software-modeling tool that provides a standardized structure for cloud software development and deployment seems to be in order. Supported by a graphical modeling environment, such a tool would utilize a Cloud Resource Provisioning language, will be able to seamlessly deploy and provision cloud resources across multiple cloud service providers. The presented research provides a practical and extensible application development model for cloud platforms. The model described can be used in a variety of applications as well as migration of legacy code to cloud platforms. By using this model, a cloud application can be deployed as small functionally separate components to different cloud nodes. Resource allocation can be defined for these nodes on a per need basis, which minimizes the cost of acquiring a fail over and load balanced system compared to the traditional on premise application development. In this paper, a detailed description of the analysis and development steps of the proposed model is also presented."
pub.1046855705,Cloud-basierte Plattformen zur Anwendungsintegration – Angebote und Praxisbeispiel,"The use of Software-as-a-Service in the company context is no longer limited to single pilot projects but increasingly includes business critical applications. This leads to a growing demand for the integration of Software-as-a-Service (e.g. Salesforce.com) and traditional on-premise applications. Cloud-based integration platforms promise to address this issue in flexible and cost-effective manner as an alternative to point-to-point integration or traditional enterprise application integration platforms. The cloud-based platforms offer a variety of standard application adapters and allow the visual development, the execution and the administration of complex integration processes within the cloud. Some vendors offer software agents for the secure communication between Software-as-a-Service, on-premise applications and the platforms. A few agents act as full local runtime environments for integration processes so that no critical data is shared with the platform."
pub.1152355545,On-Premise or Cloud Computing: An Integrated Novel Approach to Study the Adoption of Software Product’s Deployment Model with Different Scopes,"Organizations adopt software products for their betterment through on premises and cloud computing depending on their requirement. On a broad level, cloud computing itself is available in three types, namely infrastructure as a service, software as a service, and platform as a service. While each deployment model and its use are well defined, complexities and challenges are there for organizations in deciding among the various models. This conceptual paper is an attempt to represent different scopes for the adoption of appropriate model of software product. There are various scientific models available to study the adoption of technological innovation, but the process of adoptions depends on the different scopes. The use of only one scientific model to study the adoption process will not justify in studying the different scopes of adoption process. Therefore, we have used an integrated approach with three scientific models, namely DEMATEL framework, technology-organization-environment framework (TOE), and diffusion of innovation (DOI)."
pub.1090825460,A REVIEW OF EXISTING CLOUD AUTOMATION TOOLS,"Many enterprises are running distributed applications on their on-premise servers. However, if load on those servers changes unexpectedly, then itbecomes tedious to scale the resources and requires skilled human power to manage such situations. It may increase the capital expenditure. Hence,many companies have started to migrate their on-premise applications to the cloud. This migration of the applications to the cloud is one of the majorchallenges. To setup and manage the growing complex infrastructure, after migrating these applications to the cloud are really a time-consuming andtedious process which results in downtime. Hence, we need to automate this environment. To achieve architecture for the distributed systems whichsupport security, repeatability, reliability, and scalability, we require some cloud automation tools. This paper summarizes tools such as Terraform andcloud formation for infrastructure automation and Docker and Habitat for application automation."
pub.1160129174,Migrating from Monolithic Applications to Cloud Native Applications,"With the rapid development of cloud computing, more and more applications are developed explicitly for the cloud infrastructure. And there is an increasing demand to migrate monolithic applications to cloud computing environment. Because of the differences in infrastructures, the cloud native applications have different design principles with the monolithic applications. Without correct guidance when working on the migration, the migrated applications cannot take good advantage of the cloud computing environment. Even worse, it may cause a totally failure of the migration. That would be a waste of resource because migrating a monolithic application to cloud computing environment is usually a huge project. With the hope to provide a general guidance, this article introduces the design principles of cloud native applications and the technical aspects needed to be taken into consideration when migrating monolithic applications to cloud environment."
pub.1118261636,Software-Defined Cloud Computing: Architectural Elements and Open Challenges,"The variety of existing cloud services creates a challenge for service
providers to enforce reasonable Software Level Agreements (SLA) stating the
Quality of Service (QoS) and penalties in case QoS is not achieved. To avoid
such penalties at the same time that the infrastructure operates with minimum
energy and resource wastage, constant monitoring and adaptation of the
infrastructure is needed. We refer to Software-Defined Cloud Computing, or
simply Software-Defined Clouds (SDC), as an approach for automating the process
of optimal cloud configuration by extending virtualization concept to all
resources in a data center. An SDC enables easy reconfiguration and adaptation
of physical resources in a cloud infrastructure, to better accommodate the
demand on QoS through a software that can describe and manage various aspects
comprising the cloud environment. In this paper, we present an architecture for
SDCs on data centers with emphasis on mobile cloud applications. We present an
evaluation, showcasing the potential of SDC in two use cases-QoS-aware
bandwidth allocation and bandwidth-aware, energy-efficient VM placement-and
discuss the research challenges and opportunities in this emerging area."
pub.1042409532,A Framework for Secure Migration Processes of Legacy Systems to the Cloud,"The emergence of cloud computing as a major trend in the IT industry signifies that corporate users of this paradigm are confronted with the challenge of securing their systems in this new environment. An important aspect of that, includes the secure migration of an organization’s legacy systems, which run in data centers that are completely controlled by the organization, to a cloud infrastructure, which is managed outside the scope of the client’s premises and may even be to-tally off-shore. This paper makes two important contributions. Firstly, it presents a process (SMiLe2Cloud) and a framework that supports secure migration of corporate legacy systems to the cloud. We propose a process based on a continuous improvement cycle that starts with a Knowledge Discovery Meta-Model (KDM) set of models from which a security model for legacy system migration to the cloud is derived. Secondly, it provides a set of clauses (derived from the models) for security cloud providers and custom security cloud controls."
pub.1093552942,Harnessing and Securing Cloud in Patient Health Monitoring,"A first proposal to use a secure open cloud architecture (OpenCloudCare) for remote patient health monitoring defines the front end and back end architecture that would integrate healthcare devices into the enterprise cloud. This service oriented architecture (SOA) uses existing open source resources for easy implementation, migration and rapid deployment. The introduction of the atlas middleware in the cloud environment enables scalable integration of devices as software services without worrying on hardware details. OpenCloudCare has the potential to enable the independent living for the aged people with minimal cost. Like Amazon EC2, OpenCloudCare can emerge as the de-facto platform, on which different vendors and developers can offer wider range of services using existing devices and enterprise applications. The benefits of migrating existing patient monitoring systems into cloud and the migrating process are also discussed. The major components required for securing the cloud infrastructure are also identified. Complying with the federal requirements on electronic health record (EHR) protection is the major challenge."
pub.1094808840,An Extensible Architecture for Detecting Violations of a Cloud Environment's Constraints During Legacy Software System Migration,"By utilizing cloud infrastructures or platforms as services, SaaS providers can counter fluctuating loads through smoothly scaling up and down and therefore improve resource-and cost-efficiency, or transfer responsibility for the maintenance of complete underlying software stacks to a cloud provider, for instance. Our model-based approach CloudMIG aims at supporting SaaS providers to semiautomatically migrate legacy software systems to the cloud. Thereby, the analysis of conformance with the specific constraints imposed by a cloud environment candidate along with the detection of constraint violations constitutes an important early phase activity. We present an extensible architecture for describing cloud environments, their corresponding constraints, and appropriate violation detection mechanisms. There exist predefined constraint types with specified domain semantics as well as generic variants for modeling arbitrary constraints. A software system's compliance can be examined with the assistance of so called constraint validators. They operate on discovered KDM-based models of a legacy system. Additional constraint validators can be plugged into the validation process as needed. In this context, we implemented a prototype and modeled the PaaS environment Google App Engine for Java. We report on a quantitative evaluation regarding the detected constraint violations of five open source systems."
pub.1050189783,Chapter 13 Application Migration PowerBuilder to Oracle APEX,"Oracle Application Express is a free technology from Oracle that is embedded in the Oracle database and is built for cloud computing. It contains a cloud development environment, built-in multitenancy capabilities, and end-to-end disk-to-application management and migration capabilities; the latter allows the system to be viewed as one integrated unit, resulting in easy application management and provisioning in the cloud. To highlight the benefits of Oracle Application Express for cloud computing, this chapter comprises a case study describing how baby and children's clothing company Carter's, Inc., modernized a large-scale, mission-critical client/server business application to Oracle Application Express. Oracle APEX in the cloud provides many benefits for application developers and application development in general. The chapter explains how JSA2Solutions LLC, an Oracle partner, led the modernization and re-architecture efforts for Carter's, and reviews the legacy system architecture, the limitations and issues caused by legacy client/server applications, the new technology stack and architecture that was utilized, and the approach the company took in the modernization effort."
pub.1092620507,Cloud Readiness as an Enabler for Application Rationalization: A Survey in the Netherlands,"For over a decade both cloud computing and application rationalization are IT-strategy priorities in most organizations. Cloud computing has grown in the last decade and will continue to grow steadily. Most organizations struggle with reducing the number of applications. There remains a strong resistance from the business, to consolidate and standardize the legacy application platforms. Potentially migrating legacy applications to Software-as-a-Service applications resolves the business migration concerns. As the Software-as-a-Service functionality is a ready to use functionality which can be assessed prior to phasing out legacy applications. This is a distinct difference from a scenario where legacy applications will be replaced by to-be-(custom)-build applications. Also the implementation roadmap of Software-as-a-Services applications is much shorter and most of the Software-as-a-Services applications have data migration tools to import the data from the legacy applications into the new environment. A survey of 124 organizations indicated weak negative linear relationship between the estimated percentage Software as a Service applications and the envisioned number of applications for organizations with +500 applications for the period 2017–2022 (three two year intervals)."
pub.1137495035,AI and its Applications in the Cloud strategy,"The fourth industrial revolution identifies cloud computing, data, and artificial intelligence (AI) as opportunity clusters with double digit growth in the next couple of years. As part of the cloud and digital transformation, the role of AI is crucial in enabling that transformation as well as creating the new breed of applications on top. AI mechanisms can help accelerate the modernization of applications, their management, and the testing on cloud architectures. I will focus on two sub-problems: 1) Refactoring of massive monolith applications using AI techniques. This problem statement is particularly relevant in understanding legacy un-optimized code and transforming them to be more cloud-ready. Microservices are indeed becoming the de-facto design choice for software architecture. It involves partitioning the software components into finer modules such that the development can happen independently [2]. It also provides natural benefits when deployed on the cloud since resources can be allocated dynamically to necessary components based on demand. We are exploring how AI can help accelerate the transformation of existing applications to microservices. 2) Detecting faults in application behavior at runtime from operational data. This problem statement is particularly relevant in understanding how to manage this new architecture of multiple microservices across the cloud stack [1], [3]. Operational data artifacts span across logs, metrics, tickets, and traces. Looking at signals across the artifacts and across the stack presents a challenging data correlation problem. AI mechanisms can help accelerate problem determination in these complex environments. I will also share my thoughts on how fundamental breakthroughs in AI Research will be needed as we address some of the core problems of cloud computing."
pub.1035166317,Migration of an On-Premise Application to the Cloud: Experience Report,"As of today it is still not clear how and when cloud computing should be used. Developers very often write applications in a way that does not really fit a cloud environment, and in some cases without taking into account how quality attributes (like performance, security or portability) are affected. In this paper we share our experience and observations from adopting cloud computing for an on-premise enterprise application in a context of a small software company. We present experimental results concerning a comparative evaluation (w.r.t. performance and cost) of the behavior of the original system both on-premise and on the Cloud, considering different scenarios in the Cloud."
pub.1128198281,Evolving Adaptation Rules at Runtime for Multi-cloud Applications,"Cloud computing is the most prevailing computing paradigm as it has led to a proliferation of cloud-based applications, either through the migration of existing legacy or the development of novel ones from scratch. In fact, nowadays, there is also a move towards adopting multi-clouds due to the main benefits they introduce, including vendor lock-in avoidance and optimal application design and provisioning via different cloud services. Unfortunately, multi-cloud applications face the challenge that even a single cloud environment exhibits a certain level of dynamicity and uncertainty. As such, a suitable service level cannot be handed over to their customers, which leads to SLA penalty costs and application provider reputation reduction. To address this challenge, we have previously proposed a cross-level and multi-cloud application adaptation architecture. Towards realising such an architecture, this paper focuses on supporting cross-level application adaptation through the modelling of adaptation rules that enact adaptation workflows but also on evolving such an adaptation to address both the application and exploited cloud services evolution as well as the provisioning environment’s dynamicity. The modelling of such rules and their execution history is accommodated through corresponding extensions to a state-of-the-art cloud modelling language called CAMEL. Further, a specific selection algorithm for those alternative adaptation rules able to address the current problematic situation is suggested which takes into account their execution history and especially their performance."
pub.1169925656,Effectiveness of AI-Based Resource Optimization in Cloud Environments: An Empirical Study Focused on Comparative Analysis between MSA and Rehost Systems,"This study empirically analyzes the effectiveness of resource optimization utilizing artificial intelligence (AI) within cloud computing environments. It compares systems that have been transitioned from a Monolithic structure to Microservices Architecture (MSA) with those adopting a simple Rehost strategy, to validate the operational efficiency of two systems whose resource efficiency has been enhanced compared to traditional legacy systems. The research is based on systems that determined resource efficiency post-cloud migration using APM tools. It points out the limitations of traditional heuristic operation methods in resource optimization, which lead to issues such as application performance degradation, resource over-allocation, and increased operational costs. This study aims to overcome the conventional resource optimization standards that depended on the average peak usage of CPU and memory infrastructure, by introducing AI-based optimization tools, and applying on-premise standards to cloud and container environments to achieve resource optimization. The paper emphasizes the importance of resource optimization through AI-based automation tools, exploring the possibilities of resolving real-world application performance issues in cloud environments, reducing unnecessary resource usage costs, and cutting down on labor risks and operational expenses. Through this, it presents strategic directions on how AI technology can offer financial and operational advantages in managing cloud resources."
pub.1139816327,Computing Frameworks for VM Migration in Cloud,"Cloud Service users world wide are increasing with advancement in IT infrastructre. This leads to the demand of online storage for various operational reasons. The applications across various organizations are exploiting the cloud infrastructure offered by different service providers like Microsoft, Amazon, Twitter, IBM, and Hewlett-Packard, enable developers to distribute software through a central organization's hosting machine. Online storage is the next logical phase in the development of IT infrastructure. Cloud Storage Infrastructure has become a norm in distributed environment to host the multiple application on easily available networks. Rather than bothering about the Capital Expenditutre, the organizations are now using funds to invest in operational cost of running the services through Cloud service provider increasing the efficiency. While attempting to migrate the workload needs to be considered as imbalanced workload may result in system failure. Also the VM consolidation need to be done while the sytem is live without any failure. The security concerns for live migration are enlisted in limitations which is the largest concern for an organization whle deciding future course of action. This article reviews the cloud computing systems in the light of migrating the existing legacy systems to the cloud environment and suggest the operational features that an organization should consider before the migration."
pub.1095172250,Software-Defined Cloud Computing: Architectural Elements and Open Challenges,"The variety of existing cloud services creates a challenge for service providers to enforce reasonable Software Level Agreements (SLA) stating the Quality of Service (QoS) and penalties in case QoS is not achieved. To avoid such penalties at the same time that the infrastructure operates with minimum energy and resource wastage, constant monitoring and adaptation of the infrastructure is needed. We refer to Software-Defined Cloud Computing, or simply Software-Defined Clouds (SDC), as an approach for automating the process of optimal cloud configuration by extending virtualization concept to all resources in a data center. An SDC enables easy reconfiguration and adaptation of physical resources in a cloud infrastructure, to better accommodate the demand on QoS through a software that can describe and manage various aspects comprising the cloud environment. In this paper, we present an architecture for SDCs on data centers with emphasis on mobile cloud applications. We present an evaluation, showcasing the potential of SDC in two use cases-QoS-aware bandwidth allocation and bandwidth-aware, energy-efficient VM placement-and discuss the research challenges and opportunities in this emerging area."
pub.1094275707,Communications Enablement of Software-as-a-Service (SaaS) Applications,"Software-as-a-Service (SaaS) is an emerging software application delivery model. It has gained an increasing momentum, and it is being adopted at a very fast pace. However, the integration of SaaS applications with enterprise on-premise applications has become a major factor for the success of its adoption. There is a critical need for methods and communication infrastructures that can enable distributed SaaS applications over the data network. This paper presents an approach and a realization of communications enablement of SaaS applications. It focuses on three areas: 1) an approach to enabling communications as a service; 2) a SaaS adaptor framework for integrating SaaS applications with on-premise communications services; and 3) methods and communication middleware for two-way web services crossing enterprise domains. A communication enabled sales opportunity management SaaS application is implemented to verify the proposed approach. Experimental results indicated that the proposed approach is feasible and effective to communication enable SaaS applications in distributed environment that connects both cloud computing and enterprise on-premises systems."
pub.1124123596,An Open Source Approach for Modernizing Message-Processing and Transactional COBOL Applications by Integration in Java EE Application Servers,"Modernization of monolithic “legacy” mainframe COBOL applications to enable them for modern service- and cloud-centric environments is one of the ongoing challenges in the context of digital transformation for many organizations. This challenge has been addressed for many years by different approaches. However, the possibility of using a pure Open Source Software (OSS)-based approach to run existing transactional COBOL code as part of Java EE-based web applications has just recently been demonstrated by the author. Therefore, in this paper, an overview of the previously proposed Quick Web-Based Interactive COBOL Service (QWICS) is given and its new extension to run message-processing COBOL applications via JMS is described. QWICS runs on Un*x-like operating systems such as Linux, and therefore on most platforms, but in particular on the mainframe itself. This enables a mainframe-to-mainframe re-hosting, preserving the unique features of the mainframe platform like superior availability and security."
pub.1007683590,Migrating legacy software to the cloud: approach and verification by means of two medical software use cases,"Summary Cloud computing is a technology that enables elastic, on‐demand resource provisioning, allowing application developers to build highly scalable systems. Multi‐tenancy, the hosting of multiple customers by a single application instance, leads to improved efficiency, improved scalability, and less costs. While these technologies make it possible to create many new applications, legacy applications can also benefit from the added flexibility and cost savings of cloud computing and multi‐tenancy. In this article, we describe the steps required to migrate existing applications to a public cloud environment, and the steps required to add multi‐tenancy to these applications. We present a generic approach and verify this approach by means of two case studies, a commercial medical communications software package mainly used within hospitals for nurse call systems and a schedule planner for managing medical appointments. Both case studies are subject to stringent security and performance constraints, which need to be taken into account during the migration. In our evaluation, we estimate the required investment costs and compare them to the long‐term benefits of the migration. Copyright © 2015 John Wiley & Sons Ltd."
pub.1039546724,Security Analysis in the Migration to Cloud Environments,"Cloud computing is a new paradigm that combines several computing concepts and technologies of the Internet creating a platform for more agile and cost-effective business applications and IT infrastructure. The adoption of Cloud computing has been increasing for some time and the maturity of the market is steadily growing. Security is the question most consistently raised as consumers look to move their data and applications to the cloud. We justify the importance and motivation of security in the migration of legacy systems and we carry out an analysis of different approaches related to security in migration processes to cloud with the aim of finding the needs, concerns, requirements, aspects, opportunities and benefits of security in the migration process of legacy systems."
pub.1094210533,Design and Implementation of Hybrid Cloud Computing Architecture Based on Cloud Bus,"A new model of hybrid cloud computing architecture based on cloud bus is proposed. The system is based on local private cloud, combined with one or more type(s) of public cloud(s). The internal structures of private cloud and public cloud are the same, including infrastructure and virtualization layer, cloud platforms layer, cloud bus layer, cloud application layer, the management center and storage centers. The layer of infrastructure and virtualization is designed to incorporate the underlying hardware resources into a virtual cluster, providing a variety of virtual resources to the upper layer. The layer of cloud platform is used to run Web applications or services, and carry application-specific development and application integration through its open interfaces. The cloud bus layer, consisting of a control bus, a number of node buses and adapters, is designed to manage and monitor the various services of the cloud platform layer. The proposed model of the architecture can accelerate the migration of the existing IT environments to cloud computing environments, reduce the investment, and make full use of IT resources."
pub.1092981905,Cloud restriction solver: A refactoring-based approach to migrate applications to the cloud," Context The migration of legacy systems to the Platform as a Service (PaaS) model provides several benefits, but also brings new challenges, such as dealing with the restrictions imposed by the service provider. Furthermore, factors such as time, training and the extensive reengineering activities make the migration process time consuming and error prone. Although there exist several techniques for partial or total migration of legacy applications to the cloud, only a few specifically address the resolution of these constraints. Objective This paper proposes a novel semi-automatic approach, called Cloud Restriction Solver (CRS), for migrating applications to a PaaS environment that avoids the cloud restrictions through user-defined refactorings. Methods The approach is supported by two open and extensible tools. The first one, called CRSAnalyzer, identifies the pieces of code that violate the restrictions of the chosen PaaS platform, while the second one, CRSRefactor, changes those pieces by equivalent cloud-enabled services. Results The applicability of the proposed approach is presented by showing its instantiation for Google App Engine as an Eclipse plugin and by migrating three Java applications to that PaaS successfully. In addition, an instantiation for IBM Bluemix has been created and used to compare the migration of the same application using the developed tools for both cloud providers. Conclusion The proposed approach fosters software reuse, is cloud-independent, and facilitates the migration of applications to PaaS platforms."
pub.1146591292,You don't need a Microservices Architecture (yet),"Within the past decade, the advent of cloud computing in terms of infrastructure, technology stacks, availability of services and tooling, along with the gradual improvement of its market environment, has driven many organizations to either consider or migrate many existing software systems to the cloud, either fully or partially. A common predicament in most cases, is the existence of a complex, monolithic application, potentially considered legacy at the time, that was not designed to be cloud-native and therefore requires a degree of redesign/reimplementation in order to benefit from cloud deployment. In such cases, the decomposition of the monolith to a set of loosely coupled, highly cohesive and self-contained microservices is a valid recommendation, provided that the organization is prepared to withstand the additional cost, in terms of human and financial resources, along with the unavoidable development overhead, which is inevitable during the early stages. However, the tendency of the tech world to embrace new trends and jump on hype trains for fear of obsoletion, has led to an excessive adoption of the microservices architecture (MSA), even in cases where such an architecture is not viable for the organization, or does not derive from any business requirements. This research focuses on establishing the position of a traditional monolith in the modern software architecture landscape and determine use cases that can still benefit from this paradigm, as well as use cases that could benefit from a partial or full transition to microservices architectures instead."
pub.1126910736,Janus: A Tool to Modernize Legacy Applications to Containers,"Abstract
Modernizing a legacy application to a set of containers is highly desirable as containers are agile, scalable, and can be easily tested and deployed on any cloud environment. In this paper, we propose Janus, a modernization tool that helps architects and developers to transform a legacy application into a set of containers. Janus realizes two capabilities: one, it automatically discovers configurations and dependencies needed to create docker artifacts, with prior rules and knowledge mined from similar legacy applications; two, it provides a dynamic web interface to interact with architect/developer to verify the discovered configurations and dependencies and guide users in acquiring missing information. We provide a demonstration of Janus on a legacy application."
pub.1050003612,Secure Migration of Legacy Applications to the Web,"In software engineering, migration of an application is the process of moving the software from one execution platform to another. Nowadays, many desktop applications tend to migrate to the web or to the cloud. Desktop applications are not prepared to face the hostile environment of the web where applications frequently receive harmful data that attempt to exploit program vulnerabilities such as buffer overflows. We propose a migration process for desktop applications with a text-based user interface, which mitigates existing security concerns and enables the software to perform safely in the web without modifying its of the source code. Additionally, we describe an open source tool that facilitates our migration process."
pub.1113996245,A Cache-Based Data Movement Infrastructure for On-demand Scientific Cloud Computing,"As cloud computing has become the de facto standard for big data processing, there is interest in using a multi-cloud environment that combines public cloud resources with private on-premise infrastructure. However, by decentralizing the infrastructure, a uniform storage solution is required to provide data movement between different clouds to assist on-demand computing. This paper presents a solution based on our earlier work, the MeDiCI (Metropolitan Data Caching Infrastructure) architecture. Specially, we extend MeDiCI to simplify the movement of data between different clouds and a centralized storage site. It uses a hierarchical caching system and supports most popular infrastructure-as-a-service (IaaS) interfaces, including Amazon AWS and OpenStack. As a result, our system allows the existing parallel data intensive application to be offloaded into IaaS clouds directly. The solution is illustrated using a large bioinformatics application, a Genome Wide Association Study (GWAS), with Amazons AWS, HUAWEI Cloud, and a private centralized storage system. The system is evaluated on Amazon AWS and the Australian national cloud."
pub.1033054976,An Architecture Paradigm for Providing Cloud Services in School Labs Based on Open Source Software to Enhance ICT in Education,"<p>The authors present their experience and practices of introducing cloud services, as a means to simplify the adoption of ICT (Information Communication and Technology) in education, using Free/Open Source Software. The solution creates a hybrid cloud infrastructure, in order to provide a pre-installed (Ubuntu and Linux Terminal Server Project) virtual machine, acting as a server inside the school, providing desktop environment based on the Software as a Service cloud model, where legacy PCs act as stateless devices. Classroom management is accomplished using the application “Epoptes.” To minimize administration tasks, educational software is provided accordingly, either on-line or through repositories to automate software installation (including patches and updates). The advantages of the hybrid cloud implementation, include services that are not completely dependent on broadband connections’ state, minimal cost, reusability of obsolete equipment, ease of administration, centralized management, patches and educational software provisioning and, above all, facilitation of the educational procedure.</p>"
pub.1140184931,Introduction to Cloud Resource Management,"Cloud computing provides users with access to system resources on demand. Before the advent of Cloud computing, users could run applications or programs from software downloaded to their computer or server. Cloud computing allows access to the same applications through the Internet. cloud computing has brought down the cost involved in computing. Users can avail resources and services according to their need. This ability to access information anywhere, anyhow, and at any time has positively impacted migration to the Cloud by organizations. With the advancement of cloud computing, there is a paradigm shift is in data storage and usage of remote applications. Adding mobility to computing, cloud has made data and applications to move out of physical buildings. So, data are no more confined. It is available on the go. By adding these features, the cloud also facilitates reliability, efficiency, and scalability. The demand of the cloud is exponentially growing with invention of the high-end and sophisticated devices. The cloud services are transparent and easy. Hence, it addresses the increasing demand easily. Cloud computing architecture is responsible for the distribution of cloud computing services that involve numerous cloud computing constituents, which communicate between them using a technique such as messaging line. The problem of managing the resources in the cloud still needs more innovations as the problems still persist. The cloud computing architecture of a cloud solution is the structure of the system, which involves premise and cloud resources, services, middleware, and software. In cloud computing, resource management comprises of provisioning, allocation, and monitoring. Cloud resources consist of the servers, memory, storage, network, CPU, application servers, and cybernetic systems otherwise called virtual machines. These machines are the processing units in cloud. Virtualization provides solutions for managing the cloud resources. The performance of any system depends on the effective management of resources. The resource management in cloud computing systems encompasses to manage the large number of virtual machines and physical machines (Vashistha, A., Kumar, S., Verma, P., Porwal, R. A self-adaptive view on resource management in cloud data center. IEEE Computer Society, 2018). Cloud architectures are constructed as software applications that use Internet accessible on-demand services. The applications in cloud architectures make use of the computing infrastructure when it is needed. It requests the necessary resources on demand, accomplishes a prescribed job, then releases the unneeded resources, and often disposes them after the job is done."
pub.1013798929,An approach of creative application evolution on cloud computing platform,"Cloud computing is a paradigm that focuses on sharing data and computing resources over a scalable network of nodes, so it is becoming a preferred environment for those applications with large scalability, dynamic collaboration and elastic resource requirements. Creative computing is an emerging research field in these applications, which can be considered as the study of computer science and related technologies and how they are applied to support creativity, take part in creative processes, and solve creativity related problems. However, it is a very hard work to develop such applications from the very beginning under new environment, while it is a big waste for legacy systems under existing environment. Now software evolution plays an important role. In this paper, we introduced creative computing firstly, including definition, properties and requirements. Then the advantages of cloud computing platform for supporting creative computing were analysed. Next, a private cloud as experimental environment was built. Finally, the process of creative application evolution was illustrated. Our work is about research and application of software evolution methodology, also is an exploratory try to do creative computing research under cloud environment."
pub.1095045233,Migrating Legacy Software to the Cloud with ARTIST,"As cloud computing allows improving the quality of software and aims at reducing costs of operating software, more and more software is delivered as a service. However, moving from a software as a product strategy to delivering software as a service hosted in cloud environments is very ambitious. This is due to the fact that managing software modernization is still a major challenge, especially when paradigm shifts, such as moving to cloud environments, are targeted that imply fundamental changes to how software is modernized, delivered, and sold. Thus, in addition to technical aspects, business aspects need also to be considered. ARTIST proposes a comprehensive software modernization approach covering business and technical aspects. In particular, ARTIST employs Model-Driven Engineering (MDE) techniques to automate the reverse engineering of legacy software and forward engineering of cloud-based software in a way that modernized software truly benefits from targeted cloud environments. Therewith, ARTIST aims at reducing the risks, time, and costs of software modernization and lowers the barriers to exploit cloud computing capabilities and new business models."
pub.1009623883,Leveraging Potential of Cloud for Software Performance Testing,"Considerable amount of cloud adoption is proven in the area of software testing. There are several providers in the area of hosted testing tools and services like IBM and HP. The functional testing tools cover a variety of aspects in functionality testing for Web applications, Web services and mobile applications. Another set of tools in relation to cloud environments is in the area of performance testing from cloud providers such as BlazeMeter, SOASTA, LoadStorm and Keynote. This chapter discusses various options available for leveraging cloud for performance testing of software applications. Additionally, the chapter covers some of the key considerations when selecting the solution providers. The chapter also presents a conceptual reference framework for leveraging the readily available public cloud infrastructures for optimized cost and high-volume load simulations from different regions for efficient testing. Such a custom framework can help in making the enterprise system services more market ready, which in turn aids in improving the overall quality of the enterprise systems. Proposed reference framework can be used in performance testing, regression testing, benchmarking and product certification of any on-premise or cloud-deployed services. This helps in reducing the overall testing cost and test cycle duration, thereby achieving accurate capacity planning."
pub.1120886161,Reliable Cloud Software Development Architectures and Business Models Case Study,"Development of cloud applications directly on the cloud infrastructure has become a common approach. Reliability concerns have also become more of a challenge during the last years. Our goal is to assert the potentials deriving from an integrated development environment, adopting software reliability concepts and fault tolerant techniques, as part of the cloud core services. As a methodology, we propose the implementation of a new cloud service Reliable Integrated Development Environment as a Service to become part of the existing core services. Our objectives are to 1) fulfill the need of having a development environment independent from personal desktop environments; 2) code development environment should be offered directly from the cloud service providers. With fault tolerant technique integration, we also suggest that the reliability of the cloud system should be handled from the cloud developers at software level when cloud quality assurance services fail to do so at their different levels. We also propose potential business models to become part of this core service for the major cloud providers. We introduce a new High Availability Coding model providing a roadmap for the future."
pub.1154515682,Modernization of Legacy Systems,"A number of key organizations are sustaining the decades old complex legacy systems despite their types of services and operating environment. Cloud computing provides numerous processes, tools and methods to emphasize upon the pay-as-per-use utility models. The utter need to modernize these older systems leads to modernization processes involving the vision to leverage the cloud computing benefits. The proposed modernization framework includes decision support module followed by the elaboration of available modernization approaches. This legacy problem is one of the main challenges for cloud computing to deal with. When re-architecting and reengineering these legacy systems, parallel computing approaches also need to be assessed for resource optimization in cloud environments."
pub.1093590355,Data Security Issues in Cloud-Based Software-as-a-Service ERP,"This paper discusses the data security issues and concerns that are prevalent when organizations are moving their Enterprise Resource Planning (ERP) systems to the cloud. Cloud computing has reinvented how organizations conduct business and has enabled them to innovate and compete in a dynamic environment through new and innovative business models. The growing popularity and success of the cloud has led to the emergence of cloud-based Software-as-a-Service (SaaS) ERP systems, a new alternative approach to traditional on-premise ERP systems. Cloud-based ERP has a myriad of benefits for organizations. However, infrastructure engineers need to address data security issues before moving their enterprise applications to the cloud. Cloud-based ERP raises specific concerns about the confidentiality and integrity of the data stored in the cloud. Such concerns that affect the adoption of cloud-based ERP are based on the size of the organization. Small to medium enterprises (SMEs) gain the maximum benefits from cloud-based ERP as many of the concerns around data security are not relevant to them. On the contrary, larger organizations are more cautious in moving their mission critical enterprise applications to the cloud. A hybrid solution where organizations can choose to keep their sensitive applications on-premise while leveraging the benefits of the cloud is proposed in this paper as an effective solution that is gaining momentum and popularity for large organizations."
pub.1017004215,Automatic conformance checking for migrating software systems to cloud infrastructures and platforms,"SUMMARY The migration of software systems to IaaS (infrastructure as a service)‐ or PaaS (platform as a service)‐based cloud environments enables SaaS providers to benefit from the cloud's merits, such as smoothly scaling up and down existing applications. Our approach, CloudMIG, aims at supporting SaaS providers to perform those migrations. Here, validating the specific constraints that are imposed by a cloud environment constitutes an important early‐phase activity. For example, the access to the file system, number of files, or calls to specific methods may be restricted by cloud providers. Those constraints have to be considered when evaluating the suitability of competing cloud environment candidates. In this paper, we describe CloudMIG's corresponding parts: a generic cloud environment model that incorporates these constraints and appropriate violation detection mechanisms. A software system's conformance can be examined with the assistance of constraint validators. They operate on extracted Knowledge Discovery Meta‐Model‐based system models and can, among others, apply metrics formulated with the Software Metrics Meta‐Model through our metrics execution engine. Additional constraint validators can be plugged into the validation process as needed. In this context, we implemented a prototype and modeled the PaaS environment Google App Engine for Java. We report on a quantitative evaluation regarding the detected constraint violations of five open‐source systems. Copyright © 2012 John Wiley & Sons, Ltd."
pub.1038745799,Mapping design patterns to cloud patterns to support application portability,"The use of Design Patterns in software engineering is well documented and supported, with new patterns descriptions being proposed and applied to software development even today. Cloud Patterns can be seen as an evolution of classic Design Patterns, since they provide optimal solutions for software development specialized for Cloud environments. By mapping Design and Cloud Patterns elements, we think it would be possible to develop an approach to support the porting of legacy applications to the Cloud, thus easing the modernization process and laying the basis for an interoperability and portability friendly software development. Here we analyse the actual feasibility of the mapping between Design and Cloud Patterns, by proposing a simple score-based methodology which analyses characteristics of both pattern categories to propose a solution to the matching problem. In order to compare different categories of patterns we leverage a semantic based representation, defined in previous works, which describes a set of common relevant characteristics related to patterns' scope and objectives."
pub.1168089103,A Step-By-Step Decision Process for Application Migration to Cloud-Native Architecture,"Cloud-native architecture is changing the cloud computing environment and introducing new dimensions to cloud computing adoption through the advancement in virtualisation, scalability and automation. However, many organisations found it challenging to adopt cloud-native architectures because of some issues, mainly the decision-making around migration because of the need for the adoption decision process. However, there are many decision-support systems or processes for cloud computing. Nevertheless, the additional characteristic and complexity of cloud-native architecture made many organisations find it challenging to adopt cloud-native architecture solutions. The need for a new decision-support process for the migration of either a native cloud application or legacy application to cloud-native architecture is highly needed to help the developers to be able to select the choice of model and whether to migrate or not. Therefore, this paper provides a step-by-step decision process for an organisation to migrate their application to cloud-native architecture to fill the gap. The decision process depends on the step-by-step cross-analysis of the steps and the tasks to help identify and resolve critical constraints that can affect migration into cloud-native architecture. An expert evaluation method was used to evaluate the decision process and steps. The decision process was helpful as they informed the decision makers about the needful steps and tasks to consider before migrating to the cloud-native architecture."
pub.1175298811,Extending a Low-Code Tool with Multi-cloud Deployment Capabilities,"Low-code emerged as an evolution of model-driven engineering to accelerate software delivery, and it continues to gain traction today. However, low-code tools and solutions have primarily focused on development, often neglecting or offering minimal support for the application deployment process, such as lacking capabilities for multi-cloud deployments. In this paper, we propose an extension of BESSER, an open-source low-code platform, to address the packaging and deployment of applications in multi-cloud environments. This extension includes the definition of a language and a grammar to enable the modeling of the deployment architecture, also enabling the specification of public and on-premises clusters. Additionally, we have developed code generators to automate the application packaging, and cloud provisioning and deployment using Terraform. The complete infrastructure is available in an open-source repository."
pub.1120059312,Software as a Service operation model in cloud based ERP systems,"The operation model for cloud-based application has already became the new standard in ERP implementations. The model has introduced a new software abstraction layer, which hides the data center model from the customers. This layer covers the architecture, operating system, database and elements. When the layer covers the complexity of the middleware layers, allows the owners to concentrate on implementing the core business logic only. The risk is usually that more and more resources are allocated to the infrastructure; with this model the resources can be balanced with the business side. In the end this will lead to extended application lifecycle, while the core functions of the business logic are totally separated from the recently changing infrastructure layer. Standard merge and release management, starting from pre alpha state and describes the way until the gold release, was changed in this operational model with an agile way release process. This results the overall possibility of always using the latest version of the application always, when using the Software as a Service cloud model. Code refactoring, reusability is a key factor, as having another abstraction layer between the platform specific and the platform independent model. Having the proper abstract transformation layer between these two keeps the application roles separated, and the end user does not have to create a whole IT support organization, because the abstract layer will hide the low level infrastructure. This article focuses mainly to the change management questions which arise when an organization moves from standard data center solution to a cloud based (SaaS) model. The ERP solutions should be redesigned, reconstructed and sometimes rebuilt from scratch to provide continuous versioning, and be able to offer customer services. As the goal is always to be able to provide services at a lower operational cost there must be a solid reusability method in the application. Code refactoring and reusability methods should be changed also, using and implemented on a separated abstraction layer. The transformation layer has to be able to maintain the goals of the reliability of the software product during its whole lifecycle. These middleware layer have to be able to maintain the reusability of the 3rd party software elements, but the commonly used parts have to be in the same format as in the base application. The article shows different ways and possibilities which all can be used for building up an application environment in the cloud using software as a service and the other different service bus like methodologies, and focuses on switching the operation model from the on premise datacenter application to the cloud based SaaS model."
pub.1022923746,FitScale: Scalability of Legacy Applications Through Migration to Cloud,"One of the key benefits of Cloud computing is elasticity, the ability of the system infrastructure to adapt to the workload changes by automatically adjusting the resources on-demand. Horizontal scaling refers to the method of adding or removing resources from the resource pool. As such it is appealing to enterprises who seek to migrate their legacy systems as it requires no application rewrite or refactoring. Vertical scaling approach offers a mechanism to maintain continuous performance while reducing resource cost through reconfiguration of the resource. The challenge is, however, in being able to automatically identify the right size of the target resource such as a VM or a container. Moreover, choice of scalability policies is not intuitive due to application complexity, topology and variability in system performance parameters that need to be considered.This paper presents a transformation model, FitScale, which provides scalability with minimum price of resources. The paper describes the framework that employs the application functional and operational properties to recommend the target sizing and scalability policies. We evaluate proposed approach in an on-premise and cloud environments, with a dataset of 2023 servers hosting 6737 applications. The experimental results show about 5 times cost reduction with minimum performance impact."
pub.1181459702,Migration from On-Premises to Cloud: Challenges and Opportunities,"Cloud computing is a well-established technology that has been already widely used due to its extensive benefits. However, with many systems still relying on traditional architectures, existing literature has focused on aiding in their migration process. Nonetheless, comprehensive studies integrating both white and grey literature to assist professionals and researchers in understanding strategies for migrating legacy systems to the cloud are lacking. We addressed this gap by identifying challenges and opportunities related to migrating from on-premises to a cloud environment. Following this, we first conducted a systematic literature mapping to summarize the knowledge regarding migrating legacy systems to the cloud. Then, we performed an exploratory analysis of discussions on Stack Overflow and other question-and-answer (Q&A) communities within the Stack Exchange network to gather professionals’ perspectives on this topic and compare these perspectives with the knowledge found in the literature. Finally, we developed a Proof-of-Concept (PoC) of a decision support tool using a Large Language Model (LLM) that provides targeted responses to questions about migrating legacy systems to the cloud, enhanced by the Retrieval-Augmented Generation (RAG) method."
pub.1041274834,Report of the 2013 IEEE 7th international symposium on the maintenance and evolution of service-oriented and cloud-based systems (MESOCA 2013),"The 2013 IEEE 7th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems (MESOCA 2013) took place in Eindhoven, The Netherlands, on September 24, 2013, as a co-located event of the 29th IEEE International Conference on Software Maintenance (ICSM 2013). MESOCA 2013 covered a wide range of academic and industrial experiences, brought together through one keynote, two invited presentations and eleven paper presentations, which triggered lively discussions. They approached aspects related to the entire software maintenance process, from requirements to testing, with specific solutions for Service-Oriented Architecture and Cloud Computing environments. Technical and business perspectives were discussed, including issues about optimization techniques, pre-migration evaluation of legacy software, decision analysis, energy efficiency, multi-cloud architectures and adaptability. It thus confirmed MESOCA as an ongoing forum for researchers and practitioners to identify and address the increasing challenges related to the evolution of service-provisioning systems."
pub.1123774407,Migrating from Monoliths to Cloud-Based Microservices: A Banking Industry Example,"As organizations are beginning to place cloud computing at the heart of their digital transformation strategy, it is important that they adopt appropriate architectures and development methodologies to leverage the full benefits of the cloud paradigm. A mere “lift and move” approach, where traditional monolith applications are moved to the cloud will not support the demands of digital services. While monolithic applications may be easier to develop and control, they are inflexible to change to become more suitable for cloud environments. Microservices architecture, which adopts some of the concepts and principles of service-oriented architecture, provides a number of benefits, when developing an enterprise application, over a Monolithic architecture. Microservices architecture offers agility, faster development and deployment cycles, scalability of selected functionality and the ability to develop solutions using a mixture of technologies. Microservices architecture aims to decompose a monolithic application into a set of independent services which communicate with each other through open APIs or highly scalable messaging. In short, Microservices architecture is more suited for building agile and scalable Cloud-based solutions. This chapter provides a practice-based view and comparison between the monolithic and Microservices styles of application architecture in the context of cloud computing vision and proposes a methodology for transitioning from monoliths to Cloud-basedMicroservices."
pub.1094038694,Moving Business Intelligence to Cloud Environments,"Business Intelligence systems use information technology to supply integrated management support with data coming from several sources of structured and unstructured data. The integrated infrastructures of Business Intelligence (BI) are often too complex and hence costly and inflexible. A solution for these issues is to leverage cloud computing services to enhance legacy BI systems and applications with cost-efficient increased scalability and flexibility. However, the migration of BI systems to cloud environments is usually hindered by strict requirements regarding privacy, security, or availability and a multitude of interdependences with other systems. In this paper, we describe the challenges in the adoption of BI within cloud environments and propose a cloud migration framework to assist decision makers in taking into account the consequences of the migration of BI systems to cloud environments as well as the impact of privacy, security, cost, and performance in so doing."
pub.1104648915,Architecting Enterprise Applications for the Cloud: The Unicorn Universe Cloud Framework,"Recent IT advances that include extensive use of mobile and IoT devices and wide adoption of cloud computing are creating a situation where existing architectures and software development frameworks no longer fully support the requirements of modern enterprise application. Furthermore, the separation of software development and operations is no longer practicable in this environment characterized by fast delivery and automated release and deployment of applications. This rapidly evolving situation requires new frameworks that support the DevOps approach and facilitate continuous delivery of cloud-based applications using micro-services and container-based technologies allowing rapid incremental deployment of application components. It is also becoming clear that the management of large-scale container-based environments has its own challenges. In this paper, we first discuss the challenges that developers of enterprise applications face today and then describe the Unicorn cloud framework (uuCloud) designed to support the development and deployment of cloud-based applications that incorporate mobile and IoT devices. We use a doctor surgery reservation application “Lekar” case study to illustrate how uuCloud is used to implement a large-scale cloud-based application."
pub.1095587055,AppCloak: Rapid Migration of Legacy Applications into Cloud,"Although cloud has been adopted by many organizations as their main infrastructure for IT delivery, there are still a large number of legacy applications running in non-cloud hosting environments. Thus, it is crucial to have migration techniques for such legacy applications so that they can benefit from many advantages of cloud such as elasticity, low upfront investment, and fast time-to-market. However, migrating large number of legacy applications into cloud in a timely manner is a daunting task. Common techniques such as redeveloping (i.e., modernizing) them or reinstalling from the scratch entails high costs. To mitigate these problems, we have developed a rapid migration technique, called AppCloak, that allows users to literally copy an already-installed application to cloud and run it without any modifications. The technique is based on intercepting a selected set of system calls and replacing the parameters and return values to hide any differences of environments to the application. We demonstrate that our technique works in Amazon EC2 and quantify the performance overhead."
pub.1038976795,Dynamic Virtual Cluster Reconfiguration for Efficient IaaS Provisioning,"Cloud computing is an emerging paradigm to provide Infrastructure as a Service (IaaS). In this paper we present NEPTUNE-IaaS, a software system able to support the whole lifecycle of IaaS provisioning in a Virtual Cluster environment. Our system allows interactive design of complex system topologies and their efficient mapping onto the available physical resources of a cluster. It also provides transparent VM migration features across geographically distributed datacenters, thanks to the adoption of the Service Switching paradigm. We also evaluate the effectiveness of the VM mapping procedures and compare our solution against other existing IaaS solutions."
pub.1015258789,"Cloud migration process—A survey, evaluation framework, and open challenges","Moving mission-oriented enterprise software applications to cloud environments is a crucial IT task and requires a systematic approach. The foci of this paper is to provide a detailed review of extant cloud migration approaches from the perspective of the process model. To this aim, an evaluation framework is proposed and used to appraise and compare existing approaches for highlighting their features, similarities, and key differences. The survey distills the status quo and makes a rich inventory of important activities, recommendations, techniques, and concerns that are common in a typical cloud migration process in one place. This enables both academia and practitioners in the cloud computing community to get an overarching view of the process of the legacy application migration to the cloud. Furthermore, the survey identifies a number challenges that have not been yet addressed by existing approaches, developing opportunities for further research endeavours."
pub.1104218296,A New Method for Migrating Legacy Applications to the Cloud: A Finite State Process Approach,"Using the cloud computing helps the enterprises to reduce their operational costs as well as to improve the scalability, availability and reliability of their services. To take advantages of this technology, enterprises have to decide how to migrate their on-premise applications to the cloud. The problem of migrating a legacy application to the cloud is very complicated and time-consuming process, due to the complexity of applications, dynamic environment of the enterprises and the variety of available cloud services. Despite many researches in this context, a formal migration model based on known patterns has not been presented yet. In this paper, Finite State Process (FSP) algebra is applied as a formal basis by which a step by step migration support system can be built automatically from the known application and cloud profiles. The proposed step by step migration model is superior to the current optimization methods that search the optimal deployment of application components to cloud services due to the fact that a step by step approach is more appropriate for dynamic environments."
pub.1152284680,The vendor-agnostic EMPAIA platform for integrating AI applications into digital pathology infrastructures,"Automated image analysis and artificial intelligence (AI) are becoming increasingly common in digital pathology software. While various proprietary pathology systems exist, there are no fully vendor-agnostic integration approaches for AI apps. This makes it difficult for vendors of AI solutions to integrate their products into the multitude of non-standard software systems in pathology. The EMPAIA Consortium is developing an open and decentralized platform allowing AI-based apps of different vendors to be integrated with existing clinical IT infrastructures. For this purpose, we defined, analyzed, and prioritized relevant use cases and identified requirements for an open platform to support these use cases. We then designed the platform architecture described here to meet these requirements based on web technologies. For all platform services open source reference implementations are available, that are used by developers of AI apps as an integration target. Developers of compatible clinical systems can either use and integrate components of the reference implementation or directly implement the interfaces as per specification, allowing apps to run in their clinical environment. Pathology laboratories can use both on-premises and cloud deployments of the platform. Apps can be obtained via a central marketplace so that pathologists can use them in their daily workflow. An adoption of this platform will enable interoperability among different existing digital pathology software systems. This reduces integration efforts for software vendors, while users will benefit from a wider variety of tools and a quicker availability of new and innovative methods. Ultimately, the platform will reduce barriers to market entry for AI vendors and provide pathologists with access to advanced AI tools."
pub.1093694541,Cloudstep: A Step-by-Step Decision Process to Support Legacy Application Migration to the Cloud,"Cloud computing is an emerging computing paradigm whose benefits (such as high scalability., reduced IT costs., self-service on demand., and pay-as-you-go price models) have increasingly attracted the interest of the corporate world. Nevertheless., many organizations have found it difficult to adopt cloud-based solutions., particularly regarding the migration of their existing legacy applications to this new environment. One of the main obstacles faced by those organizations is the lack of a general process to help application developers not only in selecting the cloud models and services best suited for their application., but also in carefully assessing the various risks and benefits involved. To fill this gap., this paper presents Cloudstep., a step-by-step decision process aimed at supporting legacy application migration to the cloud. The process relies on the creation of template-based profiles characterizing the organization., the target legacy application and candidate cloud providers., which are then cross-analyzed to help identify and possibly resolve critical constraints (either technical or non technical) that may hinder migration to the cloud. The use of the process is illustrated through an analysis of key factors influencing the migration of a commercial medical application to different infrastructure-as-a-service cloud providers."
pub.1173102184,CLOUD COMPUTING - NAVIGATING THE DIGITAL SKY,"“Cloud Computing – Navigating the Digital Sky” is an extensive guide designed to provide a thorough understanding of cloud computing, an essential technology in today’s digital age. The book is structured into ten comprehensive chapters, each delving into various aspects of cloud computing, from its foundational principles to future trends. The journey begins with Chapter 1: Introduction to Cloud Computing, which traces the evolution of cloud computing and outlines its key characteristics and underlying principles of parallel and distributed computing. This chapter also discusses the elasticity of cloud services, on-demand provisioning, and the benefits and drawbacks of migrating to the cloud. It highlights current trends, the impact on digital transformation, and the importance of cloud standards and compliance. Chapter 2: Cloud Enabling Technologies Service Oriented Architectures explores Service Oriented Architecture (SOA), REST, web services, and the publish-subscribe model. It covers the basics of virtualization, including its characteristics, types, implementation levels, and structures. Tools and mechanisms for virtualization, such as CPU, memory, and I/O devices virtualization, are also discussed, along with disaster recovery support. In Chapter 3: Cloud Architecture, Services, and Storage, readers are introduced to layered cloud architecture design, NIST reference architecture, and various deployment models like public, private, hybrid, and community clouds. The chapter delves into service models (IaaS, PaaS, SaaS), microservices architecture, and the challenges of architectural design. It also covers cloud storage solutions, multi-cloud strategies, cloud bursting, and the integration of edge computing with cloud services. Chapter 4: Resource Management, Security in Cloud and Compliance addresses the critical aspects of resource management and security in the cloud. It discusses inter-cloud resource management, provisioning methods, data encryption, privacy, IAM, network security, and software-as-a-service security. Compliance regulations, incident response, best practices, security audits, and assessments are also covered to ensure a secure cloud environment. Chapter 5: Cloud Migration Strategies provides insights into assessing workload suitability for cloud migration, lift and shift migration, replatforming, refactoring, and data migration strategies. It emphasizes the tools and technologies for migration, managing legacy systems, and optimizing during migration. Chapter 6: Cloud Economics and Cost Management is dedicated to understanding cloud costs, comparing pay-as-you-go vs. reserved pricing, and performing total cost of ownership (TCO) analysis. It outlines various billing models, cost optimization strategies, and the role of financial operations (FinOps) in budgeting and forecasting. The chapter also introduces cloud cost management tools. Chapter 7: Cloud Networking and Connectivity explores virtual private clouds (VPCs), cloud"
pub.1172627936,"Review of Cloud Migration Strategies: Exploring Advantages, Challenges and Cost Analysis","Cloud migration has emerged as a strategic im- perative for organizations seeking to leverage the scalability, flexibility, and cost efficiencies offered by cloud computing. This paper presents a comprehensive survey of organizational cloud migration strategies, focusing on the advantages, challenges, and cost analysis associated with this transformative process. The study begins by elucidating the conceptual framework of cloud migration, delineating its fundamental principles and terminolo- gies. Subsequently, it delves into the myriad advantages that com- pel organizations to migrate from on-premises infrastructures to cloud-based solutions. These advantages include enhanced scalability, improved resource utilization, and accelerated time- to-market for new initiatives. However, the migration journey is fraught with challenges, ranging from data security concerns to compatibility issues with legacy systems. This paper meticulously examines these challenges and elucidates strategies employed by organizations to overcome them, thereby ensuring a smooth tran- sition to the cloud. Cost analysis constitutes a pivotal aspect of cloud migration decision-making. This study offers insights into the cost implications associated with various cloud deployment models, such as Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). By conducting a comparative analysis of capital expenditures (CapEx) and op- erational expenditures (OpEx) incurred in both on-premises and cloud environments, organizations can make informed decisions regarding cost optimization and resource allocation. Further- more, the paper explores the critical considerations surrounding data security, regulatory compliance, and vendor selection in the context of cloud migration. Real-world case studies are presented to illustrate successful migration strategies adopted by leading organizations across diverse industries. Keywords —Cloud migration, Cost analysis, Strategies, Re- hosting, Replatforming, Refactoring, Repurchasing, Retiring, Retaining"
pub.1172789223,Leveraging the Resiliency of the Cloud to Efficiently Operate Existing Multicast-Dependent Workloads,"Air Traffic Management (ATM) has been and continues to be the cornerstone to keeping our skies safe, with sizable investments over the last two decades to upgrade and update the National Airspace System (NAS) to maintain that charter. To continue to safely operate and increase the efficiency, decision making, and situation awareness ATM is currently charting a path for the future that will transform how future technologies are developed, implemented and how data/information is shared. In existing ATM automation systems, each system independently maintains its own technological infrastructure elements (e.g., hardware, operating systems, data management tools). This construct does not allow for the system to take advantage of shared, enterprise infrastructure elements and methodologies. Therefore, an evolution from these monolithic systems to enterprise services that can leverage layered service-based architectures with horizontal integration that supports broad sharing of mission services and data across an enterprise is the future for ATM. While this large of an endeavor will need many different capabilities and technologies to support the transition to the use of Cloud has been identified as a key enabler, this includes both fully off-premise cloud-based environment and a hybrid of off-premise and on-premise cloud environments. The goal of this transition is to support scalability, availability, and shared common data flows/infrastructure, optimize operational costs, and continue to improve ATM services. The last few years Cloud Service Providers (CSPs) have been working to provide new features and services that allow software that was previously created/developed before the Cloud to be able to be migrated to the cloud. By using a Lift-and-Shift approach which still takes advantage of the elasticity and dynamic resiliency that the cloud provides. The FAA has made huge investments over the last two decades to create applications currently being used to keep the skies safe and operationally rely on technology that was cutting-edge at the time the applications were written which in some cases could be more than a decade ago. While a lot of effort is being put into planning the replacement of these applications with Cloud Native versions of them, these efforts are likely to take many years to complete. At L3Harris we believe that the use of Cloud services can offer immediate benefits in support of many ATM missions but agree that investigation was needed to address the obvious questions around performance. Therefore, L3Harris has partnered with Amazon Web Services (AWS) to develop a series of Proof of Concept (POC) activities, that have allowed for a better understanding of the current capabilities of the to host existing applications. The current POC activities are exploring areas such as recasting existing capabilities/services related to surveillance in a Govcloud environment. Existing applications could be rehosted in the cloud with very little "
pub.1131925900,A Systematic Review on Software Architectures for IoT Systems and Future Direction to the Adoption of Microservices Architecture,"The Internet of Things-based systems and software allow computations anywhere at any time by interconnecting individuals, networks, services, computers and artefacts that allow autonomous systems to form digitized communities. As the blueprint for software-intensive applications, and software architecture that precise the complexity of a network’s planning, development, and changing phases to effectively and efficiently build complex IoT-driven applications. In any case, there exists no comprehensive analysis in the state of the research on the adoption of MSA for IoT systems. This study effort is needed to explore architectural concepts and practices for designing and developing IoT software to excel state-of-the-art for IoTs along with suggestions and recommendations for IoT software to the adoption of MSA to fulfil the identified gaps. A systematic analysis was coordinated, covering up the literature on existing IoT solutions by studying 140 qualitatively selected articles performed between 2005 and Jan 2020. One hundred forty articles were comprised in this SLR. The findings of this study demonstrated different research topics including software architectural styles, patterns, and models to build IoT software. This research presents cloud-based computing environments, autonomous, software-defined networking, and responsive applications, and IoT-driven agent-based systems, (1) thirteen MSA architectural and design patterns for IoTs and classification of patterns, (2) classification of software architectures for IoTs into nine main categories and their sub-categories, (3) twenty-three most investigated IoT challenges, and (4) mapping of IoT challenges with software architectural solutions. The study revealed the innovative work on IoT software architecture and trends that help in the creation and dynamic adaptation of IoT software for reusability, automation and human decision-making. The outputs of this SLR are useful in revealing many recommendations to the software industry, software engineering community, and computer sciences community with over the past 15 years of research into the adoption of MSA. This study reflects a distilled awareness of architectural practices and principles to assist researchers and practitioners in promoting information sharing software architectural roles and responsibilities for the Internet of Things software."
pub.1144088760,From Monolithic to Microservice Architecture: The Case of Extensible and Domain-Specific IDEs,"Integrated Development Environments (IDEs) are evolving towards cloud-native applications with the aim to relocate the language services provided by an IDE on distant servers. Existing research works focus on the overall migration process to handle more efficiently their specific requirements. However, the microservicization of legacy monolithic applications is still highly dependent on the specific properties of the application of interest. In this paper, we report our experiment on the microservicization process of the Cloud-Based graphical modeling workbench Sirius Web. We aim to identify the technical challenges related to applications with similar properties, and provide insights for practitioners to migrate their similar applications towards microservices. We discuss the main lessons learned and identify the underlying challenges to be further addressed by the community."
pub.1173476139,Navigating the Cloudscape: Framework and Strategies for Seamless Migration to the Cloud,"Enhancing agility, scalability, and cost-efficiency in IT infrastructure are some of the key objectives that push companies to consider cloud migration as a crucial strategy. This article gives an introduction about cloud migration illustrating why it is significant, difficulties experienced and its advantages. The method of moving on-premises applications, data, and infrastructure to cloud-based environments involves more preparation, evaluation and implementation to achieve seamless transition while minimizing disruptions to business operations. These must include choosing among the three main categories of service delivery models: Platform as a Service (PaaS), Software as a Service (SaaS), and Infrastructure as a Service (IaaS). Selecting the best provider for cloud computing services for a business deal with security issues compliantly and achieve optimization of different performance levels at minimum costs. This process has its share of challenges such as data migration complexities, application compatibility issues and organizational resistance towards change. In spite of this, there are ways that can be followed to unlock the full potential in cloud computing so that innovation, agility and competitive advantage may be realized in today’s digital world."
pub.1094299006,Service Migration Patterns,"In many ways cloud computing is an extension of the service-oriented computing (SOC) approach to create resilient and elastic hosting environments and applications. Service-oriented Architectures (SOA), thus, share many architectural properties with cloud environments and cloud applications, such as the distribution of application functionality among multiple application components (services) and their loosely coupled integration to form a distributed application. Existing service-based applications are, therefore, ideal candidates to be moved to cloud environments in order to benefit from the cloud properties, such as elasticity or pay-per-use pricing models. In order for such an application migration and the overall restructuring of an IT application landscape to be successful, decisions have to be made regarding (i) the portion of the application stack to be migrated and (ii) the process to follow during the migration in order to guarantee an acceptable service level to application users. In this paper, we present best practices how we addressed these challenges in form of service migration patterns as well as a methodology how these patterns should be applied during the migration of a service-based application or multiples thereof. Also, we present an implementation of the approach, which has been used to migrate a web-application stack from Amazon Web Services to the T -Systems cloud offering Dynamic Services for Infrastructure (DSI)."
pub.1148518924,SPIRIT: A Microservice-Based Framework for Interactive Cloud Infrastructure Planning,"The IaaS model provides elastic infrastructure that enables the migration of legacy applications to cloud environments. Many cloud computing vendors such as Amazon Web Services, Microsoft Azure, and Google Cloud Platform offer a pay-per-use policy that allows for a sustainable reduction in costs compared to on-premise hosting, as well as enable users to choose various geographically distributed data centers. Using state-of-the-art planning algorithms can help application owners to estimate the size and characteristics of the underlying cloud inveterate. However, it’s not always clear which is the optimal solution especially in multi-cloud environments with complex application requirements and QoS constraints. In this paper, we propose an open framework named SPIRIT, which allows a user to include cloud infrastructure planning algorithms and to evaluate and compare their solutions. SPIRIT achieves this by allowing users to interactively study infrastructure planning algorithms by adjusting parameters via a graphical user interface, which visualizes the results of these algorithms. In the current prototype, we have included from the IaaS Partial Critical Path algorithm. By taking advantage of SPIRIT’s microservice-based architecture and its generic interfaces a user can add to the framework, new planning algorithms. SPIRIT can transform an abstract workflow described using the CWL to a concrete infrastructure described using the TOSCA specification. This way the infrastructure descriptions can be ranked on various key performance indicators."
pub.1151907008,Cloud Migration: Prospects and Impediments With Special Reference to India,"<p>With regard to the scientific world of today, cloud computing is the most important repository for resource e parking and offers services to active users in the form of infrastructure, software, and platforms. Resources are online and accessible to anyone with an internet connection in a cloud computing environment. Since cloud computing makes resources easily accessible online, it is a growing sector of the ICT business. As a result, enterprises are quickly adopting this more recent technology, or cloud computing. The transition of a business from on-premises to a cloud computing environment includes benefits and challenges. The potential and barriers to different firms' adoption of cloud computing will be examined in this study report.</p>"
pub.1041721654,Security issues of cloud computing environment in possible military applications,"The evolution of cloud computing over the past few years is potentially one of major advances in the history of computing and telecommunications. Although there are many benefits of adopting cloud computing, there are also some significant barriers to adoption, security issues being the most important of them. This paper introduces the concept of cloud computing; looks at relevant technologies in cloud computing; takes into account cloud deployment models and some military applications. Additionally, Security-as-a-Service is specially considered because of its importance in the military applications. Cloud computing (CC) is such a combination of computer hardware, software, online services and expertise networked by high-speed Internet connections. What is more, cloud computing is a model of using computing resources (servers, disks, operating systems and applications) where these resources are rented and not bought. The consequence of this approach is that users pay only as many resources as they actually have used (pay-as-you-go), so they do not need to take care about either hardware procurement and its installation, or software (operating system and applications) maintenance. Most of the cloud computing architecture used nowadays includes public cloud computing networks for providing services through the Internet, such as Google Search, Microsoft Hotmail or Google Adsense. Large service providers, together with typical pioneers in adopting new technologies such as financial services and pharmaceutical companies, also apply the architecture of CC in the implementation of private cloud networks protected by firewalls. This mode of use is still in its early stages and is expected to achieve further growth in the corporate database virtualization technologies that are now being introduced. The framework used to describe cloud computing services is known as the acronym SPI (Software Platform Infrastructure) and marks the three major services provided through the clouds: Softwareas- a-Service (SaaS), Platform-as-a-Service (PaaS) and Infrastructureas- a-Service (IaaS). Traditional methods of purchasing software involve software upgrading onto hardware with paying license fees (which is capital expenditure). The user is also able to enter into any contract for maintenance and to get security patches and other services for software support at a certain price. The compatibility of operating systems and the installation of patches is the responsibility of the user. Being a simplified representation of complex, interconnected devices and connections that form the Internet, the term 'cloud' is a metaphor for the Internet. Public and private (external and internal) clouds are the parts of the Internet and are defined (or, rather, they differ) based on the connections formed by institutions. Similar to SaaS, the Security-as-a-Service cloud service model is delivered to customers on a subscription basis. In addition, SaaS is also applied in the Security-as-a"
pub.1158322989,CMP-SiL: Confidential Multi Party Software-in-the-Loop Simulation Frameworks,"Increasing complexity of systems and software in the automotive industry, coupled with distributed development environments has intensified adoption of Software in the Loop (SIL) systems, i.e. setup where traditional hardware components are designed and tested in pure virtual PC/IT environment consisting of virtualized hardware and networks. Cloud-based SiL simulation systems involving multiple contributors and orchestrators create huge risks for organizations due to potential for leakage of confidential model-IP to adversaries within the distributed infrastructure. This can create a bottleneck for wide-scale adoption of SiL-systems. We propose a data-flow architecture using trusted-computing technologies (e.g. Intel-SGX) to protect models and IP in cloud-based SiL environments. We illustrate that these protections can be designed to be compatible with existing SiL tools and workflows with minimal modifications. Further, we highlight the need for future standardization efforts of such security architectures in the SiL domain."
pub.1175567481,Microservices architecture in cloud-native applications: Design patterns and scalability,"Microservices architecture has emerged as a pivotal approach for designing scalable and maintainable cloud-native applications. Unlike traditional monolithic architectures, microservices decompose applications into small, independently deployable services that communicate through well-defined APIs. This architectural shift enhances modularity, allowing for improved scalability, resilience, and flexibility. This paper explores the core concepts of microservices, including service decomposition, inter-service communication, and data management. It delves into key design patterns such as the API Gateway, Circuit Breaker, Service Discovery, and Strangler Fig patterns, illustrating how these patterns address common challenges in microservices architecture. The discussion emphasizes the importance of these patterns in managing service interactions, ensuring fault tolerance, and facilitating gradual migration from legacy systems. Scalability is a major focus, with an examination of horizontal scaling techniques, load balancing strategies, and elasticity in cloud environments. The paper highlights best practices for scaling microservices, including auto-scaling policies and integration with cloud platforms like AWS, Azure, and GCP. Additionally, the paper addresses challenges such as complexity management, security considerations, and testing strategies. Real-world case studies provide insights into successful implementations and lessons learned. Finally, the paper considers emerging trends and future directions in microservices architecture, emphasizing its role in advancing modern application development. This exploration offers a comprehensive understanding of how microservices architecture can be effectively employed in cloud-native applications to achieve scalability and resilience.
 Keywords: Microservices, Architecture, Cloud-Native Applications, Design Patterns, Scalability."
pub.1165985801,A Micro-Service Approach to Cloud Native RAN for 5G and Beyond,"5G aims to support diverse applications with programmable infrastructure. Traditional RAN based on purpose-built-in hardware and monolithic software lack resiliency, programmability, and business agility. Cloud native virtualized RAN (vRAN) solves the issues by designing telecom applications into micro-service in cloud environment. This enables flexible virtualized network function deployment, efficient service provisioning, and on-demand resource usage. However, designing telecom RAN applications as micro-services has no guidelines but faces challenges such as high-precision synchronization and real-time processing requirements. This paper first introduces cloud native tenets and the current state of RAN cloudification and then evaluates the micro-service design of RAN software components. A micro-service approach for vDU user plane was proposed in this study, and a vDU prototype was developed based on the Intel x86 computing platform. System validation results proved the feasibility of inter-Pod communication processing latency, and capacity analytics predicted substantial capacity improvement for the proposed vDU scheme. The paper concludes with a summary and open points on the way forward for cloud native vRAN transformation."
pub.1163096280,Revolutionizing Cloud Infrastructure Management: Streamlined Provisioning and Monitoring with Automated Tools and User-Friendly Frontend Interface,"Cloud infrastructure provisioning has become a crucial aspect of the software development lifecycle, as companies shift from on-premise server management to cloud-based services. This transition allows organizations to focus on their core business rather than worrying about server and hardware management. The DevOps culture has further enhanced this approach by utilizing automated processes for delivering applications across various environments. Continuous Integration (CI) and Continuous Delivery (CD) have revolutionized application development and release management, facilitating seamless delivery with integrated feedback. To automate various tasks in the CI/CD framework, the use of GitHub Workflow has been proposed. It allows defining workflows as a series of jobs and triggering them automatically based on specific events, thus enhancing the efficiency and reliability of the development process. Additionally, its pre-built and custom actions provide added flexibility and functionality, making it a valuable tool for modern software development teams. This paper demonstrates the feasibility of designing an effective framework that achieves continuous integration, testing, and delivery of cloud resource provisioning. By employing a build pipeline concept, the framework can automate various tasks such as source code compilation, code analysis, test execution, packaging, infrastructure provisioning, deployment, and notifications. This brings an idea to develop a simple software solution that will use GitHub Actions in the CI/CD pipeline, speed up the process of provisioning cloud infrastructure through a simple frontend and track the resource utilization of provisioned resources through a dashboard, so delivery of cloud infrastructure will happen quickly."
pub.1093628933,Moving to the Cloud: Workload Migration Techniques and Approaches,"The paper presents an important aspect of cloud computing technology, namely migrating enterprise level workloads to a cloud environment, without re-architecting or re-engineering the existing applications. How readily an application can be lifted and shifted onto a cloud platform depends on factors like nature of the application, the type of cloud etc. In this respect, the paper explores the methodology of migrations along with the challenges and issues that usually acts as a barrier for organizations trying to pursue this goal. An effort is also made to see how the cloud migration framework maps to the cloud Computing Reference Architecture model. Finally, a set of migration patterns which span the continuum from legacy IT environment to the cloud are included as a common framework for aligning the various migration approaches developed in support of using cloud as a delivery paradigm."
pub.1113143756,Toward Adapting Metamodeling Approach for Legacy to Cloud Migration,"Migration of legacy application to Cloud is a fast-growing area of knowledge. Many IT-based organizations inclined toward empowering their legacy application with cloud computing capabilities. Many researchers, academicians, national, and international bodies are creating knowledge models to allow knowledge sharing and provide effective cloud migration model. This knowledge is scattered and huge, but lack of knowledge management. Our motive is to produce a metamodel, which could be able to generalize the cloud migration domain. Metamodel approach is an approach, to gather all domain concepts and their relationships. Using the metamodel, variety of domain solution models can be built. It can act as a language infrastructure which unifies describing the process model of moving legacy enterprise applications to the cloud environments. The benefits of the metamodel include simplifying the migration process, guidance, reuse specialized migration knowledge and support training and knowledge management activities. Furthermore, it reduces complexity and ambiguity in cloud migration domain."
pub.1068569268,Migration of applications to the Cloud: a user-driven approach,"During the last decade, there has been an increased interest on cloud computing and especially on the adoption of public cloud services. The process of developing cloud-based public services or migrating existing ones to the Cloud is considered to be of particular interest—as it may require the selection of the most suitable applications as well as their transformation to fit in the new cloud environment. This paper aims at presenting the main findings of a migration process regarding smart city applications to a cloud infrastructure. First, it summarises the methodology along with the main steps followed by the cities of Agueda (Portugal), Thessaloniki (Greece) and Valladolid (Spain) in order to implement this migration process within the framework of the STORM CLOUDS project. Furthermore, it illustrates some crucial results regarding monitoring and validation aspects during the empirical application that was conducted via these pilots. These findings should be received as a helpful experience for future efforts designed by cities or other organisations that are willing to move their applications to the Cloud."
pub.1095128285,DATABASE BACKED BY CLOUD DATA STORE FOR ON-PREMISE APPLICATIONS,"In the tide called Cloud Computing, people move their applications previously running on on-premise servers to the Cloud. However, this migration is not a comfortable trip. Compatibility is a critical issue of the new environment. It is reasonable for the enterprises to decide to move certain portions of their IT infrastructure to the Cloud. And database is one important piece that may be migrated. In this article, a database model backed by cloud data-store which can be consumed by on-premise application is introduced. This works addresses several issues to provide the on-premise applications access to data-store in the cloud while guarantees efficiency and compatibility."
pub.1095112646,Assessing the Cloud Migration Readiness A Fuzzy AHP Approach Based on BTR Framework,"As more and more organizations consider adopting private cloud, how to assess the readiness of moving legacy application systems to the cloud environment is a key issue faced by the practitioners. There is lack of comprehensive view on the factors affecting cloud migration at the system level. In this paper, a holistic cloud migration readiness decision framework along with the business, technical and risk perspective has been developed and aim to investigate the suitability of migrating enterprise application systems to the cloud environment based on the fuzzy Analytic Hierarchy Process (AHP) approach. This paper can advance new knowledge accumulation in the field of research on the private cloud adoption and implementation."
pub.1140184945,"Autonomic Computing: Models, Applications, and Brokerage","Autonomic computing is not a core, rather a convergence of numerous concepts and supporting technologies. It is the junction that integrates salient computing domains and subdomains to create a self-driven, self-healing, and self-manageable computing environment. The possible integration, exploration, and hybridization toward the development of the new models and applications are new knowledge contributions. Modeling is a conceptualization of autonomic computing in general and contextualization in specific applications. In the context of cloud-based autonomic computing, a model presents the fact that the operations of the autonomic computing systems are goal-oriented and driven by certain activities and follow certain policies and behavioral aspects, with the existing features. Currently, client organizations or individual clients prefer to use packaged computing products and services over the cloud or distributed systems. This chapter covers how autonomic computing models hold the promising features for simplification, and the ease of computing system management over clouds such as process management, autonomic client migration for load balancing, monitoring, energy efficiency(green), automatic updating of software tools/drivers, predictive warning before failure, error detection and correction, backups, and recovery from sudden disasters. This chapter also covers the possible applications and related issues encountered during adaption and adoption at salient scales and types of organizations. The summarized feature-based comparative analysis of the existing computing and emerging autonomic models are also incorporated. A cloud-based green broker model for cloud service selection is designed and incorporated for the autonomic brokerage of green cloud services. Furthermore, the applications of the autonomic process management architecture to salient applications such as governance, commerce, management, industrial automation, etc. are included."
pub.1016273967,Dynamic reconfiguration of cloud application architectures,"Summary  Service‐based cloud applications are software systems that continuously evolve to satisfy new user requirements and technological changes. This kind of applications also require elasticity, scalability, and high availability, which means that deployment of new functionalities or architectural adaptations to fulfill service level agreements (SLAs) should be performed while the application is in execution. Dynamic architectural reconfiguration is essential to minimize system disruptions while new or modified services are being integrated into existing cloud applications. Thus, cloud applications should be developed following principles that support dynamic reconfiguration of services, and also tools to automate these reconfigurations at runtime are needed. This paper presents an extension of a model‐driven method for dynamic and incremental architecture reconfiguration of cloud services that allows developers to specify new services as software increments, and the tool to generate the implementation code for the services integration logic and the deployment and architectural reconfiguration scripts specific to the cloud environment in which the service will be deployed (e.g., Microsoft Azure). We also report the results of a quasi‐experiment that empirically validate our method. It was conducted to evaluate their perceived ease of use , perceived usefulness , and perceived intention to use . The results show that the participants perceive the method to be useful, and they also expressed their intention to use the method in the future. Although further experiments must be carried out to corroborate these results, the method has proven to be a promising architectural reconfiguration process for cloud applications in the context of agile and incremental development processes. Copyright © 2016 John Wiley & Sons, Ltd. "
pub.1071883950,Towards AiP as a Service: An Agent Based Approach for Outsourcing Business Processes to Cloud Computing Services,"<p>The challenges that Cloud computing poses to business processes integration, emphasize the need for addressing two major issues: (i) which integration approach should be used allowing an adequate description of interaction aspects of the composed software components ? (ii) how are these interaction descriptions stored and shared to allow other software artifacts to (re)use them ? To address these issues, in this paper the authors propose an Agent Interaction Protocols (AiP)-based approach for reusing and aggregating existing Cloud services to create a new desired business application. The proposed approach facilitates rapid development and provisioning of composite Cloud services by specifying what to compose as an AiP. Furthermore, the authors develop an agent-based architecture that supports flexible scaling of business processes in a virtualized Cloud computing environment. The main goal of the proposed architecture is to address and tackle interoperability challenges at the Cloud application level. It solves the interoperability issues between heterogeneous Cloud services environments by offering a harmonized API. Also, it enables the deployment of applications at public, private or hybrid multi-Cloud environments.</p>"
pub.1048432014,Moving an application to the cloud,"When planning to move a legacy style application to the cloud various challenges arise. The potential size and complexity of such a project might especially discourage small or medium companies trying to benefit from the advantages the cloud promises. In addition, the field they have to address is still young and very dynamic and related technologies are rapidly changing. Based on on-going work in the context of the MODAClouds EU project, this paper describes an evolutionary, iterative approach to accomplish the migration of an existing application to a cloud based environment. Model based techniques are used to support the steps of this transition process by providing a baseline for the development of appropriate deployment architectures and the selection of suitable cloud providers. In addition they provide necessary abstractions in order to be less dependent on a specific technology stack or cloud provider. In order to show how we imagine the developed approach to be applied in practice we describe an existing traditional 3-tier application based on the meta-modeling platform ADOxx and how it could be moved to the cloud from the perspective of a medium-sized software manufacturing company."
pub.1041159094,User-centred cloud service adaptation: an adaptation framework for cloud services to enhance user experience,"Ubiquitous and cloud manufacturing systems have paved the way for the development of smart systems that can provide self-adaptive services to enhance quality of service and user experience, based on user and context awareness. However, current approaches to software adaptation have critical limitations when applied on software in a cloud environment, since cloud service engineers cannot be certain of the ‘nature’ of their users, regarding their needs and their evolution over time. This article proposes a user-centred framework that addresses self-adaptation of cloud services based on users’ distinct needs and requirements. It aims at leveraging existing human computer interaction approaches to explicitly and implicitly collect user-related data into the cloud. This collection later supports the delivery of adaptive cloud services, contextually relevant to users’ tasks and to their behavioural profiles. Such approaches have the potential to enhance the quality of cloud applications’ service and user experience in cloud-based manufacturing systems. A first prototype demonstration is also presented as a future platform composed of the first basic models (of the ones mentioned in this article) .Then, the platform will be further expanded for instantiation and evaluation in the context of cloud and ubiquitous computing environments."
pub.1093697682,Data Sharing in Data-Centric Multi-Tenant Software as a Service,"Cloud Computing include applications that are delivered as services using internet in the shape of infrastructure, systems or application software (SaaS) hosted at a remote computing facility. SaaS applications are deployed on a shared environment that can be accessed by its users from various client-end software by using Internet. Organizations using SaaS application do not have control over the infrastructure of the environment that is often built using multi-tenant system architecture. Usually data-centric multi-tenant applications provide a common VI for all the tenants or organizations and data of multiple tenants is saved in a single database. Core of these applications revolve around managing organizations data. For many reasons like mergers, joint marketing campaign, moving from pilot to production SaaS instance, we have to migrate data from one tenant to another. Data migration requires a very skilled and time consuming human effort and it results in data duplication. Therefore we propose a middleware for data sharing between different organizations, a design pattern for the implementation of the middleware, and extension to current multi-tenant database designs for SaaS applications which will help in eliminating human effort in data migration and avoiding data duplication. Our results shows that data can be shared between different tenants of a SaaS efficiently without making a significant change in existing architecture of SaaS."
pub.1134322772,A Conceptual View for an Enhanced Cloud Software Life-Cycle Process (CSLCP) Model,"Small to Medium-sized Enterprises (SMEs) benefit from the advantages of the cloud computing environment. These enterprises have limited resources. Consequently, SMEs require a structured software process model to develop a reliable and good quality cloud software. Existing cloud software process (CSP) models focused only on the processes of software development and ignored the other aspects of software production. In this paper, a conceptual view for Cloud Software Life Cycle Process (CSLCP) model is proposed. This model overcomes the deficiencies of existing CSP models. Also, it satisfies the development of a reliable and high quality cloud software. The CSLCP model is compatible with level two and three of the capability maturity model integration (CMMI). It extends the software process improvement (SPI) model, developed in Egypt for SMEs, to suit the cloud environment. The application of the CSLCP model in SMEs would improve their level of maturity"
pub.1137654739,Applying Patterns to Support Deployment in Cloud-Edge Environments: A Case Study,"A major trend followed by IT experts and Software developers in recent years is represented by the “Cloudification” of existing applications, with a strong shift of computations and data from local and centralized servers to remote, distributed data-centers. Indeed, using Cloud resources has reduced, for most SMEs, both the initial investments in hardware and software assets and maintenance costs, making it a viable choice in many situations. On the other hand, Cloud Computing requires to store consistent volumes of data on remote databases, with a series of consequences on data privacy that need to be carefully addressed. Moreover, the advent of the Internet of Things, with the huge quantity of data that smart devices continuously produce and consume, often in real time, renders the transfer of information to and from remote servers too cumbersome, as it relies on network speed and continuous availability. New programming paradigms have thus emerged, such as Cloud-Edge, which tries to combine benefits deriving from the exploitation of the resources offered by Cloud architecture and the need to consume data locally. The Cloud-Edge paradigm requires a careful design of the integration between Cloud and Edge architectures, in order to avoid bottlenecks and efficiently exploit both local and remote resources. In this paper a methodology based on Architectural, Computational and Deployment Patterns will be presented to support the deployment of applications in Cloud-Edge environments, starting from pre-existing software solutions."
pub.1110198106,An integration approach of hybrid databases based on SQL in cloud computing environment,"Summary As the applications with big data in cloud computing environment grow, many existing systems expect to expand their service to support the dramatic increase of data, and modern software development for services computing and cloud computing software systems is no longer based on a single database but on existing multidatabases and this convergence needs new software architecture design. This paper proposes an integration approach to support hybrid database architecture, including MySQL, MongoDB, and Redis, to make it possible of allowing users to query data simultaneously from both relational SQL systems and NoSQL systems in a single SQL query. Two mechanisms are provided for constructing Redis's indexes and semantic transforming between SQL and MongoDB API to add the SQL feature for these NoSQL databases. With the proposed approach, hybrid database systems can be performed in a flexible manner, ie, access can be either relational database or NoSQL, depending on the size of data. The approach can effectively reduce development complexity and improve development efficiency of the software systems with multidatabases. This is the result of further research on the related topic, which fills the gap ignored by relevant scholars in this field to make a little contribution to the further development of NoSQL technology."
pub.1101890636,HyVar,"Abstract
The HyVar project (www.hyvar-project.eu/) proposes a development framework for continuous and individualized evolution of distributed software applications running on remote devices in heterogeneous environments, focusing on the automotive domain. The framework combines variability modeling and software reuse from software product lines with formal methods and software upgrades and can be integrated in existing software development processes. HyVar’s objectives are: (O1) To develop a Domain Specific Variability Language (DSVL) and tool chain to support software variability for highly distributed applications; (O2) to develop a cloud infrastructure that exploits software variability as described in the DSVL to track the software configurations deployed on remote devices and to enable (i) the collection of data from the devices to monitor their behavior; and (ii) secure and efficient customized updates; (O3) to develop a technology for over-the-air updates of distributed applications, which enables continuous software evolution after deployment on complex remote devices that incorporate a system of systems; and (O4) to test HyVar’s approach as described in the above objectives in an industry-led demonstrator to assess in quantifiable ways its benefits. The end of the project is approaching and we are close to reaching all the objectives. In this paper, we present the integrated tool chain, which combines formal reuse through software product lines with commonly used industrial practices, and supports the development and deployment of individualized software adaptations. We also describe the main benefits for the stakeholders involved."
pub.1019926217,Chapter 14 Challenges and Emerging Trends,"Computing innovation is generally driven by a desire to address the most vexing problems facing IT organizations. However, the success of any new innovation depends heavily on the business value it provides. To understand trends in computing innovation, it is essential to understand the challenges driving these innovations. This chapter covers some of the most pressing challenges in database and application migration and the adoption of cloud computing by many organizations. Business-related challenges are discussed, including costs associated with migrating to the new platform, the duration of the migration project, and its impact on existing users and the IT staff. The chapter also highlights technology-related challenges, including the lack of tools for migrating data from large databases automatically and efficiently, automating the application migration effort, and automating testing scripts and use cases. It discusses services that complement public and private clouds in the integration and security space that are very important for many organizations. As adoption of cloud computing continues at a fast pace, there will be more and more offerings from platform vendors as well as other software vendors in the areas of development, business process management, and cloud management. All these efforts are also leading to the development of standard cloud APIs, which can result in simplification of management, integration, and development in a cloud environment."
pub.1133337837,Pattern-Based Cloud Migration,"To migrate on-premises business systems to the cloud environment faces challenges: the complexity, diversity of the legacy systems, cloud, and cloud migration services. Consequently, the cloud migration faces two major problems. The first one is how to select cloud services for the legacy systems, and the second one is how to move the corresponding workload from legacy systems to cloud. This chapter presents a total cloud migration solution including cloud service selection and optimization, cloud migration pattern generation, and cloud migration pattern enforcement. It takes the pattern as the core, and unifies the cloud migration request, the cloud migration service pattern, and the cloud migration service composition. A cloud migration example of blockchain system shows that the proposed approach improves the cloud service selection, cloud migration service composition generation efficiency, migration process parallelization, and enables long transaction support by means of pattern reuse."
pub.1173111015,Towards antifragility of cloud systems: An adaptive chaos driven framework,"Context Unlike resilience, antifragility describes systems that get stronger rather than weaker under stress and chaos. Antifragile systems have the capacity to overcome stressors and come out stronger, whereas resilient systems are focused on their capacity to return to their previous state following a failure. As technology environments become increasingly complex, there is a great need for developing software systems that can benefit from failures while continuously improving. Most applications nowadays operate in cloud environments. Thus, with this increasing adoption of Cloud-Native Systems they require antifragility due to their distributed nature. Objective The paper proposes UNFRAGILE framework, which facilitates the transformation of existing systems into antifragile systems. The framework employs chaos engineering to introduce failures incrementally and assess the system's response under such perturbation and improves the quality of system response by removing fragilities and introducing adaptive fault tolerance strategies. Method The UNFRAGILE framework's feasibility has been validated by applying it to a cloud-native using a real-world architecture to enhance its antifragility towards long outbound service latencies. The empirical investigation of fragility is undertaken, and the results show how chaos affects application performance metrics and causes disturbances in them. To deal with chaotic network latency, an adaptation phase is put into effect. Results The findings indicate that the steady stage's behaviour is like the antifragile stage's behaviour. This suggests that the system could self-stabilise during the chaos without the need to define a static configuration after determining from the context of the environment that the dependent system was experiencing difficulties. Conclusion Overall, this paper contributes to ongoing efforts to develop antifragile software capable of adapting to the rapidly changing complex environment. Overall, the research provides an operational framework for engineering software systems that learn and improve through exposure to failures rather than just surviving them."
pub.1027843714,"Cloud Computing Patterns, Fundamentals to Design, Build, and Manage Cloud Applications","The current work provides CIOs, software architects, project managers, developers, and cloud strategy initiatives with a set of architectural patterns that offer nuggets of advice on how to achieve common cloud computing-related goals. The cloud computing patterns capture knowledge and experience in an abstract format that is independent of concrete vendor products. Readers are provided with a toolbox to structure cloud computing strategies and design cloud application architectures. By using this book cloud-native applications can be implemented and best suited cloud vendors and tooling for individual usage scenarios can be selected. The cloud computing patterns offer a unique blend of academic knowledge and practical experience due to the mix of authors. Academic knowledge is brought in by Christoph Fehling and Professor Dr. Frank Leymann who work on cloud research at the University of Stuttgart. Practical experience in building cloud applications, selecting cloud vendors, and designing enterprise architecture as a cloud customer is brought in by Dr. Ralph Retter who works as an IT architect at T‑Systems, Walter Schupeck, who works as a Technology Manager in the field of Enterprise Architecture at Daimler AG,and Peter Arbitter, the former head of T Systems’ cloud architecture and IT portfolio team and now working for Microsoft. Voices on Cloud Computing Patterns Cloud computing is especially beneficial for large companies such as Daimler AG. Prerequisite is a thorough analysis of its impact on the existing applications and the IT architectures. During our collaborative research with the University of Stuttgart, we identified a vendor-neutral and structured approach to describe properties of cloud offerings and requirements on cloud environments. The resulting Cloud Computing Patterns have profoundly impacted our corporate IT strategy regarding the adoption of cloud computing. They help our architects, project managers anddevelopers in the refinement of architectural guidelines and communicate requirements to our integration partners and software suppliers.  Dr. Michael Gorriz – CIO Daimler AG  Ever since 2005 T-Systems has provided a flexible and reliable cloud platform with its “Dynamic Services”. Today these cloud services cover a huge variety of corporate applications, especially enterprise resource planning, business intelligence, video, voice communication, collaboration, messaging and mobility services. The book was written by senior cloud pioneers sharing their technology foresight combining essential information and practical experiences. This valuable compilation helps both practitioners and clients to really understand which new types of services are readily available, how they really work and importantly how to benefit from the cloud.   Dr. Marcus Hacke – Senior Vice President, T-Systems International GmbH This book provides a conceptual framework and very timely guidance for people and organizations building applications for the clo"
pub.1094174905,License Management in Grid and Cloud Computing,"The lack of license management schemes in distributed environments is becoming a major obstacle for the commercial adoption of Grid or Cloud infrastructures. In this paper, we present a complete license management architecture that enables a pay-per-use license management which can be deployed together with an on-demand computing scenario. Our architecture enables authenticated access to a remote license server. The license management architecture can be deployed in any distributed environment. It supports existing client/server based software license management tools (for example FlexNet Publisher). This allows an easy transition from current software license business models which support only a local license management towards business models which support license management in distributed environments."
pub.1095222959,Message from the MESOCA 2014 Co-Chairs,"We are happy to welcome you to Victoria, BC, Canada, and to the IEEE 8th Symposium on the Maintenance and Evolution of Service-Oriented Systems and Cloud-Based Environments, colocated with the 30th IEEE International Conference on Software Maintenance and Evolution (ICSME 2014). This event evolved from MESOA, The International Workshop on a Research Agenda for Maintenance and Evolution of Service-Oriented Systems, having its first edition in 2007, in Paris. It focused on Service-Oriented Architecture (SOA) as a dynamic, heterogeneous and potentially distributed development and maintenance environment, while service identification, concept location and service testing were presented as techniques to support maintenance and evolution of service-oriented systems. In 2011, in Williamsburg, the event became MESOCA, The IEEE International Workshop on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems, introducing topics related to Cloud Computing, as an emerging model for system development and deployment, and an important complement of SOA. Starting with the 2013 edition from Eindhoven, the MESOCA scope was extended again and the event was transformed into a symposium; it continued to play its part in the maintenance and evolution community, for bringing together researchers from various service-oriented approaches and from Cloud Computing, in order to analyze the common challenges, but also their differences. The event registered an increased interest for the evolution in Cloud environments and for the migration of legacy applications to provisioning services."
pub.1093319145,Message from the MESOCA Chairs,"We are happy to welcome you to Bremen, Germany, and to the IEEE 9th Symposium on the Maintenance and Evolution of Service-Oriented Systems and Cloud-Based Environments, co-located with the 31th IEEE International Conference on Software Maintenance and Evolution (ICSME 2015). This event evolved from MESOA, The International Workshop on a Research Agenda for Maintenance and Evolution of Service-Oriented Systems, having its first edition in 2007, in Paris. It focused on Service-Oriented Architecture (SOA) as a dynamic, heterogeneous and potentially distributed development and maintenance environment, while service identification, concept location and service testing were presented as techniques to support maintenance and evolution of service-oriented systems. In 2011, in Williamsburg, the event became MESOCA, The IEEE International Workshop on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems, introducing topics related to Cloud Computing, as an emerging model for system development and deployment, and an important complement of SOA. Starting with the 2013 edition from Eindhoven, the MESOCA scope was extended again and the event was transformed into a symposium; it continued to play its part in the maintenance and evolution community, for bringing together researchers from various service-oriented approaches and from Cloud Computing, in order to analyze the common challenges, but also their differences. The event registered an increased interest for the evolution in Cloud environments and for the migration of legacy applications to provisioning services."
pub.1129379486,The Comparison of Cloud Migration Effort on Platform as a Service,"Platform as a Service offers users to easily and quickly deploy applications in the cloud environment without thinking about the availability of supporting resources for developing the application. However, migrating on-premise applications to cloud vendors still has risks ranging from the compatibility of application technology with the cloud platform to the efforts needed. One obstacle in the adoption of cloud technology is the lack of visibility in planning migration efforts. So this study aims to measure and compare the feasibility of PaaS cloud vendors based on the calculation of the effort required during the migration process through an empirical approach. Specifically, an experiment was carried out by migrating on-premise application to Amazon Elastic Beanstalk, Azure App Service, and Google App Engine. Empirical results show that Azure App Service and Amazon Elastic Beanstalk have similar complexity of effort, while Google App Engine has the highest complexity of effort."
pub.1142664140,"Transitioning from Legacy Air Traffic Management to Airspace Management through Secure, Cloud-Native Automation Solutions","Advancements in Cloud-native services, Machine-Learning (ML), Artificial Intelligence (AI), and Rapid Application Development (RAD) using the Agile methodology has led countless industries to achieving desirable levels of automation while reducing cost and improving quality software deployments, timely / iterative delivery, and accountability. Coupling this framework with the principle of security as a shared responsibility further enhances the efficacy of an integrated Development, Security, and Operations (DevSecOps) Team within organizations to deliver secured digital solutions. Air Navigation Service Providers (ANSPs) around the world are currently exploring and embracing the digital evolution shifting from monolithic, legacy automation platforms to an application framework of microservices to allow for flexible operations as capabilities and airspace operations evolve. Specific to the US, the ATM automation system of today is comprised of both safety and non-safety critical systems, with mission-essential, efficiency-critical, and mission-support services that are predominately maintained and evolved through multi-year, one contractor-led programs. Although the system has proven resilient, it has not proven to be agile and flexible to allow for advances in capabilities on-board aircraft or in the data integration and sharing with other NAS automation systems. This creates significant overhead in development, sustainability, and operations of the current automation system, and leaves modernization efforts—in terms of new capabilities—in constant investment decision planning cycles, costing agencies not just money, but more time to innovate. To advance aviation into a new generation of interoperability leveraging collaborative frameworks and application specific capabilities, ANSPs must adapt to innovative methods to collect, process, and deliver critical and essential aeronautical, weather, and flight information to air traffic control operators and ultimately to airspace users. Doing so can not only lead to sustaining NAS automation systems while reducing the costs to develop and operate these systems, but it also provides an opportunity to present strategies on how to dramatically reduce the time and integration efforts needed to deploy new capabilities. Leveraging cloud-native technologies and services is a way to realize this automation evolution vision for ANSPs. This paper examines the migration from today’s systems to secure, cloud-native platforms to prove that Mission Services and Mission Applications can be rapidly available / deployable to operators who provide separation and flow management services, using a cyber-secured cloud-native environment. Aeronautical data typically used for tactical decision making is now seen as crucial to the decision-making process in Air Traffic Management (ATM). Integrating global and localized datasets into a digital aviation data platform enhances the capabilities of the solutions and opens the po"
pub.1149147273,V2PFQL: A proactive fault tolerance approach for cloud‐hosted applications in cloud computing environment,"Proactive Fault tolerant (FT) approaches for infrastructure's cloud computing which have been increasingly developed in research scope with the support of machine learning need consistent monitoring and highly accurate predicting systems. However, it is difficult to implement the proactive fault tolerant approach of infrastructure's cloud computing because of complex architecture, dynamic services, and many interdependent factors existing in cloud environment. To overcome this problem, a proactive FT approach for cloud‐hosted application is proposed to achieve a high degree of automation, including physical machine (PM) fault analysis and self‐learning virtual machine (VM) migration. It is based on MAPE‐K (Monitor‐Analyze‐Plan‐Execute over a shared Knowledge) loop to take into account the knowledge inside controllers in run time. To evaluate PM fault analysis model, the web e‐Commerce is deployed by OpenStack Stein while the prototype of VM migration component is built by CloudSim for VM migration solution evaluation. The effectiveness of the approach is demonstrated in experiment results."
pub.1158400971,Indexing legacy data-sets for global access and processing in multi-cloud environments,"Global data access and processing for multi-cloud applications proves challenging where there is a need for efficient access to large pre-existing, legacy data-sets. To address this problem, we created an indexing subsystem, allowing us to index and gather the metadata of legacy data-sets. The gathered information is later incorporated into a multi-cloud data management system to achieve global integration of legacy storage systems containing large data-sets. The solution is based on metadata management, organization and periodic monitoring of legacy data-sets, which makes possible scheme-less co-existence of a legacy storage system and a multi-cloud data management system for managing the same data collections. The approach has been initially evaluated in a multi-cloud deployment scenario. A large data collection consisting of legacy cultural data-sets was exposed to public and commercial infrastructure for data access and processing."
pub.1031146575,Application migration to cloud,"Cloud computing has attracted attention as an important platform for software deployment, with perceived benefits such as elasticity to fluctuating load, and reduced operational costs compared to running in enterprise data centers. While some software is written from scratch specially for the cloud, many organizations also wish to migrate existing applications to a cloud platform. Such a migration exercise to a cloud platform is not easy: some changes need to be made to deal with differences in software environment, such as programming model and data storage APIs, as well as varying performance qualities. We report here on experiences in doing a number of sample migrations. We propose a taxonomy of the migration tasks involved, and we show the breakdown of costs among categories of task, for a case-study which migrated a .NET n-tier application to run on Windows Azure. We also indicate important factors that impact on the cost of various migration tasks. This work contributes towards our future direction of building a framework for cost-benefit tradeoff analysis that would apply to migrating applications to cloud platforms, and could help decision-makers evaluate proposals for using cloud computing."
pub.1148983182,Cloud Services vs. On-Premises Software: Competition Under Security Risk and Product Customization,"Because of its on-demand feature and flexible pay-as-you-go mechanism, cloud service dramatically reduces the up-front information technology expenses that may deter many clients from implementing on-premises software. The associated security risks and low customization capability, however, create challenges for the adoption of cloud service. We study the competitive implications of security risks and customization capability on consumer purchase choices and vendors’ pricing and investment strategies. Although cloud services are perceived to be more vulnerable to cyberattack, our results demonstrate that in high-security-loss environments, using cloud service yields a lower average expected loss for consumers as compared with on-premises software. By endogenizing vendors’ investment decisions, our investigation highlights that the cloud vendor does not necessarily economically benefit from investing in addressing cloud security, especially in low-security-loss environments. We also find that the on-premises vendor’s security and customization investments act as strategic substitutes in low-security-loss environments and, under certain conditions, complement in high-security-loss environments. We further examine welfare-maximizing security investments and find that the socially optimal investment requires greater effort to improve cloud security in low-security-loss environments and to improve on-premises software security in high-security-loss environments.
Cloud computing services are transforming business and government at an ever-increasing rate. The associated security risk and low customization capability, however, create challenges for the adoption of cloud services. In this paper, we construct a game-theoretical model that involves two vendors—one that provides cloud service on a pay-per-use basis and the other that sells on-premises software at a one-time licensing fee—and consumers who are heterogeneous in their usage frequencies in an environment in which negative security externalities are present. We study the competitive implications of security risk and product customization capability on consumer purchase choice and vendors’ pricing and investment strategies. Although it is generally believed that cloud services are more vulnerable to security breaches, our results demonstrate that in high-security-loss environments in which consumers incur a large loss per use if struck by attacks, using cloud service yields a lower average expected loss for consumers compared with on-premises software. By endogenizing vendors’ investment decisions on security and customization, our investigation highlights that in low-security-loss environments, the cloud vendor has no incentive to invest effort in reducing security risk, but the on-premises vendor will increase security investment when the probability of attacks on its product becomes higher. We also find that the on-premises vendor’s security and customization investments act as strategic subst"
pub.1094918701,CMDSSI: A Decision Support System for Cloud Migration for SMEs in India,"Cloud computing is currently the hot topic in Information Technology (IT) as it offers a nontraditional model for managing and implementing IT in enterprises. Due to its benefits such as low investment for hardware and dynamic scaling, migrating data and application to cloud environment is gaining popularity in recent years in India. The main aim of this paper is to investigate the aspects of cloud migration with particular importance to SMEs in India and present a literature review of the existing frameworks and models available for cloud adoption. We have identified a number of issues that SMEs in India face while migrating to a cloud infrastructure and have presented our findings as an initial step towards a Decision Support System for cloud migration in India, referred to as Cloud Migration Decision Support System for India (CMDSSI)."
pub.1093745015,CIDE: An Integrated Development Environment for Microservices,"Microservices is a flexible architectural style that has many advantages over the alternative monolithic style. These include better performance and scalability. It is particularly suitable, and widely adopted, for cloud-based applications, because in this architecture a software system consisting of a large suite of services of fine granularity, each running its own process and communicating with the others. However, programming such systems is more complex. In this paper we report on CIDE, an integrated software development environment that helps with this. CIDE supports programming in a novel agent-oriented language called CAOPLE and tests their execution in a cluster environment. We present the architecture of CIDE, discuss its design based on the principles of the DevOps software development methodology, and describe facilities that support continuous testing and seamless integration, two other advantages of Microservices."
pub.1174504780,Data Management as a Pathway to Energy Industry Digital Transformation and AI Workflows Adoption – The SLB Approach,"Abstract Cloud adoption – often referred to as ""digital"" has in recent times, proven to effectively eliminate hardware and software resource efficiency and sufficiency limitations obtainable in on-premise (located within company facility) infrastructure, in the area of compute and storage. It has also become apparent, that Artificial Intelligence (AI) Assisted workflows dramatically increases productivity for domain experts (Engineers, Geoscientists, Geologists, etc.), abstracting away mundane and repetitive tasks while leaving room for greater levels of productivity. However, accessing the above stated benefits of AI and digital must be inevitably preceded by data liberation from siloed systems into a democratized cloud-ready secure platform. The SLB DMaaS (Data Management as a Service) solution, a suite of SLB ""best-in-class"" software technology which was successfully deployed for the first time in West Africa for a Nigerian Independent Energy Company in the year 2023, was designed to adequately implement a secure and democratized cloud ready collaborative environment devoid of siloed data sources. Aimed at creating an environment which is not just ready for a seamless transition to the cloud environment with minimal effort, but also a secure and accessible system to all authorized data users (domain experts) within the organization, the DMaaS solution is also OSDU (Open Subsurface Data Universe) ready. The DMaaS solution technology suite presents a key piece in the puzzle of the Global Energy Industry digital transformation journey as well as the benefits therein. Elaborately discussed in this paper, are insights to how SLB is helping the global energy industry unlock the value hidden in data through Data Management for cloud adoption, digital transformation and AI – assisted workflows."
pub.1094362913,Migrating Software Testing to the Cloud,"Tutorial Abstract Regression testing is often performed as part of the software maintenance process. The amount of tests cases for a large-scale system can range from several hundred to many thousands, requiring significant computing resources and lengthy execution times, often precluding their use in an interactive setting. Traditional approaches to reduce the execution time for regression testing typically focus on excluding selected tests from the suite that need to be run after a change is made to the system. Cloud computing offers an alternate solution to this problem: the use of virtualized hardware, effectively unlimited storage, and software services that can aid in reducing the execution time of large test suites in a cost-effective manner. However, migrating software testing to the cloud is not without cost, nor is it necessarily the best solution to all testing problems. The new area of software testing in the cloud (STITC) lies at the intersection of these key areas: software testing, cloud computing, and system migration. This tutorial presents the SMART - T decision framework for migrating software testing to the cloud. The framework is based on the “SOA Migration, Adoption, and Reuse Technique” (SMART) from Carnegie Mellon University's Software Engineering Institute, which has been successfully used to migrate legacy components to a service-oriented environment. SMART - T helps organizations identify their current testing process, describe the requirements of the target cloud computing environment for performing software testing, and through a gap analysis of these two states map out the issues, effort, and potential benefits of migrating their software testing to the cloud. Several case studies are used to illustrate the use of the SMART-T framework in real-world settings."
pub.1094335733,InCLOUDer: A Formalised Decision Support Modelling Approach to Migrate Applications to Cloud Environments,"An increasing number of organisations want to migrate their existing applications to cloud environments to benefit from the increased scalability, flexibility, and cost reduction. Additionally, systems migrated to cloud environments have to fulfil their functional requirements, satisfy their users' requirements, and meet the organisation's criteria for cloud migration. All these different dimensions driving the migration decision conflict with each other. Therefore, organisations trade them off for one another. Migrating applications to cloud environments becomes a complex decision process for which organisations need assistance. We provide a decision support system to assist organisations by taking into account the formal description of the parameters affecting the cloud migration and our proposed metrics for objective and subjective criteria. Our approach to cloud migration allows organisations to describe their cloud migration criteria; the architecture, properties, and requirements of their applications; and the available cloud service offerings. We semi-automate the migration decision with our transparent formalisations to quantify criteria and constraints."
pub.1008960466,"Testing in the Cloud: Strategies, Risks and Benefits","Testing in the cloud, commonly referred to as cloud testing, has revolutionised the approach adopted in traditional software testing. In the literal terms, it refers to testing Web applications in the “cloud” – leveraging a service provider’s ready-made testing resources. The customer boycotts the hassle and expense of procurement, setup and maintenance of test environment setup on premise. Previously, accustomed solely with non-functional testing such as performance and load testing, recent advancements have made it possible to write test scripts and modify and automate test suites – all in the cloud environment. This chapter provides an in-depth overview of contemporary cloud testing, the types and its best practices. The benefits and risks are fully discussed with recommended methods to abate these risks. A methodological approach to govern an organisation migrating to cloud testing is also presented. A unique model, which shows the complex and dynamic interrelationship among active factors and their effect on the major project success factors in a cloud testing environment, is designed and presented. These project success factors include productivity, quality and cost. This model will help management to make strategic decisions on the adoption of cloud testing and the impact of their policy adoption on the productivity, quality and cost of software development projects."
pub.1166764335,Challenges In Optimizing Migration Costs From On-Premises To Microsoft Azure,"The current paper analyzes the feasibility of a modular web application's migration procedure from on-premises to the cloud. Our focus is on identifying cost savings and options for hosting the application in the cloud. The research specifically examines the impact of architectural decisions records (ADR) on a modular monolith use case, utilizing .NET Core for the backend and Angular for the front end, with Clean Architecture as the design pattern. We investigate different cloud models (IaaS, PaaS, Serverless) considering project management's triple constraints (time, cost, performance). The study demonstrates that a modular monolith can be migrated with varying effort and costs depending on the chosen cloud model and technology stack. We explore optimizations such as resource utilization, licensing fees, and cost reduction through infrastructure reservations. Our findings show that the migration cost can range from a 20% increase with IaaS to approximately 70% cost reduction with the Serverless strategy compared to the on-premises environment, using equivalent resources. We also explore methods to lower expenses for each model, including resource modifications, Linux operating systems, and longer resource reservations. Considering limitations, we propose a two-stage migration strategy: initially lifting and shifting the application to IaaS with cost optimization, and subsequently migrating to PaaS for scalability and simplified resource management in the long term."
pub.1103154031,Lifting and shifting the Air Force retail supply system,"
                    Purpose
                    Rising operational costs and software sustainment concerns have driven the Air Force to move to newer technology to ensure that the Air Force Standard Base Supply System (SBSS) can continue to provide affordable and sustainable mission support in the years to come. This paper aims to summarize the successful software modernization effort the Air Force undertook to achieve that objective.
                  
                  
                    Design/methodology/approach
                    The paper describes the preliminary system updates that were required to isolate the SBSS software from all internal and external system and user interfaces in preparation for the subsequent successful code roll effort. Once the legacy SBSS component was fully isolated, the SBSS software modernization objective was achieved via a “code roll” conversion of the SBSS software from legacy COBOL to Java code, and movement of the integrated logistics system-supply application from a proprietary information technology (IT) platform to an open IT operating environment.
                  
                  
                    Findings
                    The SBSS system modernization yielded immediate and significant IT operational cost reductions and provided an important foundation for achieving Air Force logistics system consolidation and cloud computing objectives going forward.
                  
                  
                    Originality/value
                    The SBSS modernization experience should be useful in assisting similar data system software modernization efforts.
                  "
pub.1175529896,Exploring BI Data Transition to the Cloud: Analysis & Recommendations,"The process of transitioning Business Intelligence (BI) data to cloud environments involves migrating data, on-premises BI tools, services, and to a distributed cloud-based infrastructure. The success of this transition depends on meticulous planning and impact analysis of existing enterprise systems. A common procedure entails moving locally stored data from on-premises systems to a cloud-based BI environment. Cloud migration presents both challenges and advantages, prompting extensive scholarly investigations and technical analysis of BI data migration to cloud environments. In this paper, we dissect existing data migration strategies into three distinct approaches based on essential cloud deployment models. Each approach requires specific processes, and corresponding tasks are integrated accordingly. We analyze the similarities and differences between these strategies, emphasizing the challenges and proposing future directions for data migration to the cloud. Through a comprehensive study, we identify the key benefits and obstacles associated with migrating BI data into cloud-based BI platforms. Additionally, we recommend various BI data transition methods and models to assess performance, address security requirements, select an appropriate cloud service provider, estimate costs, and facilitate necessary business adjustments. The insights from this research paper provide a secure and effective roadmap for transitioning BI data within a cloud computing environment."
pub.1147195560,Implementation of an Optimized Solution using a Cloud-Based Production Data Management System for Production Operations,"Abstract Major oil and gas operators often face performance issues related to on-premises applications when dealing with huge amounts of data. A cloud-based digital solution was developed for an oil and gas company in Colombia to upgrade the production data management system (PDMS) by migrating from on-premises to a secure cloud-based environment, enhancing the performance of the solution and the remote access experience for end users. This implementation was carried out under a cloud-based infrastructure using a platform-as-a-service (PaaS) scheme, which includes middleware, database management systems, and backup services. The user access to the PDMS is through virtual desktop services, which enables load balancing of the users to avoid performance issues. The performance of the previous infrastructure was evaluated and considered when designing the new architecture, the database sizing, licensing, and integration with third-party applications. The data from the on-premises solution were analyzed and validated to guarantee correspondence with the cloud-based solution, and both solutions were run in parallel to verify consistency and reliability. The release of the cloud-based application was done in stages, with a stabilization period during which any issues could be detected and corrected. The PDMS solution was improved by faster data processing, reducing the execution time of calculation and allocation processes by 50%, while some heavy processes as data carry forward reached a 90% reduction. Enhancements included the generation of complex reports, such as hierarchy production allocation results, that did not run on the on-premises servers. As a result of this implementation, the application can be accessed from a personal computer as well as from a mobile phone, allowing the user access from any place or device, without security risks. The cloud-based solution reduced OPEX and increased flexibility due to lower maintenance costs for physical infrastructure and the cloud server's capacity based on demand."
pub.1005593010,Technical Strategies and Architectural Patterns for Migrating Legacy Systems to the Cloud,"With the advent of Internet, information has crossed the realms of books and gone digital, requiring data to be easily accessible and delivered anywhere speedily. There are myriads of formats in which data is currently available, such as videos, images, documents and Web pages. Accordingly, handling datasets in various formats has made the task of designing scalable and reliable application really challenging. Building the applications of tomorrow would need architects and developers to construct applications that can meet the needs that would demand handling high volumes of data and deliver substantial throughput. In today’s enterprises, there are legacy applications which may have been developed several years or even decades ago. At the time, the business may have been in its infancy, and so applications may have been designed to satisfactorily handle workloads prevalent in those times with some average growth factors built in. However, owing to the new emerging trends in the technology space, such as mobile and big data, the workloads at which businesses operate today have grown manifold. Also, the monolithic legacy systems serving those workloads have failed to keep pace, often struggling to deliver the SLAs (Service-Level Agreement). Although cloud is not a panacea for all kind of new demands, we believe that with some appropriate architectural restructuring, existing applications may go a long way in serving the demands of new growing businesses. Applications redesigned on the lines of parallel computing patterns such as master/worker or MapReduce and implemented on cloud platforms can be leveraged to add new life or re-energise legacy applications that can scale much better. In this chapter, we discuss an approach to transform legacy applications, designed to handle high-volume requests, by using re-engineering techniques and modern design patterns so as to effectively realise the benefits of cloud environment."
pub.1174659507,"Availability, Scalability, and Security in the Migration from Container-Based to Cloud-Native Applications","The shift from traditional monolithic architectures to container-based solutions has revolutionized application deployment by enabling consistent, isolated environments across various platforms. However, as organizations look for improved efficiency, resilience, security, and scalability, the limitations of container-based applications, such as their manual scaling, resource management challenges, potential single points of failure, and operational complexities, become apparent. These challenges, coupled with the need for sophisticated tools and expertise for monitoring and security, drive the move towards cloud-native architectures. Cloud-native approaches offer a more robust integration with cloud services, including managed databases and AI/ML services, providing enhanced agility and efficiency beyond what standalone containers can achieve. Availability, scalability, and security are the cornerstone requirements of these cloud-native applications. This work explores how containerized applications can be customized to address such requirements during their shift to cloud-native orchestrated environments. A Proof of Concept (PoC) demonstrated the technical aspects of such a move into a Kubernetes environment in Azure. The results from its evaluation highlighted the suitability of Kubernetes in addressing such a demand for availability and scalability while safeguarding security when moving containerized applications to cloud-native environments."
pub.1146728255,An Architecture for Autonomous Proactive and Polymorphic Optimization of Cloud Applications,"Existing autonomous Cloud application management platforms continuously monitor the load and the environment of the application react when optimization is needed. This paper introduces the concepts of polymorphic architecture optimization and proactive adaptation of Cloud applications, which is a significant improvement of the standard reactive optimization. Polymorphic architecture optimization considers the change of the technical form of the component while proactive adaptation uses the predicted future workload and context. Based on this, we propose an architecture for a Cross-Cloud application management platform that supports complex optimization of Cloud applications in terms of architecture, resources, and available offers."
pub.1143336765,An Evolution of Proxy Mobile IPv6 to the Cloud,"Network Function Virtualization (NFV) and Software Defined Networks (SDN) do not leave any legacy network services untouched. This work will present how Proxy Mobile IPv6 (PMIPv6) can evolve to the cloud. Our approach is to introduce this evolution within a step-by-step architecture guideline while keeping standards compatibility. We also show how PMIPv6 can fit into a central cloud - edge cloud environment on the top of Kubernetes and Openstack under a unified orchestration umbrella. The proper integration of PMIPv6 into this new environment does not just enforce acquiring new capabilities, e.g., scaling PMIPv6 elements; it also can ensure closed-loop orchestration where PMIPv6 can be controlled continuously by the actual network needs."
pub.1007121513,"Web4Desktop, a Framework for Improving the Usability of Web Applications","The cloud computing model leads to the increased penetration of the web applications in the office environment. Designed in many cases to replace traditional desktop software, web applications still lack many of the valuable features present on the desktop that increase usability and productivity. Due to the highly isolated design of the browser, it is currently impossible for web applications to communicate with desktop environment, which usually means sending messages or receiving event notifications. This is often required in order to let the person using the application know about the important events happening in the minimized browser window. By contrast traditional applications can take the control of the desktop at any time. The paper introduces the Web4Desktop framework, a browser/client based architecture designed to overcome these limitations by proving a secure infrastructure that allows web applications to communicate with any desktop software implementing the Web4Desktop API. The framework can be utilized to add desktop integration to existing web applications, a step that requires only minimal changes in the web application’s code and greatly improve the user experience because these applications will start to behave more like desktop software."
pub.1158055874,InSECTS Cloud Agnostic On-Orbit Processing for Satellite Development,"Interplanetary Simulation Environment for Communication and Thermal Subsystems (InSECTS) is an environment designed to support rapid prototyping of spacecraft containerized software on-orbit. The InSECTS digital architecture was developed to support NASA/JHU-APL Dragonfly autonomous software development and proliferated space-based containerized processing at Johns Hopkins Whiting School of Engineering. Processing needs for deep space, scientific, and National Security spacecraft software are continually evolving to support exceedingly diverse mission requirements. As a result of this evolution, Spacecraft software engineers require a pathway for rapidly testing, updating, and deploying software to on-orbit cloud processing clusters in an economical and responsible way. InSECTS supports this need by providing a low-cost prototype of an on-orbit processing architecture affording developers the capability to test and deploy cloud-based software to operational spacecraft, while mitigating the risks involved with traditional space system development. The InSECTS digital architecture supports any open-source initiative compliant containerized application on a hardware agnostic distributed Commercial Off the Shelf (COTS) computing environment. Additionally, by utilizing preexisting container orchestration software the environment is inherently redundant across multiple independent processing suites. Finally, InSECTS design is configurable which allows it to function as a payload on a small satellite or as its own microsatellite. The InSECTS cloud-agnostic ground system is accessible with Internet of Things (IoT) devices and has a Development Operations (DEVOPS) pipeline that supports open-source software code development. This gives engineers and researchers building software the ability to write, integrate, and deploy software containers into the InSECTS environment using development tools broadly accessible across the industry. To verify these technical solutions, an InSECTS prototype was demonstrated at Johns Hopkins University by deploying an autonomy software cluster able to control a thermal control system model designed after the NASA/APL Dragonfly thermonuclear generator. As a result of this successful test, further InSECTS development at Johns Hopkins is planned and may include a functional small satellite for on-orbit testing. This satellite would provide a testbed for containerized processing in the space environment utilizing COTS hardware for cloud-agnostic software deployments capable of supporting the diverse processing requirements of future National Security and deep space systems."
pub.1015749599,CyberLiveApp: A secure sharing and migration approach for live virtual desktop applications in a cloud environment,"In recent years, we have witnessed the rapid advent of cloud computing, in which remote software is delivered as a service and accessed by users using a thin client over the Internet. In particular, a traditional desktop application can execute in the remote virtual machines of clouds without re-architecture and provide a personal desktop experience to users through remote display technologies. However, existing cloud desktop applications have isolated environments with virtual machines (VMs), which cannot adequately support application-oriented collaborations between multiple users and VMs. In this paper, we propose a flexible collaboration approach, named CyberLiveApp, to enable live virtual desktop application sharing, based on a cloud and virtualization infrastructure. CyberLiveApp supports secure application sharing and on-demand migration among multiple users or equipment. To support VM desktop sharing among multiple users, we develop a secure access mechanism to distinguish their view privileges, in which window operation events are tracked to compute hidden areas of windows in real time. A proxy-based window filtering mechanism is also proposed to deliver desktops to different users. To achieve the goals of live application sharing and migration between VMs, a presentation redirection approach based on VNC protocol and a VM cloning service based on the Libvirt interface are used. These approaches have been preliminary evaluated on an extended MetaVNC. Results of evaluations have verified that these approaches are effective and useful."
pub.1000636805,Chapter 8 Cloud Infrastructure as a Service,"This chapter covers the various security considerations from the server virtualization perspective when implementing cloud infrastructure-as-a-service (IaaS) offerings. It goes through existing security infrastructures and techniques that are still applicable and can be reused accordingly in cloud security environments. The chapter also discusses how Unified Computing System (UCS) resolves the current server deployment constraints in enterprise DCs and how UCS is the ideal compute platform for private cloud computing and cloud IaaS offerings. The chapter also discusses the various components of the service-oriented infrastructure (SOI) and how cloud IaaS offerings can be directly overlaid on top of this SOI. UCS facilitates the rack-and-roll deployment model for modularity at the rack level as well as rapid server provisioning in existing DC or greenfield environments. Cloud management, is not as overly complex as one might think, because existing network management strategies and integration techniques are still fully applicable in cloud IaaS offerings. In large-scale private cloud computing deployments, qualities such as hierarchical management model, policy-based management, management mediation, automation, user self-service portals, and standardization with XML should be taken into consideration. With the SOI comprising submodules such as application software, virtual machine, virtual access layer, compute, storage array, SAN, SAN extension, access layer, aggregation layer, core layer, peering, and next-generation WAN in place, cloud IaaS offerings can be directly overlaid on top of this SOI. To the end-users, the environment appears as if it is dedicated to their applications alone."
pub.1154480175,Factors affecting cloud-based enterprise resource planning software adoption in Ethiopia,"Enterprise Resource Planning (erp) are integrated software solutions that transform organizations’ internal processes, provide collaboration with partners, external applications and information systems. Extant literature reveals that organizations are showing interest to transit from an on-premise erp to the new cloud-based erp solutions due to their extra benefits. There are few recent initiatives in Ethiopia to adopt cloud-based erp but challenges faced in the course of the adoption are not explored. Using the technology-organization-environment, diffusion of innovation, and the model of innovation resistance frameworks as lenses, this research aims at identifying factors contributing to the adoption of cloud-based erp in the Ethiopian context. A quantitative approach is adopted and survey was conducted using a self-administered online questionnaire using Google’s online form to gather data from employees of Ethiopian Shipping and Logistics Services Enterprise. Out of 295 questionnaires distributed, 152 valid questionnaires were collected and considered for the data analysis. The proposed model was tested using a partial least square with the help of the Smart pls software. The proposed model explained 58.5 % of the variance in cloud-based ERP adoption factors. The empirical analysis indicated that Relative advantage, Trust, IT Skill, and External pressure had a significant influence on the adoption of cloud-based erp in Ethiopia whereas Organizational Culture, Observability, and Trialability had no significant impact on the adoption of cloud erp service. The study provides a comprehensive understanding of the factors which affect the adoption of cloud-based erp technology in Ethiopia."
pub.1022289023,Message-Based System Integration,"When a cloud service provider introduces Cloud to enterprises, it should be ready to address the needs of integrating the new cloud services with hundreds of existing systems. It needs to provide a smooth transition path for enterprise users to adopt cloud services over time. At the same time, when it extends existing enterprise applications to cloud, it often needs to migrate existing systems piece by piece. So, for a considerable period of time, it will face the situation that different parts of a bigger system work together over different environments, such as on-premise, private cloud, community cloud, and public cloud."
pub.1007645802,SAP on the Cloud,"This book offers a comprehensive guide to implementing SAP and HANA on private, public and hybrid clouds. Cloud computing has transformed the way organizations run their IT infrastructures: the shift from legacy monolithic mainframes and UNIX platforms to cloud based infrastructures offering ubiquitous access to critical information, elastic provisioning and drastic cost savings has made cloud an essential part of every organization’s business strategy. Cloud based services have evolved from simple file sharing, email and messaging utilities in the past, to the current situation, where their improved technical capabilities and SLAs make running mission-critical applications such as SAP possible. However, IT professionals must take due care when deploying SAP in a public, private, or hybrid cloud environment. As a foundation for core business operations, SAP cloud deployments must satisfy stringent requirements concerning their performance, scale and security, while delivering measurable improvements in IT efficiency and cost savings. The 2nd edition of “SAP on the Cloud” continues the work of its successful predecessor released in 2013, providing updated guidance for deploying SAP in public, private and hybrid clouds. To do so, it discusses the technical requirements and considerations necessary for IT professionals to successfully implement SAP software in a cloud environment, including best-practice architectures for IaaS, PaaS, and SaaS deployments. The section on SAP’s in-memory database HANA has been significantly extended to cover Suite on HANA (SoH) and the different incarnations of HANA Enterprise Cloud (HEC) and Tailored Datacenter Integration (TDI). As cyber threats are a significant concern, it also explores appropriate security models for defending SAP cloud deployments against modern and sophisticated attacks. The reader will gain the insights needed to understand the respective benefits and drawbacks of various deployment models,and how SAP on the cloud can be used to deliver IT efficiency and cost-savings in a secure and agile manner."
pub.1107558611,Protecting Privacy in Cloud Computing by using Wireless Sensor Networks,"Popularity of cloud computing is increasing day by day in distributed computing environment. Cloud Computing is useful because it adds new capabilities to the existing system without the need to invest in new infrastructure, train new personnel, or license new software. Cloud computing offers enterprises and users high scalability, high availability, and high reliability. As the public cloud services from Amazon, Google, and Microsoft become more sophisticated and better developed, more and more companies are migration toward the cloud computing platform. The main purposes of our system is people- counting method can automatically count the number of incoming and outgoing people at a special point in real time. Wireless sensor networks have very broad application prospects including both military and civilian usage Sensor networks have the potential to radically change the way people observe and interact with their environment."
pub.1123812646,Analysis of Modern Continuous Integration/Deployment Workflows Based on Virtualization Tools and Containerization Techniques,"Modern virtualization techniques and server architecting solutions are studied. In order to reduce development time and assets cost, there is a great need in full utilization of all calculation resources for both dedicated and virtual servers. Containerization allows shorten environment integration and software deployment time basing on virtualization and tighter integration on high abstract level. The detailed description and analysis of common containerization techniques is given. Comparative analysis of modern solutions shows that the containerization orchestration systems are the best solution which advantages allow optimizing resource utilization and addressing arisen challenges. The features of the joint use of container orchestration and management system together with different cloud providers are given. Comparison of modern CI/CD approaches is given. Environment quality and its configuration for active application deployments make major impact on needed calculation resources quantity. Common environment architecture approaches in context of containerization usage are described. CI/CD solutions allow significantly reduce deployment time and increase reliability."
pub.1141111830,"IFogSim2: An Extended iFogSim Simulator for Mobility, Clustering, and Microservice Management in Edge and Fog Computing Environments","Internet of Things (IoT) has already proven to be the building block for
next-generation Cyber-Physical Systems (CPSs). The considerable amount of data
generated by the IoT devices needs latency-sensitive processing, which is not
feasible by deploying the respective applications in remote Cloud datacentres.
Edge/Fog computing, a promising extension of Cloud at the IoT-proximate
network, can meet such requirements for smart CPSs. However, the structural and
operational differences of Edge/Fog infrastructure resist employing Cloud-based
service regulations directly to these environments. As a result, many research
works have been recently conducted, focusing on efficient application and
resource management in Edge/Fog computing environments. Scalable Edge/Fog
infrastructure is a must to validate these policies, which is also challenging
to accommodate in the real-world due to high cost and implementation time.
Considering simulation as a key to this constraint, various software has been
developed that can imitate the physical behaviour of Edge/Fog computing
environments. Nevertheless, the existing simulators often fail to support
advanced service management features because of their monolithic architecture,
lack of actual dataset, and limited scope for a periodic update. To overcome
these issues, we have developed multiple simulation models for service
migration, dynamic distributed cluster formation, and microservice
orchestration for Edge/Fog computing in this work and integrated with the
existing iFogSim simulation toolkit for launching it as iFogSim2. The
performance of iFogSim2 and its built-in policies are evaluated using three use
case scenarios and compared with the contemporary simulators and benchmark
policies under different settings. Results indicate that the proposed solution
outperform others in service management time, network usage, ram consumption,
and simulation time."
pub.1110930816,Systematic and recomputable comparison of multi-cloud management platforms,"With the growth and evolution of cloud applications, more and more architectures use hybrid cloud bindings to optimally use virtual resources regarding pricing policies and performance. This process has led to the creation of multi-cloud management platforms as well as abstraction libraries. At the moment, many (multi-)cloud management platforms (CMPs) are designed to cover the functional requirements. Along with growing adoption and industrial impact of such solutions, there is a need for a comparison and test environment which automatically assesses and compares existing platforms and helps in choosing the optimal one. This paper focuses on the creation of a suitable testbed concept and an actual extensible software prototype which makes multi-cloud experiments repeatable and reusable by other researchers. The work is evaluated by an exemplary comparison of 4 CMPs bound to AWS, showcasing standardised output formats and evaluation criteria."
pub.1158841754,Deterministic Local Cloud for Industrial Applications,"Time Sensitive Networking (TSN) is a key enabler technology for Industry 4.0. TSN provides the basic building block for network convergence: instead of having multiple, parallel communication networks for each traffic type in the factory, it provides a common ground that can fulfill the Quality-of-Service requirements of all existing networks with a single, shared infrastructure. The next step on this path is the convergence of the computing infrastructure, where on-premises cloud technology will be used to aggregate the different process controllers into a cloud computing environment. In this paper, we highlight some of the challenges in the cloudification of TSN traffic endpoints, and present an architecture design for TSN and cloud integration. We have set up a testbed and carried out measurements to show how the requirements of an industrial network can be met with cloudified TSN functions."
pub.1176287847,Securing Cloud Infrastructures: The Role of Deep Neural Networks in Intrusion Detection,"of cloud systems has become paramount. This paper investigates the application of deep neural networks (DNNs) in cloud intrusion detection systems (IDS). Traditional IDS systems struggle to handle the massive amount of data generated in cloud environments, often leading to high false-positive rates and missed detections. This paper demonstrates how DNNs can analyze cloud traffic patterns, detect anomalies, and distinguish between benign and malicious activities more efficiently than traditional systems. We examine the architecture of various DNN models used in cloud IDS, including Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), focusing on their ability to process sequential cloud data and detect subtle patterns indicative of potential attacks. The paper also discusses the deployment of these DNN-based systems in real-time environments and their integration with existing cloud security frameworks. Additionally, it highlights the performance of DNN-based intrusion detection systems in terms of accuracy, recall, and precision, comparing them with legacy IDS systems. Finally, the challenges in adopting DNN models for cloud security are explored, particularly regarding computational overhead, data privacy, and the risk of adversarial attacks."
pub.1157948637,A Taxonomy of Performance Forecasting Systems in the Serverless Cloud Computing Environments,"The Serverless Clouds Computing environment (or platform) manages the resource management of its respective clients who generally submit their respective applications as sets of functions (tasks). A client may submit his application as a set of tasks (functions) or as a monolithic task (single function). Each set of functions (tasks) compiled in the form of Directed Acyclic Graph (DAG), where each node is a function representing a fine-grained task and each edge represents a dependency among two functions. The decisions made through performance forecasting systems (PFS) or resource forecasting engines are of immense importance to such resource management systems. However, the forecasting of future resources is a complex problem. Several of PFS projects span over several computer resources in several dimensions. The most of the PFS projects have already been designed for performance forecasting of resources on the Distributed Computing Environments such as Peer-Peer, Queue systems, Clusters, Grids, Virtual machine organizations and Cloud systems and therefore in software engineering point of view, the new code can be written to integrate their forecasting services on the Serverless (Edge) Clouds platforms. In this chapter the taxonomy for describing the PFS architecture is discussed. The taxonomy is used to classify and identify approaches which are followed in the implementation of the existing PFSs in the Distributed Computing Environments and to realise their adaptation in the Serverless (Edge) Cloud Computing."
pub.1139931040,Overcoming the Complexities in Decision-Making for Enterprise Software Products: Influence of Technological Factors,"Technological aspects of an Information Technology (IT) innovation have a vital role in organization’s decision for adopting the IT innovation. Due to innovations in distributed computing, customers has the provision to avail Software products in two delivery models namely On-Premise model and Cloud computing model. Each deployment model possesses advantages and disadvantages one over the other and at the same time organizations are posed with the challenge of adopting one among them. Relative advantage is one of the key parameters in decision making, it has an impact on the decision maker in terms of cost, performance, utilization etc. Complexity is how the user perceives the ease of use for an innovation. The other technical aspects are Compatibility with existing infrastructure, Data security, Data privacy, and Data backup. Observing the results of the software product under trial period and making comparisons between the two kind of delivery models helps in adopting the software product model in long term usage. In this research paper, we have determined and evaluated the technological aspects of enterprise software product which helps organization’s in decision-making for adopting software product’s delivery model. The Technological context comprises of technical characteristics of a software product which can ease the adoption of software model. Complexity, Compatibility, Relative advantage, Trialability, Observability, Data Privacy, Data Security, Data backup, Data Accessibility, Data Location are the major factors identified in this study. Focused group discussion and interviews were conducted with the organizations for finding as how various technological factors has an impact on the decision of organization’s in the adoption of software products in the form of on-premise model or the cloud computing model."
pub.1099209881,Security Migration Requirements: From Legacy System to Cloud and from Cloud to Cloud,"Cloud computing became an emerging technology. The benefits have made a lot of institutions and companies looking to use this technology. The process of migration from legacy systems to cloud computing environments is a complex process. The migration process does not represent the movement of data, applications and service only but also presents the..."
pub.1095823268,Experiences with a Private Enterprise Cloud: Providing Fault Tolerance and High Availability for Interactive EDA Applications,"Silicon Design and Electronic Design Automation (EDA) business is highly competitive and time to market is of utmost importance in the semiconductor industry where companies put in a lot of effort to make sure that the first silicon is as healthy as possible. Hence it is imperative that the EDA compute environment provides maximum uptime to design engineers by utilizing several different High Performance Computing (HPC) technologies. In this paper we present Intel's EDA compute infrastructure, along with a detailed software and system architecture for supporting workload checkpointing, restoration and migration for specifically EDA jobs that are interactive in nature. We also describe our experiences in providing high availability for such EDA applications using existing popular HA/FT techniques. We believe that this is one of the few detailed descriptions of the EDA compute infrastructure of a large and complex semiconductor design company and that this will be useful in addressing future HPC challenges for EDA workloads as HPC technologies mature and evolve."
pub.1095459633,Prototyping Efficient Desktop-as-a-Service for FPGA Based Cloud Computing Architecture,"Cloud computing, a delivery of computing as a service mainly implying how to use utilities in our context, can be provided either at infrastructure, platform or software levels. The Desktop-as-a-Service (DaaS) paradigm, derives from the software level Software-as-a-Service (SaaS) paradigm, is drawing increasing interest because of its transformation from desktops into a cost-efficient, scalable and comfortable subscription service. Unlike most existing solutions delivering service with various protocols based on image transmitting in PC dominating environment, we present a DaaS with cloud server technologies on FPGA to address the problem of high power consumption and heavy network traffic. With the booming of mobile cloud computing, users can access the service on demand with smart phones or other portable devices like iPad or Amazon kindle as well as PC. Our system provides virtual desktop web pages written in HTML/JavaScript to avoid frequent image transmissions and reduce network traffic. To build the cloud prototype system, we combine Lightweight TCP/IP stack (LwIP) and Java Optimized Processor (JOP) to build a web server enabling dynamic web page interactions. Our system significantly saves volumes of data in transmission and network bandwidth. Analytical performance evaluation shows that on average, our system suffers only 25% transmitting latency and saves 46% of energy efficiency in comparison to other solutions. Our efficient DaaS based on FPGA explores new application of embedded web server in green cloud computing as well as new service paradigm of mobile cloud computing."
pub.1147571353,"iFogSim2: An extended iFogSim simulator for mobility, clustering, and microservice management in edge and fog computing environments","Internet of Things (IoT) has already proven to be the building block for next-generation Cyber–Physical Systems (CPSs). The considerable amount of data generated by the IoT devices needs latency-sensitive processing, which is not feasible by deploying the respective applications in remote Cloud datacentres. Edge/Fog computing, a promising extension of Cloud at the IoT-proximate network, can meet such requirements for smart CPSs. However, the structural and operational differences of Edge/Fog infrastructure resist employing Cloud-based service regulations directly to these environments. As a result, many research works have been recently conducted, focusing on efficient application and resource management in Edge/Fog computing environments. Scalable Edge/Fog infrastructure is a must to validate these policies, which is also challenging to accommodate in the real-world due to high cost and implementation time. Considering simulation as a key to this constraint, various software have been developed that can imitate the physical behavior of Edge/Fog computing environments. Nevertheless, the existing simulators often fail to support advanced service management features because of their monolithic architecture, lack of actual dataset, and limited scope for a periodic update. To overcome these issues, we have developed modular simulation models for service migration, dynamic distributed cluster formation, and microservice orchestration for Edge/Fog computing based on real datasets and extended the basic components of iFogSim, a widely used Edge/Fog computing simulator for their ease of adoption as iFogSim2. The performance of iFogSim2 and its built-in service management policies are evaluated using three use case scenarios and compared with the contemporary simulators and benchmark policies under different settings. Results indicate that our simulator consumes less memory and minimizes simulation time by an average of 28% when compared to other simulators."
pub.1171443840,Enhancing Microservices Efficiency: Integrating Adaptive Learning for Automated Scaling in Cloud Environments,"Cloud services have experienced rapid growth, leading to an escalation in challenges related to service monitoring and resulting in higher maintenance costs. The size of cloud applications has expanded, and the workload on the cloud system application fluctuates over time, progressively increasing. The cloud's capability to manage additional resources and relinquish them during periods of inactivity is a valuable feature. However, traditional monolithic applications pose difficulties in terms of maintenance. To address this issue, the transformation of cloud applications into microservices has proven effective. Microservices comprise heterogeneous services with intricate communication patterns, aiming to enhance scalability and facilitate flexible provisioning. This paper proposes the integration of adaptive learning into Cloud Watch, enabling automatic, data-driven scaling decisions. This approach enhances the cost- and time-effectiveness of microservices, presenting a solution to the challenges associated with managing monolithic applications."
pub.1169049468,Cloud Adoption in Accounting Information Systems in Asia & SOC 2® Report – An Empirical Study on Industry’s Perspective,"ABSTRACT Many business and technology organizations see cloud adoption and migrating existing systems to the cloud as an accelerator of digital transformation. The benefits of cloud adoption are perceived as increased scalability and cost reduction. At the same time, there are concerns about whether the information on cloud-based systems is secure and whether the privacy of the data in a cloud environment is at risk. This research brings out the industry’s perspective, both from an end-user perspective as well as IT transformation and IT procurement decision makers of accounting information systems and enterprise resource planning systems in Asia geography, on the preference for cloud-based or on-premise systems, top enablers for cloud adoption and importance of SOC 2® as an assurance for information security and data privacy concerns. The findings of this research indicate that decision-makers for IT transformation and IT procurement prefer cloud-based accounting information systems and enterprise resource planning systems over on-premise systems. Scalability, cost reduction, business agility, business continuity and disaster recovery, and enhanced collaboration are top enablers for cloud adoption. This research also indicates that the SOC 2® report is increasingly seen to address information security and data privacy concerns. Keywords: SOC 2®; Trust services criteria; Information security; Data privacy; Cloud computing; Accounting Information Systems"
pub.1049155834,TouchCost: Cost Analysis of TouchDevelop Scripts,"TouchDevelop is a novel programming environment and language for mobile devices. These applications are typically developed by non-expert users, rather small, and published on the cloud. In this paper, we introduceTouchCost a new static analysis that infers the cost of loops in TouchDevelop programs.TouchCost (i) infers numerical invariants through an existing generic analyzer, (ii) extracts cost relation systems, and (iii) solves them using an existing upper bound solver.TouchCost has been applied to all TouchDevelop scripts that are currently published on the cloud. Experimental results show that this tool is both scalable and precise. Studying the outputs of TouchCost we glimpse two major applications: (i) establishing at runtime the cost of a loop, and in case move its execution, and (ii) helping non-expert developers to debug their programs."
pub.1024308829,A Transformation-Based Approach to Business Process Management in the Cloud,"Business Process Management (BPM) has gained a lot of popularity in the last two decades, since it allows organizations to manage and optimize their business processes. However, purchasing a BPM system can be an expensive investment for a company, since not only the software itself needs to be purchased, but also hardware is required on which the process engine should run, and personnel need to be hired or allocated for setting up and maintaining the hardware and the software. Cloud computing gives its users the opportunity of using computing resources in a pay-per-use manner, and perceiving these resources as unlimited. Therefore, the application of cloud computing technologies to BPM can be extremely beneficial specially for small and middle-size companies. Nevertheless, the fear of losing or exposing sensitive data by placing these data in the cloud is one of the biggest obstacles to the deployment of cloud-based solutions in organizations nowadays. In this paper we introduce a transformation-based approach that allows companies to control the parts of their business processes that should be allocated to their own premises and to the cloud, to avoid unwanted exposure of confidential data and to profit from the high performance of cloud environments. In our approach, the user annotates activities and data that should be placed in the cloud or on-premise, and an automated transformation generates the process fragments for cloud and on-premise deployment. The paper discusses the challenges of developing the transformation and presents a case study that demonstrates the applicability of the approach."
pub.1118853369,Towards Constraint-based High Performance Cloud System in the Process of Cloud Computing Adoption in an Organization,"Cloud computing is penetrating into various domains and environments, from
theoretical computer science to economy, from marketing hype to educational
curriculum and from R&D lab to enterprise IT infrastructure. Yet, the currently
developing state of cloud computing leaves several issues to address and also
affects cloud computing adoption by organizations. In this paper, we explain
how the transition into the cloud can occur in an organization and describe the
mechanism for transforming legacy infrastructure into a virtual
infrastructure-based cloud. We describe the state of the art of infrastructural
cloud, which is essential in the decision making on cloud adoption, and
highlight the challenges that can limit the scale and speed of the adoption. We
then suggest a strategic framework for designing a high performance cloud
system. This framework is applicable when transformation cloudbased deployment
model collides with some constraints. We give an example of the implementation
of the framework in a design of a budget-constrained high availability cloud
system."
pub.1093564442,How to Adapt Authentication and Authorization Infrastructure of Applications for the Cloud,"Migration of existing enterprise applications to the Cloud requires substantial adaptation effort in individual architectural components. Existing work has focused on migrating the application with functional and non-functional aspects. However, none of them has focused so far on the adaptation of security and privacy. In our previous work, Identity-as-a-service (IDaaS) decouples Authentication and Authorization Infrastructure (AAI) from the business logic of the application as a manageable resource for the Cloud provider to control its life cycle. Since IDaaS controls the complete security chain, it can coordinate automated trust negotiation between Cloud services in federated security domains. On the other hand, IDaaS provides identity federation for Cloud users to access multiple service providers on demand but also may preserve user's privacy. In this paper, we continue to model a security topology for the Cloud applications. A security topology describes an abstract layer of AAI's components, requirements, and trust relationship between them. It preserves the provisioning of AAI across different environments for interoperability, portability, and enables a dynamic trust relationship with other services on demand."
pub.1026275219,How to adapt applications for the Cloud environment,"The migration of existing applications to the Cloud requires adapting them to a new computing paradigm. Existing works have focused on migrating the whole application stack by means of virtualization and deployment on the Cloud, delegating the required adaptation effort to the level of resource management. With the proliferation of Cloud services allowing for more flexibility and better control over the application migration, the migration of individual application layers, or even individual architectural components to the Cloud, becomes possible. Towards this goal, in this work we focus on the challenges and solutions for each layer when migrating different parts of the application to the Cloud. We categorize different migration types and identify the potential impact and adaptation needs for each of these types on the application layers based on an exhaustive survey of the State of the Art. We also investigate various cross-cutting concerns that need to be considered for the migration of the application, and position them with respect to the identified migration types. Finally, we present some of the open research issues in the field and position our future work targeting these research questions."
pub.1123986909,Research on Construction of Cloud Computing Platform for Railway Enterprises,"Aiming at the existing problems of railway enterprise information system in cloud computing environment, this paper combined with extended TOE framework and RBV model, established a cloud computing research model for railway enterprise, and extracted the main factors affecting the adoption, and given some assumptions. This paper given the overall framework of cloud computing platform for railway enterprises, and built a complete enterprise-level cloud computing platform through open source products. At the same time, the paper elaborated the security defense system of Cloud-Pipeline-Terminal, and designed the security architecture of cloud computing, and provided multi-level capability support for railway enterprise information system, such as infrastructure, development platform, application and so on."
pub.1093389454,Nutshell: Cloud Simulation and Current Trends,"Cloud computing has experienced enormous popularity and adoption in many areas, such as research, medical, web, and e-commerce. Providers, like Amazon, Google, Microsoft, and Yahoo have deployed their cloud services for use. Cloud computing pay-as-you-go model, on demand scaling, and low maintenance cost has attracted many users. The widespread adoption of cloud paradigm upshots various challenges. The legacy data center and cloud architectures are unable to handle the escalating user demands. Therefore, new data center network architectures, policies, protocols and topologies are required. However, new solutions must be tested thoroughly, before deployment within a real production environment. As the experimentation and testing is infeasible in the production environment and real cloud setup, therefore, there is an indispensable need for simulation tools that provide ways to model and test applications, and estimate cost, performance, and energy consumption of services and application within cloud environment. Simulation tools providing cloud simulation environments currently are limited in terms of features and realistic cloud setups, focus on a particular problem domain, and require tool-specific modeling, which can be frustrating and time consuming. This paper aims to provide a detailed comparison of various cloud simulators, discuss various offered features, and highlight their strengths and limitations. Moreover, we also demonstrate our work on a new cloud simulator “Nutshell”, which offers realistic cloud environments and protocols. The Nutshell is designed to diminish flaws and limitations of available cloud simulators, by offering: (a) multiple datacenter network architectures, like three-tier, fat-tree, and dcell, (b) fine grained network details, (c) realistic cloud traffic patterns, (d) congestion control strategies and analysis, (e) energy consumption, (f) cost estimation, and (g) data center monitoring and analysis. Flexibility to stretch the architectures to simulate smart city IT infrastructure."
pub.1094659411,Tales of empirically understanding and providing process support for migrating to clouds,"Summary form only given. Cloud computing has become an attractive option for acquiring and using IT infrastructure and services. In order to exploit the potential benefits of cloud computing, existing software and services need to be migrated to cloud based infrastructures. A successful migration effort needs to be supported by a well-defined process and appropriate practices. However, there is a little evidence-based guidance for supporting cloud migration. One of our main goals is to experimentally understand the key technological, social, and organizational issues in evolving business critical software and services to cloud computing and devising and deploying appropriate solutions. To this end, we have been conducting and/or studying several case studies in industrial and academic environments. The lessons and evidence from these case studies have enabled us to pinpoint the areas of cloud migration that need immediate attention of practitioners and researchers, devise a process-centric support framework, and identify the educational and training needs for cloud migration."
pub.1123799195,Critical Influential Factors For Software Testing-as-a-Service Adoption: Preliminary Findings From Systematic Literature Review,"Software applications are currently becoming complex, dynamic, distributed, and component-based therefore, giving birth to a number of emerging challenges for the testing department. To cope with these emerging challenges employing Cloud Computing technology would be the best choice. However, moving software testing to the CC environment is not free of cost, nor it is the best possible solution for all testing problems. To guide decision maker on the adoption of CC for software testing, this study makes a significant input to the existing literature on the CC adoption in the context of software testing and can be considered the base for future research in the stated field. Specifically, this study discusses the necessary and sufficient condition for software testing in the Cloud. To guide software development organization for cloud-based testing adoption, this study identifies a list of factors using systematic literature review (SLR)."
pub.1172974256,Empowering Sustainable Industrial and Service Systems through AI-Enhanced Cloud Resource Optimization,"This study focuses on examining the shift of an application system from a traditional monolithic architecture to a cloud-native microservice architecture (MSA), with a specific emphasis on the impact of this transition on resource efficiency and cost reduction. In order to evaluate whether artificial intelligence (AI) and application performance management (APM) tools can surpass traditional resource management methods in enhancing cost efficiency and operational performance, these advanced technologies are integrated. The research employs the refactor/rearchitect methodology to transition the system to a cloud-native framework, aiming to validate the enhanced capabilities of AI tools in optimizing cloud resources. The main objective of the study is to demonstrate how AI-driven strategies can facilitate more sustainable and economically efficient cloud computing environments, particularly in terms of managing and scaling resources. Moreover, the study aligns with model-based approaches that are prevalent in sustainable systems engineering by structuring cloud transformation through simulation-supported frameworks. It focuses on the synergy between endogenous AI integration within cloud management processes and the overarching goals of Industry 5.0, which emphasize sustainability and efficiency that not only benefit technological advancements but also enhance stakeholder engagement in a human-centric operational environment. This integration exemplifies how AI and cloud technology can contribute to more resilient and adaptive industrial and service systems, furthering the objectives of AI and sustainability initiatives."
pub.1027708566,VCCBox: Practical Confinement of Untrusted Software in Virtual Cloud Computing,"Recent maturity of virtualization has enabled its wide adoption in cloud environment. However, legacy security issues still exist in the cloud and are further enlarged. For instance, the execution of untrusted software may cause more harm to system security. Though conventional sandboxes can be used to constrain the destructive program behaviors, they suffer from various deficiencies. In this paper, we propose VCCBox, a practical sandbox that confines untrusted applications in cloud environment. Leveraging the state-of-the-art hardware assisted virtualization technology and novel design, it is able to work effectively and efficiently. VCCBox implements its system call interception and access control policy enforcement inside the hypervisor and create an interface to dynamically load policies. The in-VMM design renders our system hard to bypass and easy to deploy in cloud environment, and dynamic policy loading provides high efficiency. We have implemented a proof-of-concept system based on Xen and the evaluation exhibits that our system achieves the design goal of effectiveness and efficiency."
pub.1093763352,Migrating Legacy Applications to the Cloud,"Although not a new technology, the combination of parallel computing and cloud environments can offer a number of benefits for many types of applications if the cost of application modifications combined with the costs of configuration and maintenance of the environment can be justified. This case study explores the use of cloud computing to provide a flexible deployment environment in which to run a migrated existing application using one of the popular parallel computing frameworks; Hadoop. To this end, we have documented the migration of a text-mining application, acting as a proxy for any existing application, through a number of stages progressing towards deployment in a cloud environment."
pub.1107774144,Towards an End-to-End Architecture for Run-Time Data Protection in the Cloud,"Protecting sensitive data is a key concern for the adoption of cloud solutions. Protecting data in the cloud is made particularly challenging by the dynamic changes that cloud systems may undergo at run-time, as well as the complex interactions among multiple software and hardware components, services, and stakeholders. Conformance to data protection requirements in such a dynamic environment cannot any longer be ensured during design time; e.g. due to the dynamic changes imposed by replication and migration of components. It requires run-time data protection mechanisms. This paper proposes combining multiple existing data protection approaches and extending them to run-time, ultimately delivering an end-to-end architecture for run-time data protection in the cloud. We validate the practical applicability of our approach by a commercial case study."
pub.1012360824,Performance and Cost Trade-Off in IaaS Environments: A Scientific Workflow Simulation Environment Case Study,"The adoption of the workflow technology in the eScience domain has contributed to the increase of simulation-based applications orchestrating different services in a flexible and error-free manner. The nature of the provisioning and execution of such simulations makes them potential candidates to be migrated and executed in Cloud environments. The wide availability of Infrastructure-as-a-Service (IaaS) Cloud offerings and service providers has contributed to a raise in the number of supporters of partially or completely migrating and running their scientific experiments in the Cloud. Focusing on Scientific Workflow-based Simulation Environments (SWfSE) applications and their corresponding underlying runtime support, in this research work we aim at empirically analyzing and evaluating the impact of migrating such an environment to multiple IaaS infrastructures. More specifically, we focus on the investigation of multiple Cloud providers and their corresponding optimized and non-optimized IaaS offerings with respect to their offered performance, and its impact on the incurred monetary costs when migrating and executing a SWfSE. The experiments show significant performance improvements and reduced monetary costs when executing the simulation environment in off-premise Clouds."
pub.1145705483,Security-Enhanced Cloud for Serverless Computing and Its Applications,"Computing and communication mainly rely on the infrastructure of the system. The operation of local infrastructure is very difficult in the COVID-19 pandemic situation. The operation in on-premises servers is becoming more challenging due to the huge workload and traffic. The platform must be able to provide an extra computing facility to manage the traffic. The evolution of new smart technologies with lots of sensor data and low latency mode increases the efficiency of the communication model. The latest technologically enhanced application is the cloud-centric model. The traditional cloud computing models are more complex in dealing with real-time data streams. The platform provides a place for all run-time applications to process. To improve the function of latency and efficiency of a system, new serverless computing is introduced. Serverless computing provides the application developer with emphasis on the development logic. No server management is required, and very less resource allocation is needed. The platform provides a better build in scalability and pay-as-you-go service with better utilization of resources. It is growing up as another persuading perspective change for sending cloud applications, by and large in light of the continuous development of undertaking application plans to compartments and miniature administrations. Serverless computing uses function-as-a-service (FaaS) model. FaaS liberates designers from the truly difficult work of working out or keeping up an unpredictable framework by executing code because of functions, which implies you can transfer secluded lumps of usefulness to the cloud that are executed autonomously to establish your service. Cloud computing is the process of delivering the services needed for software applications through the Internet. And the critical applications/data will be hosted in private cloud. In cloud computing, the user/client, and the hardware on which the application is run are decoupled and can be accessed/utilized remotely. Using serverless computing helps you overcome all these disadvantages/difficulties in running the application in a server-based environment. Computing and communication mainly rely on the infrastructure of the system. The operation of local infrastructure is very difficult in the COVID-19 pandemic situation. Serverless architectures try to improve the old-style customer worker structure by lessening handling load and permitting effective scaling. In general, the older method follows the client–server structure. Recalling through the authentic background of large business figuring, everything started with the concentrated PC and fundamental clients reliant on the video terminal or visual showcase unit."
pub.1118541578,Usage of Cloud Computing Simulators and Future Systems For Computational Research,"Cloud Computing is an Internet based computing, whereby shared resources,
software and information, are provided to computers and devices on demand, like
the electricity grid. Currently, IaaS (Infrastructure as a Service), PaaS
(Platform as a Service) and SaaS (Software as a Service) are used as a business
model for Cloud Computing. Nowadays, the adoption and deployment of Cloud
Computing is increasing in various domains, forcing researchers to conduct
research in the area of Cloud Computing globally. Setting up the research
environment is critical for the researchers in the developing countries to
evaluate the research outputs. Currently, modeling, simulation technology and
access of resources from various university data centers has become a useful
and powerful tool in cloud computing research. Several cloud simulators have
been specifically developed by various universities to carry out Cloud
Computing research, including CloudSim, SPECI, Green Cloud and Future Systems
(the Indiana University machines India, Bravo, Delta, Echo and Foxtrot)
supports leading edge data science research and a broad range of
computing-enabled education as well as integration of ideas from cloud and HPC
systems. In this paper, the features, suitability, adaptability and the
learning curve of the existing Cloud Computing simulators and Future Systems
are reviewed and analyzed."
pub.1167290802,Heterogeneity-aware Load Balancing in Serverless Computing Environments,"The current trend in cloud computing is the growing adoption of microservices. Microservices are specialized components of an application that can be deployed and scaled individually, potentially replacing monolithic applications. To meet this demand, cloud providers offer function-as-a-service (FaaS) or serverless computing. Additionally, there is an increasing use of containerized environments in Internet of Things (IoT) applications. It has been shown that implementing serverless computing at the edge of an IoT network for executing tasks can reduce the overall execution time of these tasks. In cloud and edge computing environments, the infrastructure typically exhibits heterogeneity, and this heterogeneity can significantly impact the performance of different invocations of a FaaS function. This paper addresses infrastructure heterogeneity by partitioning a heterogeneous cluster into homogeneous pools based on similar resource characteristics and profiling function performance. The key contributions of this paper are: (1) a serverless architecture design that extends the function placement and load-balancing capabilities of current serverless platforms to effectively manage infrastructure heterogeneity, (2) a load-balancing approach that efficiently utilizes multi-core hardware, and (3) an implementation of the architecture, thereby evaluating the impact of the extended capabilities."
pub.1107109188,Study on a collaborative platform for product development using cloud computing,"In a more digital-oriented world, companies need to be present on all customer communication channels, including collaborative social media platforms, and provide them with quick information and responses. The companies should provide unitary work experiences by integrating collaborative applications into existing systems and processes. The ability to connect employees, partners, vendors and customers to deliver real-time information is the basis for differentiation in the competitive environment. The main purpose of our work is to study a cloud collaborative platform that comes to support the companies and generates results through predefined work patterns and processes that connect employees and the necessary business decision-making data. In our case, the secret is the technology that leads to the existence of applications that can be accessed ""from the clouds"". Thanks to cloud computing, even small or medium-sized businesses that did not allow an investment in IT infrastructure or expensive software can have the most important technology in the world, just like multinational companies, but now at an insignificant cost. The medium and long-term challenge is related to integration between modules, and detailed integration with the production enterprise's internal information systems, as well as presence and integration with social media networks to create products distribution opportunities, staff recruitment, and collaboration on the ideas development and sharing of shared resources in the enterprise. Due to the diversified services offered on demand as well as to the Cloud Computing multi-tenant aspects (multi-tenant is a principle in the software architecture in which a single program running on a server simultaneously serves multiple client organizations), the traditional forms of audit and evaluation are not applicable or can have incomplete results. In the context of migration to cloud, within our research, we have sought to underline the importance of interoperability principle in the context of the current production environment which allows a rapid migration from one configuration to another using a cloud computing method. The main results of collaborative platforms 3DEXPERIENCE on CLOUD are as following: reduction by almost 10% the time needed to conclude new contracts, with about 15% the time needed to access information in the interest of the service, and with about 15% the costs of training programs for new employees. The most important consequence of the paper is the use of a 3DEXPERIENCE on CLOUD platform, providing new ideas for innovative business."
pub.1093915430,Benefits and Challenges of Three Cloud Computing Service Models,"Cloud computing can be defined as the use of new or existing computing hardware and virtualization technologies to form a shared infrastructure that enables web-based value added services. The three predominant service models are infrastructure, platform, and software-as-a-service. Infrastructure-as-a-Service (IaaS) can be defined as the use of servers, storage, and virtualization to enable utility like services for users. Security is a big concern within IaaS, especially considering that the rest of the cloud service models run on top of the infrastructure and related layers. Platform-as-a-Service (PaaS) providers offer access to APls, programming languages and development middleware which allows subscribers to develop custom applications without installing or configuring the development environment. Software-as-a-Service (SaaS) gives subscribed or pay-per-use users access to software or services which reside in the cloud and not on the user's device. Understanding the cloud service models is critical in determining if cloud services or hosting are an appropriate business solution, and if so, which model best balances the level of control required versus reduced hardware, configuration, and maintenance costs. Cloud computing offers many benefits to organizations; it has enabled collaboration amongst disparate communities and workgroups, and has overcome challenges that have plagued existing business solutions. However, the security, privacy, and integrity of the cloud are of prime importance and there are many challenges that exist. At the present time there seems to be a lot of momentum behind the adoption of cloud computing despite these. This may simply be a trend, an indication that society truly wants their data to be available whenever from anywhere, or a sign that few understand the associated risks."
pub.1152622314,Dynamic adaptation for distributed systems in model-driven engineering,"Modern-day software systems operate within complex, uncertain, and highly dynamic environments. Managing such systems is a significant challenge; developing self-managing autonomic systems is one way to reduce development and maintenance efforts. In the context of distributed systems, achieving this autonomy through dynamic adaptation is particularly challenging due to the volatile host environment. Model-Driven Engineering (MDE) is a software development paradigm that advocates the use of models as the primary artifacts rather than source code. MDE promises higher-quality software at a lower cost through abstractions, automation, and analyses. The goal of our work is to leverage MDE to facilitate the development and maintenance of distributed applications with dynamic adaptation capabilities. We assume that the structure and behavior of the application has been modeled using the Component-and-Connector (C&C) paradigm and the 'Monitor-Analyze-Plan-Execute with shared Knowledge' (MAPE-K) reference architecture. In the initial work, we have developed a model-level monitoring infrastructure, and adapted existing code generation and deployment support to generate a distributed system from the C&C models and deploy it automatically on a suitable platform. In future work, we plan to investigate how the monitoring and adaptation capabilities of cloud-native containerization and orchestration platforms (i.e., Docker and Kubernetes) can be leveraged for dynamic adaptation, and how this system-level adaptation can be combined effectively with any model-level monitoring, planning, and adaptation capabilities."
pub.1166611783,Cloud-based Data Analytics for Business Intelligence,"Abstract: In the era of data-driven decision-making, organizations are turning to cloud-based data analytics for business intelligence to overcome the limitations of traditional on-premises systems. This paradigm shift offers the promise of scalable, agile, and advanced analytics capabilities. This paper explores the landscape of cloud- based data analytics for business intelligence by investigating existing systems, challenges, and opportunities. The study first examines leading cloud platforms such as Amazon Web Services (AWS) Redshift, Microsoft Azure Synapse Analytics, Google BigQuery, and Snowflake[11]. It evaluates their features, scalability, and integration options to assess their suitability for modern BI needs. Moreover, the research identifies critical challenges in transitioning to cloud-based analytics, including data integration complexities, security concerns, and cost management. The integration of advanced analytics techniques, such as machine learning and AI, into cloud- based environments is also explored. The study delves into the benefits and challenges of predictive analytics, anomaly detection, and other emerging capabilities that empower organizations to extract deeper insights from data. Furthermore, hybrid cloud architectures, which combine on-premises infrastructure with cloud resources, are investigated. Strategies for seamless data integration and workload distribution are discussed, enabling organizations to strike a balance between performance and data governance."
pub.1102466796,Cloud Service vs. On-Premises Software: Competition Under Security Risk and Product Customization,"Cloud computing services are transforming business and government at an ever-increasing rate. However, the associated security risk and low customization capability create challenges for the penetration of cloud services. In an environment where the negative network security externalities are present, we study the competitive implications when a later entrant cloud provider, who continually makes improvement to its cloud service, competes with an on-premises software provider that offers a customized upgraded product. Using a two-stage game-theoretic model to capture the product differentiation in customization capability and security risk, we highlight the importance of cloud provider’s product development and the role of security externality on consumers’ choices and providers’ strategies. We show that the security loss borne by customers may not always increase in the risk of targeted attacks associated with cloud use, especially when the quality improvement made by the cloud provider is very low. In such circumstance, an increase in the security risk can generate higher profit for the cloud provider. We also demonstrate that under certain conditions, a higher customization capability of the on-premises software makes both providers worse off. We further investigate the cloud provider’s product investment decision. Our results show that in a low switching cost environment, the cloud provider should reduce its investment as the security risk increases, but increase its investment as the customization capability of the on-premises software becomes higher. By extending our base model to consider the providers’ investments in cloud security and product customization, we suggest that the providers should take into account the product quality differentiation when making investment decisions. If the cloud service has a sufficiently high quality, the cloud provider should invest more to reduce the security risk, and the on-premises software provider should respond the high quality service by enhancing its product customization capability."
pub.1001784996,Service delivery models of cloud computing: security issues and open challenges,"Abstract Cloud computing represents the most recent enterprise trend in information technology and refers to the virtualization of computing resources that are available on demand. Cloud computing saves cost and time for businesses. Moreover, this computing process reflects a radical technological revolution in how companies develop, deploy, and manage enterprise applications over the Internet. Virtualized cloud computing mainly offers cloud‐computing delivery models such as software as a service, platform as a service, and infrastructure as a service. Security and privacy are presently considered critical factors in the adaptation of any cloud‐service delivery model. Cloud computing leverages several technologies; in the process, this model can inherit potential security threats. Thus far, security issues in cloud computing have rarely been addressed at the service delivery level. Key security concerns include Web application security, network security, data security, integration, vulnerabilities in the virtualized environment, and physical security. The aim of this research is to comprehensively present the security threats with respect to their cloud service deliver models. This study also determines how service delivery models differ from existing enterprise applications, classify these models, and investigate the inherent security challenges. This study primarily focuses on the security concerns of each layer of the cloud‐service delivery model, as well as on existing solutions and approaches. In addition, countermeasures to potential security threats are also presented for each cloud model. Copyright © 2016 John Wiley & Sons, Ltd."
pub.1105173594,Agile development in the cloud computing environment: A systematic review,"Background: Agile software development is based on a set of values and principles. The twelve principles are inferred from agile values. Agile principles are composition of evolutionary requirement, simple design, continuous delivery, self-organizing team and face-to-face communication. Due to changing market demand, agile methodology faces problems such as scalability, more effort and cost required in setting up hardware and software infrastructure, availability of skilled resource and ability to build application from multiple locations. Twelve (12) principles may be practiced more appropriately with the support of cloud computing. This merger of agile and cloud computing may provide infrastructure optimization and automation benefits to agile practitioners. Objective: This Systematic Literature Review (SLR) identifies the techniques employed in cloud computing environment that are useful for agile development. In addition, SLR discusses the significance of cloud and its challenges. Method: By applying the SLR procedure, the authors select thirty-seven (37) studies out of six-hundred-forty-seven (647) from 2010 to 2017. Result: The result of SLR shows that the techniques using existing tools were reported in 35%, simulations in 20% and application developed in 15% of the studies. Evaluation of techniques was reported in 32% of the studies. The impact of cloud computing was measured by the classification of four major categories such as transparency 32%, collaboration 50%, development infrastructure 29% and cloud quality attributes in 39%. Furthermore, a large number of tools were reported in primary studies. The challenges posed by cloud adoption in agile was reported as interoperability 13%, security & privacy 18% and rest of the primary studies did not report any other research gaps. Conclusions: The study concludes that agile development in cloud computing environment is an important area in software engineering. There are many open challenges and gaps. In particular, more quality tools, evaluation research and empirical studies are required in this area."
pub.1095841374,Internet-of-things and Cloud Computing for Smart Industry: A Systematic Mapping Study,"Intelligent industry and manufacturing requires obtaining relevant sensor data and process information in realtime from all components in the manufacturing value-chain. It is envisioned that smart industry is achieved by embedding connectivity into industrial products, using Cloud and Internet-of-things (IoT) to leverage intelligence and actionable knowledge for machines, autonomous collaboration among machines, and integration of products and additional value-added services. For complex industrial systems, it is important to ensure a smooth transformation towards the smart industry vision despite of the associated challenges with respect to e.g., transition from the traditional multi-layered architecture to an open structured service-oriented automation system architecture, changes of business models and strategies, legacy system migration to cloud environment, etc. The focus of this study is therefore to examine the status of the existing research on cloud computing and IoT solutions that enable this transformation towards a smart industry. We applied the systematic mapping study method to obtain an overview of the existing related research literatures that focus on smart industry, industrial automation and manufacturing perspective. We also discuss the future research areas that need to be enhanced."
pub.1091032753,A Holistic Decision Framework to Avoid Vendor Lock-in for Cloud SaaS Migration,"Cloud computing offers an innovative business model to enterprise for IT services consumption and delivery. Software as a Service (SaaS) is one of the cloud offerings that attract organisations as a potential solution in reducing their IT cost. However, the vast diversity among the available cloud SaaS services makes it difficult for customers to decide whose vendor services to use or even to determine a valid basis for their selections. Moreover, this variety of cloud SaaS services has led to proprietary architectures and technologies being used by cloud vendors, increasing the risk of vendor lock-in for customers. Therefore, when enterprises interact with SaaS providers within the purview of the current cloud marketplace, they often encounter significant lock-in challenges to migrating and interconnecting cloud. Hence, the complexity and variety of cloud SaaS service offerings makes it imperative for businesses to use a clear and well understood decision process to procure, migrate and/or discontinue cloud services. To date, the expertise and technological solutions to simplify such transition and facilitate good decision making to avoid lock-in risks in the cloud are limited. Besides, little investigation has been carried out to provide a comprehensive decision framework to support enterprises on how to avoid lock-in risks when selecting and implementing cloud-based SaaS solutions within existing environments. Such decision framework is important to reduce complexity and variations in implementation patterns on the cloud provider side, while at the same time minimising potential switching cost for enterprises by resolving integration issues with existing IT infrastructures. This paper proposes a holistic 6-step decision framework that enables an enterprise to assess its current IT landscape for potential SaaS replacement, and provides effective strategies to mitigate vendor lock-in risks in cloud (SaaS) migration. The framework follows research findings and addresses the core requirements for choosing vendor-neutral interoperable and portable cloud services without the fear of vendor lock-in, and architectural decisions for secure SaaS migration. Therefore, the results of this research can help IT managers have a safe and effective migration to cloud computing SaaS environment."
pub.1099915017,Container-based microservice architecture for cloud applications,"Cloud Environment allows enterprises to scale their application on demand. Microservice design is a new paradigm for cloud application development which is gaining popularity due to its granular approach and loosely coupled services unlike monolithic design with single code base. Applications developed using microservice design results in better scaling and gives extended flexibility to the developers with minimum cost. In this paper, first, different challenges in deployment and continuous integration of microservices are analyzed. To overcome these challenges, later, an automated system is proposed and designed which helps in deployment and continuous integration of microservices. Containers are recently heavily used in deploying the applications as they are easy to manage and lightweight when compared to traditional Virtual Machines (VMs). We have deployed the proposed microservices architecture on the docker containers and tested using a social networking application as case study. Finally, the results are presented and the performance of monolithic and microservice approach is compared using various parameters such as response time, throughput, deployment time etc. Results show that application developed using microservice approach and deployed using the proposed design reduce the time and effort for deployment and continuous integration of the application. Results also shows that microservice based application outperform monolithic design because of its low response time and high throughput."
pub.1021814710,A Generic Software Development Process Refined from Best Practices for Cloud Computing,"Cloud computing has emerged as more than just a piece of technology, it is rather a new IT paradigm. The philosophy behind cloud computing shares its view with green computing where computing environments and resources are not as subjects to own but as subjects of sustained use. However, converting currently used IT services to Software as a Service (SaaS) cloud computing environments introduces several new risks. To mitigate such risks, existing software development processes must undergo significant remodeling. This study analyzes actual cases of SaaS cloud computing environment adoption as a way to derive four new best practices for software development and incorporates the identified best practices for currently-in-use processes. Furthermore, this study presents a design for generic software development processes that implement the proposed best practices. The design for the generic process has been applied to reinforce the weak points found in SaaS cloud service development practices used by eight enterprises currently developing or operating actual SaaS cloud computing services. Lastly, this study evaluates the applicability of the proposed SaaS cloud oriented development process through analyzing the feedback data collected from actual application to the development of a SaaS cloud service Astation."
pub.1035423526,Securing integration of cloud services in cross-domain distributed environments,"Traditional cloud integration scenarios, as adopted by many organizations, assume business processes to be executed in a cross-domain context, connecting on-premise and cloud applications. The emerging model of cloud-based integration platforms extends these scenarios by transferring business process execution entirely to the cloud. Although this approach provides numerous benefits and opens a new range of opportunities, its adoption requires reconsideration of currently applied practices and their adjustment to a new perspective. In this work, we analyze the existing approaches to cross-domain service composition based on cloud integration platforms. We particularly focus on the security of these approaches, considering currently dominant OAuth 2.0 web authorization protocol and emerging UMA protocol. For this purpose, we present a new tool that enables UMA support in Apache Camel integration framework. We then analyze and discuss the integration flows relying on both protocols. Finally, based on RMIAS framework, we provide a security assessment of both approaches, presenting an overview of issues and challenges for future work."
pub.1094571755,Cloud Transformation Analytics Services a Case Study of Cloud Fitness Validation for Server Migration,"Migration of IT infrastructure to the Cloud transforms enterprise data, applications, and services to one or more other Cloud environments. Cloud migration engagements often rely on an in-depth discovery of the client's (source) IT environment, which is rather costly and can take up to six weeks before any meaningful conversations with customers can begin about the migration itself. There is a demand for a more agile approach to enable sales teams to perform rapid qualification of cloud fitness and reason about the benefits of Cloud using minimal information from the clients. The existing, consulting-based approach typically relies on a number of discovery and analysis tools, yet the entire process is manual, expensive, and time consuming. In this paper we present a suite of cloud transformation analytics (CTA) services designed to streamline the process of pre-migration and migration analysis, such as cloud fitness validation and consolidation recommendations. CTA supports reasoning about diverse target clouds, as well as various transformation methods to match clients' needs, such as image migration, workload migration, and cross platform migration. We discuss our key insights and lessons learned from employing cloud fitness validation capability on datasets of up to 2000 servers to enable and accelerate the process of migration."
pub.1170400993,Secure the 5G and Beyond Networks with Zero Trust and Access Control Systems for Cloud Native Architectures,"5G networks are highly distributed, built on an open service-based architecture that requires multi-vendor hardware and software development environments, all of which create a high attack surface in the 5G networks than other proprietary fixed-function networks. Besides that, cloud-native architectures also present new security challenges. Cloud-native separates monolithic virtual machines into microservice pods, resulting in higher volumes of signaling and communication flowing through and between microservices. In addition, secure connections in monolithic applications have been replaced by untrusted communication between microservice pods, requiring additional cybersecurity capabilities. Access control systems were created to provide reliability and limit access to an organization’s assets. However, due to technology's constant evolution and dynamicity, these conventional security systems lack the security to protect an organization’s information because they were created to address access control for known users. For 5G based cloud native technology, these access controls need to be taken further by implementing a Zero Trust model to secure one’s essential assets for all users within the system. Zero Trust is implemented in an access control system under the concept ""Never Trust, Always Verify"". In this paper, we implement zero trust as a factor within access control systems by combining the principles of access control systems and zero-trust security by factoring in the user’s historical behavior and recommendations into the mix."
pub.1146010019,Workload Scheduling in Fog and Cloud Environments: Emerging Concepts and Research Directions,"In recent years, we have been witnessing the growing adoption of infrastructure virtualization technologies and cloud computing. A wide range of applications has been migrated from traditional computing environments to the cloud. On the other hand, organizations with existing on-premises infrastructure investments are making the shift to hybrid cloud, in order to leverage the security provided by the private cloud and the virtually unlimited resources of the public cloud. With the rapid expansion of the Internet of Things, fog computing emerged as a new paradigm, extending the cloud to the network edge, closer to where the data are generated. The workloads on such platforms tend to be complex, featuring various degrees of parallelism. Consequently, one of the major challenges involved with fog and cloud computing, is the effective and efficient scheduling of the workload. In this chapter, we provide the necessary background in this field and present an overview of the emerging concepts and techniques, exploring future research directions."
pub.1148001786,AWS cloud computing platforms deployment of landing zone - Infrastructure as a code,"The concept of this project is to create infrastructure as code dynamic infrastructure platforms to deploy and manage different applications design like, microservicesapplications, iot applications, legacy applications etc. so the application needs the infrastructure which supports both legacy and micro services with automations in all environments. The main concept is to move the legacy application to self-healing, self-management infrastructure cloud environment. A variety of tools exist that manage the infrastructure provisioning and use scripts to define the final state of the hardware to be deployed in the cloud. However, there are major challenges that need to be addressed to automate the infrastructure management so that they are effectively used in initiatives such as devops. In particular, the management of infrastructure as a code (iac) is one of the most important technical challenges to support activities such as the integration, deployment, and continuous delivery of applications. to address this problem, we present a support for the management of devops tools, through the definition of a domain specific language (dsl) based on the concept of infrastructure as a code, and a tool that supports this language allowing to model the final state of a provisioning infrastructure in the cloud and generating the provisioning scripts for the Amazon web services (aws) platform. The proposed tool reduces the work for development and operations personnel and facilitates their communication."
pub.1107124797,How the US Federal Communications Commission Managed the Process of IT Modernization,"Situation faced: This case examines how the U.S. Federal Communications Commission (FCC) executed its information technology (IT) modernization effort. In 2013, the FCC was spending about 80% of its IT budget on maintaining its legacy systems. Further, the FCC had experienced constant changes in top leadership that resulted in several fragmented IT modernization efforts. The outdated IT systems were not only costly to maintain but were prone to cyber-attacks and verge of major failure. And, the employee morale was lower, and they feared IT modernization and transformation. Overall, the FCC faced several technical and human challenges with IT modernization.Action taken: Acknowledging the eight previous years of fragmented implementation, the new CIO conducted inventory of both IT and human infrastructure. The CIO commissioned an IT tech team to conduct an inventory of the existing IT infrastructure in the organization with a focus to understand vulnerabilities and level of exposure to cyber security. Further, the CIO also took steps to understand the sentiments of employees, customers, and top leadership about IT modernization efforts. Public agencies often promote silo functioning and employees are fearful about change. Thus, the CIO designed several initiatives to solicit feedback from diverse stakeholders and regularly engage them in the process of IT modernization.Results achieved: The FCC moved 207 on premise IT systems to either public cloud environments or with a commercial service provider. In the process of this successful transformation, the Commission reduced the amount spent on operating and maintaining systems from over 85% to less than 50%. The FCC achieved this with a flat budget, thereby increasing the percentage of funds available for new development even. The FCC also reduced the time it took to prototype new systems from approximately 7 months to less than 48 h to produce a prototype.Lessons learned: The FCC’s IT modernization efforts offer following lessons to C-suite leaders: (1) develop a IT modernization strategy that includes both IT systems and the people supporting it; (2) plan a phased approach that achieves ‘quick wins’ in cloud implementation to increases momentum; (3) take time to align both top leadership and employees’ expectations with the IT modernization effort during planning; (4) adopt an open innovation approach that encourages and empower ‘change agents’ within the agency to creatively address in a cloud environment the longstanding challenges associated with the agency’s legacy endeavors, IT systems, and roles; and (5) effectively engage and communicate openly with internal and external stakeholders."
pub.1129826585,Multi-tenancy Cloud Access and Preservation,"Virginia Tech Libraries has developed a cloud-native, microservervices-based digital libraries platform to consolidate diverse access and preservation infrastructure into a set of flexible, independent microservices in Amazon Web Services. We have been an implementer and contributor to various community digital library and repository projects including DSpace, Fedora, and Samvera3. However, the complexity and cost of maintaining disparate application stacks have reduced our capacity to build new infrastructure. Virginia Tech has a long history of participation in and contribution to community-driven Open Source projects and has, in that time, developed more than a dozen independent applications architected on these stacks. The cost of independently addressing vulnerabilities, which often requires work to mitigate incompatibilities; reworking each application to comply with developing branding guidelines; and feature development and improvement has burgeoned, threatening to overwhelm our capacity. Like many of our peers5, our maintenance obligations have made continued growth unsustainable and have pushed older applications to near abandonware. We have designed and developed the Digital Libraries Platform to address these concerns thus reducing our maintenance obligations and costs associated with feature development across digital libraries. This approach represents a departure from the monolithic architectures of our legacy systems and, as such, shares more infrastructure among individual digital library implementations. The shared infrastructure facilitates rapid inclusion of new and improved features into each digital library instance. New features can be developed independent of any digital library instance and integrated into that instance by inclusion of that feature in the React/Amplify template. Changes to the template super class, such as those necessitated by evolving branding guidelines, may be immediately inherited by the template instances that subscribe to it. The platform implements Terraform6 deployment templates, Lambda serverless functions, and other cloud assets to form a microservices architecture on which multiple template-based sites are built. Individual sites are configured in AWS DynamoDB, Amazon's NoSQL database service, and via modification of shared template. Additional services provide digital preservation support including auditing, file fixity validation, replication to external cloud storage providers, file format characterization, and deposit to third-party preservation services. This presentation also discusses the cost of operating these services in AWS and strategies for mitigating those costs. These strategies include containerization to allow deployment of high-cost, asynchronous services to local infrastructure to take full advantage of existing infrastructure and advantageous utility pricing while allowing for local redeployment. In the past, developers worked in local, independent environments. New feature"
pub.1094158695,Intercloud Architecture Framework for Heterogeneous Cloud based Infrastructure Services Provisioning On-Demand,"This paper presents on-going research to develop the Intercloud Architecture Framework (ICAF) that addresses problems in multi-provider multi-domain heterogeneous cloud based infrastructure services and applications integration and interoperability, to allow their on-demand provisioning. The paper refers to existing standards and ongoing standardisation activity in Cloud Computing, in particular, recently published NIST Cloud Computing Reference Architecture (CCRA) and ITU-T JCA-Cloud activity. The proposed ICAF defines four complementary components addressing Intercloud integration and interoperability: multi-layer Cloud Services Model that combines commonly adopted cloud service models, such as IaaS, PaaS, SaaS, in one multilayer model with corresponding inter-layer interfaces; Intercloud Control and Management Plane that supports cloud based applications interaction; Intercloud Federation Framework, and Intercloud Operations Framework. The paper briefly describes the Service delivery and lifecycle management as an important ICAF component that provides a basis for consistent management and security of the provisioned on-demand complex cloud based services. The paper describes an implementation of the Intercloud Control and Management Plane in the GEYSERS project to allow optimal provisioning of the combined Network+IT resources in the inter-cloud environment. The proposed architecture is intended to provide an architectural model for developing Intercloud middleware and in this way will facilitate clouds interoperability and integration."
pub.1050204922,Energy-Aware Profiling for Cloud Computing Environments,"Cloud Computing has changed the way in which people use the IT resources today. Now, instead of buying their own IT resources, they can use the services offered by Cloud Computing with reasonable costs based on a “pay-per-use” model. However, with the wide adoption of Cloud Computing, the costs for maintaining the Cloud infrastructure have become a vital issue for the providers, especially with the large input of energy costs to underpin these resources. Thus, this paper proposes a system architecture that can be used for profiling the resources usage in terms of the energy consumption. From the profiled data, the application developers can enhance their energy-aware decisions when creating or optimising the applications to be more energy efficient. This paper also presents an adapted existing Cloud architecture to enable energy-aware profiling based on the proposed system. The results of the conducted experiments show energy-awareness at physical host and virtual machine levels."
pub.1095151534,Smart Fabric - An Infrastructure-Agnostic Artifact Topology Deployment Framework,"The cloud computing paradigm enables the development of applications that can elastically react to changes in their environment by autonomously provisioning and releasing infrastructure resources. However, current applications need to be specifically tailored to a concrete cloud provider infrastructure, leading to vendor lock-in. Migrating applications to the cloud or between cloud providers is challenging due to differences in deployment directives, available services, and programming interfaces. Existing infrastructure as code approaches closely tie application artifacts to their deployment directives and do not allow for a clear separation of application artifacts from deployment infrastructure. In this paper, we present Smart Fabric, a methodology and accompanying toolset for infrastructure-agnostic deployment of application artifact topologies based on a constraint-based, declarative specification of the required deployment infrastructure. Our framework allows for seamless migration of application topologies between deployment targets and enables independent, parallel evolution of both, applications and underlying infrastructure. We discuss the feasibility of the proposed methodology and prototype implementation using representative applications from the Internet of Things and smart city domains."
pub.1148348130,"CTR: Checkpoint, Transfer, and Restore for Secure Enclaves","Hardware-based Trusted Execution Environments (TEEs) are becoming
increasingly prevalent in cloud computing, forming the basis for confidential
computing. However, the security goals of TEEs sometimes conflict with existing
cloud functionality, such as VM or process migration, because TEE memory cannot
be read by the hypervisor, OS, or other software on the platform. Whilst some
newer TEE architectures support migration of entire protected VMs, there is
currently no practical solution for migrating individual processes containing
in-process TEEs. The inability to migrate such processes leads to operational
inefficiencies or even data loss if the host platform must be urgently
restarted.
  We present CTR, a software-only design to retrofit migration functionality
into existing TEE architectures, whilst maintaining their expected security
guarantees. Our design allows TEEs to be interrupted and migrated at arbitrary
points in their execution, thus maintaining compatibility with existing VM and
process migration techniques. By cooperatively involving the TEE in the
migration process, our design also allows application developers to specify
stateful migration-related policies, such as limiting the number of times a
particular TEE may be migrated. Our prototype implementation for Intel SGX
demonstrates that migration latency increases linearly with the size of the TEE
memory and is dominated by TEE system operations."
pub.1095596460,Cloud Migration Using Automated Planning,"Cloud migration transforms company's data, applications and services to (or between) one or more other Cloud environments. Enterprises are increasingly migrating their IT infrastructures to Cloud, given the appeal of (pay-per-use) elastic resources. Yet, existing IT infrastructures are complex, heterogeneous and dynamic ecosystems. As a result, there is no single standardized process to seamlessly manage migration at enterprise scale, and often significant level of manual intervention is required, both in reasoning about migration and during its execution. This paper presents a system that automates the process of migration to Cloud. It embeds a Metric-FF Artificial Intelligence (AI) planning algorithm to dynamically assemble migration plans based on the properties of source and target environments, as well as available migration tooling. The paper describes the challenges in migration planning, AI domain design for migration. This work demonstrates that the system provides an effective and scalable solution to generating plans based on the source environment of 700 servers, and varying size of the migration service requests."
pub.1120201016,"A Safe, Efficient and Integrated Indoor Robotic Fleet for Logistic Applications in Healthcare and Commercial Spaces: The ENDORSE concept","Hospitals are rightfully considered a field of indoor logistic robotics of high commercial potential. However, today, only a handful of mobile robotic solutions for hospital logistics exist that have failed to trigger widespread acceptance by the market. This is because existing systems require costly infrastructure installation, they do not easily integrate to corporate IT solutions, are not adequately shielded from cybersecurity threats, and as a result, they do not fully automate procedures and traceability of the items they carry. Moreover, existing systems are limited on scope, focusing only on delivery services, and hence do not provide any other type of support to the medical and nursing staff. ENDORSE system will address the aforementioned technical challenges and functional limitations by pursuing four innovation pillars: (i) infrastructure-less multi-robot indoor navigation; (ii) advanced Human-Robot Interaction (HRI) for resolving deadlocks and achieving efficient sharing of space resources in crowded environments; (iii) deployment of the ENDORSE software as a cloud-based service facilitating its integration with corporate software solutions, complying with GDPR data security requirements; (iv) reconfigurable and modular hardware architectures so that diverse modules can be easily swapped. ENDORSE functionality will be demonstrated via the integration of an e-diagnostic support module for vital signs monitoring on a fleet of mobile robots, facilitating connectivity to cloud-based Electronic Health Records (EHR), and validated in an operational hospital environment for realistic assessment."
pub.1093683671,"Information Modelling and Semantic Linking for a Software Workbench for Interactive, Time Critical and Self-Adaptive Cloud Applications","Cloud environments can provide elastic, controllable on-demand services for supporting complex distributed applications. However the engineering methods and software tools used for developing, deploying and executing classical time-critical applications do not, as yet, account for the programmability and controllability that can be provided by clouds, and so time-critical applications do not yet benefit from the full potential of virtualisation technologies. A software workbench for developing, deploying and controlling time-critical applications in cloud environments can address this, but needs to be able to interoperate with existing cloud standards and services in a fashion that can still adapt to the continuing evolution of the field. Semantic linking can enhance interoperability by creating mappings between different vocabularies and specifications, allowing different technologies to be plugged together, which can then be used to buld such a workbench in a flexible manner. A semantic linking framework is presented that uses a multiple-viewpoint model of a cloud application workbench as a means to relate different cloud and quality of service standards in order to aid the development of time-critical applications. The foundations of such a model, developed as part of the H2020 project SWITCH, are also presented."
pub.1166831829,Architecture of a Function-as-a-Service Application,"Serverless computing and Function-as-a-Service (FaaS) are programming paradigms that have many advantages for modern, distributed and highly modular applications. However, the process of transforming a legacy, monolithic application into a set of functions suitable for a FaaS environment can be a complex task. It may be questionable whether the obvious advantages received from such a transformation outweigh the effort and resources spent on it. In this paper we present our continuing research aimed at the transformation of legacy applications into the FaaS paradigm. Our test subject is an airport visibility system, a sub-class of the meteorological services required for airport operations. We have chosen to modularize the application, divide it into parts that can be implemented as functions in the FaaS paradigm, and provide it with a simple cloud-based data management layer. The tools that we are using are Apache OpenWhisk for FaaS and Airflow for workflow management, Apache Airflow for workflow management and NextCloud for cloud storage. Only a part of the original application has been transformed, but it already allows us to draw some conclusions and especially start forming a generalized picture of a Function-as-a-Service application."
pub.1018111651,Supporting the internet-based evaluation of research software with cloud infrastructure,"Due to license restrictions and installation issues, it is often not feasible to experiment with software without making substantial investments. Especially in the case of legacy tools, it turns out that even free software is often too costly (i.e., time-consuming) to be installed for evaluating the quality of a research contribution. After organizing a series of events related to software modeling, we have constructed (and started to use) SHARE, a system for sharing practically any type of software artifact to reviewers and to other participants who have very limited time available. The system relies on cloud-computing technologies to provide online access to interactive environments containing all the tools, documentation, input and output models to reproduce alleged research results. The system also enables one to clone such an environment and add additional models or tools in order to extend a contribution or pinpoint a problem. In retrospect, we observe that the approach is not limited to software modeling and SHARE is in fact gaining acceptance in other fields already."
pub.1182247890,Cloud Migration Analysis for Core System Infrastructure in Financial Services: Study Case PT XYZ,"Currently, cloud computing adoption is being utilized in the financial sector for its flexibility and innovative technologies. This study assesses PT XYZ's readiness to migrate its core systems to a cloud environment, which is crucial for maintaining competitive agility and robustness in Indonesia's financial services sector. Focusing on the diffusion of innovations (DOI) model and the technology-organization-environment (TOE) framework, the research examines ten factors influencing PT XYZ's migration strategy. The analysis identifies challenges related to PT XYZ's aging on-premises infrastructure, including server procurement delays and maintenance issues, which impact operational efficiency. Through semi-structured interviews with key IT personnel, this study explores perceptions of the benefits and challenges of cloud computing, as well as regulatory compliance, aiming to guide PT XYZ toward a successful cloud migration that ensures continuity, compliance, and enhanced operational capabilities. The findings contribute to understanding the relative advantages, such as cost savings and security concerns, complexity, compatibility, technology readiness, top management support, firm size, competitive pressure, and regulatory factors. PT XYZ’s decision to migrate aligns with all the factors for cloud computing adoption, with the relative advantage, complexity, compatibility, technological readiness, and top management support being high impacts on the migration decision, while regulatory compliance presents a challenge for the financial services industry in Indonesia].
  "
pub.1093198738,IntegrityMR: Integrity Assurance Framework for Big Data Analytics and Management Applications,"Big data analytics and knowledge management is becoming a hot topic with the emerging techniques of cloud computing and big data computing model such as MapReduce. However, large-scale adoption of MapReduce applications on public clouds is hindered by the lack of trust on the participating virtual machines deployed on the public cloud. In this paper, we extend the existing hybrid cloud MapReduce architecture to multiple public clouds. Based on such architecture, we propose IntegrityMR, an integrity assurance framework for big data analytics and management applications. We explore the result integrity check techniques at two alternative software layers: the MapReduce task layer and the applications layer. We design and implement the system at both layers based on Apache Hadoop MapReduce and Pig Latin, and perform a series of experiments with popular big data analytics and management applications such as Apache Mahout and Pig on commercial public clouds (Amazon Ee2 and Microsoft Azure) and local cluster environment. The experimental result of the task layer approach shows high integrity (98% with a credit threshold of 5) with non-negligible performance overhead (18% to 82% extra running time compared to original MapReduce). The experimental result of the application layer approach shows better performance compared with the task layer approach (less than 35% of extra running time compared with the original MapReduce)."
pub.1095637270,Transforming Vertical Web Applications into Elastic Cloud Applications,"There exists a huge amount of vertical applications that are developed for isolated computing environments. Due to increasing demand for additional resources there is a clear need to adapt these applications to the distributed environments. However, this is not an easy task and numerous variants are possible. Moreover, in this transition a new quality requirements become important, such as application elasticity. Application elasticity has to be built into a software system to enable smooth cost optimization at the run-time. In this paper, we provide a framework for evaluating different transformation variants of vertical Java EE multi-tiered applications into elastic cloud applications. With support of this framework the software developer is guided how to transform its application achieving optimal elasticity strategy. The framework is evaluated on slicing and evaluating elasticity of existing SaaS multi-tiered Java application used in Croatian market."
pub.1004713082,Enterprise Application Management in Cloud Computing Context,"Rapid growth of various types of cloud services and web APIs is creating new opportunities for innovative enterprise application. As a result, organizations are beginning to rely on external cloud providers to deliver a significant part of their IT infrastructure and software services. An important challenge, in particular in situations where a large number of cloud providers are involved relates to maintaining continuity of operation in the face of changes in external services. Most current research on this topic deals with this problem from service provider perspective by focusing on version management and related issues. Alternatively, the management of cloud services is delegated to a cloud service broker. There is a need to consider this problem from the perspective of service consumers and to develop effective methods that protect service consumer applications from changes in external services. In this paper, we draw on existing literature on service management and evolution, and cloud service brokerage and present a service-based framework designed to manage enterprise applications in cloud computing environments."
pub.1009207298,Mechanism and architecture for the migration of service implementation during traffic peaks,"Service-Oriented Architecture has been widely applied in enterprise computing systems for software-enabled services. However, cost efficiency and scalability requirements have moved the execution environment towards the cloud domain. Hybrid approaches have emerged, which utilise both enterprise and cloud domains in order to balance between the cost of service execution and the provided Quality of Service (QoS) for end users. This paper presents a migration, monitoring and load-balancing mechanism and architecture for scaling services between the enterprise and cloud domains during traffic peaks. The argued benefit of the proposal is the automation of the service-migration process and improvement of the QoS. A prototype system is presented as a proof of the conceptual architecture. The performance results in a hybrid cloud environment indicate that service implementation can be migrated and load can be balanced within 200 ms. Furthermore, the mechanism can improve the QoS for end users during traffic peaks. Our approach differs from existing proposals by focusing on the migration of service implementation, instead of the migration of service as part of a virtual machine."
pub.1120484025,JupyTEP IDE as an Online Tool for Earth Observation Data Processing,"The paper describes a new tool called JupyTEP integrated development environment (IDE), which is an online integrated development environment for earth observation data processing available in the cloud. This work is a result of the project entitled “JupyTEP IDE—Jupyter-based IDE as an interactive and collaborative environment for the development of notebook style EO algorithms on network of exploitation platforms infrastructure” carried out in cooperation with European Space Agency. The main goal of this project was to provide a universal earth observation data processing tool to the community. JupyTEP IDE is an extension of Jupyter software ecosystem with customization of existing components for the needs of earth observation scientists and other professional and non-professional users. The approach is based on configuration, customization, adaptation, and extension of Jupyter, Jupyter Hub, and Docker components on earth observation data cloud infrastructure in the most flexible way; integration with accessible libraries and earth observation data tools (sentinel application platform (SNAP), geospatial data abstraction library (GDAL), etc.); adaptation of existing web processing service (WPS)-oriented earth observation services. The user-oriented product is based on a web-related user interface in the form of extended and modified Jupyter user interface (frontend) with customized layout, earth observation data processing extension, and a set of predefined notebooks, widgets, and tools. The final IDE is addressed to the remote sensing experts and other users who intend to develop Jupyter notebooks with the reuse of embedded tools, common WPS interfaces, and existing notebooks. The paper describes the background of the system, its architecture, and possible use cases."
pub.1094432389,Migration to Multi-Image Cloud Templates,"IT management costs increasingly dominate the overall IT costs. The main hope for reducing them is to standardize software and processes, as this leads to economies of scale in the management services. A key vehicle by which enterprises hope to achieve this is cloud computing, and they start to show interest in clouds outside the initial sweet spot of development and test. As business applications typically contain multiple images with dependencies, one is starting to standardize on multi-image structures. Benefits are ease of deployment of the entire structure and consistent later management services for the business applications. Enterprises have huge investments in their existing business applications, e.g., their web design, special code, database schemas, and data. The promises of clouds can only be realized if a significant fraction of these existing applications can be migrated into the clouds. We therefore present analysis techniques for mapping existing IT environments to multi-image cloud templates. We propose multiple matching criteria, leading to tradeoffs between the number of matches and the migration overhead, and present efficient algorithms for these special graph matching problems. We also present results from analyzing an existing enterprise environment with about 1600 servers."
pub.1022560515,Securing elastic applications on mobile devices for cloud computing,"Cloud computing provides elastic computing infrastructure and resources which enable resource-on-demand and pay-as-you-go utility computing models. We believe that new applications can leverage these models to achieve new features that are not available for legacy applications. In our project we aim to build elastic applications which augment resource-constrained platforms, such as mobile phones, with elastic computing resources from clouds. An elastic application consists of one or more weblets, each of which can be launched on a device or cloud, and can be migrated between them according to dynamic changes of the computing environment or user preferences on the device. This paper overviews the general concept of this new application model, analyzes its unique security requirements, and presents our design considerations to build secure elastic applications. As first steps we propose a solution for authentication and secure session management between weblets running device side and those on the cloud. We then propose secure migration and how to authorize cloud weblets to access sensitive user data such as via external web services. We believe some principles in our solution can be applied in other cloud computing scenarios such as application integration between private and public clouds in an enterprise environment."
pub.1132954565,Prospective SD–WAN Shift: Newfangled Indispensable Industry Driver,"Software-Defined Wide Area Network (SD-WAN) has been developed to introduce Next Gen ready WAN infrastructure designed for easy cloud-centric application access in a secured and cost-effective manner. SD-WAN abridges maneuver and supervision of WAN by disengaging the network tool from its regulation modus operandi. The aim of SD-WAN is to replace existing inflexible WAN technologies with much advanced features to enhance ROI for the Enterprise customers. The scope of the study is to determine the drivers for the Enterprise customer, to move from Legacy WAN environment to SD-WAN for long term business benefit through a set of questionnaires to justify their investment for technology migration. It will also help IT managers to structure standard questions that they will use to select the best SD-WAN solution providers to meet their business requirements."
pub.1031303129,"Continued Rise of the Cloud, Advances and Trends in Cloud Computing","Cloud computing is no-longer a novel paradigm, but instead an increasingly robust and established technology, yet new developments continue to emerge in this area. Continued Rise of the Cloud: Advances and Trends in Cloud Computing captures the state of the art in cloud technologies, infrastructures, and service delivery and deployment models. The book provides guidance and case studies on the development of cloud-based services and infrastructures from an international selection of expert researchers and practitioners. A careful analysis is provided of relevant theoretical frameworks, practical approaches, and current methodologies. Topics and features: Presents a focus on security and access control mechanisms for cloud environments, analyses standards and brokerage services, and investigates the role of certification for cloud adoption Evaluates cloud ERP, suggests a framework for implementing “big data” science, and proposes an approach for cloud interoperability based on compliance and conformance Reviews existing elasticity management solutions, discusses the relationship between cloud management and governance, and describes the development of a cloud service capability assessment model Examines cloud applications in higher education, including the use of knowledge-as-a-service in the provision of education, and cloud-based e-learning for students withdisabilities This authoritative text/reference is essential reading for students, lecturers and researchers seeking insight into the cutting edge of cloud computing. Software developers, enterprise architects and IT infrastructure managers will also find this work to be both useful and enlightening."
pub.1181433294,Advocate -- Trustworthy Evidence in Cloud Systems,"The rapid evolution of cloud-native applications, characterized by dynamic,
interconnected services, presents significant challenges for maintaining
trustworthy and auditable systems, especially in sensitive contexts, such as
finance or healthcare. Traditional methods of verification and certification
are often inadequate due to the fast-past and dynamic development practices
common in cloud computing. This paper introduces Advocate, a novel agent-based
system designed to generate verifiable evidence of cloud-native application
operations. By integrating with existing infrastructure tools, such as
Kubernetes and distributed tracing systems, Advocate captures, authenticates,
and stores evidence trails in a tamper-resistant manner. This approach not only
supports the auditing process but also allows for privacy-preserving evidence
aggregation. Advocate's extensible architecture facilitates its deployment in
diverse environments, enabling the verification and adherence to policies and
enhance trust in cloud services."
pub.1171745845,Sensor-cloud Architecture: a Security Taxonomy in Cloud-assisted Sensor Networks,"The integration of cloud computing with wireless sensor networks (WSN), known as SensorCloud, has garnered significant attention for its application in fields such as healthcare, habitat monitoring, military surveillance, and disaster management. This fusion aims to overcome the inherent processing and storage limitations of sensor networks by leveraging the cloud's flexibility, scalability, and enhanced capacities. Despite these advantages, Sensor-Cloud systems face challenges including latency, dependability, load balancing, bandwidth constraints, resource optimization, and security vulnerabilities. Security concerns are paramount, as the architecture's integrity is threatened by potential attacks on sensor nodes, communication channels, and the cloud infrastructure. Although existing literature extensively explores these issues, a comprehensive analysis of security threats specific to Sensor-Cloud remains essential. This paper presents an in-depth examination of security challenges within Sensor-Cloud environments, proposing innovative solutions and developing taxonomies of security attacks from an architectural perspective. Through this analysis, the paper aims to fortify Sensor-Cloud architecture against diverse security threats, ensuring its robustness and reliability across various applications."
pub.1146607301,Joining Geo Data Across Different Providers to Ease Machine Learning Applications,"<p>Data integration and harmonization has been a tedious task ever since. The increase of available data in volume and variety has further increased the need for a thorough data integration. Furthermore, the application of more and more automatic algorithms stresses the need for a sensible geo data platform to avoid the ‘garbage in, garbage out’ trap and to allow for a meaningful data analysis. We reviewed different projects and learned about various needs and constraints of joint spatial research data infrastructures from local to cloud based deployments. Typically, these systems are not designed from scratch and existing systems need to be integrated or interfaced. As a result or arising from the need to support the sovereignty of distributed data centers, modern infrastructures need to be capable to support federated set-ups. Often these research data infrastructures shall not only be used to store raw data for scientists, but will also provide results (maps, derived data products, tools and applications) to the public. This goes along with the need for access delegation (e.g. OAuth). A special focus is put on the provision of the joint datasets for machine learning applications. In order to facilitate efficient learning and prediction a ML processing environment needs to be aligned with the data infrastructure. </p><p>We will present commonalities among these infrastructures and outline typical design patterns. A spatial data infrastructure based on open source software components that can be deployed on the cloud will be introduced. It features open standardized interfaces and services for easy adaptation and connectivity.</p>"
pub.1118214668,Checkpointing as a Service in Heterogeneous Cloud Environments,"A non-invasive, cloud-agnostic approach is demonstrated for extending
existing cloud platforms to include checkpoint-restart capability. Most cloud
platforms currently rely on each application to provide its own fault
tolerance. A uniform mechanism within the cloud itself serves two purposes: (a)
direct support for long-running jobs, which would otherwise require a custom
fault-tolerant mechanism for each application; and (b) the administrative
capability to manage an over-subscribed cloud by temporarily swapping out jobs
when higher priority jobs arrive. An advantage of this uniform approach is that
it also supports parallel and distributed computations, over both TCP and
InfiniBand, thus allowing traditional HPC applications to take advantage of an
existing cloud infrastructure. Additionally, an integrated health-monitoring
mechanism detects when long-running jobs either fail or incur exceptionally low
performance, perhaps due to resource starvation, and proactively suspends the
job. The cloud-agnostic feature is demonstrated by applying the implementation
to two very different cloud platforms: Snooze and OpenStack. The use of a
cloud-agnostic architecture also enables, for the first time, migration of
applications from one cloud platform to another."
pub.1164867274,Orchestrating Information Governance Workloads as Stateful Services Using Kubernetes Operator Framework,"Regulatory compliance is forcing organizations to implement an information governance (IG) strategy, but many are struggling to evolve their IG solutions due to their legacy architecture, as they are not designed to adapt to new business models and for the growing amount of unstructured data produced by a potentially worldwide audience. One of the biggest problems faced is continuously determining data value and adaptation of measures to keep risks and operational costs under control. One way to solve this issue is to leverage cloud technology and find an affordable approach to migrate legacy solutions to a cloud environment. In most cases, this means de-composing monolithic applications, refactoring components and replacing outdated homegrown deployment technologies with cloud-native, automated deployment and orchestration services. Our goal is to show how operational costs can be reduced by running refactored versions of IG solutions in clouds with a minimum of human intervention. This paper discusses the steps to evolve a legacy multi-tier IG solutions from physical to containerized environments by encapsulating human operator knowledge in cloud topology and orchestration artifacts, with the goal of enabling automated deployment and operation in Kubernetes (K8s) managed execution environments."
pub.1129424781,Architecture of a Cloud-based Fault-Tolerant Control Platform for improving the QoS of Social Multimedia Applications on SD-WAN,"Social media application are becoming multimedia centric with live and stored video, audio, augmented reality, haptic, etc. emerging as the main categories of traffic. Their QoS requirements are more stringent than their legacy counterparts. At the carrier level, Software Defined – Wide Area Network (SD-WAN) is one of the promising technologies for transporting these multimedia traffic. A SD-WAN will typically have a mesh of centralized controllers managing the networking infrastructure. Reliable operations of these controllers are a key requirement for the successful operation of the WAN. Controller failure will prevent the forwarding switches from communicating with the controller. This will prevent the switches from forwarding any new traffic, as well as flow entries from existing traffic will also time out after a period bringing the network to a standstill. Rebooting a controller or starting a new one will introduce delays degrading the QoS. This research presents an architecture for handling controller failure via transparent migration of the controller load in a semi-meshed controller environment. The architecture includes a real time cloud-based centralized storage of the flow states north of the controllers and a virtualized connection management unit at the south. The results demonstrate that the proposed model can transparently handle controller failure without affecting the QoS."
pub.1137263188,An Effective Mutual Authentication Scheme for Provisioning Reliable Cloud Computing Services,"Cloud computing facilitates an economic data storage, expertise software, and scalable computing resources through infrastructure, platform, and software (as a service). However, due to the non-transparent and openness of cloud computing, security is the pressing issue that may hamper the adoption and growth of cloud services. Moreover, conventional authentication management solutions are inadequate and cannot be directly adopted in cloud computing environments. Existing schemes have addressed the above issue, and a comprehensive study of the literature helped in formulating the current research. This research intends to develop an effective mutual authentication scheme for reliable cloud service provisioning. The performance is analysed in terms of computational overhead, communication cost, and resistance to various security characteristics."
pub.1182137558,Advocate - Trustworthy Evidence in Cloud Systems,"The rapid evolution of cloud-native applications, characterized by dynamic, interconnected services, presents significant challenges for maintaining trustworthy and auditable systems, especially in sensitive contexts, such as finance or health-care. Traditional methods of verification and certification are often inadequate due to the fast-past and dynamic development practices common in cloud computing. This paper introduces Advocate, a novel agent-based system designed to generate attested evidence of cloud-native application operations. By integrating with existing infrastructure tools, such as Kubernetes and observability services, Advocate captures, authenticates, and stores evidence trails in a tamper-resistant manner. This approach not only supports the auditing process but also allows for privacy-preserving evidence aggregation. Advocate’s extensible architecture facilitates its deployment in diverse environments, enabling the verification and adherence to policies and enhances trust in cloud services."
pub.1095787869,Cloudifier: An Ecosystem for the Migration of Distributed Applications to the Cloud,"The Cloudifier project deals with the design and development of an ecosystem of tools for the assisted migration to a cloud or multi-cloud environment of scientific and business-oriented distributed applications. Recent surveys show that many of the organizations that are not yet running their applications in the cloud are experimenting with infrastructure-as-a-service mode. The Cloudifier tools will provide capabilities to evaluate the characteristics of a legacy distributed application by profiling its behavior, to collect information about performance, cost and security on commercial cloud service providers, as well as to assess the quality of the interconnections between providers. On the basis of this data, and interacting with the customers that expose their requirements in an informal way, a “smart” brokering system will find an optimal service composition and evaluate its execution cost, by taking into account the cost plans of the providers, and possibly exploiting multi-cloud configurations."
pub.1095042282,CloudDVMM: Distributed Virtual Machine Monitor for Cloud Computing,"With the wide application of virtualization and cloud computing, it becomes an increasingly urgent problem of how to use a virtual machine monitor to integrate the increasing amount of distributed cloud computing nodes to improve their utility and reliability. Existing popular virtual machine monitors like Xen, VMware ESX Server, etc. are mostly acts as a fundamental infrastructure for the virtualization of one single cloud computing node. However, there are few researches on a virtual machine monitor for those distributed nodes under cloud computing environment. This paper introduces a novel distributed virtual machine monitor for cloud computing (CloudDVMM). We present its theoretical model, architecture and key technologies. Experiments and comparisons with existing researches show that CloudDVMM achieves better performance in distributed architecture, extensibility, etc. and is promising for meeting the integration requirements of distributed virtual computing and cloud computing environments."
pub.1149379562,Migration Patterns for Applications in Cloud Computing Environments,The trend of adopting container-based systems become increasingly relevant for companies and their IT departments. Improved scalability and shorter deployment cycles in IT production are the most mentioned benefits of the technology. The adoption of container-based technologies in an existing IT system landscape requires a consideration of migration strategies. The paper at hand examines general migration patterns for the transition of virtual systems into container-based systems from a cloud computing perspectives. Several strategies are derived for a specific use case.
pub.1169739421,Container Security in Cloud Environments,"  A bstract:   The widespread adoption of containers in modern software applications has introduced new challenges to security and integrity. Containers, known for their lightweight and portable nature, facilitate agile deployment across diverse environments. However, this popularity has led to security risks such as vulnerabilities in container images, misconfigurations, and insecure runtime environments. This paper addresses these challenges by proposing automated and robust security techniques integrated into continuous integration and continuous development pipelines. The work emphasizes the importance of a solid security policy, container image scanning, orchestration security, and runtime monitoring. The study also identifies specific issues faced by the DevSecOps community and proposes initial fixes to fortify container security.  In the cloud environment, containers play a pivotal role in application deployment by sharing the same OS kernel, reducing resource requirements, and minimizing start-up times. Despite their advantages, weak container isolation poses security challenges, including privilege escalation and information leaks. To mitigate these concerns, the paper conducts an in-depth analysis of existing access control mechanisms for container security. It discusses challenges in architecture modeling and presents use cases for fulfilling security requirements, encompassing container, inter-container, and host protection. The work emphasizes the need for both software and hardware solutions to enhance container security. Containers have emerged as a lightweight alternative to virtual machines, supporting microservices architecture. The container market is growing rapidly, but security concerns remain a significant barrier to adoption. This paper surveys existing literature on container security, categorizing it into four use cases: protecting containers from internal applications, inter-container protection, safeguarding the host from containers, and defending containers froma malicious or semi-honest host. The analysis reveals that software-based solutions, leveraging Linux kernel features and security modules, address the first three use cases, while the last use case relies on hardware-based solutions. The paper concludes with highlighting open research problems and future directions to guide further exploration in container security."
pub.1026279271,cloud-ATAM: Method for Analysing Resilient Attributes of Cloud-Based Architectures,"In this work, we argue that the existing architecture evaluation methods have limitations when assessing architectures interfacing with unpredictable environments such as the Cloud. The unpredictability of this environment is attributed to the dynamic elasticity, scale, and continuous evolution of the cloud topology. As a result, architectures interfacing such unpredictable environments are expected to encounter many uncertainties. It is however, important to focus on, and present holistic approaches combining aspects of both dynamic and static analysis of architecture resilience attributes. This paper introduces an ATAM derived methodology - cloud-ATAM - for evaluating the trade-off between multiple resilience quality attributes (i.e. availability and performance) of a cloud-based Reactive Architecture for Global Software Development."
pub.1044730840,XaaS Multi-Cloud Marketplace Architecture Enacting the Industry 4.0 Concepts,"Cloud computing in conjunction with recent advances in Cyber-Physical Systems (CPSs) unravels new opportunities for the European manufacturing industry for high value-added products that can quickly reach the market for sale. The Internet of Things joins the Internet of Services to enact the fourth industrial revolution that digitalises the manufacturing techniques and logistics while pushing forward the development of improved factories with machine-to-machine communication delivering massively customised products tailored to the individualised needs of the customer. Interconnected CPSs using internal and cross-organizational services cooperate in real time increase the business agility and flexibility of manufacturing companies. Using CPS with cloud computing architectures leverage data and services stored and run in cloud environment from different vendors that usually offer different service interfaces to share their services and data. However, when using such architectures data silos appear and different vendors having different service interfaces can easily result in vendor lock-in issues. This paper proposes a multi-cloud marketplace architecture leveraging the existing myriad of different cloud environments at different abstraction levels including the Infrastructure-, Platform-, and Software-as-a-Service cloud models—that is, Everything as a Service or XaaS—delivering services and with different properties that can control the computation happening in multiple cloud environments sharing resources with each other."
pub.1039413216,Towards Teacher-Managed Deployment and Integration of Non-SaaS Tools in Virtual Learning Environments,"Virtual Learning Environments (VLEs) such as Moodle or Sakai are now commonplace in many educational settings, but the limited set of built-in tools (e.g. chats or forums) is often mentioned as a drawback. While successful solutions to integrate Software as a Service (SaaS) tools are reported in the literature (e.g. IMS Basic LTI and GLUE!), there are other non-SaaS tools with potential for education that cannot be integrated in VLEs. These include single-user standalone tools that are run on local computers without a web-based interface (e.g. Matlab), but also virtual machines and laboratories that are common in Computer Science-related disciplines. This paper addresses the integration of these non-SaaS tools in VLEs, proposing an architecture to help teachers to select and configure non-SaaS tools using existing integration approaches from VLEs, but that also relies on cloud infrastructures for provisioning and deploying these tools transparently to the end users."
pub.1145703854,Monolithic vs. Microservice Architecture: A Performance and Scalability Evaluation,"Context. Since its proclamation in 2012, microservices-based architecture has gained widespread popularity due to its advantages, such as improved availability, fault tolerance, and horizontal scalability, as well as greater software development agility. Motivation. Yet, refactoring a monolith to microservices by smaller businesses and expecting that the migration will bring benefits similar to those reported by top global companies, such as Netflix, Amazon, eBay, and Uber, might be an illusion. Indeed, for systems that do not have thousands of concurrent users and can be scaled vertically, the benefits of such migration have not been sufficiently investigated, while the existing evidence is inconsistent. Objective. The purpose of this paper is to compare the performance and scalability of monolithic and microservice architectures on a reference web application. Method. The application was implemented in four different versions, covering not only two different architectural styles (monolith vs. microservices) but also two different implementation technologies (Java vs. C#.NET). Next, we conducted a series of controlled experiments in three different deployment environments (local, Azure Spring Cloud, and Azure App Service). Findings. The key lessons learned are as follows: (1) on a single machine, a monolith performs better than its microservice-based counterpart; (2) The Java platform makes better use of powerful machines in case of computation-intensive services when compared to.NET; the technology platform effect is reversed when non-computationally intensive services are run on machines with low computational capacity; (3) vertical scaling is more cost-effective than horizontal scaling in the Azure cloud; (4) scaling out beyond a certain number of instances degrades the application performance; (5) implementation technology (either Java or C#.NET) does not have a noticeable impact on the scalability performance."
pub.1127307084,THE USE OF THE CLOUD-BASED OPEN LEARNING AND RESEARCH PLATFORM FOR COLLABORATION IN VIRTUAL TEAMS,"The article highlights the promising ways of providing access to cloud-based platforms and tools to support collaborative learning and research processes. It is emphasized that the implementation of cloud computing services is a promising trend in the development of modern ICT pedagogical systems. The concept of cloud-based open learning and research platform is considered. The benchmarking studies of using ICT tools for learning and research within the pedagogical systems of higher education are fulfilled. The analysis and evaluation of the existing experience of using different types of software packages to support learning and research processes within the cloud-based environment are proposed. The issues of integration of knowledge-based services, language technologies and database network services within the open systems of learning and research are covered. A model of the cloud-based open learning and research collaborative environment involving the use of cloud-based components based on AWS virtual desktop, IBM Box, WPadV4 and other tools is substantiated. The reasonable ways of tools selection are considered and the prospects for their use within the cloud-based open learning and research platform in pedagogical systems of higher education are described. The research methods are the analysis of official international documents, publications on the subject of the research, observation, comparison, the analysis of the experience of using educational and scientific network packages, application of cloud technologies, benchmarking studies and experimental studies. Conclusions and recommendations encompass the application of cloud-based open learning and research technologies covering research platforms, scientific and educational network packages, and also cloud services for collecting, submitting and processing data as a promising trend in the development and modernization of the educational environment of higher education institutions and collaborative research."
pub.1106016914,Script Based Migration Toolkit for Cloud Computing Architecture in Building Scalable Investment Platforms,"The 2008 Financial Crisis which created a global financial market meltdown is mainly due to badly structured mortgage loans with poor or subpar credit quality and lack of proper tools to measure portfolio risks by the lenders. Even though several problems led to this crisis, we looked at this from a Big Data. Had the infrastructure and analytical analysis tools were present to the lenders, they would have found the various early warning signs on these mortgage loans and could have better prepared for the crisis. Aftermath of the crisis, all the big financial institutions took a fresh look and embarked onto build various tools and frameworks to address this Big Data in their portfolios with data driven analysis. The 3Vs (Velocity, Volume and Variety) of the Big Data in our Mortgage Loan Analysis System challenges our traditional approach in collecting, processing and presenting the individual and aggregated loan level data in a meaningful format to facilitate our portfolio managers in decision making. The traditional methods are implemented on a standalone on-premises SQL server. Our Framework creates the foundation of migrating from traditional standalone database architecture (on-premises) to Cloud Computing environment using “Script Based Implementation”. The methods we present are simple but effective and saves resources in terms of Hardware, Software and on-going maintenance costs. Big Data “Capture, Transform, Calculate and Visualize” (CTCV) implementation takes a phased approach rather than a big bang model. Our implementation helps the Big Data Management to be part of organizational tool kit. This saves hard dollars and brings us in line with the overall firm strategic vision of moving to Cloud Computing for Investment Management Services."
pub.1154119213,Infrastructure as Code (IaC): Insights on Various Platforms,"In the present-day tech-stack, cloud computing is evolving as a successful and one of the popular fields of technology where the new businesses are achieving success by deploying their functionalities, products, data, and services on cloud instead of on-premises system and that also without depending on any physical component. Infrastructure as code (IaC) is a set of methodologies which uses code to set up the install packages, virtual machines and networks, and configure environments. A successful IaC implementation and adoption by developers requires a broad set of skills and knowledge. It is DevelopmentOperations’ tactic of provisioning an application’s infrastructure and managing it through binary readable configuration files, instead of any hardware configuration."
pub.1164619418,Integration of cloud computing in BCI: A review,"Brain computer interface (BCI) applications are emerging from the laboratory to the field environment with ever-increasing demands for high accuracy. However, enhancements in accuracy by employing multisensory devices or via hybridization techniques are inflicting issues like big data and soaring computation complexity. Such enhancements will further aggravate the miseries in the field environments. With more computational complexity and storage requirements, even the on-premise laboratory setups can face hardware and software renewal costs. In addition, it may also come across over-provisioning or under-provisioning of resources. Under such scenarios, computation offloading to cloud machines is gaining considerable attention from academia and industry. The key premise of cloud computing is that it involves the scope of on-demand resource availability and parallel processing. In addition, cloud resources can be accessed ubiquitously. With the advancements in internet technologies, portable BCI headsets can easily be integrated with cloud computing. However, inadvertent usage of cloud resources is neither beneficial for service users nor for service providers, as cloud services are paid. Hence, this paper chronologically investigates contemporary research solutions, considering where and how the cloud has been integrated with the BCI environments. In addition, it also puts forward the related challenges and the potential issues that demand future attention for the seamless integration of the cloud and BCI environment."
pub.1128045863,A Framework for Emulating Database Operations in Cloud Data Warehouses,"In recent years, increased interest in cloud-based data warehousing technologies has emerged with many enterprises moving away from on-premise data warehousing solutions. The incentives for adopting cloud data warehousing technologies are many: cost-cutting, on-demand pricing, offloading data centers, unlimited hardware resources, built-in disaster recovery, to name a few. There is inherent difference in the language surface and feature sets of on-premise and cloud data warehousing solutions. This could range from subtle syntactic and semantic differences, with potentially big impact on result correctness, to complete features that exist in one system but are missing in other systems. While there have been some efforts to help automate the migration of on-premise applications to new cloud environments, a major challenge that slows down the migration pace is the handling of features not yet supported, or partially supported, by the cloud technologies. In this paper we build on our earlier work in adaptive data virtualization and present novel techniques that allow running applications utilizing sophisticated database features within foreign query engines lacking the native support of such features. In particular, we introduce a framework to manage discrepancy of metadata across heterogeneous query engines, and various mechanisms to emulate database applications code in cloud environments without any need to rewrite or change the application code."
pub.1124364335,Key Opportunities and Challenges of Data Migration in Cloud: Results from a Multivocal Literature Review," Cloud data migration is the procedure of moving information, localhost applications, services, and data to the distributed cloud computing infrastructure. The success of this data migration process is depending on several aspects like planning and impact analysis of existing enterprise systems. One of the most common operations is moving locally stored data in a public cloud computing environment. This paper, through a multivocal literature review, identifies the key advantages and consequences of migrating data into the cloud. There are five different cloud migration strategies and models prescribed to evaluate the performance, identifying security requirements, choosing a cloud provider, calculating the cost, and making any necessary organizational changes. The results of this research paper can give a road map for the data migration journey and can help decision makers towards a safe and productive migration to a cloud computing environment."
pub.1167342705,CAWAL: A novel unified analytics framework for enterprise web applications and multi-server environments,"In web analytics, cloud-based solutions have limitations in data ownership and privacy, whereas client-side user tracking tools face challenges such as data accuracy and a lack of server-side metrics. This paper presents the Combined Analytics and Web Application Log (CAWAL) framework as an alternative model and an on-premises framework, offering web analytics with application logging integration. CAWAL enables precise data collection and cross-domain tracking in web farms while complying with data ownership and privacy regulations. The framework also improves software diagnostics and troubleshooting by incorporating application-specific data into analytical processes. Integrated into an enterprise-grade web application, CAWAL has demonstrated superior performance, achieving approximately 24% and 85% lower response times compared to Open Web Analytics (OWA) and Matomo, respectively. The empirical evaluation demonstrates that the framework eliminates certain limitations in existing tools and provides a robust data infrastructure for enhanced web analytics."
pub.1144764144,The Challenge of Achieving Zero Trust Remote Access in Multi-Cloud Environment,"Zero-trust security models and architectures have recently increased in adoption due to several variables, such as the widespread use of off-premises cloud technologies, variety in IT devices, and diffusion in the Internet of Things (IoT). Users, devices, apps, and networks are all assumed to be untrustworthy in this approach, which is built on the idea of various tiers of Trust and authentication. Cybersecurity paradigms are developing, and the term ""zero trust"" describes the shift from static network perimeters to protecting people, things, and resources. Economic and enterprise architecture and processes can be designed using zero trust principles. In the idea of zero Trust, assets or user accounts are thought to have no implicit confidence because of their physical or network location (Internet vs local networks) or asset ownership (enterprise or personally owned). Authentication and authorization must be conducted before a connection to an organizational resource can be established. There are many different types of Cloud, including several public, private, hybrid, and on-premises. For data centers, a multi-cloud deployment strategy includes many different public cloud service providers instead of relying on a private cloud or on-premises architecture. Hybrid multi-cloud is a multi-cloud implementation that incorporates all public and private clouds and on-premises technology. This paper discusses the zero-trust security model for multi-cloud environments and applications and the obstacles to implementing it."
pub.1094620701,Research and application of migrating legacy systems to the private cloud platform with cloudstack,"Legacy systems is a difficult problem to deal with, the outdated technology makes it difficult to do directly modify to codes, aging hardware environment make the system slow to respond, but they become more and more indelible because of the deep bundle dependencies. Although this is a time-consuming and difficult task, but the renewing of legacy systems have to be seriously faced because of its importance to enterprise. The concept of cloud computing has been proposed to build a private cloud-based information systems management model into an efficient, cost-effective solutions, virtualization technology has been used a lot, and it provides a new direction for the migration of legacy systems. In this paper, wo decomposition analysis of the legacy systems, combine legacy system migration solutions and virtualization technology with the application of cloudstack which is an open source cloud platform owned by Citrix Systems, using SOA and web services technology on the part of the system functions to do the reconstruction, we build an enterprise private cloud platform."
pub.1128556623,RETRACTED ARTICLE: Efficient optimal resource allocation for profit maximization in software defined network approach to improve quality of service in cloud environments,"Software-defined networking (SDR) technology is an approach to network management that enables dynamic, programmatically efficient network configuration in order to improve network performance and monitoring making it more like cloud computing than traditional network management. Cloud Resource scheduling is used to schedule the workload-based customer request. Here, cost effective resources allocation is introduced based on arriving request and cluster allocation. The profit maximizing scheme aims is to provide probabilistic guarantee against the resource overloading and migration. In this work, proposed software defined approach namely Modified Heuristic Search (MHS) Algorithm is proposed to achieve the cost-effective resources allocation in distributing computing environment to improve the Quality of Service in Cloud environment and its applications. To achieve the profit maximization, Cost Effective Reliable Resource allocation (CERRA) algorithm is utilized to measure the effective cluster selection in MHSA which includes a fitness function for selecting the arriving cloud requests to earn profit. Speed, transfer rate and energy are measured and compared with the existing method to analysis the resource allocation system."
pub.1094933086,eXCloud: Transparent Runtime Support for Scaling Mobile Applications in Cloud,"Cloud computing augments applications with ease-of-access to the enormous resources on the Internet. Combined with mobile computing technologies, mobile applications can exploit the Cloud everywhere by statically distributing code segments or dynamically migrating running processes onto cloud services. Existing migration techniques are however too coarse-grained for mobile devices, so the overheads often offset the benefits of migration. To build a truly elastic mobile cloud computing infrastructure, we introduce eXCloud (eXtensible Cloud)-a middleware system with multi-level mobility support, ranging from as coarse as a VM instance to as fine as a runtime stack frame, and allows resources to be integrated and used dynamically. In eXCloud, a stack-on-demand (SOD) approach is used to support computation mobility throughout the mobile cloud environment. The approach is fully adaptive, goal-driven and transparent. By downward task migration, applications running on the cloud nodes can exploit or take control of special resources in mobile devices such as GPS and cameras. With a restorable MPI layer, task migrations of MPI parallel programs can happen between cloud nodes or be initiated from a mobile device. Our evaluation shows that SOD outperforms several existing migration mechanisms in terms of migration overhead and latency. All our techniques result in better resource utilization through task migrations among cloud nodes and mobile nodes."
pub.1110539716,"Cloud-Empowered, Self-Managing Wireless Sensor Networks",This article discusses the design and implementation of a scalable system architecture that integrates wireless sensor networks (WSNs) into the Internet of Things (IoT) and exploits cloud services to autonomously configure wireless sensor nodes to measure and transmit sensed data only at periods when the environment changes more often. The implementation relies on software-defined networking (SDN) features to simplify WSN management and exploits the power of existing cloud computing platforms to execute a reinforcement learning algorithm that makes decisions based on the environments evolution.
pub.1014649915,A Study on Scalability of Services and Privacy Issues in Cloud Computing,"Cloud Computing is rapidly emerging and the new development in Information Technology. There are many patterns, or categories, in the world of cloud computing that are needed for the enterprise architecture. Some of the categories of services are storage, database, information, process, application, platform, integration, security, privacy, management/governance, testing, and infrastructure. Scalability is one of the important features applied on any of the services. The existing analysis specially focuses on Architectural and policy Implications without exploring the data privacy issues. In this paper, the application scalability and data privacy initiatives on various services in cloud environments are presented, with an overview of the trends they follow."
pub.1146485251,Moving Target Defense‐Based Denial‐of‐Service Mitigation in Cloud Environments: A Survey,"With the increased frequency and intensity of denial-of-service (DoS) attacks on critical cloud-hosted services, resource adaptation schemes adopted by the cloud service providers (CSPs) need to be intelligent. Specifically, they need to be adaptable to attack behavior and be dynamic to curb resource over-utilization. The concept of moving target defense (MTD) has recently emerged as an effective and agile defense mechanism against DoS attacks that particularly target cloud-hosted applications. However, the existing surveys that seek to explore this space either focus more on MTD for generic cyberattack mitigation or on DoS attack defense on cloud systems. In this survey, we particularly provide an in-depth analysis on how MTD can help recover critical cloud assets in the face of DoS attacks and how emerging programmable technologies such as software-defined networking (SDN) can be leveraged to achieve that goal. Unlike existing surveys, we categorize DoS attacks on cloud platforms based on their working mechanism. We also discuss the non-MTD-based DoS defense strategies for both cloud and non-cloud infrastructures in order to highlight the pros and cons of MTD-based strategies. We introduce MTD working mechanisms and present how existing research is envisioning MTD’s application in mitigating DoS attacks, both with and without SDN. We also take an in-depth look at the testbed implementations and resilience and performance evaluations of MTD approaches. Finally, we articulate the existing challenges in MTD for DoS mitigation in cloud systems and how these challenges are shaping the future research in this domain."
pub.1085723122,Analyzing and Migrating an Implemented System,"While modern systems are often built in a way that respects cloud computing requirements, the majority of existing systems have been built before dynamically allocatable resources became mainstream. Those systems have been designed and implemented for dedicated hardware, often large-scale servers, which had been sized to the maximum workload the systems were expected to face. This approach led to a high amount of wasted resources, which were operated in a stand-by fashion only to deal with seldom high-load situations. Therefore, it is desirable to migrate legacy systems in a way that they can benefit from the cloud computing approach. However, several issues arise. Legacy systems were often built without upfront modeling or the models became outdated over time. Additionally, while there are several tools that analyze legacy systems to detect insufficient coding style or bad designs, almost no tooling exists that spots defects in the systems. This deficiency hinders systems to smoothly operate in cloud computing environments, i.e., these systems have a limited scalability. In CloudScale, we address this issue by dedicated, built-in method support for system evolution, i.e., for migrating legacy systems to cloud computing environments. In this chapter, we outline CloudScale’s evolution support and present tools which help software architects to migrate legacy systems to scalable, cloud computing applications.Section 7.2 describes the process steps when spotting HowNotTos. Statical detection of HowNotTos is further elaborated in Sect. 7.3, while dynamic detection is discussed in Sect. 7.4. Section 7.5 links HowNotTos with best-practice HowTos. An example is described in Sect. 7.6."
pub.1163248075,A novel architecture to virtualise a hardware-bound trusted platform module,"Security and trust are particularly relevant in modern softwarised infrastructures, such as cloud environments, as applications are deployed on platforms owned by third parties, are publicly accessible on the Internet and can share the hardware with other tenants. Traditionally, operating systems and applications have leveraged hardware tamper-proof chips, such as the Trusted Platform Modules (TPMs) to implement security workflows, such as remote attestation, and to protect sensitive data against software attacks. This approach does not easily translate to the cloud environment, wherein the isolation provided by the hypervisor makes it impractical to leverage the hardware root of trust in the virtual domains. Moreover, the scalability needs of the cloud often collide with the scarce hardware resources and inherent limitations of TPMs. For this reason, existing implementations of virtual TPMs (vTPMs) are based on TPM emulators. Although more flexible and scalable, this approach is less secure. In fact, each vTPM is vulnerable to software attacks both at the virtualised and hypervisor levels. In this work, we propose a novel design for vTPMs that provides a binding to an underlying physical TPM; the new design, akin to a virtualisation extension for TPMs, extends the latest TPM 2.0 specification. We minimise the number of required additions to the TPM data structures and commands so that they do not require a new, non-backwards compatible version of the specification. Moreover, we support migration of vTPMs among TPM-equipped hosts, as this is considered a key feature in a highly virtualised environment. Finally, we propose a flexible approach to vTPM object creation that protects vTPM secrets either in hardware or software, depending on the required level of assurance."
pub.1039011695,Research on Energy-Saving Algorithms with Migration Energy Consumption in Heterogeneous Cloud Environment,"Cloud computing is a new emerging paradigm which delivers an infrastructure, platform and software as services in a pay-as-you-go model. However, with the development of cloud computing, the large-scale data centers consume huge amounts of electrical energy resulting in high operational costs and environment problem. Nevertheless, existing energy-saving algorithms based on live migration don’t consider the migration energy consumption, and most of which are designed for homogeneous cloud environment. In this paper, we take the first step to model energy consumption in heterogeneous cloud environment with migration energy consumption. Based on this energy model, we design energy-saving Best fit decreasing (ESBFD) algorithm and energy-saving first fit decreasing (ESFFD) algorithm. We further provide results of several experiments using traces from PlanetLab in CloudSim. The experiments show that the proposed algorithms can effectively reduce the energy consumption of data center in the heterogeneous cloud environment compared to existing algorithms like NEA, DVFS, ST (Single Threshold) and DT (Double Threshold)."
pub.1061786888,Secure Cloud Connectivity for Scientific Applications,"Cloud computing improves utilization and flexibility in allocating computing resources while reducing the infrastructural costs. However, in many cases cloud technology is still proprietary and tainted by security issues rooted in the multi-user and hybrid cloud environment. A lack of secure connectivity in a hybrid cloud environment hinders the adaptation of clouds by scientific communities that require scaling-out of the local infrastructure using publicly available resources for large-scale experiments. In this article, we present a case study of the DII-HEP secure cloud infrastructure and propose an approach to securely scale-out a private cloud deployment to public clouds in order to support hybrid cloud scenarios. A challenge in such scenarios is that cloud vendors may offer varying and possibly incompatible ways to isolate and interconnect virtual machines located in different cloud networks. Our approach is tenant driven in the sense that the tenant provides its connectivity mechanism. We provide a qualitative and quantitative analysis of a number of alternatives to solve this problem. We have chosen one of the standardized alternatives, Host Identity Protocol, for further experimentation in a production system because it supports legacy applications in a topologically-independent and secure way."
pub.1152221183,Software deployment in manufacturing environments: A requirements analysis,"Digital transformation and the Industrial Internet of Things (IIoT) are driving the use of cloud, fog, and edge computing in manufacturing environments. As a result, numerous computing resources for software execution are available in manufacturing environments. The increased use of software and distributed systems is constantly raising the complexity of the software deployment process. Often, and specifically in manufacturing, software is subject to technological or usage requirements limiting possible resources related to the execution of the software. Since deployment decisions are often made manually without a profound basis, this leads to wrong decisions and to resources not being put to their best use. Also, the performance quality of software execution suffers with security gaps occurring and additional costs arising. Existing software deployment concepts are very generic and focus on cloud. They do not address the specific needs of operational technology and manufacturing environments. This work identifies important evaluation criteria for software deployment in manufacturing environments, based on a requirements analysis. The requirement analysis systematically analyzes cloud, fog, and edge environments, as well as software applications and system resources in manufacturing environments. The aim is to identify deployment-relevant resource properties and software requirements. The comparison, evaluation, elimination, and specification of the collected information results in a catalog of deployment-relevant criteria for selecting technically suitable options for a deployment decision. These criteria support decisions during the software deployment process. Also, these criteria provide the basis for automated and dynamic software deployment decisions in manufacturing environments."
pub.1173319646,Transforming business scalability and operational flexibility with advanced cloud computing technologies,"In the rapidly evolving landscape of digital transformation, cloud computing has emerged as a pivotal technology enabling businesses to achieve unparalleled scalability and operational flexibility. This review explores the transformative impact of advanced cloud computing technologies on business operations, highlighting key innovations and their implications for organizational growth and efficiency. Cloud computing offers a dynamic and scalable environment where resources can be provisioned and managed on-demand, allowing businesses to respond swiftly to changing market conditions and customer demands. By leveraging cloud infrastructure, companies can scale their operations seamlessly without the need for significant upfront investments in physical hardware. This flexibility not only reduces capital expenditure but also enhances the ability to innovate and adapt in a competitive marketplace. Advanced cloud technologies, such as multi-cloud and hybrid cloud solutions, further augment operational flexibility by enabling organizations to optimize their IT environments. Multi-cloud strategies allow businesses to distribute workloads across multiple cloud providers, mitigating the risks associated with vendor lock-in and ensuring high availability and redundancy. Hybrid cloud solutions, which integrate on-premises infrastructure with public and private clouds, provide a balanced approach to managing sensitive data and workloads while benefiting from the scalability of the cloud. Moreover, cloud-native technologies like containerization and serverless computing have revolutionized application development and deployment. Containers encapsulate applications and their dependencies, ensuring consistency across different computing environments and facilitating rapid deployment. Serverless computing reviews the underlying infrastructure, allowing developers to focus solely on code, thus accelerating the development cycle and reducing operational overhead. The integration of advanced analytics and artificial intelligence (AI) with cloud computing further enhances business capabilities. Cloud platforms offer robust analytics tools and AI services that can process vast amounts of data in real-time, providing actionable insights and enabling predictive decision-making. This integration empowers businesses to optimize operations, improve customer experiences, and drive strategic initiatives with data-driven precision. In conclusion, advanced cloud computing technologies are instrumental in transforming business scalability and operational flexibility. By harnessing the power of the cloud, organizations can achieve greater agility, cost efficiency, and innovation, positioning themselves for sustained growth and competitive advantage in the digital age. As cloud technologies continue to evolve, their potential to redefine business operations and drive economic value will only expand, making cloud adoption a critical imperative for modern enterprises.
 Keywords: Op"
pub.1127060277,FLAHP Methodology to Adopt towards Cloud Computing,"In Cloud Computing Environment (CCE), to connect the relationship with multi-dimensional, novel methodologies are expected to improve the conventional development approach. In general, numerous frameworks become legacy system because today’s programs use latest technologies like object-oriented model (OOM) and user requirements change from time to time. To help the legacy system appraisal to satisfy those alterations dependent on CCE highlights, an evaluation technique dependent on the Fuzzy Logic Based Analytic Hierarchy Process (FLAHP) is designed in this study. Fuzzy is a reasonably necessary positioning technique for origination and application when contrasted and for the other multiple criteria decision-making strategies. FLAHP procedure is proposed to conclude the legacy system appraisal from the related viewpoints. FLAHP gives a reasonable and exhaustive system for organizing a choice to communicate and measure components for making the OOM correlations and utilizing complex information about the components depending on the selected criteria. The object-oriented legacy ERP assignment (OO-LERA) framework is the initiated model for the restructuring process. Reengineering legacy system to Modern OO-LERA functionalities incorporates the source code reflection from legacy system. Presented here Legacy Migration Platform Verifier (LMPV) for legacy system transformation. As the name suggests, LMPV checks the OO-LERA indeed for fulfillment of its client’s necessities. The FLAHP is applied to a Banking System Management in result and discussion section."
pub.1093597333,Cloud Adaptation & Application (Re-)Distrihution: Bridging the two Perspectives,"Cloud developers have to make several decisions when running their application in a cloud environment that may lead to conflicting objectives, inefficient deployment, and inappropriate or not existing adaptation strategies. Proper decision-support tools and processes are therefore needed to make cloud developers aware of the issues that need to be considered when deploying and running applications in the Cloud. Current decision support tools for cloud developers do not provide a structured and organized process in which the cloud developers can systematically check their choices when planning the deployment, execution, and adaptation of applications in the Cloud. In this paper, we combine two previous works and introduce an approach for identifying the options for (re-)deploying application in cloud providers infrastructures and the possible strategies of adaptation that can be used by the deployed application at runtime. The key contribution is a support process that synthesizes the two approaches. We also describe a case study where our support process is applied and we indicate the alternatives for application (re-) deployment and adaptation."
pub.1136484688,A Kubernetes CI/CD Pipeline with Asylo as a Trusted Execution Environment Abstraction Framework,"Modern commercial software development organizations frequently prescribe to a development and deployment pattern for releases known as continuous integration / continuous deployment (CI/CD). Kubernetes, a cluster-based distributed application platform, is often used to implement this pattern. While the abstract concept is fairly well understood, CI/CD implementations vary widely. Resources are scattered across on-premise and cloud-based services, and systems may not be fully automated. Additionally, while a development pipeline may aim to ensure the security of the finished artifact, said artifact may not be protected from outside observers or cloud providers during execution. This paper describes a complete CI/CD pipeline running on Kubernetes that addresses four gaps in existing implementations. First, the pipeline supports strong separation-of-duties, partitioning development, security, and operations (i.e., DevSecOps) roles. Second, automation reduces the need for a human interface. Third, resources are scoped to a Kubernetes cluster for portability across environments (e.g., public cloud providers). Fourth, deployment artifacts are secured with Asylo, a development framework for trusted execution environments (TEEs)."
pub.1133313065,A cloud software life cycle process (CSLCP) model,"Small to medium-sized enterprises take advantage of the strengths and opportunities of cloud computing. These enterprises require a well-defined software process model to produce reliable and quality cloud software, given their limited resources. Existing related work is surveyed, and the needed missing features are determined. A cloud software life cycle process model is proposed, validated, and verified to handle the shortcomings of existing cloud software process models. A case study is used to illustrate all the activities required throughout the software life cycle of the proposed model. The proposed cloud software life cycle process model is a cyclic iterative prototyping model. It is compatible with levels two and three of the capability maturity model integration and extends the Egyptian software process improvement model to fit the cloud environment. The model helps small enterprises develop quality, maintainable, and sustainable cloud software at a reasonable cost."
pub.1127979841,Co-located and Orchestrated Network Fabric (CONF): An Automated Cloud Virtual Infrastructure for Social Network Applications,"Cloud environments can provide virtualized, elastic, controllable and high-quality on-demand infrastructure services for supporting complex distributed applications. However, existing IaaS (Infrastructure-as-a-Service) solutions mainly focus on the automated integration or deployment of generic applications; they lack flexible infrastructure planning and provisioning solutions and do not have rich support for the high service quality and trustworthiness required by social network applications. This paper introduces an automated cloud virtual infrastructure solution for social network applications, called Co-located and Orchestrated Network Fabric (CONF), which was conducted in a recently funded EU H2020 project ARTICONF. CONF aims to improve the existing infrastructure support in the DevOps lifecycle of social network applications to optimize QoS performance metrics as well as ensure fast recovery in the presence of faults or performance drops."
pub.1094138696,Checkpointing as a Service in Heterogeneous Cloud Environments,"A non-invasive, cloud-agnostic approach is demonstrated for extending existing cloud platforms to include checkpoint-restart capability. Most cloud platforms currently rely on each application to provide its own fault tolerance. A uniform mechanism within the cloud itself serves two purposes: (a) direct support for long-running jobs, which would otherwise require a custom fault-tolerant mechanism for each application; and (b) the administrative capability to manage an over-subscribed cloud by temporarily swapping out jobs when higher priority jobs arrive. An advantage of this uniform approach is that it also supports parallel and distributed computations, over both TCP and InfiniBand, thus allowing traditional HPC applications to take advantage of an existing cloud infrastructure. Additionally, an integrated health-monitoring mechanism detects when long-running jobs either fail or incur exceptionally low performance, perhaps due to resource starvation, and proactively suspends the job. The cloud-agnostic feature is demonstrated by applying the implementation to two very different cloud platforms: Snooze and OpenStack. The use of a cloud-agnostic architecture also enables, for the first time, migration of applications from one cloud platform to another."
pub.1044624970,Total Cost of Ownership for Application Replatform by Open-source SW,"In intra-company IT environment, the use of open-source software (OSS) should be expanded to reduce IT costs and to establish SW governance. This requires the migration of systems from the existing commercial SW to open-source SW, but the attempt of application replatform is prevented by the expenses for application reprogramming and data migration. This study proposes a methodology for TCO calculation of application replatform using open-source SW. In practice, a five-year TCO shows a cost reduction effect of 78% - 83%. This TCO could be further reduced if the application size is increased due to data accumulation and the company gets open-source SW capabilities internally. In addition, it is possible to directly apply an application developed from open-source SW to a virtualized infrastructure environment, which enables to operate in a hybrid cloud environment. This enables a scalable, efficient and flexible IT operation and a sustainable TCO reduction in the future"
pub.1139506237,Cipher Block Chaining Support Vector Machine for Secured Decentralized Cloud Enabled Intelligent IoT Architecture,"The growth of internet era leads to a major transformation in a storage of data and accessing the applications. One such new trend that promises the endurance is the Cloud computing. Computing resources offered by the Cloud includes the servers, networks, storage, and applications, all as services. With the advent of Cloud, a single application is delivered as a metered service to numerous users, via an Application Programming Interface (API) accessible over the network. The services offered via the Cloud are such as the infrastructure, software, platform, database and web services. The main motivation of this application model is to provide computationally secure key generation to protect the data via encryption. This key generation in the cryptography process falls into three categories in this research work. In the first part, SVM based encryption service model is constructed for which the key generation is from the conventional encryption operation mode with some improvements. To make the process more complex, the optimization techniques are taken into account for the key generation in descendant two methods application model that acts computationally more secure specifically for Cloud environment. The results of security analysis confirm the effectiveness of the proposed application model withstands potentially against various attacks such as Chosen Cipher Attack, Chosen Plain text Attack indistinguishable attacks for files. In case of images, it resists well against statistical and differential attacks. Comparative Analysis shows evidence of the efficiency of the developed pioneering application model quality and strength compared with that of the existing services."
pub.1164096488,INDICES: Applying DDDAS Principles for Performance Interference-aware Cloud-to-Fog Application Migration,"An increasing number of interactive applications and services, such as online gaming and cognitive assistance, are being hosted in the cloud because of the elastic properties and cost benefits of distributed data centers. Despite these benefits, the longer and often unpredictable end-to-end network latencies between the end user and the cloud can be detrimental to time-critical response to the applications. Although technology enablers, such as Cloudlets or Micro Data Centers (MDCs), are increasingly being leveraged by cloud infrastructure providers to address the network latency concerns, existing efforts in re-provisioning services from the cloud to the MDCs seldom focus on ensuring that the performance properties of the migrated services are met. This chapter demonstrates the application of Dynamic Data-Driven Applications Systems (DDDAS) principles in the systems software layer, to address these limitations by: (a) determining when to re-provision; (b) identifying the appropriate MDC and a suitable host within that MDC that meets the performance considerations of the applications; and (c) ensuring that the cloud service provider continues to meet customer service-level objectives while keeping its operational and energy costs low. Empirical evaluations using a setup comprising a cloud data center and multiple MDCs composed of heterogeneous hardware are presented to validate the capabilities of the INDICES (Intelligent Deployment for ubiquitous Cloud and Edge Services) framework to process DDDAS methods. It should also be noted that the capabilitis created through INDICES are aimed to satisfy a broad set of applications requiring real-time data deliver and thus also satisfy the support requirements of environments enabling DDDAS-based applications."
pub.1121658688,BigDataSDNSim: A Simulator for Analyzing Big Data Applications in Software-Defined Cloud Data Centers,"Emerging paradigms of big data and Software-Defined Networking (SDN) in cloud
data centers have gained significant attention from industry and academia. The
integration and coordination of big data and SDN are required to improve the
application and network performance of big data applications. While empirical
evaluation and analysis of big data and SDN can be one way of observing
proposed solutions, it is often impractical or difficult to apply for several
reasons, such as expensive undertakings, time consuming, and complexity; in
addition, it is beyond the reach of most individuals. Thus, simulation tools
are preferable options for performing costeffective, scalable experimentation
in a controllable, repeatable, and configurable manner. To fill this gap, we
present a new, self-contained simulation tool named BigDataSDNSim that enables
the modeling and simulating of big data management systems (YARN), big data
programming models (MapReduce), and SDN-enabled networks within cloud computing
environments. To demonstrate the efficacy, effectiveness, and features of
BigDataSDNSim, a use-case that compares SDN-enabled networks with legacy
networks in terms of the performance and energy consumption is presented."
pub.1170125286,AI-based Quality-driven Decomposition Tool for Monolith to Microservice Migration,"Businesses and organizations are increasingly moving their business-critical systems to the cloud environment to take advantage of cloud benefits, thus improving its agility, maintainability, and flexibility. Businesses decompose their legacy monolith applications to cloud-native architecture such as microservice to leverage the cloud-native potential. With the expanding distributed nature of migrated applications, new challenges in determining the migrated architecture quality arise. Previous migration framework gives minimal attention to the post-migration quality aspect. This paper presents the Structural Quality (S-Quality) tool - a quality-driven decomposition tool that uses machine learning for migrating monolith applications to the microservice architecture. This tool utilizes various clustering techniques on monolith structural design properties to determine the service boundaries. In addition, this tool facilitates identifying the best microservice candidates based on migration architectural quality objectives through the scoring algorithm method, hence another contribution of this work. We validate our developed tool and scoring algorithm using a semi-structured interview with experts from the industry. Overall, findings indicate that the proposed scoring algorithm shows positive feedback from the experts and the acceptance of the S-Quality tool applicability by the industries."
pub.1093542125,BUSINESS INTELLIGENCE IN THE CLOUD,"Business Intelligence (BI) deals with integrated approaches to management support. Currently, there are constraints to BI adoption and a new era of analytic data management for business intelligence these constraints are the integrated infrastructures that are subject to BI have become complex, costly, and inflexible, the effort required consolidating and cleansing enterprise data and Performance impact on existing infrastructure/inadequate IT infrastructure. So, in this paper Cloud computing will be used as a possible remedy for these issues. We will represent a new environment atmosphere for the business intelligence to make the ability to shorten BI implementation windows, reduced cost for BI programs compared with traditional on-premise BI software, Ability to add environments for testing, proof-of-concepts and upgrades, offer users the potential for faster deployments and increased flexibility. Also, Cloud computing enables organizations to analyze terabytes of data faster and more economically than ever before. Business intelligence (BI) in the cloud can be like a big puzzle. Users can jump in and put together small pieces of the puzzle but until the whole thing is complete the user will lack an overall view of the big picture. In this paper reading each section will fill in a piece of the puzzle."
pub.1084683400,Cloud Integration Patterns,"Enterprises use the cloud for unlimited resource, scalability and elastic provisioning along with being able to use state of the art commodity or specialized solutions available in the cloud. The challenge of this vision is the proper and safe integration of on-premise IT-Landscapes with data and applications in the cloud. To find solutions for integration of classical and cloud environments two approaches, top-down and bottom-up, were used. In the top-down approach cloud integration patterns were specified based on scenarios. In the bottom-up approach cloud integration patterns were based on case study application requirements. Results of this paper are novel cloud integration patterns for various cloud integration scenarios."
pub.1061547615,Flexible communication-bus architecture for distributed multimedia service in cloud computing platform,"This paper proposes a flexible non-uniform communication bus model for multimedia services in cloud computing environment. Key features of the proposed bus model are as follows: First, the proposed bus model consists of application and transmission bindings that hierarchically cooperate together. Second, the proposed bus model has metaspace consisting of configuration control space and functional control space. It enables real-time adaptation of the communication bus. Third, the proposed bus model carries out profile-based adaptation. Through experiments, it is confirmed that the proposed bus model offers enhanced throughput when it is compared to legacy uniform bus model. This proposed bus model can be employed as a key infrastructure for the data transmission in cloud computing platform."
pub.1123308790,Implementing OPC-UA services for Industrial Cyber-Physical Systems in Service-Oriented Architecture,"Industrial cyber-physical systems are advancing rapidly along with the emergence of the fourth industrial revolution. It is, therefore, necessary to design and develop appropriate tools and frameworks that allow the migration/integration of legacy systems into the new Industry 4.0 environment. OPC-UA is one of the recommended technologies to enable communication and interoperability of digitalized assets in Industry 4.0. This paper proposes a solution to develop an OPC-UA interface for industrial systems for service-oriented architecture. With the use of the Arrowhead Framework - a cloud-based framework facilitating interoperability and integrability of Industrial Internet of Things - and Industry 4.0-compliant technologies, the authors describe the procedure of developing different application systems which dynamically produce and consume OPC-UA services within a local automation cloud."
pub.1182040100,Strategic Migration of Adobe Experience Manager : A Comprehensive Framework for On-Premise to Cloud Transition,"This article examines the strategic migration of Adobe Experience Manager (AEM) from on-premise environments to cloud platforms, addressing the growing demand for scalable and efficient content management solutions in the digital landscape. Through a comprehensive analysis of migration processes, this research presents a systematic framework for organizations undertaking AEM cloud transitions. The article investigates key components of migration, including code refactoring, configuration adjustments, content transfer strategies, and environment adaptation. Utilizing a mixed-method approach, incorporating both quantitative performance metrics and qualitative case study analysis, we evaluate the efficacy of migration tools such as Adobe's Cloud Acceleration Manager and custom scripts for business logic. Our findings reveal significant improvements in scalability, cost-efficiency, and system performance post-migration, while also highlighting potential challenges and mitigation strategies. This article contributes to the body of knowledge on enterprise content management systems and provides practical insights for organizations considering AEM cloud migration, offering a roadmap for successful transition in an increasingly cloud-centric technological ecosystem."
pub.1173294476,CSPUMS: Pioneering Integrated Monitoring in Multi-Service Provider Ecosystems,"In the era of information interconnection, the evolution of enterprise digitalization necessitates the adoption of architectures that can support dynamic and complex business operations. Microservices architecture, characterized by its flexibility, scalability, and efficiency, has emerged as a pivotal solution for modern enterprises, enabling the decomposition of monolithic applications into independently deployable services. However, with the expansion of enterprises and the intricate nature of business operations, microservices architecture encounters challenges in service governance, monitoring, management, and integration in multi-vendor environments. Addressing these challenges, this paper introduces a Cross-Service Provider Unified Monitoring System (CSPUMS) tailored for microservice business platforms operating within multi-service provider ecosystems. CSPUMS, a novel three-layer system architecture, encompasses a data acquisition layer, a monitoring layer, and a data presentation layer, designed to ensure the reliability, performance, and security of service platforms across diverse cloud environments. Through the deployment of Docker container technology for data aggregation and the implementation of a multi-dimensional threshold alarm template, CSPUMS enhances operational monitoring, fault management, and service continuity in complex multi-service settings. This paper significantly enhances microservice platforms by providing integrated monitoring solutions for multi-cloud and multi-service environments, ensuring operational efficiency and agility for digitally transforming enterprises."
pub.1094747164,Enterprise IT as a Service: Transforming the Delivery Model of IT Services,"Enterprises are under great pressure to be nimble and adjust quickly to ever shifting business priorities and constantly evolving technologies. This is usually seen as a driver to adopt Cloud technologies; however, the existing large investments in on premise IT infrastructure and data residency constraints combined with decreasing IT budgets precludes a fast and immediate migration to public or private clouds; hence the realization that Hybrid IT environment are here to stay for a while, making the efficient management of hybrid IT environments extremely critical. The pressure of decreasing IT budgets also precludes the high cost of investment in traditional tools and systems to manage those environments. This has resulted in the rise of an on-demand software-as-a-service (SaaS) solutions to manage these hybrid clouds. However, these SaaS solutions are typically delivered as island solutions and lack some of the characteristics that SaaS solutions in this space should exhibit in order to provide an efficient Enterprise IT management eco-system. In this paper, we first present the set of properties that SaaS solutions must exhibit in order to become part of the eco-system and describe the architecture of a platform where those SaaS solutions can operate and integrate with each other. We present the challenges of building such a platform, and how our architecture addresses these challenges. We showcase how we are leveraging this platform to transform a large service management provider's IT services to fit this new paradigm."
pub.1146630601,Towards securely migrating webassembly enclaves,"To run services in Trusted Execution Environments and Secure Enclaves is an established approach to protect privacy-sensitive data or payment processing. The increased importance of service elasticity in e.g. cloud computing, but also between edge and cloud, highlights the need of (hardware) architecture agnostic secure migration of such processing. We present an enclave software design, based on a WebAssembly (WASM) runtime, that allows for secure migration of enclave services with integrity and confidentiality guarantees for both enclave code and software state (data). We provide security analysis for our migration approach, and architecture benchmarking for a wide variety of existing (and future) secure enclave hardware as proof that elasticity in computation---without compromising security---is an achievable goal."
pub.1050893161,Dynamic Provisioning of Virtual Clusters for Grid Computing,"Virtual machines can greatly simplify grid computing by providing an isolated, well-known environment, while increasing security. Also, they can be used as the base technology to dynamically modify the computing elements of a grid, so providing an adaptive environment. In this paper we present a Grid architecture that allows to dynamically adapt the underlying hardware infrastructure to changing Virtual Organization (VO) demands. The backend of the system is able to provide on-demand virtual worker nodes to existing clusters and integrate them in any Globus-based Grid. In this way, we establish a basis to deploy self-adaptive Grids, which can support different VOs in shared physical infrastructures and dynamically adapt its software configuration. Experimental results on a prototyped testbed show less than a 10% overall performance loss including the hypervisor overhead."
pub.1107916205,Building lean continuous integration and delivery pipelines by applying DevOps principles: a case study at Varidesk,"Continuous Integration (CI) and Continuous Delivery (CD) are widely considered to be best practices in software development. Studies have shown however, that adopting these practices can be challenging and there are many barriers that engineers may face, such as – overly long build times, lack of support for desired workflows, issues with configuration, etc. At Varidesk, we recently began shifting our primary web application (from a monolithic) to a micro-services-based architecture and also adapted our software development practices to aim for more effective CI/CD. In doing so, we also ran into some of the same afore-mentioned barriers. In this paper we focus on two specific challenges that we faced – long wait times for builds/releases to be queued and completed, and the lack of support for tooling, especially from a cross-cloud perspective. We then present the solutions that we came up with, which involved re-thinking DevOps as it applied to us, and re-building our own CI/CD pipelines based on DevOps-supporting approaches such as containerization, infrastructure-as-code, and orchestration. Our re-designed pipelines have led us to see speed increases, in terms of total build/release time, in the range of 330x-1110x and have enabled us to seamlessly move from a single-cloud to a multi- cloud environment, with no architectural changes to any apps."
pub.1031561244,Inter-cloud Data Integration System Considering Privacy and Cost,"In spite of all the advantages delivered by cloud computing, still many challenges are hindering the migration of customer software and data into the cloud. Whenever information is shared in the cloud, privacy and security questions may arise. Although many technologies have been proposed in order to meet users’ requirements from privacy concern, however, at the same time, with the increasing number of processed data, the cost for privacy protection also increases dramatically. In this paper, we present a privacy-aware inter-cloud data integration system considering tradeoff between the privacy requirements from users and the charging for those data protection and processing. In contrast to existing data sharing techniques, our method is more practical as the cost for technical supporting privacy must be considered in the commoditized cloud computing environment."
pub.1124639733,Security Architecture for Cloud-Based Command and Control System in IoT Environment,"With the development of the fourth industrial technology, such as the Internet of Things (IoT) and cloud computing, developed countries including the U.S. are investigating the efficiency of national defense, the public sector and national innovation, and constructing the infrastructure for cloud computing environments through related policies. The Republic of Korea is enacting the related legislation and considering the fourth industrial technology in various fields. Particularly, it is considering the adaptation of the cloud to the command and control system in the national defense sector; hence, related research and pilot projects are being conducted. However, if the existing information system is converted to a cloud computing system by introducing IoT devices, existing security requirements cannot solve problems related to the security vulnerabilities of cloud computing. Therefore, to build a cloud-based secure command and control system, it is necessary to derive additional cloud computing-related security requirements that are lacking in the existing security requirements, and to build a secure national defense command and control system architecture based upon it. In this paper, we derive security requirements for a cloud-based command control system, propose a security architecture designed based thereupon, and implement a security architecture with an open-stack-based cloud platform, “OpenStack”."
pub.1095429287,Openstack-Paradigm Shift to Open Source Cloud Computing & Its Integration,"With emergence in cloud computing there was a huge demand for the data center technology and the operating system which can handle the data center. Increasing demand of infrastructure services leading the organizations to move towards Cloud. The aim is to provide an opportunity to the industry to build a hosting architecture, massively scalable which is completely open source, and to provide a solution to manage their on premises datacenters or private cloud and public cloud data centers simultaneously. When we talk about the combination of private as well as public cloud workloads, here term comes into existence i.e. Hybrid Cloud. To build and manage the Hybrid cloud, Openstack is the open source solution available in market. Here we will be discussing the whole concept of Open Stack in detail, It's architecture, functionalities and how we setup it in our environment tested different use cases."
pub.1138216120,"Multi-Site Network and Security Services with NSX-T, Implement Network Security, Stateful Services, and Operations","Know the basics of network security services and other stateful services such as NAT, gateway and distributed firewalls (L2-L7), virtual private networks (VPN), load balancing (LB), and IP address management. This book covers these network and security services and how NSX-T also offers integration and interoperability with various other products that are not only created by VMware, but are also referred by VMware as third-party integrated vendors. With the integration of VMware vRealize Automation, you can automate full application platforms consisting of multiple virtual machines with network and security services orchestrated and fully automated. From the operational perspective, this book provides best practices on how to configure logging, notification, and monitoring features and teaches you how to get the required visibility of not only your NSX-T platform but also your NSX-T-enabled network infrastructure. Another key part of this book is the explanation of multi-site capabilities and how network and security services can be offered across multiple on-premises locations with a single management pane. Interface with public cloud services also is included. The current position of NSX-T operation in on-premises private clouds and the position and integration with off-premises public clouds are covered as well. This book provides a good understanding of integrations with other software to bring the best out of NSX-T and offer even more features and capabilities. You will: Understand the NSX-T security firewall and advanced security Become familiar with NAT, DNS, DHCP, and load balancing features Monitor your NSX-T environment Be aware of NSX-T authentication and authorization possibilities Understand integration with cloud automation platforms Know what multi-cloud integrations are possible and how to integrate NSX-T with the public cloud"
pub.1093854485,Client-Cloud Computing: A Framework for Service Integration,"Many current users of cloud computing document-sharing services such as Google Docs (i.e., those who primarily access client-only mind map features) require a fast and simple mechanism for accessing mind map files in clouds. Based on a service-oriented architecture, we propose using a client-cloud computing (CCC) approach that integrates client software and cloud services. As part of this design, we have added a clientcloud framework (CCF) that supports component reuse and service integration. We used FreeMind Cloud to demonstrate how our proposed CCC approach can integrate client-only mind map software products such as FreeMind with an existing cloud service such as Google Docs. This integration allows users to share their local mind map files with other members of a group for reading or co-editing purposes. Our goal is to create an environment in which researchers or service providers can use the proposed CCC/CCF approach to integrate services with minimum reformatting requirements for file conversion."
pub.1150494049,Cloud Process Execution Engine: Architecture and Interfaces,"Process Execution Engines are a vital part of Business Process Management
(BPM) and Manufacturing Orchestration Management (MOM), as they allow the
business or manufacturing logic (expressed in a graphical notation such as
BPMN) to be executed. This execution drives and supervises all interactions
between humans, machines, software, and the environment. If done right, this
will lead to a highly flexible, low-code, and easy to maintain solution, that
allows for ad-hoc changes and functional evolution, as well as delivering a
wealth of data for data-science applications. The Cloud Process Execution
Engine CPEE.org implements a radically distributed scale-out architecture,
together with a minimal set of interfaces, to allow for the simplest possible
integration with existing services, machines, and existing data-analysis tools.
Its open-source components can serve as a blueprint for future development of
commercial solutions, and serves as a proven testbed for academic research,
teaching, and industrial application since 2008. In this paper we present the
architecture, interfaces that make CPEE.org possible, as well as discuss
different lifecycle models utilized during execution to provide overarching
support for a wide range of data-analysis tasks."
pub.1000298891,Cloud Blueprints for Integrating and Managing Cloud Federations,"Contemporary cloud technologies face insurmountable obstacles. They follow a pull-based, producer-centric trajectory to development where cloud consumers have to ‘squeeze and bolt’ applications onto cloud APIs. They also introduce a monolithic SaaS/PaaS/IaaS stack where a one-size-fits-all mentality and vendor lock-in prevail. As a consequence consumers cannot mix and match functionality from diverse tiers of the cloud stack – horizontally or vertically – dynamically configuring it to address their needs.This paper presents an approach that overcomes these obstacles by providing an innovative reference architecture, an enhanced cloud delivery model which breaks down the rigidity of the cloud stack and transforms it into modular and easily combined components, and a complete integration and management environment that offers integration-as-a-service functionality."
pub.1095115115,Legacy Applications on the Cloud: Challenges and Enablers Focusing on Application Performance Analysis and Providers Characteristics,"The advent of Cloud computing led to new ways for developing, engineering, providing and consuming services. As a paradigm building on a set of combined technologies, clouds enable on-demand service provisioning with guaranteed levels of quality on virtualized resources across disparate administrative domains. The latter has been one of the main factors for the wide adoption of clouds. Nevertheless, one class of applications has not yet taken advantage of the added-value of clouds: legacy applications. In this paper we highlight the challenges that arise when porting legacy applications on cloud environments and propose enabling technologies for overcoming these challenges and the corresponding limitations. We emphasize on application analysis and aspects oriented to quality of service, multi - tenancy contexts that affect the application behaviour at runtime, as well as characterization of providers in order to support decision making regarding the optimum ones to host the aforementioned legacy applications following their analysis."
pub.1100551072,CloudMF,"
                    While the number of cloud solutions is continuously increasing, the development and operation of large-scale and distributed cloud applications are still challenging. A major challenge is the lack of interoperability between the existing cloud solutions, which increases the complexity of maintaining and evolving complex applications potentially deployed across multiple cloud infrastructures and platforms. In this article, we show how the Cloud Modelling Framework leverages model-driven engineering and supports the DevOps ideas to tame this complexity by providing:
                    (i)
                    a domain-specific language for specifying the provisioning and deployment of multi-cloud applications, and
                    (ii)
                    a models@run-time environment for their continuous provisioning, deployment, and adaptation.
                  "
pub.1148159665,Improved Data Security in Cloud Environment for Test Automation Framework and Access Control for Industry 4.0,"In analyzing project regressions, automation has emerged as a major agenda in managing changes in software which requires minimum manual intervention. For rapid testing environment, software development processes such as Agile, Scrum, and XP processes depend on continuous integration tools. There is no single tool to handle the project automation, and the main challenge is dependency on multiple tools. The proposed automation tool should support configuration, execution, and debugging facility. Integrating the project automation works such as software configuration management tools Mercurial and Git, job scheduling tools like Jenkins and Apache Continuum, test management tools like TestNG and Selenium need tight integration which is a challenge. The existing PKI infrastructure for access control does not share data among the software tools and processes increasing the complexity when an organization needs to leverage the existing cloud services. The proposed approach optimizes the execution time by taking single CSV with input test case and metadata information and efficiently group and executes the tests automatically. The proposed method includes implementation of security access control mechanism for the jobs execution platform in cloud environment."
pub.1163763317,COGNIT: Challenges and Vision for a Serverless and Multi-Provider Cognitive Cloud-Edge Continuum,"Use of the serverless paradigm in cloud application development is growing rapidly, primarily driven by its promise to free developers from the responsibility of provisioning, operating, and scaling the underlying infrastructure. However, modern cloud-edge infrastructures are characterized by large numbers of disparate providers, constrained resource devices, platform heterogeneity, infrastructural dynamicity, and the need to orchestrate geographically distributed nodes and devices over public networks. This presents significant management complexity that must be addressed if serverless technologies are to be used in production systems. This position paper introduces COGNIT, a major new European initiative aiming to integrate AI technology into cloud-edge management systems to create a Cognitive Cloud reference framework and associated tools for serverless computing at the edge. COGNIT aims to: 1) support an innovative new serverless paradigm for edge application management and enhanced digital sovereignty for users and developers; 2) enable on-demand deployment of large-scale, highly distributed and self-adaptive serverless environments using existing cloud resources; 3) optimize data placement according to changes in energy efficiency heuristics and application demands and behavior; 4) enable secure and trusted execution of serverless runtimes. We identify and discuss seven research challenges related to the integration of serverless technologies with multi-provider Edge infrastructures and present our vision for how these challenges can be solved. We introduce a high-level view of our reference architecture for serverless cloud-edge continuum systems, and detail four motivating real-world use cases that will be used for validation, drawing from domains within Smart Cities, Agriculture and Environment, Energy, and Cybersecurity."
pub.1043660625,A Conceptual 5G Vehicular Networking Architecture,"This chapter presents a thorough investigation on current vehicular networking architectures (access technologies and overlay networks) and their (r)evolution towards the 5G era. The main driving force behind vehicular networking is to increase safety, with several other applications exploiting this ecosystem for traffic efficiency and infotainment provision. The most prominent existing candidates for vehicular networking are based on dedicated short range communications (DSRC) and cellular (4G) communications. In addition, the maturity of cloud computing has accommodated the invasion of vehicular space with cloud-based services. Nevertheless, current architectures can not meet the latency requirements of Intelligent Transport Systems (ITS) applications in highly congested and mobile environments. The future trend of autonomous driving pushes current networking architectures further to their limits with hard real-time requirements. Vehicular networks in 5G have to address five major challenges that affect current architectures: congestion, mobility management, backhaul networking, air interface and security. As networking transforms from simple connectivity provision, to service and content provision, fog computing approaches with caching and pre-fetching improve significantly the performance of the networks. The cloudification of network resources through software defined networking (SDN)/network function virtualization (NFV) principles, is another promising enabler for efficient vehicular networking in 5G. Finally, new wireless access mechanisms combined with current DSRC and 4G will enable to bring the vehicles in the cloud."
pub.1011250190,Requirements model driven adaption and evolution of Internetware,"Today’s software systems need to support complex business operations and processes. The development of the web-based software systems has been pushing up the limits of traditional software engineering methodologies and technologies as they are required to be used and updated almost real-time, so that users can interact and share the same applications over the internet as needed. These applications have to adapt quickly to the diversified and dynamic changing requirements in the physical, technological, economical and social environments. As a consequence, we are expecting a major paradigm shift in software engineering to reflect such changes in computing environment in order to better address the fundamental needs of organisations in this new era. Existing software technologies, such as model driven development, business process engineering, online (re-)configuration, composition and adaptation of managerial functionalities are being repurposed to reduce the time taken for software development by reusing software codes. The ability to dynamically combine contents from numerous web sites and local resources, and the ability to instantly publish services worldwide have opened up entirely new possibilities for software development. In retrospect to the ten years applied research on Internetware, we have witnessed such a paradigm shift, which brings about many changes to the developmental experience of conventional web applications. Several related technologies, such as cloud computing, service computing, cyber-physical systems and social computing, have converged to address this emerging issue with emphasis on different aspects. In this paper, we first outline the requirements that the Internetware software paradigm should meet to excel at web application adaptation; we then propose a requirement model driven method for adaptive and evolutionary applications; and we report our experiences and case studies of applying it to an enterprise information system. Our goal is to provide high-level guidelines to researchers and practitioners to meet the challenges of building adaptive industrial-strength applications with the spectrum of processes, techniques and facilities provided within the Internetware paradigm."
pub.1162993509,Efficient Dynamic Load Balancing in Cloud Network,"Cloud computing provides diversity of services as a platform, software and infrastructure in distributed environment. With its massive usage, clients are requesting for more administration and better outcomes on demand basis which requires cutting edge task schedulers for scheduling of tasks among virtual machines in data center. In spite of the many past explorations directed in the Cloud Computing field, a few difficulties still exist associated with balancing load in cloud-based applications and exclusively in the Infrastructure as service (IaaS) cloud model. Load balancing came into picture to optimally achieve the goal by balancing load among virtual machines and achieve fair utilization of resources using Meta Heuristic algorithms. In this paper, dynamic load balancing has been taken care among virtual machines using hybridization of Harris Hawks optimization with sine cosine algorithm (IHHOSCA) and evaluates fitness function to obtain optimal VM for balancing load in cloud network. The proposed algorithm is carried out using CloudSim 3.0.3 simulator for evaluating performance and efficacy of the system. The results proven to show better performance in means of Makespan, Resource utilization, Migration time, Power consumption, and CPU load on compared with many existing heuristic algorithms."
pub.1117877704,A Simulation Framework for Virtualized Resources in Cloud Data Center Networks,"Many IT companies are embracing the new softwarization paradigm through the adoption of new architecture models, such as software-defined network and network function virtualization, primarily to limit the costs of maintaining and deploying their network infrastructures, by giving the possibility to service/application providers to reconfigure and programmatically perform actions on the network. Accordingly, the dynamic management of the data center networks requires complex operations to ensure high availability and continuous reliability in order to guarantee full functionality of the virtualized resources. In this context, simulator-based approaches are helpful for planning and evaluating the deployment of the cloud data center networking, but existing cloud simulators have several limitations: they have too high overhead for wide-scale data center networks, complex configuration, and too abstract deployment models. For these motivations, we propose DCNs-2, a novel extension for the Ns-2 simulator, as a valid solution to efficiently simulate a cloud network infrastructure, with all the involved entities, such as switches, physical/virtual machines, and racks. The proposed solution not only makes configuration easier, but through extensive tests, we show that its execution overhead is limited to less than 130 MB of memory and the execution time is acceptable even for very wide-scale and complex deployment environments."
pub.1122141389,How Do you Migrate your System to the Cloud Environment? An Example of Conversion from Silo Type to Horizontal Type,"At Fukuoka University, we are promoting the transition of information systems centered on administrative systems to the cloud environment. Existing systems are composed of on-premises, and migration over 100 servers to the cloud environment including Azure is our task. Our main missions are two. One is to unify the complex system infrastructure by utilizing virtualization technology and the cloud. The second is to reduce the operating costs of those system infrastructures for the future. In this transition, we aimed to standardize the system environment by establishing a new server infrastructure utilizing the cloud, from silo type system construction. As an actual effort, we created standard service specifications to present to each system construction vendor, unified support windows, and routinely apply server patches. As a result of this standardization, centralized management operation of the system infrastructure was achieved. Moreover, we were able to contribute to a reduction in human cost and operation cost associated with system operation. In this report, we will introduce our approach and report the position of each step. In particular, we would like to share our approach and experiences with the vendors and departments that build and operate each system as an information infrastructure department and how we organized the structure and operation of the campus information system."
pub.1138989831,A Layered Approach for Modular Container Construction and Orchestration in HPC Environments,"Large-scale, high-throughput computational science faces an accelerating convergence of software and hardware. Software container-based solutions have become common in cloud-based datacenter environments, and are considered promising tools for addressing heterogeneity and portability concerns. However, container solutions reflect a set of assumptions which complicate their adoption by developers and users of scientific workflow applications. Nor are containers a universal solution for deployment in high-performance computing (HPC) environments which have specialized and vertically integrated scheduling and runtime software stacks. In this paper, we present a container design and deployment approach which uses modular layering to ease the deployment of containers into existing HPC environments. This layered approach allows operating system integrations, support for different communication and performance monitoring libraries, and application code to be defined and interchanged in isolation. We describe in this paper the details of our approach, including specifics about container deployment and orchestration for different HPC scheduling systems. We also describe how this layering method can be used to build containers for two separate applications, each deployed on clusters with different batch schedulers, MPI networking support, and performance monitoring requirements. Our experience indicates that the layered approach is a viable strategy for building applications intended to provide similar behavior across widely varying deployment targets."
pub.1093574732,SIMPLE AND EFFICIENT INTEGRATION OF AERONAUTICAL SUPPORT TOOLS FOR HUMAN-IN-THE-LOOP EVALUATIONS,"Automation of aeronautical procedures and automated support tools for controllers and pilots have to be evaluated carefully with human operators in the loop. However, the integration of different vendor's operational aeronautical software applications with experimental support tools in simulated air-space environments is demanding and costly. Existing tools are usually not designed to be integrated with each other, thus an efficient and vendor-neutral way to integrate existing tools in a simple and efficient way is needed. This paper discusses a simple, efficient, and proven approach for the integration of a wide range of aeronautical software applications (commercial and academic, operational and experimental) in a unified human-in-the-loop evaluation environment. This approach integrates existing simulation and support tools using a simple software adapter to distribute shared simulation state in an internet-protocol-based multicast cloud using XML as presentation layer."
pub.1093688490,Cloud Portability Standardization Overview,"With the development of various cloud technologies and their increased usage, the newly arisen challenge is the migration of a cloud-based application and data among various cloud environments and vendors. In the absence of a standardised mean by which full portability can be achieved, various tools exist in the form of APIs, standards, and protocols. In this paper we conduct an overview of the state of the art regarding the topic of migration and portability of cloud-based applications, in order to determine the possible challenges that today's technologies might have, as well as to find the most appropriate automated way to port an application from one to another cloud, or to migrate an on-premise application to the cloud."
pub.1136924018,An adaptive and cost‐efficient migration to cloud approach in dynamic environments,"Summary Due to the variety in available cloud providers along with the frequent changes in strategic objectives of an enterprise, migrating existing software components to the cloud has become a challenging decision in the software maintenance phase. “Financial” and “customer satisfaction” viewpoints are two important strategic objectives of all enterprises that greatly affect the decision about migration to the cloud. Moreover immense number of target cloud services with too many configurations and cost models has made the search space of possible migrations huge. Many existing approaches of software migration to the cloud have modeled the problem as deployment optimization of software components over available platforms, while in this research following a valid migration plan is intended rather than proposing a final optimal migration solution (deployment). A migration plan is a sequence of actions to be taken by the technical team to move the software components to the cloud stepwise. Since at each stage of the migration there might be many valid alternative paths to follow, a recommender module is proposed to direct the management by recommending the best migration plan out of all valid plans in a Labeled Transition System. The recommendation is based on the current state of the enterprise which is estimated using a two‐state Hidden Markovian Model by observing ambient signals. The empirical study showed that particularly in dynamic and changing conditions the proposed adaptive and plan‐oriented method succeeded in posing lower accrued maintenance costs on the enterprise over time with confidence 90% compared to the non‐adaptive method due to its reactive and self‐balancing nature."
pub.1181426179,Scalable Microservices for Cloud Based Distributed Systems,"In the evolving landscape of cloud computing, scalable microservices have emerged as a pivotal architecture for developing distributed systems. This approach facilitates the decomposition of applications into smaller, independently deployable services, allowing for greater agility and scalability. This paper explores the essential principles of microservices architecture, highlighting its advantages over monolithic systems, such as improved fault isolation, enhanced scalability, and streamlined continuous integration and deployment processes. We examine the key design patterns and technologies that support microservices, including containerization, orchestration, and service discovery. Additionally, the role of cloud platforms in enabling microservices is analyzed, focusing on how they provide the infrastructure necessary for dynamic resource allocation and automated scaling.
 The paper also addresses the challenges associated with implementing microservices in distributed environments, such as inter-service communication, data consistency, and security concerns. Solutions for these challenges, including API gateways and circuit breakers, are discussed. Through case studies and practical examples, we demonstrate how organizations can leverage scalable microservices to enhance operational efficiency and accelerate time-to-market for new features and services. Ultimately, this study emphasizes that adopting scalable microservices in cloud-based distributed systems is not merely a technological shift but a strategic imperative for businesses seeking to innovate and remain competitive in the digital age. By fostering an adaptable and resilient architecture, organizations can better meet the demands of modern applications and customer expectations."
pub.1048320468,Migrating e-Science Applications to the Cloud: Methodology and Evaluation,"This chapter introduces an application migration methodology that incorporates the aspects and a decision support, application refactoring and data migration tool which supports application developers in realizing this methodology. It analyses the proposed methodology and enabling tool using a case study conducted in the context of an e-science project. Typically, cloud-enabling applications are related to the migration of whole systems or parts of them on a public or private cloud environment. The migration of the SimTech Scientific Workflow Management Systems has been done using the Cloud Data Migration Support Tool—a proof-of-concept implementation of the methodology. The chapter reviews existing literature on recommendations, benefits, and use cases with respect to the usage of cloud computing for e-science. It investigates available vendor-specific and vendor-independent methodologies and guidelines for migrating either the database layer or the whole application to the cloud. The chapter considers available recommendations and decision support systems with respect to migration to the cloud."
pub.1171019943,ADVANCED METHODS FOR MAINTAINING AND MANAGING THE LIFE CYCLE OF CLOUD ENVIRONMENTS: SURVEY,"Resource management is a fundamental concept in cloud computing and virtualization, encompassing the allocation, release, coordination, and monitoring of cloud resources to optimize efficiency. The complexity arises from the virtualized, heterogeneous, and multi-user nature of these resources. Effective governance is challenging due to uncertainty, large-scale infrastructures, and unpredictable user states. This paper presents a comprehensive taxonomy of resource management technologies, offering a detailed analysis of design architecture, virtualization, and cloud deployment models, along with capabilities, objectives, methods, and mechanisms. In a cloud computing environment, deploying application-based resource management techniques necessitates understanding the system architecture and deployment model. This paper explores centralized and distributed resource management system architectures, providing a review of effective resource management techniques for both, accompanied by a comparative analysis. The evolution of cloud computing from a centralized to a distributed paradigm is examined, emphasizing the shift towards distributed cloud architectures to harness the computing power of smart connected devices at the network edge. These architectures address challenges like latency, energy consumption, and security, crucial for IoT-based applications. The literature proposes various methods for distributed resource management, aligning with the distributed nature of these architectures. Resource management in cloud computing involves discovery, provisioning, allocation, and monitoring functions, with sub-functions like mapping and scheduling. Integrated approaches to consolidation and resource management have been explored in numerous studies. This paper summarizes and analyzes existing research on resource management functions, focusing on identification, provisioning, allocation planning, and monitoring, based on their objectives and methods."
pub.1094494344,Towards Weather Forecasting in the Cloud,"Scientific applications process a large amount of data and need huge computing power. Traditionally, they are executed in supercomputers, cluster or grid environments. Recently, the cloud also emerged as a feasible execution environment for this kind of application. The viability of using cloud was already extensively validated. In this work, we migrated a weather forecast application to the Microsoft Azure cloud. The intention of our work is to perform an application modernization using the cloud technology. Several pre- and post-processing procedures were modified to decrease or eliminate repetitive tasks. Additionally, the configuration of the application is no longer necessary in the user perspective. We can conclude that with some modification, scientific applications can benefit from cloud technologies, and it is possible to create modern services based on legacy applications."
pub.1122790790,"Understanding Azure Monitoring, Includes IaaS and PaaS Scenarios","Explore the architectural constructs of Azure monitoring capabilities and learn various design and implementation aspects for complex use cases. This book covers the different scenarios in a modern-day multi-cloud enterprise and the tools available in Azure for monitoring and securing these environments. Understanding Azure Monitoring starts by discussing the rapid changes happening in the cloud and the challenges faced by cloud architects. You will then look at the basics of Azure monitoring and the available tools, including service level agreements (SLAs), auditing, and security. Next, you will learn how to select the best tools for monitoring, operational strategy, and integration with on-premises SIEM systems. You’ll work through some scenario-based examples to monitor the workload and respond to failures. Here, you will monitor a simple web application on Azure, a multi-region web application, and applications that include PaaS and IaaS services. Towards the end of the book, you will explore monitoring in DevOps and see why it is important to be aware of continuous changes. You will: Work with Azure IaaS and PaaS resources and monitoring and diagnostics capabilities Discover how the operational landscape changes on Azure Look at cloud-only and on-premises hybrid integration Study architectural constructs for design and implementation"
pub.1155892199,Shipping code towards data in an inter-region serverless environment to leverage latency,"Serverless computing emerges as a new standard to build cloud applications, where developers write compact functions that respond to events in the cloud infrastructure. Several cloud service industries started adopting serverless for deploying their applications. But one key limitation in serverless computing is that it disregards the significance of data. In the age of big data, when applications run around a huge volume, to transfer data from the data side to the computation side to co-allocate the data and code, leads to high latency. All existing serverless architectures are based on the data shipping architecture. In this paper, we present an inter-region code shipping architecture for serverless, that enables the code to flow from computation side to the data side where the size of the code is negligible compared to the data size. We tested our proposed architecture over a real-time cloud platform Amazon Web Services with the integration of the Fission serverless tool. The evaluation of the proposed code shipping architecture shows for a data file size of 64 MB, the latency in the proposed code shipping architecture is 8.36 ms and in existing data shipped architecture is found to be 16.8 ms. Hence, the proposed architecture achieves a speedup of 2x on the round latency for high data sizes in a serverless environment. We define round latency to be the duration to read and write back the data in the storage."
pub.1168666979,Flexible zero trust architecture for the cybersecurity of industrial IoT infrastructures,"The growing digitalization of industrial systems and the increasing adoption of cloud technologies pose significant challenges to the secure management of modern industrial infrastructures integrating different Industrial Internet of Things (IIoT). Existing cybersecurity solutions can manage uniform and centralized software systems but are not designed to accommodate the requirements of heterogeneous IIoT devices, such as hard real-time operations, high reliability, and decentralization for distributed decision-making. We present a novel security architecture that is specifically designed to address the stringent requirements of IIoT systems. It is based on a network micro-segmentation that can be seamlessly integrated into existing environments, and two main components: a software-defined network (SDN) ensuring a unified abstraction layer for policy enforcement across diverse environments; and a centralized security management layer that simplifies the policy execution of any architectural design. We demonstrate the feasibility and effects of this original combination through a prototype. It experimentally demonstrates that our peer-to-peer SDN coupled with an asynchronous policy distribution process guarantees resiliency to individual failures, and enables fully decentralized operations while still ensuring a central flexible management of network topology and security policies."
pub.1164025993,Towards characterization of edge-cloud continuum,"Internet of Things and cloud computing are two technological paradigms that
reached widespread adoption in recent years. These paradigms are complementary:
IoT applications often rely on the computational resources of the cloud to
process the data generated by IoT devices. The highly distributed nature of IoT
applications and the giant amounts of data involved led to significant parts of
computation being moved from the centralized cloud to the edge of the network.
This gave rise to new hybrid paradigms, such as edge-cloud computing and fog
computing. Recent advances in IoT hardware, combined with the continued
increase in complexity and variability of the edge-cloud environment, led to an
emergence of a new vision of edge-cloud continuum: the next step of integration
between the IoT and the cloud, where software components can seamlessly move
between the levels of computational hierarchy. However, as this concept is very
new, there is still no established view of what exactly it entails. Several
views on the future edge-cloud continuum have been proposed, each with its own
set of requirements and expected characteristics. In order to move the
discussion of this concept forward, these views need to be put into a coherent
picture. In this paper, we provide a review and generalization of the existing
literature on edge-cloud continuum, point out its expected features, and
discuss the challenges that need to be addressed in order to bring about this
envisioned environment for the next generation of smart distributed
applications."
pub.1175314476,Migration of On-Premises Database to Cloud and Perform Explanatory Analytics on Sales Data,"This paper explores the migration of an industrial company's sales database from its servers to the cloud using Microsoft Azure, emphasizing three unique aspects of the process. First, it integrates the Delta Lake format within Azure Data Lake Storage Gen2, which is critical for maintaining multiple versions of data securely and ensuring ACID compliance. Second, the paper addresses significant adoption challenges such as data security, recovery, and vendor lock-in. It provides practical advice and strategic insights to help organizations navigate the complexities of cloud migration. Third, it offers a comparative analysis of two cloud storage methods—serverless and dedicated pool storage. This analysis evaluates their performance, cost-effectiveness, and suitability for different workload sizes, providing valuable insights for selecting the optimal storage strategy. Overall, this study contributes to a deeper understanding of cloud migrations, emphasizing practical applications and strategic decision-making necessary for enhancing operational efficiency and effective data management in cloud environments."
pub.1094818665,Change Impact Analysis and Propagation in Service Based Business Process Management Systems Preliminary Results from a Systematic Review,"Change Impact analysis and propagation have widely been studied in Software engineering research, but most studies are related to monolithic software applications, and very few studies have focused on distributed environments. Newer technologies like SOA, BPM and Cloud demand different perspectives and newer tools and techniques to support impact analysis and propagation in distributed environments. SOA adoption is fairly recent and major concern is now shifting towards maintenance and evolution of the service based business Process management systems. Change impact analysis and propagation have been identified as a potential research area in this context. This study is part of a larger study to systematically review all available research on impact analysis and propagation in context of Business process management (BPM) and Service Oriented Architecture (SOA). Preliminary results have been reported by answering 2 selected research Questions. 43 studies were selected out of initial set of 182 research articles. Studies answering selected research questions have been included in this report. BPM is considered at Business Level for Business operations and Process Models, while SOA is considered as a Deployment Architecture at service level. We have extended the scope of our study to Inter-Process and inter-service change analysis in addition to Top-Down, Bottom-Up analysis. This study revealed that although evolution of service based systems is getting significant attention, very few approaches and tools exist to support impact analysis and propagation activities."
pub.1141401516,Investigation of Cloud ERP Adoption in the Healthcare Industry Through Technology-Organization-Environment (TOE) Framework,"<p>There is an accelerated migration from on-premise ERP to Cloud ERP systems in many industries, but this transition is relatively slow in the healthcare industry. To address this concern, we developed a research model based on Technology-Organization-Environment (TOE) framework, and explored it in the healthcare industry through semi-structured interviews with IT managers and finance managers. We found noticeable differences between small-sized and large-sized healthcare organizations, as well as the perceptions of IT managers and finance managers in Cloud ERP adoption decisions. We discussed these findings, and proposed future research questions on Cloud ERP adoption in the healthcare industry.</p>"
pub.1067539497,Migration of enterprise applications to the cloud,"Abstract The migration of existing applications to the cloud enables enterprises to preserve their previous investments and—at the same time—to benefit from the properties of the cloud. This article presents a semi-automated approach for migrating existing enterprise applications to the cloud. Thereby, information about the application is gathered in the source environment, the application is extracted, transformed, and cloud-enabled. This makes the application ready for provisioning in the target cloud. Cloud-enabling an application preserves its business functionality and does not change the fundamental way the application was built."
pub.1149968095,Dual-architecture application parallel and traffic switching solution,"With the continuous growth of business volume and functional requirements, the application of major business systems is gradually implementing the upgrade from the Spring Boot architecture to the Spring Cloud microservice architecture. Due to the large degree of version change, it needs to undergo sufficient internal testing and external debugging. can be officially launched. Under the requirements of limited machine resources in the DMZ domain in the existing joint debugging environment and minimizing the exposure of the public network, the article proposes a dual-architecture application parallel and traffic switching scheme based on Nginx - F5, so that the test system has both The external debugging and testing function of Boot and Cloud architecture applications. This solution splits the requests of external merchants according to attribute identifiers such as merchant code, business type or provincial code, and a certain percentage, and forwards them to the microservice application system, so that the microservice The application of the service version has been fully debugged, providing a reference for system expansion and construction that requires parallel testing of large versions."
pub.1146066487,A Case Study of Cloud Computing Service Models for the General Computing Environment in a University,"<p>Cloud computing technology has emerged as the basic infrastructure technology in the 4th industrial revolution and has been utilized in various fields. Therefore, the authors analyze the requirements of academic and administrative affairs, education affairs, and affiliated institutions in the university. Proposed cloud computing service models in the universities are implemented using an OpenStack in CentOS 7.3 and Windows. To evaluate the performance, the author compares various performance factors including launching time, booting time, ping time, and software installation time. Lastly, the economic benefits in the university will be evaluated by comparing the adoption cost, maintenance cost, and total cost between the existing client-server system and cloud computing system. Future work aims to implement a cloud computing system that can be applied to the entire university. Finally, based on these studies, the authors will implement a regional base cloud system that can connect all universities in the region with one cloud computing system.</p>"
pub.1159889728,A Quality Driven Framework for Decomposing Legacy Monolith Applications to Microservice Architecture,"<p>Transforming monolith applications to microservice architecture is a common cloud migration strategy for businesses to accomplish cloud-native benefits. However, decomposing monolith applications is a challenging task that requires experience, skills, and dedication to initiate this process, and often, the migrated product quality is neglected. The lack of relevant guidelines on the design quality for distributed cloud environment architecture such as microservice further exacerbates this concern. We propose a quality-driven decomposition framework for migrating monolith applications to the cloud-native architecture. Our approach implies six activities in decomposing monolith applications from the source code to the microservice architecture. This framework supports various architectural design properties related to maintainability quality. Furthermore, this framework enhances the machine learning approach to enable automatic microservice identification, hence evaluating the design quality using a scoring-based approach. We use five applications to evaluate our approach, and the results show that our framework can provide insightful judgment to the designer regarding microservice design quality.</p>"
pub.1160169398,Transformation of a Legacy Airport Meteorology Application into a Serverless Cloud Application,"In this article we describe our continuing work on transforming a legacy application dealing with airport meteorology into a cloud application, using the concepts of serverless computing and Function-as-a-Service. In our previous work [1] we have described the architecture and concepts of our solution, and initial experiments with several FaaS tools. Here we continue the experiments, concentrating on the Apache OpenWhisk framework, which we have selected as our target serverless environment. We have further developed some of the application’s components and tried to execute them via OpenWhisk. During these experiments we have found multiple practical problems of transforming and executing legacy applications as serverless functions in OpenWhisk, and have devised solutions for these problems."
pub.1106815386,PhenoMeNal: Processing and analysis of Metabolomics data in the Cloud,"Abstract
                
                  Background
                  Metabolomics is the comprehensive study of a multitude of small molecules to gain insight into an organism’s metabolism. The research field is dynamic and expanding with applications across biomedical, biotechnological and many other applied biological domains. Its computationally-intensive nature has driven requirements for open data formats, data repositories and data analysis tools. However, the rapid progress has resulted in a mosaic of independent – and sometimes incompatible – analysis methods that are difficult to connect into a useful and complete data analysis solution.
                
                
                  Findings
                  The PhenoMeNal (Phenome and Metabolome aNalysis) e-infrastructure provides a complete, workflow-oriented, interoperable metabolomics data analysis solution for a modern infrastructure-as-a-service (IaaS) cloud platform. PhenoMeNal seamlessly integrates a wide array of existing open source tools which are tested and packaged as Docker containers through the project’s continuous integration process and deployed based on a kubernetes orchestration framework. It also provides a number of standardized, automated and published analysis workflows in the user interfaces Galaxy, Jupyter, Luigi and Pachyderm.
                
                
                  Conclusions
                  PhenoMeNal constitutes a keystone solution in cloud infrastructures available for metabolomics. It provides scientists with a ready-to-use, workflow-driven, reproducible and shareable data analysis platform harmonizing the software installation and configuration through user-friendly web interfaces. The deployed cloud environments can be dynamically scaled to enable large-scale analyses which are interfaced through standard data formats, versioned, and have been tested for reproducibility and interoperability. The flexible implementation of PhenoMeNal allows easy adaptation of the infrastructure to other application areas and ‘omics research domains.
                "
pub.1141689680,"An Architecture for Digital Processes in Manufacturing with Blockchain, Docker and Cloud Storage","The Blockchain is one of the last decade emerging technologies in software architectures. Its nature of a distributed ledger database allowing verifiable and tamper-proof transactions between untrusted parties makes it suitable for a vast class of domains concerning business processes, including Cloud Manufacturing, a new paradigm for the manufacturing industry based on cloud technologies, for which decentralization and security are key factors. However, existing solutions are still weak in terms of collaboration in providing and consuming heterogeneous services in the Cloud, therefore a standard framework is necessary to overcome this limit. In this paper, we suggest an architecture for consuming digital processes in a manufacturing environment, based on Blockchain and Smart Contracts. Our primary contribution is the integration of Blockchain with two other popular technologies: Docker, a highly portable and scalable container-based platform to run applications, and Cloud Storage. In this system, the logic of a single process is defined by the owner in a self-contained Docker image, whose digest is safely stored in the chain, while input and output files can be stored in a traditional cloud storage service. On each new consumer request, the best runner node is selected through the solution of a simple task assignment problem with a deep learning approach."
pub.1023136006,Towards a scalable metacomputing storage service,"We describe a prototypical storage service through which we are addressing some of the open storage issues in wide-area distributed high-performance computing. We discuss some of the relevant topics such as latency-tolerance, hierarchical storage integration, and legacy and commercial application support. Existing high-performance computing environments are either ad-hoc or focus narrowly on the simple clienserver case. The storage service which we are developing as part of the DISCWorld metacomputing infrastructure, will provide high-performance access to a global “cloud”, of storage resources in a manner which is scalable, secure, adaptive and portable requiring no application or operating system modifications. Our system design provides flexible, modular and user-extensible access to arbitrary storage mechanisms and on-demand data generation and transformations. We describe our current prototype's status, some performance analysis, other related research and our future plans for the system."
pub.1112074557,Software Evolution,"Software evolution plays an ever-increasing role in software development. Programmers rarely build software from scratch but often spend more time in modifying existing software to provide new features to customers and fix defects in existing software. Evolving software systems are often a time-consuming and error-prone process. This chapter overviews key concepts and principles in the area of software evolution and presents the fundamentals of state-of-the art methods, tools, and techniques for evolving software. The chapter first classifies the types of software changes into four types: perfective changes to expand the existing requirements of a system, corrective changes for resolving defects, adaptive changes to accommodate any modifications to the environments, and finally preventive changes to improve the maintainability of software. For each type of changes, the chapter overviews software evolution techniques from the perspective of three kinds of activities: (1) applying changes, (2) inspecting changes, and (3) validating changes. The chapter concludes with the discussion of open problems and research challenges for the future."
pub.1181581047,Development Towards a Cloud-Based Failover Architecture to Support Zambia Revenue Authority’s Domestic Tax Systems,"The research abstract explores the necessity of transitioning Zambia's domestic tax systems to a cloud-based failover architecture. Identifying existing challenges such as manual disaster recovery, system failures requiring human intervention, and inadequate logging, causing extended downtimes, data loss, and reduced taxpayer confidence. The study aims to introduce a hybrid cloud web application architecture, focusing on eradicating application downtime and ensuring taxpayer data security. Cloud-based standby failover architectures promise high uptime and data integrity, with servers hosted online to ensure uninterrupted business operations. The research objectives include creating a warm backup server on Amazon Web Services (AWS) private cloud servers, implementing data encryption policies, and evaluating the impact on tax application uptime and recovery metrics. The significance lies in enhancing system resilience, scalability, cost-efficiency, security, and accessibility, providing benefits like continuity, cost savings, security enhancements, and technological advancements. The methodology involved expert consultations and historical analysis to identify system inefficiencies, determine efficient failover architecture, and gather relevant literature for a secure hybrid cloud approach. The prototype test environment was successfully set up and tests were conducted, requiring scaling to mirror the ZRA production environment. Tasks including data encryption and security checks on application databases were implemented. Results support the hypothesis that the advantages of a cloud-based failover system outweigh potential challenges like data security, sovereignty concerns, cost management, integration complexity, legal compliance, and cybersecurity threats."
pub.1034952758,Self-adaptive Architectures for Autonomic Computational Science,"Self-adaptation enables a system to modify it’s behaviour based on changes in its operating environment. Such a system must utilize monitoring information to determine how to respond either through a systems administrator or automatically (based on policies pre-defined by an administrator) to such changes. In computational science applications that utilize distributed infrastructure (such as Computational Grids and Clouds), dealing with heterogeneity and scale of the underlying infrastructure remains a challenge. Many applications that do adapt to changes in underlying operating environments often utilize ad hoc, application-specific approaches. The aim of this work is to generalize from existing examples, and thereby lay the foundation for a framework for Autonomic Computational Science (ACS). We use two existing applications – Ensemble Kalman Filtering and Coupled Fusion Simulation – to describe a conceptual framework for ACS, consisting of mechanisms, strategies and objectives, and demonstrate how these concepts can be used to more effectively realize pre-defined application objectives."
pub.1049389045,Multi-tenancy Performance Benchmark for Web Application Platforms,"Cloud environments reduce data center operating costs through resource sharing and economies of scale. Infrastructure-as-a-Service is one example that leverages virtualization to share infrastructure resources. However, virtualization is often insufficient to provide Software-as-a-Service applications due to the need to replicate the operating system, middleware and application components for each customer. To overcome this problem, multi-tenancy has emerged as an architectural style that allows to share a single Web application instance among multiple independent customers, thereby significantly improving the efficiency of Software-as-a-Service offerings. A number of platforms are available today that support the development and hosting of multi-tenant applications by encapsulating multi-tenancy specific functionality. Although a lack of performance guarantees is one of the major obstacles to the adoption of cloud computing, in general, and multi-tenant applications, in particular, these kinds of applications and platforms have so far not been in the focus of the performance and benchmarking community. In this paper, we present an extended version of an existing and widely accepted application benchmark adding support for multi-tenant platform features. The benchmark is focused on evaluating the maximum throughput and the amount of tenants that can be served by a platform. We present a case study comparing virtualization and multi-tenancy. The results demonstrate the practical usability of the proposed benchmark in evaluating multi-tenant platforms and gives insights that help to decide for one sharing approach."
pub.1121648646,An Architecture Model for a Distributed Virtualization System,"The Thesis is about an architecture model for a Distributed Virtualization System, which could expand a virtual execution environment from a single physical machine to several nodes of a cluster. With current virtualization technologies, computing power and resource usage of Virtual Machines (or Containers) are limited to the physical machine where they run. To deliver high levels of performance and scalability, cloud applications are usually partitioned in several Virtual Machines (or Containers) located on different nodes of a virtualization cluster. Developers often use that processing model because the same instance of the operating system is not available on each node where their components run. The proposed architecture model is suitable for new trends in software development because it is inherently distributed. It combines and integrates Virtualization and Distributed Operating Systems technologies with the benefits of both worlds, providing the same isolated instance of a Virtual Operating System on each cluster node. Although it requires the introduction of changes in existing operating systems, thousands of legacy applications would not require modifications to obtain their benefits. A Distributed Virtualization System is suitable to deliver high-performance cloud services with provider-class features, such as high-availability, replication, migration, and load balancing. Furthermore, it is able to concurrently run several isolated instances of different guest Virtual Operating Systems, allocating a subset of nodes for each instance and sharing nodes between them. Currently, a prototype is running on a cluster of commodity hardware provided with two kinds of Virtual Operating Systems tailored for internet services (web server) as a proof of concept."
pub.1171428156,A software-defined connectivity service for multi-cluster cloud native applications,"Containerization technologies have risen in popularity for deploying microservices applications in cloud-native environments, offering the benefits of traditional virtualization with reduced overhead. However, existing container networking solutions lack support for applications requiring isolated link-layer communications among containers in different clusters. These communications are fundamental to enable the seamless integration of cloud-native solutions in 5G and beyond networks. Accordingly, we present an SDN-enabled networking solution that supports the creation of isolated link-layer virtual networks between containers across different Kubernetes clusters by building virtual circuits that dynamically adapt to changes in the topology. In this article, we introduce our solution, highlighting its advantages over existing alternatives, and provide a comprehensive design overview. Additionally, we validate it through an experiment, offering a deeper understanding of its functionality. Our work fills an existing gap for applications with inter-cluster link-layer networking access requirements in the cloud-native ecosystem."
pub.1112924560,"Fog computing with the integration of Internet of things: Architecture, Applications and Future Directions","Information Technology industry has competitiveness on the basis of technological environment. In this environment, the use of cloud services has been increasing to provide high quality services and fast delivery of products to cloud users. But still some issues are unresolved especially, related to latency between cloud data center and end user. Fog computing is used to support increasing demand of IT service with the collaboration of cloud computing. It provides computational and storage services of cloud proximate to IoT devices. Fog computing is enhancement of the cloud-based network and computing services. This paper discusses the concept, architecture of fog computing and implemented application. It also highlights about resource provisioning techniques to identify over utilization of fog nodes. Along with the resource utilization, different scheduling terminologies have also been discussed on various parameters. The motive of this survey is to understand the application of fog computing to improve the existing smart healthcare system."
pub.1105045260,Implementing an OPC UA Interface for Legacy PLC-Based Automation Systems Using the Azure Cloud: An ICPS-Architecture with a Retrofitted RFID System,"Companies often have legacy systems within production processes. Many of these systems use proprietary information-communication-control-technologies (ICCT), among others, and for that reason it is difficult to integrate them into new industrial environments that require Industrie 4.0-compliances. The migration process of such industrial systems, i.e., retrofitting it into an Industrie 4.0-compliant, ICPS-Infrastructure, leads to the necessity of new features and components in the legacy architecture. This paper summarizes the process for retrofitting a flexible manufacturing system located at the Institute for Industrial Informatics, Automation and Robotics (I2AR) of the University of Applied Sciences Emden/Leer, integrating legacy PLCs, RFID and commercial cloud technology on the basis of an Industry 4.0-compliant digitalization of components with OPC UA interfaces. With the use of common technologies recommended for migrating legacy industrial systems into Industrie 4.0-compliant systems, the authors describe the major specifications and characteristics of a migration approach and resulted novel ICPS-architecture that allows the operator and/or other legacy automation and management systems to monitor and control the PLC component via a RAMI4.0-compliant digital interface, via a retrofitted RFID System."
pub.1160739057,Towards Characterization of Edge-Cloud Continuum,"Internet of Things and cloud computing are two technological paradigms that reached widespread adoption in recent years. These paradigms are complementary: IoT applications often rely on the computational resources of the cloud to process the data generated by IoT devices. The highly distributed nature of IoT applications and the giant amounts of data involved led to significant parts of computation being moved from the centralized cloud to the edge of the network. This gave rise to new hybrid paradigms, such as edge-cloud computing and fog computing. Recent advances in IoT hardware, combined with the continued increase in complexity and variability of the edge-cloud environment, led to an emergence of a new vision of edge-cloud continuum: the next step of integration between the IoT and the cloud, where software components can seamlessly move between the levels of computational hierarchy. However, as this concept is very new, there is still no established view of what exactly it entails. Several views on the future edge-cloud continuum have been proposed, each with its own set of requirements and expected characteristics. In order to move the discussion of this concept forward, these views need to be put into a coherent picture. In this paper, we provide a review and generalization of the existing literature on edge-cloud continuum, point out its expected features, and discuss the challenges that need to be addressed in order to bring about this envisioned environment for the next generation of smart distributed applications."
pub.1093953602,SaaS Application Framework using Information Gateway Enabling Cloud Service with Data Confidentiality,"Along with the rapid evolution of cloud computing, outsourcing service has also changed significantly. One of the main concerns for cloud computing is security. In particular, it becomes more important to handle client data securely in such a consulting service. In this paper, we present the design and implementation of Software as a Service (SaaS) application framework using Information Gateway that enables cloud service while maintaining data confidentiality. By setting up Information Gateway in the client environment, the executing location is dynamically controlled according to whether the data contains confidential information or not, and only secured data is routed to the SaaS application in the cloud. Therefore, if the data policy is registered by the client administrator beforehand, the user is able to use the cloud service appropriately without being aware of it. Moreover, we also describe that existing applications can be easily ported into the SaaS application because our framework allows application developers to define complicated routing logic briefly."
pub.1140201646,Comparative Analysis: Intrusion Detection in Multi-Cloud Environment to Identify Way Forward,"Cloud computing is the emerging platform that is covering individual and corporate needs swiftly. The spread of this global platform is ranging from infrastructure to various middleware, front-end and back- end services. At corporate level, another effective configuration of this phenomenon is multi-cloud environment, which is depicting the ultimate control of the end-user on engaging services from various cloud service providers depending on the service ranking, cost and availability. It is therefore, now very much desirable to have infrastructure services from one service provider while data services are performed on another cloud or having infrastructure services in a distributed environment on multiple clouds. Multi-cloud environment is closely linked with smartly configured security mechanism to ensure the security at rest and in transit. Intrusion detection at various levels and services of cloud platform is not an easy task and when it is spread over multiple clouds then the challenge becomes more complex and tedious. On the other side, managing and integrating a multi-cloud computing environment is also highly complex. From technical point of view, it requires experience and hi-tech skills to formulate sustainable integration between multiple clouds and a coherence among various services to provide an encapsulated platform for the end-user. As in a multicolor environment, the integration can be focused on Infrastructure-as-a-Service (IaaS), Software-as-a-Service (SaaS) and Platform-as-a-Service (PaaS) from various cloud service providers therefore an API-consistent cloud environment is required which leads to the security and more specifically intrusion detection. The problem arises when most of the existing network based intrusion detection systems are designed to deal with the known threats and attacks. These systems are dependent on a rule base that is sufficient to work in certain environment but in case of multi-cloud integration, such fixed rule bases and known-resilience becomes a point of concern. It is therefore, required to look at the intrusion detection system, which may adapt the environmental changes as well as can at least indicate the unknown / anomaly attacks or detection. Honeypot is a vibrant mechanism to divert attention of the unknown attackers and able to capture data to analyze the anomaly. Honeypots may not be so useful independently but along with an intrusion detection system; this mechanism works efficiently and provides tangible results. This research paper is focused on analyzing the multi-cloud environment, intrusion detection systems and the use of honeypots in the existing solutions to understand the possible configurations for effective results in making a sustainable, secure and scalable multi-cloud environment."
pub.1095063550,"Simulation, Modeling and Performance Evaluation Tools for Cloud Applications","As cloud computing adoption and deployment increase, the performance evaluation of the cloud environments is becoming very important. Cloud applications have different composition, configuration, and deployment requirements. Simulation and modeling techniques are suitable to quantify the performance of resource allocation policies and application scheduling algorithms in Cloud computing environments for different application and service models according to different work loads, energy performance and system size. In this paper, we give an overview of the existing distributed systems simulation and modeling tools in order to outline the main characteristics and peculiarities. We then present an outlook on new requirements to be addressed for performance evaluation of cloud applications through simulation and modeling."
pub.1165708797,Method of creation and deployment of FPGA projects resistant to change of requirements and development environments for cloud infrastructures,"The subject of study in this article is the modern technologies of programmable logic devices, the history of product changes of leading manufacturers, including development environments, and project optimization processes. The goal is to improve modern methods, technologies, and software tools for the development and integration of FPGA-as-a-Service in the form of services in cloud infrastructures, data centers, and on-board high-performance systems, with taking into account the ever-changing conditions and requirements, and the constraints of platforms. Task : to analyze the history of product changes of the leading vendors and manufacturers of programmable logic devices; analyze the dynamics of changes in the functionality of project development environments for FPGA-based systems, with taking into account the existing requirements and restrictions from vendors of software and hardware platforms and components; perform a comparative analysis of modern development boards and accelerator cards for the prototyping and testing of projects based on the chips of programmable logic; analyze the use of an existing services and solutions based on FPGA technologies as part of cloud services from modern cloud providers; propose practical steps for the development of systems based on FPGA resistant to project requirements change; propose the sequence for the development and optimization of high-performance systems with their implementation based on FPGA; and to provide practical example of the use of the proposed method. According to the tasks, the following results were obtained. The history of changes in the leading companies of programmable logic manufacturers, as well as changes in versions of the development environments and the products of one of the largest companies, is analyzed. The use of existing FPGA-based solutions as part of cloud services is analyzed in detail. Two sequences for optimizing projects and increasing their productivity with reducing of the prototyping efforts during their creation and porting to new versions of software and hardware platforms are provided. Based on the results of the research, a prototype was developed and tested, which allowed the application of the proposed method in practice for adapting and porting the FPGA as a service project during the transfer to another version of the accelerator card. Conclusions. The main contribution and scientific novelty of the obtained results is that an experimental study of the paradigm of runtime reprogramming of programmable logic was performed, which made it possible to formulate the elements of a new method of creation projects as a service for cloud infrastructures, data centers, and artificial intelligence systems. A set of practical steps for the development of systems that are tolerant to changes in conditions and requirements is proposed. The application of the proposed method allows to avoid costs of project support in the case of changes in requirements."
pub.1006030557,Cloud Erp: A New Dilemma to Modern Organisations?,"For almost two decades, on-premise ERP has been adopted very prevalently in the industry. Accompanied with the emergence of cloud computing technologies in the late 2000s, there is an increasing trend for companies to migrate their hitherto internal ERP applications and databases into the cloud. Such ERP resources re-migration, which can offer a range of opportunities to user companies, is also associated with new challenges. The study reported in this paper aimed to explore potential benefits and barriers associated with the adoption of cloud ERPs. A set of in-depth interviews were conducted with 16 ERP and cloud consultants. The findings, derived from a thematic analysis, identified that whilst the economic and technical benefits promised by cloud vendors are attractive, the success of cloud ERP adoption can be affected by critical challenges related to diverse organisational factors as well as with current legal and technical complexity in the cloud environment."
pub.1175958358,Optimizing Machine Learning Models for Predictive Analytics in Cloud Environments,"The integration of machine learning (ML) models with cloud computing has transformed the landscape of predictive analytics, offering scalable, efficient, and flexible solutions for organizations. Cloud platforms such as AWS, Google Cloud, and Microsoft Azure enable businesses to deploy and manage complex ML models without the need for extensive on-premise infrastructure. However, optimizing these ML models for performance and cost-efficiency in cloud environments presents unique challenges, including resource management, latency, scalability, and data security.
 This paper focuses on strategies to optimize machine learning models specifically for predictive analytics in cloud environments. It explores key techniques such as auto-scaling, model compression, and hyperparameter tuning, which are critical for improving the accuracy and speed of predictions while minimizing computational costs. The research also examines advanced tools such as containerization, serverless computing, and cloud-native services that further streamline the deployment and management of ML models.
 In the Indian context, where cloud adoption is growing rapidly, optimizing ML models is crucial for businesses across various sectors, including finance, healthcare, and e-commerce. By leveraging cloud-based ML solutions, Indian companies can enhance their predictive analytics capabilities, driving smarter decision-making and operational efficiency.
 This abstract presents an overview of how optimized machine learning models can unlock the full potential of predictive analytics in cloud environments, leading to better business outcomes. Through case studies and practical applications, this paper provides actionable insights into the best practices for optimizing ML models in a cloud-based setting."
pub.1095310273,SRL: A Scalability Rule Language for Multi-Cloud Environments,"The benefits of cloud computing have led to a proliferation of infrastructures and platforms covering the provisioning and deployment requirements of many cloud-based applications. However, the requirements of an application may change during its life cycle. Therefore, its provisioning and deployment should be adapted so that the application can deliver its target quality of service throughout its entire life cycle. Existing solutions typically support only simple adaptation scenarios, whereby scalability rules map conditions on fixed metrics to a single scaling action targeting a single cloud environment (e.g., scale out an application component). However, these solutions fail to support complex adaptation scenarios, whereby scalability rules could map conditions on custom metrics to multiple scaling actions targeting multi-cloud environments. In this paper, we propose the Scalability Rule Language (SRL), a language for specifying scalability rules that support such complex adaptation scenarios of multi-cloud applications. SRL provides Eclipse-based tool support, thus allowing modellers not only to specify scalability rules but also to syntactically and semantically validate them. Moreover, SRL is well integrated with the Cloud Modelling Language (CloudML), thus allowing modellers to associate their scalability rules with the components and virtual machines of provisioning and deployment models."
pub.1145987070,Intrinsic Security: A Robust Framework for Cloud-Native Network Slicing via a Proactive Defense Paradigm,"Opening-up sharing has prompted the multi-tenancy architecture, whereby different vendors (including outsourcees) work together with network operators to form a vibrant service ecosystem, resulting in several advantages as well as risks. In particular, the static nature of existing architectures in network functions virtualization-based (NFV-based) clouds facilitate hacking. Thus, much attention has been focused on determining how to avoid the uncontrollable cloud security induced by complex production relations and non-trustworthy software/hardware sources when the two sets of security risks intersect. In this article, we investigate latent persistent threats against cloud environments and determine a high degree of complementarity and consistency between the NFV-based cloud environment and the dynamic defense concept. More specifically, new NFV-based cloud features provide an effective implementation for dynamic defense, while the generalized robustness of dynamic defense theory allows for high security gains. Intrinsic cloud security (iCS) is then proposed to align NFV-based clouds, mimicking defense and the moving target defense (MTD) paradigm to implement a seamless integration and symbiosis evolution between security and NFV-based clouds. We quantify the impact on system overhead to account for efficiency and cost issues. The simulation analysis demonstrates that the enhanced mode is able to consistently obtain a more beneficial and stable defense compared with the counterparts."
pub.1032339442,Automatic elasticity in OpenStack,"Cloud computing infrastructures are the most recent approach to the development and conception of computational systems. Cloud infrastructures are complex environments with various subsystems, each one with their own challenges. Cloud systems should be able to provide the following fundamental property: elasticity. Elasticity is the ability to automatically add and remove instances according to the needs of the system. This is a requirement for pay-per-use billing models. Various open source software solutions allow companies and institutions to build their own Cloud infrastructure. However, in most of these, the elasticity feature is quite immature. Monitoring and timely adapting the active resources of a Cloud computing infrastructure is key to provide the elasticity required by diverse, multi-tenant and pay-per-use business models. In this paper, we propose Elastack, an automated monitoring and adaptive system, generic enough to be applied to existing IaaS frameworks, and intended to enable the elasticity they currently lack. Our approach offers any Cloud infrastructure the mechanisms to implement automated monitoring and adaptation as well as the flexibility to go beyond these. We evaluate Elastack by integrating it with the OpenStack showing how easy it is to add these important features with a minimum, almost imperceptible, amount of modifications to the default installation."
pub.1048664784,A New Performance Modelling and Measurement Method for Service-Oriented Cloud Applications (SOCAs),"Cloud computing has been at the centre of intense discussion particularly for its intrinsic power of bringing out a steady flow of innovations on both the business and the IT domains. Debates on performance, security and reliability form the centre stage of most of these deliberations. Third-party public clouds hosting and delivering scores of social, collaborative, and business acceleration applications to global users are also gaining momentum. On the other hand, the on-premise web, application and database servers are being still sustained due to the lack of unbreakable and impenetrable security capabilities in public cloud infrastructures.The brewing trend is that a wider variety of legacy, web, enterprise, consumer, social, telecom and embedded services are being deployed and delivered from clouds to global users with all the quality of service (QoS) attributes. That is, next-generation applications are derived out of these distributed and decentralised services. In other words, service oriented cloud applications will become popular and pervasive. In this paper, we have focused on the performance concerns and challenges of SOCAs. We have come out with a few formulae for service utilization, availability and performance in a cloud environment."
pub.1173734338,Secure and Scalable Architectures for Cloud Computing,"Cloud computing has emerged as a cornerstone of modern computing infrastructure, offering unparalleled scalability and flexibility for diverse applications. However, the widespread adoption of cloud services has also raised significant concerns regarding security and scalability. As organizations increasingly rely on cloud environments to store and process sensitive data, ensuring robust security measures while maintaining scalability becomes paramount. This paper presents a comprehensive examination of secure and scalable architectures for cloud computing, addressing the challenges and proposing innovative solutions to enhance the resilience and efficiency of cloud infrastructures. The first section of the paper delves into the fundamental principles of cloud computing, outlining its key characteristics and architectural components. By understanding the underlying architecture of cloud systems, stakeholders can better appreciate the intricate interplay between security and scalability. Subsequently, the paper elucidates the multifaceted security threats facing cloud environments, ranging from data breaches and malicious attacks to insider threats and compliance issues. These challenges underscore the critical importance of implementing robust security mechanisms to safeguard sensitive data and infrastructure resources. In response to these challenges, the paper presents a systematic analysis of existing security mechanisms and architectural paradigms designed to fortify cloud environments. This includes encryption techniques, access control mechanisms, authentication protocols, and intrusion detection systems, among others. Furthermore, the paper explores the concept of defense-in-depth strategies, emphasizing the importance of layering security measures to mitigate the impact of potential breaches and vulnerabilities. In tandem with security considerations, the paper examines strategies for achieving scalability in cloud architectures. Scalability is a defining characteristic of cloud computing, enabling on-demand resource allocation and elastic scaling to accommodate fluctuating workloads. However, achieving scalability while ensuring security presents unique challenges, particularly in multi-tenant environments where resources are shared among disparate users. To address these challenges, the paper explores architectural frameworks such as microservices, containerization, and serverless computing, which offer inherent scalability benefits while facilitating robust isolation and security. Moreover, the paper investigates the role of automation and orchestration in optimizing cloud scalability and security. Through the use of DevOps practices and infrastructure- as-code tools, organizations can streamline deployment processes, enforce security policies, and dynamically scale resources in response to evolving demands. Additionally, the paper examines emerging technologies such as edge computing and federated architectures, which extend the scala"
pub.1107111174,Cracking the monolith,"Application modernization is the process of transforming a monolithic application to cloud native. This involves gradually building a new application consisting of microservices, and running it in conjunction with the monolithic application. Over a period of time, the functionality of the monolith shrinks drastically or transforms into yet another microservice. Solution architects are often faced with the task of ensuring this smooth transition - from monolith to cloud native. For large and complex monoliths, this task is non-trivial as the code base grows non-linearly over a period of time, thus posing multiple challenges. The complexity of a monolith is moved to the interconnections between microservices, leading to multiple points of failure. This also has implications on scalability and the need for replication. One of the biggest challenges is to maintain data consistency and statefulness across the services and enable a smooth transition of the data. Tracing performance issues also becomes complex as a single transaction can encompass multiple service calls, along with increased operational complexity due to increased demand of managing services. In this paper, we envision an automated approach that will enable a smooth transition from the monolith to microservices, thus alleviating the complexities faced by a solution architect. Our system leverages the existing data schema along with details obtained using profiling tools (in production or development environment), to understand the data flow and access patterns and use this information to to propose functional modules (microservices)."
pub.1149020365,The Education Cloud Platform for Digital Resources with Block Chain under Intelligent Learning Environment,"The education cloud platform has developed the education cloud standard specification system, which has laid the foundation for the establishment of the cloud public service system. It has developed the education cloud basic support, conquered the relevant key technologies of education cloud services, modeled the education cloud green security service guarantee, and built a safe and reliable education cloud public service. Infrastructure is the foundation of the environment ; without infrastructure, there is no ecological soil for growth. The most difficult thing about the combination of education and technology is that the integration itself is an integration of value orientations. The overall structure is based on the structure of the blockchain, which is designed according to the security needs of digital educational resources. Under the premise of solving the problem, it can design and implement the operation of forming a chain of data blocks, so as to solve the problem of data asynchrony."
pub.1168311884,Detection of DDOS Attacks in Cloud Environment Using Deep Learning,"The rapid evolution of cloud computing and the increasing reliance on Software -Defined Networking (SDN) have introduced new challenges in ensuring the security and availability of cloud based services. Distributed Denial of Service (DDoS) attacks, in particular, pose a significant threat to the stability and performance of cloud infrastructures. As network traffic patterns continue to increase and evolve, it is crucial to extract pertinent features that aid in identifying attacks and improving the accuracy of detection. Therefore, effective detection mechanisms are crucial to identify and mitigate DDoS attacks in SDN-based cloud environments. This study presents a novel approach that combines Decision Tree with Long Short-Term Memory (LSTM) for effective DDoS detection in cloud infrastructures. The model is evaluated using the 2019 dataset. The simulation findings validateits efficacy and show that it improves upon existing works in terms of accuracy."
pub.1142657309,Temporal-Perturbation aware Reliability Sensitivity Measurement for Adaptive Cloud Service Selection,"Benefiting from the pay-as-you-go business model, cloud computing has significantly promoted service computing techniques in real-world industrial applications. Software applications based on cloud computing are becoming more and more popular. By integrating existing component cloud services through the internet, composite cloud systems can be built to meet sophisticated application logic. Stable execution of such systems is desirable in the long term so that the service-level agreements (SLAs), as well as users’ quality of experience (QoE), can be fulfilled. To achieve this goal, it is critical to identify and fault-tolerate system components at high risks of failing. This is extremely challenging due to the dynamic and uncertainty of the cloud environment that hosts the component cloud services. Nevertheless, existing approaches pay little attention to the modeling and analysis of system components’ reliability time series. To address the above issues, we first present a reliability evaluation method for component cloud services based on the reliability model and their failure probability under continuous client-side invocation tests. Then, we propose a perturbation-aware reliability sensitivity measurement approach (named PARS) for measuring the reliability sensitivity of component cloud services. It first analyzes the negative perturbations in component cloud services’ historical reliability time series based on the Markov chain rule. Then, it calculates the reliability sensitivity of component cloud services by analyzing how their reliability perturbations impact the reliability of the entire cloud system. To guarantee the execution quality of the composite cloud system, we further propose a proactive adaptation approach named PA-PARS that enables 1-out-of-2 N-version Programming fault-tolerance for composite cloud systems based on PARS. PA-PARS takes the reliability sensitivity of component cloud services estimated by PARS as input to assure the reliability of the cloud system. It consists of four parts: 1) risky system component identification; 2) adaptation trigger; 3) candidate component cloud service selection; and 4) NVP-based system construction as the proactive adaptation for the composite cloud system. The results of experiments conducted on two widely-used datasets demonstrate the effectiveness and efficiency of the proposed approaches in ensuring the reliability of composite cloud systems."
pub.1095023805,Integration of Several University E-Services in the Cloud,"Service integration in Cloud computing reusing existing services nowadays presents a very important research problem. It is presented during different software's and application implementations and the idea comes for something new, innovative, and contemporary, enabling several services to be integrated and by providing stable, decentralized and just in time services. In SEE University there are different web application platforms that serve to offer services to the students but not integrated, they are isolated in psychical and business process aspects. The aim of this paper is to propose a modeling platform (framework) in SEEU, which is tailored to build efficient, elastic and autonomous applications from tasks and services provided by the Cloud environment, and to define patterns that can result in the efficient optimization of money and resources."
pub.1137480544,Opportunities and Challenges of Data Migration in Cloud,"Cloud data migration is the process of moving data, localhost applications, services, and data to the distributed cloud processing framework. The success of this data migration measure is relying upon a few viewpoints like planning and impact analysis of existing enterprise systems. Quite possibly the most widely recognized process is moving locally stored data in a public cloud computing environment. Cloud migration comes along with both challenges and advantages, so there are different academic research and technical applications on data migration to the cloud that will be discussed throughout this paper. By breaking down the research achievement and application status, we divide the existing migration techniques into three strategies as indicated by the cloud service models essentially. Various processes should be considered for different migration techniques, and various tasks will be included accordingly. The similarities and differences between the migration strategies are examined, and the challenges and future work about data migration to the cloud are proposed. This paper, through a research survey, recognizes the key benefits and challenges of migrating data into the cloud. There are different cloud migration procedures and models recommended to assess the presentation, identifying security requirements, choosing a cloud provider, calculating the expense, and making any essential organizational changes. The results of this research paper can give a roadmap for data migration and can help decision-makers towards a secure and productive migration to a cloud computing environment."
pub.1168691372,Securing Large Healthcare Files in Cloud-Blockchain Integration: A Comprehensive Review,"The management of large medical files is a critical challenge in the health sector. Conventional systems have security, scalability, and efficiency deficiencies. This project proposes a hybrid architecture between blockchain and cloud computing to address these challenges. Blockchain ensures the immutability and traceability of medical records, while the cloud allows scalable and efficient storage. Together, these technologies can transform data management in electronic health record applications. The essential objective of this research is to thoroughly look at the existing strategies, devices, and conventions that encourage a secure interaction between blockchain applications and the cloud. Special emphasis is set on maintaining the integrity and security of the blockchain while tackling the potential and efficiency of cloud infrastructures. Through a meticulous investigation of these approaches, along with their successful usage and encountered challenges, this paper offers a comprehensive overview of the present state of this integration. Altogether, this paper does not propose new concepts or developments. Its central focus is to provide a comprehensive and insightful examination of the modern landscape concerning the integration of blockchain and cloud advances. By shedding light on the existing approaches and assessing their viability, this work aims to highlight the current challenges and build up a solid establishment for future development. This, in turn, will cultivate a consistent integration of blockchain security with the dynamic potential of cloud computing while guaranteeing information integrity and security remain uncompromised. In conclusion, this paper serves as a important resource for analysts, specialists, and partners looking for to dig into and development the integration of blockchain and cloud innovations, subsequently contributing to a more secure and productive cloud environment."
pub.1181139773,Risk Management Framework for Cloud Migration and Selection of Suitable Cloud Service Provider,"Businesses frequently manage the complicated challenge of transitioning from on-premises systems to the cloud in the evolving world of technology. An integrated risk framework is critical for analyzing and auditing this transformation, ensuring a smooth transfer while limiting potential risks. This methodology provides proactive risk management by predicting issues such as compliance gaps, and service outages. In addition, this framework includes a thorough selection procedure for the best cloud service providers (CSPs). Organizations may find the best partner for their specific needs by examining criteria like security standards, compliance adherence, scalability, and data governance. This complete strategy not only reduces possible interruptions during migration but also fortifies the company against unanticipated obstacles, promoting a safe and efficient cloud environment, and educated decision making. As the technology landscape is changing exponentially, it is important to integrate risk framework as a strategic compass in intricacy of migration to reinforce digital infrastructure."
pub.1028286744,Towards exploiting the full adaptation potential of cloud applications,"Current technology for cloud application adaptation fails to capture two fundamental aspect of cloud environments: multiple adaptation options and interferences and dependencies among these multiple mechanisms. Addressing these aspects requires a significant extension of existing cloud tools and frameworks for engineering and executing cloud application adaptations. They should explicitly take into account: all entities of the cloud environment relevant for adaptation decisions; the concrete adaptation actions that these cloud entities may perform; and the mutual dependencies between those entities and actions. In this paper we provide the first insights towards such novel technology. As main contribution, we systematically elicit the key entities related to adaptations inside a cloud environment and explicitly document those in a conceptual model. To build this model we surveyed the literature, discussed with industrial partners with experience in cloud computing, and analyzed commercial solutions. We also provide a case study based on Amazon Web Services solutions, to show how our conceptual model can be instantiated and help developers to identify possible cloud application adaptation strategies."
pub.1135086152,Ananke : A framework for Cloud-Native Applications smart orchestration,"Micro-service architecture enables smarter management of applications life-cycle. However, the increasing of the number of components also increases complexity, especially on operations like migration and horizontal scaling. While operations, in monolithic systems, involve only one component, operations in micro-services based applications can get complex and should involve parameters and properties like connection throughput, resources usage, robustness or consistency and reliability. To perform this kind of operations and optimization strategies in micro-services applications, we propose Ananke , a framework consisting of a time-varying multi-layer graph-based model and architecture to profile micro-services and their interactions in a platform-as-a-service environment. The aim of Ananke is to provide support and facilities for optimization strategies that a Cloud Provider can exploit to guarantee quality of service and service-level agreements better."
pub.1113330251,Scalable Software Infrastructure for Integrating Supercomputing with Volunteer Computing and Cloud Computing,"Volunteer Computing (VC) is a computing model that uses donated computing cycles on the devices such as laptops, desktops, and tablets to do scientific computing. BOINC is the most popular software framework for VC and it helps in connecting the projects needing computing cycles with the volunteers interested in donating the computing cycles on their resources. It has already enabled projects with high societal impact to harness several PetaFLOPs of donated computing cycles. Given its potential in elastically augmenting the capacity of existing supercomputing resources for running High-Throughput Computing (HTC) jobs, we have extended the BOINC software infrastructure and have made it amenable for integration with the supercomputing and cloud computing environments. We have named the extension of the BOINC software infrastructure as BOINC@TACC, and are using it to route *qualified* HTC jobs from the supercomputers at the Texas Advanced Computing Center (TACC) to not only the typically volunteered devices but also to the cloud computing resources such as Jetstream and Chameleon. BOINC@TACC can be extremely useful for those researchers/scholars who are running low on allocations of compute-cycles on the supercomputers, or are interested in reducing the turnaround time of their HTC jobs when the supercomputers are over-subscribed. We have also developed a web-application for TACC users so that, through the convenience of their web-browser, they can submit their HTC jobs for running on the resources volunteered by the community. An overview of the BOINC@TACC project is presented in this paper. The BOINC@TACC software infrastructure is open-source and can be easily adapted for use by other supercomputing centers that are interested in building their volunteer community and connecting them with the researchers needing multi-petascale (and even exascale) computing power for their HTC jobs."
pub.1007090479,Cross-ISA Container Migration,"Containers are a convenient way of encapsulating and isolating applications. They incur less overhead than virtual machines and provide more flexibility and versatility to improve server utilization. Many new cloud applications are being written in the microservices style to take advantage of container technologies. Each component of the application can be encapsulated in a separate container, which enables the use of other features such as auto-scaling. However, legacy applications can also benefit from containers which provide more efficient development and deployment models. In modern data centers, orchestration middle-ware is responsible for container placement, SLA enforcement and resource management. The orchestration software can implement various policies for managing the resources. The orchestration software can take corrective actions when detecting inefficiencies in the data center operation to satisfy the current policy. Power efficiency is becoming one of the most important characteristics taken into account when designing a data center and defining policy for the orchestration middleware [4]. Different server architectures have different power efficiency and energy proportionality characteristics. Recent research has shown that heterogeneous systems have the potential to significantly improve energy efficiency[3, 5]. Our work focuses on the mechanism required by the middle-ware to implement a power optimization policy. We research migration of containerized applications between servers inside a heterogeneous data center, for the purpose of optimizing power efficiency. Migrating a running container between different architectures relies on the compatibility of the application environment on the source and destination servers. Containers are viewed as a set of one or more processes and each process must have the ability to be migrated. A modified compiler is used to build executables in a manner allowing the program migration between different architectures. The source and destination servers must also have a shared file system and comparable networking capabilities. We take advantage of the recently added user-space page fault feature in the Linux kernel [2] to implement post-copy container migration in CRIU [1]. Post-copy migration significantly reduces perceived down-time of the container, and can potentially reduce network traffic as well. We propose creating a cluster of servers with different architectures (i.e., ARM, POWER, and x86) connected with a high-speed, low-latency network. This cluster will run SaaS applications in a containerized environment. The applications will be built using a specialized toolchain that ensures an identical memory layout across all architectures, enabling seamless migration at runtime. The majority of the challenges in cross-ISA migration are related to the toolchain adaptation, and ensuring the compatibility of the runtime environment across various servers in the cluster. The ability to efficien"
pub.1126800044,A REVIEW STUDY ON THE ADOPTION OF CLOUD COMPUTING FOR HIGHER EDUCATION IN KURDISTAN REGION- IRAQ,"Cloud computing is considering as a popular computing model in the Western World. It is still not well understood by much higher education (HE) institutions in the developing world. CC will have a huge effect on the academic and learning environment which will enable their users (i.e., students, educators, and heads) to play out their undertakings adequately with less expense by using the accessible applications based on the clouding offered by the cloud specialist organizations. This study aims to evaluate the factors that influence the adoption of CC for Higher Education within the Kurdistan Region in Iraq. The study was performed utilizing a non-experimental study exploratory research design. This exploratory study included an essential investigation into secondary data. The study development and modeling of secondary data to highlight the final results of the research. Through reviewing the literature of the existing frameworks in CC adoption, it is showed that there are limited institutions developed over the latest years. Moreover, HE in Kurdistan Region needs continued attention to get government support and redesign the educational system to cover all the core aspects in a better way. Here, at any time there is a need to access the applications, software and hardware, platform, and infrastructure; the most required is to have the internet service."
pub.1127202242,A REVIEW STUDY ON THE ADOPTION OF CLOUD COMPUTING FOR HIGHER EDUCATION IN KURDISTAN REGION- IRAQ,"Cloud computing is considering as a popular computing model in the Western World. It is still not well understood by much higher education (HE) institutions in the developing world. CC will have a huge effect on the academic and learning environment which will enable their users (i.e., students, educators, and heads) to play out their undertakings adequately with less expense by using the accessible applications based on the clouding offered by the cloud specialist organizations. This study aims to evaluate the factors that influence the adoption of CC for Higher Education within the Kurdistan Region in Iraq. The study was performed utilizing a non-experimental study exploratory research design. This exploratory study included an essential investigation into secondary data. The study development and modeling of secondary data to highlight the final results of the research. Through reviewing the literature of the existing frameworks in CC adoption, it is showed that there are limited institutions developed over the latest years. Moreover, HE in Kurdistan Region needs continued attention to get government support and redesign the educational system to cover all the core aspects in a better way. Here, at any time there is a need to access the applications, software and hardware, platform, and infrastructure; the most required is to have the internet service."
pub.1124782182,Survey of Hybrid VANET Design for Provisioning Infotainment Application,"This paper presents a survey of various existing VANET model for provisioning real-time (i.e. safety critical application) and non-real smart infotainment application for user on the go. Further, it carries out survey of various existing hybrid VANET for provisioning QoS (Quality of services), increase network density using SDN (software defined network), fog computing and cloud computing framework, and future generation high speed wireless network such as LTE (Long term evolution), 5G (Fifth generation), CRAN (Cloud based Radio area network). However, from extensive analysis carried out it can be seen existing hybrid model do not consider feature of each framework/architecture such as VANET communication (i.e. among V2I and V2V), QoS controlling mechanism in SDN for provisioning data from cloud to VANET user and also do not provide efficient resource allocation technique in cloud computing environment to minimize computation cost. This paper shows meeting aforementioned feature in designing hybrid VANET model will aid in offering scalable and flexible smart infotainment application to VANET user. Further, the survey show how propagation model is affected due to environmental factor and presence of obstacle in LOS. Further, discusses about the need for new MAC (Medium access model). Then, discusses about selecting simulation environment/tool to evaluate performance of Hybrid VANET model. Lastly, a novel possible solution is described to overcome research problem which will be the future research direction of this survey. The proposed envisioned model will aid in offering superior performance than existing models."
pub.1140666852,Development of Feedback-Based Trust Evaluation Scheme to Ensure the Quality of Cloud Computing Services,"The expeditious rise in cloud computing facilitates economical data storage, expertise software, and high-speed and scalable computing resources through infrastructures, platforms, and software (as a service). However, due to the openness and highly non-transparent nature of cloud computing, trust is the pressing issue that may hamper the adoption and growth of cloud services. Moreover, conventional trust management solutions are inadequate and cannot be directly adopted in cloud computing environments. Many existing techniques have addressed the above issue, and a comprehensive study of the literature helped in formulating the current research objectives. This research intends to develop an effective feedback-based trust evaluation scheme to ensure the quality of cloud computing services. The proposed scheme finds an accurate global trust value of the cloud services based on the aggregation of the genuine feedbacks trust, reputation trust, and Quality of Service (QoS) trust. Suitable performance analysis is done using Google cloud trace logs. The results show that the feedback-based trustworthy monitoring system is very much desirable and solely needed for reliable cloud computing service provisioning."
pub.1023896814,Ambient Middleware for Context-Awareness (AMiCA),"<p>Recent advances in wireless networking technologies and the growing success of mobile computing devices are enabling new classes of applications. Distributed applications running in a mobile environment are often subject to varying qualities of service from the underlying infrastructure. The objective of the work outlined in this research is the development of an Ambient Middleware framework for Context-awareness which will offer new opportunities for application developers. Context-awareness has been a central issue in Ambient Intelligent research for the last decade. Ambient Intelligent systems and applications represent extremely complex and heterogeneous distributed systems, composed of hardware and software, and the need for middleware for seamless integration is well recognised. Research shows that existing middleware solutions need to evolve, thus permitting the dissemination of omnipresent attributes within the Ubiquitous and Pervasive computing environments of today. To determine the effectiveness of this middleware framework, a cloud computing based application will be developed.</p>"
pub.1132035030,Container-based load balancing for energy efficiency in software-defined edge computing environment,"The workload generated by the Internet of Things (IoT)-based infrastructure is often handled by the cloud data centers (DCs). However, in recent time, an exponential increase in the deployment of the IoT-based infrastructure has escalated the workload on the DCs. So, these DCs are not fully capable to meet the strict demand of IoT devices in regard to the lower latency as well as high data rate while provisioning IoT workloads. Therefore, to reinforce the latency-sensitive workloads, an intersection layer known as edge computing has successfully balanced the entire service provisioning landscape. In this IoT-edge-cloud ecosystem, large number of interactions and data transmissions among different layer can increase the load on underlying network infrastructure. So, software-defined edge computing has emerged as a viable solution to resolve these latency-sensitive workload issues. Additionally, energy consumption has been witnessed as a major challenge in resource-constrained edge systems. The existing solutions are not fully compatible in Software-defined Edge ecosystem for handling IoT workloads with an optimal trade-off between energy-efficiency and latency. Hence, this article proposes a lightweight and energy-efficient container-as-a-service (CaaS) approach based on the software-define edge computing to provision the workloads generated from the latency-sensitive IoT applications. A Stackelberg game is formulated for a two-period resource allocation between end-user/IoT devices and Edge devices considering the service level agreement. Furthermore, an energy-efficient ensemble for container allocation, consolidation and migration is also designed for load balancing in software-defined edge computing environment. The proposed approach is validated through a simulated environment with respect to CPU serve time, network serve time, overall delay, lastly energy consumption. The results obtained show the superiority of the proposed in comparison to the existing variants."
pub.1103159955,An Efficient SDN Multicast Architecture for Dynamic Industrial IoT Environments,"Large-scale industrial IoT services appear in smart factory domains such as factory clouds which integrate distributed small factories into a large virtual factory with dynamic combination based on orders of consumers. A smart factory has so many industrial elements including various sensors/actuators, gateways, controllers, application servers, and IoT clouds. Since there are complex connections and relations, it is hard to handle them in point-to-point manner. In addition, many duplicated traffics are exchanged between them through the Internet. Multicast is believed as an effective many-to-many communication mechanism by establishing multicast trees between sources and receivers. There are, however, some issues for adopting multicast to large-scale industrial IoT services in terms of QoS. In this paper, we propose a novel software-defined network multicast based on group shared tree which includes near-receiver rendezvous point selection algorithm and group shared tree switching mechanism. As a result, the proposed multicast mechanism can reduce the packet loss by 90% compared to the legacy methods under severe congestion condition. GST switching method obtains to decreased packet delay effect, respectively, 2%, 20% better than the previously studied multicast and the legacy SDN multicast."
pub.1113704531,Securing and Self recovery of Virtual Machines in cloud with an Autonomic Approach using Snapshots,"The advent of cloud computing has been so enormous that, cloud has been seen as the most enduring technology of today’s technology scenario. It is known to be the service oriented architecture, granted to the end-users as Infrastructure as a service, platform as a service, software as a service, business process as a service, security as a service and so on. Most of the underlying models used in cloud depend on the existing technologies for support; in particular, virtualization component imparts on-demand resource provisioning and multitenancy. Security of the resources provisioned in the form of images or instances play a major part in the security aspect of cloud. Users access software and hardware in cloud environment, through several virtual machines (VM) instances. These VM instances are isolated from each other, providing hardware utilization, easier management and migration compared to its physical counterpart. Services are shared among a group of service consumers, partners and vendors. The resources of a single physical machine are shared among the various created VM’s giving rise to new architectures and computing paradigms. Thus, security problems pose a major challenge in the virtual machines in the cloud environment. This paper presents an autonomic prediction model for auto recovery of the attacked VM instances in cloud. This approach would enable continuous progression of any process in the schedule without any job disruption. A method to propose the possible solution for auto recovery and self healing in the VM’s by an intelligent hypervisor is summarized."
pub.1122419605,DualFog-IoT: Additional Fog Layer for Solving Blockchain Integration Problem in Internet of Things,"Integration of blockchain and Internet of Things (IoT) to build a secure, trusted and robust communication technology is currently of great interest for research communities and industries. But challenge is to identify the appropriate position of blockchain in current settings of IoT with minimal consequences. In this article we propose a blockchain-based DualFog-IoT architecture with three configuration filter of incoming requests at access level, namely: Real Time, Non-Real Time, and Delay Tolerant Blockchain applications. The DualFog-IoT segregate the Fog layer into two: Fog Cloud Cluster and Fog Mining Cluster. Fog Cloud Cluster and the main cloud datacenter work in a tandem similar to existing IoT architecture for real-time and non-real-time application requests, while the additional Fog Mining Cluster is dedicated to deal with only Delay Tolerant Blockchain application requests. The proposed DualFog-IoT is compared with existing centralized datacenter based IoT architecture. Along with the inherited features of blockchain, the proposed model decreases system drop rate, and further offload the cloud datacenter with minimal upgradation in existing IoT ecosystem. The reduced computing load from cloud datacenter doesn’t only help in saving the capital and operational expenses, but it is also a huge contribution for saving energy resources and minimizing carbon emission in environment. Furthermore, the proposed DualFog-IoT is also being analyzed for optimization of computing resources at cloud level, the results presented shows the feasibility of proposed architecture under various ratios of incoming RT and NRT requests. However, the integration of blockchain has its footprints in terms of latent response for delay tolerant blockchain applications, but real-time and non-real-time requests are gracefully satisfying the service level agreement."
pub.1092055098,Understanding Live Migration Techniques Intended for Resource Interference Minimization in Virtualized Cloud Environment,"Cloud computing is consolidated as an environment which allows concurrent execution of various cloud applications of different organizations via a shared pool of resources. Each cloud user is provided with virtual machine to have further interaction with the cloud architecture components. Effective management of these virtualized machines along with satisfactory level of SLA is major challenge. Due to the resource overbooking over the physical host running, virtual machines need to be migrated from source to destination host. The migrated machine may disrupt other ongoing virtualized machines on destination host which can lead the application performance degradation. This paper provides insight of existing interference-aware live virtual machine migration techniques. As well taxonomy of the resource interference has been introduces. This paper also contains the comparative study of the performance assessment matrix, issues resolved, and mathematical models used by available live migration techniques that can act major key point while making live migration decisions. This paper is useful to cloud architect and the researchers working on automated live VM migration decision support system to achieve higher-level satisfaction of SLA by providing maximum quality-of-service parameters."
pub.1122783890,"Enabling privacy and security in Cloud of Things: Architecture, applications, security & privacy challenges","The Cloud of Things (IoT) that refers to the integration of the Cloud Computing (CC) and the Internet of Things (IoT), has dramatically changed the way treatments are done in the ubiquitous computing world. This integration has become imperative because the important amount of data generated by IoT devices needs the CC as a storage and processing infrastructure. Unfortunately, security issues in CoT remain more critical since users and IoT devices continue to share computing as well as networking resources remotely. Moreover, preserving data privacy in such an environment is also a critical concern. Therefore, the CoT is continuously growing up security and privacy issues. This paper focused on security and privacy considerations by analyzing some potential challenges and risks that need to be resolved. To achieve that, the CoT architecture and existing applications have been investigated. Furthermore, a number of security as well as privacy concerns and issues as well as open challenges, are discussed in this work."
pub.1131871221,Blockchain as a service models in the Internet of Things management: Systematic review,"Abstract Today, blockchain uses a list of various blocks for storing a distributed flat of invariable information that informs of a set of replicated logical things in the Internet of Things (IoT). In the blockchain, a set of blocks includes various transactions containing a cryptographic hash value and a timestamp. Blockchain‐as‐a‐service (BaaS) as a new service in cloud providers presents an infrastructure for accessing users to execute, manage, and monitor blockchain applications without high secured infrastructure requirements. Recently, BaaS concepts widely have used the management of IoT applications in different layers such as network, data, control, and resource. This paper provides a systematic review of recent research studies in BaaS models. The main goal behind this review is to categorize the applied scenarios, trends, evaluated Quality of Service (QoS) factors, new challenges, and open directions on BaaS models in IoT management. To evaluate the existing research studies in this field, five analytical research questions are proposed to analyze the technical aspects of each study. The analytical results based on existing research questions specify that the BaaS models are applied to network layer to manage IoT environment more than other layers. Also, security and privacy are two important factors to evaluate the existing BaaS models in cloud‐edge IoT environments. Finally, the integration of BaaS models on IoT environments with interconnections of cloud‐edge computing and software defined networks creates great secure opportunities for smart environment applications to monitor, manage, and improve all the atomic services and resources."
pub.1125711486,EAAM: Energy-aware application management strategy for FPGA-based IoT-Cloud environments,"An efficient integration of Internet of Things (IoT) and cloud computing techniques accelerates the evolution of next-generation smart environments (e.g., smart homes, buildings, cities). The advanced modern cloud networking architecture also helps to efficiently host, manage and optimize the IoT services in smart environments. In this paper, we have considered an “IoT-Cloud” environment where servers are composed of Field Programmable Gate Arrays (FPGAs) which are reconfigurable in nature. The energy consumption is considered as a major driving factor for the operational cost of the “IoT-Cloud” platform. We have proposed an “energy-aware application management” strategy for FPGA-based IoT-Cloud environments, which can efficiently handle sensors’ data transmission by positioning them into the best possible coordinates and execute the Service Requests requested by the users.
We have compared our strategy performances with an existing technique and the results show that our proposed strategy is capable to achieve high resource utilization with low energy consumption over different simulation scenarios."
pub.1094911358,Blueprint for Business Middleware as a Managed Cloud Service,"Cloud offers numerous technical middleware services such as databases, caches, messaging systems, and storage but very few business middleware services as first tier managed services. Business middleware such as business process management, business rules, operational decision management, content management and business analytics, if deployed in a cloud environment, is typically only available in a hosted (black-box) model. This is partly due to where cloud is in its evolution, and mostly due to the relatively higher complexity of business middleware vs. technical middleware in the deployment, provisioning, usage, etc. Business middleware consists of multiple functions for business processes design and modeling, execution, optimization, monitoring, and analysis. These functions and their associated complexity have inhibited the wholesale migration of existing business middleware to the cloud. To better understand the complexity in bringing business middleware to the cloud and to develop a systematic cloud enablement approach, we studied the deployment of IBM's Operational Decision Manager (ODM) business middleware product as a managed service (Cloud Decision Service) in IBM's BlueMix cloud platform. Our study indicates that complex middleware must be componentized along functional boundaries, and provide these functions for different business users and developers with cloud experience. In addition, middleware services must leverage other cloud services and they should provide interfaces so that they can be consumed by Java applications as well as by polyglot applications (JavaScript, Ruby, Python, etc). Applications can bind to and use our Cloud Decision Service in a matter of seconds. In contrast, it takes hours to days to setup such a service in the traditional packaged software model. Based on the lessons learned from this experiment we develop a blueprint for enabling high value business middleware as managed cloud services."
pub.1094075023,Cloud Migration: Planning Guidelines and Execution Framework,"Cloud computing is becoming increasingly popular because it offers valuable benefits, such as on-demand services, resource pooling, rapid elasticity, measured service, etc. However, recent studies indicate that many enterprises still hesitate to adopt cloud services due to various security concerns and implementation obstacles. Particularly, there is little research devoted to exploring the influences on cloud migration execution. To bridge this gap, our study derived some comprehensive cloud migration execution influences by interview approach, based on the technology-organization-environment (TOE) theory. We posit that system configuration (the technological aspect, which consists of complexity and compatibility), organizational fit (the organizational aspect, which consists of member reflection and operation process), and external support (the environmental aspect, which affects the migration execution of cloud computing). The findings in this paper can help enterprises facilitate various cloud migration activities, achieve their goals of migrating applicable legacy applications to the cloud environment, and realize the benefits provided by the cloud computing paradigm."
pub.1157354566,Towards a Benchmark for Fog Data Processing,"Fog data processing systems provide key abstractions to manage data and event
processing in the geo-distributed and heterogeneous fog environment. The lack
of standardized benchmarks for such systems, however, hinders their development
and deployment, as different approaches cannot be compared quantitatively.
Existing cloud data benchmarks are inadequate for fog computing, as their focus
on workload specification ignores the tight integration of application and
infrastructure inherent in fog computing.
  In this paper, we outline an approach to a fog-native data processing
benchmark that combines workload specifications with infrastructure
specifications. This holistic approach allows researchers and engineers to
quantify how a software approach performs for a given workload on given
infrastructure. Further, by basing our benchmark in a realistic IoT sensor
network scenario, we can combine paradigms such as low-latency event
processing, machine learning inference, and offline data analytics, and analyze
the performance impact of their interplay in a fog data processing system."
pub.1158829634,COSTA: A cost-driven solution for migrating applications in multi-cloud environments,"This paper focuses on the runtime replacement of microservice-based applications (MBAs), considering cost aspects. Several existing solutions also focus on replacement issues, but they usually concentrate on performance aspects and do not consider cost in the decision-making process. This paper presents COSTA (COST management and Adaptive system), a solution that monitors public clouds' costs and manages the migration of applications between them, considering the application's budget and execution cost. Several experiments were conducted to assess the solution, looking for scenarios ideal for evaluating a migration among clouds. In the end, COSTA effectively reduced the cost of applications through cloud providers migration."
pub.1144158438,Adoption and Success of e-HRM in a Cloud Computing Environment,"This qualitative study examines the digitisation of HRM in a cloud-based environment. The influencing factors for the transformation from conventional HRM to eHRM are examined with a special focus on the success factors from a strategic to the operational level. Additionally, an in-depth analysis of the currently existing and new HR metrics which emerge during the transformation takes place. The study is based on interviews with HR experts with extensive experience in transforming and working with the new technology. Active participation of the HR department is relevant for the success of the digital transformation HRM project. HR metrics have not been applied extensively so far and are used less for controlling and optimizing HR processes. New metrics would increase the acceptance of the new technology and thus the success of the overall HR transformation. The main contribution is related to the field of HR software adoption of cloud-based solutions."
pub.1151316435,Overcoming Virtual Training Challenges to Promote the Adoption of Cloud Solutions,"Abstract Cloud solutions play an essential role in digital transformation in the oil and gas industry. Training is key to promote cloud solutions and accelerate digital transformation. Unlike legacy computer-based software, agile development, which continuously releases new features of cloud solutions, makes the learning process more challenging because of the necessarily fast-paced adoption of the knowledge. Another challenge was that we had to virtually deliver the training sessions because of the pandemic during 2020 and 2021, which made the interaction with students very difficult without face-to-face contact. To be ready for the virtual training challenges, a self-guided quick-learning page was created and leveraged to bridge the gap between the continuous release of new features in cloud solutions and the fast-paced knowledge adoption. Training materials, including the self-training page, were updated once every quarter to reflect the new features in the cloud solutions. Unlike traditional software, after each training, the web-based training environment (tenant) was occupied by many training projects created by students, which made it difficult to be used by the new group of students. To solve this issue, a new procedure was developed to efficiently reset the tenant to ensure a smooth and data-secured training. As the training requests grew exponentially, more than 35 training tenants were created to meet the high demand and frequency of training from different customers across the world. The self-guided quick-training page has more than 17,454 visits and 600 unique users, with positive feedback from students. In 2021, more than 630 users from 40 countries were successfully trained. This is a two-fold increase from 2020 and a five-fold increase from 2019. In this paper, we provide new insight to successfully deliver virtual training and promote the adoption of cloud solutions in the digital transformation of the exploration and production industry."
pub.1112935310,Adoption and Success of e-HRM in a Cloud Computing Environment: A Field Study,"<p>This qualitative study examines the digitisation of HRM in a cloud-based environment. The influencing factors for the transformation from conventional HRM to eHRM are examined with a special focus on the success factors from a strategic to the operational level. Additionally, an in-depth analysis of the currently existing and new HR metrics which emerge during the transformation takes place. The study is based on interviews with HR experts with extensive experience in transforming and working with the new technology. Active participation of the HR department is relevant for the success of the digital transformation HRM project. HR metrics have not been applied extensively so far and are used less for controlling and optimizing HR processes. New metrics would increase the acceptance of the new technology and thus the success of the overall HR transformation. The main contribution is related to the field of HR software adoption of cloud-based solutions.</p>"
pub.1112945655,Micro-segmentation: securing complex cloud environments,"The datacentre as we know it has undergone a radical transformation in recent years – to the point where the term ‘centre’ is probably a misnomer. Enterprises of all sizes and across all sectors are rapidly migrating their datacentre operations to DevOps-based, hybrid-cloud environments, typically involving some combination of private cloud, public cloud and on-premise facilities. The datacentre as we know it has undergone a radical transformation. Mission-critical data, applications and workloads are constantly moving among the various installations as traffic levels and processing demands dictate. This agility is what makes these dynamic infrastructures attractive to organisations looking for efficiencies and competitive advantage. It is also what makes them scary from a security perspective. Micro-segmentation is an effective way to secure valuable datacentre assets and implement a zero trust model in a hybrid or multi-cloud environment, says Dave Klein of GuardiCore."
pub.1093427432,Integrating the business cloud,"Summary form only given, as follows. Today's organizations consume an increasing share of their computing resources as computing services, using an ondemand model and paying only for what they use. They consume cloud business applications and processes, and store and analyze vast amounts of information on cloud provisioned resources. Cloud computing is dramatically reducing the capital required to support IT operations, while bringing unprecedented openness, flexibility and access to enterprise computing. One of the paradoxes of this new environment is that openness and easy access to services and resources results in the creation of new barriers and computing silos. New 'born in the cloud' enterprises are quickly discovering the difficulty of integrating SaaS services into their daily operations in efficient, secure and reliable ways. SaaS APIs are as siloed as traditional enterprise applications, if not more. Data is still represented in heterogeneous formats and kept in multiple application stores. Thus, while access and delivery have fundamentally improved, the core problems of process and data integration remain. If anything, they are getting harder. Successful technologies for process and data integration in the cloud will differ fundamentally from existing approaches for enterprise integration. While existing approaches focus on eliminating silos and leveling access to process and information, cloud integration will acknowledge barriers as the price of extending our reach beyond the limits a single organization and focus on linking capabilities and information. Instead of reengineering processes, new process integration technology will link processes and application services offered by specialized providers, leveraging instead of replacing available processes. Instead of coercing heterogeneous information into unified schemas, the new approach to information integration will link enterprise data and open data to offer virtual access to much richer information set. New cloud integration services are already being built leveraging a collection of technologies born on the Web and also within the enterprise. They include ""low touch"" technologies such as process correlation and mining, process analytics, linked and open data, and provenance among others. This keynote will discuss trends in SaaS and cloud-based integration, and review some of the foundational technologies that can support a new cloud centric data and application integration paradigm."
pub.1094316421,The Actual Cost of Software Switching for NFV Chaining,"Network Function Virtualization (NFV) is a novel paradigm that enables flexible and scalable implementation of network services on cloud infrastructure. An important enabler for the NFV paradigm is software switching, which should satisfy rigid network requirements such as high throughput and low latency. Despite recent research activities in the field of NFV, not much attention was given to understand the costs of software switching in NFV deployments. Existing approaches for traffic steering and orchestration of virtual network functions either neglect the cost of software switching or assume that it can be provided as an input, and therefore real NFV deployments of network services are often suboptimal. In this work, we conduct an extensive and in-depth evaluation that examines the impact of service chaining deployments on Open vSwitch - the de facto standard software switch for cloud environments. We provide insights on network performance metrics such as throughput, CPU utilization and packet processing, while considering different placement strategies of a service chain. We then use these insights to provide an abstract generalized cost function that accurately captures the CPU switching cost of deployed service chains. This cost is an essential building block for any practical optimized placement management and orchestration strategy for NFV service chaining"
pub.1095109146,"Improving the Manageability of Enterprise Topologies Through Segmentation, Graph Transformation, and Analysis Strategies","The software systems running in an enterprise consist of countless components, having complex dependencies, are hosted on physical or virtualized environments, and are scattered across the technical infrastructure of an enterprise, ranging from on-premise data centers up to public cloud deployments. The resulting topology of the current IT landscape of an enterprise is often extremely complex. We show that information about this complex ecosystem can be captured in a graph-based structure, the enterprise topology graph. We argue that, using such graph-based representation, many challenges in Enterprise Architecture Management (EAM) can be tackled through the aid of graph processing algorithms. However, the high complexity of an enterprise topology graph is the main obstacle to this approach. An enterprise topology graph may consist of millions of nodes, each representing an element of the enterprise IT landscape. Further, these nodes comprise a large variety of properties and relationships, making the topology hardly manageable by human users and software tools. To address this complexity problem, we propose different mechanisms to make enterprise topology graphs manageable. Segmentation techniques, tailored to specific use cases, extract manageable segments from the enterprise topology graph. Based on a set of formally defined transformation operations we then demonstrate the power of the approach in three application scenarios."
pub.1104307679,Energy Conserving Secure VM Allocation in Untrusted Cloud Computing Environment,"Cloud computing is the latest buzz in most of the IT organizations which are witnessing a a trend of migration from traditional computing to cloud computing, thereby reducing their infrastructure cost and improving efficiency and performance. Cloud computing provides services through virtualization layer, which helps to execute more than one operating systems and applications on a single machine. Being a crucial part of cloud computing, virtualization layer faces major security threats, most challenging being an insider threat wherein attacker can either compromise existing virtual machines (VMs) or create rogue VMs. The objective of this work is to propose virtual machine (VM) allocation algorithm which operates in an untrusted cloud computing environment with non-trustworthy VMs. Our approach is based on the notion of trust. Lack of trust is modeled by either introducing faults or monitoring SLAs per host on which VMs are hosted. Detailed experiments considering varying cloud infrastructure and varying workloads are conducted using CloudSim. Results show that proposed algorithm works well in untrusted environment while at the same time is energy efficient and reduces the computational costs by decreasing the number of migrations and SLA violations."
pub.1005427727,Design Issue and Performance Analysis of Data Migration Tool in a Cloud-Based Environment,"With the popularization of Web applications and the emergence of cloud computing technology, database management and storage has evolved from PC to Web-based, even to cloud-based services as well. Also, with big data applications in cloud computing, more many organizations will eventually move their data from Web applications to a cloud-based environment. Except the cloud migration method, the cloud platform must provide an automatic tool in application and data migration for easy implementation. The existing Sqoop data migration tool requires users to be familiar with existing migration commands or move the data manually to a cloud-based database. Hence, the purpose of this paper is to design a new automatic data migration tool named MSCH using the migration of MySQL to HBase as an example to improve on the processing performance of Sqoop. After simulations, the experimental results show that the MSCH migration tool was 33 %, 25 %, and 15 % faster than Sqoop in terms of number of entries. CPU utilization during migration was also reduced by 35 %, 43 %, and 33 %, while memory usage was reduced by 90 %, 72 %, and 74 %, proving that the MSCH data migration tool shortens data migration time and lowers the load on computing resources to enhance the performance in data migration under cloud-based environment. Further studies will be conducted to determine the migration accuracy ratio of MSCH migration tools and Sqoop."
pub.1095469843,Delivering bioinformatics MapReduce applications in the cloud,"The ever-increasing data production and availability in the field of bioinformatics demands a paradigm shift towards the utilization of novel solutions for efficient data storage and processing, such as the MapReduce data parallel programming model and the corresponding Apache Hadoop framework. Despite the evident potential of this model and existence of already available algorithms and applications, especially for batch processing of large data sets as in the Next Generation Sequencing analysis, bioinformatics MapReduce applications are yet to become widely adopted in the bioinformatics data analysis. We identify two prerequisites for their adaptation and utilization: (1) the ability to compose complex workflows from multiple bioinformatics MapReduce tools that will abstract technical details of how those tools are combined and executed allowing bioinformatics domain experts to focus on the analysis, and (2) the availability of accessible and flexible computing infrastructure for this type of data processing. This paper presents integration of two existing systems: Cloudgene, a bioinformatics MapReduce workflow framework, and CloudMan, a cloud manager for delivering application execution environments. Together, they enable delivery of bioinformatics MapReduce applications in the Cloud."
pub.1165617999,ECM: Enhanced confidentiality method to ensure the secure migration of data in VM to cloud environment,"Cloud is the technology behind all modern IT paradigms today. All kinds of applications run in the cloud environment and host their data. Enterprises show interest in migrating their data and servers to the cloud to benefit the cloud. The cloud is an open distributed network environment; hence, it is vulnerable to data security attacks. Migration in cloud computing permits the handover of resources, for example, virtual machine (VM) and data from off-premises to on-premises. During the migration, the data needs to be secured. This paper proposes an enhanced confidentiality technique (ECM) to ensure data security when migrated to the cloud. The proposed enhancement is on the advanced encryption standard (AES) to strengthen the protection level of the AES. The standard AES runs for rounds with four unique stages of data processing: Substitute bytes, add round key, shift rows and mix columns. The proposed ECM enhances the substitute bytes stage with dynamic substitution boxes for each round of the AES. The proposed ECM is evaluated for its efficiency by the avalanche effect. The result of the ECM improves data protection when the data is migrated to the cloud."
pub.1181426192,Designing Resilient Multi-Tenant Architectures in Cloud Environments,"The increasing demand for scalable and cost-effective cloud solutions has led to the widespread adoption of multi-tenant architectures, where multiple customers share a single instance of software while maintaining data isolation. However, designing resilient multi-tenant architectures poses significant challenges, particularly in ensuring reliability, security, and performance across diverse tenant workloads. This paper explores the key design principles and strategies essential for building resilient multi-tenant systems in cloud environments. We discuss the importance of fault tolerance, emphasizing redundancy and data replication to ensure uninterrupted service delivery. Additionally, we address the critical need for robust security measures, including access control and encryption, to protect tenant data from breaches and unauthorized access. Performance optimization techniques, such as resource allocation and load balancing, are examined to enhance responsiveness and scalability while maintaining isolation among tenants. Furthermore, we highlight the role of monitoring and analytics in identifying potential failures and bottlenecks, enabling proactive management of system health. Through a comprehensive review of existing frameworks and best practices, this paper provides insights into the effective design of resilient multi-tenant architectures that meet the growing demands of cloud-based services while ensuring a secure and high-performance environment for all users. The findings aim to guide cloud architects and developers in creating robust systems that enhance customer satisfaction and operational efficiency."
pub.1170328889,"A Comprehensive Review of Migration of Big Data Applications to Public Clouds: Current Requirements, Types, Strategies, and Case Studies","In today’s world, data is continuously being generated at a high rate by not just technological companies, but also by various other industries like the banking sector. The amount of data generated is in the Gigabytes and Terabytes per day. These processes are becoming difficult to store on-premise or in private clouds due to high volume requirements along with high associated costs. Therefore, many of these companies are shifting toward cloud-based solutions. Although in the long run, these cloud-based solutions provide exciting opportunities, the data migration from the on-premise or private cloud environments to the public clouds presents a tough issue to solve in itself. This paper aims to survey the field of data migration and provide any reader with a comprehensive understanding of all that there is to learn when it comes to data migration. An extensive literature survey followed by a comprehensive understanding of the latest advancements and categories of data migration has been given. It is believed that upon going through the contents of this paper, one will be able to find the right strategy and process they would like to follow if they choose to migrate their big data applications to the cloud."
pub.1099626538,"Edge of Things: The Big Picture on the Integration of Edge, IoT and the Cloud in a Distributed Computing Environment","A centralized infrastructure system carries out existing data analytics and decision-making processes from our current highly virtualized platform of wireless networks and the Internet of Things (IoT) applications. There is a high possibility that these existing methods will encounter more challenges and issues in relation to network dynamics, resulting in a high overhead in the network response time, leading to latency and traffic. In order to avoid these problems in the network and achieve an optimum level of resource utilization, a new paradigm called edge computing (EC) is proposed to pave the way for the evolution of new age applications and services. With the integration of EC, the processing capabilities are pushed to the edge of network devices such as smart phones, sensor nodes, wearables, and on-board units, where data analytics and knowledge generation are performed which removes the necessity for a centralized system. Many IoT applications, such as smart cities, the smart grid, smart traffic lights, and smart vehicles, are rapidly upgrading their applications with EC, significantly improving response time as well as conserving network resources. Irrespective of the fact that EC shifts the workload from a centralized cloud to the edge, the analogy between EC and the cloud pertaining to factors such as resource management and computation optimization are still open to research studies. Hence, this paper aims to validate the efficiency and resourcefulness of EC. We extensively survey the edge systems and present a comparative study of cloud computing systems. After analyzing the different network properties in the system, the results show that EC systems perform better than cloud computing systems. Finally, the research challenges in implementing an EC system and future research directions are discussed."
pub.1171898737,Native Cloud Object Storage in Db2 Warehouse: Implementing a Fast and Cost-Efficient Cloud Storage Architecture,"Database systems built on traditional storage subsystems typically store their data in small blocks referred to as data pages (commonly sized in a multiple of 4KB for historical reasons). These traditional storage subsystems, for example network attached block storage, were designed for efficient random-access I/O patterns at the block level, and the block size is usually configurable by the application based on its needs. For large scale analytic databases in cloud environments, these traditional storage subsystems are not cost effective when compared to cloud object storage, and database systems that exploit them risk becoming uncompetitive. This paper describes the modernization of the storage architecture of Db2 Warehouse, a traditional full feature and high-performance database system with 3 decades of development, to exploit the new paradigm of cost-effective storage for the cloud. We discuss a solution based on the integration of LSM trees as part of the storage subsystem, that enables Db2 Warehouse to efficiently store data pages within object storage, and through the application of special techniques to minimize read and write latencies as well as all of the amplification factors (write, read, and storage), achieve not only storage cost savings, but also higher performance. Further, by retaining the traditional data page format, we are able to avoid significantly re-architecting the database kernel and thereby retain the substantial capabilities and optimizations of the existing system."
pub.1132951297,IoTSim-Osmosis: A framework for modeling and simulating IoT applications over an edge-cloud continuum,"The osmotic computing paradigm sets out the principles and algorithms for simplifying the deployment of Internet of Things (IoT) applications in integrated edge-cloud environments. Various existing simulation frameworks can be used to support integration of cloud and edge computing environments. However, none of these can directly support an osmotic computing environment due to the complexity of IoT applications and heterogeneity of integrated edge-cloud environments. Osmotic computing suggests the migration of workload to/from a cloud data center to edge devices, based on performance and security trigger events. We propose ‘IoTSim-Osmosis– a simulation framework to support the testing and validation of osmotic computing applications. In particular, our detailed related work analysis demonstrates that IoTSim-Osmosis is the first simulation framework to enable unified modeling and simulation of complex IoT applications over heterogeneous edge-cloud environments. IoTSim-Osmosis is demonstrated using an electricity management and billing application case study, for benchmarking various run-time QoS parameters, such as IoT battery use, execution time, network transmission time and consumed energy."
pub.1155831367,Secure Cloud Migration Strategy (SCMS): A Safe Journey to the Cloud,"The state of cloud security is evolving. Many organizations are migrating their on-premises data centers to cloud networks at a rapid pace due to the benefits like cost-effectiveness, scalability, reliability, and flexibility. Yet, cloud environments also raise certain security concerns that may hinder their adoption. Cloud security threats may include data breaches/leaks, data loss, access management, insecure APIs, and misconfigured cloud storage. The security challenges associated with cloud computing have been widely studied in previous literature and different research groups. This paper conducted a systematic literature review and examined the research studies published between 2010 and 2023 within popular digital libraries. The paper then proposes a comprehensive Secure Cloud Migration Strategy (SCMS) that organizations can adopt to secure their cloud environment. The proposed SCMS consists of three main repeatable phases/processes, which are preparation; readiness and adoption; and testing. Among these phases, the author addresses tasks/projects from the different perspectives of the three cybersecurity teams, which are the blue team (defenders), the red team (attackers), and the yellow team (developers). This can be used by the Cloud Center of Excellence (CCoE) as a checklist that covers defending the cloud; attacking and abusing the cloud; and applying the security shift left concepts. In addition to that, the paper addresses the necessary cloud security documents/runbooks that should be developed and automated such as incident response runbook, disaster recovery planning, risk assessment methodology, and cloud security controls. Future research venues and open cloud security problems/issues were addressed throughout the paper. The ultimate goal is to support the development of a proper security system to an efficient cloud computing system to help harden organizations’ cloud infrastructures and increase the cloud security awareness level, which is significant to national security. Furthermore, practitioners and researchers can use the proposed solutions to replicate and/or extend the proposed work."
pub.1136499853,Optimize Continuous Integration and Continuous Deployment in Azure DevOps for a controlled Microsoft .NET environment using different techniques and practices,"Delivering a software is not a simple process, even if it is a completely new software created from the scratch, the enhancement of an existing software or fixing the bugs of an existing application. As per the new industrial standard most of the companies follow the DevOps methodologies to deliver the software products, which leverage the flexibilities of the software delivery with expected quality on time with minimal errors. One of the most important factors of the DevOps methodology is Continuous Integration and Continuous Deployment. As the entire software industry is moving towards Cloud Computing one of the most powerful, cost effective and easily maintainable environment to host the software applications. There are various Cloud computing providers available in the industry now. Most of them have its own advantage and dis-advantages. There are many researches going on various topics on these areas and the importance of these topics are getting more relevant in the industry now. There are lot of gaps on the Continuous Integration and Continuous Deployment concepts and more improvement options have to be identified and certified throughout the various researches. To understand the importance of this platform, we decided to concentrate the research on the Continuous Integration and Continuous Deployment in Azure DevOps for a controlled Microsoft .NET environment using different techniques and practices. Here our intention is to identify the various factors which helps the industry to do a better Continuous Integration and Continuous Deployment in the Azure DevOps for a controlled Microsoft .NET environment using different techniques and practices. Among the “n” number of factors identified on this subject, only couple of factors are considered as part of this research."
pub.1107389409,Distributed cache memory data migration strategy based on cloud computing,"Summary In view of the huge scale of cloud computing, cloud computing systems face many problems in terms of scalability and high availability. In view of the distributed data storage in the cloud computing environment, a recursion based N boundary network model is constructed. A data management model is given on the basis of the data center network structure. A replica distribution strategy and a copy selection strategy are designed to improve the data availability and load balance. On the premise of guaranteeing data availability, a data migration algorithm based on coverage set is proposed, which uses node selection strategy to reduce the migration cost as much as possible. Data migration makes more machines dormant and reduces energy consumption. By comparing and analyzing the experimental data, the correctness and effectiveness of the proposed network topology, data management model, and data migration technology are verified. In the edge cloud computing architecture, in order to ensure the response time of the cloud computing service and minimize the data redundancy in the system, this paper proposes and analyzes the data migration strategy and the pre stored data migration strategy for the network transmission, and gives the suitable application scene. On this basis, in order to control the balance of access network transmission and storage flexibly, a data migration strategy based on network performance is proposed, which ensures the real‐time response of the service."
pub.1040081156,A Secure Architecture for Inter-cloud Virtual Machine Migration,"Virtual machine migration is an important tool that can be used in cloud computing environment for load balancing, disaster recovery, server consolidation, hardware maintenance, etc. Currently a few techniques have been proposed to secure the virtual machine migration process. However, these techniques have number of limitations e.g. lack of standard access control, mutual authentication, confidentiality, non-repudiation and integrity of VM data. Some of the techniques provide security services such as mutual authentication using TPM (Trusted Platform Module), however, not all the hardware platforms yet possess the TPM capability. This limits the deployment of such solutions in legacy systems. The architecture, presented in this paper, attempts to overcome these limitations with existing hardware support. In particular, we designed a secure and efficient protocol that migrates virtual machine from source cloud domain to destination cloud domain by considering fundamental security services such as confidentiality, integrity, standard access control and non-repudiation."
pub.1022518767,"Access Control As a Service in Cloud: Challenges, Impact and Strategies","The evolution of service-oriented architecture has given birth to the promising cloud technology, which enables the outsourcing of existing hardware and software information technology (IT) infrastructure via the Internet. Since the cloud offers services to a variety of organizations under the same umbrella, it raises security issues including unauthorized access to resources and misuse of data stored in third-party platform. The fact that the cloud supports multiple tenants is the cause for the biggest concern among organizations: how to prevent malicious users from accessing and manipulating data they have no right to access. In this regard, various access control techniques have been proposed, which concentrate on certain authorization issues like the ease of privilege assignment or the resolution of policy conflicts, while ignoring other important weaknesses such as the lack of interoperability and management issues which arise in the dynamic cloud environment. To cover all these challenges, access control as a service (ACaaS), which stems from its significantly more popular parent, security as a service (SECaaS), is considered a viable solution for mediating cloud service consumers’ access to sensitive data. In this chapter, we assist the cloud community in understanding the various issues associated with providing authorization services in the cloud that may be technical, such as privilege escalation and separation of duties, or managerial, such as the steep requirement of time and money for this purpose. ACaaS is the comprehensive solution to some of the issues highlighted previously. We have also discussed the significance and impact of ACaaS, along with the strategies reported in the literature for providing a secure access to the applications hosted on the cloud. We then holistically cover the authorization requirements of the cloud environment, specifically for software as a service (SaaS) model, evaluating the extant relevant solutions based on certain defined factors from the National Institute of Standards and Technology (NIST)-. The outcome of our research is that an ideal ACaaS should be extensive and holistic, which encompasses all the requisite security and managerial features and provides an efficient and reliable access control mechanism to the cloud consumers that complies with international standards."
pub.1152036905,"Crazy Nodes: Towards Ultimate Flexibility in Ubiquitous Big Data Stream Engineering, Visualisation, and Analytics, in Smart Factories","Smart Factories characterize as context-rich, fast-changing environments where heterogeneous hardware appliances are found beside of also heterogeneous software components deployed in (or directly interfacing with) IoT devices, as well as in on-premise mainframes, and on the Cloud. This inherent heterogeneity poses major challenges particularly when a high degree of resiliency is needed, and the ubiquitously deployed software components must be replaced or reconfigured at real-time to respond to the most diverse events, ranging from an out-of-range sensor detection, to a new order issued by a customer. In this work, a software framework is presented, which allows to deploy, (re)configure, run, and monitor the most diverse software across all the three layers of the Smart Factory (edge, fog, Cloud), from remote, via API calls, in a standardised uniform manner, relying on containerization technologies, and on a variety of software technologies, frameworks, and programming languages, including Node-RED, MQTT, Scala, Apache Spark, and Kafka. The most recent advances in the framework design, implementation, and demonstration, which led to the introduction of the so-called Crazy Nodes, are presented and motivated. A comprehensive proof-of-concept is given, where user interfaces and distributed systems are created from scratch via API calls to implement AI-based alerting systems, Big Data stream filtering and transformation, AI model training, storage, and usage for one-shot as well as stream predictions, and real-time Big Data visualization through line plots, histograms, and pie charts."
pub.1046310553,A cloud-based Farm Management System: Architecture and implementation,"Recent technological advances have paved the way for developing and offering advanced services for the stakeholders in the agricultural sector. A paradigm shift is underway from proprietary and monolithic tools to Internet-based, cloud hosted, open systems that will enable more effective collaboration between stakeholders. This new paradigm includes the technological support of application developers to create specialized services that will seamlessly interoperate, thus creating a sophisticated and customisable working environment for the end users. We present the implementation of an open architecture that instantiates such an approach, based on a set of domain independent software tools called “generic enablers” that have been developed in the context of the FI-WARE project. The implementation is used to validate a number of innovative concepts for the agricultural sector such as the notion of a services’ market place and the system’s adaptation to network failures. During the design and implementation phase, the system has been evaluated by end users, offering us valuable feedback. The results of the evaluation process validate the acceptance of such a system and the need of farmers to have access to sophisticated services at affordable prices. A summary of this evaluation process is also presented in this paper."
pub.1171283648,Dynamic Load Balancing with Task Migration: A Genetic Algorithm Approach for Optimizing Cloud Computing Infrastructure,"The advancement of cloud computing is contingent upon the effective establishment of infrastructure and the flexible use of accessible resources. An essential aspect of this evolutionary process is load balancing, a critical task involving fair allocation of dynamic workloads among different nodes. The primary aim is to mitigate the issues of resource overload or underutilization issues, which presents a complex challenge in optimization. A proficient load balancer must be able to adapt its method to the dynamic environment and work demands. This research presents a novel load-balancing technique incorporating task migration using a Genetic Algorithm (GA). The objective is to achieve optimal load distribution within the cloud infrastructure while minimizing the makespan for a given set of tasks. By utilizing the inherent optimization capabilities of genetic algorithms and the flexibility of task migration, the suggested approach demonstrates dynamic adaptation to the constantly evolving demands of cloud computing. In order to evaluate the effectiveness of the plan, it is implemented and assessed using simulations carried out on the Cloud Analyst simulator. The findings from the simulation outcomes, as demonstrated in a representative application scenario, provide evidence for the effectiveness of the task migration-integrated genetic algorithm-based approach compared to existing methods such as First Come, First Serve (FCFS), Round Robin (RR), and the local search algorithm Stochastic Hill Climbing (SHC). This comprehensive method not only improves resource utilization efficiency but also showcases the capacity to mitigate performance limitations through intelligent task reallocation. The proposed technique, which incorporates task migration capabilities, shows potential in facilitating the development of a more efficient and adaptable cloud infrastructure as the cloud landscape undergoes ongoing changes."
pub.1113448289,FogBus: A Blockchain-based Lightweight Framework for Edge and Fog Computing,"Recently much emphasize is given on integrating Edge, Fog and Cloud infrastructures to support the execution of various latency sensitive and computing intensive Internet of Things (IoT) applications. Although different real-world frameworks attempt to assist such integration, they have limitations in respect of platform independence, security, resource management and multi-application execution. To address these limitations, we propose a framework, named FogBus that facilitates end-to-end IoT-Fog(Edge)-Cloud integration. FogBus offers platform independent interfaces to IoT applications and computing instances for execution and interaction. It not only assists developers to build applications but also helps users to run multiple applications at a time and service providers to manage their resources. Moreover, FogBus applies Blockchain, authentication and encryption techniques to secure operations on sensitive data. Due to its simplified and cross platform software systems, it is easy to deploy, scalable and cost efficient. We demonstrate the effectiveness of FogBus by creating a computing environment with it that integrates finger pulse oximeters as IoT devices with Smartphone-based gateway and Raspberry Pi-based Fog nodes for Sleep Apnea analysis. We also evaluate the characteristics of FogBus in respect of other existing frameworks and the impact of various FogBus settings on system parameters through deployment of a real-world IoT application. The experimental results show that FogBus is comparatively lightweight and responsive, and different FogBus settings can tune the computing environment as per the situation demands."
pub.1119409125,A Survey on Security Issues in Cloud Computing,"Cloud Computing holds the potential to eliminate the requirements for setting
up of high-cost computing infrastructure for the IT-based solutions and
services that the industry uses. It promises to provide a flexible IT
architecture, accessible through internet for lightweight portable devices.
This would allow many-fold increase in the capacity or capabilities of the
existing and new software. In a cloud computing environment, the entire data
reside over a set of networked resources, enabling the data to be accessed
through virtual machines. Since these data centers may lie in any corner of the
world beyond the reach and control of users, there are multifarious security
and privacy challenges that need to be understood and taken care of. Also, one
can never deny the possibility of a server breakdown that has been witnessed,
rather quite often in the recent times. There are various issues that need to
be dealt with respect to security and privacy in a cloud computing scenario.
This extensive survey paper aims to elaborate and analyze the numerous
unresolved issues threatening the Cloud computing adoption and diffusion
affecting the various stake-holders linked to it."
pub.1061278026,Enabling performance intelligence for application adaptation in the Future Internet,"Today's Internet which provides communication channels with best-effort end-to-end performance is rapidly evolving into an autonomic global computing platform. Achieving autonomicity in the Future Internet will require a performance architecture that (a) allows users to request and own 'slices' of geographically-distributed host and network resources, (b) measures and monitors end-to-end host and network status, (c) enables analysis of the measurements within expert systems, and (d) provides performance intelligence in a timely manner for application adaptations to improve performance and scalability. We de- scribe the requirements and design of one such ""Future Internet performance architecture"" (FIPA), and present our reference implementation of FIPA called 'OnTimeMeasure.' OnTimeMeasure comprises of several measurement-related services that can interact with each other and with existing measurement frameworks to enable performance intelligence. We also explain our OnTimeMea- sure deployment in the global environment for network innovations (GENI) infrastructure collaborative research initiative to build a sliceable Future Internet. Further, we present an application- adaptation case study in GENI that uses OnTimeMeasure-enabled performance intelligence in the context of dynamic resource allocation within thin-client based virtual desktop clouds. We show how a virtual desktop cloud provider in the Future Internet can use the performance intelligence to increase cloud scalability, while simultaneously delivering satisfactory user quality-of-experience."
pub.1175289684,Improved Integrated Model for Data Storage in the Cloud,"The software model showcases the effective movement of files across storage units in a cloud environment. It includes features such as user identification, registration, and file upload capabilities. The experimental findings demonstrate an efficient procedure that has been confirmed through user interfaces for logging in, registering, and uploading files. The admin dashboard streamlines user administration and file transfer processes, utilising a time-efficient data migration technique. Comparative analysis reveals a substantial enhancement in data migration throughput, as the suggested method reduces migration time by an average of 50% in comparison to existing systems. The results validate the system's improved efficiency in managing file transfers and user administration in a cloud-based environment."
pub.1006753355,Architecture Proposal for MCloud IoT,"The world is now heading towards an era shaped by things that able to act and interact through Internet. Despite of many IoT devices suffer from limitations regarding to storage, processing capability, and communication, IoT plays a major role providing a new set of applications and services. Cloud computing provides a supplement solution for IoT limitation. Integration of IoT and Cloud Computing is considered a new direction that both scientist and business are seeking for bringing new applications and benefits to existing applications and services. Moreover, the fast development of mobile devices produces powerful devices that are able to play many roles, creating better IoT scenarios. In this paper; we propose a new MCloud IoT architecture that works on an IoT environment, which is composed by mobile devices such as smart phones, tablets, and smart sensors. MCloud IoT architecture is designed to deliver the applications and services demanded by end users. Moreover, we have included a layered communication model for devices’ communication. In our design we have taken into account the system performance and to provide QoS. We also analyze the benefits of our design. This new architecture provides a revolutionary vision that meets the future expectations of cloud systems."
pub.1024705505,Towards jungle computing with Ibis/Constellation,"The scientific computing landscape is becoming more and more complex. Besides traditional supercomputers and clusters, scientists can also apply grid and cloud infrastructures. Moreover, the current integration of many-core technologies such as GPUs with such infrastructures adds to the complexity. To make matters worse, data distribution, hardware availability, software heterogeneity, and increasing data sizes, commonly force scientists to use multiple computing platforms simultaneously: a true computing jungle. In this paper we introduce Ibis/Constellation, a software platform specifically designed for distributed, heterogeneous and hierarchical computing environments. In Ibis/Constellation we assume that applications consist of several distinct (but somehow related) activities. These activities can be implemented independently using existing, well understood tools (e.g. MPI, CUDA, etc.). Ibis/Constellation is then used to construct the overall application by coupling the distinct activities. Using application defined labels in combination with context-aware work stealing, Ibis/Constellation provides a simple and efficient mechanism for automatically mapping the activities to the appropriate resources, taking data locality and heterogeneity into account. We show that an existing supernova detection application can be ported to Ibis/Constellation with little effort. By making small changes to the application defined labels, this example application can run efficiently in three very different HPC computing environments: a distributed set of clusters, a large 48-core machine, and a GPU cluster."
pub.1094511733,A Multi-Agent Approach to the Monitoring of Cloud Computing System with Dynamically Changing Configuration,"Cloud based distributed systems rely on scheduling and resources allocation to function. In complex distributed systems a distribution of many jobs of different types is required. At the same time, a problem of virtual machines migration to physical servers must be solved. Therefore, configuration of a cloud system may be very dynamic, meaning that not only number of existing computational servers but also their location on physical servers might change. Optimal control strategies aimed to solve these problems are effective only when updated information about system's components is available. However, gathering this information from many distributed components of a cloud system, such as physical nodes or virtual machines may significantly decrease overall performance. These problems can be solved by applying different optimization techniques such as multi-agent approach. Agents decide if the information is outdated and needs to be updated by them. This paper describes a cloud system architecture that uses agents of different types. Agents’ algorithms and their interaction schemes are defined. Software implementation in form of software environment is presented. Simulation experiments to compare performance of the system when using default monitoring methods and a multiagent approach were conducted."
pub.1024297383,Cloud Security,"This chapter provides a comprehensive study on the existing cloud security solutions and analyzes its challenges and trend. It presents an OpenFlow‐based intrusion detection and prevention systems (IDPS) solution, called FlowIPS, that focuses on the intrusion prevention in the cloud virtual networking environment. FlowIPS provides network reconfiguration (NR) features by programming POX controllers to enable the FlowIPS mitigation approaches. The performance evaluation of FlowIPS demonstrates the feasibility of the proposed solution, which is more efficient compared to traditional IPS approaches. The chapter discusses the technical background of the software‐defined networking (SDN) and intrusion detection system. It presents the existing solutions of the cloud security and describes the transformation from the existing cloud security solutions to the next‐generation SDN‐based solutions."
pub.1124838413,Middleware,"In the context of IT applications and especially in large organizations, integration of existing information systems into new IT environments poses many challenges. One of the biggest issue in this regard is dealing with the systems’ heterogenity in terms of used programming languages, operating systems, or even data formats. In order to ensure communication between different information systems, developers must establish common interfaces. This chapter introduces middleware as a type of software which manages and facilitates interactions between applications across computing platforms. Besides a brief definition and overview of middleware, several of its characteristics are described. Furthermore, the differences between the three middleware categories (message-oriented, transaction-oriented and object-oriented middleware) are defined. In addition to these theoretical foundations, some practical implementations are presented."
pub.1125836776,Application of Cloud Computing in Geological Exploration,"The wide application of information technology in geological exploration will greatly improve the modernization level of geological exploration. At present, geological survey information work has entered the era of big data. How to break the limitation of independent management of geological exploration database and effectively integrate and share the accumulated massive geological exploration data has become a problem that the geological exploration information workers are eager to solve. The emergence and maturity of cloud computing technology provides a feasible solution to this problem. Firstly, from the perspective of public welfare and social service of geological exploration data, this paper summarizes and analyzes the application requirements of cloud computing in geological exploration work by studying the theoretical basis and relevant examples of cloud computing technology. Use Microsoft azure, SQL azure and other related products of Microsoft cloud computing platform azure to achieve the corresponding needs, research and clarify the relationship between geological exploration data and specific applications in geological cloud computing; use VMware vSphere virtualization platform to share existing and available hardware resources, and at the same time, research and build geoscience cloud storage mode based on virtualization technology to realize the sea Reasonable storage of quantitative geological exploration data; establishment of development environment with SDK provided by azure as the core for the development of application program of geological exploration data cloud computing; realization of data storage in the cloud for the purpose of integrating and sharing the achievement data of national mineral resource potential evaluation, and virtualization application of geoscience software."
pub.1169585286,Deploying Pangeo on HPC: our experience with the Remote Sensing Deployment Analysis environmenT on SURF infrastructure,"The Pangeo software stack includes powerful tools that have the potential to revolutionize the way in which research on big (geo)data is conducted. A few of the aspects that make them very attractive to researchers are the ease of use of the Jupyter web-based interface, the level of integration of the tools with the Dask distributed computing library, and the possibility to seamlessly move from local deployments to large-scale infrastructures.  The Pangeo community and project Pythia are playing a key role in providing training resources and examples that showcase what is possible with these tools. These are essential to guide interested researchers with clear end goals but also to provide inspiration for new applications.  However, configuring and setting up a Pangeo-like deployment is not always straightforward. Scientists whose primary focus is domain-specific often do not have the time to spend solving issues that are mostly ICT in nature. In this contribution, we share our experience in providing support to researchers in running use cases backed by deployments based on Jupyter and Dask at the SURF supercomputing center in the Netherlands, in what we call the Remote Sensing Deployment Analysis environmenT (RS-DAT) project.  Despite the popularity of cloud-based deployments, which are justified by the enormous data availability at various public cloud providers, we discuss the role that HPC infrastructure still plays for researchers, due to the ease of access via merit-based allocation grants and the requirements of integration with pre-existing workflows. We present the solution that we have identified to seamlessly access datasets from the SURF dCache massive storage system, we stress how installation and deployment scripts can facilitate adoption and re-use, and we finally highlight how technical research-support staff such as Research Software Engineers can be key in bridging researchers and HPC centers. "
pub.1164386104,Wawel: Architecture for Scalable Attestation of Heterogeneous Virtual Execution Environments,"Existing attestation mechanisms lack scalability and support for heterogeneous virtual execution environments (VEEs), such as virtual machines and containers executed inside or outside hardware isolation on different vendors' hardware in clouds managed by various organizations. To overcome these limitations, hardware vendors and cloud providers implement proprietary mechanisms (Intel DCAP, Amazon NitroTPM, Google Titan) to support their offerings. However, due to their plurality, the attestation becomes cumbersome because it increases maintenance and integration costs and reduces portability required in hybrid- and multi-cloud deployments. We introduce WAWEL, a framework that enables scalable attestation of heterogeneous VEEs. WAWEL can be plugged into existing hardware-specific attestation mechanisms, offering a unified interface. WAWEL supports the widely adopted trusted platform module (TPM) attestation standard. We implemented a prototype and integrated it with three different VEEs. It supports runtime integrity attestation with Linux integrity measurement architecture (IMA) and legacy applications requiring zero-code changes. The evaluation demonstrated that the WAWEL prototype achieves very good performance and scalability despite the indirections between the VEE and hardware root of trust."
pub.1039336690,Improving readiness for enterprise migration to the cloud,"Enterprises are increasingly moving their IT infrastructures to the Cloud, driven by the promise of low-cost access to ready-to-use, elastic resources. Given the heterogeneous and dynamic nature of enterprise IT environments, a rapid and accurate discovery of complex infrastructure dependencies at the application, middleware, and network level is key to a successful migration to the Cloud. Existing migration approaches typically replicate source resources and configurations on the target site, making it challenging to optimize the resource usage (for reduced cost with same or better performance) or cloud-fit configuration (no misconfiguration) after migration. The responsibility of reconfiguring the target environment after migration is often left to the users, who, as a result, fail to reap the benefits of reduced cost and improved performance in the Cloud. In this paper we propose a method that automatically computes optimized target resources and identifies configurations given discovered source properties and dependencies of machines, while prioritizing performance in the target environment. From our analysis, we could reduce service costs by 60.1%, and found four types of misconfigurations from real enterprise datasets, affecting up to 81.8% of a data center's servers."
pub.1173707261,Managing and Orchestrating Cross-Cloud VNFs with Deployable Sidecar VNF Coordinators,"This work presents a novel approach for managing and orchestrating Virtual Network Functions (VNFs) across cloud domains. We introduce deployable Sidecar VNF (S-VNF) coordinators designed for seamless integration into existing infrastructure, enabling operators to leverage infrastructure services provided by different providers. This approach offers several advantages beyond flexibility: domain-specific adaptation to diverse environments, non-intrusive deployment minimizing disruption to existing NFV Management and Orchestration (MANO) systems, enhanced MANO interoperability, and improved security through proxying and shielding the underlying MANO management interface. Experiments confirm that the delay introduced by our S-VNF coordinators has a negligible impact on overall performance, making it a viable solution for cross-cloud VNF management."
pub.1155975244,A Heterogeneous Hybrid Cloud Storage Service Using Storage Gateway with Transfer Acceleration and Diff Algorithm,"Recently, the cloud service has the potential to replace conventional cluster and grid systems. The objective of migrating apps to the cloud is to minimize maintenance and procurement expenses while simultaneously boosting scalability and availability. However, embra=cing cloud technology created some challenges, such as the complexity of cloud storage. In addition, many clients underestimate if it is not plug-and-play. Each vendor has its access methods, and nonstandard application programming interfaces (APIs) make integrated applications, such as archiving or sharing data with cloud storage, complicated, costly, and require high throughput. Furthermore, organizations did not have many alternatives for implementing high-performance object storage systems in the cloud and on-premises data centers until now. In this paper, we would like to suggest a storage gateway as a solution to this issue and will optimize it using Transfer Acceleration and Diff algorithms to improve the performance, Intelligent Tiering to reduce costs, and Server-Side encryption for extra protection. Moreover, utilizing Storage Gateway has proven can provide more efficient integration between the on-premises data center environment and the AWS Cloud Storage ecosystem that is safer and more reliable. This technology can work in a common data center environment regardless of the vendor used by the company it can communicate seamlessly with the AWS Environment."
pub.1094840259,U.S. Army Modeling and Simulation Executable Architecture Deployment Cloud Virtualization Strategy,"Our research has included leveraging Virtualization Technologies to provide integration, configuration and execution relief of Modeling & Simulation (M&S) event planning, instantiation and analysis. We have achieved this through a single service that is used to deploy and execute stand-alone applications as well as separate, but cooperative, applications on a dynamic virtual machine-based cloud. This use of virtualization technology shows significant cost savings in reducing the human effort for integration, test and execution by providing a powerful virtual machine environment that combines new and existing applications and their configurations. Our effort eliminates the time needed to manually configure and execute these applications on physical hardware once they are captured in the system."
pub.1149797765,"An Architecture Combining Blockchain, Docker and Cloud Storage for Improving Digital Processes in Cloud Manufacturing","The Blockchain has been given great attention in recent literature among emerging technologies in software architectures. More specifically, when verifiable transactions between untrusted parties are concerned in a safe and reliable environment, its peculiar decentralized and tamper-proof structure makes it suitable for a vast class of business domains, such as Cloud Manufacturing, which is a new paradigm in the industry based on cloud technologies. However, the stiffness of existing solutions, that are unable to provide and implement heterogeneous services in a Cloud environment, emphasizes the need of a standard framework to overcome this limit and improve collaboration. Firstly, this paper introduces a Blockchain based platform designed with Smart Contracts for improving digital processes in a manufacturing environment. The primary contribution is the integration of two popular cloud technologies within the Blockchain: Docker, a scalable platform to run applications in lightweight environments, and Cloud Storage. Each process available in the platform requires input files and produces output files by using cloud storage as a repository and it is delivered by the owner as a self-contained Docker image, whose digest is safely stored in the chain. Secondly, with the purpose of selecting the fastest node for each new process instance required by consumers, we introduce a task assignment problem based on a deep learning approach and past metrics. The proposed platform is applied to a real-world industrial case study regarding ophthalmic lenses manufacturing and the optimization of lens surface calculation."
pub.1171443725,Performance Analysis of RESTFUL Web Services and RABBITMQ for Microservices based Systems on Cloud Environment,"Nowadays, every other company is switching or wants to switch from monolithic to microservices architecture, which comes with its own set of challenges. One major challenge is inter-service communication, because when we think of microservices, we expect a collection of separate services communicating with one another to provide the desired output/result and making the system functional in the expected way. There are various modes of communication and at times selecting the appropriate mechanism for interservice communication could be difficult for developers/architects, as it is critical to the overall functionality and performance of the application. The purpose of this research is to compare synchronous RESTful web services and asynchronous RabbitMQ for microservices based systems on cloud ecosystem, exploring their key features, advantages and disadvantages, and evaluating their performance in different use cases on cloud."
pub.1139044356,Cloud doctrine: impact on cloud adoption in the government organizations of India,"
                    Purpose
                    Government is the biggest spender on cloud computing technology but a very limited study and data sets are available to assess the cloud adoption trends in government organizations in India. As India is ushering towards “Digital India” it becomes essential for the government to embrace the cloud to enhance governance and meet the citizen expectations. This paper aims to discuss the evolution of cloud computing (Meghraj) in government organizations by examining the various information technology (IT) and cloud policies, thereby focusing on the policy gaps. The second part of this study assesses the cloud adoption trend by analyzing adopted cloud services, deployments models, leading sectors in cloud adoption and cloud approach. Eventually, in consultation with experts, a conceptual framework for cloud adoption in the government organizations of India is developed for wider cloud adoption.
                  
                  
                    Design/methodology/approach
                    The authors reviewed various IT/cloud policies and related literature to find the policy gaps for slow cloud adoption in government organizations. Authors have researched to collect the data from the various government procurement portals and analysed the tender and contracts of 500 organizations for cloud requirements to infer the cloud adoption trends. Based on the review of policy gaps, adoption trends and by consulting the experts a conceptual cloud adoption framework has been developed for wider cloud adoption in government organizations.
                  
                  
                    Findings
                    This study can be a pathfinder where the most innovative findings are about the cloud adoption trends in the government organizations in the time frame from 2013 till 2020. Several key findings are – the public cloud are the most widely adopted, infrastructure as a service model is the most used services, the majority of the applications migrating to the cloud are legacy applications, the leading sector in cloud adoption are – IT, transport and education. It is observed that the pandemic Covid-19 has acted as a catalyst and accelerated cloud adoption in government organizations. Eventually, a conceptual cloud adoption framework has been suggested addressing the policy gaps, deficiencies, overcoming the gaps and their related outcomes for the wider cloud adoption in the government organizations.
                  
                  
                    Practical implications
                    The findings of this work highlight the cloud adoption trends in government organizations which can prove vital to the policymakers. This work will assist policymakers, government organizations, researchers, IT professionals and others interested in analyzing the state of cloud adoption. The conceptual cloud adoption framework developed endeavours to uncover the policy gaps, suggest the gap resolu"
pub.1149764017,Efficient Resource Management in Cloud Environment,"In cloud computing resource management plays a significant role in data
centres and it is directly dependent on the application workload. Various
services such as Infrastructure as a Service (IaaS), Platform as a Service
(PaaS), and Software as a Service (SaaS) are offered by cloud computing to
provide compute, network, and storage capabilities to the cloud users utilizing
the pay-per-usage approach. Resource allocation is a prior solution to address
various demanding situations like the under/overload handling, resource
wastage, load balancing, Quality-of-Services (QoS) violations, VM migration and
many more. The primary aim of Virtual Machine Placement (VMP) is mapping of
Virtual Machines (VMs) to physical machines (PMs), such that the PMs may be
utilized to their maximum efficiency, where the already active VMs are not to
be interrupted. It provides a list of live VM migrations that must be
accomplished to get the optimum solution and reduces energy consumption to a
larger extent. The inefficient VMP leads to wastage of resources, excessive
energy consumption and also increase overall operational cost of the data
center. On this context, this article provides an extensive survey of resource
management schemes in cloud environment. A conceptual scheme for resource
management, grouping of current machine learning based resource allocation
strategies, and fundamental problems of ineffective distribution of physical
resources are analyzed. Thereafter, a complete survey of existing techniques in
machine learning based mechanisms in the field of cloud resource management are
explained. Ultimately, the paper explores and concludes distinct approaching
challenges and future research guidelines associated to resource management in
cloud environment."
pub.1166354282,CATS: Cloud-native time-series data annotation tool for intensive care,"Intensive care units are complex, data-rich healthcare environments which provide substantial opportunities for applications in machine learning. While certain solutions can be derived directly from data, complex problems require additional human input provided in the form of data annotations. Due to the large size and complexities associated with healthcare data, the existing software packages for time-series data annotation are infeasible for effective use in the clinical setting and frequently require significant time commitments and technical expertise. Our software provides a comprehensive, end-to-end solution to the time-series data annotation and proposes a novel approach for a semi-automated annotation in the cloud. It allows for conducting large-scale, asynchronous data annotation activities across multiple, geographically distributed users. The adoption of our software could benefit the wider research community by enhancing existing datasets, creating novel avenues for research that uses them and allowing for meaningful data annotation within smaller and highly specialised populations."
pub.1094989594,A Survey of Intrusion Detection Systems for Cloud Computing Environment,"Cloud computing is a newly emerged technology, and the rapidly growing field of IT. It is used extensively to deliver Computing, data Storage services and other resources remotely over internet on a pay per usage model. Nowadays, it is the preferred choice of every IT organization because it extends its ability to meet the computing demands of its everyday operations, while providing scalability, mobility and flexibility with a low cost. However, the security and privacy is a major hurdle in its success and its wide adoption by organizations, and the reason that Chief Information Officers (CIOs) hesitate to move the data and applications from premises of organizations to the cloud. In fact, due to the distributed and open nature of the cloud, resources, applications, and data are vulnerable to intruders. Intrusion Detection System (IDS) has become the most commonly used component of computer system security and compliance practices that defends network accessible Cloud resources and services from various kinds of threats and attacks. This paper presents an overview of different intrusions in cloud, various detection techniques used by IDS and the types of Cloud Computing based IDS. Then, we analyze some pertinent existing cloud based intrusion detection systems with respect to their various types, positioning, detection time and data source. The analysis also gives strengths of each system, and limitations, in order to evaluate whether they carry out the security requirements of cloud computing environment or not. We highlight the deployment of IDS that uses multiple detection approaches to deal with security challenges in cloud."
pub.1156265823,Towards a Domain-Specific Language for Provisioning Multiple Cloud Testing Environments for Mobile Applications,"Towards a Domain-Specific LanguagTowards a Domain-Specific Languag Provisioning testing environments for mobile applications is one of the most significant challenges within the software industry. Due to this high complexity that exists when provisioning test environments within the multiple available cloud platforms, it is necessary to make a significant investment in human resources, like time and effort for the implementation and execution of testing. There is additional complexity: testing software in a single environment is no longer sufficient. Today's mobile industry is constantly growing, and execution environments tend to be always different; the hardware configuration is usually different and sometimes exceeds the software barrier. It is challenging to execute testing on each of the existing devices, as this requires a long task of human intervention. Today some platforms provide testing services in different environments. However, not all providers have the complete set of environments that one would like to have, and specific knowledge is mandatory for using each available tool. It is a task that requires expertise and time. This work seeks to mitigate the impact on time and the learning curve through a high-level tool developed using a model-oriented approach, thus reducing the time needed for setting up each required platform for organizations. As a solution, we propose a Domain-Specific Language for provisioning multiple cloud testing environments for mobile applications. The configuration of the environment is done with the Domain Specific Language to make the usage easier by the final user. The necessary code is generated through transformations to set up an environment on cloud platforms such as Amazon Web Services (AWS) and Google Cloud Platform (GCP). This usage of this platform results in fewer code lines written and less time learning about the specific knowledge for each platform."
pub.1105017413,Improving Efficiency of Edge Computing Infrastructures through Orchestration Models †,"Edge computing is an effective paradigm for proximity in computation, but must inexorably face mobility issues and traffic fluctuations. While software orchestration may provide effective service handover between different edge infrastructures, seamless operation with negligible service disruption necessarily requires pre-provisioning and the need to leave some network functions idle for most of the time, which eventually results in large energy waste and poor efficiency. Existing consolidation algorithms are largely ineffective in these conditions because they lack context, i.e., the knowledge of which resources are effectively used and which ones are just provisioned for other purposes (i.e., redundancy, resilience, scaling, migration). Though the concept is rather straightforward, its feasibility in real environments must be demonstrated. Motivated by the lack of energy-efficiency mechanisms in cloud management software, we have developed a set of extensions to OpenStack for power management and Quality of Service, explicitly targeting the introduction of more context for applications. In this paper, we briefly describe the overall architecture and evaluate its efficiency and effectiveness. We analyze performance metrics and their relationship with power consumption, hence extending the analysis to specific aspects that cannot be investigated by software simulations. We also show how the usage of context information can greatly improve the effectiveness of workload consolidation in terms of energy saving."
pub.1109832373,Evidence-Based Systematic Literature Reviews in the Cloud,"Systematic literature reviews and mapping studies are useful research methods used to lay the foundations of further research. These methods are widely used in the Health Sciences and, more recently, also in Computer Science. Despite existing tool support for systematic reviews, more automation is required to conduct the complete process. This paper describes CloudSERA, a web-based app to support the evidence-based systematic review of scientific literature. The tool supports researchers to carry out studies by additional facilities as collaboration, usability, parallel searches and search integration with other systems, The flexible data scheme of the tool enables the integration of bibliographic databases of common use in Computer Science and can be easily extended to support additional sources. It can be used as a service in a cloud environment or as on-premises software."
pub.1137104493,PTEMagnet: fine-grained physical memory reservation for faster page walks in public clouds,"The last few years have seen a rapid adoption of cloud computing for data-intensive tasks. In the cloud environment, it is common for applications to run under virtualization and to share a virtual machine with other applications (e.g., in a virtual private cloud setup). In this setting, our work identifies a new address translation bottleneck caused by memory fragmentation stemming from the interaction of virtualization, colocation, and the Linux memory allocator. The fragmentation results in the effective cache footprint of the host PT being larger than that of the guest PT. The bloated footprint of the host PT leads to frequent cache misses during nested page walks, increasing page walk latency. In response to these observations, we propose PTEMagnet, a new software-only approach for reducing address translation latency in a public cloud. PTEMagnet prevents memory fragmentation through a fine-grained reservation-based allocator in the guest OS. Our evaluation shows that PTEMagnet is free of performance overheads and can improve performance by up to 9% (4% on average). PTEMagnet is fully legacy-preserving, requiring no modifications to either user code or mechanisms for address translation and virtualization."
pub.1156950573,Scalable Attestation of Virtualized Execution Environments in Hybrid- and Multi-Cloud,"Existing attestation mechanisms lack scalability and support for
heterogeneous virtual execution environments (VEEs), such as virtual machines
and containers executed inside or outside hardware isolation on different
vendors' hardware in clouds managed by various organizations. To overcome these
limitations, hardware vendors and cloud providers implement proprietary
mechanisms (Intel DCAP, Amazon NitroTPM, Google Titan) to support their
offerings. However, due to their plurality, the attestation becomes cumbersome
because it increases maintenance and integration costs and reduces portability
required in hybrid- and multi-cloud deployments.
  We introduce WAWEL, a framework that enables scalable attestation of
heterogeneous VEEs. WAWEL can be plugged into existing hardware-specific
attestation mechanisms, offering a unified interface. WAWEL supports the widely
adopted trusted platform module (TPM) attestation standard. We implemented a
prototype and integrated it with three different VEEs. It supports runtime
integrity attestation with Linux integrity measurement architecture (IMA) and
legacy applications requiring zero-code changes. The evaluation demonstrated
that the WAWEL prototype achieves very good performance and scalability despite
the indirections between the VEE and hardware root of trust."
pub.1035924776,An energy-efficient video transport protocol for personal cloud-based computing,"Recently, we are surrounded by a collection of heterogeneous computing devices such as desktop computers, laptops, smart phones, smart televisions, and tablet PCs. Each device is operated with its own host operating system, has its own internal architecture, and performs its independent tasks with its own applications and data. A common property amongst these devices, namely that of internet-connectivity, allows them to configure personal virtual cloud system by interconnecting each other through an intermediate switching device. The personal cloud service should provide a seamlessly unified computing environment across multiple devices with synchronized data and application programs. As such, it allows users to freely switch their workspace from one device to another while continuing the interaction with the available applications. In order to support video applications, the cloud system should provide seamless video synchronization among the multiple devices. However, we note that the current cloud services do not provide efficient data flow among devices. In this paper, we propose and develop a new reliable transport protocol to support video synchronization for the personal virtual cloud system. In order to cope with battery limitation of many mobile devices in personal virtual cloud system, the proposed protocol is designed to provide energy efficient video communications. Our simulation results show that the proposed protocol can reduce end users power consumption up to 25 % compared to the legacy TCP with given packet loss probabilities and the average length of error bursts."
pub.1117365780,Maintaining Fog Trust Through Continuous Assessment,"Cloud computing continues to provide flexible and efficient way for delivery of services, meeting user requirements and challenges of the time. Software, Infrastructures, and Platforms are provided as services in cloud and fog computing in a cost-effective manner. Migration towards fog instigate new aspects of research for security & privacy. Trust is dependent on measures taken for availability, security, and privacy of users’ services as well as data in fog as well as sharing of these statistics with stakeholders. Any type of lapses in measures for security & privacy shatter user’s trust. In order to provide a trust worthy security and privacy system, we have conducted a thorough survey of existing techniques. A generic model for trustworthiness is proposed in this paper. This model yields a comprehensive component-based architecture of a trust management system to aid fog service providers to preserve users’ Trust in a fog computing environment."
pub.1101908899,Existing and Relevant Methodologies for Energy Efficient Cloud Data Centers,"Cloud computing is a novel paradigm which defines a new style of deploying systems. It realizes the vision of delivering Computing as a Utility. The demand for Cloud infrastructure has expanded with rapid growth of computing applications and data thereby amplifying energy-consumption. These results in heightened cost of ownership, reduced return on investment, narrowed profit margin, decreased reliability and availability of datacenter resources, and above all it adversely affects the environment by increases carbon footprint. Hence there is need for green computing and for energy-efficient management of cloud datacenter resources besides meeting the QoS constraints. Within a datacenter, maximum energy is consumed by cooling systems and ICT Infrastructure, particularly, the servers. So it is of utmost importance to optimize energy utilization in datacenter servers. In order to achieve this, we can leverage the power of virtualization which is intrinsic in Cloud Computing. Virtualization opens the doors for VM consolidation by allowing dynamic migration of virtual machines across physical machines. In this paper, give the detailed explanation of existing and relevant methodologies for energy efficient cloud data centers. Also explain limitations of existing methodologies and techniques."
pub.1152348050,Accelerated FDP Reservoir Studies in Challenging Brownfields Utilizing Digital Could Technologies,"Abstract
                  PETRONAS Baronia field is a mature oil field with over 45 years of production history, located offshore Sarawak, Malaysia. It consists of several vertically stacked clastic sandstone reservoirs, namely two major reservoirs: S and V2 reservoirs. Both reservoirs have been on production since 1970's with the production strategy evolving over the years to maximize recovery. Natural depletion, infill drilling, water and gas injection, and recently Immiscible Water-Alternating-Gas (IWAG) IOR/EOR strategies have been implemented. All these elements combined with the subsurface uncertainties pose challenges to history match and to conduct probabilistic forecast studies on the dynamic models. Conventionally, the development scenarios for subsurface investigation are limited due to finite computing resources. As PETRONAS is shifting its portfolios to develop more complex and challenging fields, the need for transformation in development concept evaluation is evident. This is key for proper risk and uncertainties quantification. The notable challenges are a) limited number of development scenarios being investigated, evaluated, and compared; b) limited software licenses and infrastructure availability; c) lack of data and decisions traceability.
                  These limitations are addressed by the PETRONAS LiveFDP digital transformation initiative commenced in 2019, through deployment of digital cloud technologies and solutions with scalable High- Performance Computing (HPC) environment. The cloud-based native and Petrotechnical applications enable remote work, ensure full data traceability and auditability, enable multi-realization ensemble analysis, and streamline the automated integration from the reservoir engineering ensemble workflow to economic analysis. Unlimited cloud computing power and licenses facilitate a broader spectrum of reservoir simulation cases to be investigated in a fast-tracked manner.
                  The cloud HPC infrastructure has shortened the history matching cycle from 3 months to 1.5 months. The team has also observed over 5 times speed enhancement on simulation run performance using cloud computing compared to virtual machine and on-premise infrastructure. Utilizing the cloud solutions and ensemble probabilistic approach, the team has achieved over 90% of history match quality through 300 realizations per ensemble running concurrently and completed within 2 hours. The optimized IWAG injection resulted in 2% (~1MMStb) higher oil reserves with 37% less gas injection and 40% shorter injection cycles. This has improved gas sales and prioritization in the field while also monetizing the oil
                  reserves. The ensemble analyses are then visualized using cloud-based data analytics system whereby key realizations and uncertainty parameters are further reviewed and highlighted across various disciplines collaboratively at real time."
pub.1120521733,ReplicaTEE: Enabling Seamless Replication of SGX Enclaves in the Cloud,"With the proliferation of Trusted Execution Environments (TEEs) such as Intel SGX, a number of cloud providers will soon introduce TEE capabilities within their offering (e.g., Microsoft Azure). The integration of SGX within the cloud considerably strengthens the threat model for cloud applications. However, cloud deployments depend on the ability of the cloud operator to add and remove application dynamically; this is no longer possible given the current model to deploy and provision enclaves that actively involves the application owner. In this paper, we propose ReplicaTEE, a solution that enables seamless commissioning and decommissioning of TEE-based applications in the cloud. ReplicaTEE leverages an SGX-based provisioning service that interfaces with a Byzantine Fault-Tolerant storage service to securely orchestrate enclave replication in the cloud, without the active intervention of the application owner. Namely, in ReplicaTEE, the application owner entrusts application secret to the provisioning service; the latter handles all enclave commissioning and decommissioning operations throughout the application lifetime. We analyze the security of ReplicaTEE and show that it is secure against attacks by a powerful adversary that can compromise a large fraction of the cloud infrastructure. We implement a prototype of ReplicaTEE in a realistic cloud environment and evaluate its performance. ReplicaTEE moderately increments the TCB by ≈800 LoC. Our evaluation shows that ReplicaTEE does not add significant overhead to existing SGX-based applications."
pub.1154300773,Ensure secured data transmission during virtual machine migration over cloud computing environment,"The explosive growth of cloud usage encourages several challenges, especially high energy consumption of Cloud Data Centers, new security risks to Virtual Machines (VMs) resulting from co-residency with other risky VMs on the same Physical Machine, and the Quality of Service (QoS) degradation due to sharing resources. Many recent studies have used Dynamic VM Consolidation to save energy with minimum degradation of the QoS. However, the existing system has issue with security data transmission during VM migration over cloud environment. To overcome the above mentioned issue, in this work, Software Defined Networks (SDN) based Capability and Access Control service (CAC) scheme is proposed. This work contains main phases are such as system model, Resource Allocation (RA) using Adaptive Fire FLY Algorithm (AFFA) and ensure secured data transmission using SDN based CAC scheme.The system model contains data center model, migration request model and energy model over the cloud computing environment. Then the RA is done by using AFFA which is focused to select optimal resources. Adaptively it updates best resource allocation for the corresponding cloud requirements which meets QoS efficiently. AFFA is focused to satisfy the user requirements effectively using best fitness values (QoS services) such as lower delay, cost energy consumption and fast response. Then the secured data transmission is performed using SDN based CAC scheme andCAC framework builds collaboration between hypervisor & SDN Controller to drive the secured VM migration from source host to destination host. It enhances the security level while data transmission and also it is used for better access control to the authorized users. The simulation result proves that the proposed SDN based CAC scheme provides better performance in terms of higher reliability, throughput and lower energy consumption, cost complexity and time complexity than the existing methods."
pub.1124072608,Load balancing in cloud computing environments based on adaptive starvation threshold,"Summary Clouds provide to users on‐demand access to large computing and storing resources and offer over on premise IT infrastructures many advantages in terms of cost, flexibility, and availability. However, this new paradigm still faces many challenges, and in this paper, we address the load balancing problem. Even though many approaches have been proposed to balance the load among the servers, most of them are too sensitive to the fluctuation in the clouds load and produce unstable systems. In this paper, we propose a new distributed load balancing algorithm, based on adaptive starvation threshold. It tries to balance the load between the servers while minimizing the response time of the cloud, maximizing the utilization rate of the servers, decreasing the overall migration cost, and maintaining the stability of the system. The performance of the proposed algorithm was compared to a well‐known load balancing algorithm, inspired from the honey bee behavior (HBB). The experimental results showed that the application of the proposed load balancing algorithm gives considerable performance gains and a significant reduction in number of migrations when compared to the performance of the HBB algorithm."
pub.1165720663,Towards a Benchmark for Fog Data Processing,"Fog data processing systems provide key abstractions to manage data and event processing in the geo-distributed and heterogeneous fog environment. The lack of standardized benchmarks for such systems, however, hinders their development and deployment, as different approaches cannot be compared quantitatively. Existing cloud data benchmarks are inadequate for fog computing, as their focus on workload specification ignores the tight integration of application and infrastructure inherent in fog computing. In this paper, we outline an approach to a fog-native data processing benchmark that combines workload specifications with infrastructure specifications. This holistic approach allows researchers and engineers to quantify how a software approach performs for a given workload on given infrastructure. Further, by basing our benchmark in a realistic IoT sensor network scenario, we can combine paradigms such as low-latency event processing, machine learning inference, and offline data analytics, and analyze the performance impact of their interplay in a fog data processing system."
pub.1092861919,Integration of Static Worst-Case Execution Time and Stack Usage Analysis for Embedded Systems Software in a Cloud-Based Development Environment,"New applications relying on embedded systems technologies often come with an increased number of features and functionalities. For instance, improved safety, reliability, usability or reduced power consumption are commonly encountered aspects. Those in turn, however, come usually at the cost of increased complexity. Managing the latter can become challenging, especially when looking at (worst-case) execution times or memory usage of embedded systems. In particular, many applications, e.g., safety-critical or real-time applications, require knowledge about the worst-case execution time and stack usage to make a clear statement on important system parameters such as the overall performance or schedulability with regard to critical deadlines. Assessing these properties require elaborate tool support and profound knowledge and skills of the developers. In this paper, an evaluation of static analysis tools and the required steps to integrate these in a existing development environment is presented. The toolchain is either considered to be offline or deployed within a cloud-based integrated development environment. The cloud-approach enables ubiquitous access to the results and a unique visualization across multiple platforms. Additionally, the results are demonstrated along with a small use case.Copyright © 2017 by ASME"
pub.1129139669,Determinants of cloud ERP adoption in Jordan: an exploratory study,"Enterprise resource planning (ERP) systems are usually adopted through two major options: cloud computing or on-premise infrastructure. The issues related to hardware, servers, implementation costs, and facilities, which are necessary to run on-premise ERP systems, are rather great for small and mid-sized enterprises (SMEs). The purpose of this research is to highlight the factors influencing cloud ERP adoption among SMEs. The technological, organisational, and environmental (TOE) model will be used as a theoretical base. This qualitative, exploratory study gathers data from 13 different SMEs and cloud ERP service providers through semi-structured interviews, focusing on SMEs in Jordan. According to our results, support from both service providers and top management within the company play an important role in whether a firm adopts cloud ERP services, in addition to a number of other factors at varying rates. Research findings can be used by service providers and business owners to enhance their approach to cloud ERP by showing the reasons why some SMEs chose to adopt this technology and others did not. Cloud ERP providers need to intensify their efforts to build a progressive environment for their services that will eliminate any ambiguity regarding this type of technology."
pub.1167169733,"Recent Trends, Issues and Challenges in Container and VM Migration","The enabling of businesses to move between multiple cloud environments, even when there is a need to recreate applications on different clouds, is facilitated by Inter Cloud (C2C) migration. During C2C migration, both Virtual Machines (VMs) and Containers get transferred from one remote environment to another. However, in traditional migration strategies, the VMs are temporarily halted, resulting in the unavailability of services operating on several VMs and Containers during the Cloud-to- Cloud transfer. In Dynamic Resource Management (DRM), VM and container migration play essential roles as components. This paper aims to review recent trends in migration for both forms of virtualization. Surprisingly, there is currently no comprehensive review that explores the dynamics influencing the adoption of Virtual Machine and container technology for virtualization. The lack of overview hampers the quick and informed selection of technology. To address the existing knowledge gap and identify the most recent advancements in Virtual Machine and Container technologies and research trends. This paper presents a comprehensive literature review and valuable insights that facilitate informed decision-making in the realm of Virtual Machine and Container migration."
pub.1158050249,"VMware Cloud on AWS, Insights on the First VMware Enterprise-Proven SaaS Solution","Learn how VMware has brought its enterprise-class software-defined Data Center software to the AWS Cloud. This book will show how to enable customers to run production applications across VMware vSphere®-based private, public, and hybrid cloud environments, with optimized access to AWS services. The book begins by introducing the value of VMware Cloud on AWS and talking about the different use cases that can be addressed by this SaaS solution and how it can accelerate cloud migration. In subsequent chapters, you’ll adopt a more pragmatic approach with practical examples of how to successfully plan, design and deploy VMware Cloud on AWS. VMware Cloud on AWS also covers technical requirements as well as the different options to prepare for a successful deployment and make the right decision to interconnect the solution to any existing environment through a dedicated link. In a dedicated chapter, it approaches the challenge of migrating workloads on the platform and address all the current capabilities offered by the solution and more specifically HCX, the Hybrid Cloud Extension offering that works in concert with VMware Cloud on AWS to facilitate moving complex workloads to the cloud. You’ll also review the advanced networking and security options available in the platform that help reinforce the level of security with features like traditional Gateway Firewall, Distributed Firewall, Micro-segmentation, Identity Firewall, and IDS/IPS. The book concludes with best practices to address operation tasks and get valuable insights into the VMware Cloud on AWS deployment. You will: Plan and Deploy VMware Cloud on AWS Migrate your workloads to the cloud with VMware Cloud on AWS and HCX Secure workloads on VMware Cloud on AWS Operate VMware Cloud on AWS"
pub.1012653095,A GENTL Approach for Cloud Application Topologies,"The availability of an increasing number of cloud offerings allows for innovative solutions in designing applications for the cloud and in adapting existing ones for this environment. An important ingredient in identifying the optimal distribution of an application in the cloud, potentially across offerings and providers, is a robust topology model that can be used for the automated deployment and management of the application. In order to support this process, in this work we present an application topology language aimed for cloud applications that is generic enough to allow the mapping from other existing languages and comes with a powerful annotation mechanism already built-in. We discuss its supporting environment that we developed and show how it can be used in practice to assist application designers."
pub.1121662606,ACT Testbot and 4S Quality Metrics in XAAS Framework,"The purpose of this paper is to analyze all Cloud based Service Models, Continuous Integration, Deployment and Delivery process and propose an Automated Continuous Testing and testing as a service based TestBot and metrics dashboard which will be integrated with all existing automation, bug logging, build management, configuration and test management tools. Recently cloud is being used by organizations to save time, money and efforts required to setup and maintain infrastructure and platform. Continuous Integration and Delivery is in practice nowadays within Agile methodology to give capability of multiple software releases on daily basis and ensuring all the development, test and Production environments could be synched up quickly. In such an agile environment there is need to ramp up testing tools and processes so that overall regression testing including functional, performance and security testing could be done along with build deployments at real time. To support this phenomenon, we researched on Continuous Testing and worked with industry professionals who are involved in architecting, developing and testing the software products. A lot of research has been done towards automating software testing so that testing of software product could be done quickly and overall testing process could be optimized. As part of this paper we have proposed ACT TestBot tool, metrics dashboard and coined 4S quality metrics term to quantify quality of the software product. ACT testbot and metrics dashboard will be integrated with Continuous Integration tools, Bug reporting tools, test management tools and Data Analytics tools to trigger automation scripts, continuously analyze application logs, open defects automatically and generate metrics reports. Defect pattern report will be created to support root cause analysis and to take preventive action."
pub.1119283419,ReplicaTEE: Enabling Seamless Replication of SGX Enclaves in the Cloud,"With the proliferation of Trusted Execution Environments (TEEs) such as Intel
SGX, a number of cloud providers will soon introduce TEE capabilities within
their offering (e.g., Microsoft Azure). Although the integration of SGX within
the cloud considerably strengthens the threat model for cloud applications, the
current model to deploy and provision enclaves prevents the cloud operator from
adding or removing enclaves dynamically - thus preventing elasticity for
TEE-based applications in the cloud.
  In this paper, we propose ReplicaTEE, a solution that enables seamless
provisioning and decommissioning of TEE-based applications in the cloud.
ReplicaTEE leverages an SGX-based provisioning layer that interfaces with a
Byzantine Fault-Tolerant storage service to securely orchestrate enclave
replication in the cloud, without the active intervention of the application
owner. Namely, in ReplicaTEE, the application owner entrusts application secret
to the provisioning layer; the latter handles all enclave commissioning and
de-commissioning operations throughout the application lifetime. We analyze the
security of ReplicaTEE and show that it is secure against attacks by a powerful
adversary that can compromise a large fraction of the cloud infrastructure. We
implement a prototype of ReplicaTEE in a realistic cloud environment and
evaluate its performance. ReplicaTEE moderately increments the TCB by ~800 LoC.
Our evaluation shows that ReplicaTEE does not add significant overhead to
existing SGX-based applications."
pub.1174583334,Data Migration between on prim to Cloud using Generative AI to Reduce Costing And Overheads,"“Cloud data migration” describes the method of transferring digital data to new cloud services or garage structures. Data have to be transferred in a way that guarantees it remains available, secure, and uncompromised. Businesses might also easily adjust to new needs and trends in era by way of transferring their statistics, apps, or workloads to the cloud. Scalability, adaptability, and optimising charges are all made feasible by this. Generative AI can help us evaluate existing applications and provide insights into how they can be optimized for the target cloud environments. In the ever-evolving world of technology, Generative AI (GenAI) has emerged as a transformative force, poised to revolutionize various industries and domains, including cloud migration. As the global landscape of migration continues to shift, GenAI is prepared to play an essential role in streamlining processes, enhancing accuracy, and improving overall migration outcomes. The review present paper investigates the utilisation of generative AI in order to facilitate the transfer of data from on-premises to the cloud. While highlighting the potential of generative AI to automate transfer operations, it tackles obstacles such as economic implications and security concerns. Migration techniques, the benefits of cloud migration, and pertinent literature are examined in this study, which also provides a summary of key findings and recommendations for future research."
pub.1084931095,A Proposed Conceptual Framework to Migrate Microsoft Technology Based Applications to Cloud Environment,"With the evolution of ICT in different business domain, it has become essential to almost all the IT project stakeholders to move applications into cloud for saving IT cost and ensuring sustainability of IT solutions for future. On this fact, it is already proven that cloud implementation has given ample opportunity to reduce overall IT cost of the organization. In cloud implementation, Project stakeholders would have to bear only cost which is project owners ought to pay according to uses of IT resources on pay-per-use cost model. But, in the cloud implementation journey, the most impacting and challenging activity is to do migration of existing application to cloud platform smoothly without interruption of existing application’s user’s experiences and on-going business activities. In this study, authors’ main focus is to propose conceptual framework for assisting to do migration of IT projects in cloud platform. This generic framework will mostly cover End to End processes which require migrating Microsoft based technology applications those are mostly developed in Dot Net and SQL server technology and deployed in Internet Information Server (IIS). The holistic objective of the proposed framework is to facilitate business to provide faster, more reliable, robust and cost effective migration process. This study also has taken in scope to develop very simple proof of concept (POC) to do some sort of validity of the proposed idea."
pub.1095251193,Nomad: A Framework for Developing Mission-Critical Cloud-based Applications,"The practicality of existing techniques for processing encrypted data stored in untrusted cloud environments is a limiting factor in the adoption of cloud-based applications. Both public and private sector organizations are reluctant to push their data to the cloud due to strong requirements for security and privacy of their data. In particular, mission-critical defense applications used by governments do not tolerate any leakage of sensitive data. In this paper, we propose Nomad, a framework for developing mission-critical cloud-based applications. The framework is comprised of: 1) a homomorphic encryption-based service for processing encrypted data directly within the untrusted cloud infrastructure, and 2) a client service for encrypting and decrypting data within the trusted environment, and storing and retrieving these data to and from the cloud. Both services are equipped with GPU-based parallelization to accelerate the expensive homomorphic encryption operations. To evaluate the Nomad framework, we developed CallForFire, a mission-critical application which enables defense personnel to call for fire on targets. Due to the nature of the mission, this application requires guaranteed security. The experimental results highlight the performance enhancements of the GPU-based acceleration mechanism and the feasibility of the Nomad framework."
pub.1048330946,Dashboard Services for Pragmatics-Based Interoperability in Cloud and Ubiquitous Manufacturing,"<p>The real Cloud and Ubiquitous Manufacturing systems require effectiveness and permanent availability of resources, their capacity and scalability. One of the most important problems for applications management over cloud based platforms, which are expected to support efficient scalability and resources coordination following SaaS implementation model, is their interoperability. Even application dashboards need to easily incorporate those new applications, their interoperability still remains a big problem to override. So, the possibility to expand these dashboards with efficiently integrated communicational cloud based services (cloudlets) represents a relevant added value as well as contributes to solving the interoperability problem. Following the architecture for integration of enriched existing cloud services, as instances of manufacturing resources, this paper: a) proposes a cloud based web platform to support dashboard integrating communicational services, and b) describe an experimentation to sustain the theory that the effective and efficient interoperability, especially in dynamic environments, could be achieved only with human intervention.</p>"
pub.1119458235,Survey on Security Issues in Cloud Computing and Associated Mitigation Techniques,"Cloud Computing holds the potential to eliminate the requirements for setting
up of high-cost computing infrastructure for IT-based solutions and services
that the industry uses. It promises to provide a flexible IT architecture,
accessible through internet for lightweight portable devices. This would allow
multi-fold increase in the capacity or capabilities of the existing and new
software. In a cloud computing environment, the entire data reside over a set
of networked resources, enabling the data to be accessed through virtual
machines. Since these data-centers may lie in any corner of the world beyond
the reach and control of users, there are multifarious security and privacy
challenges that need to be understood and taken care of. Also, one can never
deny the possibility of a server breakdown that has been witnessed, rather
quite often in the recent times. There are various issues that need to be dealt
with respect to security and privacy in a cloud computing scenario. This
extensive survey paper aims to elaborate and analyze the numerous unresolved
issues threatening the cloud computing adoption and diffusion affecting the
various stake-holders linked to it."
pub.1136907937,Auto-scaling techniques for IoT-based cloud applications: a review,"Cloud and IoT applications have inquiring effects that can strongly influence today’s ever-growing internet life along with necessity to resolve numerous challenges for each application such as scalability, security, privacy, and reliability. During the deployment of IoT-based Cloud applications, the demand for Cloud tenants is dynamic that makes challenging to maintain scalability of the system. Developing an effective scaling technique is not merely a big concern, but how to achieve autonomic scaling results using future load prediction and migration policies is also a crucial phase. Also, to evaluate such auto-scaling strategy, certain Quality of Service (QoS) metrics must be recognized, explored and leveraged to enhance the performance of the system. Therefore, in this paper, a survey of existing auto-scaling, load prediction and VM migration techniques for IoT-based Cloud applications has been carried out along with the evaluation of various QoS parameters. Further, the future trends have also been discussed for performing auto-scaling in a Cloud environment."
pub.1022276298,An Adaptive Framework for Applying Cloud Computing in Virtual Learning Environment at Education a Case Study of “AASTMT”,"This study aims to provide a Cloud Computing (CC) proposed framework using Application programming interface (API's) to deliver connectivity and interaction of Software as a Service (SaaS) in Virtual Learning Environment (VLE) system at higher education institute. The framework is adopted and implemented to enhance the existing VLE to meet the incremental increasing of users’ needs and expectations. Different research's methodologies and techniques are used to measure the students and instructors satisfaction, and to measure the impact of the adoption of CC on business value for VLE as well. In addition, the study identifies and explores the idea of covering the gap between the advance of adopting CC as a new technology and the benefits of implementing cloud techniques in education. The findings of implementing the adopted framework equate the study expectations, where the user's satisfaction significantly increased compared with the existing system. The users found that the system performance and response to their tasks are improved. Meanwhile, the users found that the new adopted system make it easier for them to achieve their academic activities and goals."
pub.1133883793,Temporal-Perturbation Aware Reliability Sensitivity Measurement for Adaptive Cloud Service Selection,"Benefiting from the pay-as-you-go business model, cloud-based software applications are becoming more and more popular. A composite cloud system can be constructed by integrating existing component cloud services available over the internet as its system components. In order to fulfill the service-level agreements (SLAs), as well as users’ quality of experience (QoE), a stable execution of the constructed system is desirable in the long term. To achieve this goal, system components at high risk of failing must be identified and fault-tolerated. This is extremely challenging in the dynamic cloud environment that host the component cloud services. However, existing approaches are constrained by their lack of modeling and analysis of system components’ fluctuating reliability time series. To systematically address these issues, in this article, we propose PARS, a perturbation-aware approach, for measuring the reliability sensitivity of component cloud services. It first analyzes the negative perturbations in component cloud services’ historical reliability time series. Then, it calculates the reliability sensitivity of the component cloud services by analyzing how their reliability perturbations impact the reliability of the entire cloud system. Based on PARS, we propose a proactive adaptation approach for constructing and operating composite cloud systems with 1-out-of-2 N-version Programming fault-tolerance. This approach takes the reliability sensitivity of component cloud services estimated by PARS as input to assure the reliability of the cloud system. The results of experiments conducted on two widely used datasets demonstrate the effectiveness and efficiency of the proposed approaches in ensuring the reliability of composite cloud systems."
pub.1150761444,Model integration methods for hydro-model platform under cloud computing environments,"Computing platforms providing cloud simulation services have raised new challenges on the model integration. Unlike calls to the model programs (components) in traditional simulation software, here the models should be dynamically integrated in the “plug and play” mode regardless of the differences in model type and developer. To this end two integration methods have been proposed, i.e., coarse-grained EXE integration and interactive integration. In an EXE integration method, the simulation program is directly called and thus only a data conversion interface is needed while rewriting of the model source code is not required. In contrast, an interactive integration method wraps the model components using the standard wrapper with communication interfaces, and therefore, it can communicate and exchange data with the platform in a real time. The first method is suitable for the integration of legacy models, while the second one can control the progress of simulation schemes and facilitate the scheduling of computing resources. Examples of the model integration and platform application have been presented in hydraulics/hydrodynamics to demonstrate the effectiveness of the integration method and the cloud computing platform."
pub.1149604369,A Continuous Risk Assessment Methodology for Cloud Infrastructures,"Cloud systems are dynamic environments which make it difficult to keep track of security risks that resources are exposed to. Traditionally, risk assessment is conducted for individual assets to evaluate existing threats-their results, however, are quickly outdated in such a dynamic environment. In this paper, we propose an adaptation of the traditional risk assessment methodology for cloud infrastructures which loosely couples manual, in-depth analyses with continuous, automatic application of their results. These two parts are linked by a novel threat profile definition that allows to reusably describe configuration weaknesses based on properties that are common across assets and cloud providers. This way, threats can be identified automatically for all resources that exhibit the same properties, including new and modified ones. We also present a prototype implementation which automatically evaluates an infrastructure as code template of a cloud system against a set of threat profiles, and we evaluate its performance. Our methodology not only enables organizations to reuse their threat analysis results, but also to collaborate on their development, e.g. with the public community. To that end, we propose an initial open-source repository of threat profiles."
pub.1052932818,Enabling secure VM-vTPM migration in private clouds,"The integration of Trusted Computing technologies into virtualized computing environments enables the hardware-based protection of private information and the detection of malicious software. Their use in virtual platforms, however, requires appropriate virtualization of their main component, the Trusted Platform Module (TPM) by means of virtual TPMs (vTPM). The challenge here is that the use of TPM virtualization should not impede classical platform processes such as virtual machine (VM) migration. In this work, we consider the problem of enabling secure migration of vTPM-based virtual machines in private clouds. We detail the requirements that a secure VM-vTPM migration solution should satisfy in private virtualized environments and propose a vTPM key structure suitable for VM-vTPM migration. We then leverage on this structure to construct a secure VM-vTPM migration protocol. We show that our protocol provides stronger security guarantees when compared to existing solutions for VM-vTPM migration. We evaluate the feasibility of our scheme via an implementation on the Xen hypervisor and we show that it can be directly integrated within existing hypervisors. Our Xen-based implementation can be downloaded as open-source software. Finally, we discuss how our scheme can be extended to support live-migration of vTPM-based VMs."
pub.1181186465,Developing AI Applications for the HPC-Cloud Continuum with ColonyOS,"Artificial Intelligence (AI) and machine learning have seen significant growth in recent years, leading to an increased demand for computational resources. To meet this demand and to boost Europe’s competitive edge, the European Commission has built several supercomputers across the continent. However, these traditional supercomputers often lack modern APIs and automation tools necessary for AI development. This paper outlines how High-Performance Computing (HPC) systems and cloud platforms can be seamlessly integrated using ColonyOS, an open-source meta-operating system designed to connect and integrate diverse computing environments into a cohesive compute continuum. ColonyOS enables development of AI workflows that are portable across HPC systems and cloud environments, including Kubernetes, Docker, and Slurm. This integration makes it possible to develop automation workflows, such as training AI models on HPC systems and automatically deploy trained models to cloud platforms for inference. The paper details the architecture of ColonyOS and how it can be used to build AI applications that can run in an HPC-Cloud Continuum. This will be exemplified through a satellite image segmentation case study, showcasing the benefits of combining the Leonardo EuroHPC supercomputer with a Kubernetes cluster. Ultimately, ColonyOS paves the way for hyper-distributed AI applications that can seamlessly utilize both cloud and HPC systems, including existing EuroHPC supercomputers."
pub.1174674958,AI-POWERED THREAT DETECTION IN CLOUD ENVIRONMENTS,"This study assesses the effectiveness of artificial intelligence (AI) technologies in enhancing threat detection within cloud environments, a critical component given the escalating security challenges in cloud computing. Leveraging various AI methodologies, including machine learning models, deep learning, and anomaly detection techniques, the research aims to improve the accuracy and efficiency of security systems. These AI methods were applied to a series of simulated threat scenarios across diverse cloud platforms to evaluate their capability in real-time threat identification and mitigation. Results demonstrated a significant enhancement in detection rates and a decrease in false positives, indicating that AI can substantially improve the robustness of cloud security systems against sophisticated cyber threats. The study highlights the transformative potential of AI in cloud security, showing not only improvements in threat detection but also in the speed and reliability of responses to security incidents. Furthermore, the findings advocate for the integration of AI technologies into existing cloud security infrastructures to achieve more dynamic and adaptable security solutions. The conclusion points towards the need for ongoing research into advanced AI applications in cloud security, suggesting future directions such as the development of self-learning security systems and the exploration of AI's predictive capabilities in pre-empting security breaches. This research provides a foundation for further exploration and potential real-world application of AI in securing cloud environments against an increasingly complex landscape of cyber threats."
pub.1137070298,Improving the performance of AI models in tactical environments using a hybrid cloud architecture,"As the Department of Defense (DoD) looks to exploit and scale Artificial Intelligence (AI) capabilities across the warfighting domains, the Army plans to integrate advanced features into many of its combat systems. The benefits of cloud technologies offer promising solutions to these needs. While cloud-based AI-enabled capabilities leverage flexibility, common interfaces, and virtually infinite scale of resources, they suffer from their lack of proximity to the tactical edge. Tactical AI-enabled systems cannot reliably leverage advantages provided by cloud resources due to limited standardized practices for integration of on-premise/edge systems required by the deployed military. Future high-intensity conflict will be fought in a degraded, denied, intermittent, and lowbandwidth (DDIL) digital environment. As a result, tactical AI-enabled systems will be required to operate in a scenario where high speed, reliable cloud access is unavailable. This paper proposes a hybrid-cloud architecture that leverages resources of the cloud, when available, while also maintaining the capability to retrain tactical AI models in the field environment, using on-site computation and storage. The hybrid cloud construct consists of tactical cloud nodes that reside in closer proximity to AI-enabled systems at the edge. They may retain connectivity to the enterprise cloud yet have the ability to provide the common AI development platform and tool sets to support continuous integration, delivery, and deployment. Thus, its ultimate objective is to enable the seamless and expeditious operation of a distributed AI development environment for the Army and DoD that bridges the tactical edge and enterprise cloud."
pub.1132658581,Optimizing Provisioning of LCG Software Stacks with Kubernetes,"The building, testing and deployment of coherent large software stacks is very challenging, in particular when they consist of the diverse set of packages required by the LHC*** experiments, the CERN Beams department and data analysis services such as SWAN. These software stacks comprise a large number of packages (Monte Carlo generators, machine learning tools, Python modules, HEP**** specific software), all available for several compilers, operating systems and hardware architectures. Along with several releases per year, development builds are provided each night to allow for quick updates and testing of development versions of packages such as ROOT, Geant4, etc. It also provides the possibility to test new compilers and new configurations.
                  Timely provisioning of these development and release stacks requires a large amount of computing resources. A dedicated infrastructure, based on the Jenkins continuous integration system, has been developed to this purpose. Resources are taken from the CERN OpenStack cloud; Puppet configurations are used to control the environment on virtual machines, which are either used directly as resource nodes or as hosts for Docker containers. Containers are used more and more to optimize the usage of our resources and ensure a consistent build environment while providing quick access to new Linux flavours and specific configurations.
                  In order to add build resources on demand more easily, we investigated the integration of a CERN provided Kubernetes cluster into the existing infrastructure. In this contribution we present the status of this prototype, focusing on the new challenges faced, such as the integration of these ephemeral build nodes into CERN’s IT infrastructure, job priority control, and debugging of job failures."
pub.1012047429,A Classification of Intrusion Detection Systems in the Cloud,"Security is one of the most prominent challenges that hinder the acceleration of cloud adoption. Intrusion detection systems (IDSs) can be used to increase the security level of cloud environments. Therefore, the effectiveness of the IDS is a crucial issue for cloud security. However, the cloud presents new challenges and requirements, including scalability and adaptability, which effective IDSs need to address. Choosing the right deployment architecture significantly impacts the effectiveness of IDSs in the cloud. Additionally, robust IDSs need novel detection techniques to keep up with modern sophisticated attacks that target cloud environments. Hence, it is important to understand the advantages and limitations of different IDSs and how the deployment choice in cloud environments impacts the IDSs' effectiveness. This paper presents a novel classification scheme of the state-of-the-art of intrusion detection approaches in the cloud. This classification sheds light on the existing approaches with respect to the following aspects: deployment architecture and detection technique. We first classify the existing approaches based on their deployment architectures. Then, we present a comparative analysis of these approaches with respect to the detection techniques. We also provide detailed analysis of the strengths and weaknesses of existing approaches. The classification and analysis will help in the selection of the proper deployment architectures and detection techniques of IDSs in cloud environments."
pub.1094424373,Cloud Data Migration Method Based On PSO Algorithm,"Cloud storage system can play an important role in large-scale, and it supports high-performance cloud applications. To cloud storage systems, data migration is key technology to realize the nodes dynamically extensible and elastic load balancing. How to reduce migration cost of time is the problem that cloud service providers need to solve. Existing research efforts were focused on the data migration issues under the non-virtualized environments, which often do not applicable to cloud storage systems. In response to these challenges, we put data migration issues into the load-balancing scenarios to solve. We propose an algorithm based on particle swarm optimization algorithm which can reduces the cost of time. In the experiment, we can use Yahoo services benchmarking YCSB tool which could verify the validity of the method. It is a test framework designed to help users understand the different cloud computing, database performance."
pub.1093875464,Adapted E-Assessment System Based on Cloud Computing,"Using Cloud Computing environment, the e-assessment process becomes an orchestration of a set of dedicated cloud services. In this paper, we propose an architecture for an e-assessment environment based on cloud services. This environment implements an approach that we have proposed to develop a generic e-assessment process which will be adapted to a learner profile. The e-assessment process activities and the adaptation process are implemented as Cloud services. Furthermore, they could be invoked from any existing Learning Management System (LMS)."
pub.1148728594,A Continuous Risk Assessment Methodology for Cloud Infrastructures,"Cloud systems are dynamic environments which make it difficult to keep track
of security risks that resources are exposed to. Traditionally, risk assessment
is conducted for individual assets to evaluate existing threats; their results,
however, are quickly outdated in such a dynamic environment. In this paper, we
propose an adaptation of the traditional risk assessment methodology for cloud
infrastructures which loosely couples manual, in-depth analyses with
continuous, automatic application of their results. These two parts are linked
by a novel threat profile definition that allows to reusably describe
configuration weaknesses based on properties that are common across assets and
cloud providers. This way, threats can be identified automatically for all
resources that exhibit the same properties, including new and modified ones. We
also present a prototype implementation which automatically evaluates an
infrastructure as code template of a cloud system against a set of threat
profiles, and we evaluate its performance. Our methodology not only enables
organizations to reuse their threat analysis results, but also to collaborate
on their development, e.g. with the public community. To that end, we propose
an initial open-source repository of threat profiles."
pub.1118825093,An Application-oriented Model for Wireless Sensor Networks integrated with Telecom Infra,"This paper aims to propose a significant way of remote access and real time
monitoring of a particular geographic area by integrating wireless sensor
clouds with existing Telecom infrastructure and applications built around them
through a gateway. This utility is very potent for environment monitoring in
harsh and inaccessible places like mines, nuclear reactors, etc. We demonstrate
a scaled down version of multi-hop network of wireless sensor nodes and its
integration with existing telecom network infrastructure via a gateway."
pub.1146680145,Whole Slide Image to DICOM Conversion as Event-Driven Cloud Infrastructure,"The Digital Imaging and Communication in Medicine (DICOM) specification is
increasingly being adopted in digital pathology to promote data standardization
and interoperability. Efficient conversion of proprietary file formats into the
DICOM standard format is a key requirement for institutional adoption of DICOM,
necessary to ensure compatibility with existing scanners, microscopes, and data
archives. Here, we present a cloud computing architecture for DICOM conversion,
leveraging an event-driven microservices framework hosted in a serverless
computing environment in Google Cloud to enable efficient DICOM conversion at
scales ranging from individual images to institutional-scale datasets. In our
experiments, employing a microservices-based approach substantially reduced
runtime to process a batch of images relative to parallel and serial
processing. This work demonstrates the importance of designing scalable systems
for enabling enterprise-level adoption of digital pathology workflows, and
provides a blueprint for using a microservice architecture to enable efficient
DICOM conversion."
pub.1158313118,Role of IoT in Smart Homes and Offices,"The digitization of the physical world and obstacles in day‐to‐day life has opened up more avenues in terms of research and innovation for researchers for making our daily lives easier. The important technologies are either a part of cloud computing or the internet of things. Existing applications include scenarios in smart offices and homes. The key effort lies in the integration and control of the device services in a smart environment. The internet of things (IoT) is a technology that allows us to add a device to an inert object, such as physical microelectric hardware units and automobiles, at times covering entire structures over IP networks, enabling them to interconnect. All these devices are embedded with software, sensors and actuators for collecting and exchanging data amongst themselves. Cloud computing plays a crucial part in IoT as associated devices can attain supplementary services like servers, databases, software, networks, analytics and other computing functions that can be operated through the cloud. This chapter discusses the concept of smart offices and homes, which are a part of the smart building environment structure, and the role of IoT and cloud computing in establishing communication amongst devices. It further discusses the components of each technology and their areas of application, and future aspects and limitations."
pub.1039827183,Cloud computing and equal access for all,"Web-2.0 applications use the Web as a platform for delivering end-user applications. This transformation has a profound impact on how applications are authored, deployed and consumed. Software applications in this environment are no longer monolithic --- instead, they are naturally separated into distributed components that implement application and interaction logic. The application logic along with user data resides in the network cloud; the user interface made up of presentation and interaction logic is delivered in a form best suited to the user's needs, e.g., via a universal client such as a Web browser. The advantages of this usage/delivery model for mainstream users has been widely explored in the last 18 months. This keynote focuses on the impact of this transformation on users with special needs. Today, the potential for universal access presented by applications delivered via the Web remains largely unrealized. This is partly due to the impedance mismatch that results from trying to treat interactive Web applications as static Web documents. Eliminating this impedance mismatch requires innovation at all levels of the technology stack with respect to: How Web applications are authored and deployed, How Web applications are consumed, How Web interaction is augmented by adaptive technologies. This keynote will describe some of these challenges and the accompanying opportunities."
pub.1000374543,Critical analysis of vendor lock-in and its impact on cloud computing migration: a business perspective,"Vendor lock-in is a major barrier to the adoption of cloud computing, due to the lack of standardization. Current solutions and efforts tackling the vendor lock-in problem are predominantly technology-oriented. Limited studies exist to analyse and highlight the complexity of vendor lock-in problem in the cloud environment. Consequently, most customers are unaware of proprietary standards which inhibit interoperability and portability of applications when taking services from vendors. This paper provides a critical analysis of the vendor lock-in problem, from a business perspective. A survey based on qualitative and quantitative approaches conducted in this study has identified the main risk factors that give rise to lock-in situations. The analysis of our survey of 114 participants shows that, as computing resources migrate from on-premise to the cloud, the vendor lock-in problem is exacerbated. Furthermore, the findings exemplify the importance of interoperability, portability and standards in cloud computing. A number of strategies are proposed on how to avoid and mitigate lock-in risks when migrating to cloud computing. The strategies relate to contracts, selection of vendors that support standardised formats and protocols regarding standard data structures and APIs, developing awareness of commonalities and dependencies among cloud-based solutions. We strongly believe that the implementation of these strategies has a great potential to reduce the risks of vendor lock-in."
pub.1163215364,Development of an automated system for testing a cloud service for deploying virtual machines using modern monitoring tools,"The object of this study is a service for managing virtual machines in a cloud environment. When developing and operating such a service, it becomes necessary to assess its availability and reliability for compliance with the selected quality level that the client can count on. This paper presents a developed system that allows testing the availability of a cloud service for managing virtual machines. The method of integration with the existing monitoring system at the enterprise using open source software in order to reduce the cost of development and operation is considered. A test case for deploying and removing a virtual machine using a graphical user interface has been developed and implemented, and triggering criteria have been defined. The requirements for the architecture and implementation of the system based on the production statistics of the virtual machine creation service using the Prometheus monitoring system are collected and analyzed. The novelty of the research lies in the development of a new method of testing a cloud service for managing virtual machines in order to increase its reliability and availability. Based on this method, a system for testing virtual machines is described and implemented, as well as a method for integration into the monitoring system of the Intel cloud service. During the operation of cloud environments with the help of this system, problem areas were identified in the architecture of the virtual machine creation service, which made it possible to optimize the system operation in a timely manner. The described method is an effective way to test cloud services, and can also be used to analyze and improve reliability and availability."
pub.1160673285,Adoption Strategy for Cloud Computing in Kenyan Research Institutions,"Cloud computing has transformed the aspect of distributed computing from many other prevailing methods by offering more unlimited benefits, like cutting down computing costs and allowing more capacity and agility. It is viewed as a game-changer in how information technology is provided since it allows computer resources like storage, processing capacity, network infrastructure, and applications to be offered as a service via the Internet. In business, it seems to have become strategic, with the potential for use in crucial parts of an organization's IT infrastructure, as it offers a potential alternative to traditional Enterprise Resource Planning (ERP) systems. Most Kenyan research institutes' systems experienced certain flaws and recognized incidents of system rigidity, which prevented these institutions from efficiently meeting their mandate, especially when they desired to access services from home via the Internet. The Research design that was utilized in the study was a survey research design. The study employed a systematic review approach that examined the current literature on cloud computing adoption strategy. The systematic literature review revealed that existing research was dominated by issues of the characteristics/peculiarities and IT environments of research institutions in Kenya and if they were amenable to cloud solutions, the current cloud computing technologies, adoption approaches and drivers fostering cloud computing and the adoption strategy that met the requirements for cloud computing for Kenyan research institutions. The present research was reviewed for knowledge gap identification purposes and future research opportunities. The researcher developed an adoption strategy favorable to research Institutions that was flexible to the provision of cloud computing resources, enhanced greater cloud computing efficiency, reduced capital expenditure in hardware and software and increased the performance of the research Institution's operations, provided the efficiency of data and information and supported the effectiveness of the management activities since the cloud strategies discussed did not promote cooperation, agility, scalability and flexibility and saving costs by using calculating resources efficiently and optimally. This study employed a purposive sampling approach that picked a sample size of twenty respondents from the KMFRI and related organizations. The analytical review of specifically identified studies exposed the kind of challenges that existed and gave an indication that cloud computing was applicable for Kenyan Research Institutions because of the organization size, possession of required skills to make use of cloud-based services, Government regulations, industry regulations and internal policies necessitated the move to cloud computing strategy. The cloud service providers have adequate capacity to run the services and are ready with the services they provide. Researchers noted that all the research Institu"
pub.1139702168,Recent Advances in Energy Efficient Resource Management Techniques in Cloud Computing Environments,"Nowadays cloud computing adoption as a form of hosted application and
services is widespread due to decreasing costs of hardware, software, and
maintenance. Cloud enables access to a shared pool of virtual resources hosted
in large energy-hungry data centers for diverse information and communication
services with dynamic workloads. The huge energy consumption of cloud data
centers results in high electricity bills as well as emission of a large amount
of carbon dioxide gas. Needless to say, efficient resource management in cloud
environments has become one of the most important priorities of cloud providers
and consequently has increased the interest of researchers to propose novel
energy saving solutions. This chapter presents a scientific and taxonomic
survey of recent energy efficient cloud resource management' solutions in cloud
environments. The main objective of this study is to propose a novel complete
taxonomy for energy-efficient cloud resource management solutions, review
recent research advancements in this area, classify the existing techniques
based on our proposed taxonomy, and open up new research directions. Besides,
it reviews and surveys the literature in the range of 2015 through 2021 in the
subject of energy-efficient cloud resource management techniques and maps them
to its proposed taxonomy, which unveils novel research directions and
facilitates the conduction of future researches."
pub.1093299950,Building IaaS Clouds and the Art of Virtual Machine Management,"Nowadays, Infrastructure as a Service (IaaS) clouds are considered a viable solution for the on-demand provisioning of computational resources. Since its popularization in 2006 by the Amazon Elastic Computing Cloud (EC2), the IaaS paradigm is being adopted by many organizations not only to lease resources from a Cloud provider, but also to implement on-premise IaaS Clouds. The former, and original usage, is usually referred as Public Clouds while the latter is commonly named Private Cloud. Public as well as private clouds have rapidly evolved since the advent of the IaaS cloud paradigm. The public IaaS market has been enriched with multiple providers each one with different price model and offers, Cloud interfaces and APIs and even a disparate features. The private ecosystem is not different and multiple technologies both open-source and private can be used today to build on-premise clouds. Again the features, characteristics and adoption levels greatly vary among these technologies. In this talk we will review the main characteristic of the IaaS model for resource provisioning. In particular, we will analyze one of the key components to fulfill this view, the Virtual Resource management system or Cloud OS. The Cloud OS manages the physical and virtual infrastructures, and commands and controls the service provisioning, by orchestrating the deployment of virtual resources. This analysis is performed using OpenNebula as a paradigmatic example. In particular, the presentation will analyze the challenges of managing compute, storage and networking resources in a distributed environment; present the VM management model adopted by OpenNebula; and study the operation of OpenNebula in clouds environments. Finally, the talk will include the description of the Hybrid Cloud computing model, a paradigm that combines on-premise private clouds with the resources of public clouds. This new model is not yet fully developed, and there are still work to be done before true multi-cloud installations become mature enough to be used in production environments. We will discuss some of them and present some use cases."
pub.1135698222,ОСОБЛИВОСТІ СИСТЕМИ АВТОМАТИЗАЦІЇ ПРОЦЕСІВ ПОБУДОВИ ГІБРИДНОЇ ІНФРАСТРУКТУРИ З ПЕРЕНЕСЕННЯМ ВСІХ КОНФІГУРАЦІЙНИХ ДАНИХ,"The main features of the automated system development for synchronizing user data and various aspects of the Exchange Serverinfrastructure are discussed in the article. The system must make an inventory of the existing environment, make reports, accept data of theenterprise wishes and independently configure the components involved in user migration. The system must also restore all user settings that existedbefore the migration. It is the description of the automation of the building a hybrid infrastructure processes with the transfer of all configuration dataand subsequent automatic configuration of the data migration path in a large enterprise with a distributed infrastructure of data centers.The automation of step-by-step migration is considered, namely: automation of building a hybrid infrastructure based on Microsoft Exchange Server and Office 365. In terms of the method of transfer from the local system to the cloud, ""export / import"" is used to migrate data policies and configurations that do not change often. User data that is migrated by another system is used but not developed. An automated system designed to automate the transfer of data from the local infrastructure in the Exchange Server environment. The software product has a functionality that automates almost all the work on the analysis of local infrastructure and further configuration of the cloud part, which minimizes the impact of the human factor on the success of projects. There is also a module that defines the wishes of the business on the conditions of migration, which allows you to dynamically detect the path of data migration. All data, collected during infrastructure analysis, as well as system user data, is exported by Microsoft Exchange to external media files as CSV files to be used by other modules and as TXT (HTML if required) for a tabular report that analyzed by staff. This kind of development allows many large companies to avoid the problems connected with the complex process of migrating global local infrastructure parameters to the cloud environment and improves the process of enterprise data analysis with subsequent automatic management of data migration in IT environments with complex network infrastructure. Time for pre-migration work is significantly reduced, as well as the probability of mistakes, made by staff, is reduced."
pub.1005978435,A Integration Research of Cloud Component based on PaaS for Enhancing Software Reusability,"This paper will provide the cloud service based on PaaS that can enhance reusability of development in the cloud computing environment. The cloud service based on PaaS is the cloud service of platform in the side of development, which provide the reusable framework service that is beyond the existing development tool or management tool service. This reusable framework service will be enhanced reusability using a variety of distributed services."
pub.1168478103,Cloud Technologies and the Need for Hybrid Cloud Implementation in the Military Environment,"Cloud computing has been widely adopted as the next-generation digitisation model for transforming the organisation into an entity that drives development and innovation. Nowadays, users of computing systems aim to get quick access to the virtual ecosystem and a new experience in cyberspace that seamlessly integrates with the existing services they use today. From the perspective of the military organisation/Military Enterprise, the cloud provides services for users and structures in a scalable, highly reliable and highly available manner, specifically with different security levels associated with the individual profile and functional roles in the organisation. From the end user’s perspective, the cloud provides a simple model for accessing information technology/IT services without the need for the human factor to fully understand the transport/transmission infrastructure and technology used. This article explores how modern military-classified and unclassified cloudnative infrastructures can be secured and managed in a national, military private, or mixed hybrid deployment cyber environment, along with various requirements and considerations for adapting cloud-native applications for military systems. In context, the article provides a simple but comprehensive introduction to the cloud native overview and the major technologies that developers use to build such reliable environments in cyberspace that could be the subject of a feasibility study to implement the concept of hybrid cloud in cyberspace with military use. The material is intended for IT experts, DevOps engineers, CIS (Communications and Information Technology) systems and infrastructure architects, cloud enthusiasts, cloud security experts, and any military professional involved in the development, migration, deployment, and management of current services and operations. of a cloud-native system."
pub.1174223029,Security Threats and Existing Solutions on Cloud Computing,"Cloud computing as the greatest evolution of distributed computing, gets benefitted from the technology advancement due to its data versatility and economies of scale. Cloud computing aims to offer flexible services that can minimize the customer's need for investing in new hardware and software. However, the rapid extension of cloud computing raises some concerns regarding security and privacy of data that appears to be an inhibitor in the adoption of cloud services. It is, therefore, necessary to address these issues and their existing solutions. For this purpose, the present study is engrossed in the security threats to data that have been listed as the top threats in the previous years. Mainly, this study provides a review of top threats to cloud security and the solutions based on different models, frameworks, and algorithms to ensure data security in a cloud environment. The significance of this paper is twofold as it provides an overview of security threats as well as it explores the existing solutions to the problem in data security in cloud computing. The outcomes of this study presented that there is still a need for accurate mechanism or technique that can minimize the security threats faced in cloud computing.   "
pub.1009569076,Considerations for Healthcare Applications in a Platform as a Service Environment,"With the introduction of software as a service projects a fundamental change for the world of the IT services and also for the development of software systems goes on. One of the major challenges for today’s companies is to detect and manage this change and business development. This are not only IT provided internal considerations, but must also be considered for business processes. Software as a service and platform as a service seems at first glance to have only advantages. Users do not need to install software anymore and also the maintenance is eliminated. Developers do not have to worry longer about infra-structure, software requirements and distribution of the results. It seems the perfect world for everyone. But special considerations must be taken into account when developing software for health care and medical solutions. Not only must such software meet highest security standards patient data must never be com-promised or altered unobserved. The primary example under discussion consists of a clinic portal with numerous collaboration possibilities for surgeons.The first question in traditional system development is for the components to be used. This means both hardware and software. This includes for example the question according to the database, the operating system, application server, or also the programming language. All this requires many decisions and a high risk, which affect the development process of the application much later already in advance.With the introduction of the cloud and service platforms all of these decisions are moved to the operator of the platform. The selection of the appropriate platform remains for us as system architects, and application developers. This means a comfortable situation for the projects decision maker. However, a later change between platforms is not easily possible.One of the advantages of a service platform is that infrastructure and platform are provided globally for all users. Modifications allow for adaptations to special requirements. Accordingly, a considerable part of software development consists of the definition of required components in the platform metadata.To develop healthcare applications on a platform, special challenges are faced for the architecture. A big difference is added in the health sector. Here, security issues and the location of the data play a larger role. In the full paper we will discuss this in more detail. A first insight is that it requires not only a single platform. Due to the still existing, identifiable specialization of the platforms we need to connect the strengths from multiple platforms together. In addition, we must comply with common international standards such as HIPAA compliance and FDA compliance.In the case study we consider, the client is one of the leading companies in the development of integrated medical software systems. The client at its sites already deploys salesforce.com. An exclusive portal for doctors, especially neurosurgeons, sh"
pub.1160000943,Data Protection based on Searchable Encryption and Anonymization Techniques,"Data leakage compromises companies’ confidentiality and directly impacts the existing privacy laws, as well as it is necessary to perform a light integration with the legacy systems, in order to not harm the performance of its services. Within this context, this paper presents an innovative cloud system to protect the private data of existing databases (legacy systems of clients) based on Searchable Symmetric Encryption for Databases (SSEDB) and Permutation and Proprieties Maintenance Anonymization (PPM-Anon), attaching a security solution to the existing databases (without any change in these legacy systems). Results from real experiments using a real cloud environment suggest that the proposed solution is suitable for protecting the data without harming the performance of the existing services."
pub.1145845444,BBServerless: A Bursty Traffic Benchmark for Serverless,"Serverless is a mainstream computing mode in modern cloud native systems. Different from traditional monolithic cloud, workloads for Serverless architecture are disaggregated into short-lived and fine-grained functions. In Serverless, functions are usually invoked with a bursty pattern, which means the system needs to deliver these functions at high throughput to meet SLA (Service-level agreement) requirements. To explore bursty traffic implications on Serverless platforms, in this paper, we propose a novel benchmarking suite for serverless systems - BBServerless. BBServerless is designed to capture end-to-end and system-level performance in bursty-traffic workloads which help reveal performance bottlenecks of Serverless platform and guide better architecture design for cloud systems. To demonstrate performance variations on Serverless platforms, we also design a traffic generating algorithm (based on Poisson distribution) for four mainstream cloud workloads, i.e. BigData, Stream processing (STREAM), Web Applications (WebApps), and Machine Learning Inference (MaLI). We conduct experiments with trace-driven simulations in a private cloud environment. With data collected from evaluations, we observe that the performance of time-localized components like CPU migration, branch prediction, and cache is highly correlated with end-to-end workload performance. and publicly available at Github (https://github.com/whoszus/BurstyServerlessBenchmark)."
pub.1014420334,Systematic performance evaluation based on tailored benchmark applications,"Performance (i.e., response time, throughput, resource consumption) is a key quality metric of today's applications as it heavily affects customer satisfaction. SAP strives to identify and fix performance problems before customers face them. Therefore, performance engineering methods are applied in all stages of the software lifecycle. However, especially in the development phase continuous performance evaluations can introduce a lot of overhead for developers which hinders their broad application in practice. In order to evaluate the performance of a certain software artefact (e.g. comparing two design alternatives), a developer has to run measurements that are tailored to the software artefact under test. The use of standard benchmarks would create less overhead, but the information gain is often not sufficient to answer the specific questions of developers. In this industrial paper, we present an approach that enables exhaustive, tailored performance testing with minimal effort for developers. The approach allows to define benchmark applications through a domain-specific model and realizes the transformation of those models to benchmark applications via a generic Benchmark Framework. The application of the approach in the context of the SAP Netweaver Cloud development environment demonstrated that we can efficiently identify performance problems that would not have been detected by our existing performance test infrastructure."
pub.1175519304,Monolith to Microservices: Refractor A Java Full Stack Application for Serverless AI Deployment in The Cloud,"Radix, the original monolithic Java full-stack application that would be refactored and deployed in a serverless environment, is described in this project as a Java application with web, API, email, message processing, and batch services. The goals of the transformation process include borrowing from the best practices in Big Data Architecture and improving computational scalability, execution time, system availability, and system cost. Such planning, decomposition, and implementation are grounded by specifying the advantages of microservices and serverless computing. It also discusses data consistency, distributed transactions, security, inter-service communication, and how to solve the problem of central monitoring and logging efficiently. The project regularly handles 1000s of requests and achieves significant performance gains with opex-cost savings through serverless technology like AWS Lambda and the integration of AI models. Based on the results shown, modern architectural patterns can be seen as the critical enabler of highly maintainable, scalable, and efficient applications."
pub.1124668427,Efficient resourceful mobile cloud architecture (mRARSA) for resource demanding applications,"For mobile clients, sufficient resources with the assurance of efficient performance and energy efficiency are the core concerns. This article mainly considers this need and proposes a resourceful architecture, called mRARSA that addresses the critical need in a mobile cloud environment. This architecture consists of cloud resources, mobile devices, and a set of functional components. The performance efficiency evaluates implementing the proposed context-aware multi-criteria decision offloading algorithm. This algorithm considers both device context (network parameters) and application content (task size) at run time when offloading an executable code to allocate the cloud resources. The appropriate resources select based on offloading decisions and via the wireless communication channels. The architecture’s remarkable component is the signal strength analyzer that determines the signal quality (e.g.-60 dBm) and contributes to performance efficiency. The proposed prototype model has implemented several times to monitor the performance efficiency, mobility, performance at communication barriers, and the outcomes of resource-demanding application’s execution. Results indicate performance improvement, such as the algorithm appropriately decides the cloud resources based on device network context, application content, mobility, and the signal strength quality and range. Moreover, the results also show significant improvement in achieving performance and energy efficiency. Sufficient resources and performance efficiency are the most significant features that distinguish this framework from the other existing frameworks."
pub.1174353028,КЕШУВАННЯ ДАНИХ У ДОДАТКАХ З ВИКОРИСТАННЯМ БЕЗСЕРВЕРНОЇ АРХІТЕКТУРИ,"The response time of a modern web service is a crucial characteristic that has significant importance for meeting user needs. Users who receive instant responses from a web service perceive it as more convenient and appealing to use. At the same time, it is important for developers to quickly and efficiently deploy and scale their applications with minimal costs for maintenance and infrastructure. Therefore, the widespread adoption of cloud technologies for developing and deploying various applications is gaining popularity, and the ability to cache data allows for significant improvements in the performance of developed software. The purpose of the work is to consider various existing options for data caching in the cloud environment using AWS as an example and to justify the use of serverless architecture, which has become a new approach for deploying web services in the cloud environment. The methodology of the comparative analysis involves presenting the main principles of operation, features, and limitations for FaaS (Function as a Service) on AWS, reviewing existing tools for implementing data caching, such as Amazon ElastiCache for Redis, API Gateway Caching, and CloudFront in front of API Gateway. The use of a simple web service to retrieve a schedule of classes in an educational institution for a specified day of the week, consisting of serverless components such as Amazon API Gateway, AWS Lambda, and Amazon DynamoDB, is considered. A comparison of the performance of the created web service using different approaches to data caching is conducted by deploying four different application variants with the mentioned data caching options and preparing tests for load testing using the Locust framework. The scientific novelty of the results obtained in the work is formulating possible strategies for implementing caching for web services deployed in a cloud environment. Conclusions. The implementation of data caching has shown its effectiveness in increasing the performance of the web service. Furthermore, justifying the choice of serverless components for implementing data caching remains relevant in the future."
pub.1114364317,Identity-as-a-Service: An Adaptive Security Infrastructure and Privacy-Preserving User Identity for the Cloud Environment,"In recent years, enterprise applications have begun to migrate from a local hosting to a cloud provider and may have established a business-to-business relationship with each other manually. Adaptation of existing applications requires substantial implementation changes in individual architectural components. On the other hand, users may store their Personal Identifiable Information (PII) in the cloud environment so that cloud services may access and use it on demand. Even if cloud services specify their privacy policies, we cannot guarantee that they follow their policies and will not (accidentally) transfer PII to another party. In this paper, we present Identity-as-a-Service (IDaaS) as a trusted Identity and Access Management with two requirements: Firstly, IDaaS adapts trust between cloud services on demand. We move the trust relationship and identity propagation out of the application implementation and model them as a security topology. When the business comes up with a new e-commerce scenario, IDaaS uses the security topology to adapt a platform-specific security infrastructure for the given business scenario at runtime. Secondly, we protect the confidentiality of PII in federated security domains. We propose our Purpose-based Encryption to protect the disclosure of PII from intermediary entities in a business transaction and from untrusted hosts. Our solution is compliant with the General Data Protection Regulation and involves the least user interaction to prevent identity theft via the human link. The implementation can be easily adapted to existing Identity Management systems, and the performance is fast."
pub.1168215515,Efficient Workload Allocation and Scheduling Strategies for AI-Intensive Tasks in Cloud Infrastructures.,"- The rapid proliferation of Artificial Intelligence (AI) applications has underscored the need for advanced cloud infrastructures capable of efficiently managing AI-intensive workloads. This paper delves into the intricacies of workload allocation and scheduling in the context of cloud environments, specifically focusing on the challenges posed by AI-intensive tasks. Our research endeavors to scrutinize existing strategies, discern their limitations, and proffer innovative approaches tailored to optimize the allocation and scheduling of AI workloads within cloud infrastructures. In elucidating the challenges, we pinpoint resource heterogeneity, dynamic workload characteristics, and scalability as the crux of the issues confronting AI-intensive workload management. The diverse computational demands of AI workloads make it challenging to allocate resources optimally, while the dynamic nature of these tasks necessitates adaptive strategies to accommodate varying computational requirements over time. [1] Additionally, as AI models and datasets burgeon in complexity and size, ensuring scalability becomes paramount for sustaining performance in cloud environments. Our literature review encompasses an examination of both traditional and state-of-the-art workload allocation strategies, shedding light on their respective strengths and shortcomings. We also delve into scheduling techniques employed for managing AI-intensive tasks, providing a comprehensive overview of the existing landscape. To address these challenges, we propose a novel framework centered around dynamic resource provisioning, machine learning-based scheduling, and efficient task migration strategies. The framework aims to adaptively allocate resources based on the evolving nature of AI workloads, leveraging machine learning algorithms to predict workload characteristics and employing efficient task migration to handle workload fluctuations. The paper concludes with an experimental evaluation of the proposed strategies, conducted in a simulated environment using diverse datasets. Key performance metrics, such as throughput, latency, and resource utilization, are employed to assess the effectiveness of our strategies compared to existing approaches. By offering insights into the efficient management of AI-intensive workloads in cloud infrastructures, this research contributes to the ongoing efforts to enhance the scalability and performance of cloud environments in the face of burgeoning AI applications."
pub.1014535289,LS-ADT: Lightweight and Scalable Anomaly Detection for Cloud Datacentres,"Cloud data centres are implemented as large-scale clusters with demanding requirements for service performance, availability and cost of operation. As a result of scale and complexity, data centres typically exhibit large numbers of system anomalies resulting from operator error, resource over/under provisioning, hardware or software failures and security issus anomalies are inherently difficult to identify and resolve promptly via human inspection. Therefore, it is vital in a cloud system to have automatic system monitoring that detects potential anomalies and identifies their source. In this paper we present a lightweight anomaly detection tool for Cloud data centres which combines extended log analysis and rigorous correlation of system metrics, implemented by an efficient correlation algorithm which does not require training or complex infrastructure set up. The LADT algorithm is based on the premise that there is a strong correlation between node level and VM level metrics in a cloud system. This correlation will drop significantly in the event of any performance anomaly at the node-level and a continuous drop in the correlation can indicate the presence of a true anomaly in the node. The log analysis of LADT assists in determining whether the correlation drop could be caused by naturally occurring cloud management activity such as VM migration, creation, suspension, termination or resizing. In this way, any potential anomaly alerts are reasoned about to prevent false positives that could be caused by the cloud operator’s activity. We demonstrate LADT with log analysis in a Cloud environment to show how the log analysis is combined with the correlation of systems metrics to achieve accurate anomaly detection."
pub.1099914375,Privacy Aware Web Services in the Cloud,"Data privacy and security continues to hinder wider adoption of cloud based web services for small to medium businesses. Existing privacy aware systems for cloud environments either assume that web service providers are trustworthy and can adequately enforce a client's privacy policies or adapt computationally expensive encryption techniques to minimize data security risks. In this paper, we propose, PASiC, a framework for Privacy Aware RESTful Web Services in the Cloud. PASiC provides lightweight data privacy features by allowing clients to define their specific privacy policies, obfuscation/encryption methods and collaboratively engage with the service providers to enforce these policies. Our framework is designed to facilitate integration with legacy systems. Our experimental evaluation shows that PASiC safeguards sensitive data throughout the data staging process, and show how it operates over different methods of encryption and obfuscation in terms of their performance."
pub.1095485869,Open Big Data Infrastructures to Everyone,"The evolution of big data has increased the complexity of the respective software. Big data infrastructures require progressively more time and effort to setup, configure, maintain and integrate with existing systems. In absence of a big data “expert”, users are often discouraged from using such solutions. The option of consuming big data infrastructures as a service seems to be a viable one, yet it is not without drawbacks. Such an option a) is costly, b) often locks users down to a vendor, and c) is limited to what the vendor decides to make available. In this paper we present Juju, an open source service modelling approach by Canonical that addresses the above shortcomings. With Juju users can deploy and maintain their infrastructures to a rich variety of target environments that include almost any cloud, local machines (using containers & VMs), bare metal systems and any remote machine the user might have ssh access to. The Juju big data community makes sure that deploying big data infrastructures is as simple as running “juju deploy hadoop”, while interfaces among infrastructures allow for easy system integration. In this work we also show how the operational knowledge of complex software such as Apache Spark can be encapsulated in a few hundreds lines."
pub.1091883988,RINGA: Design and verification of finite state machine for self-adaptive software at runtime,"ContextIn recent years, software environments such as the cloud and Internet of Things (IoT) have become increasingly sophisticated, and as a result, development of adaptable software has become very important. Self-adaptive software is appropriate for today's needs because it changes its behavior or structure in response to a changing environment at runtime. To adapt to changing environments, runtime verification is an important requirement, and research that integrates traditional verification with self-adaptive software is in high demand.ObjectiveModel checking is an effective static verification method for software, but existing problems at runtime remain unresolved. In this paper, we propose a self-adaptive software framework that applies model checking to software to enable verification at runtime.MethodThe proposed framework consists of two parts: the design of self-adaptive software using a finite state machine and the adaptation of the software during runtime. For the first part, we propose two finite state machines for self-adaptive software called the self-adaptive finite state machine (SA-FSM) and abstracted finite state machine (A-FSM). For the runtime verification part, a self-adaptation process based on a MAPE (monitoring, analyzing, planning, and executing) loop is implemented.ResultsWe performed an empirical evaluation with several model-checking tools (i.e., NuSMV and CadenceSMV), and the results show that the proposed method is more efficient at runtime. We also investigated a simple example application in six scenarios related to the IoT environment. We implemented Android and Arduino applications, and the results show the practical usability of the proposed self-adaptive framework at runtime.ConclusionsWe proposed a framework for integrating model checking with a self-adaptive software lifecycle. The results of our experiments showed that the proposed framework can achieve verify self-adaptation software at runtime."
pub.1165022178,The Challenge for the Oil & Gas Industry to Face a New Way of Working,"Abstract  OBJECTIVES As a result of the COVID-19 pandemic, most companies had to adopt home as an office to stay safe and continue operations. Based on the above, the purpose of this article is to present how a cloud solution helped Oil & Gas companies adopt a new working scheme during a global health crisis.   PROCESS With the introduction of cloud solutions to help transition to a new working scheme, it was necessary to implement a migration process that would be the least impactful to both operations and end users. This migration process consisted of the following elements: (1) Infrastructure preparation based on workflows executed and computational demand;(2) Data Migration defining what is priority and what is not; (3) On boarding sessions guiding the users during the adoption of new way of working, explaining the new infrastructure, how to connect, find data, request support, and how the backup works, and (4) Online support avalable24*7 managing basic, medium, and advanced levels.   CONCLUSIONS The emerging new way of working taught us that integration of teams can be achieved digitally, and independently of where teams are located under the following premises: : (1) The infrastructure was prepared to face company's needs, like disk space, workflow demand and memory; (2) Data was available in an organized, clean, and single environment; (3) Data was protected in very rigorous backup plan; (4) Users were able to work in a collaborative, secure and confident environment that let them to reach their business objectives during the pandemic; (5) A Support system was in place to guide the users during the transition and adoption phase; (6) The pieces of the puzzle (infrastructure, data, applications, users) were integrated to give the Oil & Gas business the continuity it needed in a difficult period of consolidation. A more intelligent work framework is available, where artificial intelligence is used to create automatic interpretations improving considerably the time dedicated to critical tasks and allowing end users to focus on other jobs. Data Managers and Infrastructure Engineers can use this document as a reference of how crisis have been addressed in the past and how project innovation and digital enhancements have been a key part of the solution. "
pub.1093563270,Virtual Machine Migration Plan Generation Through A* Search,"Modern virtual machine (VM) management software includes components that enable consolidation of VMs for power savings or load-balancing for performance. While the existing literature provides various methods for computing a better load-balanced, or consolidated goal state, it fails to adequately suggest the best path from the system's current state to the desired goal allocation. This paper discusses an approach to efficient path finding in VM placement plan generation for cloud computing environments. Initial results indicate our prototype solution is viable for managing hundreds of VMs through the application of A* search."
pub.1141081402,A Review Paper on Fog Computing Paradigm to solve Problems and Challenges during Integration of Cloud with IoT,"Abstract
                  In Today’s world technologies such as Internet of Things, Cloud Computing as well as Fog Computing are growing at an exponential rate which depend upon each other directly or indirectly. The Internet of Things can be described as a network of substantial matter such as cars, washing machines, refrigerator which can interact with each other through internet. Billions of devices will be IoT enabled in near future and generate enormous amount of data but IoT devices has some limitations like storage capabilities, processing capabilities and utilization of resources which can only be handled by integrating it with cloud technology. Cloud model provide environment in which software, Infrastructure, sharable pool of configurable resources, virtual environment, sensors, hardware and database is provided as a utility for IoT devices and users. In cloud computing paradigm some limitations exist for example distance of the data source from multi-hop, geological unified structure, latency, heterogeneity and many more. To address such limitations, Fog computing approach can be used to bring computing assets nearer to IoT devices. Fog computing is an enhancement of the cloud-based Network and computing services. It provides computational and storage services of cloud proximate to IoT devices. This paper provides an overview regarding the cloud computing uses in IoT devices and issues or problems that occur during integration. Handling of problems that occurs during integration of cloud with IoT can be done through fog computing. The purpose of this survey is to understand the concept of fog computing to improve the existing system of Integration of Cloud with IoT."
pub.1173293790,Metadata-Driven Cloud-Agnostic Data Integration Framework,"In an era where cloud computing is becoming essential, organizations are actively seeking to build cloud-based data solutions or migrate existing on-premises systems to the cloud. This shift is driven by the need to benefit from the scalability, flexibility, and efficiency offered by cloud environments. However, this transition presents a complex challenge: the integration of cloud data practices that are universally applicable yet sufficiently adaptable to domain specific needs. A significant number of cloud data initiatives fail due to poor planning and the absence of appropriate methodologies for effective cloud data integration. This failure often results in issues related to the completeness, accuracy, and consistency of data, which are essential for building reliable and trustworthy data platforms. This paper addresses this challenge by proposing a cloud-agnostic framework for cloud data integration. The paper’s contribution is a comprehensive, adaptable metadata-driven data integration framework that is flexible to be tailored to specific domain requirements and scalable for various cloud service providers. The proposed framework is designed to guide and assist organizations in their cloud data integration efforts, enabling them to focus on deriving organizational value."
pub.1095406173,"Towards an Environment Supporting Resilience, High-Availability, Reproducibility and Reliability for Cloud Applications","This paper presents a design study of an environment that would provide for resilience, high-availability, reproducibility and reliability of Cloud-based applications. The approach involves the use of a resilient container overlay, which provides tools for tracking and optimizing container placement during the course of a scientific experiment execution. The system is designed to detect failure and current performance bottlenecks and be capable of migrating running containers on the fly to servers more optimal for their execution. This work is in the design phase and therefore in this paper, we outline the proposed architecture of system and identify existing container management and migration tools that can be used in the implementation, where appropriate."
pub.1001197688,Towards a Taxonomy for Cloud Computing from an e-Science Perspective,"In the last few years, cloud computing has emerged as a computational paradigm that enables scientists to build more complex scientific applications to manage large data sets or high-performance applications, based on distributed resources. By following this paradigm, scientists may use distributed resources (infrastructure, storage, databases, and applications) without having to deal with implementation or configuration details. In fact, there are many cloud computing environments already available for use. Despite its fast growth and adoption, the definition of cloud computing is not a consensus. This makes it very difficult to comprehend the cloud computing field as a whole, correlate, classify, and compare the various existing proposals. Over the years, taxonomy techniques have been used to create models that allow for the classification of concepts within a domain. The main objective of this chapter is to apply taxonomy techniques in the cloud computing domain. This chapter discusses many aspects involved with cloud computing that are important from a scientific perspective. It contributes by proposing a taxonomy based on characteristics that are fundamental for scientific applications typically associated with the cloud paradigm."
pub.1061390033,Managing Uncertainty in Autonomic Cloud Elasticity Controllers,"Elasticity allows a cloud system to maintain an optimal user experience by automatically acquiring and releasing resources. Autoscaling-adding or removing resources automatically on the fly-involves specifying threshold-based rules to implement elasticity policies. However, the elasticity rules must be specified through quantitative values, which requires cloud resource management knowledge and expertise. Furthermore, existing approaches don't explicitly deal with uncertainty in cloud-based software, where noise and unexpected events are common. The authors propose a control-theoretic approach that manages the behavior of a cloud environment as a dynamic system. They integrate a fuzzy cloud controller with an online learning mechanism, putting forward a framework that takes the human out of the dynamic adaptation loop and can cope with various sources of uncertainty in the cloud."
pub.1095385790,An A-RBAC Mechanism for a Multi-Tenancy Cloud Environment,"With the evolution of software technology, companies require more highperformance hardware to enhance their competitiveness. Cloud computing is the result of distributed computing and grid computing processes and is gradually being seen as the solution to the companies. Cloud computing can virtualizes existing software and hardware to reduce costs. Thus, companies only require high Internet bandwidth and devices to access cloud service on the Internet. This would decrease many overhead costs and the number of IT staff required. When many companies rent a cloud service simultaneously, this is called a multi-tenancy cloud service. However, how to access resource safely is important if adopt multi-tenancy cloud computing technology. The cloud computing environment is vulnerable to network-related attacks. This research improves the role-based access control authorization mechanism and combines it with attribute check mechanism to determine which tenant that user can access. The enhanced authorization can improve the safety of cloud computing services and protected the data privacy."
pub.1132888258,An Evolution of Mobile IPv6 to the Cloud,"The disruptions caused by Network Function Virtualization (NFV) and Software Defined Networking (SDN) introduce severe impacts also on mobility management. In this work, we will show how IP-based mobility management, especially Mobile IPv6 (MIPv6), can be redesigned to fit into the new environment. We will present an evolutionary procedure on how MIPv6 can arrive in the cloud. At the end of the process, the unique architecture of MIPv6 can run on the top of Kubernetes and OpenStack while keeping compatibility with the existing standards. Orchestration is also an environmental change, comes with cloud, where MIPv6 should be aligned with closed-loop orchestration support."
pub.1106705033,Secure‐CamFlow: A device‐oriented security model to assist information flow control systems in cloud environments for IoTs,"Summary Recent developments in the cloud technologies have motivated the migration of distributed large systems, specifically the Internet of Things to the cloud architecture. Since Internet of Things consist of a vast network and variety of objects, the cloud platform proves to be an ideal option. It is essential for the proper functioning of the Internet of Things to be able to share data among the system processes. The biggest problem faced during the transition of the IoTs to the cloud is the security of data especially while data sharing within the cloud and among its tenants. Information Flow Control mechanisms are one of the many solutions to enable a controlled sharing of data. Integration of Information Flow Control Systems to the existing architecture requires various levels of re‐engineering efforts. Moreover, most of the Information Flow Control systems focus on data flow within the cloud and neglect the security and integrity of data while it is being transferred to the cloud from various devices. This research focuses on securing the entire process of data migration to cloud from devices while the in‐cloud data flow is monitored by the Information Flow Control policies specified by the users. We have developed a prototype for the proposed model, and results are evaluated on the basis of energy consumption and execution time. As proposed model provides security services such as privacy, integrity, and authentication, hence it takes more execution time and consumes more energy as compared with the existing model."
pub.1168478078,Tehnologiile cloud și necesitatea implementării cloud-ului hibrid în mediul militar,"Cloud computing has been widely adopted as the next-generation digitisation model for transforming the organisation into an entity that drives development and innovation. Nowadays, users of computing systems aim to get quick access to the virtual ecosystem and a new experience in cyberspace that seamlessly integrates with the existing services they use today. From the perspective of the military organisation/Military Enterprise, the cloud provides services for users and structures in a scalable, highly reliable and highly available manner, specifically with different security levels associated with the individual profile and functional roles in the organisation. From the end user’s perspective, the cloud provides a simple model for accessing information technology/IT services without the need for the human factor to fully understand the transport/transmission infrastructure and technology used. This article explores how modern military-classified and unclassified cloudnative infrastructures can be secured and managed in a national, military private, or mixed hybrid deployment cyber environment, along with various requirements and considerations for adapting cloud-native applications for military systems. In context, the article provides a simple but comprehensive introduction to the cloud native overview and the major technologies that developers use to build such reliable environments in cyberspace that could be the subject of a feasibility study to implement the concept of hybrid cloud in cyberspace with military use. The material is intended for IT experts, DevOps engineers, CIS (Communications and Information Technology) systems and infrastructure architects, cloud enthusiasts, cloud security experts, and any military professional involved in the development, migration, deployment, and management of current services and operations. of a cloud-native system."
pub.1093739421,Forensic Investigation in Mobile Cloud Environment,"Cloud computing is changing the way we use mobile application by offering a powerful, scalable and on-demand computational resources to mobile users. However, this new paradigm is a challenging issue for forensic investigators since it combines two different environments. The adoption of Mobile Cloud computing solutions is increasing rapidly, so it is necessary for digital investigation applications and procedure to be adapted to this new environment. This paper provides an overview of mobile cloud forensics including challenging issues and some existing proposals in order to overcome these challenges."
pub.1158161192,"Towards Computation Offloading Approaches in IoT-Fog-Cloud Environment: Survey on Concepts, Architectures, Tools and Methodologies","The Internet of Things (IoT) provides communication and processing power to different entities connected to it, thereby redefining the way objects interact with one another. IoT has evolved as a promising platform within short duration of time due to its less complexity and wide applicability. IoT applications generally rely on cloud for extended storage, processing and analytics. Cloud computing increased the acceptance of IoT applications due to enhanced storage and processing. However, the integration does not offer support for latency-sensitive IoT applications. The latency-sensitive IoT applications had greatly benefited with the introduction of fog/edge layer to the existing IoT-Cloud architecture. The fog layer lies close to the edge of the network making the response time better and reducing the delay considerably. The three-tier architecture is still in its earlier phase and needs to be researched further. This paper addresses the offloading issues in IoT-Fog-Cloud architecture which helps to evenly distribute the incoming workload to available fog nodes. Offloading algorithms have to be carefully chosen to improve the performance of application. The different algorithms available in literature, the methodologies and simulation environments used for the implementation, the benefits of each approach and future research trends for offloading are surveyed in this paper. The survey shows that the offloading algorithms are an active research area where more explorations have to be done."
pub.1176070468,Data Protection based on Searchable Encryption and Anonymization Techniques,"Data leakage compromises companies’ confidentiality and directly impacts the existing privacy laws, as well as it is necessary to perform a light integration with the legacy systems, in order not to harm the performance of its services. Within this context, this paper presents an innovative cloud system to protect the private data of existing databases (legacy systems of clients) based on Searchable Symmetric Encryption for Databases (SSE-DB) and Permutation and Proprieties Maintenance Anonymization (PPM-Anon), attaching a security solution to the existing databases (without any change in these legacy systems). Results from real experiments using a real cloud environment suggest that the proposed solution is suitable for protecting the data without harming the performance of the existing services."
pub.1095853495,Enterprise Energy Analytic Cloud Portal for User-Centric On-Demand Profile Access in Smart Green Energy Management System,"The absence of real time energy consumption information (RTECI) has often made electricity users in many developing countries to be at loggerheads with their service providers. A cloud based metering infrastructure is developed as a viable solution. This paper offers basic information regarding cloud based advanced metering infrastructure (AMI) for analysis in extended smart green energy management system (e-SGEMS). The system has novel flexibilities in automated energy monitoring platform for both end-users and their utility vendors. Lean system approach involving Software Development Life Cycle framework is applied. The system allows users to obtain their exact consumption pattern in Kw, Kw/Month and Kw/Year using a negligible cost reflective tariff of 0.17 as derived from a renewable metered micro-gird. The proposed system contrasts with existing systems in terms of full ISO 14000 quality assurance compliance metrics and on-demand/real time consumption profiling. MySQL and Java server page are used for the metering simulation and cloud integration. The hardware runs on C++ executable scripts. The system is validated through a full cycle usability tests in loaded and non-loaded case scenarios. This solution provides real-time access to energy consumption information at the point of metering in an access controlled cloud environment. Flexibility and cost effectiveness characterizes the system for smart grid mixed energy metering paradigm."
pub.1166627085,An empirical study on the intention to switch to cloud-based hospital information systems in Korea: a push–pull–mooring model perspective," Purpose Recently, interest and necessity for cloud-based hospital information systems (HISs) have emerged as an appropriate alternative for revitalizing medical information exchange between hospitals, analyzing “big data” medical information and developing the use of new medical technologies. The purpose of this paper is to investigate factors that affect the switching of information systems in existing on-premise environments into cloud-based HISs.   Design/methodology/approach A research model was developed using the push–pull–mooring model based on migration theory. The research model was analyzed using confirmatory factor analysis and path analysis using partial least squares structural equation modeling.   Findings The results of this study showed that low compatibility, perceived value, low cost and inertia influenced the intention to switch to cloud-based HISs; low flexibility and low compatibility influenced dissatisfaction; and low cost, ease of maintenance and ease of managing indicators influenced perceived value.   Originality/value This study is expected to be used as the basis for developing a research model in subsequent studies to analyze the transition to new innovative technologies. Also, in practice, it is expected to contribute to the activation of cloud computing environments in hospitals. "
pub.1149297215,The practice of developing the academic cloud using the Proxmox VE platform,"Cloud technologies provide users with efficient and secure tools for data management, computing, storage and other services. The article analyzes the projects for the introduction of cloud technologies in education and identifies the main advantages and risks in creating a cloud infrastructure for the university. Such startups contribute to the formation of a new paradigm of education. It involves the virtualization of education, the introduction of mobile and blended learning, ie the combination of cloud computing with modern learning concepts. In this paper, we highlight our experience in improving the academic cloud for the training of a bachelor's degree in computer science. This is through the integration of the Proxmox VE platform into existing computing power by deploying the Proxmox VE system. In the study, we reveal some technical and methodological aspects of the organization of the educational process using this corporate cloud platform. The scheme of the organization of physical components of cloud infrastructure (nodes, virtual networks, routers, domain controller, VPN server, backup system of students' virtual machines) is given. All characteristics of this environment and possibilities of their application are studied."
pub.1119404579,SeeMoRe: A Fault-Tolerant Protocol for Hybrid Cloud Environments,"Large scale data management systems utilize State Machine Replication to
provide fault tolerance and to enhance performance. Fault-tolerant protocols
are extensively used in the distributed database infrastructure of large
enterprises such as Google, Amazon, and Facebook, as well as permissioned
blockchain systems like IBM's Hyperledger Fabric. However, and in spite of
years of intensive research, existing fault-tolerant protocols do not
adequately address all the characteristics of distributed system applications.
In particular, hybrid cloud environments consisting of private and public
clouds are widely used by enterprises. However, fault-tolerant protocols have
not been adapted for such environments. In this paper, we introduce SeeMoRe, a
hybrid State Machine Replication protocol to handle both crash and malicious
failures in a public/private cloud environment. SeeMoRe considers a private
cloud consisting of nonmalicious nodes (either correct or crash) and a public
cloud with both Byzantine faulty and correct nodes. SeeMoRe has three different
modes which can be used depending on the private cloud load and the
communication latency between the public and the private cloud. We also
introduce a dynamic mode switching technique to transition from one mode to
another. Furthermore, we evaluate SeeMoRe using a series of benchmarks. The
experiments reveal that SeeMoRe's performance is close to the state of the art
crash fault-tolerant protocols while tolerating malicious failures."
pub.1174000258,Data Protection based on Searchable Encryption and Anonymization Techniques,"Data leakage compromises companies’ confidentiality and directly impacts the existing privacy laws, as well as it is necessary to perform a light integration with the legacy systems, in order not to harm the performance of its services. Within this context, this paper presents an innovative cloud system to protect the private data of existing databases (legacy systems of clients) based on Searchable Symmetric Encryption for Databases (SSE-DB) and Permutation and Proprieties Maintenance Anonymization (PPM-Anon), attaching a security solution to the existing databases (without any change in these legacy systems). Results from real experiments using a real cloud environment suggest that the proposed solution is suitable for protecting the data without harming the performance of the existing services."
pub.1100745386,Serverless computing for container-based architectures,"New architectural patterns (e.g. microservices), the massive adoption of Linux containers (e.g. Docker containers), and improvements in key features of Cloud computing such as auto-scaling, have helped developers to decouple complex and monolithic systems into smaller stateless services. In turn, Cloud providers have introduced serverless computing, where applications can be defined as a workflow of event-triggered functions. However, serverless services, such as AWS Lambda, impose serious restrictions for these applications (e.g. using a predefined set of programming languages or difficulting the installation and deployment of external libraries). This paper addresses such issues by introducing a framework and a methodology to create Serverless Container-aware ARchitectures (SCAR). The SCAR framework can be used to create highly-parallel event-driven serverless applications that run on customized runtime environments defined as Docker images on top of AWS Lambda. This paper describes the architecture of SCAR together with the cache-based optimizations applied to minimize cost, exemplified on a massive image processing use case. The results show that, by means of SCAR, AWS Lambda becomes a convenient platform for High Throughput Computing, specially for highly-parallel bursty workloads of short stateless jobs."
pub.1149188383,Role of Cloud Computing for Improvement in Healthcare Services,"Cloud helps in offering on-demand latest technology that helps in deploying, accessing and using network-accessed information along with various applications and resources. Nowadays electronic health records are maintained by many hospitals that want to undergo a change in their legacy system. This type of transformation has helped physicians, nurses and also administrative staff access the desired record whenever needed. They believe that this may change the complete face of health information technology. However, lack of security and privacy are two important concerns that may provide hazards when choosing cloud solutions for various health-related factors. This problem can be avoided to some extent by evaluating cloud technology in an effective manner before its complete adoption. This paper uses four major aspects i.e., technology, security, legal and management for finding different types of challenges of this computing model. When any health services want to migrate from traditional to cloud-based health services then they can do different types of strategic planning for determining strategy, allocated resources and direction for maintaining a cloud environment in their organization."
pub.1094390283,Cyber Laboratory: Migration to the Hybrid Cloud Solution for Device Dependent Hardware Experiments,"Cyber laboratory for FPGA design experiments combines the actual laboratory and the remote laboratory in a seamless way in a hybrid cloud environment. The hybrid cloud consists of on premise private cloud to support device dependent services and a public cloud to support rest of the laboratory services. The on premise private cloud allows concurrent use of device dependent tasks, such as FPGA run services, by making use of the Web services. The elastic nature of the public cloud is suitable for coping with dynamically changing student experimental usages. The number of servers in the public cloud can be scale out or shrink according to the number of simultaneous students accesses. Expensive license CAD tools, such as Verilog-HDL synthesis tools, are allocated for a limited number of the servers in the public cloud and the private cloud, where Verilog-HDL synthesis Web services are realized to achieve remote sharing of these tools. For remote laboratory case, student virtual machines are allocated in the public cloud and remotely issue the FPGA run services and the Verilog-HDL Synthesis services. The existing faculty file/database and education support system serves as an adhesive agent to combine the on premise cloud and the public cloud to minimize the data transfer among them and to perform student VM migrations between remote and actual laboratories. The Cyber Laboratory achieves an efficient use of laboratory platforms and can reduce TCO (total cost of ownership), as the public cloud can cope with the dynamically changing experimental loads."
pub.1169945465,An integrated SDN framework for early detection of DDoS attacks in cloud computing,"Cloud computing is a rapidly advancing technology with numerous benefits, such as increased availability, scalability, and flexibility. Relocating computing infrastructure to a network simplifies hardware and software resource monitoring in the cloud. Software-Defined Networking (SDN)-based cloud networking improves cloud infrastructure efficiency by dynamically allocating and utilizing network resources. While SDN cloud networks offer numerous advantages, they are vulnerable to Distributed Denial-of-Service (DDoS) attacks. DDoS attacks try to stop genuine users from using services and drain network resources to reduce performance or shut down services. However, early-stage detection of DDoS attack patterns in cloud environments remains challenging. Current methods detect DDoS at the SDN controller level, which is often time-consuming. We recommend focusing on SDN switches for early detection. Due to the large volume of data from diverse sources, we recommend traffic clustering and traffic anomalies prediction which is of DDoS attacks at each switch. Furthermore, to consolidate the data from multiple clusters, event correlation is performed to understand network behavior and detect coordinated attack activities. Many existing techniques stay behind for early detection and integration of multiple techniques to detect DDoS attack patterns. In this paper, we introduce a more efficient and effectively integrated SDN framework that addresses a gap in previous DDoS solutions. Our framework enables early and accurate detection of DDoS traffic patterns within SDN-based cloud environments. In this framework, we use Recursive Feature Elimination (RFE), Density Based Spatial Clustering (DBSCAN), time series techniques like Auto Regressive Integrated Moving Average (ARIMA), Lyapunov exponent, exponential smoothing filter, dynamic threshold, and lastly, Rule-based classifier. We have evaluated the proposed RDAER model on the CICDDoS 2019 dataset, that achieved an accuracy level of 99.92% and a fast detection time of 20 s, outperforming existing methods."
pub.1106705393,Simulated Domain-Specific Provenance,"The main driver for provenance adoption is the need to collect and understand knowledge about the processes and data that occur in some environment. Before analytical and storage tools can be designed to address this challenge, exemplar data is required both to prototype the analytical techniques and to design infrastructure solutions. Previous attempts to address this requirement have tried to use existing applications as a source; either by collecting data from provenance-enabled applications or by building tools that can extract provenance from the logs of other applications. However, provenance sourced this way can be one-sided, exhibiting only certain patterns, or exhibit correlations or trends present only at the time of collection, and so may be of limited use in other contexts. A better approach is to use a simulator that conforms to explicitly specified domain constraints, and generate provenance data synthetically, replicating the patterns, rules and trends present within the target domain; we describe such a constraint-based simulator here. At the heart of our approach are templates - abstract, reusable provenance patterns within a domain that may be instantiated by concrete substitutions. Domain constraints are configurable and solved using a Constraint Satisfaction Problem solver to produce viable substitutions. Workflows are represented by sequences of templates using probabilistic automata. The simulator is fully integrated within our template-based provenance server architecture, and we illustrate its use in the context of a clinical trials software infrastructure."
pub.1107137185,Adaptation of System Dynamics Model Execution Algorithms for Cloud-based Environment,"This paper presents a process of adaptation of system dynamics models execution algorithms to cloud-based environment. System dynamics is an aspect of systems theory as a method to understand the dynamic behaviour of complex systems. Existing modeling algorithms used in popular modeling solutions are either not available for free use or have several disadvantages which prevent them from being used in distributed cloud environment. Adaptation of execution algorithms aimed not only to adapt execution process to distributed parallel environments with higher reliability and wider range of possible applications, but also to improve system dynamics model execution performance. For example, existing algorithms of model execution which are not ready for distributed environments will fail to complete modeling task in case of hardware failure, and optimized ones are able to smoothly transfer execution process from one node to another with minimal impact on overall model execution progress. Such capabilities help to save many resources and, especially, time on execution re-runs. In this paper described algorithms and approaches designed for sdCloud solution which are focused on transferring execution of system dynamics models into distributed cloud-based environment and shown extra benefits brought to modeling process by shift to the cloud."
pub.1175647145,CLOUD SECURITY POSTURE MANAGEMENT AUTOMATING RISK IDENTIFICATION AND RESPONSE IN CLOUD INFRASTRUCTURES,"Cloud Security Posture Management (CSPM) tools have become essential in addressing the growing security challenges faced by organizations as they migrate to cloud environments. This study explores the effectiveness of CSPM tools in automating the identification and response to security risks within cloud infrastructures, highlighting their role in reducing misconfigurations, improving compliance, and enhancing overall security posture. Through a mixed-method approach, combining a comprehensive literature review, a survey of IT security professionals, and detailed case study analyses, this research provides a robust evaluation of CSPM tools' capabilities and the challenges associated with their implementation. The findings reveal that organizations utilizing CSPM tools experience significant reductions in security incidents and operational inefficiencies, with automation playing a crucial role in enabling real-time threat detection and response. However, the study also identifies critical barriers to CSPM adoption, including integration complexities, cost concerns, and organizational resistance to automated security solutions. These challenges suggest that while CSPM tools offer substantial benefits, their successful deployment requires careful planning, adequate resource allocation, and strategic change management to address both technical and human factors. This study contributes to the existing literature by providing detailed insights into the practical applications and limitations of CSPM tools, offering valuable guidance for organizations seeking to enhance their cloud security strategies through automation."
pub.1171600068,Observability and Incident Response in Managed Serverless Environments Using Ontology-Based Log Monitoring,"In a fully managed serverless environment, the cloud service provider is
responsible for securing the cloud infrastructure, thereby reducing the
operational and maintenance efforts of application developers. However, this
environment limits the use of existing cybersecurity frameworks and tools,
which reduces observability and situational awareness capabilities (e.g., risk
assessment, incident response). In addition, existing security frameworks for
serverless applications do not generalize well to all application architectures
and usually require adaptation, specialized expertise, etc. for use in fully
managed serverless environments. In this paper, we introduce a three-layer
security scheme for applications deployed in fully managed serverless
environments. The first two layers involve a unique ontology based solely on
serverless logs which is used to transform them into a unified application
activity knowledge graph. In the third layer, we address the need for
observability and situational awareness capabilities by implementing two
situational awareness tools that utilizes the graph-based representation: 1) An
incident response dashboard that leverages the ontology to visualize and
examine application activity logs in the context of cybersecurity alerts. Our
user study showed that the dashboard enabled participants to respond more
accurately and quickly to new security alerts than the baseline tool. 2) A
criticality of asset (CoA) risk assessment framework that enables efficient
expert-based prioritization in cybersecurity contexts."
pub.1011442549,Cloud-Assisted IoT-Based SCADA Systems Security: A Review of the State of the Art and Future Challenges,"Industrial systems always prefer to reduce their operational expenses. To support such reductions, they need solutions that are capable of providing stability, fault tolerance, and flexibility. One such solution for industrial systems is cyber physical system (CPS) integration with the Internet of Things (IoT) utilizing cloud computing services. These CPSs can be considered as smart industrial systems, with their most prevalent applications in smart transportation, smart grids, smart medical and eHealthcare systems, and many more. These industrial CPSs mostly utilize supervisory control and data acquisition (SCADA) systems to control and monitor their critical infrastructure (CI). For example, WebSCADA is an application used for smart medical technologies, making improved patient monitoring and more timely decisions possible. The focus of the study presented in this paper is to highlight the security challenges that the industrial SCADA systems face in an IoT-cloud environment. Classical SCADA systems are already lacking in proper security measures; however, with the integration of complex new architectures for the future Internet based on the concepts of IoT, cloud computing, mobile wireless sensor networks, and so on, there are large issues at stakes in the security and deployment of these classical systems. Therefore, the integration of these future Internet concepts needs more research effort. This paper, along with highlighting the security challenges of these CI’s, also provides the existing best practices and recommendations for improving and maintaining security. Finally, this paper briefly describes future research directions to secure these critical CPSs and help the research community in identifying the research gaps in this regard."
pub.1095758294,A Visualized Framework of Automatic Orchestration Engine Supporting Hybrid Cloud Resources,"Along with the widespread adoption of Cloud infrastructure, more and more E-Commerce systems are built using this new platform. It is very burdensome, complicated and error-prone for system administrators to manually deploy such Cloud platforms or multi-layers Web service applications. However, existing automatic deployment tools have following two shortcomings. Firstly, most automatic deployment tools require users to have programming skills using high-level programming language. But, many system administrators have no related programming experience. Therefore, it is difficult for them to use these tools. Secondly, existing automatic deployment tools are not competent to manage pre-existing physical servers and dynamic created virtual machines in a hybrid Cloud environment. In this paper, we propose a visualized automatic orchestration engine supporting hybrid Cloud resources. It allows users to orchestrate templates of distributed systems through a Web-based visualized user interface, and it can repeatedly automatically deploy distributed systems from a template. In our experiment, we take the deployment of CloudStack system and Hadoop system as examples to evaluate our system's efficacy. Users without high-level programming experience can conveniently design templates of custom systems, and the template can be used to deploy real world systems repeatedly and automatically."
pub.1138789043,Denial of ARP spoofing in SDN and NFV enabled cloud-fog-edge platforms,"In order to support a variety of Internet of Things (IoT) and smart city applications, it is necessary to provide computing and networking resources at cloud, fog and edge levels. Fortunately, evolution of Network Function Virtualization (NFV) and Software Defined Networking (SDN) technologies are greatly supporting operators to deploy their data centers at reduced expenditures by integrating cloud, fog and edge (Cloud-Fog-Edge) platforms. Although Cloud-Fog-Edge environments provide economic platforms, due to their multi-tenant sharing platforms customer applications could face a variety of security issues in terms of networking and computing resources. For instance, in cloud environments Intrusion Detection Systems (IDS) and authentication mechanisms are useful for enforcing security policies and improving operational security, but internal malicious users can do Address Resolution Protocol (ARP) spoofing attacks by exploiting shared networking environments. Mainly, ARP spoofing attacks could lead to VLAN-ID spoofing, Denial of Service (DoS) and distributed DoS (DDoS), Man in the Middle (MITM) and session hijack attacks in the network. In this work we are proposing a Denial of ARP Spoofing (D-ARPSpoof) approach to prevent ARP spoofing in SDN and NFV enabled Cloud-Fog-Edge platforms. Unlike existing IDS and anomaly detection systems, D-ARPSpoof prevents ARP spoofing attacks by reducing overhead towards the centralized controllers and OpenFlow switches. In this work, D-ARPSpoof performance is compared with recent ARP spoofing mitigation approaches and anomaly detection systems using important metrics like number of successful connections, controller processing messages overhead and number of flow rules installed in OpenFlow switches. In results, we found that in comparison with existing approaches D-ARPSpoof successfully prevents all malicious connections at reduced overhead towards controllers and switches."
pub.1095602394,An Iterative Hierarchical Key Exchange Scheme for Secure Scheduling of Big Data Applications in Cloud Computing,"As the new-generation distributed computing platform, cloud computing environments offer high efficiency and low cost for data-intensive computation in big data applications. Cloud resources and services are available in pay-as-you-go mode, which brings extraordinary flexibility and cost-effectiveness as well as zero investment in their own computing infrastructure. However, these advantages come at a price - people no longer have direct control over their own data. Based on this view, data security becomes a major concern in the adoption of cloud computing. Authenticated Key Exchange (AKE) is essential to a security system that is based on high efficiency symmetric-key encryption. With virtualization technology being applied, existing key exchange schemes such as Internet Key Exchange (IKE) becomes time-consuming when directly deployed into cloud computing environment. In this paper we propose a novel hierarchical key exchange scheme, namely Cloud Background Hierarchical Key Exchange (CBHKE). Based on our previous work, CBHKE aims at providing secure and efficient scheduling for cloud computing environment. In our new scheme, we design a two-phase layer-by-layer iterative key exchange strategy to achieve more efficient AKE without sacrificing the level of data security. Both theoretical analysis and experimental results demonstrate that when deployed in cloud computing environment, efficiency of the proposed scheme is dramatically superior to its predecessors CCBKE and IKE schemes."
pub.1094353420,Dynamic User Revocation and Key Refreshing for Attribute-Based Encryption in Cloud Storage,"Cloud storage provides the potential for on-demand massive data storage, but its highly dynamic and heterogeneous environment presents significant data protection challenges. Ciphertext-policy attribute-based encryption (CP-ABE) enables fine-grained access control. However, important issues such as efficient user revocation and key refreshing are not straightforward, which constrains the adoption of CP-ABE in cloud storage systems. In this paper we propose a dynamic user revocation and key refreshing model for CPABE schemes. A key feature of our model is its generic possibility in general CP-ABE schemes to refresh the system keys or remove the access from a user without issuing new keys to other users or re-encrypting existing ciphertexts. Our model is efficient and suitable for application in cloud storage environments. As an example, we use BSW's CP-ABE scheme to show the adaptation of our model to a CP-ABE scheme."
pub.1149659375,Secure cloud adoption model: novel hybrid reference model,"This article  discusses research conducted to conceptualise a secure cloud adoption model. The study surveyed SMEs in the Sri Lankan information technology industry using a questionnaire to determine cloud computing adoption factors. The study used Rogers' diffusion of innovation (DOI), Tornatzky and Fleischer's technology-organization-environment (TOE) framework, Venkatesh and Bala's technology acceptance model 3 (TAM3), and Venkatesh, Thong, and Xu's Unified theory of acceptance and use of technology 2 (UTAUT2) as the theoretical foundation for evaluating the reference model. Two hundred and fifty-six key officials from information technology (IT) organisations in Sri Lanka participated in the survey. The study used quantitative data coding and analysis methods with the SPSS and AMOS softwares. The findings from previous research and existing technology adoption frameworks and models were summarised to support the secure cloud adoption model (SCAM)."
pub.1122516239,Migration of Legacy Industrial Automation Systems in the Context of Industry 4.0- A Comparative Study,"Automation systems are driven by industrial requirements such as sustainability, flexibility, efficiency and competitiveness. Those in turn are driven by a societal threats such as impacts on environment, sustainability of energy & natural resources, globalization, and rapidly changing market trends. A specific case of automation system engineering, concerns with the reengineering of old systems, in order to upgrade to a newer technology version; is referred as “Migration”. The engineering process of automation systems is a challenging job; in which, it’s required to realize and formulate the desired functionality and turn it into a physical system that fulfills the required process objectives, consequently selection of the most suitable engineering tools and methodologies to achieve that objectives. Although, most engineering specializations and application domains have developed a standardized engineering tools and methods. However, these standards vary greatly because the interaction between different specializations and domains are limited. Therefore, there are a continued efforts for having a common and unified ones. The other challenge in the engineering process is due to the increased complexity of automation systems, and accordingly, increased functionalities and amount data processing. These challenges makes the use of new technologies, engineering tools and methodologies to become a task that must be faced. In the context of the fourth industrial revolution (industry 4.0), the development and adaptation of more smart, more inter-connected automation systems, is a coherent approach to the engineering and management of the System of Systems (SOS). That system, which connects a massive numbers of devices, expected to be operating across multiple automation domains and serving different specializations. The SOS will utilize the industry 4.0 technologies such as internet of things (IoT), cyber physical systems (CPS) and cloud computing. In the same context; the engineering of SOS has to consider critical challenges faced by the legacy automation systems such as: 1) integration with the aforementioned technologies. 2) Interoperability between varies IoT devices and existing system. 3) Guarantee of stable performance of the system in regards to latency, availability and real-time information transfer. 4) Scalability and safety of process know-how and automation system related information. 5) Ease of application. 6) Feasibility of multi-vendor and multi-stakeholders integration. In this paper we present a comparative study for different migration strategies of old automation systems; with emphasis on migration of the two most widely systems, DCS and SCADA."
pub.1084139112,The Hierarchical Distributed Agent Based Approach to a Modern Data Center Management,"This paper overviews and analyzes progressive trends in modern data center, and existing solutions to build the distributed cloud data center. Authors present the hierarchical distributed agent-based control plane architecture to build a web-scale control layer based on software-defined domains. The goal of this approach is to design the simple extensible agent that could be used for any management purposes, just by adding some specific code. Using of this approach makes easier the scalability and increases the efficiency of the management of a multi-site environment. There are five main use-cases of using this approach: distributed cloud, hybrid cloud, hyperscale data center, IoT, Continuous Integration."
pub.1132424505,Microservice approach to the qualitative study of attractors of binary dynamic systems based on the Boolean constraint method,"In recent years, due to the active development of distributed computing technologies, the automation of computations based on these technologies became very important for solving the problems of qualitative research of binary dynamic systems, which are widely used as mathematical models in cryptography, bioinformatics, the study of fault tolerance of computer networks, and some other domains. In this regard, the issues are actualized of developing tools and providing infrastructures for the microservice-based implementation of scientific applications, which are characterized by the dynamism of resource requirements and oriented to use in a hybrid cloud environment. We offer microservice-oriented tools for the automation of solving computationally complex problems of a qualitative study of attractors of binary dynamic systems in a hybrid computing environment. This study is based on the Boolean constraint method and allows the finding of evolutionary parameters of system states (such as, for example, radius, period, and branching) without modeling system dynamics. These tools provide the agents implemented as web-services for the integration of resources of on-premises computers with cloud resources using Dew computing. An example of the application of the developed tools is given."
pub.1172294157,An IoT based Environment Monitoring System,"With increasing environmental awareness, the demand for robust environmental monitoring systems has surged. Such systems play a pivotal role not only in safeguarding the environment but also in ensuring occupational safety, particularly in hazardous industries such as mining. However, large-scale sensor deployment poses significant challenges related to data collection, management, connectivity, and power consumption. Leveraging IoT technology, this paper introduces a novel framework for environmental monitoring utilizing sensors, microcontrollers, and IoT infrastructure. Our system enables users to monitor temperature, humidity, and detect harmful gases indoors and outdoors. Data is securely stored on a web server, accessible globally via the internet. Additionally, we present a web application offering real-time data visualization and customizable notification alerts for critical sensor readings. Compared to existing systems, our solution offers cost-effectiveness, accuracy, user-friendliness, and cloud- based architecture. Extensive evaluation demonstrates the system's high accuracy and reliability across diverse operating conditions, underscoring its potential for widespread adoption in environmental monitoring applications."
pub.1085054564,Leveraging Fog Computing for Scalable IoT Datacenter Using Spine‐Leaf Network Topology,"With the Internet of Everything (IoE) paradigm that gathers almost every object online, huge traffic workload, bandwidth, security, and latency issues remain a concern for IoT users in today’s world. Besides, the scalability requirements found in the current IoT data processing (in the cloud) can hardly be used for applications such as assisted living systems, Big Data analytic solutions, and smart embedded applications. This paper proposes an extended cloud IoT model that optimizes bandwidth while allowing edge devices (Internet-connected objects/devices) to smartly process data without relying on a cloud network. Its integration with a massively scaled spine-leaf (SL) network topology is highlighted. This is contrasted with a legacy multitier layered architecture housing network services and routing policies. The perspective offered in this paper explains how low-latency and bandwidth intensive applications can transfer data to the cloud (and then back to the edge application) without impacting QoS performance. Consequently, a spine-leaf Fog computing network (SL-FCN) is presented for reducing latency and network congestion issues in a highly distributed and multilayer virtualized IoT datacenter environment. This approach is cost-effective as it maximizes bandwidth while maintaining redundancy and resiliency against failures in mission critical applications."
pub.1173453098,Kubernetes Deployment Options for On-Prem Clusters,"Over the last decade, the Kubernetes container orchestration platform has
become essential to many scientific workflows. Despite its popularity,
deploying a production-ready Kubernetes cluster on-premises can be challenging
for system administrators. Many of the proprietary integrations that
application developers take for granted in commercial cloud environments must
be replaced with alternatives when deployed locally. This article will compare
three popular deployment strategies for sites deploying Kubernetes on-premise:
Kubeadm with Kubespray, OpenShift / OKD and Rancher via K3S/RKE2."
pub.1026743202,Energy-Aware Application Scheduling and Consolidation in Mobile Cloud Computing with Load Balancing,"Mobile Cloud Computing (MCC) extends cloud computing with the advantages of mobility and wireless networks to create a new infrastructure where cloud takes over mobile devices’ responsibilities of executing tasks and storing enormous amounts of data. Through offloading, all the major data processing work takes place in the cloud instead of the mobile devices. The main aim of MCC is to achieve a rich user experience by enabling wide range of mobile devices to execute rich mobile applications. Scheduling of tasks require minimum completion time, better performance, effective utilization of resources and quick response time for which cloud uses virtualization concept. For task allocation, cloud provides virtual machines which are scalable but scheduling them while efficiently utilizing the idle service capacities of the mobile devices are still remains major problem. Likewise, there are other issues faced in MCC such as insufficient resource, low connectivity and limited energy due to which utilizing its full capability is a challenge. The existing application scheduling algorithms in MCC do not take each task’s profit or the overall energy consumption of mobile devices into consideration. Also it cannot increase the profit of the system, which is an import target for scheduling the tasks in commercial mobile cloud environment. In this paper, E-MACS (Energy-aware Mobile Application Consolidation and Scheduling) algorithm is proposed to make the mobile devices contribute their computing and sensing capabilities to attain efficient scheduling of application in hybrid cloud model. The consolidation of application minimizes the overall energy consumption in cloudlet. The proposed system minimizes the response latency, cost of application migration and it improves quality of service like throughout and scalability among resources using load balancing techniques by mobile cloud computing."
pub.1095358391,Securing User Authentication using Single Sign-On in Cloud Computing,"In past three decades., the world of computation has changed from centralized (client-server not web-based) to distributed systems and now we are getting back to the virtual centralization (Cloud Computing). This paper aims to design and implement an optimized infrastructure for secure authentication and authorization in Cloud Environment. SSO (Single Sign-On) is a process of authenticate once and gain access of multiple resources. Aim of SSO is to reduce number of login and password in heterogeneous environment and to gain balance in Security, Efficiency and Usability. This paper leads to implementation of Cloud for Storage and Virtual Machines Images to run the SSO on the top layer of Cloud. This has entailed a review and comparison of existing single sign-on architectures and solutions, the development of a new architecture for single sign-on, an analysis of single sign-on threats within a Cloud context, a derivation of single sign-on objectives in Cloud, leading up to the security requirements for single sign-on in Cloud. Security and functionality are the main driving factors in the design. Others factors include performance, reliability, and the feasibility of integration."
pub.1151327621,Recent Advances in Energy-Efficient Resource Management Techniques in Cloud Computing Environments,"Nowadays cloud computing adoption as a form of hosted application and services is widespread due to decreasing costs of hardware, software, and maintenance. Cloud enables access to a shared pool of virtual resources hosted in large energy-hungry data centers for diverse information and communication services with dynamic workloads. The huge energy consumption of cloud data centers results in high electricity bills as well as emission of a large amount of carbon dioxide gas. Needless to say, efficient resource management in cloud environments has become one of the most important priorities of cloud providers and consequently has increased the interest of researchers to propose novel energy-saving solutions. This chapter presents a scientific and taxonomic survey of recent energy-efficient cloud resource management’ solutions in cloud environments. The main objective of this study is to propose a novel complete taxonomy for energy-efficient cloud resource management solutions, review recent research advancements in this area, classify the existing techniques based on our proposed taxonomy, and open up new research directions. Besides, it reviews and surveys the literature in the range of 2015 through 2021 in the subject of energy-efficient cloud resource management techniques and maps them to its proposed taxonomy, which unveils novel research directions and facilitates the conduction of future researches."
pub.1121041049,A work load prediction strategy for power optimization on cloud based data centre using deep machine learning,"The application development industry has moved into cloud computing for stratifying the need of the customer for higher availability and higher scalability. The cloud computing environment provides the infrastructure off the premises for the application owner, which reduces the need for cost and security aspects. The primary intension of the cloud-based data centres is to virtualize the infrastructure for the applications and present it as Infrastructure As A Service (IAAS). The deployment of the applications from the application owner is done on the data centres and must be allocated to any of the pre-configured instances. The instances are again configured with virtual machines for virtualizing the computing capabilities, memory capabilities, network bandwidth capabilities and finally the storage capabilities. The deployed applications on the virtual machines must be accessible by the application consumers or the clients of the application owners. Load balancing typically includes committed programming or equipment, for example, a multilayer switch or a Domain Name System server process. Once the load is balanced, then the application performances can be justified. Great number of research endeavors have endeavored to build the presentation of the applications during load balancing by sending different calculations for distinguishing proof of the loaded virtual machines and less loaded cases for determination of the goal servers. Nevertheless, the performances of these strategies for load balancing is always criticized by various research attempts for being highly time complex and directly effecting the overall performances. Extraordinary number of research tries have attempted to construct the introduction of the applications during load balancing by sending various estimations for recognizing verification of the loaded virtual machines and less loaded cases for assurance of the objective servers. The outcome from this research is highly satisfactory and demonstrates nearly 98% accuracy on the load prediction and nearly 85% reduction on the time complexity with 88% reduction on the SLA violation."
pub.1113487820,Engineering Authentication Process with Cloud Computing,"Cloud computing is a way to increase the capacity or add capabilities dynamically without any upfront investments. Despite the growth achieved from the cloud computing, security is still questionable which impacts the cloud model adoption. Aside of having network and application securities being adopted, there must be a security that authenticate the user when accessing the cloud services that is bound to the rules between the cloud computing provider and the client side. The existing system provides authentication based on keys Encryption algorithms either symmetric key-based or asymmetric are key-based. Both encryption approaches have a major problem related to encryption key management i.e. how to securely generate, store, access and exchange secrete keys. In this paper, an optimized infrastructure for secure authentication and authorization in Cloud Environment using SSO (Single Sign-On) is proposed. SSO is a process of authenticating once and gain access of multiple resources that aims at reducing number of login and password in heterogeneous environment and to gain balance in Security, Efficiency and Usability. Also an authentication model for cloud computing based on the Kerberos protocol to provide single sign-on and to prevent against DDOS attacks is also presented in this paper."
pub.1124020143,Transitioning a Legacy Reservoir Simulator to Cloud Native Services," The digital transformation journey provides new opportunities for running simulations through cloud computing, with flexibility in hardware resources and availability of a wide array of software tools that can enhance decision-making. This work reviews the benefits of running reservoir simulations in a cloud environment and demonstrates the efficiency and cost savings. Additionally, a workflow for uncertainty analysis and history matching that integrates data analysis and machine-learning tools is presented. First, the hardware architecture must be designed to meet parallel reservoir simulation needs: significant message passing occurs between computer nodes, and for satisfactory performance, these nodes must be connected by a low-latency network, rather than be randomly located. Second, to ensure portability and easy replication across multiple cloud sites and platforms, the software performing the simulations must be containerized. Third, to reduce the time required to start a new simulation run, the Kubernetes platform is used to optimize resource allocation. Finally, reservoir simulation in the cloud is no longer merely the running of the simulation model, but it is integrated with data management and data analysis tools for decision-making. The cloud-based simulation services discussed herein exhibit good results during scale up, when a simulation operation requires a larger number of central processing units and/or greater memory, and also during scale out, when thousands of operation scenarios are necessary for history matching. The ""pay as you go"" pricing model reduces the time and capital costs of acquiring the new computing infrastructure to nearly zero, and the effectively unlimited scale-out capability can reduce the elapsed time for history matching by 80%. The availability of data centers in different regions is good for team collaborations. It serves the data management tool well to track history data, perform data mining, extract more information, and make decisions. Compared to traditional reservoir simulation, the cloud-based reservoir simulation software as a service model simplifies the process and reduces hardware acquisition and maintenance costs. Integrating intelligent data analysis with simulation helps quantify the uncertainty in the model and enables improved decisions."
pub.1093445255,A Framework for Elastic Execution of Existing MPI Programs,"There is a clear trend towards using cloud resources in the scientific or the HPC community, with a key attraction of cloud being the elasticity it offers. In executing HPC applications on a cloud environment, it will clearly be desirable to exploit elasticity of cloud environments, and increase or decrease the number of instances an application is executed on during the execution of the application, to meet time and/or cost constraints. Unfortunately, HPC applications have almost always been designed to use a fixed number of resources. This paper describes our initial work towards the goal of making existing MPI applications elastic for a cloud frame-work. Considering the limitations of the MPI implementations currently available, we support adaptation by terminating one execution and restarting a new program on a different number of instances. The components of our envisioned system include a decision layer which considers time and cost constraints, a framework for modifying MPI programs, and a cloud-based runtime support that can enable redistributing of saved data, and support automated resource allocation and application restart on a different number of nodes. Using two MPI applications, we demonstrate the feasibility of our approach, and show that outputting, redistributing, and reading back data can be a reasonable approach for making existing MPI applications elastic."
pub.1094363787,Coflourish: An SDN-Assisted Coflow Scheduling Framework for Clouds,"Existing coflow scheduling frameworks effectively shorten communication time and completion time of cluster applications. However, existing frameworks only consider available bandwidth on hosts and overlook congestion in the network when making scheduling decisions. Through extensive simulations using the realistic workload probability distribution from Facebook, we observe the performance degradation of the state-of-the-art coflow scheduling framework, Varys, in the cloud environment on a shared data center network (DCN) because of the lack of network congestion information. We propose Coflourish, the first coflow scheduling framework that exploits the congestion feedback assistances from the software-defined-networking(SDN)-enabled switches in the networks for available bandwidth estimation. Our simulation results demonstrate that Coflourish outperforms Varys by up to 75.5% in terms of average coflow completion time under various workload conditions. The proposed work also reveals the potentials of integration with traffic engineering mechanisms in lower levels for further performance optimization."
pub.1171579984,Transitioning from Data Centers to Cloud: An In-depth Analysis of Microsoft SQL Server's Role in DBaaS and On-Premise Solutions,"In the ever-evolving landscape of technology, the migration from traditional data centers to cloud-based solutions has become a pivotal paradigm shift. This research paper delves into the intricate realm of this transition by conducting an extensive examination of the role played by Microsoft SQL Server in both Database-as-a-Service (DBaaS) and On-Premise environments. Our study aims to provide a holistic understanding of the benefits and challenges associated with adopting Microsoft SQL Server in these distinct settings. By scrutinizing a wide array of literature, including national and international journals, we establish a comprehensive backdrop of DBaaS services within the cloud computing domain. Furthermore, we explore the dynamic landscape of cloud database systems, elucidating their advantages and limitations. A pivotal component of this research involves a meticulous exploration of Microsoft SQL Server's journey over various editions and versions, tracing its evolution from the nascent days of data centers to its present role in cloud solutions. In pursuit of our objectives, we conduct practical experiments where SQL queries are executed both in On-Premise and DBaaS environments. This experimentation provides valuable insights into the practical implications of the transition, shedding light on performance, functionality, and compatibility aspects. Additionally, we delve into storage allocation considerations, critically comparing the storage expansion processes in these environments. The study provides insights valuable to diverse stakeholders, including database administrators, IT managers, architects, researchers, and educators. By shedding light on the nuanced interplay between Microsoft SQL Server and evolving database paradigms, our research aims to inform decision-making processes and encourage informed choices in the realm of database technology adoption. As a continuation of our research, we also envision exploring similar dynamics in other cloud environments, such as AWS, to further enrich the discourse around database technology transitions."
pub.1151864798,Service Mesh and eBPF-Powered Microservices: A Survey and Future Directions,"Modern software development practice has seen a profound shift in architectural design, moving from monolithic approaches to distributed, microservice-based architectures. This allows for much simpler and faster application orchestration and management, especially in cloud-based systems, with the result being that orchestration systems themselves are becoming a key focus of computing research. Orchestration system research addresses many different subject areas, including scheduling, automation, and security. However, the key characteristic that is common throughout is the complex and dynamic nature of distributed, multi-tenant cloud-based microservice systems that must be orchestrated. This complexity has led to many challenges in areas such as inter-service communication, observability, reliability, single cluster to multi-cluster, hybrid environments, and multi-tenancy. The concept of service meshes has been introduced to handle this complexity. In essence, a service mesh is an infrastructure layer built directly into the microservices - or the nodes of orchestrators - as a set of configurable proxies that are responsible for the management, observability, and security of microservices. Service meshes aim to be a full networking solution for microservices; however, they also introduce overhead into a system - this can be significant for low-powered edge devices, as service mesh proxies work in user space and are responsible for processing the incoming and outgoing traffic of each service. To mitigate performance issues caused by these proxies, the industry is pushing the boundaries of monitoring and security to kernel space by employing eBPF for faster and more efficient responses. We propose that the movement towards the use of service meshes as a networking solution for most of the required features by industry - combined with their integration with eBPF - is the next key trend in the evolution of microservices. This paper highlights the challenges of this movement, explores its current state, and discusses future opportunities in the context of microservices."
pub.1142331402,Heuristic and Reinforcement Learning Algorithms for Dynamic Service Placement on Mobile Edge Cloud,"Edge computing hosts applications close to the end users and enables
low-latency real-time applications. Modern applications inturn have adopted the
microservices architecture which composes applications as loosely coupled
smaller components, or services. This complements edge computing infrastructure
that are often resource constrained and may not handle monolithic applications.
Instead, edge servers can independently deploy application service components,
although at the cost of communication overheads. Consistently meeting
application service level objectives while also optimizing application
deployment (placement and migration of services) cost and communication
overheads in mobile edge cloud environment is non-trivial. In this paper we
propose and evaluate three dynamic placement strategies, two heuristic (greedy
approximation based on set cover, and integer programming based optimization)
and one learning-based algorithm. Their goal is to satisfy the application
constraints, minimize infrastructure deployment cost, while ensuring
availability of services to all clients and User Equipment (UE) in the network
coverage area. The algorithms can be extended to any network topology and
microservice based edge computing applications. For the experiments, we use the
drone swarm navigation as a representative application for edge computing use
cases. Since access to real-world physical testbed for such application is
difficult, we demonstrate the efficacy of our algorithms as a simulation. We
also contrast these algorithms with respect to placement quality, utilization
of clusters, and level of determinism. Our evaluation not only shows that the
learning-based algorithm provides solutions of better quality; it also provides
interesting conclusions regarding when the (more traditional) heuristic
algorithms are actually better suited."
pub.1166311684,A Novel Energy and Communication Aware Scheduling on Green Cloud Computing,"The rapid growth of service-oriented and cloud computing has created large-scale data centres worldwide. Modern data centres’ operating costs mostly come from back-end cloud infrastructure and energy consumption. In cloud computing, extensive communication resources are required. Moreover, cloud applications require more bandwidth to transfer large amounts of data to satisfy end-user requirements. It is also essential that no communication source can cause congestion or bag loss owing to unnecessary switching buffers. This paper proposes a novel Energy and Communication (EC) aware scheduling (EC-scheduler) algorithm for green cloud computing, which optimizes data centre energy consumption and traffic load. The primary goal of the proposed EC-scheduler is to assign user applications to cloud data centre resources with minimal utilization of data centres. We first introduce a Multi-Objective Leader Salp Swarm (MLSS) algorithm for task sorting, which ensures traffic load balancing, and then an Emotional Artificial Neural Network (EANN) for efficient resource allocation. EC-scheduler schedules cloud user requirements to the cloud server by optimizing both energy and communication delay, which supports the lower emission of carbon dioxide by the cloud server system, enabling a green, unalloyed environment. We tested the proposed plan and existing cloud scheduling methods using the GreenCloud simulator to analyze the efficiency of optimizing data centre energy and other scheduler metrics. The EC-scheduler parameters Power Usage Effectiveness (PUE), Data Centre Energy Productivity (DCEP), Throughput, Average Execution Time (AET), Energy Consumption, and Makespan showed up to 26.738%, 37.59%, 50%, 4.34%, 34.2%, and 33.54% higher efficiency, respectively, than existing state of the art schedulers concerning number of user applications and number of user requests."
pub.1164178424,"Proceedings of the 2008 Ambi-Sys workshop on Software Organisation and MonIToring of Ambient Systems, SOMITAS '08","Weiser's dream of an environment enhanced with a set of invisible computing devices is slowly becoming a reality. While most technological requirements can be fulfilled with the current technology, there are still many open questions regarding how to design, build and deploy this kind of systems. It is a quite remarkable fact that the world of software engineering has been in a sense surprised by the world of electronics in such a way that we now have sensors and multimodal interactors and no rigourous methodology to create context-aware programs. Moreover, existing monitoring systems for networked computerized systems are not obviously able to adapt to these new systems. We can think of a future where humans interact with a seamlessly integrated cloud of processes and a partly invisible set of devices. What are suitable system architectures for dynamic multi-device appliances? Which programming languages cope best with the needs? How should we devise our applications in order to adapt to hardware evolution? How can our systems interact with previously unknown sensor networks? How flexible can applications adapt to new situations and what level of anticipation is unavoidable? How should systems be presented to end users with such a variety of computing devices? How shall we monitor the different devices and the global architecture ?"
pub.1094864648,Desktop to Cloud Transformation Planning,"Traditional desktop delivery model is based on a large number of distributed PCs executing operating system and desktop applications. Managing traditional desktop environments is incredibly challenging and costly. Tasks like installations, configuration changes, security measures require time-consuming procedures and dedicated deskside support. Also these distributed desktops are typically underutilized, resulting in low ROI for these assets. Further, this distributed computing model for desktops also creates a security concern as sensitive information could be compromised with stolen laptops or PCs. Desktop virtualization, which moves computation to the data center, allows users to access their applications and data using stateless “thin-client” devices and therefore alleviates some of the problems of traditional desktop computing. Enterprises can now leverage the flexibility and cost-benefits of running users' desktops on virtual machines hosted at the data center to enhance business agility and reduce business risks, while lowering TCO. Recent research and development of cloud computing paradigm opens new possibilities of mass hosting of desktops and providing them as a service. However, transformation of legacy systems to desktop clouds as well as proper capacity provisioning is a challenging problem. Desktop cloud needs to be appropriately designed and provisioned to offer low response time and good working experience to desktop users while optimizing back-end resource usage and therefore minimizing provider's costs. This paper presents tools and approaches we have developed to facilitate fast and accurate planning for desktop clouds. We present desktop workload profiling and benchmarking tools as well as desktop to cloud transformation process enabling fast and accurate transition of legacy systems to new cloud-based model."
pub.1126924446,Cloud Migration Process A Survey Evaluation Framework and Open Challenges,"Moving mission-oriented enterprise applications to cloud environments is a
major IT strategic task and requires a systematic approach. The foci of this
paper are to review and examine existing cloud migration approaches from the
process models perspective. To this aim, an evaluation framework is proposed
and used to analyse and compare existing approaches for highlighting their
features, similarities, and key differences. The survey distills the state of
the art in cloud migration research and makes a rich inventory of important
activities, recommendations, techniques, and concerns that are commonly
involved in the migration process in one place. This enables academia and
practitioners in the cloud computing community to get an overarching view of
the cloud migration process. Furthermore, the survey identifies a number
challenges that have not been yet addressed by existing approaches, developing
opportunities for further research endeavors."
pub.1148907510,IT Infrastructure for Smart City: Issues and Challenges in Migration from Relational to NoSQL Databases,"Smart city applications collect massive amount of data from various types of IoT sensors, engines, and people. Most of the data generated are heterogeneous in nature. However, this data need to be integrated with legacy applications based on SQL. Some of these applications also require migration to NoSQL for improved performance and fault tolerance. This paper addresses challenges of working in hybrid environments and migration issues from SQL to NoSQL databases. Rapid rate of growth in heterogeneous data and characteristics of NoSQL database like easy scalability, high availability, high performance, and low cost are the motivating factors to migrate toward NoSQL from relational databases, especially for applications requiring dealing with unstructured data. NoSQL database gives dynamic schema, adjustable data model, scale out architecture, and allows storage of big data and access to it in an efficient manner. The relational databases store data in form of tables with fixed schema. Relational databases are structured and not capable of handling unstructured and big data. In relational databases, because of normalization, data is spread across multiple tables and expensive join operation is required to integrate data. Due to limitations of relational databases, most of the leading organizations are migrating toward NoSQL. NoSQL databases are analyzed into four categories- Key-value database, Column-oriented database, Document-based database, and Graph database. Key research issues and challenges in migrating from relational to NoSQL include model transformation (including mapping and schema conversion), application integration, strategies related to perform join in different scenarios, use of indexes, storage issues, etc. Massive growth is likely in the area of NoSQL over cloud because of initiatives like Smart City, however, these applications required integration with legacy applications built over SQL. Thus need for migration as well as bridges for connection leading to requirements of our research area. This chapter tends to explore the comparative study of Relational databases and NoSQL databases, classification of NoSQL databases, case study, popular approaches for migration, and some of the key issues and challenges in migrating from Relational to NoSQL databases. This chapter will be fruitful for students and researchers too."
pub.1140505890,Application Of The IoT Technology and Cloud Services for Radiation Monitoring,"Introduction. Cloud services are the most promising technologies for monitoring radiation pollution. They are a set of geographically distributed wireless sensor nodes designed to collect, sometimes pre-process, information about environmental parameters, as well as to transmit this information to remote users.  Purpose. Development of basic methods for applying cloud services for IoT radiation and Environmental Research Technology. A comprehensive assessment of the state of the ecosystem, including its impact on humans, was carried out. At the same time, a promising direction is proposed, namely the integration of on-premises measuring devices with cloud services using M2M/IoT technology for remote measurement, the use of promising semiconductor sensors based on CdTe and CdZnTe radiation detectors, and modern microcontrollers.  Methods. Use of methods Wi-Fi access point, control of the sensor network via a smartphone to transfer data parameters to the ThingSpeak cloud service. Results. When writing approaches to building cloud services, the composition of each sensor node and the task that it performs are considered, such as: the type of data collected, location, power sources, and the possibility of using certain protocols for data exchange relationships.  Conclusion. The analysis of unified cloud services that include methods of designing information and measurement systems, methods of building machine-machine and human-machine interfaces, methods of designing sensor networks, methods of computer modeling of electronic circuits and systems, hardware emulation method (based on QEMU), methods of analysis, system analysis, synthesis, logical generalization of results. It includes selecting and connecting layout hardware, data module software that is developed in the ThingSpeak environment using the HTML markup language to describe the configuration web page."
pub.1171755927,Towards a federated and hybrid cloud computing environment for sustainable and effective provisioning of cyber security virtual laboratories,"Cloud Computing (CC) and virtualization concepts are two advanced technologies introduced to empower distance and blended learning. Besides, they play a crucial role in equipping learners with practical skills and fostering hands-on experience to defend against cyber-attacks. Many Higher Education Institutions (HEIs) in developed countries have already embraced the promise of CC to raise educational standards. However, the pace of its adoption in developing countries has stagnated. Moreover, existing solutions in the literature are not sustainable. They either rely on on-premise infrastructure or are bound by a single cloud service provider. Consequently, they are likely prone to failures and a sudden outage. To fill this gap, this paper is a first that comprehensively addresses the above issues and introduces a federated hybrid CC system based on an extension of Apache Virtual Computing Lab (VCL). The proposed system provides an independent open-source implementation, greater configuration flexibility, and methodological improvements as compared to existing studies in the literature. In addition, it promotes the sustainability of the CC services, extensible cloud architecture, and fault tolerance. VCL is primarily focused on provisioning Virtual Laboratories (VL) for remote cybersecurity and computer networks education, with potential expansion to other domains of engineering education. In addition, this paper introduces GPT-TerminalPro, a terminal-based tool driven by OpenAI’s Generative Pretrained Transformer (GPT-3.5) that provides intelligent assistance to users while performing lab tasks. To experimentally evaluate the VCL’s performance, the standard Linux tools as well as the Apache benchmark and httperf HTTP load generators are utilized. VCL has been tested with 30 users and 61 virtual user computing environments provisioning to validate the overall performance. The results are fascinating: the provisioning time including all VCL background tasks is always less than a minute and utilizes fewer computing resources while providing a better user experience. This paper will encourage the adoption of CC in low-income countries."
pub.1068798919,FEATURES OF THE CLOUD SERVICES IMPLEMENTATION IN THE NATIONAL NETWORK SEGMENT OF UKRAINE,"Background. Cloud computing environments and services on their basis offering unprecedented cost savings, improved exchange of information and the efficiency of the infrastructure. Accordingly, by using these solutions we can increase the efficiency of the national network services segment. Thus the scalable service architecture development of network systems is a key aspect which directly forms the preferred national electronic services, both public and private sectors. Objective. The aim of the paper is to find the ways of optimal consolidation of cloud service systems for the timely transformation of single national information space Ukraine. According to the authors, providing an opportunity to influence these processes by government oversight agencies is critical to Ukraine Methods. Relevance of research in this area is confirmed by the rapid development of commercial technologies that make cloud implementation process simpler, safer and much more productive on the criteria of transparency, integration, expansion (scalability), quality of service. The introduction of cloud services by public and private operators of distributed service platforms requires consolidation of needs for cloud service technologies, and the harmonization of tariffs for their use by public oversight bodies such as NCCIR of Ukraine. Obviously, the developers of cloud service solutions must have the tools available to maintain service availability and quality, based on the technical and target parameters. Results. Proposed architectural features of the cloud services implementation technologies, including private cloud using existing structures to support the government and civil service systems at significantly reducing capital expenditures, provided support to the required level of security. Conclusions. Creation of a common cloud service network information environment requires data-centered model (in the long term DaaS), which has trusted data processing center based core, provided with the decent level of protection and functionality, spreading a corresponding policy on all joint data processing centers of national segment cloud network system by means of interoperability, which allows to deliver services demanded here and now, using replication and migration of corresponding services and link data (including inter-operator and cross-platform)."
pub.1128493278,Security and trust in cloud application life-cycle management," Cloud technologies are currently experiencing a remarkable degree of pervasiveness in most of the applications and services that are consumed daily by both individual users and companies. The sheer volume of sensitive data and operations that are regularly outsourced to the cloud calls for the adoption of adequate measures to fulfill the existing security requirements and to increase the trust into cloud providers. The papers included in this special issue address a variety of concerns related to the development of secure, reliable, and trusted cloud applications, including: enforcement of specific security mechanisms (e.g., to protect shared data), integrity verification of virtualized environments, and selection of the most appropriate cloud services/resources for application deployment."
pub.1160834341,Enabling enterprise autonomy development with a unified data infrastructure,"Data is the cornerstone of Artificial Intelligence (AI) and Machine Learning (ML) systems. As the Department of Defense (DoD) leverages AI/ML to develop, test, and deploy autonomous vehicle capabilities, management of autonomy data will become increasingly important. Modern sensors on autonomous vehicles generate an enormous amount of data, and making this data available for further research presents a significant challenge. Moving such large volumes of data from a field environment to a centralized, cloud-based data lake is not straightforward, nor necessarily efficient for data of unknown enterprise utility. As a result, much of DoD’s autonomy data remains siloed in geographically or logically separated on-premises and cloud-based data stores in mixed formats. Organizations within DoD’s modernization enterprise require a mature data infrastructure to store, discover, share, and collaborate upon datasets, models, and other artifacts efficiently. In this paper, we examine the characteristics a data infrastructure must exhibit to meet the needs of the DoD for autonomy research. These characteristics are identified through a review of existing solutions, use cases, and current industry best practices. On the basis of this review, we propose a set of requirements for DoD’s data infrastructure for autonomous systems research. Moreover, an analysis of the viability of various options, including centralized and decentralized architectures, is provided through the lens of DoD data requirements and unique organizational constraints. While data infrastructure for autonomy is our primary concern, the requirements and design we propose generalize to other AI tasks that are of interest to DoD."
pub.1165853341,A Framework for Anything-as-a-Service on a Cloud platform,"There is an increasing need for computational and storage capabilities for complex distributed applications. Existing solutions need to be deployed in an environment that allows for an increase in performance, scalability, and availability. This paper takes looks at the state-of-the-art regarding methods that take existing applications and make them more efficient by using Cloud services. The novelty of the paper consists of a proposed framework for deploying applications on three major Cloud providers (i.e., Amazon’s AWS, Google Cloud and Microsoft Azure) and on the OpenStack open-source Cloud. After the main services from the four Cloud providers are identified, different deployment methods are described depending on the Cloud services and on the requirements of the application. Also, some examples of migrations are discussed with reference to specific Cloud provider services. The proposed solution for Anythingas-a-Service (YaaS) is a straightforward framework for taking different types of applications and migrating them to the Cloud. Therefore, the deployed applications benefit from Cloud features such as resource pooling, availability or scalability, while also being wary of the incurring costs."
pub.1068569265,Smart cities and cloud computing: lessons from the STORM CLOUDS experiment,"Since the emergence of cloud computing paradigm, there has been an increasing interest on the adoption of cloud computing from municipalities and city governments towards their effort to address complex urban problems. This paper explores the significant role that cloud computing can play in helping cities on their way to become smart. We focus on the STORM CLOUDS paradigm as a solution for municipalities everywhere in order to (i) deploy a portfolio of smart cities applications related to governance, economy and quality of life on a single cloud-based platform and (ii) use the platform and its accompanied tools to migrate their existing applications to the cloud environment. Besides the conclusions from the STORM experience, the paper closes with a number of research trends and future challenges that are expected to define the adoption of cloud computing from municipalities and city governments in the following years"
pub.1154038259,Requirements and architecture design for cloud PaaS orchestrator,"Cloud technologies provide abilities for simple and reliable scaling of resources, due to which they have become widespread. The task of managing distributed services in a cloud environment is especially relevant today. Special programs are used for that purpose named “orchestrators” which implement the functions of lifecycle management for applications. However, the existing solutions have many limitations and are not applicable in the general case. Also there is no single standard or protocol for interaction with such tools which requires adaptation of programs for each particular case. The main objectives of this paper are to identify the requirements for a platform-level cloud computing (PaaS) orchestrator, as well as to propose flexible architecture patterns for such tools."
pub.1103266296,Local Storage-Based Consolidation With Resource Demand Prediction and Live Migration in Clouds,"Server consolidation is a useful solution aiming at cost-efficiency and high resource utilization of data centers and clusters. Nowadays, as the data-intensive and I/O intensive applications are widely used, more attention is paid to the local storage-based clouds which can offer much better I/O performance at relatively low price compared with the shared storage. However, it will obviously increase the migration cost (e.g., energy and time). Meanwhile, there is few suitable resource demand estimation method for local storage-based clouds at present, which plays an important role in system's migration efficiency. And we find out that in this specific storage architecture, almost all the existing server consolidation algorithms do not have a suitable resource demand estimation method and a live migration scheme. To solve this problems, this paper designs and implements Combining Three (C3), a cloud architecture for local storage, C3 significant modules: prediction, consolidation, and migration. It was proved in statistical analysis that ARIMA may be the most suitable prediction model for the server workload, which motivates us to propose the resource estimation predictor. Also, we improve the existing consolidation method by adjusting the sorting index and fit degree during migration. Then, we propose a live migration scheme for local storage environment as the third module of C3. We conduct extensive experiments using real-world traces from Google to validate the effectiveness and superiority of our proposed algorithm."
pub.1154464442,Validation of NFV management and orchestration on Kubernetes-based 5G testbed environment,"Prior work has shown that the integration of Kubernetes orchestration tools with present Network Functions Virtualization infrastructures toward Cloud-based 5G deployments may be the key to unlock beyond 5G communications. However, before we reach that point, further work is required to define, implement and validate practical Cloud-native augmentations that will enable virtual network functions management and orchestration via Kubernetes architectures in existing 5G platforms. In this paper, we present our approach in the context of the 5GEPICENTRE project, drawing from ETSI-compliant reference implementation frames in order to enhance an experimental 5G testbed with capacity to host containerized network applications. We study how the proposal can be used to validate key performance indicators on high-demanding 5G applications, such as those characteristic of the Public Protection and Disaster Relief vertical."
pub.1118023461,A Comprehensive Survey of Load Balancing Strategies Using Hadoop Queue Scheduling and Virtual Machine Migration,"The recent growth in the demand for scalable applications from the consumers of the services has motivated the application development community to build and deploy the applications on cloud in the form of services. The deployed applications have significant dependency on the infrastructure available with the application providers. Bounded by the limitations of available resource pools on-premises, many application development companies have migrated the applications to third party cloud environments called data centers. The data center owners or the cloud service providers are entitled to ensure high performance and high availability of the applications and at the same time the desired scalability for the applications. Also, the cloud service providers are also challenging in terms of cost reduction and energy consumption reductions for better manageability of the data center without degrading the performance of the deployed applications. It is to be noted that the performance of the application does not only depend on the responsiveness of the applications rather also must be measured in terms of service level agreements. The violation of the service level agreements or SLA can easily disprove the purpose of application deployments on cloud-based data centers. Thus, the data center owners apply multiple load balancing strategies for maintaining the desired outcomes from the application owners at the minimized cost of data center maintainability. Hence, the demand of the research is to thoroughly study and identify the scopes for improvements in the parallel research outcomes. As the number of applications ranging from small data-centric applications coming with the demand of frequent updates with higher computational capabilities to the big data-centric application as big data analytics applications coming with efficient algorithms for data and computation load managements, the data center owners are forced to think for efficient algorithms for load managements. The algorithms presented by various research attempts have engrossed on application specific demands for load balancing using virtual machine migrations and the solution as the proposed algorithms have become application problem specific. Henceforth, the further demand of the research is a guideline for selecting the appropriate load balancing algorithm via virtual machine migration for characteristics-based specific applications. Hence, this paper presents a comprehensive survey on existing virtual machine migration and selection processes to understand the specific application-oriented capabilities of these strategies with the advantages and bottlenecks. Also, with the understanding of the existing measures for load balancing, it is also important to furnish the further improvement strategies, which can be made possible with a detailed understanding of the parallel research outcomes. Henceforth, this paper also equips the study with guidelines for improvements and for further study. N"
pub.1099908346,Dependency management in the cloud: An efficient proposal for Java,"Since the beginning of the 90s, different tools have emerged to support integration and upgrading of most of the existing operating systems. These tools, called Package Managers, are used to atomically add and remove software pieces coming from external repositories. Based on Package Managers, the software community created Dependency Managers with the aim to give support to the development of wide-scale software. These tools interact with library repositories, aiding development environments in the dependency recovery and closure processes. Although the adaptation of the Package Manager model to software development has been successful, it suffers from certain drawbacks that significantly impact on productivity. This work offers a proposal that updates the way in which Java software dependencies are resolved, moving from a model centered on workstations, descriptor files and repositories to a service specialized in resolving dependencies that can support current and future demands of the software industry."
pub.1154119704,Convergence Perceptual Model for Computing Time Series Data on Fog Environment,"The evolution of fog/edge paradigm is at the rising edge that complements data analysis and data computing algorithms in a big data platform, The role of fog nodes in a fog computing set up shall best suit time sensitive applications especially on virtual clusters supporting edge devices in sensing, processing, controlling and action planning, there by replacing the non-virtualized complicated computing mechanisms resolving the existing complexity in cloud computing storage and retrieval algorithms. There is an emerging need for addressing collaborative processes in various sectors not limiting to computing digital data on big data platform but also the futuristic industrial revolution to replace the existing techniques and technologies in industrial automation. The proposed work reveals a novel framework that supports the cutting edge of fog node fitting as an intermediate layer between the edge devices and cloud storage. Computational by means of fog-node attracts the need by replacing existing cloudlets time complexity in resource management. Hence fog as things operates as a controller in cyber physical systems like controlling transmission lines of high volt system connections, monitoring smart metering that has a wide range of applications through the integration of the industrial automation process."
pub.1169585151,The European Weather Cloud (EWC) – Collaboration Platform for Meteorological Development from Research to Applications,"The European Weather Cloud (EWC) is the cloud-based collaboration platform for meteorological application development and operations in Europe and to enable the digital transformation of the European Meteorological Infrastructure. It consists of data-proximate cloud infrastructure established by the EUMETSAT and ECMWF. The EWC is open and partners partners can federate the access to their data or infrastructure assets. The EWC is available for EUMETSAT and ECMWF Member and Cooperating States and EUMETSAT Satellite Application Facilities (SAFs) covering both research and operational use cases. Resources are also available for research initiatives, undertaken by one or more EUMETSAT or ECMWF Member States, via specific EUMETSAT Research and Developoperament (R&D) calls and ECMWF Special Projects. Currently, EWC hosts 16 R&D calls and Special Projects, lasting 1-3 years. The EWC focuses very much on the community taking an iterative user needs-based approach in the development. Notably, research projects and operational applications use the very same environment, which smooths the transition from research to operations (R2O). The hosted services will also be augmented with the Software Marketplace, providing EWC users with the ability to easily share and exploit meteorological applications, algorithms, and machine-learning models. The EWC facilitates a Rocket.chat-based discussion platform for users to discuss and work together, promoting in practice the fundamental collaborative nature of this cloud offering.   EWC hosts over 132 diverse types of use cases containing, for example, data processing, data services, application development, training, EO and weather data image production, post-processing, and experimenting with cloud technologies. To name a few examples in more detail, the FEMDI project, consisting of 11 European meteorological services, develops data services employing EWC for distributing open meteorological data to fulfil the EU Open Data directive requirements. Second, the Norwegian Meteorological Institute (MET) is piloting an infrastructure federation to create water-quality products by locating the processing chain close to the data. Lastly, numerous projects are developing machine-learning-based models in EWC, including e.g. nowcasting, medium-term weather forecasting, and feature detection from climate data.   The main data holding accessible to the EWC users is the sum of all online data and products available at ECMWF and EUMETSAT. Services to access the data support both pull and push paradigms for long time series and time-critical access respectively. The services are supported by related functions, such as display, reformat, etc., as per applicable policies. The data offering will be augmented over time based on user needs.  From a technological viewpoint, the initiative offers services that carry the highest benefits from cloud technology taking the users’ needs, use cases, and existing software into account. EWC looks f"
pub.1157221540,Raven: Benchmarking Monetary Expense and Query Efficiency of OLAP Engines on the Cloud,"Nowadays, it is prevalent to build OLAP services on cloud platforms. Cloud OLAP adopters are eager to understand and characterize the performance of OLAP engines on the cloud. However, traditional OLAP benchmarks are usually designed for on-premise environments. When evaluating cloud OLAP engines, they have limitations on cloud environment adaption and cloud scenario benchmark execution. To address these issues, this paper proposes Raven, a cloud-oriented OLAP benchmark with flexible system architecture and diversified workloads. Raven supports cloud service deployment and various cloud OLAP engine integration. In addition, to simulate complex cloud query scenarios, we design a group of timeline-based and service-oriented workloads. We implement Raven on the Amazon AWS cloud platform and use it to evaluate typical types of widely-used OLAP engines, including Presto, SparkSQL, Kylin, and Athena. Experimental results show that Raven can effectively benchmark diversified OLAP engines. Besides, Raven can benchmark various configuration settings of an identical OLAP engine. We also explore an OLAP case study on the cloud using Raven."
pub.1146291384,Service-Oriented Reliability Modeling and Autonomous Optimization of Reliability for Public Cloud Computing Systems,"The ubiquitous adoption of public clouds has resulted in the bloom of big data, Internet of Things, and artificial intelligence (AI) and a great capacity for applications. However, the efficient operation of these applications is challenging due to the infrastructure heterogeneity, computing hierarchy, scale distribution, and stochastic user behaviors. Among these challenges, reliability has received intense interest in the cloud community. Most existing research on the reliability of clouds is on fault discovery or fault tolerance. However, a public cloud has a wider range of failures, which makes it difficult to analyze the association between reliability and other indicators. More accurate reliability modeling methods and advanced optimization approaches are needed to ensure the reliable operation of a public cloud. To address these challenges, this article models the reliability of cloud computing from the service perspective and reasonably divides cloud services into the request processing phase and request execution phase when facing multiple users and service types. Moreover, this article combines a variety of AI-based methods, such as autonomous scheduling mechanisms, anomaly detection, and autonomous learning improvement, which can effectively adapt to the dynamic and complex cloud service environment and ensure the service reliability of a public cloud. Numerical results clearly indicate that the proposed service reliability model and autonomous optimizations are efficient for recovering the reliability of cloud computing systems in terms of multiple AI-based failure prognostics and intelligent learning ability."
pub.1107488206,The Conjunction of Fog Computing and the Industrial Internet of Things - An Applied Approach,"Industrial applications benefit from cloud computing deployments in terms of scalability, manageability, and cost. However, cloud-only implementations do not meet the requirements of industrial applications such as latency, network traffic reduction, and reliability. Fog computing is a promising approach to overcome the limitations of cloud computing in terms of realtime and network utilization. Fog computing already found its application in various areas such as Augmented Reality and Smart Cities. This paper presents two case studies which show the applicability of the fog paradigm for Industrial Internet of Things (IIoT) applications. IIoT applications are characterized by a high degree of integration and flexibility. The FEAt case study shows how the availability of a system can be increased by using a fog computing architecture and at the same time low latency for the analysis of sensor data can be achieved. In the second case study, FARADAY, we optimize bandwidth utilization in an existing industry environment by applying a fog-based solution. We show, that fog computing is applicable for industrial applications, helps to overcome problems associated with cloud deployments, and leads to reliable applications."
pub.1113440581,Osmotic Bio-Inspired Load Balancing Algorithm in Cloud Computing,"Cloud computing is increasing rapidly as a successful paradigm presenting on-demand infrastructure, platform, and software services to clients. Load balancing is one of the important issues in cloud computing to distribute the dynamic workload equally among all the nodes to avoid the status that some nodes are overloaded while others are underloaded. Many algorithms have been suggested to perform this task. Recently, worldview is turning into a new paradigm for optimization search by applying the osmosis theory from chemistry science to form osmotic computing. Osmotic computing is aimed to achieve balance in highly distributed environments. The main goal of this paper is to propose a hybrid metaheuristics technique which combines the osmotic behavior with bio-inspired load balancing algorithms. The osmotic behavior enables the automatic deployment of virtual machines (VMs) that are migrated through cloud infrastructures. Since the hybrid artificial bee colony and ant colony optimization proved its efficiency in the dynamic environment in cloud computing, the paper then exploits the advantages of these bio-inspired algorithms to form an osmotic hybrid artificial bee and ant colony (OH_BAC) optimization load balancing algorithm. It overcomes the drawbacks of the existing bio-inspired algorithms in achieving load balancing between physical machines. The simulation results show that OH_BAC decreases energy consumption, the number of VMs migrations and the number of shutdown hosts compared to existing algorithms. In addition, it enhances the quality of services (QoSs) which is measured by service level agreement violation (SLAV) and performance degradation due to migrations (PDMs)."
pub.1164106631,Dual-Phase Resource Allocation Algorithm in Software-Defined Network SDN-Enabled Cloud,"Software Defined Networks enabled-cloud (SDN-Cloud) is experiencing rapid evolution to accommodate the explosive growth of data-driven applications. However, traditional resource allocation algorithms are encountering limitations in efficient resources management. While some existing algorithms strive to minimize power consumption, they introduce network delays, impacting overall performance. Thus, this study aims to address the prevalent challenges of performance efficiency and energy saving within distributed systems. Artificial Intelligence techniques including machine learning and fuzzy logic, are increasingly utilized to develop more adaptive and intelligent resource management models. However, given the dynamic nature of SDN-cloud environments, rapid decision-making during VM allocation is essential to prevent network delays. Furthermore, the limited computational resource of SDN controller requires cautious consideration, as extensive calculations will result in network overhead or increased power consumption. Moreover, achieving subtle balance between network performance and power efficiency still an open challenge. This research introduces Dual-Phase resource allocation Algorithm (D-Ph) for heterogeneous SDN-Cloud networks with the integration of fuzzy logic. D-Ph algorithm indicates the level of utilization of both physical and virtual machines (PM and VM) in datacenters. It aims to find the appropriate host with the necessary capabilities to meet VM resource requirements, specifically processing capacity and memory. The performance of the D-Ph algorithm is evaluated by measuring the response time, serve time of network and central processing unit (CPU), Quality of Service (QoS) violation rate, and power consumption. Results have shown distinctly that D-Ph algorithm maintain high network performance while significantly reduce total power consumption in heavy-loaded large scale network."
pub.1163926121,An IoT-based Cloud Data Platform with Multi-Region Fog Cloud for Worldwide Ship Multi-Service,"As technologies such as eco-friendly ships, electric propulsion vessels, and multi-fuel propulsion systems advance, the scope of IoT applications in maritime field is expanding, resulting in increased complexity in control factors. The gradual progression towards Maritime Autonomous Surface Ships (MASS) is further driving the evolution of ship-based IoT applications. These advancements underscore the necessity for a platform capable of ensuring reliable connectivity between ships and onshore, while also being adaptable for deployment and operation to meet the demands of service availability. The limitations of the existing single cloud architecture become evident in this context. In response to these emerging challenges, this paper presents a cloud-based horizontal data platform structure anchored in the architecture of a Multi-Region Fog Cloud. This approach is tailored to fulfill the crucial requirement of maintaining robust and seamless connectivity for vessels navigating globally. Leveraging the capabilities of the Fog Cloud, the platform enhances service responsiveness by facilitating localized data collection and distribution, complemented by efficient data caching from the Cloud Center, serving users within the same geographical region. We outline a strategic framework for enhancing and executing key ship services within the proposed architecture. These services encompass data sharing between ships to elevate collision avoidance, disseminating weather data for optimized route planning, and constructing an operator training simulator (OTS) environment through the integration of digital twin technology. After, we implement an architecture using the AWS Cloud and gauges latency and Packet Loss Rate (PLR) through connections to Seoul, California, and Frankfurt regions. The test results reveal that latency in different regions is about 2-5 times higher than that in the same region for the payload. Furthermore, packet loss occurred at roughly 2-3 Hz packet generation rate (PGR) in different regions, whereas in the same region, it commenced at approximately 10-20 Hz PGR."
pub.1014017089,Recent advancements in resource allocation techniques for cloud computing environment: a systematic review,"There are two actors in cloud computing environment cloud providers and cloud users. On one hand cloud providers hold enormous computing resources in the cloud large data centers that rent the resources out to the cloud users on a pay-per-use basis to maximize the profit by achieving high resource utilization. On the other hand cloud users who have applications with loads variation and lease the resources from the providers they run their applications within minimum expenses. One of the most critical issues of cloud computing is resource management in infrastructure as a service (IaaS). Resource management related problems include resource allocation, resource adaptation, resource brokering, resource discovery, resource mapping, resource modeling, resource provisioning and resource scheduling. In this review we investigated resource allocation schemes and algorithms used by different researchers and categorized these approaches according to the problems addressed schemes and the parameters used in evaluating different approaches. Based on different studies considered, it is observed that different schemes did not consider some important parameters and enhancement is required to improve the performance of the existing schemes. This review contributes to the existing body of research and will help the researchers to gain more insight into resource allocation techniques for IaaS in cloud computing in the future."
pub.1148158741,Autonomous Sensing at the Edge,"Migrating data processing tasks from the Cloud to Edge nodes are becoming standard practice. Such migration in-creases the complexity of system design and implementation. This paper introduces AES2F, an autonomous Edge-sensing software framework that seeks to resolve complexity and scalability issues. AES2F harnesses a microservice architecture; in particular, the framework is characterized by its communication, naming conventions, consistency, and fault-tolerance aspects. AES2F aggregates different kinds of data from diverse sources, for example, in-situ sensors, satellites, and legacy data. Such aggregation is a prerequisite for enabling intelligent decision support such as would be expected in Smart Agriculture. Experimental results show that AES2F can manage some inherent complexity and scalability issues common in dynamic environments."
pub.1148167522,Resource Allocation Using Phase Change Hyper Switching Algorithm in the Cloud Environment,"Cloud computing is one of the emerging technology; it provides various services like Software as a Service, Platform as a Service, and Infrastructure as a Service on demand. It reduces the cost of traditional computing by renting the resources instead of buying them for a huge cost. The usage of cloud resources is increasing day by day. Due to the heavy workload, all users cannot get uninterrupted service at some time. And the response time of some users also gets increased. Resource allocation is one of the primary issues of a cloud environment, one of the challenging problems is improving scheduling performance and reducing waiting time performance. In this research work, a new approach is proposed to distribute the heavy workload to overcome this problem. In addition to that, the resource allocation of dynamic user requests is also taken into study. The Max-Min scheduling algorithm is modified concerning Dynamic Phase Change Memory (DPCM), and the Hyper switching algorithm is implemented to speed up the resource allocation process. The proposed DPCM algorithm provides efficient performance, improves scheduling results, and minimizes the waiting time. In the experimentation analysis, it is observed that this proposed approach optimizes the response time and waiting time of the dynamic user requests and distributes the workloads effectively, compared with other existing approaches. So the performance of the resource allocation process is improved, which enhances the efficiency of the cloud system."
pub.1093865051,CEPSim: A Simulator for Cloud-Based Complex Event Processing,"As one of the Vs defining Big Data, data velocity brings many new challenges to traditional data processing approaches. The adoption of cloud environments in complex event processing (CEP) systems is a recent architectural style that aims to overcome these challenges. Validating cloud-based CEP systems at the required Big Data scale, however, is often a laborious, error-prone, and expensive task. This article presents CEPSim, a new simulator that has been developed to facilitate this validation process. CEPSim extends CloudSim, an existing cloud simulator, with an application model based on directed acyclic graphs that is used to represent continuous CEP queries. Once defined, the queries can be simulated in different cloud environments under diverse load conditions. Moreover, CEPSim is also customizable with different operator placement and scheduling strategies. These features enable researchers and system architects to experiment with different configurations and strategies and to promote research in this field. Experimental results show that CEPSim can successfully simulate existing cloud-based CEP systems."
pub.1061786720,Reliability-Based Design Optimization for Cloud Migration,"The on-demand use, high scalability, and low maintenance cost nature of cloud computing have attracted more and more enterprises to migrate their legacy applications to the cloud environment. Although the cloud platform itself promises high reliability, ensuring high quality of service is still one of the major concerns, since the enterprise applications are usually complicated and consist of a large number of distributed components. Thus, improving the reliability of an application during cloud migration is a challenging and critical research problem. To address this problem, we propose a reliability-based optimization framework, named ROCloud, to improve the application reliability by fault tolerance. ROCloud includes two ranking algorithms. The first algorithm ranks components for the applications that all their components will be migrated to the cloud. The second algorithm ranks components for hybrid applications that only part of their components are migrated to the cloud. Both algorithms employ the application structure information as well as the historical reliability information for component ranking. Based on the ranking result, optimal fault-tolerant strategy will be selected automatically for the most significant components with respect to their predefined constraints. The experimental results show that by refactoring a small number of error-prone components and tolerating faults of the most significant components, the reliability of the application can be greatly improved."
pub.1118264746,Decentralized Overlay for Federation of Enterprise Clouds,"This chapter describes Aneka-Federation, a decentralized and distributed
system that combines enterprise Clouds, overlay networking, and structured
peer-to-peer techniques to create scalable wide-area networking of compute
nodes for high-throughput computing. The Aneka-Federation integrates numerous
small scale Aneka Enterprise Cloud services and nodes that are distributed over
multiple control and enterprise domains as parts of a single coordinated
resource leasing abstraction. The system is designed with the aim of making
distributed enterprise Cloud resource integration and application programming
flexible, efficient, and scalable. The system is engineered such that it:
enables seamless integration of existing Aneka Enterprise Clouds as part of
single wide-area resource leasing federation; self-organizes the system
components based on a structured peer-to-peer routing methodology; and presents
end-users with a distributed application composition environment that can
support variety of programming and execution models. This chapter describes the
design and implementation of a novel, extensible and decentralized peer-to-peer
technique that helps to discover, connect and provision the services of Aneka
Enterprise Clouds among the users who can use different programming models to
compose their applications. Evaluations of the system with applications that
are programmed using the Task and Thread execution models on top of an overlay
of Aneka Enterprise Clouds have been described here."
pub.1143000161,ЕКОНОМІЧНИЙ АСПЕКТ ВИКОРИСТАННЯ ХМАРНИХ ТЕХНОЛОГІЙ У ДІЯЛЬНОСТІ ОРГАНІВ ПУБЛІЧНОЇ ВЛАДИ ТА БІЗНЕС-СТРУКТУР,"The article analyzes the development trends of the cloud services market in Ukraine and the world. It is established that the national market of cloud services is still in the stage of active growth and its potential is much greater than available. The peculiarities of the use of cloud services in the activities of entities from different sectors of the economy (financial institutions, pharmacies, insurance companies, travel companies, car companies, retailers, oil and gas companies, renewable energy companies) are considered. The main advantages for business structures from the transition to cloud solutions and the risks to which they are exposed are outlined. The characteristics of economic effects that will be obtained as a result of the introduction of cloud technologies in the public sector are given. The focus is on highlighting a number of issues related to the modification of the entire system of its work (change of business models, operations, processes that will require reorganization and retraining of staff), rising costs in the initial stages of cloud solutions, when there were expectations to reduce negative public opinion (for example, the widespread perception of a lower level of security in the cloud compared to local networks). It is substantiated that the introduction of cloud technologies should be regulated, safe, cost-effective and take into account the individual needs of the subject of cloud transformation. The stages of the process of transferring the IT infrastructure to the cloud are described (development of a migration strategy to the cloud; inventory and audit of the existing IT infrastructure; drawing up a migration plan; drawing up a migration roadmap; migration; optimization). Various business scenarios for the use of cloud services to government agencies and businesses have been proposed (Scenario 1: backup storage; Scenario 2: backup site; Scenario 3: peak load site; Scenario 4: project deployment environment; Scenario 5: cloud migration of all infrastructure and the abandonment of local «iron»), depending on their concepts and capabilities to meet the needs of cloud solutions."
pub.1181318621,The MANAGEMENT OF SCALABILITY IN CLOUD-BASED APPLICATIONS,"The following is an abstract of the article. The article presents an analysis of the challenges associated with monitoring and managing the scalability of a cloud application. To this end, a module for monitoring and managing the scalability of a cloud application has been developed as part of this study. The development process included the introduction of automatic scaling, and monitoring using Prometheus and Grafana, which allows for a high level of availability and resource efficiency. The study comprised a series of phases, including requirements analysis, system design, development, testing, and evaluation. Consequently, the system's performance, stability, and capacity to scale in response to fluctuating workloads were enhanced. The module exhibits a high degree of adaptability to changes in system requirements and load, which is a crucial attribute for the dynamic development of business applications. This solution assists in optimizing the allocation of resources and reducing infrastructure costs. The project has been found to fully meet the set goals and objectives, as well as the requirements for effective resource management of the Amazon Web Services cloud platform using Terraform, Prometheus, and Grafana. The practical value of the developed module is evidenced by a significant improvement in resource efficiency, service stability and cost optimisation. The module design has been subjected to rigorous testing and has been successfully implemented in a test environment, thereby demonstrating the sustainability and efficiency of the developed solution. The experience gained in the implementation and operation of this solution may prove useful for further expansion and optimization of cloud solutions in other projects and companies specializing in the provision of cloud solutions. The findings of this study were validated in a test environment at an IT company with a specialization in cloud technologies. The objective was to ascertain the functionality and efficiency of the developed module in a real-world context of cloud infrastructure operation. The testing process entailed the configuration of the module on pre-existing cloud infrastructure systems, its integration with Prometheus and Grafana for monitoring purposes, and the execution of a series of stress tests designed to assess the module's scalability. As a result of this testing, a number of critical points were identified that required further optimization. The results of the study and the issues identified during the project testing have enabled the identification of several areas for further improvement and development of the system. First and foremost, the optimization of automatic scaling algorithms represents a crucial avenue for improvement. The development of these algorithms should be oriented towards utilizing historical monitoring data to anticipate potential shifts in system load. Another pivotal area for enhancement is the precision of monitoring systems. The inte"
pub.1172534509,A Queuing Model for Software-Defined Networking in Cloud and Multi-Controller Environments,"Enterprises are increasingly adopting Software-Defined Networking (SDN) for advanced infrastructure management, cost reduction, and adaptation to technological challenges. Cloud-based SDN controllers play a pivotal role in ensuring flexibility, security, and scalability. The transition from a single-controller to a multi-controller paradigm in cloud networks not only improves security and availability but also reinforces the overall resilience of SDN. Our scientific study aims to propose a novel approach for deploying SDN networks and to introduce a queueing model for this proposed concept. The work focuses on modeling SDN networks using Markovian stochastic processes, taking into consideration the availability criterion through the multiplicity of SDN controllers at the control layer, specifically utilizing queuing theory. The proposed model brings improvement and optimization compared to existing ones. The evaluation of Quality of Service (QoS), based on response time measurements, has demonstrated the efficiency of the model. Additionally, observing the model through the conducted experiments has led us to conclude that scalability is ensured concerning the multiplicity of controllers. Our model has successfully addressed the multi-criteria constraint and ensured an optimal combination that incorporates cloud concepts, QoS, and multi-controllers."
pub.1139744055,A Vision to Software-Centric Cloud Native Network Functions: Achievements and Challenges,"Network slicing qualitatively transforms network infrastructures such that they have maximum flexibility in the context of ever-changing service requirements. While the agility of cloud native network functions (CNFs) demonstrates significant promise, virtualization and softwarization severely degrade the performance of such network functions. Considerable efforts were expended to improve the performance of virtualized systems, and at this stage 10 Gbps throughput is a real target even for container/VM-based applications. Nonetheless, the current performance of CNFs with state-of-the-art enhancements does not meet the performance requirements of next-generation 6G networks that aim for terabit-class throughput. The present pace of performance enhancements in hardware indicates that straightforward optimization of existing system components has limited possibility of filling the performance gap. As it would be reasonable to expect a single silver-bullet technology to dramatically enhance the ability of CNFs, an organic integration of various data-plane technologies with a comprehensive vision is a potential approach. In this paper, we show a future vision of system architecture for terabit-class CNFs based on effective harmonization of the technologies within the wide-range of network systems consisting of commodity hardware devices. We focus not only on the performance aspect of CNFs but also other pragmatic aspects such as interoperability with the current environment (not clean slate). We also highlight the remaining missing-link technologies revealed by the goal-oriented approach."
pub.1092910538,Virtualize Manufacturing Capabilities in the Cloud: Requirements and Architecture,"In recent years, Cloud Manufacturing concept has been proposed by taking advantage of Cloud Computing to improve the performance of manufacturing industry. Cloud Manufacturing attempts can be summarized as two sectors, i.e. manufacturing version of Computing Cloud, and a distributed environment that is networked around Manufacturing Cloud. In this paper, manufacturing resource, ability and relevant essentials are discussed in the service-oriented perspective. The functional requirements of a Cloud Manufacturing environment are discussed, along with an interoperable manufacturing system framework. Cloud resource integration models are developed that are compliant with existing international standards. It is possible to achieve a collaborative, intelligent, and distributed environment via Cloud Manufacturing technologies.Copyright © 2013 by ASME"
pub.1009540140,Business Intelligence Security,"Abstract
                  Excess information characteristic to the current environment leads to the need for a change of the organizations’ perspective and strategy not only through the raw data processing, but also in terms of existing applications generating new information. The overwhelming evolution of digital technologies and web changes led to the adoption of new and adapted internal policies and the emergence of regulations at level of governments or different social organisms. Information security risks arising from the current dynamics demand fast solutions linked to hardware, software and also to education of human resources. Business Intelligence (BI) solutions have their specific evolution in order to bring their contribution to ensure the protection of data through specific components (Big Data, cloud, analytics). The current trend of development of BI applications on mobile devices brings with it a number of shortcomings related to information security and require additional protective measure regarding flows, specific processing and data storage."
pub.1140184946,Issues and Challenges in Autonomic Computing and Resource Management,"Autonomic computing is an operating environment that is on demand and responds to problems, threats, and failures automatically. It provides a computing environment that can manage itself and dynamically adapt to change. In a self-managing computing environment, system components such as hardware (desktop computers storage units, and servers), software (business applications, middleware, and operating system) include embedded control loop functionality. These control loops consist of the same fundamental parts. Their functions can be divided into four broad embedded control loop categories. These categories are the attributes of the autonomic computing systems. The self-configuring attribute enables autonomic systems to dynamically adapt to changing environments. This is achieved, by using policies provided by the administrator. These changes include the deployment of new components, the removal of existing ones, or any other dramatic changes in the system. Dynamic adaptation helps ensure continuous strength and productivity of the IT infrastructure, resulting in business growth and flexibility. The self-healing attribute enables autonomic systems to discover, diagnose, and react to disruptions. Self-healing components have the ability to detect any system malfunctions and can initiate policy-based corrective action without disrupting the computing environment. Corrective action includes a software or resource altering its own state or effecting changes in other components in the environment. The self-optimizing attribute enables autonomic systems to monitor and tune resources accordingly and automatically. Self-optimizing components are designed to be able to tune themselves to meet end user or business needs. The actions include reallocating resources in response to dynamically changing workloads to improve overall utilization, or ensuring a particular business transaction can be completed in a timely fashion. Self-optimization helps provide a high standard of service for both the system’s end users and a business’s customers. The self-protecting attribute enables autonomic systems to anticipate threats if any. In such a case, the self-protecting attribute can detect, identify, and protect against threats from anywhere. Self-protecting components are designed to be able to detect hostile behaviors such as unauthorized access, virus infection and proliferation, and denial-of-service attacks. The self-protecting attribute then takes corrective actions to make the attack less vulnerable."
pub.1106287668,Cloud-based ubiquitous object sharing platform for heterogeneous logistics system integration," The intelligence of infrastructure gradually becomes the straw for logistics enterprises to make data-based or date-driven optimization. The integration of heterogeneous logistics systems with existing enterprise information systems is one of the most critical steps to achieve the intelligent infrastructure. Unfortunately, the integration is always a time-consuming process with heavy investment, which suppresses the longings of enterprises, especially for small and medium enterprises (SMEs). Aiming at simplifying the system integration, this paper proposed a cloud-based ubiquitous object sharing platform (CUOSP) to share the integration across SMEs based on the concept of sharing economy. CUOSP acts as a middleware system to make heterogeneous logistics systems universal plug-and-play (UPnP) for enterprise information systems. A kernel-based agent (KBA) is designed as the sharing entity of physical systems. It maintains the features of physical systems and is scalable for different application scenarios. A series of cloud gateway services are emerged not only to provide the basic running and sharing environment, but also to remedy KBA’s weaknesses in computing capacity. A prototype system is developed and implemented based on the framework of CUOSP and a laboratory case according to the consolidation scenario in E-commerce logistics is demonstrated. Comparison experiments are also conducted to explore the real-time and multitasking capacity of KBAs with different kernel characteristics and different computing resources."
pub.1174434342,cDEM: Extract Core Data Elements for Migration Accelerating from Legacy Platform to Modern Cloud Environment,"Data migration from legacy systems to modern environments, such as migrating DB2 from z/OS to the cloud, often requires significant effort and time. Customers often encounter confusion when determining the appropriate migration route, use case, and efficient movement of source data to the target data repository. This paper explores various migration patterns and provides recommendations on target technology and platform selection. We propose a core Data Elements Migration (cDEM) strategy, focusing on schema, objects, and data volume, rather than a complete data warehouse. By enabling users to quickly assess workload, extract core data elements, map components from the source data pool to the target repository, and evaluate data volume and migration complexity, this approach reduces workload and accelerates the overall process. The model presented in this paper illustrates the migration of end-to-end data objects from legacy DB2 on z/OS to the public cloud, specifically Azure SQL Server as a demonstration case, while remaining applicable to other cloud providers and popular data repositories like MySQL."
pub.1095367216,Comparative Analysis of Access Control Systems on Cloud,"Cloud computing, a relatively new concept and has gained an immense attention of research community in the past few years. R&D organizations and industry are investing a lot in cloud based research and applications. Similarly on the consumers' side organizations are moving their business on cloud to provide flexibility and conceive ever increasing computational power requirements. In spite of significant advantages, and its demand, different stakeholders are still reluctant to migrate to cloud. A major hindrance is the absence of reliable and comprehensive access control mechanism for cloud resources. We have analyzed existing cloud based access control systems and evaluated those using NIST defined access control systems evaluation criteria. Based on our analysis we have proposed future research direction in the domain of access control systems for cloud based environments, which will eventually pave the way towards cloud adoption."
pub.1168203089,Dynamic Load Balancing for Energy-Delay Tradeoff in a Cloud-RSU-Vehicle Architecture,"Advanced autonomous driving services at Level 4 and above eliminate the need for constant driver supervision, enabling vehicles to respond autonomously to various driving scenarios. The integration of components under edge environment such as the On Board Unit (OBU), Road Side Units (RSUs), and cloud infrastructure, facilitated by V2X communication, enhances perception, decision-making, and control capabilities. However, existing standards lack guidance on efficient computing resource management in autonomous driving systems. To address this, we propose a dynamic computing load balancing algorithm for the cloud-RSU-vehicle architecture. This algorithm optimizes resource allocation and utilization, considering network conditions, computational capabilities, and processing queues using Lyapunov optimization techniques. The stability of the algorithm and trade-off between processing delay and energy consumption is inspected through simulation."
pub.1181201253,A deep reinforcement learning approach towards distributed Function as a Service (FaaS) based edge application orchestration in cloud-edge continuum,"Serverless computing has emerged as a new cloud computing model which in contrast to IoT offers unlimited and scalable access to resources. This paradigm improves resource utilization, cost, scalability and resource management specifically in terms of irregular incoming traffic. While cloud computing has been known as a reliable computing and storage solution to host IoT applications, it is not suitable for bandwidth limited, real time and secure applications. Therefore, shifting the resources of the cloud-edge continuum towards the edge can mitigate these limitations. In serverless architecture, applications implemented as Function as a Service (FaaS), include a set of chained event-driven microservices which have to be assigned to available instances. IoT microservices orchestration is still a challenging issue in serverless computing architecture due to IoT dynamic, heterogeneous and large-scale environment with limited resources. The integration of FaaS and distributed Deep Reinforcement Learning (DRL) can transform serverless computing by improving microservice execution effectiveness and optimizing real-time application orchestration. This combination improves scalability and adaptability across the edge-cloud continuum. In this paper, we present a novel Deep Reinforcement Learning (DRL) based microservice orchestration approach for the serverless edge-cloud continuum to minimize resource utilization and delay. This approach, unlike existing methods, is distributed and requires a minimum subset of realistic data in each interval to find optimal compositions in the proposed edge serverless architecture and is thus suitable for IoT environment. Experiments conducted using a number of real-world scenarios demonstrate improvement of the number of successfully composed applications by 18%, respectively, compared to state-of-the art methods including Load Balance, Shortest Path algorithms."
pub.1125398577,A Survey on Machine Learning Based Fault Tolerant Mechanisms in Cloud Towards Uncertainty Analysis,"Cloud computing has the tendency to provide on-demand resources. Recently, there has been a large-scale migration of enterprise applications to the cloud. Any unexpected events that occur in cloud due to its dynamic nature is termed as uncertainty. The most cause of uncertainty can be the unexpected fault that arises in cloud environment. Hence the early detection and recovery of fault can abruptly reduce the uncertainty by enhancing the Quality of Service in cloud applications. This paper discusses the types of faults and failures present in cloud environment and it gives an overview on the existing fault handling mechanisms."
pub.1147295559,From the Cloud to the Edge Towards a Distributed and Light Weight Secure Big Data Pipelines for IoT Applications,"Part of the broader development of Internet-of-Things (IoT) architecture for intelligent environments is the developing of sophisticated Edge communications that support the modern requirements of IoT-to-Cloud connectivity. Big Data (BD) and Cloud computing represent a practical and cost-effective solution for supporting IoT operations and advanced analytics. Such a vision involves developing data pipelines, facilitating BD flow from the network’s edge (e.g., sensors, actors, etc.) to the cloud data warehouse. However, security is always a concern among practitioners and developers alike. There are vital factors relating to performance impacts and security vulnerabilities that may emerge during the increased deployment of such a system. Highly secure integration of BD pipelines is a cornerstone in our ability to exploit modern IoT-to-Cloud applications. Given the high impact of BD pipelines security measurements on the system performance, it was not a subject of intensive analysis in the literature. In this study, we analyze the building blocks that support data pipelines as a commodity service for IoT-to-Cloud applications to address the previous research gap. A structured multi-layer security pattern method supporting Edge/Cloud architectures is presented. Data confidentiality was investigated in the complete data pipeline life cycle, i.e., allocation, transmission, and storage. Our study carried the examination of access control, wire encryption, and at-rest data encryption techniques impact on the overall performance. The analysis guides potential large-scale data analytics to model their infrastructure in a secure context using an integrated scheme, technologies, and frameworks. It also highlights a timely demand for lightweight security51approaches that supports the widespread of BD pipelines. Our findings point out the critical need for future research in Edge Intelligence and Artificial Intelligence IoT (AIoT) for sustainable Edge integration in the Cloud. Finally, this study is bridging a knowledge gap between the existing BD pipeline security approaches and problems related to security impact on large-scale edge data processing performance, emphasizing the necessity of lightweight security techniques toward achieving this vision. In this chapter, the realization of secure data pipelines and the Edge-to-Cloud data transmission security challenges are investigated. A secure data pipeline for modern IoT applications is introduced. Security services in the internal cloud communication must ensure the IoT application security against any threat. It is valid to state that data-driven improvement using SDN and blockchain technologies is a starting point for secure IoT applications in the network’s edge. The proposed method uses mechanisms expressed in the IoT-to-Cloud data pipeline to maintain privacy among the edge resource. Any comprehensive discussion on the security impact on performance has to concede all the aspects that may affect the s"
pub.1095289813,Chord Based Identity Management for e-Healthcare Cloud Applications,"Increase in healthcare awareness has raised the number of subscriptions for e-Healthcare applications. Consequently e-Healthcare application providers are adopting Cloud computing to handle immense processing load and to reduce service delivery cost. Identity management has been an issue which hinders in adoption of e-Healthcare applications, due to sensitivity of the data involved in it. Existing Cloud based e-Healthcare applications provide access to their services though Single-Sign-On (SSO) protocols. Traditional SSO uses asymmetric encryption, thus increasing the execution load on Cloud gateway and on Identity Provider as well. In this paper we propose a methodology of SSO for Cloud applications by utilizing Peer-to-Peer concepts to distribute processing load among computing nodes within Cloud. The proposed scheme, called Chord for Cloud (C4C), decreases the number of authentication request send to Identity Provider and disseminates the authentication process within the federated environment of Cloud, through Chord algorithm. The effectiveness of the proposed technique has been shown through argumentation scenarios."
pub.1181464568,A Novel Device Based Edge-Cloud Architecture for Vehicular Edge Computing,"The rapid advancement of vehicular technology and the proliferation of connected vehicles have given rise to the demand for efficient and responsive computing solutions within the vehicular environment. Vehicular Edge Computing (VEC) emerges as a promising paradigm to meet these demands by leveraging the computational resources at the network edge. This paper presents an in-depth exploration of Vehicular Edge Computing Architecture (VECA), a novel framework designed to enhance the capabilities of connected vehicles through edge computing. VECA integrates edge computing nodes, vehicle-to-everything (V2X) communication technologies, and intelligent algorithms to create a dynamic and distributed computing environment within the vehicular network. This architecture addresses critical challenges related to latency, bandwidth, and scalability, enabling a wide range of applications, including real-time navigation, autonomous driving, traffic management, and infotainment services. Key components of VECA include edge servers strategically placed at roadside infrastructure and within vehicles, a robust communication infrastructure that supports low-latency data exchange, and machine learning algorithms for predictive analytics and decision-making. The architecture fosters efficient resource allocation, load balancing, and secure data management, ensuring optimal utilization of computational resources while preserving data privacy. This paper provides a comprehensive overview of VECA’s architecture, highlighting its technical specifications, benefits, and potential use cases. This research paper also discusses the integration of the novel Device Based Edge-Cloud Architecture into existing vehicular networks, along with challenges and future research directions. Through the adoption of this new architecture, connected vehicles can harness the power of edge computing to enhance safety, efficiency, and user experience, ushering in a new era of intelligent and responsive vehicular systems."
pub.1153912304,Research on anti-attack performance of a private cloud safety computer based on the Markov-Percopy dynamic heterogeneous redundancy structure,"Abstract With the increasing computing demand of train operation control systems, the application of cloud computing technology on safety computer platforms of train control system has become a research hotspot in recent years. How to improve the safety and availability of private cloud safety computers is the key problem when applying cloud computing to train operation control systems. Because the cloud computing platform is in an open network environment, it can face many security loopholes and malicious network attacks. Therefore, it is necessary to change the existing safety computer platform structure to improve the attack resistance of the private cloud safety computer platform, thereby enhancing its safety and reliability. Firstly, a private cloud safety computer platform architecture based on dynamic heterogeneous redundant (DHR) structure is proposed, and a dynamic migration mechanism for heterogeneous executives is designed. Then, a generalized stochastic Petri net (GSPN) model of a private cloud safety computer platform based on DHR is established, and its steady-state probability is solved by using its isomorphism with the continuous-time Markov model (CTMC) to analyse the impact of different system structures and executive migration mechanisms on the system's anti-attack performance. Finally, through experimental verification, the system structure proposed in this paper can improve the anti-attack capability of the private cloud safety computer platform, thereby improving its safety and reliability."
pub.1142210347,CFHider: Protecting Control Flow Confidentiality With Intel SGX,"Program control flow reflects the algorithm of that program and may reveal implementation vulnerabilities. Thus its confidentiality needs to be protected, especially in a cloud setting. However, most existing control flow obfuscation methods are software-based, which cannot offer high confidentiality while maintaining low performance overhead. In this paper, we propose CFHider, a hardware-assisted solution. By performing program transformation and leveraging Trusted Execution Environments (Intel SGX), CFHider moves branch statement conditions to an opaque and trusted memory space during the program execution. We proved that by generating Obfuscation Invariants, CFHider is able to provide provable control flow confidentiality protection. Based on the design of CFHider, we also developed a prototype system for Java applications. Our security analysis and experimental results indicate that CFHider is effective in protecting control flow confidentiality and incurs a much reduced performance overhead than existing software-based solutions (by a factor of 18.1)."
pub.1051742475,Virtual Infrastructures as a service enabling converged optical networks and Data Centres,"Cloud computing service emerged as an essential component of the Enterprise IT infrastructure. Migration towards a full range and large-scale convergence of Cloud and network services has become the current trend for addressing requirements of the Cloud environment. Our approach takes the infrastructure as a service paradigm to build converged virtual infrastructures, which allow offering tailored performance and enable multi-tenancy over a common physical infrastructure. Thanks to virtualization, new exploitation activities of the physical infrastructures may arise for both transport network and Data Centres services. This approach makes network and Data Centres׳ resources dedicated to Cloud Computing to converge on the same flexible and scalable level. The work presented here is based on the automation of the virtual infrastructure provisioning service. On top of the virtual infrastructures, a coordinated operation and control of the different resources is performed with the objective of automatically tailoring connectivity services to the Cloud service dynamics. Furthermore, in order to support elasticity of the Cloud services through the optical network, dynamic re-planning features have been provided to the virtual infrastructure service, which allows scaling up or down existing virtual infrastructures to optimize resource utilization and dynamically adapt to users׳ demands. Thus, the dynamic re-planning of the service becomes key component for the coordination of Cloud and optical network resource in an optimal way in terms of resource utilization. The presented work is complemented with a use case of the virtual infrastructure service being adopted in a distributed Enterprise Information System, that scales up and down as a function of the application requests."
pub.1140637255,"Autonomic cloud computing based management and security solutions: State‐of‐the‐art, challenges, and opportunities","Abstract The advancements and rapid adoption of service oriented architecture, utility computing, virtualization, etc., have emerged in a new computing paradigm called cloud computing, in the last few years. Cloud computing enables on‐demand delivery of data, software, and hardware in the form of services over the Internet. But the growing scale of cloud‐users and type‐of‐services urge for an autonomous environment to automate the highly complex cloud environment in a suitable way to improve the overall management of the cloud systems. To counter these automation related issues, autonomic computing (AC) emerges as a potential solution to optimize the performance of cloud computing. The merge of these two prominent technologies gives rise to a new technological domain called autonomic cloud computing (ACC). In this survey article, the existing AC assisted solutions for cloud computing is broadly categorized into two groups, namely features based and parameters based. Features based solutions are further classified into management (of services, workload, and resources) and security aware solutions. Similarly, parameters based solutions are grouped into quality‐of‐service aware and performance management solutions. Finally, the comparative analysis of state‐of‐the‐art ACC approaches with respect to the approach referred, performance metrics, outcome, and the derived inference is also presented to have an in‐depth understanding of the existing literature. Overall, the work excites the research community to design the effective autonomous solutions for ACC resources and services. Additionally, several research gaps and challenges are identified and discussed in this article based on the study with a focus on the future research directions."
pub.1174989864,Optimizing Cloud Security with an Intelligent Framework: Combining Machine Learning and Probabilistic Techniques,"The swift evolution and extensive adoption of cloud computing have fundamentally revolutionized data management, storage, and processing within organizations. Cloud environments are celebrated for their unmatched scalability, flexibility, and cost-effectiveness, solidifying their role as crucial elements of modern information technology infrastructure. However, these benefits come with significant security challenges. The growing complexity and data richness of cloud environments make them prime targets for cyber threats and malicious activities. Traditional security measures, though essential, often fall short in addressing the dynamic and sophisticated nature of contemporary cyber threats. The need for advanced, adaptive, and intelligent security frameworks has never been more critical. This study proposes an intelligent security framework tailored for cloud environments, leveraging machine learning and probabilistic techniques to enhance security and mitigate associated risks.Machine learning algorithms have proven highly effective in detecting anomalies, identifying patterns, and predicting potential security breaches. By continuously learning from data, these algorithms can adapt to evolving threats, providing a proactive defense mechanism. Simultaneously, probabilistic techniques offer a robust mathematical approach for managing uncertainty and making informed decisions under uncertain conditions. The integration of these methods aims to deliver a comprehensive security solution that is both effective and efficient. The proposed method achieves an accuracy of 91.8%, with a mean absolute error (MAE) of 0.403 and a root mean square error (RMSE) of 0.203, demonstrating its effectiveness in enhancing security measures for cloud environments. The primary goal of this research is to develop and evaluate this intelligent security framework, assessing its capability to detect and respond to a wide range of security threats, including data breaches, unauthorized access, and insider threats. Detailed testing and analysis validate the framework’s potential in improving the security posture of cloud-based systems. The following sections review existing literature on cloud security, machine learning applications in cybersecurity, and probabilistic techniques. The design and implementation of the proposed framework are then described, followed by a comprehensive evaluation of its performance. Finally, the implications of the findings are discussed, and future research directions are proposed."
pub.1093389928,A Multi-Agent Based Load Balancing Framework in Cloud Environment,"Cloud computing is a commercial application of distributed system. With the rapid development of cloud computing, resource management problem has been widely studied and discussed. Most of the existing researches concern resource allocation by launching virtual machines, however, instant monitoring of the hosts, load balancing of the cloud platform is much more important for cloud providers. In this paper, a multi-Agent based distributed load balancing framework is proposed for cloud platform, it uses workload prediction technology and threshold buffering strategies to reduce the migration of VMs. Experiments show that our method can significantly reduce the workload data transmission and solve the frequent migration problem, which can effectively achieve load balancing, promote the utilization of the entire data center resources."
pub.1106012923,"Big Data Integration in Cloud Environments: Requirements, Solutions and Challenges","This chapter focuses on the existing solutions of the state of the art supporting Big Data integration in cloud environments. Optimization is the ‘holy grail’ of database management and, in the context of Big Data integration, it is clearly a major challenge. Choosing one or multiple data stores based on data requirements is a very important step before integrating heterogeneous data stores and deploying and running applications in a Cloud environment. Ruiz‐Alvarez proposes an automatic approach to selecting a cloud storage service according to the application requirements and the storage services capabilities. Object NoSQL Datastore Mapper (ONDM) is a framework aiming to facilitate persistent object storage and retrieval in NoSQL data stores. The chapter presents some substantial work proposing different unified data models to manage heterogeneous data integration. It analyzes how global queries are processed."
pub.1111127410,"Industry 4.0, How to Integrate Legacy Devices: A Cloud IoT Approach","The Industry 4.0 approach purpose the massive transformation of the traditional plants into smart factories, however the current industrial shop floor, is composed of a large number of dumb devices (legacy devices). This work describes an approach for integrating legacy devices with a cloud-based IoT platform. This proposal uses the following steps: Virtualize the real equipment, connect the virtual equipment to the cloud, make all the necessary tests with the virtual equipment, and after testing it connects the real equipment with the cloud. We opted for the initial use of a virtual equipment (virtual Modular Production System-CIROS®), as this creates a totally free and secure development and a testing environment without having to stop the real equipment in the shop floor for testing. In this approach were used the same tools that the companies use, such as OPC UA, UaGateway, CODESYS®, and the integration was carried out with SAP IoT Cloud Platform."
pub.1095301850,Business Process Centric Platform-as-a-Service Model and Technologies for Cloud Enabled Industry Solutions,"With the popularity of cloud computing, Platform-as-a Service (PaaS) becomes one of the core technical enablers by enterprise to change the services to both customers and internal organizations. An application in an enterprise needs to take into account various specific requirements for hosting in private and hybrid cloud, with unique requirements on rapid development, simplicity for deployment and management, integration with existing solution and compliance to industry standards, etc. In this paper, a novel business process centric PaaS model is introduced, which is targeted at supporting above requirements for cloud enabled industry solutions in an enterprise. Firstly, the emerging requirements of PaaS for cloud enabled industry solutions and the general features to meet such requirements are discussed. Then, the architecture and patterns for integrating with existing solutions are introduced. And the technologies to implement such PaaS model are presented including codeless developer workspace and automatic application generator. As well, to enable this PaaS model for “programmable SaaS”, BPM multi-tenancy is introduced. Based on the model and technologies, we designed and implemented cloud enabled industry solutions for Telecommunication, Chemical and Petroleum, Financial and Healthcare industries. This paper demonstrates how these technologies and architectures significantly enhance the capability of PaaS in the context of industry solutions and enterprise environments."
pub.1175606934,KModels: Unlocking AI for Business Applications,"As artificial intelligence (AI) continues to rapidly advance, there is a
growing demand to integrate AI capabilities into existing business
applications. However, a significant gap exists between the rapid progress in
AI and how slowly AI is being embedded into business environments. Deploying
well-performing lab models into production settings, especially in on-premise
environments, often entails specialized expertise and imposes a heavy burden of
model management, creating significant barriers to implementing AI models in
real-world applications.
  KModels leverages proven libraries and platforms (Kubeflow Pipelines, KServe)
to streamline AI adoption by supporting both AI developers and consumers. It
allows model developers to focus solely on model development and share models
as transportable units (Templates), abstracting away complex production
deployment concerns. KModels enables AI consumers to eliminate the need for a
dedicated data scientist, as the templates encapsulate most data science
considerations while providing business-oriented control.
  This paper presents the architecture of KModels and the key decisions that
shape it. We outline KModels' main components as well as its interfaces.
Furthermore, we explain how KModels is highly suited for on-premise deployment
but can also be used in cloud environments.
  The efficacy of KModels is demonstrated through the successful deployment of
three AI models within an existing Work Order Management system. These models
operate in a client's data center and are trained on local data, without data
scientist intervention. One model improved the accuracy of Failure Code
specification for work orders from 46% to 83%, showcasing the substantial
benefit of accessible and localized AI solutions."
pub.1048842707,Generic event‐based monitoring and adaptation methodology for heterogeneous distributed systems,"SUMMARY The Cloud computing paradigm provides the basis for a class of platforms and applications that face novel challenges related to multi‐tenancy, adaptivity, and elasticity. To account for service delivery guarantees in the face of ever increasing levels of heterogeneity, scale, and dynamism, service provisioning in the Cloud has raised the demand for systematic and flexible approaches to monitoring and adaptation of applications. In this paper, we tackle this issue and present a framework for efficient runtime management of Cloud environments and distributed heterogeneous systems in general. A novel domain‐specific language termed MONINA is introduced that allows to define integrated monitoring and adaptation functionality for controlling such systems. We propose a mechanism for optimal deployment of the defined control operators onto available computing resources. Deployment is based on solving a quadratic programming problem, which aims at achieving minimized reaction times, low overhead, and scalable monitoring and adaptation. The monitoring infrastructure is based on a distributed messaging middleware, providing high level of decoupling and allowing new monitoring nodes to join the system dynamically. We provide a detailed formalization of the problem domain, discuss architectural details, highlight the implementation of the developed prototype, and put our work into perspective with existing work in the field. Copyright © 2014 John Wiley & Sons, Ltd."
pub.1182220965,"A Critical Analysis of Foundations, Challenges and Directions for Zero Trust Security in Cloud Environments","This review discusses the theoretical frameworks and application prospects of
Zero Trust Security (ZTS) in cloud computing context. This is because, as
organisations move more of their applications and data to the cloud, the old
borders-based security model that many implemented are inadequate, therefore a
model that has a trust no one, verify everything approach is required. This
paper analyzes the core principles of ZTS, including micro-segmentation, least
privileged access, and continuous monitoring, while critically examining four
major controversies: scalability issues, Economics, Integration issues with
existing systems, and Compliance to legal requirements. In this paper, having
reviewed the existing literature in the field and various implementation cases,
the main barriers to implementing zero trust security were outlined, including
the dimensions of decreased performance in large-scale production and the need
for major upfront investments that can be difficult for small companies to meet
effectively. This research shows that there is no clear correlation between
security effectiveness and operational efficiency: while organisations
experience up to 40% decrease of security incidents after implementation, they
note first negative impacts on performance. This study also shows that to
support ZTS there is a need to address the context as the economics and
operations of ZTS differ in strengths depending on the size of the
organizations and the infrastructures. Some of these are: performance
enhancement and optimizations, economic optimization, architectural blend, and
privacy-preserving technologies. This review enriches the existing literature
on cloud security by presenting both the theoretical framework of ZTS and the
observed issues, and provides suggestions useful for future research and
practice in the construction of the cloud security architecture."
pub.1133343792,Feature Model-Guided Online Reinforcement Learning for Self-Adaptive Services,"A self-adaptive service can maintain its QoS requirements in the presence of dynamic environment changes. To develop a self-adaptive service, service engineers have to create self-adaptation logic encoding when the service should execute which adaptation actions. However, developing self-adaptation logic may be difficult due to design time uncertainty; e.g., anticipating all potential environment changes at design time is in most cases infeasible. Online reinforcement learning addresses design time uncertainty by learning suitable adaptation actions through interactions with the environment at runtime. To learn more about its environment, reinforcement learning has to select actions that were not selected before, which is known as exploration. How exploration happens has an impact on the performance of the learning process. We focus on two problems related to how a service’s adaptation actions are explored: (1) Existing solutions randomly explore adaptation actions and thus may exhibit slow learning if there are many possible adaptation actions to choose from. (2) Existing solutions are unaware of service evolution, and thus may explore new adaptation actions introduced during such evolution rather late. We propose novel exploration strategies that use feature models (from software product line engineering) to guide exploration in the presence of many adaptation actions and in the presence of service evolution. Experimental results for a self-adaptive cloud management service indicate an average speed-up of the learning process of 58.8% in the presence of many adaptation actions, and of 61.3% in the presence of service evolution. The improved learning performance in turn led to an average QoS improvement of 7.8% and 23.7% respectively
."
pub.1092335013,IoT Infrastructure: Fog Computing Surpasses Cloud Computing,"The Internet of Things can be defined as an expansion of Internet network further off conventional gadgets like computers, smart phones, and tablets to an assorted scope of gadgets and ordinary things that use embedded technology to communicate with the outer environment, all by means of the Internet. With the increasing time, data access and computing are proceeding towards more complications and hurdles requiring more efficient and logical data computation infrastructures but the vision of future IoT seems foggy as it is a matter of concern that our current infrastructure may not be able to handle large amount of data efficiently involving growing number of IoT devices. To get diminished from this issue, Fog computing has been introduced which is an expansion of cloud computing working as a decentralized infrastructure in which reserving or computing data, service, and applications are distributed in the most efficient and logical position between source and the cloud. In this paper, the current infrastructure has been depicted and proposed another model of IoT infrastructure to surpass the difficulties of the existing infrastructure, which will be a coordinated effort of Fog computing amalgamation with Machine-to-Machine (M2M) intelligent communication protocol followed by incorporation of Service-Oriented Architecture (SOA) and finally integration of agent-based SOA. This model will have the capacity to exchange data by breaking down dependably and methodically with low latency, less bandwidth, heterogeneity in less measure of time maintaining the Quality of Service (QoS) precisely."
pub.1118839325,An Application-oriented Model for Wireless Sensor Networks integrated with Telecom Infra,"This paper aims to propose a significant way of remote access and real time
monitoring of a particular geographic area by integrating wireless sensor
clouds with existing Telecom infrastructure and applications built around them
through a gateway. This utility is very potent for environment monitoring in
harsh and inaccessible places like mines, nuclear reactors, etc. We demonstrate
a scaled down version of multi-hop network of wireless sensor nodes and its
integration with existing telecom network infrastructure via a gateway. The
kind of results achieved like temperature monitoring etc. gives a glimpse of an
enormous step ahead in mine safety."
pub.1164675632,A Versatile Data Fabric for Advanced IoT-Based Remote Health Monitoring,"This paper presents a data-centric and security-focused data fabric designed
for digital health applications. With the increasing interest in digital health
research, there has been a surge in the volume of Internet of Things (IoT) data
derived from smartphones, wearables, and ambient sensors. Managing this vast
amount of data, encompassing diverse data types and varying time scales, is
crucial. Moreover, compliance with regulatory and contractual obligations is
essential. The proposed data fabric comprises an architecture and a toolkit
that facilitate the integration of heterogeneous data sources, across different
environments, to provide a unified view of the data in dashboards. Furthermore,
the data fabric supports the development of reusable and configurable data
integration components, which can be shared as open-source or inner-source
software. These components are used to generate data pipelines that can be
deployed and scheduled to run either in the cloud or on-premises. Additionally,
we present the implementation of our data fabric in a home-based telemonitoring
research project involving older adults, conducted in collaboration with the
University of California, San Diego (UCSD). The study showcases the streamlined
integration of data collected from various IoT sensors and mobile applications
to create a unified view of older adults' health for further analysis and
research."
pub.1163618313,A Versatile Data Fabric for Advanced IoT-Based Remote Health Monitoring,"This paper presents a data-centric and security-focused data fabric designed for digital health applications. With the increasing interest in digital health research, there has been a surge in the volume of Internet of Things (IoT) data derived from smartphones, wearables, and ambient sensors. Managing this vast amount of data, encompassing diverse data types and varying time scales, is crucial. Moreover, compliance with regulatory and contractual obligations is essential. The proposed data fabric comprises an architecture and a toolkit that facilitate the integration of heterogeneous data sources, across different environments, to provide a unified view of the data in dashboards. Furthermore, the data fabric supports the development of reusable and configurable data integration components, which can be shared as open-source or inner-source software. These components are used to generate data pipelines that can be deployed and scheduled to run either in the cloud or on-premises. Additionally, we present the implementation of our data fabric in a home-based telemonitoring research project involving older adults, conducted in collaboration with the University of California, San Diego (UCSD). The study showcases the streamlined integration of data collected from various IoT sensors and mobile applications to create a unified view of older adults’ health for further analysis and research."
pub.1085044891,On the power consumption modeling for the simulation of Heterogeneous HPC clouds,"During the last years, except from the traditional CPU based hardware servers, hardware accelerators are widely used in various HPC application areas. More specifically, Graphics Processing Units (GPUs), Many Integrated Cores (MICs) and Field-Programmable Gate Arrays (FPGAs) have shown a great potential in HPC and have been widely mobilized in supercomputing. With the adoption of HPC from cloud environments, the realization of HPC-Clouds is evolving since many vendors provide HPC capabilities on their clouds. With the increase of the interest on clouds, there has been an analogous increase in cloud simulation frameworks. Cloud simulation frameworks offer a controllable environment for experimentation with various workloads and scenarios, while they provide several metrics such as server utilization and power consumption. For providing these metrics, cloud simulators propose mathematical models that estimate the behavior of the underlying hardware infrastructure. This paper focuses on the power consumption modeling of the main compute elements of heterogeneous HPC servers, i.e. CPU servers and pairs of CPU-accelerators. The modeling approaches of existing cloud simulators are examined and extended, while new models are proposed for estimating the power consumption of accelerators."
pub.1002342126,Decentralized Overlay for Federation of Enterprise Clouds,"This chapter describes Aneka-Federation, a decentralized and distributed system that combines enterprise Clouds, overlay networking, and structured peer-to-peer techniques to create scalable wide-area networking of compute nodes for high-throughput computing. The Aneka-Federation integrates numerous small scale Aneka Enterprise Cloud services and nodes that are distributed over multiple control and enterprise domains as parts of a single coordinated resource leasing abstraction. The system is designed with the aim of making distributed enterprise Cloud resource integration and application programming flexible, efficient, and scalable. The system is engineered such that it: enables seamless integration of existing Aneka Enterprise Clouds as part of single wide-area resource leasing federation; self-organizes the system components based on a structured peer-to-peer routing methodology; and presents end-users with a distributed application composition environment that can support variety of programming and execution models. This chapter describes the design and implementation of a novel, extensible and decentralized peer-to-peer technique that helps to discover, connect and provision the services of Aneka Enterprise Clouds among the users who can use different programming models to compose their applications. Evaluations of the system with applications that are programmed using the Task and Thread execution models on top of an overlay of Aneka Enterprise Clouds have been described here."
pub.1135326559,Improving the hybrid cloud performance through disk activity-aware data access,"Cloud computing has been making significant contributions to Internet of Things, Big Data, and many other cutting-edge research areas in recent years. To deal with the cloud bursting, on-premises private clouds often extend their service capacity with off-premises public clouds, which causes the migration of jobs and their corresponding data from private clouds to public clouds. For jobs executed in public clouds, promptly transferring data they need from private clouds to public clouds is essential for their quick completion as the volume of data is often large for cloud applications. The Internet connection between private clouds and public clouds is with limited bandwidth in most cases. Therefore, it will be valuable if the underlying operating system could expedite the course of reading data from hard drives to speed up the process of moving data from private clouds to public clouds. The Apache Hadoop is considered as one of the most widely used cloud platforms in the community of cloud computing. It keeps multiple replicas of data across its cluster nodes to improve data availability. We designed and implemented a new model enabling computing nodes in Hadoop to get data in request from a cluster node with the least amount of disk activity regardless of its location to hasten the course of accessing data. Experimental results show that jobs could reduce their execution time by up to 80.83% in our model. Accordingly, our model could help accelerate the completion of job execution in both private clouds and public clouds in the environment of hybrid clouds."
pub.1134735955,An SDN-Assisted Defense Mechanism for the Shrew DDoS Attack in a Cloud Computing Environment,"The integration of cloud computing with Software Defined Networking (SDN) addresses several challenges of a typical cloud infrastructure such as complex inter-networking, data collection, fast response, etc. Though SDN-based cloud opens new opportunities, the SDN controller may itself become vulnerable to several attacks. The unique features of SDN are used by the attackers to implement the severe Distributed Denial of Service (DDoS) attacks. Several approaches are available in literature to defend against the traditional DDoS flooding attacks in SDN-cloud. To elude the detection systems, attackers try to employ the cultivated attack strategies. Such sophisticated DDoS attack strategies are implemented by generating low-rate attack traffic. The most common type of Low-Rate DDoS (LR-DDoS) attack is the Shrew attack. The existing approaches are not capable to detect, mitigate, and traceback such attacks. Thus, this work discusses a new mechanism which not only detects and mitigates the shrew attack but traces back the location of the attack sources as well. The attack is detected using the information entropy variations, and the attack sources are traced-back using the deterministic packet marking scheme. The experiments are performed in a real SDN-cloud scenario, and the experimental results show that the approach requires 1 packet and 8.27 packets on an average to locate the bots and attackers respectively. The approach detects and traces back the attack sources in between 14.45 ms to 10.02 s and provides 97.6% accuracy."
pub.1158579053,Design and development of novel security approach designed for cloud computing with load balancing,"A cloud computing environment is a computer environment that provides services and stores data as virtualized a data warehouse and information that is accessed remotely using scalable and measurable resources. As more enterprise applications and critical information and data migrate to cloud platforms, cloud computing is growing more popular. On the other side, a major impediment to cloud adoption is a lack of security. As a result, when we store data in cloud platforms, cloud computing security is critical. In this case, load balancing should be seen as the bare minimum of algorithms for balancing the load. Cloud-related infrastructure should be safeguarded.The majority of existing security systems involve traditional encryption, decryption techniques, and proxy signatures. They focused on minimising energy use in the existing load balancing system, and agents were utilised to balance the load. This study proposes a holistic approach to cloud computing load balancing with security. The Quantum Based Security Framework has been created, and the load is balanced using fuzzy logic. The major security policies are efficiently assessed, and service is supplied based on the number of conditions set by the user.The Security Framework established a method for storing data in the cloud by producing check bits instead of keys and allowing users to access their data after verifying the check bits. Only the user can use the services and load balancing if the check bits generated by the user and the cloud service provider are the same. Due to the possibility of data loss due to failure or outage, mirrored copies are made available. We can attain a high level of security by employing this security strategy."
pub.1145336023,Assessing Vulnerabilities and IoT-Enabled Attacks on Smart Lighting Systems,"The rapid evolution of the Internet-of-Things (IoT) introduces innovative services that span across various application domains. As a result, smart automation systems primarily designed for non-critical environments may also be installed in premises of critical sectors, without proper risk assessment. In this paper we focus on IoT-enabled attacks, that utilize components of the smart lighting ecosystem in popular installation domains. In particular, we present a holistic security evaluation on a popular smart lighting device (The specific model is not referred in this paper, since we are currently in the process of a responsible disclosure procedure with the vendor.), that is focused on vulnerabilities and misconfigurations found on hardware, embedded software, cloud services and mobile applications. In addition, we construct a Common Vulnerability Scoring System (CVSS) like vector for each attack scenario, in order to define the required capabilities and potential impact of these attack scenarios and examine their potential exploitability and impact."
pub.1107019288,Service Management of Blockchain Networks,"Business interest in blockchain technology is increasing due the potential for change that it offers. Large scale adoption in enterprise IT environments brings about a need for service management and DevOps processes for blockchain deployments. Nodes in the network may fail or be taken down for maintenance requiring visibility across hybrid, traditional, cloud enabled and cloud native deployments. Service management is necessary to ease deployment woes for those looking to install code within their own data centers on-premises or on dedicated cloud servers and further to handle the life cycle of the deployments that includes security patches, critical updates, upgrades, change management and restarting services. In this paper, we provide a mechanism to optimally perform such operations with no disruption to the consensus and transactions for such emerging blockchain applications."
pub.1092938293,Process and Production Planning in a Cloud Manufacturing Environment,"Cloud manufacturing provides a way for manufacturing companies to rapidly form a flexible production network to respond to the growing demand of highly personalised products. Computer-aided Process Planning (CAPP) is an important element of production planning. However, existing methodologies failed to meet the requirements for CAPP systems in the cloud, which is a distributed, collaborative, and web-based environment. This paper discusses the requirements of CAPP systems in a cloud environment and proposes a feasible system framework for next-generation CAPP systems. The proposed system is built upon Service Oriented Architecture (SOA) architecture, which enables smooth system integration among different manufacturers. A two-stage strategy for generating feasible production plans for a given manufacturing request is discussed in this paper. Moreover, a feasible mechanism to incorporate shared engineering practices and knowledge from each manufacturer is also presented as part of the proposed system.Copyright © 2015 by ASME"
pub.1124869747,Big Data & Disruptive Computing Platforms Braced Internet of Things: Facets & Trends,"The Internet of Things (IoT) has been evolving in tandem with centralized cloud computing to fog computing, edge computing, Semantic computing, etc. Pervasive IoT applications like Healthcare applications generate a huge amount of sensor data and imaging data that needs to be handled rightly for further processing. In traditional IoT ecosystem, Cloud computing ensures solution for efficient management of huge data with its ability to access shared resources and provide common infrastructure in a ubiquitous manner. Majority of the IoT applications are highly sensitive to time and necessities latency bounded execution. There is time delay introduced when the data transmission occurs between the cloud and the application which is unacceptable. This chapter will disclose the various facets and trends in the integration/coupling of evolving computing platforms and disruptive technologies like Fog/Edge Computing, Big Data, blockchain with IoT that overcomes challenges existing in the traditional deployment of IoT environment. Further, there are several issues with IoT and Cloud Computing framework like each individual part of the IoT architecture could act as a point of failure that can interrupt the whole network and secondly, the centralized cloud model is vulnerable to loss of integrity of data. This chapter will review and reveal the trends in bracing computing approaches, managing the huge data using datacenters in Internet of Things ecosystem. The review is advocated with a case study on waste management system applied with the fog/edge computing and cloud computing. Further, various trends in applicability of block chain in the IoT ecosystem is also reviewed. This chapter discusses the fundamental facets of various computing paradigms and approaches that may help to address issues of big data through creating IoT ecosystems."
pub.1126121877,Swarm Intelligence based Virtual Machine Migration Techniques in Cloud Computing,"Cloud computing is a utility where compute resources are accessed on pay-per-use ground. It is enacted through virtualizationwhich is a central technology in cloud computing. Virtualization provides a secure and commendable environment for the functioning of cloud applications. Virtual machine (VM) plays a crucial role in the cloud data center when a host machine is exhausted due to tremendous network traffic, then the load is stabilized by shifting highly loaded VM to the under-utilized host machine. This review article recapitulates energy-efficient VM migration, VM migration techniques, objectives and issues related to it. The existing Swarm intelligence based VMmigration techniques are conferred and compared."
pub.1158111917,Methodology for the generation of 3D city models and integration of HBIM models in GIS: Case studies,"The Architecture, Engineering and Construction (AEC) industry increasingly demands the availability of semantic and interactive digital models with the environment, capable of simulating decision-making during its life cycle and representing the results achieved. This motivates the need to develop models that integrate spatial information (GIS) and construction information (HBIM), favouring the achievement of the Smart City and Digital Twin concepts. GIS & HBIM platform is a useful tool, with potential applications in the world of built heritage; but it still has certain inefficiencies related to interoperability, the semantics of the formats and the geometry of the models. The objective of this contribution is to suggest a procedure for the generation of 3D visualization models of existing cities by integrating HBIM models in GIS environments. For this, three software and two types of data sources (existing plans and point cloud) are used. The methodology is tested in four locations of different dimensions, managing to identify the advantages/disadvantages of each application."
pub.1118270240,FlexCloud: A Flexible and Extendible Simulator for Performance Evaluation of Virtual Machine Allocation,"Cloud Data centers aim to provide reliable, sustainable and scalable services
for all kinds of applications. Resource scheduling is one of keys to cloud
services. To model and evaluate different scheduling policies and algorithms,
we propose FlexCloud, a flexible and scalable simulator that enables users to
simulate the process of initializing cloud data centers, allocating virtual
machine requests and providing performance evaluation for various scheduling
algorithms. FlexCloud can be run on a single computer with JVM to simulate
large scale cloud environments with focus on infrastructure as a service;
adopts agile design patterns to assure the flexibility and extensibility;
models virtual machine migrations which is lack in the existing tools; provides
user-friendly interfaces for customized configurations and replaying. Comparing
to existing simulators, FlexCloud has combining features for supporting public
cloud providers, load-balance and energy-efficiency scheduling. FlexCloud has
advantage in computing time and memory consumption to support large-scale
simulations. The detailed design of FlexCloud is introduced and performance
evaluation is provided."
pub.1145789418,An Automated Framework for Generating Synthetic Point Clouds from as-Built BIM with Semantic Annotation for Scan-to-BIM,"Data scarcity is a major constraint which hinders Scan-to-BIM's generalizability in unseen environments. Manual data collection is not only time-consuming and laborious but especially achieving the 3D point clouds is in general very limited due to indoor environment characteristics. In addition, ground-truth information needs to be attached for the effective utilization of the achieved dataset which also requires considerable time and effort. To resolve these issues, this paper presents an automated framework which integrates the process of generating synthetic point clouds and semantic annotation from as-built BIMs. A procedure is demonstrated using commercially available software systems. The viability of the synthetic point clouds is investigated using a deep learning semantic segmentation algorithm by comparing its performance with real-world point clouds. Our proposed framework can potentially provide an opportunity to replace real-world data collection through the transformation of existing as-built BIMs into synthetic 3D point clouds."
pub.1154109606,Methodology for the generation of 3D city models and integration of HBIM models in GIS: Case studies,"The Architecture, Engineering and Construction (AEC) industry increasingly demands the availability of semantic and interactive digital models with the environment, capable of simulating decision-making during its life cycle and representing the results achieved. This motivates the need to develop models that integrate spatial information (GIS) and construction information (HBIM), favouring the achievement of the Smart City and Digital Twin concepts. GIS & HBIM platform is a useful tool, with potential applications in the world of built heritage; but it still has certain inefficiencies related to interoperability, the semantics of the formats and the geometry of the models. The objective of this contribution is to suggest a procedure for the generation of 3D visualization models of existing cities by integrating HBIM models in GIS environments. For this, three software and two types of data sources (existing plans and point cloud) are used. The methodology is tested in four locations of different dimensions, managing to identify the advantages/disadvantages of each application."
pub.1175048053,AI-Driven Security Protocols for Modern Cloud Engineers,"In the era of digital transformation, cloud computing has become integral to modern enterprises, offering scalable resources and flexibility. However, this rapid adoption has also introduced a new landscape of security challenges, including data breaches, insider threats, and misconfigurations, all of which can compromise sensitive information and disrupt operations. Traditional security measures often fall short in addressing these complex threats, prompting the need for more advanced solutions. This article explores the pivotal role of AI-driven security protocols in fortifying cloud infrastructures against evolving cyber threats. By leveraging AI, cloud engineers can implement real-time threat detection, automate incident responses, and enhance identity and access management (IAM), significantly reducing the risk of unauthorized access and data leakage. AI's capability to analyze vast amounts of data and identify anomalies allows for more proactive security measures, adapting to new threats as they emerge. Additionally, AI can streamline the management of data encryption and privacy, ensuring compliance with regulatory standards. The article also examines implementation strategies, emphasizing the integration of AI with existing security frameworks and the importance of continuous learning and collaboration between AI systems and human experts. As cloud environments grow increasingly complex, AI-driven security protocols represent a critical advancement in safeguarding digital assets. Through case studies and analysis, this research highlights the effectiveness of AI in enhancing cloud security and the future trends that will shape its ongoing development."
pub.1019836139,Semantic-based QoS management in cloud systems: Current status and future challenges,"Cloud Computing and Service Oriented Architectures have seen a dramatic increase of the amount of applications, services, management platforms, data, etc. gaining momentum for the necessity of new complex methods and techniques to deal with the vast heterogeneity of data sources or services. In this sense Quality of Service (QoS) seeks for providing an intelligent environment of self-management components based on domain knowledge in which cloud components can be optimized easing the transition to an advanced governance environment. On the other hand, semantics and ontologies have emerged to afford a common and standard data model that eases the interoperability, integration and monitoring of knowledge-based systems. Taking into account the necessity of an interoperable and intelligent system to manage QoS in cloud-based systems and the emerging application of semantics in different domains, this paper reviews the main approaches for semantic-based QoS management as well as the principal methods, techniques and standards for processing and exploiting diverse data providing advanced real-time monitoring services. A semantic-based framework for QoS management is also outlined taking advantage of semantic technologies and distributed datastream processing techniques. Finally a discussion of existing efforts and challenges is also provided to suggest future directions."
pub.1171683642,Enhancement of Privacy Preservation and Security in Cloud Databases using Blockchain Technology,"Cloud databases provide organizations with scalable and flexible data storage and management capabilities. However, concerns around data privacy and security in cloud environments remain a major barrier to adoption. This paper explores how blockchain technology can be applied to enhance privacy preservation and security for cloud databases. A review of cloud database architectures, privacy and security risks, and existing solutions is provided. The unique capabilities of blockchain to enable tamper-proof, decentralized data sharing are discussed. A proposed blockchain framework for securing cloud databases is presented, including system components and workflow. Central elements include encryption, access control, auditing, and smart contracts enabled by blockchain and cryptographic techniques. Benefits of the blockchain approach such as immutable audit logs, prevention of unauthorized data modification, and cryptographic privacy are highlighted. Implementation considerations including performance, cost, and deployment tradeoffs are analyzed. Overall, blockchain shows strong potential to mitigate cloud database privacy and security concerns through its decentralized trust model, although further research and development is needed to address scalability and efficiency challenges. The paper provides comprehensive analysis of the application of blockchain for enhanced cloud database security to benefit both academic researchers and industry practitioners."
pub.1134367977,Overlay Networks for Edge Management,"Edge computing has emerged as a solution to address existing limitations of cloud computing for bandwidth-heavy and time-sensitive applications, by moving (some) computations from bandwidth saturated Cloud infrastructures closer to client devices, where data is effectively produced and consumed. However, existing materializations of the edge computing paradigm take limited advantage of computational and storage power that exists in the edge and between client devices and the cloud. Most of these leverage static hierarchical topologies (e.g., Fog Computing) to pre-process data before sending it to the Cloud, which limits the advantages that can be extracted from the edge computing paradigm. In the past, peer-to-peer systems have sought to tackle the challenges of increasing scalability and availability for very large systems, with a large number of solutions being proposed namely, distributed overlay networks for resource management. In this paper, we argue that the clever adaptation of peer-to-peer solutions can enable novel applications to fully exploit the potential of the edge. In particular, we study the viability of taking advantage of specialized overlay networks in edge environments to enable the management of a large number of computational resources. Contrary to previous proposals, that assume the environment to be composed of mostly homogeneous devices, our proposal embraces existing heterogeneity and exploits the location of computational resources to devise a (partially) self-organizing overlay network that can be exploited both to provide membership information to applications, but also do efficiently disseminate management information across edge devices. We have conducted an experimental evaluation using container-based emulation in an heterogeneous network composed by 100 devices, with results showing that our protocol is able to maximize the bandwidth usage of the system, allowing more data to flow throughout the network, while retaining high robustness to failures."
pub.1125684847,Towards Optimistic Access Control for Cloud-based Collaborative Editors,"Collaborative editing applications for cloud environment play an important role in many fields and communities since they allow users to communicate and collaborate using their mobile devices. Mobile devices are usually cloned in the cloud to minimize computation cost and energy consumption. Moreover, these mobile and cloud interactions lead to online and offline switching in easy and continuous ways in order to edit shared multimedia documents. However, the main concern of these applications is still maintaining consistent and secure copies of the shared documents with low latency and high local responsiveness. Indeed, appropriate access control models are needed to preserve the features of collaborative editing applications when combining mobile and cloud environments. In this paper, we present a study on existing cloud-based collaborative editors and the current state of the art of access control models used in the collaborative edition context. This study has raised a series of shortcomings that have enabled us to sketch a new access model for deploying securely mobile collaborative editing applications in the cloud."
pub.1093886189,Architecture and Protocol for User-Controlled Access Management in Web 2.0 Applications,"The rapidly developing Web environment provides users with a wide set of rich services as varied and complex as desktop applications. Those services are collectively referred to as “Web 2.0”, with examples such as Google Docs, Flickr, or Wordpress, that allow users to create, manage and share their content online. By switching from desktop applications to their cloud-based Web equivalents users release even more data online. It is the user who creates this data, who disseminates it and who shares it with other users and services. Storing and sharing resources on the Web poses new security challenges. Access control, in particular, is currently poorly addressed in such an environment and is not well suited to the increasing number of resources that are available online. We propose a new approach to access control for the Web. Our approach puts a user in full control of assigning access rights to their resources which may be spread across multiple cloud-based Web applications. Unlike existing authorization systems, it relies on a user's centrally located security requirements for these resources."
pub.1182042458,Adaptive Security Paradigms: The Role of Al in Safeguarding Distributed Data Across Multi-cloud Platforms,"The proliferation of multi-cloud infrastructures in modern data management strategies has introduced complex security challenges that traditional measures struggle to address effectively. This article investigates the potential of AI-powered security frameworks to enhance distributed data protection across diverse cloud environments. By leveraging advanced machine learning algorithms and predictive analytics, these frameworks offer real-time threat detection, adaptive access controls, and intelligent encryption management. The article examines the key components of AI-driven security systems, including automated anomaly detection and behavior analysis, and their integration with existing security protocols. Through a series of case studies and real-world applications, we demonstrate the efficacy of these frameworks in identifying vulnerabilities, initiating proactive security measures, and maintaining compliance with industry regulations. Our findings indicate that AI-powered security frameworks provide a scalable, adaptive, and robust solution for safeguarding distributed data assets in the dynamic landscape of multi-cloud infrastructures. However, the research also acknowledges potential limitations and ethical considerations, paving the way for future advancements in this critical area of cybersecurity."
pub.1174438761,"Model-based tool for the design, configuration and deployment of data-intensive applications in hybrid environments: An Industry 4.0 case study","The fourth industrial revolution advocates the reformulation of industrial processes to achieve the end-to-end (provider-customer) digitalisation of the industrial sector. As is well known, the industrial environment is very complex, where legacy systems must interoperate and integrate with modern devices and sensors. Communication among them requires specific and costly developments, so architectures based on data sharing and services implementation are considered one of the most flexible and appropriate technological solutions to gradually achieve the desired horizontal and vertical integration of the value chain. The design and deployment of data-intensive applications is not straightforward, therefore this paper proposes a model-based tool to characterise the different elements to be configured in an application and to make its deployment easier by generating configuration, orchestration and deployment files and sending them to the corresponding nodes for their execution. In few words, this article highlights the advantages of distributed and data-centric architectures to face the challenge of integration and interoperability in data-intensive complex systems and presents the extension of the RAI4 metamodel proposed in Martínez et al. (2021) that now allows specifying how, containerised or not, and where, on the cloud, fog, edge or on-premise, each service can be hosted according to its functional and non-functional requirements, mainly issues related with real-time, security and cyber physical hardware dependencies. For the sake of comprehension, a pseudo-real use case addressed to pre-process and store pollution data from environmental sensors installed in a smart city is described in detail, including different deployment settings."
pub.1093788430,Analysis of Integrity Vulnerabilities and a Non-repudiation Protocol for Cloud Data Storage Platforms,"Data storage technologies have been recognized as one of the major dimensions of information management along with the network infrastructure and applications. The prosperity of cloud computing requires the migration from server-attached storage to network-based distributed storage. Along with variant advantages, distributed storage also poses new challenges in creating a secure and reliable data storage and access facility. The data security in cloud is one of the challenges to be addressed before the novel pay-as-you-go business model can be accepted and applied widely. Concerns are raised from both insecure/unreliable service providers and potential malicious users. In this article, we analyze the integrity vulnerability existing in the current cloud storage platforms and show the problem of repudiation. A novel nonrepudiation (NR) protocol specifically designed in the context of cloud computing environment is proposed. We have also discussed the robustness of the NR protocol against typical attacks in the network environments."
pub.1136817048,Hybrid SDN Evolution: A Comprehensive Survey of the State-of-the-Art,"Software-Defined Networking (SDN) is an evolutionary networking paradigm
which has been adopted by large network and cloud providers, among which are
Tech Giants. However, embracing a new and futuristic paradigm as an alternative
to well-established and mature legacy networking paradigm requires a lot of
time along with considerable financial resources and technical expertise.
Consequently, many enterprises can not afford it. A compromise solution then is
a hybrid networking environment (a.k.a. Hybrid SDN (hSDN)) in which SDN
functionalities are leveraged while existing traditional network
infrastructures are acknowledged. Recently, hSDN has been seen as a viable
networking solution for a diverse range of businesses and organizations.
Accordingly, the body of literature on hSDN research has improved remarkably.
On this account, we present this paper as a comprehensive state-of-the-art
survey which expands upon hSDN from many different perspectives."
pub.1010033980,A security-improved scheme for virtual TPM based on KVM,"Virtual trusted platform module (vTPM) is an important part in building trusted cloud environment. Aiming at the remediation of lack of effective security assurances of vTPM instances in the existing virtual TPM architecture, this paper presents a security-improved scheme for virtual TPM based on kernel- based virtual machine (KVM). By realizing the TPM2.0 specification in hardware and software, we add protection for vTPM’s secrets using the asymmetric encryption algorithm of TPM. This scheme supports the safety migration of a TPM key during VM-vTPM migration and the security association for different virtual machines (VMs) with vTPM instances. We implement a virtual trusted platform with higher security based on KVM virtual infrastructure. The experiments show that the proposed scheme can enhance the security of virtual trusted platform and has fewer additional performance loss for the VM migration with vTPM."
pub.1136696735,Hybrid SDN evolution: A comprehensive survey of the state-of-the-art,"Software-Defined Networking (SDN) is an evolutionary networking paradigm which has been adopted by large network and cloud providers, among which are Tech Giants. However, embracing a new and futuristic paradigm as an alternative to well-established and mature legacy networking paradigm requires a lot of time along with considerable financial resources and technical expertise. Consequently, many enterprises cannot afford it. A compromise solution then is a hybrid networking environment (a.k.a. Hybrid SDN (hSDN)) in which SDN functionalities are leveraged while existing traditional network infrastructures are acknowledged. Recently, hSDN has been seen as a viable networking solution for a diverse range of businesses and organizations. Accordingly, the body of literature on hSDN research has improved remarkably. On this account, we present this paper as a comprehensive state-of-the-art survey which expands upon hSDN from many different perspectives."
pub.1153390337,Cloud infrastructure architecture and the zero trust model as a cybersecurity strategy,"Since the emergence of the internet, the vertiginous growth in the use of electronic media has grown in the same proportion as cyber crimes, applied in an increasingly sophisticated way. In addition to these two factors that impact the use of the internet, the speed with which the resources and tools inherent in the area of Information and Communication Technology (ICTs) evolve, which enhance the need to continuously develop and improve the means for the protection of sensitive data of people and public and private organizations. In this perspective, Cloud Computing emerges, which allows the storage of data, networks and applications, and other resources through integrated environments through the internet, from collective providers, as opposed to the on-premise system, which is based on the custody and access through local servers, including mainframes, which are still maintained in most large organizations, such as the banking system, for example. Added to Cloud Computing are Zero Trust practices, whose main innovation is the adoption of several layers of access verification. This article was prepared using bibliographical research as a methodology. The question that arises on the subject is: how does Zero Trust provide greater security to network users? The objective is to demonstrate the advantages of security provided by Zero Trust, combined with Cloud Computing. In view of the analyzed literature, it was possible to conclude the existence of two main aspects brought by Zero Trust: internet user access only from 7 layers of verification, and the mitigation of vulnerabilities, including the idea of responsible browsing by different users."
pub.1041109563,Self-configuring Software-defined Overlay Bypass for Seamless Inter- and Intra-cloud Virtual Networking,"Many techniques have been proposed to provide, transparently, the abstraction of a layer-2 virtual network environment within a provider, e.g. by leveraging Software-Defined Networking (SDN). However, cloud providers often constrain layer-2 communication across instances; furthermore, SDN integration and layer-2 messaging between distinct domains distributed across the Internet is not possible, hindering the ability for tenants to deploy their virtual networks across providers. In contrast, overlay networks provide a flexible foundation for inter-cloud virtual private networking (VPN), by tunneling virtual network traffic through private, authenticated end-to-end overlay links. However, overlays inherently incur network virtualization overheads, including header encapsulation and user/kernel boundary crossing. This paper proposes a novel system -- VIAS (VIrtualization Acceleration over SDN) -- that delivers the flexibility of overlays for inter-cloud virtual private networking, while transparently applying SDN techniques (available in existing OpenFlow hardware or software switches) to selectively bypass overlay tunneling and achieve near-native performance for TCP/UDP flows within a provider. Architecturally, VIAS is unique in how it integrates SDN and overlay controllers in a distributed fashion to coordinate the management of virtual network links and flows. The approach is self-organizing, whereby overlay nodes can detect that peer endpoints are in the same network and program bypass flows between OpenFlow switches. While generally applicable, VIAS in particular applies to nested VMs/containers across cloud providers, supporting seamless communication within and across providers. VIAS has been implemented as an extension to an existing virtual network overlay platform (IP-over-P2P, IPOP) by integrating OpenFlow controller functionality with distributed overlay controllers. We evaluate the performance of VIAS in realistic cloud environments using an implementation based on IPOP, the RYU SDN framework, Open vSwitch, and LXC containers across various cloud environment including Amazon, Google compute engine, and CloudLab."
pub.1166457691,Cloudy: A Python-based Simulator for Modern Cloud Environments,"<p>Cloud computing is a pervasive technology, underpinning numerous modern services. However, understanding and optimizing cloud environments can be complex and challenging, demanding sophisticated simulation tools. Existing cloud simulators, predominantly developed in languages like Java, often lack flexibility, struggle with rapid adaptation to emerging technologies, and can be difficult to integrate with contemporary libraries, especially in Artificial Intelligence (AI) and Linear Programming (LP) domains. To address these limitations , we introduce Cloudy, a Python-based simulator designed to model and simulate modern cloud environments. As Python offers simplicity, readability, and extensive library support, Cloudy provides an easily accessible, yet powerful , tool for comprehensive cloud simulations. It leverages Python's seamless integration with AI and LP libraries for advanced experimentation and supports modern cloud peripherals, such as GPUs. Moreover, Cloudy accommodates various cloud service models, making it highly versatile for different cloud-based scenarios. Compared to the established CloudSim simulator, Cloudy demonstrated a significant reduction in memory usage and simulation time by 15.6x and 2.75x, respectively. This paper offers an in-depth exploration of the Cloudy simulator, its features, and potential applications.</p>"
pub.1094008987,Enhancing a Virtual Security Lab with a Private Cloud Framework,"Tele-teaching and hands-on exercises are of major importance in modern cybersecurity training. The Tele-Lab platform provides a virtual training environment using virtualized computers and networks, accessible via the Internet. With the rise of Massive Open Online Courses, it is also desirable to have such lab environments available. To make Tele-Lab's virtual laboratory environment more flexible, scalable and faster, a private cloud framework should become the new basic layer to control and manage virtual machines. This paper explains the integration of OpenNebula into the existing software and implementation of an additional middleware layer that takes advantage of the cloud frameworks functionality."
pub.1123308333,Sensyml: Simulation Environment for large-scale IoT Applications,"IoT systems are becoming an increasingly important component of the civil and industrial infrastructure. With the growth of these IoT ecosystems, their complexity is also growing exponentially. In this paper we explore the problem of testing and evaluating large scale IoT systems at design time. To this end we employ simulated sensors with the physical and geographical characteristics of real sensors. Moreover, we propose Sensyml, a simulation environment that is capable of generating big data from cyber-physical models and real-world data. To the best of our knowledge it is the first approach to use a hybrid integration of real and simulated sensor data, that is also capable of being integrated into existing IoT systems. Sensyml is a cloud based Infrastructure-as-a-Service (IaaS) system that enables users to test both functionality and scalability of their IoT applications."
pub.1095008239,Services Enhancing Usage of Large Equipments in a Private Cloud,"Large equipments are important research resources in colleges and universities, while the efficiently use into scientific research is a key issue. Few equipment management information systems (MIS) are built nowadays, even in areas like equipment reservation. Existing systems are geo-diverse and hard to be integrated, thus services that could enhance the usage of large equipments is needed. Cloud computing has SOA architecture based on the pay-for-use model, the underlying infrastructure is suited for highly distributed environments. This paper introduces China Equipment and Education Resource System (CERS), and proposes a system named LECloud (Large Equipments Cloud) based on a private cloud, providing large equipment information service and equipment knowledge service as an e-learning platform. It's a highly heterogeneous and geo-distributed system that provides on-demand services. We drew up standards for interfaces and realized the business integration for large equipment usage. We also designed a cloud management framework along with an effective scheduling strategy, aiming to provide better services to scientific researchers in SaaS level."
pub.1094843593,ABS: Agent-Based Scheduling for Data-Intensive Workflow in Software-as-a-Service Environments,"Efficient allocation and execution of data-intensive scientific workflows to reduce execution time and the data transferring overhead still remains enormous challenge for the adoption of cloud computing to execute data-intensive scientific workflows. To face such a challenge, in this paper, we introduce an agent-based scheduling approach, namely ABS, that automates the deployment of data-intensive workflows in Software-as-a-Service (SaaS) environments considering two objectives: makespan and data transfer overhead. The algorithm ABS includes two phases: 1) planning phase, task agents in the same workflow form groups through cooperation to reduce data transfer among tasks; 2) scheduling phase, task groups will be assigned to VMs such that reducing makespans for workflows. The experiments conducted using real-world workflow applications demonstrate that our solution outperform the solution produced by the existing algorithm."
pub.1118214542,Integration of Cloud Computing and Web2.0 Collaboration Technologies in E-Learning,"Cloud computing technology is an emerging new computing paradigm for
delivering computing services. Although it still in its early stage, it has
changed the way how many applications are developed and accessed. This
computing approach relies on a number of existing technologies, such as Web2.0,
virtualization, Service oriented architecture SOA, Web services,etc.Cloud
computing is growing rapidly and becoming an adoptable technology for the
organizations especially education institutes, with its dynamic scalability and
usage of virtualized resources as a service through the Internet.Today,
eLearning is also becoming a very popular and powerful trend.However,in
traditional web based eLearning systems,building and maintenance are located
onsite in institutions or enterprises, which cause lot of problems to appear
such as lacking the support of underlying infrastructure, which can dynamically
allocate the needed calculation and storage resources for eLearning systems.As
the need for e learning is increasing continuously and its necessary for
eLearning systems to keep pace with the right technology needed for development
and improvement.However, todays technologies such as Web 2.0, Cloud, etc.enable
to build more successful and effective educational environment,that provide
collaboration and interaction in eLearning environments.The challenge is to use
and integrate these technologies in order to construct tools that allow the
best possible learning results.Cloud computing and Web 2.0 are two areas that
are starting to strongly effect how the development,deployment and usage of
eLearning application.This paper presents the benefits of using cloud computing
with the integration of Web 2.0 collaboration technologies in eLearning
environment."
pub.1101375503,Chapter 17 Cloud-Based Computing,"Cloud computing is a disruptive technological paradigm for providing on-demand access to data and applications from anywhere at any time. Flexible payment models, a familiar Web interface, scalability, and independence from local machines have made cloud computing one of the most popular and fast growing technological advancements. Cloud computing has therefore the potential to improve healthcare delivery and existing health information systems and practices. Examples are as follows: being able to utilize highly scalable computing resources, having remote access to electronic health records via computer or mobile devices, and storing and analyzing data from wearable devices. Despite numerous opportunities of cloud computing, there are still major security, privacy, and regulatory concerns hindering the integration of healthcare services with the cloud environment. This chapter aims to describe the basic concepts of cloud computing and identify its potential applications, benefits, and challenges in healthcare."
pub.1154226623,Distributed energy sharing algorithm for Micro Grid energy system based on cloud computing,"Abstract The reduction of adverse environmental effects and the socioeconomic advantages of renewable energy systems promote greater integration of distributed energy systems into the traditional electrical networks. A new type of sharing economy is emerging with the sharing of energy resources to reduce transaction costs by using platform services in the cloud. Given the obstacles posed by the legacy system and various forms of renewable energy integration, Distributed Energy and Micro Grids (DE‐MG) are an efficient means of raising the quality of energy services. Rules for microgrid scalability, maintaining a budget, and security can make this difficult. Consumers are better at receiving the best renewable energy allotment price using a cloud‐based Peer‐to‐Peer (P2P) network. The main objective is to lower the daily energy cost of microgrid power in commercial buildings. In the proposed work, cloud‐based P2P for peer‐Multi Agent System (p‐MAS) optimization techniques are used to reduce system peak and integrated Demand Response (DR) with Energy Management System (EMS) in a commercial MG. To fill knowledge gaps about how various power market architectures and individual decision‐making processes impact local interactions and market outcomes, cloud‐based P2P for Modelling Leveraging Agents (MLA) is used for bill calculation. A performance measure is finally created for cost evaluation and reliability to measure the social benefits of cloud‐based P2P models for exchanging energy. For various price environments and resource types, a comparison between the proposed cloud‐based P2P model with an existing P2P model for exchanging energy is provided. The primary use of a distributed P2P model for exchanging power in a microgrid is to reduce electricity costs and increase grid environment reliability."
pub.1013989606,WAYFINDER: parallel virtual machine reallocation through A* search,"Modern virtual machine (VM) management software enables consolidation of VMs for power savings or load-balancing for performance. While existing literature provides various methods for computing a better load-balanced, or consolidated goal state, it fails to adequately suggest the best path from the system’s current state to the desired goal allocation. This paper discusses an approach to efficient path finding in VM placement problems for cloud computing environments of moderate scale with results indicating the solution is reasonable for managing hundreds of VMs. We present an overview of known approaches to dynamic VM placement and discuss their shortcomings with respect to dynamic reallocation. We then describe a novel design and implementation of a heuristic search algorithm to determine optimal sequential migration plans to transition from a given VM-to-host allocation to an arbitrary desired allocation state. We then elaborate nuances of A* application to this domain along with our simulation-based validation approach. Finally, this work demonstrates a novel and highly effective technique for exploiting migration parallelism in order to rapidly achieving VM reallocation convergence suitable for continual workload rebalancing in cloud data centers."
pub.1134563989,Partitioning Stateful Data Stream Applications in Dynamic Edge Cloud Environments,"Computation partitioning is an important technique to improve the application performance by selectively offloading some computations from the mobile devices to the nearby edge cloud. In a dynamic environment in which the network bandwidth to the edge cloud may change frequently, the partitioning of the computation needs to be updated accordingly. The frequent updating of partitioning leads to high state migration cost between the mobile side and edge cloud. However, existing works don’t take the state migration overhead into consideration. Consequently, the partitioning decisions may cause significant network congestion and increase overall completion time tremendously. In this article, with considering the state migration overhead, we propose a set of novel algorithms to update the partitioning based on the changing network bandwidth. To the best of our knowledge, this is the first work on computation partitioning for stateful data stream applications in dynamic environments. The algorithms aim to alleviate the network congestion and minimize the make-span through selectively migrating state in dynamic edge cloud environments. Extensive simulations show our solution not only could selectively migrate state but also outperforms other classical benchmark algorithms in terms of make-span. The proposed model and algorithms will enrich the scheduling theory for stateful tasks, which has not been explored before."
pub.1084612637,Global experimental verification of Docker-based secured mVoIP to protect against eavesdropping and DoS attacks,"The cloud-computing paradigm has been driving the cloud-leveraged refactoring of existing information and communications technology services, including voice over IP (VoIP). In this paper, we design a prototype secure mobile VoIP (mVoIP) service with the open-source Asterisk private branch exchange (PBX) software, using Docker lightweight virtualization for mobile devices with the immutable concept of continuous integration and continuous deployment (CI/CD). In addition, the secure mVoIP service provides protection against eavesdropping and denial-of-service (DoS) attacks, using secure voice coding and real-time migration. We also experimentally verify the quality of the secure voice and the associated communication delay over a distributed global connectivity environment to protect against eavesdropping and real-time migration to mitigate DoS attacks."
pub.1169306016,SecFlow: Adaptive Security-Aware Workflow Management System in Multi-cloud Environments,"In this paper, we propose an architecture for a security-aware workflow management system (WfMS) we call SecFlow in answer to the recent developments of combining workflow management systems with Cloud environments and the still lacking abilities of such systems to ensure the security and privacy of cloud-based workflows. The SecFlow architecture focuses on full workflow life cycle coverage as, in addition to the existing approaches to design security-aware processes, there is a need to fill in the gap of maintaining security properties of workflows during their execution phase. To address this gap, we derive the requirements for such a security-aware WfMS and design a system architecture that meets these requirements. SecFlow integrates key functional components such as secure model construction, security-aware service selection, security violation detection, and adaptive response mechanisms while considering all potential malicious parties in multi-tenant and cloud-based WfMS."
pub.1094571770,Facilitating the Execution of HPC Workloads in Colombia through the Integration of a Private IaaS and a Scientific PaaS/SaaS Marketplace,"Many small and medium size research groups have limitations to execute their HPC workloads because of the need to buy, configure and maintain their own cluster or grid solutions. At the same time, some research groups have large infrastructures with low utilization levels due in part to the tools they offer to end users, that require that each end user configures complex and distributed environments. In this paper we present an effort between a private and public institution to offer scientific applications as a service taking advantage of an existing infrastructure to create a private IaaS using OpenStack, and offering scientific applications through a friendly user interface. This strategy facilitates that researchers can run their HPC workloads on a private cloud in a transparent way, hiding the complexities of distributed and scalable cloud environments. We show how this strategy may help to increase the utilization of infrastructures, how it allows end users to easily execute and share their applications through a SaaS marketplace, and how new applications can be configured and deployed using a PaaS platform.1"
pub.1046178186,Symmetric Matrix-based Predictive Classifier for Big Data computation and information sharing in Cloud,"Big Data requires real-time data-intensive processing that runs on high-performance clusters. In Big Data applications, data collection has grown exponentially. It is highly complex to extract, identify and transmit information using existing software tools. Big Data applications increase the gaps in performance between legitimate classifiers. In this paper, a Parallel Symmetric Matrix-based Predictive Bayes Classifier (PSM-PBC) model is developed for efficient Big Data computation and information sharing in Cloud environment. Initially, the Tridiagonal Symmetric Matrix is constructed on distributed Big Data in parallel. This approach enables an increase in the rate of data computation using a Householder transformation. A Cross-Validated Bayes Classifier then evaluates real-value diagonal search data to improve the prediction rate. Finally, the MapReduce function on Bayes Classes provides efficient predictive analytics regarding Big Data. The experimental evaluations are conducted with Amazon EC2 Cloud Big Data sets and exhibit improvement of the prediction rate by 10.55% along with a reduction in computation time by 40.93% compared to state-of-the-art methods."
pub.1164001826,Phase space load balancing priority scheduling algorithm for cloud computing clusters,"Due to the development of new technologies such as the Internet and cloud computing, high requirements have been placed on the storage and management of big data. At the same time, new applications in the cloud computing environment also pose new requirements for cloud storage systems, such as strong scalability and high concurrency. Currently, the existing nosql database system is based on cloud computing virtual resources, supporting dynamic addition and deletion of virtual nodes. Based on the study of phase space reconstruction, the necessity of considering traffic flow as a chaotic time series is analyzed. In addition, offline data migration methods based on load balancing are also studied. Firstly, a data migration model is proposed through analysis, and the factors that affect migration performance are analyzed. Based on this, optimization objectives for migration are proposed. Then, the system design of data migration is presented, and optimization research is conducted from two aspects around the migration optimization objectives: optimizing from the data source layer, and proposing the LBS method to convert data sources into distributed data sources, ensuring the balanced distribution of data and meeting the scalability requirements of the system. This paper applies cloud computing technology and phase space reconstruction to load balancing scheduling algorithms to promote their development."
pub.1128635900,An improved migration scheme to achieve the optimal service time for the active jobs in 5G cloud environment,"Among the emerging technologies, cloud computing plays a vital role in the current business world. Through its various deployment models ‘n’ number of vendors of the IT industry utilizes cloud computing to achieve their business deals. Though the usage of cloud increases day by day, there are some major challenges in the cloud computing such as load balancing issues, device utilization and energy optimization. In this paper, load balancing issues are addressed through the proposed Server Distinction based on the Corresponding Active Weights of the Jobs (SDCAW) algorithm. The proposed algorithm concentrates on the live migration of the active jobs among the virtual machines in the cloud environment. The algorithm itself contains an instruction queue where the active jobs are stored and scheduled based on the procedure. Many existing systems concentrated only on migration of non-active jobs in the cloud environment. The proposed algorithm has been compared with the existing systems and produces optimal solution for the cloud environment by minimizing the makespan of the environment."
pub.1091053004,SCARF: A container-based approach to cloud-scale digital forensic processing,"The rapid growth of raw data volume requiring forensic processing has become one of the top concerns of forensic analysts. At present, there are no readily available solutions that provide: a) open and flexible integration of existing forensic tools into a processing pipeline; and b) scale-out architecture that is compatible with common cloud technologies.Containers, lightweight OS-level virtualized environments, are quickly becoming the preferred architectural unit for building large-scale data processing systems. We present a container-based software framework, SCARF, which applies this approach to forensic computations. Our prototype demonstrates its practicality by providing low-cost integration of both custom code and a variety of third-party tools via simple data interfaces. The resulting system fits well with the data parallel nature of most forensic tasks, which tend to have few dependencies that limit parallel execution.Our experimental evaluation shows that for several types of processing tasks–such as hashing, indexing and bulk processing–performance scales almost linearly with the addition of hardware resources. We show that the software engineering effort to integrate new tools is quite modest, and all the critical task scheduling and resource allocation are automatically managed by the container orchestration runtime–Docker Swarm, or similar."
pub.1094786792,E-Grant: Enforcing Encrypted Dynamic Security Constraints in the Cloud,"Cloud computing is an established paradigm that attracts enterprises for offsetting the cost to more competitive outsource data centres. Considering economic benefits offered by this paradigm, organisations could outsource data storage and computational services. However, data in the cloud environment is within easy reach of service providers. One of the strong obstacles in widespread adoption of the cloud is to preserve confidentiality of the data. Generally, confidentiality of the data could be guaranteed by employing existing encryption schemes. For regulating access to the data, organisations require access control mechanisms. Unfortunately, access policies in cleartext might leak information about the data they aim to protect. The major research challenge is to enforce dynamic access policies at runtime, i.e., enforcement of dynamic security constraints (including dynamic separation of duties and Chinese wall) in the cloud. The main challenge lies in the fact that dynamic security constraints require notion of sessions for managing access histories that might leak information about the sensitive data if they are available as cleartext in the cloud. In this paper, we present E-GRANT: an architecture able to enforce dynamic security constraints without relying on a trusted infrastructure, which can be deployed as Software-as-a-Service (SaaS). In E-G RANT, sessions' access histories are encrypted in such a way that enforcement of constraints is still possible. As a proof-of-concept, we have implemented a prototype and provide a preliminary performance analysis showing a limited overhead, thus confirming the feasibility of our approach."
pub.1124783534,Performance and Energy Balanced Algorithm for Executing High Performance Computing Application,"NoC (Network-on-Chip) in recent times has been broadly used in majority of present multi-core (MC) processing environment (i.e., processor) (especially in cloud computing servers). Then, SoCs (System-on-Chips), as a scalable and flexible strategy to the widespread integration of higher number of element in the chips. Nonetheless, a main issue is reliable processing, performance efficiency and energy efficacy perquisite of BigData processing framework. This work assume self-aware MC architectures that autonomously adjust or optimize their performance to accommodate users quality of service (QoS) performance requirement, job execution performance, energy efficiency, and resource accessibility. Numerous researchers are concerned about the lack of energy conservation in cloud computing environment and hence numerous power efficient technologies are introduced by different researchers to protect environment by enormous amount of power dissipation and enhance the performance of the environment. However, these power efficient technologies requires enormous amount of interaction cost between inter-processors. Moreover, these technologies provides insufficient results and energy consumption of cloud computing devices cannot be reduced largely due to enormous amount of memory utilization, routing etc. For overcoming research challenges, this work present a novel performance and energy balanced scheduling (PEBS) approach in cloud environment based on Dynamic voltage and Frequency Scaling (DVFS) technique is proposed. Experiment outcome shows PEBS_DVFS model attains good trade-off between system performance and energy consumption in multicore cloud computing (CC) architectures when compared with existing model."
pub.1140003167,Guest Editors’ Introduction: Machine Intelligence at the Edge,"Recent years have seen widespread application of machine learning (ML) to a diverse range of industries and problem domains. By taking advantage of the availability of massive amounts of data and scalable compute resources, ML methods are able to outperform traditional hand-tuned models on today’s wide-range of AI tasks, particularly in the settings of server computing and/or cloud computing. However, the success of ML in sensing applications such as object detection and speech recognition has also driven a demand for such technology (both training and inference) in edge settings, for applications such as autonomous vehicles, mobile devices, and embedded/Internet of Things (IoT) systems. Unfortunately, most existing ML models, hardware, and frameworks are tailored toward the server and/or cloud computing environments and are ill-equipped for edge computing. State- of-the-art ML systems in industry today already use custom frameworks, algorithms, and hardware built for a server infrastructure. Addressing the unique challenges of ML at the edge will similarly require specialization, codesign, and integration of domain knowledge for the edge across the computing stack. This timely special issue IEEE Design&Test of called for novel research on ML models, hardware architectures, programming tools, and design methodologies for ML at the edge."
pub.1085049030,CloudPerf,"The evolution of cloud-computing imposes many challenges on performance testing and requires not only a different approach and methodology of performance evaluation and analysis, but also specialized tools and frameworks to support such work. In traditional performance testing, typically a single workload was run against a static test configuration. The main metrics derived from such experiments included throughput, response times, and system utilization at steady-state. While this may have been sufficient in the past, where in many cases a single application was run on dedicated hardware, this approach is no longer suitable for cloud-based deployments. Whether private or public cloud, such environments typically host a variety of applications on distributed shared hardware resources, simultaneously accessed by a large number of tenants running heterogeneous workloads. The number of tenants as well as their activity and resource needs dynamically change over time, and the cloud infrastructure reacts to this by reallocating existing or provisioning new resources. Besides metrics such as the number of tenants and overall resource utilization, performance testing in the cloud must be able to answer many more questions: How is the quality of service of a tenant impacted by the constantly changing activity of other tenants? How long does it take the cloud infrastructure to react to changes in demand, and what is the effect on tenants while it does so? How well are service level agreements met? What is the resource consumption of individual tenants? How can global performance metrics on application- and system-level in a distributed system be correlated to an individual tenant's perceived performance? In this paper we present CloudPerf, a performance test framework specifically designed for distributed and dynamic multi-tenant environments, capable of answering all of the above questions, and more. CloudPerf consists of a distributed harness, a protocol-independent load generator and workload modeling framework, an extensible statistics framework with live-monitoring and post-analysis tools, interfaces for cloud deployment operations, and a rich set of both low-level as well as high-level workloads from different domains."
pub.1168458499,Authentication and Identity Management Based on Zero Trust Security Model in Micro-cloud Environment,"The abilities of traditional perimeter-based security architectures are rapidly decreasing as more enterprise assets are moved toward the cloud environment. From a security viewpoint, the Zero Trust framework can better track and block external attackers while limiting security breaches resulting from insider attacks in the cloud paradigm. Furthermore, Zero Trust can better accomplish access privileges for users and devices across cloud environments to enable the secure sharing of resources. Moreover, the concept of zero trust architecture in cloud computing requires the integration of complex practices on multiple layers of system architecture, as well as a combination of a variety of existing technologies. This paper focuses on authentication mechanisms, calculation of trust score, and generation of policies in order to establish required access control to resources. The main objective is to incorporate an unbiased trust score as a part of policy expressions while preserving the configurability and adaptiveness of parameters of interest. Finally, the proof-of-concept is demonstrated on a micro-cloud platform solution."
pub.1136275128,Energy-Aware Service Function Chain Embedding in Edge–Cloud Environments for IoT Applications,"The implementation of Internet-of-Things (IoT) applications faces several challenges in practice, such as compliance with Quality-of-Service requirements, resource constraints, and energy consumption. In this context, the joint edge–cloud paradigm for IoT applications can resolve some of the issues arising in pure cloud computing scenarios, such as those related to latency, energy, or privacy. Therefore, an edge–cloud environment could be promising for resource and energy-efficient IoT applications that implement virtual network functions (VNFs) bound together into service function chains (SFCs). However, a resource and energy-efficient SFC placement requires smart SFC embedding mechanisms in the edge–cloud environment, as several challenges arise, such as IoT service chain modeling and evaluation, the tradeoff between resource allocation, energy efficiency and performance, and the resource dynamics. In this article, we address issues in modeling resource and energy utilization for IoT applications in edge–cloud environments. A smart traffic monitoring IP camera system is deployed as a use case for a realistic modeling of a service chain. The system is implemented in our testbed, which is designed and developed specifically to model and investigate the resource and energy utilization of SFC embedding strategies. A resource and energy-aware SFC strategy in the edge–cloud environment for IoT applications is then proposed. Our algorithm is able to cope with dynamic load and resource situations emerging from dynamic SFC requests. The strategy is evaluated systematically in terms of the acceptance ratio of SFC requests, resource efficiency and utilization, power consumption, and VNF migrations depending on the offered system load. Results show that our strategy outperforms some existing approaches in terms of resource and energy efficiency, thus it overcomes the relevant challenges from practice and meets the demands of IoT applications."
pub.1156685227,Real-Time Cost Optimization Approach Based on Deep Reinforcement Learning in Software-Defined Security Middle Platform,"In today’s business environment, reducing costs is crucial due to the variety of Internet of Things (IoT) devices and security infrastructure. However, applying security measures to complex business scenarios can lead to performance degradation, making it a challenging task. To overcome this problem, we propose a novel algorithm based on deep reinforcement learning (DRL) for optimizing cost in multi-party computation software-defined security middle platforms (MPC-SDSmp) in real-time. To accomplish this, we first integrate fragmented security requirements and infrastructure into the MPC-SDSmp cloud model with privacy protection capabilities to reduce deployment costs. By leveraging the power of DRL and cloud computing technology, we enhance the real-time matching and dynamic adaptation capabilities of the security middle platform (Smp). This enables us to generate a real-time scheduling strategy for Smp resources that meet low-cost goals to reduce operating costs. Our experimental results demonstrate that the proposed method not only reduces the costs by 13.6% but also ensures load balancing, improves the quality-of-service (QoS) satisfaction by 18.7%, and reduces the average response time by 34.2%. Moreover, our solution is highly robust and better suited for real-time environments compared to the existing methods."
pub.1143481196,A Dynamic Decision Support System for Selection of Cloud Storage Provider,"In recent years, the cloud computing model has gained increasing attention and popularity in the field of information technology. For this reason, people are migrating their applications to public, private, or hybrid cloud environments. Many cloud vendors offer similar features with varying costs, so an appropriate choice will be the key to guaranteeing comparatively low operational costs for an organization. The motivation for this work is the necessity to select an appropriate cloud storage provider offering for the migration of applications with less cost and high performance. However, the selection of a suitable cloud storage provider is a complex problem that entails various technical and organizational aspects. In this research, a dynamic Decision Support System (DSS) for selection of an appropriate cloud storage provider is proposed. A web-based application is implemented using PHP and MySQL to facilitate decision makers. The proposed mechanism has been optimized in a way that enables the system to address static database issues for which a user might not acquire the best solution. It focuses on comparing and ranking cloud storage providers by using two modules: scraping and parsing. The evaluation of the proposed system is carried out with appropriate test cases and compared with existing tools and frameworks."
pub.1100829410,Energy-efficient virtual content distribution network provisioning in cloud-based data centers,"Cloud-based content distribution networks (CDNs) consist of multiple servers that consume large amounts of energy. However, with the development of a cloud-based software defined network (SDN), a new paradigm of the virtual content distribution network (vCDN) has emerged. In an emerging cloud-based vCDN environment, the development and adjustment of vCDN components has become easier with the aid of SDN technology. This transformation provides the opportunity to use vCDNs to reduce energy consumption by adjusting the scale of the vCDN components. Energy costs can be reduced by deactivating the commercial servers carrying the software components of the vCDN, such as replica servers, the firewall or routers. In addition, the CDN requires a high service level agreement (SLA) to respond to clients’ requests, potentially consuming large amounts of energy. In this research, we focus on the issue of the energy savings of a CDN in a cloud computing environment while maintaining a high quality of service (QoS). We propose an approximate algorithm termed max flow forecast (MFF) to determine the number of software components in the vCDN. Additionally, we use a real traffic trace from a website to assess our algorithm. The experimental results show that MFF can produce a larger energy reduction than the existing algorithms for an identical SLA. We fully justify our research as a good example for the emerging cloud."
pub.1049683910,Cloud Computing and SBSE,"Global spend on Cloud Computing is estimated to be worth $131 billion and growing at annual rate of 19% [1]. It represents one of the most disruptive innovations within the computing industry, and offers great opportunities for researchers in Search Based Software Engineering (SBSE).In the same way as the development of large scale electricity generation, the physical centralisation of computing resources that Cloud Computing involves provides opportunities for economies of scale [2]. Furthermore, it enables more extensive consolidation and optimisation of resource usage than previously possible: whereas in the past we have been resigned to the phenomenon of wasted spare cycles, the Cloud offers opportunities for large-scale consolidation, along with standardised measurement and control of resource consumption.There are two key stakeholders within the Cloud Computing sector: providers, such as Amazon and Google, and clients, the consumers of Cloud services paying for infrastructure and other utilities by the hour. Both providers and clients are greatly concerned with solving optimisation problems that determine their resource usage, and ultimately their expenditure. Example resources include CPU time, physical RAM usage, storage consumption, and network bandwidth.From the point of view of the provider, improving utilisation leads to greater return on capital investment – hence the emergence of oversubscription policies [3], where more resources are promised to clients than are physically provisioned. Similarly, clients wish to reduce their resource demands because there is a direct relationship between resource consumption and operating cost. The rise of Cloud Computing has in many cases made explicit the cost of computation that was previously considered only as depreciation of capital investment.Example optimisation problems include: the configuration of servers, virtualisation, and operating systems to reduce storage and memory usage; transformation of software and architectures to adapt them to a Cloud platform; the intelligent placement of virtual machines to improve consolidation, reduce business risks and manage network usage; extensive, online and continuous testing to improve robustness; policy-level decisions such as management of scalability, demand modelling and spot market participation; [4].A relentless focus on efficiency offers researchers in SBSE the opportunity to make an important contribution. The field has an established record in solving similar optimisation problems within software engineering, and treating non-functional concerns as first class objectives.The way in which software is deployed within Cloud systems also offers exciting possibilities for SBSE researchers, principally due to its centralised nature. Software is no longer deployed or versioned in the traditional manner, and this consequently enables dynamic, controlled and sometimes partial deployment of new code (a procedure known as “canarying” [5]). A key ins"
pub.1011389286,Cloud Computing in Support of Supply Chain Information System Infrastructure: Understanding When to go to the Cloud,"Research suggests that there are other, more granular factors within the domain of innovation diffusion theory that influence the adoption of technological innovations. In this study, the circumstances that affect a firm's intention to adopt cloud computing technologies in support of its supply chain operations are investigated by considering tenets of classical diffusion theory as framed within the context of the information processing view. We posit that various aspects of an organization and its respective environment represent both information processing requirements and capacity, which influence the firm's desire to adopt certain information technology innovations. We conducted an empirical study using a survey method and regression analysis to examine our theoretical model. The results suggest that business process complexity, entrepreneurial culture and the degree to which existing information systems embody compatibility and application functionality significantly affect a firm's propensity to adopt cloud computing technologies. The findings support our theoretical development and suggest complementarities between innovation diffusion theory and the information processing view. We encourage other scholars to refine our model in future supply chain innovation diffusion research. The findings of this study might also be used by industry professionals to aid in making more informed adoption decisions in regard to cloud computing technologies in support of the supply chain."
pub.1146089636,A Practical Application Using Parametric Modeling for As-Built BIM Generation from Point Clouds,"Physical 3D objects can be expressed as an abstracted format in digital space through parametrized profile. The idea of using object-based parametric modeling has been adopted in the Building Information Modeling (BIM) domain by reducing human labor efforts via digitizing conventional manual drawing-based 2D modeling into a computing-based 3D modeling. However, research efforts on the adoption of parametric modeling has been primarily placed on early design, while applications have been rare for generating an as-built BIM of existing structures. Thus, the main objective of this paper is to investigate viability of leveraging parametric modeling paradigm for generating as-built BIM from semantically segmented objects. Practical implementations are shown with a commercial parametric modeling software, Dynamo visual Application Programming Interface (API), which is adopted for constructing 3D parametric models within the Autodesk Revit BIM environment. Using the Stanford Large-Scale 3D Indoor Spaces Dataset (S3DIS) as baseline point cloud segments, a detailed set of modeling procedures is illustrated for three object categories which are grouped based on their parametric properties. Our study provides a stepwise advancement toward automating Scan-to-BIM, avoiding dependence on modeler experience for reconstructing an as-built 3D model from raw point clouds."
pub.1100509560,A Distributed Snapshot Protocol for Efficient Artificial Intelligence Computation in Cloud Computing Environments,"Many artificial intelligence applications often require a huge amount of computing resources. As a result, cloud computing adoption rates are increasing in the artificial intelligence field. To support the demand for artificial intelligence applications and guarantee the service level agreement, cloud computing should provide not only computing resources but also fundamental mechanisms for efficient computing. In this regard, a snapshot protocol has been used to create a consistent snapshot of the global state in cloud computing environments. However, the existing snapshot protocols are not optimized in the context of artificial intelligence applications, where large-scale iterative computation is the norm. In this paper, we present a distributed snapshot protocol for efficient artificial intelligence computation in cloud computing environments. The proposed snapshot protocol is based on a distributed algorithm to run interconnected multiple nodes in a scalable fashion. Our snapshot protocol is able to deal with artificial intelligence applications, in which a large number of computing nodes are running. We reveal that our distributed snapshot protocol guarantees the correctness, safety, and liveness conditions."
pub.1172981706,Flow Optimization at Inter-Datacenter Networks for Application Run-time Acceleration,"In the present-day, distributed applications are commonly spread across
multiple datacenters, reaching out to edge and fog computing locations. The
transition away from single datacenter hosting is driven by capacity
constraints in datacenters and the adoption of hybrid deployment strategies,
combining on-premise and public cloud facilities. However, the performance of
such applications is often limited by extended Flow Completion Times (FCT) for
short flows due to queuing behind bursts of packets from concurrent long flows.
To address this challenge, we propose a solution to prioritize short flows over
long flows in the Software-Defined Wide-Area Network (SD-WAN) interconnecting
the distributed computing platforms. Our solution utilizes eBPF to segregate
short and long flows, transmitting them over separate tunnels with the same
properties. By effectively mitigating queuing delays, we consistently achieve a
1.5 times reduction in FCT for short flows, resulting in improved application
response times. The proposed solution works with encrypted traffic and is
application-agnostic, making it deployable in diverse distributed environments
without modifying the applications themselves. Our testbed evaluation
demonstrates the effectiveness of our approach in accelerating the run-time of
distributed applications, providing valuable insights for optimizing
multi-datacenter and edge deployments."
pub.1095257509,CMcloud: Cloud Platform for Cost-Effective Offloading of Mobile Applications,"Recent efforts towards mobile cloud propose to offload mobile applications to cloud servers for the improved performance and battery life of mobile devices. However, existing schemes completely ignore the costs of cloud resources by assuming that idle servers are always available for free of charge. These unrealistic assumptions make each server run only a small load to achieve the guaranteed high offload performance. Therefore, these schemes cannot be applied to real-world commercial clouds which aim to minimize the operation costs by maximizing the server throughput, and then charge users for their resource usage. In this paper, we propose CMcloud, a novel cost-effective mobile-to-cloud offloading platform, which works nicely under the real-world cloud environments. CMcloud minimizes both the server costs and the user service fee by offloading as many mobile applications to a single server as possible, while satisfying the target performance of all applications. To achieve such goals, CMcloud exploits novel architecture performance modeling and server migration techniques. Our implementation shows that CMcloud can improve the datacenter throughput by 84% over a conventional static light-load scheme (or a 2.7x higher per-socket throughput.) Alternatively, CMcloud reduces the number of service failures by 83% over a static high-load scheme, while even improving the throughput by 31%."
pub.1145746271,Combining Node-RED and Openwhisk for Pattern-based Development and Execution of Complex FaaS Workflows,"Modern cloud computing advances have been pressing application modernization
in the last 15 years, stressing the need for application redesign towards the
use of more distributed and ephemeral resources. From the initial IaaS and PaaS
approaches, to microservices and now to the serverless model (and especially
the Function as a Service approach), new challenges arise constantly for
application developers. This paper presents a design and development
environment that aims to ease application evolution and migration to the new
FaaS model, based on the widely used Node-RED open source tool. The goal of the
environment is to enable a more user friendly and abstract function and
workflow creation for complex FaaS applications. To this end, it bypasses
workflow description and function reuse limitations of the current FaaS
platforms, by providing an extendable, pattern-enriched palette of ready-made,
reusable functionality that can be combined in arbitrary ways. The environment
embeds seamless DevOps processes for generating the deployable artefacts (i.e.
functions and images) of the FaaS platform (Openwhisk). Annotation mechanisms
are also available for the developer to dictate diverse execution options or
management guidelines towards the deployment and operation stacks. The
evaluation is based on case studies of indicative scenarios, including
creating, registering and executing functions and flows based on the Node-RED
runtime, embedding of existing legacy code in a FaaS environment, parallelizing
a workload, collecting data at the edge and creating function orchestrators to
accompany the application. For the latter, a detailed argumentation is provided
as to why this process should not be constrained by the ""double billing""
principle of FaaS."
pub.1022478721,Scaling Agile to Lean – Track Summary,"There are many books, journals and articles explaining agile and to a much lesser extent lean software development methods. Technical competences such as software architecture, automated testing and quality assurance are key focal areas of these materials on agile and lean methods. While some are prescriptive, there is often a substantial difference between the textbook ‘vanilla’ version of the method and the method actually enacted in practice. Prescribed practices are inevitably interpreted in diverse ways or tailored to suit the specific needs of teams. The constantly evolving technological environment that software development projects are enacted in also highlights the need to tailor prescribed agile practices to work in emerging deployment models, such as cloud computing and mobile computing. Quite a few empirical studies focus on how agile methods were adopted, tailored and used in realworld contexts (e.g., Rasmusson, 2003, Fitzgerald et al., 2006). However, there is a distinct absence of lean software development cases, and cases of agile deployment tend to be weak in terms of theoretical foundation, fail to build on previous lessons, and often lack consistency and coherence (Abrahamsson et al., 2009, Conboy, 2009). In the absence of sound, systematic research, there are few lessons learned across studies, and thus the existing body of knowledge is somewhat fragmented and inconclusive. A systematic and insightful understanding of agile adoption, tailoring and execution is yet to be achieved, and research on lean software development is yet to begin."
pub.1150023142,A Cooperative Perception Environment for Traffic Operations and Control,"Existing data collection methods for traffic operations and control usually
rely on infrastructure-based loop detectors or probe vehicle trajectories.
Connected and automated vehicles (CAVs) not only can report data about
themselves but also can provide the status of all detected surrounding
vehicles. Integration of perception data from multiple CAVs as well as
infrastructure sensors (e.g., LiDAR) can provide richer information even under
a very low penetration rate. This paper aims to develop a cooperative data
collection system, which integrates Lidar point cloud data from both
infrastructure and CAVs to create a cooperative perception environment for
various transportation applications. The state-of-the-art 3D detection models
are applied to detect vehicles in the merged point cloud. We test the proposed
cooperative perception environment with the max pressure adaptive signal
control model in a co-simulation platform with CARLA and SUMO. Results show
that very low penetration rates of CAV plus an infrastructure sensor are
sufficient to achieve comparable performance with 30% or higher penetration
rates of connected vehicles (CV). We also show the equivalent CV penetration
rate (E-CVPR) under different CAV penetration rates to demonstrate the data
collection efficiency of the cooperative perception environment."
pub.1042522676,TSMC: A Novel Approach for Live Virtual Machine Migration,"Cloud computing attracted more and more attention in recent years, and virtualization technology is the key point for deploying infrastructure services in cloud environment. It allows application isolation and facilitates server consolidation, load balancing, fault management, and power saving. Live virtual machine migration can effectively relocate virtual resources and it has become an important management method in clusters and data centers. Existing precopy live migration approach has to iteratively copy redundant memory pages; another postcopy live migration approach would lead to a lot of page faults and application degradation. In this paper, we present a novel approach called TSMC (three-stage memory copy) for live virtual machine migration. In TSMC, memory pages only need to be transmitted twice at most and page fault just occurred in small part of dirty pages. We implement it in Xen and compare it with Xen’s original precopy approach. The experimental results under various memory workloads show that TSMC approach can significantly reduce the cumulative migration time and total pages transferred and achieve better network IO performance in the same time."
pub.1127655090,Computer Vision Toolkit for Non-invasive Monitoring of Factory Floor Artifacts,"Digitization has led to smart, connected technologies be an integral part of
businesses, governments and communities. For manufacturing digitization, there
has been active research and development with a focus on Cloud Manufacturing
(CM) and the Industrial Internet of Things (IIoT). This work presents a
computer vision toolkit (CV Toolkit) for non-invasive digitization of the
factory floor in line with Industry 4.0 requirements for factory data
collection. Currently, technical challenges persist towards digitization of
legacy systems due to the limitation for changes in their design and sensors.
This novel toolkit is developed to facilitate easy integration of legacy
production machinery and factory floor artifacts with the digital and smart
manufacturing environment with no requirement of any physical changes in the
machines. The system developed is modular, and allows real-time monitoring of
production machinery. Modularity aspect allows the incorporation of new
software applications in the current framework of CV Toolkit. To allow
connectivity of this toolkit with manufacturing floors in a simple, deployable
and cost-effective manner, the toolkit is integrated with a known manufacturing
data standard, MTConnect, to ""translate"" the digital inputs into data streams
that can be read by commercial status tracking and reporting software
solutions. The proposed toolkit is demonstrated using a mock-panel environment
developed in house at the University of Cincinnati to highlight its usability."
pub.1045922438,HKE‐BC: hierarchical key exchange for secure scheduling and auditing of big data in cloud computing,"Summary Big data is one of the most referred key words in recent information and communications technology industry. As the new‐generation distributed computing platform, cloud environments offer high efficiency and low cost for data‐intensive storage and computation for big data applications. Cloud resources and services are available in pay‐as‐you‐go mode, which brings extraordinary flexibility and cost‐effectiveness as well as minimal investments in their own computing infrastructure. However, these advantages come at a price—people no longer have direct control over their own data. Based on this view, data security becomes a major concern in the adoption of cloud computing. Authenticated key exchange is essential to a security system that is based on high‐efficiency symmetric‐key encryptions. With virtualisation technology being applied, existing key exchange schemes such as Internet key exchange become time consuming when directly deployed into cloud computing environment, especially for large‐scale tasks that involve intensive user–cloud interactions, such as scheduling and data auditing. In this paper, we propose a novel hierarchical key exchange scheme, namely hierarchical key exchange for big data in cloud, which aims at providing efficient security‐aware scheduling and auditing for cloud environments. In this novel key exchange scheme, we developed a two‐phase layer‐by‐layer iterative key exchange strategy to achieve more efficient authenticated key exchange without sacrificing the level of data security. Both theoretical analysis and experimental results demonstrate that when deployed in cloud environments with diverse server layouts, efficiency of the proposed scheme is dramatically superior to its predecessors cloud computing background key exchange and Internet key exchange schemes. Copyright © 2014 John Wiley & Sons, Ltd."
pub.1174061080,Reassessing Critical Success Factors for ERP Implementation in the Digital Era,"This paper examines the recent evolution of Enterprise Resource Planning (ERP) systems and explores the critical success factors (CSFs) for project implementation in the digital age. Adopting a qualitative inductive approach, the article first reports on CSFs evident in relevant literature drawn from the past two decades. In the second research phase, interview feedback from nine industry project managers is analysed to identify the CSFs now considered of particular relevance in the digital era. The article concludes that many of the established CSFs remain relevant, but recent research suggests the deployment of digital technologies and the availability of the cloud for ERP operation will mean that CSFs will be re-formulated in new technology and business environments. CSFs related to cloud-based vs on-premise software operation, system configuration and functionality trade-offs, and the integration of digital technologies into ERP products, are likely to emerge in the digital era. Future studies could profitably focus on these largely unresearched aspects of ERP projects, to which this article makes a small contribution that may provide a useful point of reference for subsequent studies."
pub.1128673873,Computer Vision Toolkit for Non-invasive Monitoring of Factory Floor Artifacts," Digitization has led to smart, connected technologies be an integral part of businesses, governments and communities. For manufacturing digitization, there has been active research and development with a focus on Cloud Manufacturing (CM) and the Industrial Internet of Things (IIoT). This work presents a computer vision toolkit (CV Toolkit) for non-invasive digitization of the factory floor in line with Industry 4.0 requirements for factory data collection. Currently, technical challenges persist towards digitization of legacy systems due to the limitation for changes in their design and sensors. This novel toolkit is developed to facilitate easy integration of legacy production machinery and factory floor artifacts with the digital and smart manufacturing environment with no requirement of any physical changes in the machines. The system developed is modular, and allows real-time monitoring of production machinery. The modularity aspect allows the incorporation of new software applications in the current framework of CV Toolkit. To allow connectivity of this toolkit with manufacturing floors in a simple, deployable and cost-effective manner, the toolkit is integrated with a known manufacturing data standard, MTConnect, to “translate” the digital inputs into data streams that can be read by commercial status tracking and reporting software solutions. The proposed toolkit is demonstrated using a mock-panel environment developed in house at the University of Cincinnati to highlight its usability."
pub.1094376308,Towards a dynamic cloud-enabled service eco-system,"Cloud computing re-defined what, in ICT, can be delivered “as a service”. The new types of services increase the potential of some Service-Oriented Architecture concepts, e.g. dynamic discovery and composition of services. We envision a dynamic cloud-enabled service ecosystem in which different business actors cooperate in delivering scalable high-value services to the end user. The different providers should be able to discover, request, use and discard services during runtime based on some internal management logic. We define and use a reference model to explain this dynamicity, characterized by the introduction of the concept of Virtual Distributed Execution Environment (VDEE) between the virtual resources and the applications. Based on a study of the state of the art on cloud and SOA technologies, we identify functions of existing platforms that could be integrated to implement the proposed reference model. We also identify some challenges that still need to be addressed. Finally we anticipate the business-related evolution towards the dynamic cloud."
pub.1171048890,A Proposed Framework of VAPT Services in Web Application Deployed on Infrastructure as a Service (Iaas),"Most companies in Malaysia require their employees to work from home due to the COVID-19 pandemic. This situation also increased the number of data generated from various sources, thus exposing them to different security risks. Even though the employees are encouraged to work from home because of the COVID-19 pandemic, they still need to communicate among themselves to do their work. However, working from home depends mainly on cloud computing (CC) applications that help employees accomplish their daily work efficiently. Injection attacks, such as SQL injection and Cross-Site Scripting (XSS), are critical security vulnerabilities that can lead to unauthorized access, data breaches, and potential service disruptions in web applications. With the increasing adoption of cloud computing, web applications deployed on cloud platforms like Amazon Web Services (AWS) are becoming more prevalent and vulnerable to such attacks. Therefore, it is crucial to develop practical Vulnerability Assessment and Penetration Testing (VAPT) techniques specifically tailored to identify and detect injection vulnerabilities in web applications deployed on AWS. However, existing VAPT methodologies often need more comprehensive coverage for injection vulnerabilities in cloud-based web applications, and they may not consider the unique characteristics and challenges associated with the AWS environment. This research addresses this gap by proposing an enhanced VAPT framework focusing specifically on injection attacks in web applications deployed on AWS."
pub.1094514610,Virtual Topology Mapping in SDN-enabled Clouds,"Software-Defined Networking (SDN) is transforming the way networks are designed and built, by decoupling control and data forwarding planes and centralizing network intelligence, mainly through the use of the OpenFlow standard. On the other hand the cloud is evolving to the networked cloud paradigm, integrating communication resources. Due to its functional characteristics (network programmability, enables network partitioning and virtualization) OpenFlow lends well to networked cloud concept. In order however to provide networked cloud resources with minimal management effort, it is essential to address efficiently the resource mapping problem at hand. The current study adapts an existing resource mapping algorithm, to be applied in an SDN-enabled networked cloud environment. The adaptation provides flexibility in the setup of virtual network topologies and efficient use of the flowspace."
pub.1170883284,Quantum based Fault-Tolerant Load Balancing in Cloud Computing with Quantum Computing,"Cloud computing has become an integral part of modern information technology infrastructure, providing scalability and flexibility to meet the demands of various applications. However, ensuring high availability and fault tolerance in cloud services is crucial for maintaining uninterrupted operations. Load balancing plays a vital role in distributing traffic efficiently across multiple servers to avoid overload and minimize service disruptions. Existing load balancing techniques require help to adapt to dynamic workloads, making them less reliable in cloud environments. Quantum computing handles complex optimization problems and offers a promising avenue for enhancing load balancing in the cloud. This research explores the concept of Fault-Tolerant Load Balancing (FTLB) in cloud computing with the integration of quantum computing. It investigates how quantum algorithms can optimize load-balancing decisions in real-time, considering dynamic workloads and server failures– the proposed novel quantum-load balancing (NQ-LB) algorithm designed to improve fault tolerance and resource utilization. The study employs a comparative analysis, evaluating the performance of the quantum-inspired load-balancing approach against classical load-balancing methods. We consider factors such as response time, resource allocation, fault tolerance, and scalability in a cloud computing environment. The results reveal the potential benefits of quantum computing in achieving fault-tolerant load balancing, particularly in scenarios with rapidly changing workloads and server failures. This research contributes to the advancement of cloud computing by harnessing the capabilities of quantum computing to enhance fault tolerance and load balancing. The findings can have implications for industries that rely on highly available cloud services, offering a more resilient infrastructure for critical applications."
pub.1156078610,The First Application of a Cloud-Based Well Engineering Solution in the Middle East Region,"Abstract
                  Low oil prices over the past few years have led oil and gas organizations to embrace digital transformation to improve the efficiency and quality of well design. Nowadays, digital transformation and reducing well construction can have significant impact on determining the financial viability of a discovery. Teamwork and the collaboration of numerous experts in different disciplines are required to achieve a properly engineered well design that can be executed with minimal risks.
                  Present-day well planning uses several increasingly obsolete techniques, including maintaining spreadsheets as risk registers, sharing multiple design iterations between disciplines, drilling, and geological concerns, manually replanning to accommodate changes, and people working in ""silos""—often from different offices or from home. Along with this, planning software, when installed on a computer, may be difficult to maintain and update with new software releases. Often an expert in one discipline will not have visibility on the work done by other engineers, impeding collaboration. Many linear processes lead not only to increased well planning time but also to suboptimal well design resulting in higher planning and execution costs (Bello et al. 2014).
                  Today, the oil and gas industry lag in the adoption of digital technologies and most of the ones that exist are not used commercially. Often, the reason is that these solutions address only a very small part of the entire well construction workflow. They are not fully matured, have a poor user interface, and require heavy computing power. Operators, on the other hand, are looking for a full-suite solution and they do not have the resources or expertise to connect various digital solutions existing in the market from different vendors. The need of the hour is an integrated solution that is easy to adopt, provides a collaborative work environment, is cloud-based to leverage computing power (Tanaka et al. 2018), and most importantly supports all the major well design workflows.
                  In this paper, we discuss the first commercial application of a digital cloud-based well-planning solution in the Middle East region, which enabled Crescent Petroleum to become the first operator to adopt the system in UAE."
pub.1140385367,Towards Edge Computing as a Service: Dynamic Formation of the Micro Data-Centers,"Edge computing brings cloud services closer to the edge of the network, where data originates, and dramatically reduces the network latency of the cloud. It is a bridge linking clouds and users making the foundation for novel interconnected applications. However, edge computing still faces many challenges like remote configuration, well-defined native applications model, and limited node capacity. It lacks geo-organization and a clear separation of concerns. As such edge computing is hard to be offered as a service for future real-time user-centric applications. This paper presents the dynamic organization of geo-distributed edge nodes into micro data-centers to cover any arbitrary area and expand capacity, availability, and reliability. A cloud organization is used as an influence with adaptations for a different environment, and a model for edge applications utilizing these adaptations is presented. It is argued that the presented model can be integrated into existing solutions or used as a base for the development of future systems. Furthermore, a clear separation of concerns is given for the proposed model. With the separation of concerns setup, edge-native applications model, and a unified node organization, we are moving towards the idea of edge computing as a service, like any other utility in cloud computing."
pub.1029683346,User value design for cloud courseware system,"A cloud learning environment enables an enriched learning experience compared to conventional methods of learning. Employing a value-sensitive approach, we undertook theoretical and empirical analyses to explore the values that influence potential users’ adoption of cloud courseware, by integrating cognitive motivations and user values as primary determining factors. We found that users’ intentions and behaviours are largely influenced by their perceptions of what is valuable about the cloud courseware in terms of sociability, learnability, and usability. These evaluations were found to be significant antecedents of cloud-computing intentions. This study makes a contribution to theory development as our model extends existing technology acceptance models and can be used to design user interfaces and promote the acceptance of cloud computing. For practical applications, the study findings can be used by industries promoting cloud services to increase user acceptance by addressing user values and incorporating them into cloud-computing design."
pub.1107690900,An Energy-Efficient Resource Management System for a Mobile <italic>Ad Hoc</italic> Cloud,"Recently, mobile ad hoc clouds have emerged as a promising technology for mobile cyber physical system applications, such as mobile intelligent video surveillance and smart homes. Resource management plays a key role in maximizing resource utilization and application performance in mobile ad hoc clouds. Unlike resource management in traditional distributed computing systems, such as clouds, resource management in a mobile ad hoc cloud poses numerous challenges owing to the node mobility, limited battery power, high latency, and dynamic network environment. The real-time requirements associated with mobile cyber physical system applications make the problem even more challenging. Currently existing resource management systems for mobile ad hoc clouds are not designed to support mobile cyber physical system applications and energy-efficient communication between application tasks. In this paper, we propose a new energy-efficient resource management system for mobile ad hoc clouds. The proposed system consists of two layers: a network layer and a middleware layer. The network layer provides ad hoc network and communication services to the middleware layer and shares the collected information in order to allow efficient and robust resource management decisions. It uses: (1) a transmission power control mechanism to improve energy efficiency and network capacity; (2) link lifetimes to reduce communication and energy consumption costs; and (3) link quality to estimate data transfer times. The middleware layer is responsible for discovery, monitoring, migration, and the allocation of resources. It receives application tasks from users and allocates tasks to nodes on the basis of networkand node-level information."
pub.1122152005,Major Challenges of Systems-of-Systems with Cloud and DevOps – A Financial Experience Report,"The term Systems-of-Systems (SoS) refers to a complex system that comprises other systems (the constituent systems), which have operational and managerial independence, geographical distribution, emergent behavior, and evolutionary development processes. DevSecOps (or SecDevOps) offers an approach to guide the implementation of IT Processes, which in turn may support the integration of a cloud environment to a systems-of-systems environment, incorporating information security practices, as well as fostering collaboration between both development and operation teams. It also aims to promote automation of IT processes so that the development of applications and / or services is fast and secure. However, there is a lack of detail in the process definitions to guide the implementation and use of DevSecOps in a Systems-of-Systems environment, especially when it is intended to merge cloud computing into pre-existing conventional infrastructures. In this context, this paper aims at describe the main actions, concerns and lessons learned, during planning and implementation phases, about IT Processes and IT Governance Model to transform an IT traditional environment into Systems-of-Systems environment, considering DevSecOps standards in a large Brazilian financial institution. It will show how IT Processes and IT Governance Model should be changed for incorporating a Cloud environment to a SoS. For doing so, we proposed the use of DevOps techniques as a means to reduce development time without to affect the quality and information security."
pub.1002088397,An integrated architectural framework for geoprocessing in cloud environment,"Spatial data infrastructure had ensured the availability of geospatial data over the local and global boundaries to be used in more flexible and efficient manner. With the enhancement in web service technologies the need of geoprocessing the geospatial data from various heterogeneous sources over the web, to produce valuable information is of great interest among research community. Towards this implementation of geoprocessing web, open geospatial consortium (OGC) has provided several web processing service standards for handling the geoprocess over the web. Most of the current existing geoprocessing web platforms inherit the capabilities of these web processing service standards and protocols define by OGC. However there exist several challenges that includes, growing intensity of data, high performance computing, real time data processing, and to provide robust, reliable, and cost effective geoprocess services to the end user. The future of geo-processing lies in the adoption of cloud computing to overcome the existing challenges. Characteristics of cloud computing including scalability, elasticity, pay-per-use, and self provisioning, offers a more reliable, on demand, and cost effective geoprocessing services to its end user. In this context the study makes an attempt to analyze the current state of geoprocessing services over cloud, with exiting GIS Cloud platform solutions in the literature to counter the challenges of GIS cloud. Furthermore, study proposes a design and architectural framework for geoprocessing over Cloud platform, based on OGC standards. The architecture includes developing geoprocessing service, and data management services as an essential part while storing/processing the geospatial data from heterogeneous sources. The architecture model including data management and geoprocessing can be bind together in a cloud platform to have an on demand service as per the requirement of end user."
pub.1119458478,An Energy-Efficient Resource Management System for a Mobile Ad Hoc Cloud,"Recently, mobile ad hoc clouds have emerged as a promising technology for
mobile cyber-physical system applications, such as mobile intelligent video
surveillance and smart homes. Resource management plays a key role in
maximizing resource utilization and application performance in mobile ad hoc
clouds. Unlike resource management in traditional distributed computing
systems, such as clouds, resource management in a mobile ad hoc cloud poses
numerous challenges owing to the node mobility, limited battery power, high
latency, and the dynamic network environment. The real-time requirements
associated with mobile cyber-physical system applications make the problem even
more challenging. Currently, existing resource management systems for mobile ad
hoc clouds are not designed to support mobile cyber-physical system
applications and energy-efficient communication between application tasks. In
this paper, we propose a new energy-efficient resource management system for
mobile ad hoc clouds. The proposed system consists of two layers: a network
layer and a middleware layer. The network layer provides ad hoc network and
communication services to the middleware layer and shares the collected
information in order to allow efficient and robust resource management
decisions. It uses (1) a transmission power control mechanism to improve energy
efficiency and network capacity, (2) link lifetimes to reduce communication and
energy consumption costs, and (3) link quality to estimate data transfer times.
The middleware layer is responsible for the discovery, monitoring, migration,
and allocation of resources. It receives application tasks from users and
allocates tasks to nodes on the basis of network and node-level information."
pub.1181865681,Authentication and identity management based on zero trust security model in micro-cloud environment,"The abilities of traditional perimeter-based security architectures are
rapidly decreasing as more enterprise assets are moved toward the cloud
environment. From a security viewpoint, the Zero Trust framework can better
track and block external attackers while limiting security breaches resulting
from insider attacks in the cloud paradigm. Furthermore, Zero Trust can better
accomplish access privileges for users and devices across cloud environments to
enable the secure sharing of resources. Moreover, the concept of zero trust
architecture in cloud computing requires the integration of complex practices
on multiple layers of system architecture, as well as a combination of a
variety of existing technologies. This paper focuses on authentication
mechanisms, calculation of trust score, and generation of policies in order to
establish required access control to resources. The main objective is to
incorporate an unbiased trust score as a part of policy expressions while
preserving the configurability and adaptiveness of parameters of interest.
Finally, the proof-of-concept is demonstrated on a micro-cloud plat-form
solution."
pub.1095575933,Flexcloud: A Flexible and Extensible Simulator for Performance Evaluation of Virtual Machine Allocation,"Cloud Data centers aim to provide reliable, sustainable and scalable services for all kinds of applications. Resource scheduling is one of keys to cloud services. To model and evaluate different scheduling policies and algorithms, we propose FlexCloud, a flexible and scalable simulator that enables users to simulate the process of initializing cloud data centers, allocating virtual machine requests and providing performance evaluation for various scheduling algorithms. FlexCloud can be run on a single computer with JVM to simulate large scale cloud environments with focus on infrastructure as a service; adopts agile design patterns to assure the flexibility and extensibility; models virtual machine migrations which is lack in the existing tools; provides user-friendly interfaces for customized configurations and replaying. Comparing to existing simulators, FlexCloud has combined features for supporting public cloud providers, load-balance and energy-efficiency scheduling. FlexCloud has advantage in computing time and memory consumption to support large-scale simulations. The detailed design of FlexCloud is introduced and performance evaluation is provided."
pub.1096145516,Latency-based benchmarking of cloud service providers,"With the ever-increasing trend of migration of applications to the Cloud environment, there is a growing need to thoroughly evaluate quality of the Cloud service itself, before deciding upon a hosting provider. Benchmarking the Cloud services is difficult though, due to the complex nature of the Cloud Computing setup and the diversity of locations, of applications and of their specific service requirements. However, such comparison may be crucial for decision making and for troubleshooting of services offered by the intermediate businesses - the so-called Cloud tenants. Existing cross-sectional studies and benchmarking methodologies provide only a shallow comparison of Cloud services, whereas state-of-the-art tooling for specific comparisons of application-performance parameters, such as for example latency, is insufficient. In this work, we propose a novel methodology for benchmarking of Cloud-service providers, which is based on latency measurements collected via active probing, and can be tailored to specific application needs. Furthermore, we demonstrate its applicability on a practical longitudinal study of real measurements of two major Cloud-service providers - Amazon and Microsoft."
pub.1100380088,A Portable Load Balancer for Kubernetes Cluster,"Linux containers have become very popular these days due to their lightweight nature and portability. Numerous web services are now deployed as clusters of containers. Kubernetes is a popular container management system that enables users to deploy such web services easily, and hence, it facilitates web service migration to the other side of the world. However, since Kubernetes relies on external load balancers provided by cloud providers, it is difficult to use in environments where there are no supported load balancers. This is particularly true for on-premise data centers, or for all but the largest cloud providers. In this paper, we proposed a portable load balancer that was usable in any environment, and hence facilitated web services migration. We implemented a containerized software load balancer that is run by Kubernetes as a part of container cluster, using Linux kernel's Internet Protocol Virtual Server(IPVS). Then we compared the performance of our proposed load balancer with existing iptables Destination Network Address Translation (DNAT) and the Nginx load balancers. During our experiments, we also clarified the importance of two network conditions to derive the best performance: the first was the choice of the overlay network operation mode, and the second was distributing packet processing to multiple cores. The results indicated that our proposed IPVS load balancer improved portability of web services without sacrificing the performance."
pub.1163402862,The Application of Multi-integration Teaching Mode Based on Cloud Computing in Computer Professional Teaching,"With the development of Internet + and cloud wisdom, the teaching mode of universities has changed greatly. Among them, online and offline hybrid teaching method is widely used in college teaching. Through analyzing the current teaching methods, the application of multi-integration teaching mode based on cloud computing in Computer Professional Teaching is put forward. Under the condition of existing teaching, using cloud computing technology, combining with the characteristics of cloud service platform, and constructing a learning environment. From the three main links of project driven theoretical teaching, cascading experimental teaching, and independent content expansion teaching, a cloud computing diversity auxiliary teaching platform of “Principles of Computer Organization” is built, to improve students’ interest in learning initiative, and to cultivate students’ logical analysis and reasoning ability. Through analysis and verification, this teaching mode integrates the concept and advantages of MOOC, effectively changes the traditional teaching mode of pure classroom teaching and the phenomenon of students’ cramming in listening, makes up for the shortcomings of the online and offline hybrid teaching, and improves the training mode in the computer professional education system, and achieves efficient teaching effect and teaching quality."
pub.1093868515,Service-Centric Business Model in Cloud Environment,"The development of Cloud Computing drives the change of value creation towards services and creates a new networked economic structure for challenging organizations to adapt their existing business models. Despite agreement on that the transformation of business model is importance to an organization's success, the evolution of business model enabled by Cloud is still fuzzy and vague within many research fields including both Management Strategy Theory and Information Systems. This paper, following a review of existing works and a series of case studies from several typical industries, employs a deductive reasoning method to investigate the influence of cloud computing on business model evolution and presents a specific service-centric business model. First, the evolution of business model in cloud environment is developed. Second, by analyzing business model innovations on business model nine building blocks in cloud environment, the element of service-centric business model is defined. Then, an ontological framework which includes value, service, and business model dimension for service-centric innovative business model is proposed. Finally, several industrial cases validate that this paper provides a comprehensive framework of organization's business model which can properly reveals an organization's complex nature of business logic and operation management in cloud environment."
pub.1109820751,ANALYSIS OF CLOUD COMPUTING SECURITY IN PERSPECTIVE OF SAUDI ARABIA,"The cloud computing technology provides computing resources as services over the internet. Efficiency and cost-effectiveness are the main drivers for cloud computing adoption since it promises better scalability over legacy enterprise systems. With all benefits found in cloud technology, there are still some security issues because information and system components are completely controlled by an external company. Most of the discussions on cloud computing security topic are mainly focused on the organizational means to overcome these issues. This paper focusses on the main obstacles to adopting cloud computing technology in Saudi Arabia. It will also cover the technical means to secure cloud computing environment along with real cloud hacking scenarios."
pub.1061541763,Transformation-Based Monetary CostOptimizations for Workflows in the Cloud,"Recently, performance and monetary cost optimizations for workflows from various applications in the cloud have become a hot research topic. However, we find that most existing studies adopt ad hoc optimization strategies, which fail to capture the key optimization opportunities for different workloads and cloud offerings (e.g., virtual machines with different prices). This paper proposes ToF, a general transformation-based optimization framework for workflows in the cloud. Specifically, ToF formulates six basic workflow transformation operations. An arbitrary performance and cost optimization process can be represented as a transformation plan (i.e., a sequence of basic transformation operations). All transformations form a huge optimization space. We further develop a cost model guided planner to efficiently find the optimized transformation for a predefined goal (e.g., minimizing the monetary cost with a given performance requirement). We develop ToF on real cloud environments including Amazon EC2 and Rackspace. Our experimental results demonstrate the effectiveness of ToF in optimizing the performance and cost in comparison with other existing approaches."
pub.1037627805,GEMBus Based Services Composition Platform for Cloud PaaS,"Cloud Platform as a Service (PaaS) provides an environment for creating and deploying applications using one of popular development platforms. This paper presents a practical solution for building a service composition platform based on the GEMBus (GEANT Multi-domain Bus) that extends the industry accepted Enterprise Service Bus (ESB) platform with automated services composition functionality and core services to support federated network access to distributed applications and resources, primarily targeted for GEANT research and academic community. The ESB is widely used as a platform for SOA and Web Services based integrated enterprise solutions. However in existing practices ESB design is still based on manual development, configuration and integration. GEMBus with its extended functionality and orientation on distributed resources integration can be considered as a logical choice for creating cloud PaaS services composition and provisioning platform. The paper describes Composable Services Architecture that creates a basis for automated services composition and lifecycle management and explains how this can be implemented with GEMBus. The paper describes the combined GEMBus/ESB testbed and provides an example of the simple services composition."
pub.1110977697,An Implementation of Harmonizing Internet of Things (IoT) in Cloud,"With the evolution of Internet of Things (IoT), everything is going to be connected to the Internet and the data produced by IoT, will be used for different purposes. Since IoT generates huge amount of data, we need some scalable storage to store and compute the data sensed from the sensors. To overcome this issue, we need the integration of cloud and IoT so that the data might be stored and computed in a scalable environment. Harmonization of IoT in Cloud might be a novel solution in this regard. IoT devices will interact with each other using Constrained Application Protocol (CoAP). All the IoT devices will be assigned IP addresses for unique identification. In this paper, we have implemented harmonizing IoT in Cloud. We have used CoAP to get things connected to each other through the Internet. For the implementation we have used two sensors, fire detector and the sensor attached with the door which is responsible for opening the door. Thus the proposed implementation will be storing and retrieving the sensed data from the cloud. We have also compared our implementation with different parameters. The comparison shows that our implementation significantly improves the performance compared to the existing system."
pub.1175172767,One System Call Hook to Rule All TEE OSes in the Cloud,"Confidential computing has revolutionized the way of in-use data protection in the Cloud, using the concept of Trusted Execution Environments (TEEs). Emerging from this paradigm are TEE OSes. They are extensively deployed in production settings, providing isolation protection and allowing legacy code to execute with minimal changes. However, they encounter challenges in cloud environments, particularly in creating compatibility layers, ensuring runtime protection, and efficiently managing TEE boundary transitions. In response, our work proposes to extend TEE OSes through a unified approach centered on system call (syscall) rewriting and interposition. We present xpoline++ - a stepwise (++) binary rewriting strategy executed on-the-fly with its trampoline set up at a manageable address (x), This allows for efficient construction of a compatibility layer at the binary syscall level and seamless transition to custom hook functions. Further, we introduce two syscall interposition extensions, namely xfilter and xswitchless, which respectively reduce the attack surface and improve the efficiency of TEE boundary switching to better serve the needs of cloud applications. Evaluations on a set of real-world workloads confirmed their effectiveness."
pub.1111513824,On using collaborative economy for test cost reduction in high fragmented environments,"The grown adoption of mobile devices makes the development of applications a very attractive market. On top of it, run tests is a crucial activity and a big challenge due to the high fragmentation on Android ecosystem. In this paper, we discuss how a new platform based on Collaborative Economy could be used to create a new alternative to software testing. We present an analysis of using this platform and we confirm its advantages over existing cloud solutions, from a scalability and cost viewpoints. Our solution can provide an average cost reduction upper to 85% and a potential increasing in scalability."
pub.1094875482,Service Usage Metering in Hybrid Cloud Environments,"With the proliferation of cloud based services-IaaS, PaaS, and SaaS-enterprises are increasingly consuming all type of IT services delivered by cloud providers. These cloud services are increasingly being used to create integrated solutions where some of the component services are delivered from on-premise private cloud environments and the remaining are provided by off-premise providers. While this best-of-breed approach results in flexible and agile business solutions, it also raises significant problems related to metering, billing, charge back and accounting. In this paper, after reviewing common hybrid cloud integration patterns, we discuss the importance of usage metering and the associated challenges in hybrid cloud environments. We then present a novel solution for metering of services delivered from multiple cloud providers. In this approach, service metering is keyed off of the life-cycle events generated by the service management controls across the hybrid cloud. By identifying life-cycle events associated with services consumed, service usage is tracked from all sources and then filtered and aggregated in a centralized manner. The aggregated information can then be used in an on-line manner for policy-based service delivery, charge-back, billing and reporting, and auditing, Based on this approach we have developed an end-to-end service usage metering and billing system for hybrid cloud environments. Some of the key underlying technologies have been incorporated in IBM Smart Cloud Orchestrator-an IBM offering for managing workload patterns in multi cloud environments."
pub.1093740572,EnaCloud: An Energy-saving Application Live Placement Approach for Cloud Computing Environments,"With the increasing prevalence of large scale cloud computing environments, how to place requested applications into available computing servers regarding to energy consumption has become an essential research problem, but existing application placement approaches are still not effective for live applications with dynamic characters. In this paper, we proposed a novel approach named EnaCloud, which enables application live placement dynamically with consideration of energy efficiency in a cloud platform. In EnaCloud, we use a Virtual Machine to encapsulate the application, which supports applications scheduling and live migration to minimize the number of running machines, so as to save energy. Specially, the application placement is abstracted as a bin packing problem, and an energy-aware heuristic algorithm is proposed to get an appropriate solution. In addition, an over-provision approach is presented to deal with the varying resource demands of applications. Our approach has been successfully implemented as useful components and fundamental services in the iVIC platform. Finally, we evaluate our approach by comprehensive experiments based on virtual machine monitor Xen and the results show that it is feasible."
pub.1042549610,Development of template management technology for easy deployment of virtual resources on OpenStack,"In this paper, we describe the development of template management technology to build virtual resources environments on OpenStack. In recent days, Cloud computing has been progressed and also open source Cloud software has become widespread. Authors are developing cloud services using OpenStack. There are technologies which deploy a set of virtual resources based on system environmental templates to enable easy building, expansion or migration of cloud resources. OpenStack Heat and Amazon CloudFormation are template deployment technologies and build stacks which are sets of virtual resources based on templates. However, these existing technologies have 4 problems. Heat and CloudFormation transaction managements of stack create or update are insufficient. Heat and CloudFormation do not have sharing mechanism of templates. Heat cannot extract templates from existing virtual environments. Heat does not reflect actual environment changes to stack information. Therefore, we propose a new template management technology with 4 improvements. It has a mechanism of transaction management like roll back or roll forward in case of abnormal failure during stack operations. It shares templates among end users and System Integrators. It extracts templates from existing environments. It reflects actual environment changes to stack information. We implemented the proposed template management server and showed that end users can easily replicate or build virtual resources environments. Furthermore, we measured the performance of template extraction, stack create and update and showed our method could process templates in a sufficient short time."
pub.1138604099,Nature‐inspired resource management and dynamic rescheduling of microservices in Cloud datacenters,"Abstract Distributed Cloud environments are now resorting to Cloud applications composed of heterogeneous microservices. Cloud service providers strive to provide high quality of service (QoS) and response time is one of the key QoS attributes for microservices. The dynamism of microservice ecosystems necessitates runtime adaptations and microservices rescheduling to avoid performance degradation. Existing works target rescheduling in hypervisor‐based systems, while ignoring the influence of configuration parameters of container‐based microservices. In an effort to address these challenges, this article describes a novel microservice rescheduling framework, throttling and interaction‐aware anticorrelated rescheduling for microservices, to proactively perform rescheduling activities whilst ensuring timely service responses. Based on periodic monitoring of the performance attributes, the framework schedules container migrations. Considering the exponentially large solution space, a metaheuristic approach based on multiverse optimization is developed to generate the near‐optimal mapping of microservices to the datacenter resources. Experimental results indicate that our framework provides superior performance with a reduction of up to 13.97% in the average response time, when compared with systems with no support for rescheduling."
pub.1153794558,Research on PCEP Extension for VLAN-based Traffic Forwarding in cloud network integration,"With the development of SDN technology, PCEP has become a path computing protocol widely used in the existing network. It takes the controller as the core of the overall architecture, and can be applied to calculate constrained paths in cross layer and cross domain environments in complex networks. According to the discussion on the application of PCEP in cloud-network integration scenarios, this paper proposes a VLAN-based traffic assurance mechanism based on the extension of PCEP. It can meet the requirements of key service assurance in the native IP environment and establish a connection oriented network tunnel. Under the VLAN-based architecture based on PCEP, the operator can perform closed-loop automatic control of the network and the mechanism helps to improve the intelligent scheduling of the network and the ability of real-time perception which meets the maintenance and operation requirements like flexible architecture, comprehensive opening of capabilities and global scheduling of resources."
pub.1176104166,Exploring the Application of Artificial Neural Networks in Enhancing Security Measures For Cloud Computing: A Survey,"Cloud computing has become integral to modern business operations, offering scalability, flexibility, and cost-effectiveness. However, the increasing adoption of cloud services also brings forth significant security challenges. In response, researchers and practitioners have explored various approaches to enhance security measures in cloud computing environments. Among these approaches, Artificial Neural Networks (ANNs) have emerged as promising tools for bolstering cloud security. This survey aims to explore the application of Artificial Neural Networks in enhancing security measures for cloud computing. Through an extensive review of existing literature, this survey provides insights into the role of ANNs in addressing key security challenges faced by cloud service providers and users. The survey methodology involves the selection of relevant studies, data collection, and analysis to identify trends, challenges, and opportunities in the application of ANNs for cloud security. The findings of this survey offer valuable insights into the current state of research in this field, highlighting emerging trends, potential areas for further exploration, and practical implications for industry stakeholders. Ultimately, this survey contributes to the understanding of how Artificial Neural Networks can be leveraged to enhance security measures and mitigate risks in cloud computing environments, paving the way for more robust and resilient cloud security solutions"
pub.1133596583,Equipment Data Integration Architecture Based on Data Middle Platform,"This paper proposes a solution for real-time data integration of complex equipment in cloud computing environment, which involves data collection, preprocessing, integration, storage, intelligent analysis services and other life-cycle management. First, it analyzes the development status and serious challenges in the process of existing equipment data integration, puts forward the development direction that data comes from business and will also serve business, then analyzes the advantages of data middle platform in cloud computing data processing, the development prospect of edge computing integrated with intelligent processing, as well as the key technologies of combination of edge computing and data middle platform, finally according to equipment management. According to the application requirements and fusion characteristics of business and data fusion, the design scheme of data middle platform based on edge computing is proposed, and the equipment data integration architecture based on data middle platform is designed."
pub.1125095515,Integration of Logic Controller with IoT to Form a Manufacturing Edge Computing Environment: A Premise," Communication latency and large amount of sensing data hinder the application of IoT in smart manufacturing when cloud computing is in association with the IoT applications. Recent concept of edge computing try to resolve the problem, because the edge of the network can handle the data locally and respond timely. Edge computing enables IoT automation in the manufacturing control. However, in reality, there is still a gap to transform from traditional hierarchical ISA-95 control architecture to IoT automation. To reduce the gap problem, this research first introduces a 5-level architecture for smart manufacturing, which includes equipment layer, connection layer, conversion layer, cognition and knowledge layer, and application layer. This research develops a premise of Smart Control Box (SCB). Each SCB include a PLC, a Micro Control Unit (MCU), and a single board computer. By wiring IoT with MCU (Micro Control Unit) to the PLC and install the intelligence on the single board computer, we can realize intelligent manufacturing without changing the manufacturing operations in situ. However, SCB can provide further guidance and advices, because the data collected from the equipment layer is learned in the cognition and knowledge layer to assist the operations on equipment layer, e.g., preventive maintenance. Further, the application layer can supervise the control of the operations in the equipment layer when MCU can take place of PLC. The design premise of SCB in this research can help transform the manufacturing automation from Industry 3.0 to Industry 4.0. The intelligence can be installed in the single computer board as a function of edge computing while not changing the settings on the existing PLCs. However, functionalities of the edge computing for smart manufacturing should be recognized from the proposed 5-level architecture."
pub.1155155908,Web service adaptation: A decade’s overview,"With the exponential growth of communication and information technologies, adaptation has gained a significant attention as it becomes a key feature of service-based systems, allowing them to operate and evolve in highly dynamic and uncertain environments. Although several Web service standards and frameworks have been proposed and extended, existing solutions do not provide a suitable architecture, in which all aspects of monitoring and adaptation (e.g., proactive, cross-layer, and autonomic adaptation) can be expressed. In addition, the emergence of new computing environments to host and execute various types of services (Web/cloud services, big data-intensive services, mobile services, microservices, etc.) raises the need for more efficient monitoring and adaptation systems. This survey aims to bring a synthesis and a road-map to the adaptation of service-based systems. We also discuss adaptation solutions in emerging service models, such as cloud services and big services. Based on an adaptation taxonomy which we extracted from the surveyed approaches, and by identifying the main requirements and goals of service adaptation in Web, cloud and big data environments, detailed analysis and discussions, as well as the open issues, are provided."
pub.1020307361,SICE,"SICE is a novel framework to provide hardware-level isolation and protection for sensitive workloads running on x86 platforms in compute clouds. Unlike existing isolation techniques, SICE does not rely on any software component in the host environment (i.e., an OS or a hypervisor). Instead, the security of the isolated environments is guaranteed by a trusted computing base that only includes the hardware, the BIOS, and the System Management Mode (SMM). SICE provides fast context switching to and from an isolated environment, allowing isolated workloads to time-share the physical platform with untrusted workloads. Moreover, SICE supports a large range (up to 4GB) of isolated memory. Finally, the most unique feature of SICE is the use of multicore processors to allow the isolated environments to run concurrently and yet securely beside the untrusted host. We have implemented a SICE prototype using an AMD x86 hardware platform. Our experiments show that SICE performs fast context switching (67 microseconds) to and from the isolated environment and that it imposes a reasonable overhead (3% on all but one benchmark) on the operation of an isolated Linux virtual machine. Our prototype demonstrates that, subject to a careful security review of the BIOS software and the SMM hardware implementation, current hardware architecture already provides abstractions that can support building strong isolation mechanisms using a very small SMM software foundation of about 300 lines of code."
pub.1094580179,An Adjustable Risk Assessment Method for a Cloud System,"Although cloud computing technologies provide many advantages for organizations, security is still a barrier for wide-spread adoption to the public. Many cloud systems suffer from various attacks, including unauthorized data modification, denial of service, etc. The existing researches use risk assessments to evaluate the security of a cloud environment either from a CSp's viewpoint or from a user's point of view. The results of these works may not be precise enough, nor can they satisfy both CSp's and user's security requirements. We propose an Adjustable Cloud Risk Assessment systeM (ACRAM) for Cloud Service Providers (CSPs) and users to assess the cloud security. ACRAM consists of a risk assessment module running at two modes (Offline or Online mode) with the help of Security Service Level Agreement (SecSLA) signed by the CSP and the cloud user. The Offline mode is used for assessing the risk of a cloud based on the historical software vulnerabilities, while the Online mode is for assessing the risk of a cloud system at RUNTIME. To explain how ACRAM works for altering the potential threats in a cloud system, we conduct an experiment using different weights in Confidentiality (C), Integrity (I) and Availability (A). The results show that 1) CSP can protect future users from being co-located with a possible attacker; 2) CSP can take some risk mitigation to meet a user's requirements and keep the user from being attacked."
pub.1093205936,CloudSim4DWf: A CloudSim-Extension for Simulating Dynamic Workflows in a Cloud Environment,"The resources provisioning for workflow applications has become one of the most difficult challenges in the cloud. It consists in taking an appropriate decision when mapping tasks to resources while meeting QoS requirements. The need to modify workflows while they are being executed has become a major requirement to deal with unusual situations and evolution. Therefore, it is necessary to implement a provisioning policy which takes into account dynamic changes of workflow, while satisfying some performance criteria defined by the user. Simulation tools are an efficient solution to evaluate the performance of cloud applications. They offer a free environment that can mimic the behavior of a real cloud environment. Nevertheless, existing simulators are based on static application models. In this paper, we introduce CloudSim4DWf, which extends the existing CloudSim simulator with a new resource provisioning policy for dynamic workflows. The different experiments that we present show the efficiency of our tool."
pub.1165957568,Cloud Computing Migration: A Thoughtful Decision,"The reliance on cloud computing services and related technologies has increased due to their many benefits, including resource sharing and the need for organizations to maintain their competitive advantage in the digital market. Also, the pace of cloud growth has significantly increased because of COVID-19. Many companies and businesses have shifted their core processes to the cloud for seamless operations during the pandemic. This migration of legacy systems to the cloud creates many opportunities for businesses. However, on the other hand, it may also bring substantial challenges. Hence, a careful decision is needed to align with company goals. In this paper, we demonstrate the need and advantages of cloud computing, followed by highlighting AWS's characteristics as one of the leading cloud service providers. We also discuss the merger of AWS with SDN to improve cloud data security and performance. The outcomes of this study can provide a data migration strategy and guide decision-makers toward a secure and sustainable migration to a cloud computing environment."
pub.1144636381,Machine Learning-based Orchestration of Containers: A Taxonomy and Future Directions,"Containerization is a lightweight application virtualization technology, providing high environmental consistency, operating system distribution portability, and resource isolation. Existing mainstream cloud service providers have prevalently adopted container technologies in their distributed system infrastructures for automated application management. To handle the automation of deployment, maintenance, autoscaling, and networking of containerized applications, container orchestration is proposed as an essential research problem. However, the highly dynamic and diverse feature of cloud workloads and environments considerably raises the complexity of orchestration mechanisms. Machine learning algorithms are accordingly employed by container orchestration systems for behavior modeling and prediction of multi-dimensional performance metrics. Such insights could further improve the quality of resource provisioning decisions in response to the changing workloads under complex environments. In this article, we present a comprehensive literature review of existing machine learning-based container orchestration approaches. Detailed taxonomies are proposed to classify the current researches by their common features. Moreover, the evolution of machine learning-based container orchestration technologies from the year 2016 to 2021 has been designed based on objectives and metrics. A comparative analysis of the reviewed techniques is conducted according to the proposed taxonomies, with emphasis on their key characteristics. Finally, various open research challenges and potential future directions are highlighted."
pub.1139148287,Machine Learning-based Orchestration of Containers: A Taxonomy and Future Directions,"Containerization is a lightweight application virtualization technology,
providing high environmental consistency, operating system distribution
portability, and resource isolation. Existing mainstream cloud service
providers have prevalently adopted container technologies in their distributed
system infrastructures for automated application management. To handle the
automation of deployment, maintenance, autoscaling, and networking of
containerized applications, container orchestration is proposed as an essential
research problem. However, the highly dynamic and diverse feature of cloud
workloads and environments considerably raises the complexity of orchestration
mechanisms. Machine learning algorithms are accordingly employed by container
orchestration systems for behavior modelling and prediction of
multi-dimensional performance metrics. Such insights could further improve the
quality of resource provisioning decisions in response to the changing
workloads under complex environments. In this paper, we present a comprehensive
literature review of existing machine learning-based container orchestration
approaches. Detailed taxonomies are proposed to classify the current researches
by their common features. Moreover, the evolution of machine learning-based
container orchestration technologies from the year 2016 to 2021 has been
designed based on objectives and metrics. A comparative analysis of the
reviewed techniques is conducted according to the proposed taxonomies, with
emphasis on their key characteristics. Finally, various open research
challenges and potential future directions are highlighted."
pub.1170674023,Security System for Services Access in Smart City System Using Internet of Everything and Cloud,"With the advancement of technologies like cloud computing and Internet of things, there are enormous areas in which these can be utilized. One such domain is Smart City concept utilizing or based on the architecture of Internet of everything. In a smart city the distinguish concept is the utilization of internet of everything architecture to effectively manage the services provided to users. Moreover, integration of internet of everything with cloud computing architecture would provide a better ecosystem in the area of smart cities. Providing services to smart city users could pose certain challenges and security issues with the integration of cloud computing. As per previous studied literature there is scope for improving access and authentication-based security of services imparted to smart city users. In this work, initially the consolidation of internet of everything (IoE) and cloud computing in smart cities is put forward. Further, this work focusses upon issues and challenges encountered in leveraging IoE in smart city environments with analysis of existing techniques for authentication and access control of cloud-based services and later provides a proposed system focusing upon imparting access control and authentication-oriented security for services delivered over a cloud and IoE followed by its analysis."
pub.1093539286,Performance Analysis of Secure Unified Communications in the VMware-Based Cloud,"Unified communications as a service (UCaaS) can be regarded as a cost-effective model for on-demand delivery of unified communications services in the cloud. However, addressing security concerns has been seen as the biggest challenge to the adoption of IT services in the cloud. This study set up a cloud system via VMware suite to emulate hosting unified communications (UC), the integration of two or more real time communication systems, services in the cloud in a laboratory environment. An Internet Protocol Security (IPSec) gateway was also set up to support network-level security for UCaaS against possible security exposures. This study was aimed at analysis of an implementation of UCaaS over IPSec and evaluation of the latency of encrypted UC traffic while protecting that traffic. Our test results show no latency while IPSec is implemented with a G.711 audio codec. However, the performance of the G.722 audio codec with an IPSec implementation affects the overall performance of the UC server. These results give technical advice and guidance to those involved in security controls in UC security on premises as well as in the cloud."
pub.1095140439,A Framework for Nonrepudiatable and Scalable Cross-Enterprise Workflow Management Systems in the Cloud,"Cloud computing is gaining tremendous momentum in both academia and industry, with more and more people and enterprises migrating their data and applications into the cloud. Cloud computing provides a new computing model with elastic scaling, a resource pool of unprecedented size, and the on-demand resource provisioning mechanism, which bring numerous challenges in implementing workflow management systems (WfMSs) in the cloud. Establishing scalable and crossenterprise WfMSs in the cloud requires the adaptation and extension of existing concepts for process management. This paper presents a framework for how cross-enterprise processes can be implemented, secured, controlled, and scaled. We also explain why existing engine-based centralized and distributed WfMSs cannot guarantee the nonrepudiation requirement. The proposed framework is a document-routing system that implements major required security features such as authentication, confidentiality, data integrity, and nonrepudiation in the cloud computing environment. Its security framework is built by applying element-wise encryption and a cascade-based method of embedding digital signatures. The implementation and experimental results demonstrate the feasibility of the proposed framework."
pub.1149285559,Enhancing Response Time of Cloud Resources Through Energy Efficient Cloud Scheduling Algorithm,"Cloud Computing is becoming a dominant trend in providing information technology (IT) services. The cloud comprises many hardware and software resources today, and more people are switching to such services. Users' requests for cloud resources must incur a minimum amount of load on the system while getting a rapid response. In the cloud today, there is too much computational power. Load balancing makes it possible for various components of the cloud computing environment to work efficiently. To balance client requests to available resources so that the system is not overloaded, and the requested resources are delivered as quickly as possible, an effective load balancing strategy is essential. In this research article, we have presented a critical analysis of various existing cloud load balancing and scheduling algorithms. Several task scheduling approaches have been proposed in the literature review, but there appears to be a lack of scheduling algorithms for real-time task works based on historical scheduling records (HSR). The proposed algorithm uses information available in HSR to efficiently distributes incoming user requests to available virtual machines. The proposed scheduling algorithm uses the scaleup and scale down resource algorithm which helps in achieving maximum resource utilization. The algorithm tries to balance the load on VMs by scaling up and down cloud resources. WorkflowSim is used to analyze the performance of the algorithm proposed. The simulation results are compared with the existing scheduling algorithm which shows the proposed algorithm outperforms existing scheduling algorithms in terms of makespan."
pub.1104196160,Improving Energy Efficiency of Virtual Resource Allocation in Cloud Datacenter,"Objective: To minimize the energy frenzied around cloud datacenters. The proposed system provides a dynamic and continuous load prediction along with existing heuristic load prediction to improve the adaptation performance for green computing through Markov chaining of resources. Methodology: The trouble of energy competence through resource management for a large-scale cloud environment to facilitate hosts sites is resolute here. In apply there will be number of underutilized servers in the datacenter which guzzle bulky part of established energy. To minimize the energy frenzied around datacenters the existing system outlines a distributed middleware architecture and presenting one of its key elements, a gossip protocol, Skewness algorithm and load balancing that meets our design goals: justice of resource allocation with respect to hosted sites through overload escaping and efficient edition to load changes and scalability in terms of together the numeral of machines and sites thereby turning off the unused servers for green computing through Markov Chaining Model. This model is scalable enough to signify systems composed of thousands of resources and it makes potential to represent both physical and virtual wealth exploiting cloud explicit concepts such as the infrastructure elasticity. Results: To certify the model, simulation is conducted within the Network Simulator (NS) 2.28 have platform with GCC 4.3 and Fedora 13. The load prediction algorithm and Markov chain model achieves both overload avoidance and green computing for systems with multiresource limitations. Application: Green Wireless Network: To improve the energy efficient, data retrieval and resource service based on coaching, computing and networking in the area of Green Wireless. Green Big Data: To reduce the power consumption, big data merge the concept with green computing. Green job Scheduling: Each Server has different jobs, to save the energy server merged with green computing. Green Cloud data center: To use the green computing in cloud data center to increases the energy efficiency. Keywords: Green Computing, Load Prediction, Markov Chaining, Virtualization"
pub.1130237707,Trusted Virtual Machine Model Based on High-Performance Cipher Coprocessor,"In the cloud computing environment, with the complex network environment, the virtualization platform faces many security problems. At the same time, trusted computing can greatly enhance the architecture security of virtualization platform systems, but there are many problems when trusted computing is deployed directly in the cloud environment. Therefore, this paper proposes a trusted virtual machine model based on high-performance cipher coprocessor to solve the security problems such as the isolation and insufficient performance of virtual TPM (vTPM) on the existing virtual platform. In this model, virtio technology was used to realize the virtualization of TPM, and a management architecture was designed to manage the life cycle of vTPM. The analysis shows that the model can complete the isolation of vTPM, and protect the security of vTPM during the migration process through the migration control server, and can strengthen the security of the virtualization platform. Finally, the simulation results show that the model is more feasible and suitable for cloud platform than hardware TPM."
pub.1154981071,Experiments and Evaluation of a Container Migration Data-Auditing System on Edge Computing Environment,"With the proliferation of IoT sensors and devices, storing collected data in the cloud has become common. A wide variety of data with different purposes and forms are not directly stored in the cloud but are sent to the cloud via edge servers. At the edge server, applications are running in containers and virtual machines to collect data. However, the current deployment and movement mechanisms for containers and virtual machines do not consider any conventions or regulations for the applications and the data it contains. Therefore, it is easy to deploy and migrate containers and virtual machines. However, the problem arises when it is deployed or migrated, which may violate the licensing terms of the contained applications, the rules of the organization, or the laws and regulations of the concerned country. We have already proposed a data-audit control mechanism for the migration of virtual machines. The proposed mechanism successfully controls the unintentional and malicious migration of virtual machines. We expect similar problems with containers to occur as the number of edge servers increases. Therefore, we propose a policy-based data-audit control system for container migration. The proposed system was verified in the implemented edge computing environment and the results showed that adding the proposed data-audit control mechanism had a minimal impact on migration time and that the system was practical enough. In the future, we intend to conduct verification not in a very compact and short-range environment such as this one but on an existing wide-area network."
pub.1022328484,Integration of Internet of Everything (IoE) with Cloud,"This chapter presents a roadmap of key developments in IoT-Cloud research in the context of different application domains and its applicability to IoE. IoT can be extended to IoE, if it is possible to connect everything to the Internet. Different layers of IoT protocol stack are discussed in this chapter. In IoT, physical objects connected to the Internet generate huge amount of data. Here, one of the main challenges is to move these data from the underlying IoT to the cloud. Cloud technologies are appealing due to the fact that the requirements for developing such IoE environment match very closely what cloud can offer in terms of computational and storage resources. Various application areas and existing works are also discussed."
pub.1101517635,Secure and efficient data forwarding in untrusted cloud environment,"Nowadays, cloud storage services increased the popular for data storage in the cloud and retrieve from any location without any time limitations. In recent days one of the most important demands required in cloud is secured data transmission in un-trusted cloud applications. Due to user’s data security, the encrypted data is stored in cloud server to protect from unauthorized users. Existing methods offer either data transformation efficiency or security. They fail to maintain end to end security during massive transformations. However, existing methods are not capable of solving the key complexity and avoiding key secrecy disclosure. The main objective of this study is to design and develop a secured efficient data forwarding algorithm for increasing the security level. It is developed specially for untrusted cloud environment. In order to provide a better solution, an efficient framework is proposed for forwarding and retrieving the content in the untrusted cloud environment. Proposed system implements dual privacy for reliable data transmission in an untrusted cloud environment. It develops efficient secret key exposure to minimize the key complexity during data transmission. SEDFA is used for one to many communications as a public key used for encryption and decryption. This scheme provides reliable data transmission between the data owner and end user in untrusted Cloud Environment. Proposed mechanisms minimized the data encryption time, decryption time and improved the communication cost. Based on experimental results, SEDFA reduces the communication cost 5%, encryption time (ET), 2 s, decryption time (DT) 0.5 s."
pub.1172915386,POSMind: developing a hierarchical GNSS/SINS post-processing service system for precise position and attitude determination,"The School of Geodesy and Geomatics at Wuhan University has developed a GNSS/SINS post-processing service system named POSMind. With the growing demand for mobile mapping in scientific and engineering applications, such as earth observation and high-precision mapping, there is a crucial need for efficient and accurate direct georeference based on GNSS/SINS integration. To accommodate diverse applications, an analysis of existing service forms was conducted, culminating the development of the hierarchical post-processing service system. This system consists of three service forms: module for interface calls, software for fine processing, and web for efficient cluster processing. POSMind has assimilated existing excellent methodologies and constructed a high-precision GNSS/SINS integration algorithm framework through theoretical derivations and experimental tests. Refinements have been introduced in several facets, including pre-processing, quality control, ambiguity resolution, and smoothing schemes. To assess the performance of POSMind, a series of experiments and analyses were conducted. The first experiment is conducted in open-sky environments (including carborne, airborne, and shipborne) to evaluate the consistency between POSMind and Inertial Explorer. Additionally, experiment under urban environments is carried out to assess the performance of POSMind in realistic cases. Moreover, the practical performance of POSMind was also demonstrated with two mobile mapping cases, with the evaluation of the accuracy of point cloud. Looking forward, we plan to enhance POSMind by introducing reliable filters or optimizers, integrating observations from other sensors and utilizing the benefits of post-processing in existing powerful GNSS/SINS processing platforms. The goal is to provide a powerful GNSS/SINS post-processing service that delivers high-precision, excellent availability, and utmost reliability for diverse scenes and applications. The POSMind web and software can be freely accessed at posmind-web.com and on Kaggle website at kaggle.com/datasets/fengzhusgg/smartpnt-pos."
pub.1132890124,Modular and distributed IDE,"Integrated Development Environments (IDEs) are indispensable companions to programming languages. They are increasingly turning towards Web-based infrastructure. The rise of a protocol such as the Language Server Protocol (LSP) that standardizes the separation between a language-agnostic IDE, and a language server that provides all language services (e.g., auto completion, compiler...) has allowed the emergence of high quality generic Web components to build the IDE part that runs in the browser. However, all language services require different computing capacities and response times to guarantee a user-friendly experience within the IDE. The monolithic distribution of all language services prevents to leverage on the available execution platforms (e.g., local platform, application server, cloud). In contrast with the current approaches that provide IDEs in the form of a monolithic client-server architecture, we explore in this paper the modularization of all language services to support their individual deployment and dynamic adaptation within an IDE. We evaluate the performance impact of the distribution of the language services across the available execution platforms on four EMF-based languages, and demonstrate the benefit of a custom distribution."
pub.1128583637,Readiness Exercises: Are Risk Assessment Methodologies Ready for the Cloud?,"Cloud computing is a type of service that allows the use of computing resources from a distance, rather than a new technology. Various services exist on-demand, ranging from data storage and processing to software as a service, like email and developing platforms. Cloud computing enables ubiquitous, on-demand access over the net to a shared pool of configurable resources, like servers, applications, etc. that can be accessed, altered or even restored rapidly with minimal service provider interaction or management effort. Still, due to the vast growth of cloud computing, new security issues have been introduced. Key factors are the loss of control over any outsourced resources and cloud’s computing inherent security vulnerabilities. Managing these risks requires the adoption of an effective risk management method, capable of involving both the Cloud customer and the Cloud Service Provider. Risk assessment methods are common tools amongst IT security consultants for managing the risk of entire companies. Still, traditional risk management methodologies are having trouble managing cloud services. Extending our previous work, the purpose of this paper is to compare and examine whether popular risk management methods and tools (e.g. NIST SP800, EBIOS, MEHARI, OCTAVE, IT-Grundschutz, MAGERIT, CRAMM, HTRA, Risk-Safe Assessment, CORAS) are suitable for cloud computing environments. Specifically, based upon existing literature, this paper points out the essential characteristics that any risk assessment method addressed to cloud computing should incorporate, and suggests three new ones that are more appropriate based on their features."
pub.1023462569,Integration of science gateways,"Science gateways are collaborative software environments designed to enable community-driven development and use of cyberinfrastructure services, software tools, applications, and data through common interfaces, typically an online portal, customized to meet the needs of individual communities [1]. By abstracting the assemblage of cyberinfrastructure needed by the research communities and democratizing access to high-end computational resources, science gateways (e.g. those on XSEDE) provide a shared problem-solving environment and promote collaborations among community users. The integration work presented here represents a cutting-edge approach to coupling two independent geospatial software environments developed separately, namely CyberGIS [2] and OpenTopography [3]. CyberGIS -- defined as cyberinfrastructure-based geographic information systems (GIS) -- represents a new generation of GIS based on seamless synthesis of cyberinfrastructure, geographic information science, and spatial analysis and modeling [4]. Funded by the National Science Foundation (NSF), the CyberGIS project (http://cybergis.org) advances the science of CyberGIS, with a particular focus on enabling the analysis of big spatial datasets, computationally intensive spatial analysis and modeling, and collaborative geospatial problem solving and decision making [2]. The CyberGIS Gateway provides an online, high-performance and collaborative geospatial problem-solving environment to allow for the contribution, sharing of and access to CyberGIS services and tools by a broad community of geospatial scientists and GIS users. As a NSF cyberinfrastructure-based data facility, OpenTopography (http://opentopography.org) provides its community with access to high-resolution Earth science oriented topography data, related tools and geoprocessing applications published as a suite of Web services. All Web services are built and deployed by leveraging the open source Opal toolkit (http://www2.nbcr.net/data/docs/opal/), which provides a mechanism to streamline the process of encapsulating existing scientific applications as Web services. CyberGIS and OpenTopography represent two software environments that complement each other; with CyberGIS providing access to cyberinfrastructure-based spatial data and related analytics and OpenTopography serving as a data source of LiDAR-based high-resolution terrain data. Hence, integrating these software environments will extend their capabilities, improve their usability and bring benefits to the science communities by enabling large-scale geospatial problem solving through shared interoperable analytical and data services. The goal of this integration is to make seamless access to OpenTopography data when an analysis is planned on the CyberGIS Gateway and vice versa. More importantly, this work demonstrates multiple innovative aspects of software integration research and development that could serve as a template for integrating other such independently"
pub.1093759118,"Workflow Scheduling Algorithms in Cloud Environment: a Review, Taxonomy, and Challenges","Cloud computing offers Information and Communication Technologies (ICT) user's new aspects of ease of services through the Internet. Cloud is a gradual evolution of distributed, cluster, grid and utility computing. Cloud environment attracts the myriads of service providers for development and deployment of their application because it provides ubiquitous and on-demand access to shared resources. The majority of cloud applications like scientific applications are represented as workflow. Workflow is related to the automation of a process that represented by task and data/control information is passed between dependent tasks based on defined set of principles, execution of these tasks require different resources with different Quality of Service parameters. Workflow scheduling is most important concerned in a cloud computing environment because it depends on optimization objectives (like makespan, cost, reliability, security, energy consumption and energy cost, load balancing etc.) of scientific applications. In this review paper, we describe various scientific applications, taxonomy and comprehensive survey of existing workflow scheduling algorithms. We also, classify these algorithms based on nature of the algorithm, optimization model used, optimization objectives and tool used."
pub.1154206802,Inferencing Big Data with Artificial Intelligence & Machine Learning Models in Metaverse,"This quantitaive study provides different methods of visualization for processing Big Data sets in augmented and virtual reality. The goal is to provide the detailed implementation of the statistical methods and modeling techniques i.e. using machine learning algorithms and artificial intelligence. The Statistical analysis is performed on the Big Data sets in Metaverse, that points towards real time inferencing infrastructure and techniques. Also, This paper elaborates the importance of pervasive and Ubiquitous Computing Architecture and Applications that enable High-end computing on Big Data sets. In this paper, we evaluate the need for a combination of cognitive mechanism and high-end infrastructure to address the performance, latency, and security issues when working on Big Data sets in environments like virtual or Augmented reality. Despite the modernization and advancement in technology there are increasing number of breaches in cloud and hybrid infrastructures, this justifies the need to meet the requirements of strengthening security, safeguard mechanisms and privacy. This paper discusses methods beyond basic visualizations through a deep dive exploration on improvising the applications of existing analytical methods, and use of advanced exploratory tools and visualization techniques on the next generation platforms."
pub.1181426191,Innovative Uses of OData Services in Modern SAP Solutions,"OData (Open Data Protocol) has emerged as a pivotal framework in the realm of modern SAP solutions, facilitating seamless data integration and interoperability across diverse platforms. This abstract explores the innovative applications of OData services within SAP environments, highlighting their transformative impact on enterprise data management. By leveraging OData, organizations can enhance data accessibility, allowing developers to create rich, interactive applications that draw data from multiple sources. The protocol supports RESTful architecture, making it easier to consume and manipulate data via standard HTTP methods. This adaptability enables businesses to implement real-time analytics, fostering agile decision-making and improving overall operational efficiency. Furthermore, the integration of OData with SAP Fiori applications empowers users with intuitive interfaces, streamlining workflows and enhancing user experience. As companies continue to embrace digital transformation, OData services offer a robust solution for connecting cloud and on-premise applications, enabling enterprises to harness the power of data-driven insights. This paper aims to showcase various innovative use cases of OData services in modern SAP solutions, including their role in developing APIs, enhancing mobile applications, and driving business intelligence initiatives. By examining these applications, this research underscores the significance of OData as a catalyst for innovation and efficiency in the evolving landscape of enterprise resource planning (ERP) systems. Through practical examples and case studies, we demonstrate how leveraging OData services can lead to a more responsive, integrated, and data-centric approach in SAP implementations."
pub.1170879414,Dynamic Construction Method of Container Clusters for Highly Mobile Environments,"In the context of the overall industry's digital transformation, Edge Computing technology continues to make significant advancements. In edge environments, there is a plethora of computing and processing devices used for constructing various types of information systems. However, these devices exhibit disparities in software and hardware resources, and their performance can vary greatly, with some being notably subpar. Terminal devices, in particular, face severe limitations in computing, storage, and network resources, which hinder the integration and deployment of functional services in various regions. To offer a greater variety of high-quality services to various endpoint devices, it is possible to construct clusters using the server resources available in edge environments to provide cloud services. Nevertheless, there is currently a lack of a dynamic construction method for container clusters under highly mobile conditions in edge environments. In response to the short-comings of existing technologies, this paper presents a dynamic construction method for container clusters tailored to highly mobile environments in edge computing. This method aims to address the challenges of organizing a self-adaptive construction environment for container clusters that can operate effectively under high-mobility conditions in edge application scenarios."
pub.1163677508,Survey on Secure Keyword Search over Outsourced Data: From Cloud to Blockchain-assisted Architecture,"Secure keyword search is a prevailing search service offered in outsourced environments. However, with the increasingly severe security vulnerabilities of conventional centralized outsourcing, the architecture of secure keyword search, with searchable encryption (SE) as the underlying technique, has recently shifted from cloud-centered models to blockchain-assisted models. Existing surveys commonly fail to capture such an evolution and the corresponding benefits. What on earth does blockchain bring about and what are the unexplored challenges? This survey provides a systematic review of secure keyword search over outsourced data from cloud to blockchain-assisted architectures. We propose a taxonomy assorting present studies, depending on whether cloud/blockchain and data sharing are included, in which blockchain-assisted architecture is further divided into blockchain-side and cloud-side keyword search, respectively. Technically, we conclude five types of representative SE techniques with fitting architectures, either cryptographic-based or hardware-dependent. Notably, we propose comprehensive methodologies to select relevant papers, discuss, and compare existing schemes regarding functionalities, security, efficiency, and fairness (up to 21 compared items). Finally, open issues and potential research directions are identified for future work. We aspire to help pave the way for addressing the theoretical and empirical aspects of secure keyword search and full-fledged real-world implementation of blockchain-based keyword search applications."
pub.1163825937,Implementing and Automating Security Scanning to a DevSecOps CI/CD Pipeline,"With the growing adoption of DevOps and the rise of containerization and Continuous Integration/ Continuous Deployment (CI/CD) in software development life cycle (SDLC) has brought significant changes to the industry. While these methods offer many advantages, they also present unique security challenges, as containerized applications are more susceptible to cyber attacks than traditional deployments, security has become a significant concern. Security scanning is an essential aspect of DevSecOps pipelines, involving the analysis of software images deployed to cloud environments to identify vulnerabilities and mitigate security threats. This study will involve a thorough review of existing literature on containerization and CI/CD security and will analyze current security practices and measures used in containerization-based CI/CD systems. Various tools and techniques have been proposed for implementing and automating image security scanning in DevSecOps pipelines by integrating DAST (Dynamic application security testing) and SAST (Static application security testing) vulnerability scanning.. This research proposes a method for implementing and automating image security scanning using the Snyk and StackHawk tool, which provides a dashboard for SAST and DAST separately for monitoring scanning results and automating vulnerability fixes. The proposed method can be integrated with GitHub, enabling automatic vulnerability scanning and fixing during the build process. The research evaluates the effectiveness of the proposed method by demonstrating the ability of the method to improve the security of DevSecOps pipelines. The findings suggest that the proposed method can enhance the overall security of the application by reducing the time to detect and fix vulnerabilities."
pub.1118418056,Using Cloud-Aware Provenance to Reproduce Scientific Workflow Execution on Cloud,"Provenance has been thought of a mechanism to verify a workflow and to
provide workflow reproducibility. This provenance of scientific workflows has
been effectively carried out in Grid based scientific workflow systems.
However, recent adoption of Cloud-based scientific workflows present an
opportunity to investigate the suitability of existing approaches or propose
new approaches to collect provenance information from the Cloud and to utilize
it for workflow repeatability in the Cloud infrastructure. This paper presents
a novel approach that can assist in mitigating this challenge. This approach
can collect Cloud infrastructure information from an outside Cloud client along
with workflow provenance and can establish a mapping between them. This mapping
is later used to re-provision resources on the Cloud for workflow execution.
The reproducibility of the workflow execution is performed by: (a) capturing
the Cloud infrastructure information (virtual machine configuration) along with
the workflow provenance, (b) re-provisioning the similar resources on the Cloud
and re-executing the workflow on them and (c) by comparing the outputs of
workflows. The evaluation of the prototype suggests that the proposed approach
is feasible and can be investigated further. Moreover, there is no reference
reproducibility model exists in literature that can provide guidelines to
achieve this goal in Cloud. This paper also attempts to present a model that is
used in the proposed design to achieve workflow reproducibility in the Cloud
environment."
pub.1095694959,"All Silicon Data Center, the Energy Perspectives","The energy consumption related to data centers is ever increasing, despite the significant efficiency improvements in the involved technologies and the economic crisis. Pervasive adoption of virtualization, decreasing of PUE as well as new features related to energy-efficiency are not able to stop this growth trend, mainly sustained from the explosion of cloud services in both the fixed and mobile ICT markets. One of the most important factors that contributes to the increase in the data center energy demand is the evolution of data utilization in cloud environments. Big data support is one of the main driver of this process, new data sources continuously produce huge amounts of mostly unstructured data, that should be stored in large centralized storage systems to be accessed by the cloud users and analyzed by the companies in real time. This has a disruptive effect for the energy consumption of the data center hosting the above storage resources, that's why the data need to be stored and analyzed in IOPS intensive style. Flash memory technologies can play a key role in such scenario, since they can reduce dramatically both the storage footprint and energy consumption and increase applications performances bringing appreciable economic benefits. This fundamental efficiency and performance improvement for storage architectures can't be supported by legacy technologies because they are not designed to use flash technologies but rather they are based on energy-hungry legacy mechanical disks. That is limit the energy efficiency and performance delivered by solid state disks (SSD) installed in legacy storage systems, opening the door to the development of new technologies that will be able to fully exploit the potential of flash disks. The aim of this work is presenting a brief survey about the potential of flash technologies for improving the energy efficiency of data center storage systems supporting the incoming Big Data services."
pub.1061786683,A Scalable Architecture for Automatic Service Composition,"This paper addresses automatic service composition (ASC) as a means to create new value-added services dynamically and automatically from existing services in service-oriented architecture and cloud computing environments. Manually composing services for relatively static applications has been successful, but automatically composing services requires advances in the semantics of processes and an architectural framework that can capture all stages of an application's lifecycle. A framework for ASC involves four stages: planning an execution workflow, discovering services from a registry, selecting the best candidate services, and executing the selected services. This four-stage architecture is the most widely used to describe ASC, but it is still abstract and incomplete in terms of scalable goal composition, property transformation for seamless automatic composition, and integration architecture. We present a workflow orchestration to enable nested multilevel composition for achieving scalability. We add to the four-stage composition framework a transformation method for abstract composition properties. A general model for the composition architecture is described herein and a complete and detailed composition framework is introduced using our model. Our ASC architecture achieves improved seamlessness and scalability in the integrated framework. The ASC architecture is analyzed and evaluated to show its efficacy."
pub.1154236328,The Internet of Things and Cyber-Physical Systems,"With the emergence of the Industry 4.0 concept, such as IoT, cloud computing, big data analytics, use of smart sensors, robotics, and more, various industries, such as organizations, are implementing innovative technologies in IoT for competitive advantage and future sustainable supply chain applications. IoT creates an intelligent environment that uses collected information to make critical infrastructure components more informed, interactive, and well-performing. With IoT applications, organizations can respond quickly to customer demand, increase productivity, enable stakeholders to make faster decisions in real time, and improve the production process. With the emergence of the Industry 4.0 concept, such as IoT, cloud computing, big data analytics, use of smart sensors, robotics, and more, various industries, such as organizations, are implementing innovative technologies in IoT for competitive advantage and future sustainable supply chain applications. IoT creates an intelligent environment that uses collected information to make critical infrastructure components more informed, interactive, and well-performing. In smart agriculture, the IoT application is implemented for professionals, farmers, consultants, and researchers, with equipment and sensors placed on the field, creating a large database over a virtual network platform. The smart factory has emerged as a connected, fully automated, flexible production system supported by robots to improve the existing production systems in the industrial sector. Smart factories will be the center of industry transformation with their potential. The Real-Time Global Positioning System using the Internet of Things has a significant impact on ensuring a safe and intelligent transportation process."
pub.1122171110,Cloud adoption for e-learning: Survey and future challenges,"The cloud computing platform has become prevalent today and is being utilized by many organizations in government, industry, and academia. For teaching and training, its virtual environment provides a cost-effective alternative to physical labs which may be rapidly configured to provide hands-on lab exercises, as well as to run scientific applications for e-learning. In this paper, we present a survey of cloud usage aimed at empowering students with practical skills in an educational environment. We present a taxonomy of cloud usage for e-learning, and also analyze existing major contributions relevant to cloud usage for e-learning. We also perform a comparative analysis of the frameworks and models found in the literature along with a comparative evaluation of the implementations carried out for e-learning. The future challenges and major issues being faced in adopting cloud technology for e-learning are also discussed along with recommendations for possible solutions."
pub.1112146198,Trust Evaluation in Cross-Cloud Federation,"Cross-Cloud Federation (CCF) is beneficial for heterogeneous Cloud Service Providers (CSPs) for leasing additional resources from each other. Despite the benefits of on-demand scalability and enhanced service footprints for better service quality, the adoption of CCF is however mainly hindered due to the lack of a comprehensive trust model. The basic aim of such a model should be to address the security and performance concerns of a home CSP on its foreign peers before placing its users’ data and applications in their premises. A transitivity of users’ trust on home CSP and home CSP's trust on its foreign CSPs marks the uniqueness of trust paradigm in CCF. Addressing the concerns of cloud-to-cloud trust paradigm is inevitable to achieve users’ trust in a federation. Various trust models have been proposed in literature for conventional and multi-cloud computing environments. They focus on user requirements but none on federation perspective. Their applicability to CCF for addressing the concerns of cloud-to-cloud trust paradigm requires further consideration. For this reason, we have first outlined the general characteristics of CCF as being dynamic, multi-level and heterogeneous. Afterwards, cloud-to-cloud trust paradigm is proposed based on a set of unique principles identified as (i) trust bi-directionality, (ii) trust composition, (iii) delegation control, and (iv) Resource awareness. An insightful review of Trust Management Systems (TMS) proposed in literature reveals their shortcomings in addressing the requirements of cloud-to-cloud trust paradigm. To overcome these shortcomings, we suggest that some challenges can be merely addressed by aligning the existing methods to the nature of CCF. The remaining challenges require entirely new mechanisms to be introduced. A demonstration of this concept is presented in the form of a requirement matrix suggesting how the characteristics and properties of both CCF and the TMS are influenced by each other. This requirement matrix reveals the potential avenues of research for a TMS aimed specifically for CCF."
pub.1004713500,Private Cloud with e-Learning for Resources Sharing in University Environment,"Most existing e-Learning platforms are unable to share learning resources between cloud platform and public network, and normally need additional cost to deploy the environment. This paper presents a new e-Learning model based on the virtual private network and private cloud, which could help the student to easily setup and configure his own e-Learning environment with less cost for efficiently sharing resources in universities. Firstly this paper describes the academic private cloud tool that offers a simple environment to experiment cloud computing concepts. Based on this private cloud, e-Learning system can be easily installed by using standard local computing resources, without the need of different hardware or external resources. Hence, this paper illustrates the framework of VPN and Private Cloud Integration, aiming at resource sharing in the university environment. The e-Learning platform is also scalable and capable to interconnect other multi-platform developed in different locations. This proposed framework solves the various challenges faced by e-Learning, and increases the availability, reliability and scalability of cloud based e-Learning systems."
pub.1124425632,CDFM-based Secure & Efficient Architecture for Data Management in Cloud Computing,"Cloud has become a prominent technology of data storage and computation. This allows user and institutions to depend on Cloud providers for storage and computing to offer system as a service. The growing significance of extensive acquisition and adoption of cloud computing have imposed security assurance on cloud. Security assurance provides integrity, confidentiality and reliability of data by performing computation and retrieving in compliance with the cloud providers. The storage, processing and managing of data for complex application becomes cumbersome and burdensome on cloud provider platforms. A management mechanism is required to manage the resources and reduce the complexity for maximizing performance with minimum human intervention. Even handling security threats occurring with varying workloads and failures in cloud environment is entailed. Motivated from this self management mechanism, we proposed a novel Cuckoo-based Data Fragmentation and Metadata (CDFM) secure and efficient approach. It manages the complex data of Translation and retrieves the translated data securely. In this work, cuckoo pools divide the data into different number of fragments and send it to different data pools. Data pools first encrypt the data fragment and assign color code to this encrypted fragment and then prepare indexing according to this color coding. The performance analysis, exhibit higher performance than existing approach for security and data fragmentation in cloud scenario."
pub.1092033328,Migrating SOA Applications to Cloud: A Systematic Mapping Study,"Cloud Computing has emerged as an economical option to use IT resources when needed without considerations about where they are allocated or how they are delivered. Cloud Computing expands the SOA capabilities by adding scalability, elasticity and other relevant quality attributes. In this context, many companies have started to migrate their SOA applications to Cloud environments without proper support. We conducted a systematic mapping study to gather the current knowledge about existing strategies for migrating SOA applications to cloud computing. 105 papers were identified and the results show that most of the approaches follow a semi-automated (conventional) strategy for migrating to the Cloud (93%) and that most of the reported works follow a hybrid deployment model (60%). We additionally identify several research gaps such as the need for more technology-independent solutions, a common definition for concepts and resources, tool support, and validation."
pub.1144943802,QoS-aware placement of microservices-based IoT applications in Fog computing environments,"The Fog computing paradigm, offering cloud-like services at the edge of the network, has become a feasible model to support computing and storage capabilities required by latency-sensitive and bandwidth-hungry Internet of Things (IoT) applications. As fog devices are distributed, heterogeneous and resource-constrained, efficient application scheduling mechanisms are required to harvest the full potential of such computing environments. Due to the rapid evolution in IoT ecosystems and also to better suit fog environment characteristics, IoT application development has moved from the monolithic architecture towards the microservices architecture that enhances scalability, maintainability and extensibility of the applications. This architecture improves the granularity of service decomposition, thus providing scope for improvement in QoS-aware placement policies. Existing application placement policies lack proper utilisation of these features of microservices architecture, thus failing to produce efficient placements. In this paper, we harvest the characteristics of microservice architecture to propose a scalable QoS-aware application scheduling policy for batch placement of microservices-based IoT applications within fog environments. Our proposed policy, QoS-aware Multi-objective Set-based Particle Swarm Optimisation (QMPSO), aims at maximising the satisfaction of multiple QoS parameters (makespan, budget and throughput) while focusing on the utilisation of limited fog resources. Besides, QMPSO adapts and improves the Set-based Comprehensive Learning Particle Swarm Optimisation (S-CLPSO) algorithm to achieve better convergence in the fog application placement problem. We evaluate our policy in a simulated fog environment. The results show that compared to the state-of-the-art solutions, our placement algorithm significantly improves QoS in terms of makespan satisfaction (up to 35% improvement) and budget satisfaction (up to 70% improvement) and ensures optimum usage of computing and network resources, thus providing a robust approach for QoS-aware placement of microservices-based heterogeneous applications within fog environments."
pub.1094162740,Service Storm: A Self-Service Telecommunication Service Delivery Platform with Platform-as-a-Service Technology,"To attract and serve an expanded customer base, telecommunication service providers need to provide more targeted, focused and personalized services, which brings challenges to both business model and technologies to development of value added services. In this paper, we introduce Service Storm, a novel self-service telecommunication Service Delivery Platform with Platform-as-a-Service technologies to support rapid and flexible construction and delivery of value added services in the cloud environment as an open developer community. Firstly, we analyze the characteristics of four categories of users including Individual User, Organization User, Telecommunication Operator, and Business Partner, and present the architecture of Service Storm to support new business model targeting at long tail applications as well as conventional business models. Then we highlight the key technologies including 1) Rapid assembly model and tools for codeless and off-premise integration of telecom services and application logic based on Web 2.0 technologies, 2) Cloud based resource isolation, management, capacity planning and scaling for dynamic topology deployment and elastic infrastructure support, and 3) Automatic deployment and monitoring in runtime. Finally we illustrate the sample of SMS and Web based mobile workflow for insurance order management to show the advantages of Service Storm. We demonstrate how these technologies and architectures enable the new business model for telecommunication operator and significantly enhance the diversity of value added services with lower cost and shorter time to market."
pub.1137100334,Towards Independent Run-Time Cloud Monitoring,"Cloud computing services are integral to the digital transformation. They deliver greater connectivity, tremendous savings, and lower total cost of ownership. Despite such benefits and benchmarking advances, costs are still quite unpredictable, performance is unclear, security is inconsistent, and there is minimal control over aspects like data and service locality. Estimating performance of cloud environments is very hard for cloud consumers. They would like to make informed decisions about which provider better suits their needs using specialized evaluation mechanisms. Providers have their own tools reporting specific metrics, but they are potentially biased and often incomparable across providers. Current benchmarking tools allow comparison but consumers need more flexibility to evaluate environments under actual operating conditions for specialized applications. Ours is early stage work and a step towards a monitoring solution that enables independent evaluation of clouds for very specific application needs. In this paper, we present our initial architecture of the Cloud Monitor that aims to integrate existing and new benchmarks in a flexible and extensible way. By way of a simplistic demonstrator, we illustrate the concept. We report some preliminary monitoring results after a brief time of monitoring and are able to observe unexpected anomalies. The results suggest an independent monitoring solution is a powerful enabler of next generation cloud computing, not only for the consumer but potentially the whole ecosystem."
pub.1151953206,Improving Context-Aware Synthesis and Placement of Object Instances,"A vast amount of research has been conducted about deep learning and its applications in Computer Vision (CV). However, the application to project an object instance onto a real image or video in a semantically coherent manner, such that the projected object is indistinguishable from a real object, is only in its infancy. In our research, we aim to evaluate a generative model which is able to generate and place an object instance onto an image in a semantically coherent manner using a where and a what module; both of these employ Generative Adversarial Networks (GANs). Furthermore, we improve the shape generation by adding a classifier before the training data is used. Finally, we intend to increase the training stability by using an alternative training methodology and adjusting the Jenson-Shannon divergence to the Wasserstein distance. The implication of this work is the improved stability of an existing generative model, which inserts instances onto an image. Furthermore, we were also able to improve its performance."
pub.1181782739,Performance Analysis of a Virtualized Cloud Data Center through Efficient VM Migrations,"Virtualized cloud data centers play a crucial role in modern computing infrastructure, facilitating dynamic resource allocation and scalability. Efficient management of virtual machines (VMs) within these environments is essential to optimize performance, computational cost, energy consumption, and resource utilization. This paper presents a comprehensive performance analysis of a virtualized cloud data center, focusing on minimizing the number of VM migrations through an innovative approach leveraging Integer Linear Programming (ILP) and a Greedy algorithm. The proposed methodology aims to address the challenge of excessive migrations, which can lead to performance degradation and resource inefficiencies. Through extensive experimentation and performance evaluation, we demonstrate the effectiveness of our approach in reducing migration overhead while maintaining system performance and availability. Our findings indicate significant improvements in migration efficiency and overall data center performance compared to existing strategies."
pub.1100790001,"NOR: Towards Non-intrusive, Real-Time and OS-agnostic Introspection for Virtual Machines in Cloud Environment","Cloud platforms of large enterprises are witnessing increasing adoption of the Virtual Machine Introspection (VMI) technology for building a wide range of VM monitoring applications including intrusion detection systems, virtual firewall, malware analysis, and live memory forensics. In our analysis and comparison of existing VMI systems, we found that most systems suffer one or more of the following problems: intrusiveness, time lag and OS-dependence, which are not well suited to clouds in practice. To address these problems, we present NOR, a non-intrusive, real-time and OS-agnostic introspection system for virtual machines in cloud environment. It employs event-driven monitoring and snapshot polling cooperatively to reconstruct the memory state of guest VMs. In our evaluation, we show NOR is capable of monitoring activities of guest VMs instantaneously with minor performance overhead. We also design some case studies to show that NOR is able to detect kernel rootkits and mitigate transient attacks for different Linux systems."
pub.1063164226,COMMA,"Multi-tier applications are widely deployed in today's virtualized cloud computing environments. At the same time, management operations in these virtualized environments, such as load balancing, hardware maintenance, workload consolidation, etc., often make use of live virtual machine (VM) migration to control the placement of VMs. Although existing solutions are able to migrate a single VM efficiently, little attention has been devoted to migrating related VMs in multi-tier applications. Ignoring the relatedness of VMs during migration can lead to serious application performance degradation. This paper formulates the multi-tier application migration problem, and presents a new communication-impact-driven coordinated approach, as well as a system called COMMA that realizes this approach. Through extensive testbed experiments, numerical analyses, and a demonstration of COMMA on Amazon EC2, we show that this approach is highly effective in minimizing migration's impact on multi-tier applications' performance."
pub.1013146901,COMMA,"Multi-tier applications are widely deployed in today's virtualized cloud computing environments. At the same time, management operations in these virtualized environments, such as load balancing, hardware maintenance, workload consolidation, etc., often make use of live virtual machine (VM) migration to control the placement of VMs. Although existing solutions are able to migrate a single VM efficiently, little attention has been devoted to migrating related VMs in multi-tier applications. Ignoring the relatedness of VMs during migration can lead to serious application performance degradation. This paper formulates the multi-tier application migration problem, and presents a new communication-impact-driven coordinated approach, as well as a system called COMMA that realizes this approach. Through extensive testbed experiments, numerical analyses, and a demonstration of COMMA on Amazon EC2, we show that this approach is highly effective in minimizing migration's impact on multi-tier applications' performance."
pub.1167780691,Artificial Intelligence Based Store Management,"The project proposes innovative ideas such as personalized customer interactions through a mobile application and optimizing queues through the sliding checkout model. It also leverages existing kiosks for digital customer connections. The project's methodology is based on comprehensive needs analysis, consultations with industry experts, and the identification of processes suitable for automation. It also prioritizes Research and Development (R&D) in retail merchandising by securing R&D licenses from industry giants. The project's technological infrastructure is designed for the Azure cloud environment, ensuring operational efficiency and seamless integration with various systems. A robust logging infrastructure is in place to maintain an uninterrupted connection between artificial intelligence support and the backend architecture. The project also develops a mobile application with user-friendly interfaces and cross-platform functionality using Flutter. The anticipated benefits of the project include time savings for store managers, data-driven decision-making, and experimental positioning for testing and implementing novel methods in store operation. Overall, the ""Artificial Intelligence Based Store Management"" project aims to set new industry standards by integrating artificial intelligence and machine learning into retail merchandising."
pub.1093951297,A Generic Framework for Application Configuration Discovery with Pluggable Knowledge,"Discovering application configurations and dependencies in the existing runtime environment is a critical prerequisite to the success of cloud migration, which attracts many attentions from both researchers and commercial vendors. However, the high complexity and diversity of enterprise applications as well as their runtime environment challenge the existing approaches which generally depend on the pre-built domain specific knowledge. In this paper, we propose a generic framework for application configuration discovery which can be applied even when the domain knowledge is missing or incomplete. We design a generic approach to significantly narrow down the configuration discovery scale based on the iterative comparison and enable users to manually identify configurations from reasonable scaled file sets with semantic tags. To maximize automation, we further design an easy extensible and pluggable knowledge base to assist configuration discovery. Through extensive case study, the capability and efficiency of our framework have been demonstrated."
pub.1169427404,Toward Secure and Trustworthy Vehicular Fog Computing: A Survey,"The integration of fog computing in vehicular networks has led to significant advancements in road safety, traffic control, entertainment, and comfort services. Vehicular Fog Computing (VFC) emerges as an optimistic solution, offering a pathway to responsive service requests. VFC uses either onboard vehicle computers or vehicles as fog infrastructure between the underlying networks and the cloud, addressing the limitations of centralized data processing in traditional cloud computing. However, VFC faces security vulnerabilities due to the open nature of its deployment. In this survey, we explore the security threats confronting VFC and review existing solutions related to their detection and mitigation capabilities. This paper provides a comprehensive overview of the foundational concept of Fog Computing, VFC architectures, and their critical role in various intelligent computing applications. Moreover, it spotlights the trust and security concerns in deploying VFC and real-time big data analytics within the vehicular environment. This survey identifies pressing issues and outlines potential research directions, offering insights for the research community to address while designing secure VFC architectures."
pub.1019000717,vPerfGuard,"Many business customers hesitate to move all their applications to the cloud due to performance concerns. White-box diagnosis relies on human expert experience or performance troubleshooting ""cookbooks"" to find potential performance bottlenecks. Despite wide adoption, the scalability and adaptivity of such approaches remain severely constrained, especially in a highly-dynamic, consolidated cloud environment. Leveraging the rich telemetry collected from applications and systems in the cloud, and the power of statistical learning, vPerfGuard complements the existing approaches with a model-driven framework by: (1) automatically identifying system metrics that are most predictive of application performance, and (2) adaptively detecting changes in the performance and potential shifts in the predictive metrics that may accompany such a change. Although correlation does not imply causation, the predictive system metrics point to potential causes that can guide a cloud service provider to zero in on the root cause. We have implemented vPerfGuard as a combination of three modules: a sensor module, a model building module, and a model updating module. We evaluate its effectiveness using different benchmarks and different workload types, specifically focusing on various resource (CPU, memory, disk I/O) contention scenarios that are caused by workload surges or ""noisy neighbors"". The results show that vPerfGuard automatically points to the correct performance bottleneck in each scenario, including the type of the contended resource and the host where the contention occurred."
pub.1144412753,Optimal Autonomic Management of Service-Based Business Processes in the Cloud,"Cloud computing is an emerging paradigm that provides hardware, platform and software resources as services over the internet in a pay-as-you-go model. It is being increasingly used for hosting and executing service-based business processes. These business processes are exposed to dynamic evolution during their life-cycle due to the highly dynamic evolution of cloud environments. The main adopted technique is to couple cloud computing with autonomic management in order to build autonomic computing systems. Almost all the existing approaches on autonomic computing have been focused on modeling and implementing autonomic mechanisms without paying any attention to the optimization of the autonomic management cost. Therefore, in this paper, we propose a novel approach based on binary linear program for determining the optimal allocation of cloud resources to manage a service-based business process which guarantees the specific requirements of customers and minimizes the management monetary cost. Then, to validate our approach under realistic conditions and inputs, we extend the CloudSim simulator to model and simulate the behaviour of business processes and their management in a cloud environment. Experiments conducted on two real datasets highlight the effectiveness of our approach."
pub.1093377152,Critical Review of Vendor Lock-in and its Impact on Adoption of Cloud Computing,"Cloud computing offers an innovative business model for organizations to adopt IT services at a reduced cost with increased reliability and scalability. However organizations are slow in adopting the cloud model due to the prevalent vendor lock-in issue and challenges associated with it. While the existing cloud solutions for public and private companies are vendor locked-in by design, their existence is subject to limited possibility to interoperate with other cloud systems. In this paper we have presented a critical review of pertinent business, technical and legal issues associated with vendor lock-in, and how it impacts on the widespread adoption of cloud computing. The paper attempts to reflect on the issues associated with interoperability and portability, but with a focus on vendor lock-in. Moreover, the paper demonstrates the importance of interoperability, portability and standards applicable to cloud computing environments along with highlighting other corporate concerns due to the lock-in problem. The outcome of this paper provides a foundation for future analysis and review regarding the impact of vendor neutrality for corporate cloud computing application and services."
pub.1061576576,Cloud Gaming: Understanding the Support From Advanced Virtualization and Hardware,"Existing cloud gaming platforms have mainly focused on private nonvirtualized environments with proprietary hardware. Modern public cloud platforms heavily rely on virtualization for efficient resource sharing, the potentials of which have yet to be explored. Migrating gaming to a public cloud is nontrivial, however, particularly considering the overhead for virtualization and that the graphics processing units (GPUs) for game rendering has long been an obstacle in virtualization. This paper takes a first step toward bridging the online gaming system and the public cloud platforms. We present the design and implementation of a fully virtualized cloud gaming platform with the latest hardware support for both remote servers and local clients. We explore many critical design issues inherent in cloud gaming, including the choice of hardware or software video encoding, and the configuration and the detailed power consumption of thin client. We demonstrate that with the latest hardware and virtualization support, gaming over virtualized cloud can be made possible with careful optimization and integration of the different modules. We also highlight critical challenges toward full-fledged deployment of gaming services over the public virtualized cloud."
pub.1175029962,Flow Optimization at Inter-Datacenter Networks for Application Run-time Acceleration,"In the present-day, distributed applications are commonly spread across multiple datacenters, reaching out to edge and fog computing locations. The transition away from single datacenter hosting is driven by capacity constraints in datacenters and the adoption of hybrid deployment strategies, combining on-premise and public cloud facilities. However, the performance of such applications is often limited by extended Flow Completion Times (FCT) for short flows due to queuing behind bursts of packets from concurrent long flows. To address this challenge, we propose a solution to prioritize short flows over long flows in the Software-Defined Wide-Area Network (SD-WAN) interconnecting the distributed computing platforms. Our solution utilizes eBPF to segregate short and long flows, transmitting them over separate tunnels with the same properties. By effectively mitigating queuing delays, we consistently achieve a 1.5 times reduction in FCT for short flows, resulting in improved application response times. The proposed solution works with encrypted traffic and is application-agnostic, making it deployable in diverse distributed environments without modifying the applications themselves. Our testbed evaluation demonstrates the effectiveness of our approach in accelerating the run-time of distributed applications, providing valuable insights for optimizing multi-datacenter and edge deployments."
pub.1147202935,Effective Application of Cloud Platform in Classroom Online Teaching,"In recent years, with China entering the ""Internet plus” , China' s distance education has entered a period of profound transformation based on information technology. The rapid rise of online education has brought new opportunities and challenges to the development of educational reform and teaching. Online education, or E-Learning, is a method of content dissemination and rapid learning through the application of information and Internet technologies. "" E"" stands for electronic learning, efficient learning, explorative learning, experiential learning, extended learning, extensible learning, easy-to-use learning and enhanced learning.In the ""Internet plus"" era, education and learning are more universal, autonomous and free, online education is a"" Internet plus"" environment of the pan-network education, emphasizing the full development of modern education technology, especially the advantages of internet technology, and advocating that everyone can learn, always learn and everywhere.In this era of information explosion, access to information through the lnternet is the most convenient way. And no one can talk about the Internet without mentioning the Cloud, its computing power and storage capacity are superior, and its software application power is more diverse, which can let the user take what they want, at any time. This makes the Cloud its own characteristics, which can be summed up in four “V”, the volumn, the velocity, the variety, the veracity.Since the outbreak of the new crown, offline education has been severely affected worldwide. In response to this situation, many countries have launched online education. Cloud Class has emerged and gradually become universal and at present, it has been more and more recognized and popular. Especially after the announcement of the Chinese government' s policy of “double reduction”.It has reduced the burden of schoolwork for primary and middle school students and reduced their participation in out-of-school training institutions.Therefore, China' s application and demand for online education will be more extensive.The use of Cloud Class in education has created an entirely new educational system in which students can browse educational resources at any time using any Internetenabled terminal (such as a mobile phone, tablet computer etc ), thus achieving personalized self-learning, and teachers can use the platform for teaching discussion and teacher-student interaction.But while the rapid rise of online education and the Cloud Class bring great opportunities and convenience to the development of educational reform, there are many shortcomings and challenges such as how to accomplish the teaching goals effectively without self-restraint, how to improve students' knowledge and abilities, how to make and organize online courses for their personal interests and how to transform teachers from traditional communicators and indoctrinators into organisers and facilitators of teaching activities.This paper main"
pub.1170324959,PerOS: Personalized Self-Adapting Operating Systems in the Cloud,"Operating systems (OSes) are foundational to computer systems, managing
hardware resources and ensuring secure environments for diverse applications.
However, despite their enduring importance, the fundamental design objectives
of OSes have seen minimal evolution over decades. Traditionally prioritizing
aspects like speed, memory efficiency, security, and scalability, these
objectives often overlook the crucial aspect of intelligence as well as
personalized user experience. The lack of intelligence becomes increasingly
critical amid technological revolutions, such as the remarkable advancements in
machine learning (ML).
  Today's personal devices, evolving into intimate companions for users, pose
unique challenges for traditional OSes like Linux and iOS, especially with the
emergence of specialized hardware featuring heterogeneous components.
Furthermore, the rise of large language models (LLMs) in ML has introduced
transformative capabilities, reshaping user interactions and software
development paradigms.
  While existing literature predominantly focuses on leveraging ML methods for
system optimization or accelerating ML workloads, there is a significant gap in
addressing personalized user experiences at the OS level. To tackle this
challenge, this work proposes PerOS, a personalized OS ingrained with LLM
capabilities. PerOS aims to provide tailored user experiences while
safeguarding privacy and personal data through declarative interfaces,
self-adaptive kernels, and secure data management in a scalable cloud-centric
architecture; therein lies the main research question of this work: How can we
develop intelligent, secure, and scalable OSes that deliver personalized
experiences to thousands of users?"
pub.1111456234,Security in hardware assisted virtualization for cloud computing—State of the art issues and challenges,"The advantages of virtualization technology have resulted in its wide spread adoption in cloud computing infrastructures. However it has also introduced a new set of security threats that are serious in nature. Many of these threats are unique in virtualized environments and not pertinent in the traditional computing scenarios. Hence these threats have been less studied and thus less addressed by most of the security application vendors. For this reason, it becomes important to carefully analyze the various threats arising at different components of virtualization and thus effectively create solutions to defend the systems against them. This survey attempts to highlight the significant vulnerabilities and expose the readers to the various existing attacks related to Hardware assisted virtualization, as it has become the most widely used form of virtualization in building modern day massive data centers and cloud infrastructures. A Bayesian attack graph model is presented for evaluating the risks associated with the identified threats. A detailed discussion of various countermeasures proposed against the identified threats is presented along with the enumeration of challenges in adopting them."
pub.1106490525,Modeling industry 4.0 based fog computing environments for application analysis and deployment," The extension of the Cloud to the Edge of the network through Fog Computing can have a significant impact on the reliability and latencies of deployed applications. Recent papers have suggested a shift from VM and Container based deployments to a shared environment among applications to better utilize resources. Unfortunately, the existing deployment and optimization methods pay little attention to developing and identifying complete models to such systems which may cause large inaccuracies between simulated and physical run-time parameters. Existing models do not account for application interdependence or the locality of application resources which causes extra communication and processing delays. This paper addresses these issues by carrying out experiments in both cloud and edge systems with various scales and applications. It analyses the outcomes to derive a new reference model with data driven parameter formulations and representations to help understand the effect of migration on these systems. As a result, we can have a more complete characterization of the fog environment. This, together with tailored optimization methods than can handle the heterogeneity and scale of the fog can improve the overall system run-time parameters and improve constraint satisfaction. An Industry 4.0 based case study with different scenarios was used to analyze and validate the effectiveness of the proposed model. Tests were deployed on physical and virtual environments with different scales. The advantages of the model based optimization methods were validated in real physical environments. Based on these tests, we have found that our model is 90% accurate on load and delay predictions for application deployments in both cloud and edge."
pub.1099994660,Impregnable Defence Architecture using Dynamic Correlation-based Graded Intrusion Detection System for Cloud,"<p class=""p1"">Data security and privacy are perennial concerns related to cloud migration, whether it is about applications, business or customers. In this paper, novel security architecture for the cloud environment designed with intrusion detection and prevention system (IDPS) components as a graded multi-tier defense framework. It is a defensive formation of collaborative IDPS components with dynamically revolving alert data placed in multiple tiers of virtual local area networks (VLANs). The model has two significant contributions for impregnable protection, one is to reduce alert generation delay by dynamic correlation and the second is to support the supervised learning of malware detection through system call analysis. The defence formation facilitates malware detection with linear support vector machine- stochastic gradient descent (SVM-SGD) statistical algorithm. It requires little computational effort to counter the distributed, co-ordinated attacks efficiently. The framework design, then, takes distributed port scan attack as an example for assessing the efficiency in terms of reduction in alert generation delay, the number of false positives and learning time through comparison with existing techniques is discussed.</p>"
pub.1029359779,A Review of the Current Level of Support to Aid Decisions for Migrating to Cloud Computing,"Cloud computing provides an innovative delivery model that enables enterprises to reduce operational costs and improve flexibility and scalability. Organisations wishing to migrate their legacy systems to the cloud often need to go through a difficult and complicated decision-making process. This can be due to multiple factors including restructuring IT resources, the still evolving nature of the cloud environment, and the continuous expansion of the services offered. These have increased the requirement for tools and techniques to help the decision-making process for migration. Although significant contributions have been made in this area, there are still many aspects which require further support. This paper evaluates the existing level of support to aid the decision-making process. It examines the complexity of decisions, evaluates the current state of Decision Support Systems in respect of migrating to the cloud, and analyses three models that proposed support for the migration processes. This paper identifies the need for a coherent approach for supporting the whole decision-making process. Further, it explores possible new approaches for addressing the complex issues involved in decision-making for migrating to the cloud."
pub.1173222509,Generative AI Empowered LiDAR Point Cloud Generation with Multimodal Transformer,"Integrated sensing and communications is a key enabler for the 6G wireless
communication systems. The multiple sensing modalities will allow the base
station to have a more accurate representation of the environment, leading to
context-aware communications. Some widely equipped sensors such as cameras and
RADAR sensors can provide some environmental perceptions. However, they are not
enough to generate precise environmental representations, especially in adverse
weather conditions. On the other hand, the LiDAR sensors provide more accurate
representations, however, their widespread adoption is hindered by their high
cost. This paper proposes a novel approach to enhance the wireless
communication systems by synthesizing LiDAR point clouds from images and RADAR
data. Specifically, it uses a multimodal transformer architecture and
pre-trained encoding models to enable an accurate LiDAR generation. The
proposed framework is evaluated on the DeepSense 6G dataset, which is a
real-world dataset curated for context-aware wireless applications. Our results
demonstrate the efficacy of the proposed approach in accurately generating
LiDAR point clouds. We achieve a modified mean squared error of 10.3931. Visual
examination of the images indicates that our model can successfully capture the
majority of structures present in the LiDAR point cloud for diverse
environments. This will enable the base stations to achieve more precise
environmental sensing. By integrating LiDAR synthesis with existing sensing
modalities, our method can enhance the performance of various wireless
applications, including beam and blockage prediction."
pub.1163868950,Enterprise Transformation Projects/Cloud Transformation Concept,"This chapter presents the fundaments of the cloud transformation concept (CTC), and this concept is a basic component of the author's framework. The implementation of the CTC compute system (CTC-CS) is supported the author's applied holistic mathematical model (AHMM) for CTC (AHMM4CTC) and many research works on compute systems (CS), mathematical models, artificial intelligence (AI), and business/financial/organizational transformations projects. The AHMM is based on cross-functional research on an authentic and proprietary mixed research method that is supported by his own version of an AI search tree, which is combined with an internal heuristic's algorithm. The main focus is on CTC-CS requirements and transformation strategy. The proposed AHMM4CTC based CTC-CS is a virtual secured computing environment which uses an integrated empiric decision-making process. The CTC-CS is supported by a real-life case of business transformation project, which needs a Cloud infrastructure that is supported by the alignment of various existing Cloud standards."
pub.1124465172,An Efficient and Transparent Approach for Adaptive Intra- and Inter-node Virtual Machine Communication in Virtualized Clouds,"Network I/O workloads are dominating as one of the leading costs for most of the virtualized clouds. One way to improve the inter virtual machine (VM) inefficiency is to build shared memory channels between VMs co-located on the same physical node to by-pass traditional TCP/IP network stack, so that the overhead is reduced by shorter communication path and fewer kernel interactions. However, it is a key challenge for existing work to achieve high performance inter-Vm communication while keeping the capability of VM live migration, and most of existing work are neither seamlessly agile in the presence of VM live migration nor transparent to upper users as well as to operating system kernels, which limits the application of current co-location aware shared-memory based approaches. In this paper, we present the design and implementation of XenVmc, an adaptive and transparent inter-Vm communication system for high performance network I/O in virtualized clouds. With proposed dynamic colocated VM membership update mechanism, XenVMC is applicable not only to intra-node VM communication, but also to cross-node communication. It also supports adaptive switching between shared-memory based channel and traditional network-based channel in case of VM live migration, with the aid of proposed VM migration perception and handling mechanisms. XenVmc enables efficient data transmission for both TCP and UDP workloads, with multilevel transparency guaranteed. Extensive experiments show that XenVMC achieves better performance for both TCP and UDP workloads with high transparency, compared with both native virtualized environment and representative existing work. Experimental results also show that it is capable of automatically handling VM migration correctly with acceptable latency."
pub.1106136081,De-Convolving Migration Methodology via Detailed Assessment and Cognitive Learning Generating Migration Assessment Report,"The application (workload) migration process from the Mainframe to the Cloud environment turns out to be quite complicated: error-prone, time-consuming and costly. Even worse, the application may not work correctly after the sophisticated migration process. Existing approaches mainly complete this process in an ad-hoc manual manner and thus the chances of error are very high. Thus how to migrate the applications to the Cloud platform correctly and effectively poses a critical challenge for the IT service industry as well as the enterprise clients. This paper dissects the migration process into various touch-points to be addressed while deciding for the Migration and addresses it using an adaptive assessment to generate scores for various possible solution approaches to Migration; which are then used to decide upon an approach for the workload."
pub.1094739409,MAT: A Migration Assessment Toolkit for PaaS Clouds,"Different PaaS (Platform as a Service) Clouds offer different set of capabilities and services and have different constraints on types of application that can be hosted on their platforms. Migrating existing enterprise applications to such platforms thus is non-trivial and needs a thorough assessment of the system to be migrated. In this paper, we present a novel approach for automated assessment of applications for migration to a target PaaS platform. We take an approach of systematically studying typical external technical services that different types of applications need in a traditional non-PaaS deployment and evaluate support for each of services in major PaaS environments. We have created rich sets of repositories each for technical capabilities and services used by typical enterprise applications as well as for the different technical services exposed for use by PaaS platforms along with their limitations and caveats. Using these repositories, our approach analyses the source code as well as the configuration files to recursively extract the services it requires and then tries to map them to a target PaaS platform. The approach results in a detailed report of the parts of the system that can migrate as-is, which need some changes, as well as those which can't be migrated at all due to the limitations of the chosen PaaS platform."
pub.1095580615,Semantic access control for cloud computing based on e-Healthcare,"With the increased development of cloud computing, access control policies have become an important issue in the security filed of cloud computing. Semantic web is the extension of current Web which aims at automation, integration and reuse of data among different web applications such as clouding computing. However, Semantic web applications pose some new requirements for security mechanisms especially in the access control models. In this paper, we analyse existing access control methods and present a semantic based access control model which considers semantic relations among different entities in cloud computing environment. We have enriched the research for semantic web technology with role-based access control that is able to be applied in the field of medical information system or eHealthcare system. This work demonstrates how the semantic web technology provides efficient solutions for the management of complex and distributed data in heterogeneous systems, and it can be used in the medical information systems as well."
pub.1093343588,Smart Grid Cloud for Indian Power Sector,"Smart grid is maximum optimization of energy management achieved through transmission and distribution automation, efficient use of existing network and integration of smart devices. The intelligent monitoring sensors generate enormous heterogeneous, uncorrelated and unstructured data which need large number of scalable storage servers. The analytic tools and control and optimization algorithms require reliable computation servers for self-healing, fault tolerant, load balancing, demand response and optimal power flow features. Also the customer web applications for real-time consumption patterns, flexible tariffs and online bill payments need designing and deployment tools. The lack of computing infrastructure and financial constraints are the primary challenges for full realization of complex smart grid in developing countries like India. The appliance of the cloud computing model meets the requirement of data and computing intensive smart grid applications. Through the proposed cloud computing model, smart grid can enhance the reliability, availability, safety, efficiency and environment friendliness of power sector."
pub.1121803243,An In-Memory Checkpoint-Restart Mechanism for a Cluster of Virtual Machines,"A cluster of virtual machines can be used to execute parallel applications in Cloud Computing environments. However, the cloud infrastructure may fail at any time for a variety of reasons. Although a coordinated checkpointing capability at the hypervisor level is highly transparent to parallel applications, existing solutions still suffer from excessive checkpoint time and downtime. They also cause significant application execution delays due to packet loss. This paper introduces IMVCCR, a novel in-memory Checkpoint-Restart mechanism for a virtual cluster. IMVCCR consists of a framework that performs coordinated checkpointing for the entire cluster. It reduces checkpoint time and downtime by applying live migration and using main memory as transient checkpoint storage. IMVCCR also uses an efficient synchronization mechanism to reduce packet loss. Preliminary experiments show that IMVCCR generates very low checkpoint times and downtimes. It also incurs low overheads in the total execution time of parallel applications."
pub.1094661484,An Overview of Application Traffic Management Approaches: Challenges and Potential Extensions,"The Internet has seen a strong move to support overlay applications and services, which demand a coherent and integrated control in underlying heterogeneous networks in a scalable, resilient, and energy-efficient manner. To do so, a tighter integration of network management and overlay service functionality can lead to cross-layer optimization of operations and management, which is a promising approach and may offer a large business potential in operational perspectives for all players involved. Therefore, the objective of this paper is to present and discuss the impact of new paradigms such as cloud computing and software-defined networking which will play central role in the Future Internet, discuss major traffic trends and identify key challenges due to the adoption and operation of the new applications. Translating the key challenges to requirements for Future Internet traffic management mechanisms, the paper provides an overview of existing mechanisms in literature, assesses them w.r.t. the aforementioned requirements, and qualitatively estimates the expected optimization potential and gain, as well as provides hint for their potential extension and exploitation within the challenging environment of the Future Internet."
pub.1061258403,A Survey of Network Isolation Solutions for Multi-Tenant Data Centers,"The infrastructure-as-a-service model is one of the fastest growing opportunities for cloud-based service providers. It provides an environment that reduces operating and capital expenses while increasing agility and reliability of critical information systems. In this multitenancy environment, cloud-based service providers are challenged with providing a secure isolation service combining different vertical segments, such as financial or public services, while nevertheless meeting industry standards and legal compliance requirements within their data centers. In order to achieve this, new solutions are being designed and proposed to provide traffic isolation for a large numbers of tenants and their resulting traffic volumes. This survey highlights key challenges that cloud-based service providers might encounter while providing multitenant environments. It also succinctly describes some key solutions for providing simultaneous tenant and network isolation, as well as highlights their respective advantages and disadvantages. We begin with generic routing encapsulation introduced in 1994 in “RFC 1701,” and will conclude with today's latest solutions. We detail 15 of the newest architectures and then compare their complexities, the overhead they induce, their VM migration abilities, their resilience, their scalability, and their multidata center capacities. This paper is intended for, but not limited to, cloud-based service providers who want to deploy the most appropriate isolation solution for their needs, taking into consideration their existing network infrastructure. This survey provides details and comparisons of various proposals while also highlighting possible guidelines for future research on issues pertaining to the design of new network isolation architectures."
pub.1095542618,Multiobjective Communication Optimization for Cloud-Integrated Body Sensor Networks,"This paper focuses on push-pull hybrid communication in a cloud-integrated sensor networking architecture, called Sensor-Cloud Integration Platform as a Service (SC-iPaaS). SC-iPaaS consists of three layers: sensor, edge and cloud layers. The sensor layer consists of wireless body sensor networks, each of which operates several sensors for a homebound patient for a remote physiological and activity monitoring. The edge layer consists of sink nodes that collect sensor data from sensor nodes in the sensor layer. The cloud layer hosts cloud applications that obtain sensor data through sink nodes in the edge layer. This paper formulates an optimization problem for SC-iPaaS to seek the optimal data transmission rates for individual sensor and edge nodes and solves the problem with respect to multiple objectives (e.g., data yield, bandwidth consumption and energy consumption) subject to given constraints. This paper sets up a simulation environment that performs remote multi-patient monitoring with five on-body sensors including ECG, pulse oximeter and accelerometer per a patient. Simulation results demonstrate that the proposed optimizer successfully seeks Pareto-optimal data transmission rates for sensor/sink nodes against data request patterns placed by cloud applications. The results also confirm that the proposed optimizer outperforms an existing well-known optimization algorithm."
pub.1033333559,A conceptual framework for delivering cost effective business intelligence solutions as a service,"Smart use of business intelligence (BI) can allow organizations to leverage the huge amounts of transactional data at their disposal and turn it into a powerful decision support mechanism that gives them competitive advantage. Despite the potential benefits of an effective BI system, the adoption and use of BI systems within the enterprise remains low, especially among smaller companies with resource constraints. This can partly be explained by the predominant deployment approach available today in which a firm needs to procure, install, configure and operate a BI system in-house. Barriers of high cost, complexity and lack of in-house expertise discourage many firms from adopting BI systems. This paper argues that adopting a cloud computing model, where BI is offered as a service over the Internet can lower these barriers and accelerate the pace of BI adoption. However, migrating BI systems from traditional on-premise environments to the cloud presents huge challenges. There are technical, economic, organizational and regulatory hurdles to overcome. Further, BI systems are multi-component (ETL, Data warehouse, data marts, OLAP, reporting, data mining etc.) and deciding which component(s) to move to the cloud, and which ones to leave on-premise needs careful consideration. In addition, the fact that cloud computing is still in its infancy means there is a general lack of conceptual and architectural frameworks to guide companies considering migrating enterprise systems to the cloud. This paper takes a closer look into traditional BI and proposes a conceptual framework that companies can use to chart an adoption path for cloud BI. The framework combines attributes of IT outsourcing, traditional BI, cloud computing as well as decision theory to present a consolidated view of cloud BI. The domain of South African Higher Education was chosen as the target in which the framework will be tested."
pub.1112885457,A Secure G-Cloud-Based Framework for Government Healthcare Services,"Within the literature, we have witnessed in the healthcare sector, the growing demand for and adoption of software development in the cloud environment to cope with and fulfill current and future demands in healthcare services. In this paper, we propose a flexible, secure, cost-effective, and privacy-preserved cloud-based framework for the healthcare environment. We propose a secure and efficient framework for the government EHR system, in which fine-grained access control can be afforded based on multi-authority ciphertext-policy attribute-based encryption (CP-ABE), together with a hierarchical structure, to enforce access control policies. The proposed framework will allow decision-makers in Saudi Arabia to develop the healthcare sector and to benefit from the existing e-government cloud computing platform “Yasser,” which is responsible for delivering shared services through a highly efficient, reliable, and safe environment. This framework aims to provide health services and facilities from the government to citizens (G2C). Furthermore, multifactor applicant authentication has been identified and proofed in cooperation with two trusted authorities. The security analysis and comparisons with the related frameworks have been conducted."
pub.1093197922,Analysing Evolution of Work and Load,"Evolution of work and load is required for investigating elasticity and cost-efficiency of cloud computing applications as well as their underlying architecture. Existing modelling environments have fixed load and work, therefore rendering model-and-analyse approaches infeasible for such applications. This deficiency particularly leads to high risks that applications will violate their service level objectives. Therefore, this article describes how we can model and analyse the evolution of both work and load. We have created a corresponding meta model for usage evolution, and describe how we have integrated our meta model with Palladio, a common model-and-analyse environment. To model evolution we use the Descartes Load Intensity Model (DLIM). DLIM is coupled with the scenario model in the Palladio Component Model (PCM). We illustrate evolution of both work and load within Palladio simulations using a simple image server example."
pub.1119983314,A Lightweight Framework for Research Data Management,"We describe a framework for managing live research data involving two major components. First, a system for the scalable scheduling and execution of automated policies for moving, organizing, and archiving data. Second, a system for managing metadata to facilitate curation and discovery with minimal change to existing workflows. Our approach is guided by four main principles: 1) to be non-invasive and to allow for easy integration into existing workflows and computing environments; 2) to be built on established, cloud-aware, open-source tools; 3) to be easily extensible and configurable, and thus, adaptable to different academic disciplines; and 4) to integrate with and take advantage of infrastructure and services available on academic campuses and research computing environments. These principles give our solution a well-defined place along the spectrum of research data management software such as sophisticated electronic lab notebooks and science gate-ways. Our lightweight and flexible data management framework provides for curation and preservation of research data within a lab, department or university cyberinfrastructure."
pub.1094476718,DCCSOA: A Dynamic Cloud Computing Service-Oriented Architecture,"The emerging field of Cloud Computing provides several advantages over traditional in-house IT services, such as accessing to elastic on-demand computing and storage over the Internet, and cost effective pay-per-use subscription plans. However, according to the International Data Corporation (IDC), cloud computing has several issues, such as a lack of standardization, a lack of customization, and limited interoperability. In addition, there is an increasing demand for introduction and migration of a variety of services to cloud computing systems, which are abstract their offering services into various *-as-a-Services (*aaS) layers. Although each such service provides a new feature (e.g., simulation services in cloud), it aggravates the issues due to the lack of standardization and inability to customize services by a vendor because each *aaS has its own features, requirements and output. In this paper, we propose a cloud architecture to alleviate issues associated with standardization and customization. In the cloud, the proposed architecture uses a single layer, called Template-as-a-Service (TaaS), to provide: (i) a single service layer for interaction with all resources and major cloud services (e.g., IaaS, PaaS, SaaS and *aaS), (ii) a standardization for existing services and future *aaS across different cloud environments, and (iii) a customizable architecture which can be modified on demand by a cloud vendor, and its partners to provide the flexibility on cloud computing systems. A comparison with previous studies show that the proposed architecture provides customization and standardization for cloud services with minimum modifications."
pub.1170895740,Patient Report Management System,"In response to the persistent need for robust security and privacy measures in e-health data management within the realm of cloud computing, this paper presents the Patient Report Management System (PRMS). Developed as a comprehensive solution tailored for cloud environments, PRMS eliminates the reliance on Blockchain or mobile applications. Instead, it focuses on fortifying security while effectively addressing latency and throughput challenges inherent in cloud-based systems. The system is implemented as a cloud-hosted website, leveraging HTML, CSS, Java, and MySQL for efficient database management. A thorough performance analysis of PRMS conducted on diverse third-party cloud platforms demonstrates its superior effectiveness, surpassing existing approaches. The core objectives of the project encompass the establishment of a secure website with email authentication, the creation of a user-friendly interface, the implementation of admin controls for user management, and the integration of data encryption and decryption for enhanced privacy. Additionally, PRMS ensures scalability and adaptability while prioritizing comprehensive security audits. This paper contributes to the advancement of e-health data management by providing a secure and efficient solution tailored for cloud environments."
pub.1142211895,Rapid Prototype for Shifting HPC to the Cloud,"With the growing adoption of cloud computing, we have watched the rapidly changing landscape looking for unique ways to leverage cloud resources in an on-premise batch scheduled HPC environment. Research Computing at Purdue University has a long history of providing low cost HPC compute and storage resources to researchers on campus. Two areas we have recently targeted for cloud adoption are large scale workload bursting and rapid resource prototyping. Utilizing the new HBv3 instance types in Microsoft Azure, we were able to access the most recent third generation of AMD EPYC CPUs sooner than the delivery and deployment of our equivalent on-site resource, the upcoming NSF funded Anvil supercomputer. This rapid access allowed our colleagues and system designers to complete various tasks needed to evaluate and verify the expected performance of the newer generation hardware that will be available with Anvil in parallel to the on-site deployment."
pub.1174698405,Data augmentation and generative machine learning on the cloud platform,"This paper aims to explore the image data augmentation application on the cloud platform utilizing state-of-the-art generative machine learning techniques. This paper further highlights these techniques’ significance in addressing the challenge of data generation and emphasizes the need for further research in this area. This research adopts an in-depth exploration approach to examine the burgeoning domain of generative machine learning techniques. It discusses the evolution of these techniques and their integration with cloud services powered by Graphical Processing Unit (GPU)-enabled computational engines. Practical experimentation involving Modified National Institute of Standards and Technology (MNIST) data is conducted to showcase the capabilities of generative models, with a focus on the core Generative Adversarial Network (GAN). The findings reveal the potential of generative machine learning techniques in generating new data images, as demonstrated through practical experimentation with MNIST data. It also highlights the ongoing evolution of these techniques and their challenges, particularly in terms of computational requirements and integration with cloud computing services. This research originally contributes to the existing literature by providing insights into recent advancements and challenges in GANs and their synergies with cloud computing. It presents results from experimentation and emphasizes the importance of cost-effective development environments for implementing generative machine learning techniques."
pub.1126052878,Implementation of an energy saving cloud infrastructure with virtual machine power usage monitoring and live migration on OpenStack,"Cloud computing is Internet-based computing which requires more physical machines and consumes a large amount of power. By this means, that will reduce the profit of the service providers and harm the environment. How to effectively handle the power consumption of cloud computing has been an issue in recent years. When making a large number of operations, and power consumption cannot be underestimated. In this case, the usage of Virtualization that become widely in cloud computing nowadays, also need energy efficient scheduling methods. However, existing energy efficient scheduling methods of virtual machines (VMs) in the cloud cannot work well if the physical machines (PMs) are heterogeneous and their total power is considered. In this paper, we propose an implementation of a cloud infrastructure that can monitor the status of OpenStack and monitor the real-time status of a virtual machine on OpenStack. Also, achieve energy saving through live migration. The projects of monitoring include the utilization of CPU, load of memory, and power consumption. These data show in real-time, thoroughly monitor the real-time status of physical machines and virtual machines. It also records the utilization and power consumption of physical machines then show on this cloud infrastructure, to provide experimental evidence for the user as a reference. Based on the power consumption monitoring system, we can automatically allocate virtual machines on every physical machine by live migration, to balance the power consumption of every physical machine. It is not only can avoid idle and a waste of resources but also can avoid reducing machine life-time because of the physical machines always keep in high usage."
pub.1085791533,生産制御システムの変革と未来,"Change of technologies and market situation strongly affects the environment of the control systems. In recent 10-15 years particularly, various standardization and open technologies such as MS-Windows, field digital, and alarm management are introduced, and each control system vender has been applying those to its control system with avoiding the impact for the fundamental functionalities and roles of control systems, In coming decade, some larger changes are expected such as standardization of control system architecture, that of application schemes, integration between control systems and production management systems,CAPEX reduction with de-bottlenecking of project execution, and applying of cloud technology.Control systems venders are required to adopt such technologies efficiently in order to provide customers benefits without losing the current roles of the control systems in customers' plants. Control systems venders are also required to consider the system continuity and compatibility to propose their customers the system migration plan from the existing control systems to the latest with minimum risk."
pub.1119352347,A Scalable Model for Secure Multiparty Authentication,"Distributed system architectures such as cloud computing or the emergent
architectures of the Internet Of Things, present significant challenges for
security and privacy. Specifically, in a complex application there is a need to
securely delegate access control mechanisms to one or more parties, who in turn
can govern methods that enable multiple other parties to be authenticated in
relation to the services that they wish to consume. We identify shortcomings in
an existing proposal by Xu et al for multiparty authentication and evaluate a
novel model from Al-Aqrabi et al that has been designed specifically for
complex multiple security realm environments. The adoption of a Session
Authority Cloud ensures that resources for authentication requests are
scalable, whilst permitting the necessary architectural abstraction for myriad
hardware IoT devices such as actuators and sensor networks, etc. In addition,
the ability to ensure that session credentials are confirmed with the relevant
resource principles means that the essential rigour for multiparty
authentication is established."
pub.1128199369,Workflow-as-a-Service Cloud Platform and Deployment of Bioinformatics Workflow Applications,"Workflow management systems (WMS) support the composition and deployment of
workflow-oriented applications in distributed computing environments. They hide
the complexity of managing large-scale applications, which includes the
controlling data pipelining between tasks, ensuring the application's
execution, and orchestrating the distributed computational resources to get a
reasonable processing time. With the increasing trends of scientific workflow
adoption, the demand to deploy them using a third-party service begins to
increase. Workflow-as-a-service (WaaS) is a term representing the platform that
serves the users who require to deploy their workflow applications on
third-party cloud-managed services. This concept drives the existing WMS
technology to evolve towards the development of the WaaS cloud platform. Based
on this requirement, we extend CloudBus WMS functionality to handle the
workload of multiple workflows and develop the WaaS cloud platform prototype.
We implemented the Elastic Budget-constrained resource Provisioning and
Scheduling algorithm for Multiple workflows (EBPSM) algorithm that is capable
of scheduling multiple workflows and evaluated the platform using two
bioinformatics workflows. Our experimental results show that the platform is
capable of efficiently handling multiple workflows execution and gaining its
purpose to minimize the makespan while meeting the budget."
pub.1110427297,A Scalable Model for Secure Multiparty Authentication,"Distributed system architectures such as cloud computing or the emergent architectures of the Internet Of Things, present significant challenges for security and privacy. Specifically, in a complex application there is a need to securely delegate access control mechanisms to one or more parties, who in turn can govern methods that enable multiple other parties to be authenticated in relation to the services that they wish to consume. We identify shortcomings in an existing proposal by Xu et al for multiparty authentication and evaluate a novel model from Al-Aqrabi et al that has been designed specifically for complex multiple security realm environments. The adoption of a Session Authority Cloud ensures that resources for authentication requests are scalable, whilst permitting the necessary architectural abstraction for myriad hardware IoT devices such as actuators and sensor networks, etc. In addition, the ability to ensure that session credentials are confirmed with the relevant resource principles means that the essential rigour for multiparty authentication is established."
pub.1173479009,Practical Evaluation of Dynamic Service Function Chaining (SFC) for Softwarized Mobile Services in an SDN-based Cloud Network,"The evolving mobile network specifications, as well as the increasingly stringent requirements for bandwidth, quality of service, and dynamic on-demand adaptability, push the boundaries of what is achievable with legacy networking technologies. Software-defined networking (SDN) and Network Functions Virtualization (NFV) provide agile, cost-efficient solutions by enabling an automated Service Management and Orchestration (SMO), allowing for dynamic resource allocation and network (re-)configuration. In this paper, we present an experimental demonstration of an SMO solution for virtualized mobile networks based on a seamless integration of an SDN-based transport network and an cloud computing environment. Our proposed SMO solution is evaluated by investigating the time required for establishing a virtual mobile core network service chain (i.e. a core network slice) on demand within an SDN-based cloud network considering its specific traffic and QoS requirements."
pub.1175392393,Protecting AI-Enabled Industrial Engineering in Cloud and Edge Environments,"Industrial engineering processes have been transformed by the integration of artificial intelligence (AI) into industrial processes. Leveraging AI algorithms and data analytics, manufacturing facilities, supply chains, and logistics networks have become smarter and more responsive. This transformation has extended beyond the confines of traditional on-premises systems, with cloud and edge computing environments playing pivotal roles in facilitating real-time decision-making and remote management. The expansion of AI into these distributed ecosystems, however, raises significant security and privacy concerns. They must be addressed in order to protect critical industrial processes and data integrity. The adoption of AI in industrial settings has created a complex attack surface, vulnerable to threats such as data breaches, system vulnerabilities, and adversarial attacks. We examine the unique security requirements of cloud and edge computing, as well as the specific vulnerabilities introduced by AI systems in these settings. Managing security challenges requires a multifaceted approach. To detect potential adversaries, we use real-time monitoring of AI algorithms as well as robust access controls, encryption, and intrusion detection systems. Furthermore, the integration of AI-enabled security tools can enhance threat detection and response, reducing the window of vulnerability. Additionally, data privacy is a paramount concern in AI-enabled industrial engineering. Proper data anonymization, regulatory compliance, and privacy-preserving techniques must be integrated into the AI ecosystem. Special attention is given to edge computing environments, where data is often processed locally to minimize latency and bandwidth requirements, thus posing unique privacy challenges. As AI continues to transform the landscape of industrial engineering, protecting these systems in cloud and edge environments is a critical imperative. By developing a comprehensive security strategy, organizations can make use of full potential of AI while mitigating the risks associated with its deployment in distributed ecosystems. This acts as a starting point for understanding the complex interplay of AI, cloud, and edge security in the context of industrial engineering, paving the way for further research and practical implementation in this dynamic and evolving field."
pub.1158056006,Satellite Operations in a Future Cloud Based Ground System Architecture,"One of the Strategic Objectives of the National Oceanic and Atmospheric Administration's (NOAA) National Environmental Satellite, Data, and Information Service (NESDIS) is to develop agile, scalable ground capabilities to improve efficiency of service deliverables and ingest of satellite observation data from a broad variety of internal and external sources. In addition to the NESDIS Ground Enterprise Study, which examines the feasibility of various approaches to achieve these Strategic Objectives, NESDIS is also performing ongoing market research such as Requests for Information (RFIs), Cooperative Research and Development Agreements (CRADAs), and Vendor Demonstrations to understand potential on-ramps to more rapidly achieve a ground enterprise architecture that meets this Strategic Objective. In the case of satellite operations, an envisioned future ground enterprise would heavily exploit new cloud-based technologies, capabilities, and resources. Specific study goals include the ability for cloud solutions to enable command and control across several constellations, rapid integration of new satellites and new missions, as well as developing specialized applications in support of new satellites/missions in cloud-based environments. In this paper, we will first present a summary of the most feasible cloud-based enterprise ground system applications identified from the NESDIS Ground Enterprise Study along with the various NESDIS market research activities. We will further identify and provide a description of the most cost-effective alternatives and capabilities in support of satellite operations to date, and how these could impact satellite operations. We will assess the ability to evolve existing systems to incorporate the recommendations of these research activities, consider the use of services-based approaches, and address the system maintainability process with respect to the alternatives under consideration. We will describe specific scenarios under the future enterprise architecture that would enhance existing functionality and how recent market research potentially either increases and or decreases near-term viability of specific NESDIS near-term objectives. We will also critically analyze the feasibility of cloud-based satellite operations capabilities from a Low Earth Orbit (LEO) operations perspective, in order to address the impact of a given alternative on NOAA operations staff. Finally, we will identify any lessons learned gleaned from the various cloud-based market research activities and their effects on future LEO ground system alternatives."
pub.1151549267,Digital transformation strategy of business models: A framework approach in improving the traditional business model of software industry,"Organizations have leveraged the business model concept from the past two decades and emerging digital technologies that helped them remain competitive in the market. This paper mainly focuses on the various literature reviews that introduced the concept of digital transformation applied to business models, phases, and scope of changes in business models that can be leveraged in value proposition and innovation. The purpose of this paper is to integrate digital technologies like social media, analytics, cloud, and the Internet of Things (IoT)with the business model of startups and other software organizations. A Framework that captures the value proposition, process architecture, and financial stability of the organizations in the digital economy. This paper aims to improve the various existing models that lack digital aspects in the operational, value chain, and customer aspects. Leveraging the Shaping and visionary approach, this paper promotes the digital transformation of incumbents’ business model to venture into a sustainable competitive environment."
pub.1043510018,Intelligent business processes composition based on multi-agent systems,"This paper proposes a novel model for automatic construction of business processes called IPCASCI (Intelligent business Processes Composition based on multi-Agent systems, Semantics and Cloud Integration). The software development industry requires agile construction of new products able to adapt to the emerging needs of a changing market. In this context, we present a method of software component reuse as a model (or methodology), which facilitates the semi-automatic reuse of web services on a cloud computing environment, leading to business process composition. The proposal is based on web service technology, including: (i) Automatic discovery of web services; (ii) Semantics description of web services; (iii) Automatic composition of existing web services to generate new ones; (iv) Automatic invocation of web services. As a result of this proposal, we have presented its implementation (as a tool) on a real case study. The evaluation of the case study and its results are proof of the reliability of IPCASCI."
pub.1094233798,On Security and Privacy Issues of FOG Computing supported Internet of Things Environment,"Recently the concept of Internet of Things (IoT) is attracting much attention due to the huge potential. IoT uses the Internet as a key infrastructure to interconnect numerous geographically diversified IoT nodes which usually have scare resources, and therefore cloud is used as a key back-end supporting infrastructure. In the literature, the collection of the IoT nodes and the cloud is collectively called as an IoT cloud. Unfortunately, the IoT cloud suffers from various drawbacks such as huge network latency as the volume of data which is being processed within the system increases. To alleviate this issue, the concept of fog computing is introduced, in which fog-like intermediate computing buffers are located between the IoT nodes and the cloud infrastructure to locally process a significant amount of regional data. Compared to the original IoT cloud, the communication latency as well as the overhead at the backend cloud infrastructure could be significantly reduced in the fog computing supported IoT cloud, which we will refer as IoT fog. Consequently, several valuable services, which were difficult to be delivered by the traditional IoT cloud, can be effectively offered by the IoT fog. In this paper, however, we argue that the adoption of IoT fog introduces several unique security threats. We first discuss the concept of the IoT fog as well as the existing security measures, which might be useful to secure IoT fog. Then, we explore potential threats to IoT fog."
pub.1096016769,AQUAMan,"As more interactive and multimedia-rich applications are migrating to the cloud, end-user satisfaction and her Quality of Experience (QoE) will become a determinant factor to secure success for any Software as a Service (SaaS) provider. Yet, in order to survive in this competitive market, SaaS providers also need to maximize their Quality of Business (QoBiz) and minimize costs paid to cloud providers. However, most of the existing works in the literature adopt a provider-centric approach where the end-user preferences are overlooked. In this article, we propose the AQUAMan mechanism that gives the provider a fine-grained QoE-driven control over the service acceptability rate while taking into account both end-users' satisfaction and provider's QoBiz. The proposed solution is implemented using a multi-agent simulation environment. The results show that the SaaS provider is capable of attaining the predefined acceptability rate while respecting the imposed average cost per user. Furthermore, the results help the SaaS provider identify the limits of the adaptation mechanism and estimate the best average cost to be invested per user."
pub.1020390220,"Seamless application execution in mobile cloud computing: Motivation, taxonomy, and open challenges","Seamless application execution is vital for the usability of various delay-sensitive mobile cloud applications. However, the resource-intensive migration process and intrinsic limitations of the wireless medium impede the realization of seamless execution in mobile cloud computing (MCC) environment. This work is the first comprehensive survey that studies the state-of-the-art cloud-based mobile application execution frameworks (CMAEFs) in perspective of seamless application execution in MCC and investigates the frameworks suitability for the seamless execution. The seamless execution enabling approaches for the CMAEFs are identified and classified based on the implementation locations. We also investigate the seamless application execution enabling approaches to identify advantages and disadvantages of employing such approaches for attaining the seamless application execution in MCC. The existing frameworks are compared based on the significant parameters derived from the taxonomy of the seamless application execution enabling approaches. The principles for enabling the seamless application execution within the MCC are also highlighted. Finally, open research challenges in realizing the seamless application execution are discussed."
pub.1085607724,Cloud robotics in Smart Manufacturing Environments: Challenges and countermeasures,"In Smart Manufacturing Environments (SME), the use of cloud robotics is based on the integration of cloud computing and industrial robots, which provides a new technological approach to task execution and resource sharing compared to traditional industrial robots. However, research on cloud robotics in SME still faces some challenges. First, highly flexible load scheduling mechanisms are immature. Second, traditional optimization mechanisms for the network service quality do not meet the requirements of smart manufacturing due to time variability and service quality dynamics. And, finally, existing learning algorithms used without cloud-assisted resources cause great resource wasting. Accordingly, this paper explores main technologies related to cloud robotics in SME. The research contents include self-adaptive adjustment mechanisms for the service quality of a cloud robot network, computing load allocation mechanisms for cloud robotics, and group learning based on a cloud platform. The results presented in this paper are helpful to understand the internal mechanisms of perception and interaction, intelligent scheduling and control of cloud robot systems oriented to smart manufacturing, and the design of a cloud architecture oriented to group learning."
pub.1149131456,A Fog-Cluster Based Load-Balancing Technique,"The Internet of Things has recently been a popular topic of study for developing smart homes and smart cities. Most IoT applications are very sensitive to delays, and IoT sensors provide a constant stream of data. The cloud-based IoT services that were first employed suffer from increased latency and inefficient resource use. Fog computing is used to address these issues by moving cloud services closer to the edge in a small-scale, dispersed fashion. Fog computing is quickly gaining popularity as an effective paradigm for providing customers with real-time processing, platforms, and software services. Real-time applications may be supported at a reduced operating cost using an integrated fog-cloud environment that minimizes resources and reduces delays. Load balancing is a critical problem in fog computing because it ensures that the dynamic load is distributed evenly across all fog nodes, avoiding the situation where some nodes are overloaded while others are underloaded. Numerous algorithms have been proposed to accomplish this goal. In this paper, a framework was proposed that contains three subsystems named user subsystem, cloud subsystem, and fog subsystem. The goal of the proposed framework is to decrease bandwidth costs while providing load balancing at the same time. To optimize the use of all the resources in the fog sub-system, a Fog-Cluster-Based Load-Balancing approach along with a refresh period was proposed. The simulation results show that “Fog-Cluster-Based Load Balancing” decreases energy consumption, the number of Virtual Machines (VMs) migrations, and the number of shutdown hosts compared with existing algorithms for the proposed framework."
pub.1095116058,Simulation Tools for Cloud Computing: A Survey and Comparative Study,"Today, cloud computing has become a promising paradigm that aims at delivering computing resources and services on demand. The adoption of these services has been rapidly increasing. One of the main issues in this context is how to evaluate the ability of cloud systems to provide the desired services while respecting the QoS constraints. Experimentation in a real environment is a hard problem. In fact, the financial cost and the time required are very high. Also, the experiments are not repeatable, because a number of variables that are not under control of the tester may affect experimental results. Therefore, using simulation frameworks to evaluate cloud applications is preferred. This paper presents a survey of the existing simulation tools in cloud computing. It provides also a critical and comparative analysis of the studied tools. Finally, it stands out a major challenge to be addressed for further research."
pub.1172819744,ПЕРЕОСМИСЛЕННЯ ОБЛІКОВИХ СИСТЕМ В УКРАЇНСЬКІЙ ТОРГІВЛІ: ШЛЯХ ДО АВТОНОМІЇ ТА БЕЗПЕКИ,"This article elucidates the importance of ensuring autonomy and independence of accounting systems in Ukraine, with a special focus on technological and innovative aspects that meet modern requirements for security, flexibility, and efficiency. In the context of increasing political and economic risks associated with the use of software products, particularly «1C» as an example, the article underscores the necessity of developing and implementing autonomous accounting systems. These systems can provide Ukrainian enterprises with the necessary level of independence from external software suppliers. The goal of the article is not only to highlight existing challenges but also to review potential technological solutions that can assist Ukrainian companies in transitioning to more reliable and flexible accounting systems. Methodology. This includes considering cloud solutions, open-source software, and other innovative approaches that ensure a high level of adaptation to the specific needs of businesses. It's important to realize that such solutions can significantly simplify data management processes and optimize work procedures, making businesses more competitive in the market. Moreover, adapting to modern technologies opens up new opportunities for integration with global economic systems, enhancing the efficiency and transparency of business operations. Scientific novelty. This article discusses the key technical challenges associated with moving away from outdated systems to modern accounting solutions, specifically integration, scalability, security requirements, and migration costs. The emphasis is placed on the need to examine potential issues that companies face when changing software, based on interviews with experts and analysis of various organizations' experiences. Conclusions. The article aims to provide readers with a deep understanding of how the ability to independently choose and control accounting systems can contribute to the growth and development of Ukrainian enterprises in the complex conditions of the modern economic and political environment."
pub.1148737333,Predictive Scaling of Elastic Pod Instances for Modern Applications on Public Cloud through Long Short-Term Memory.,"In Cloud Computing (CC) environment, container virtualization through Docker engine is bringing the new digital transformation in the enterprise multi-tier application architecture in this modern era. Elastic pod containers attracts and helps the application developers in developing and executing the cloud-native modern applications with key benefits such as light weight, agility to launch, easy deployment through images, consuming less power, minimum cost, less carbon footprints with increased resource utilization and provisioning on public cloud data centers. In existing, reactive auto-scaling mechanism of pod resources is used to add or remove resources manually or rule based for handling static workloads from users and most of the times it may lead to over provision or under provisioning of instances that violates QOS and SLA. In this paper, predictive horizontal scaling of pods utilizing custom metrics with orchestration through Kuberenetes is proposed with LSTM (Long Short-Term Memory) based on Deep Learning (DL) technique. DL is data hungry for right prediction of replicas of a cluster that is needed in advance to run cyclic workloads and to handle the sudden spike of demand by using the cloud large dataset real time traces. Moreover, LSTM model is compared with GRU (Gated Recurrent Unit) and experimental results shows that the results of LSTM prediction has less absolute error rate on comparing with GRU to keep the resource provisioning accuracy better for running the modern workloads seamlessly on public cloud."
pub.1091593206,An iterative mathematical decision model for cloud migration: A cost and security risk approach,"Summary This paper presents an iterative mathematical decision model for organizations to evaluate whether to invest in establishing information technology (IT) infrastructure on‐premises or outsourcing IT services on a multicloud environment. This is because a single cloud cannot cover all types of users’ functional/nonfunctional requirements, in addition to several drawbacks such as resource limitation, vendor lock‐in, and prone to failure. On the other hand, multicloud brings several merits such as vendor lock‐in avoidance, system fault tolerance, cost reduction, and better quality of service. The biggest challenge is in selecting an optimal web service composition in the ever increasing multicloud market in which each provider has its own pricing schemes and delivers variation in the service security level. In this regard, we embed a module in the cloud broker to log service downtime and different attacks to measure the security risk. If security tenets, namely, security service level agreement, such as availability, integrity, and confidentiality for mission‐critical applications, are targeted by cybersecurity attacks, it causes disruption in business continuity, leading to financial losses or even business failure. To address this issue, our decision model extends the cost model by using the cost present value concept and the risk model by using the advanced mean failure cost concept, which are derived from the embedded module to quantify cloud competencies. Then, the cloud economic problem is transformed into a bioptimization problem, which minimizes cost and security risks simultaneously. To deal with the combinatorial problem, we extended a genetic algorithm to find a Pareto set of optimal solutions. To reach a concrete result and to illustrate the effectiveness of the decision model, we conducted different scenarios and a small‐to‐medium business IT development for a 5‐year investment as a case study. The result of different implementation shows that multicloud is a promising and reliable solution against IT on‐premises deployment."
pub.1138199950,Greenhouse Automation Using Wireless Sensors and IoT Instruments Integrated with Artificial Intelligence,"Automation of greenhouse environment using simple timer-based actuators or by means of conventional control algorithms that require feedbacks from offline sensors for switching devices are not efficient solutions in large-scale modern greenhouses. Wireless instruments that are integrated with artificial intelligence (AI) algorithms and knowledge-based decision support systems have attracted growers’ attention due to their implementation flexibility, contribution to energy reduction, and yield predictability. Sustainable production of fruits and vegetables under greenhouse environments with reduced energy inputs entails proper integration of the existing climate control systems with IoT automation in order to incorporate real-time data transfer from multiple sensors into AI algorithms and crop growth models using cloud-based streaming systems. This chapter provides an overview of such an automation workflow in greenhouse environments by means of distributed wireless nodes that are custom-designed based on the powerful dual-core 32-bit microcontroller with LoRa modulation at 868 MHz. Sample results from commercial and research greenhouse experiments with the IoT hardware and software have been provided to show connection stability, robustness, and reliability. The presented setup allows deployment of AI on embedded hardware units such as CPUs and GPUs, or on cloud-based streaming systems that collect precise measurements from multiple sensors in different locations inside greenhouse environments."
pub.1139349517,5G based Blockchain network for authentic and ethical keyword search engine,"Abstract The evolution of 4G telecommunication propagated various resource‐crunched clients to experience rate‐effective resources at ease. However, it extends its underlying centralised architecture, which arouses various challenges correlated with network data availability, network information protection, and operational infrastructure charges. With the recent revolution of telecommunication, 5G networks promised to provide credible schemes like the high quality of service, ultra‐low latency, and much security over the pre‐existing architecture. However, the deployment of end‐to‐end 5G network cutting‐edge systems in the present heterogeneous world limits its core idea of extensive data privacy, native interoperability, risk‐free interference, and radio spectrum sharing. Perhaps, to achieve its true capability, improved versions of blockchain technology could be aligned to strengthen various real‐time complex applications at a flourishing rate. One of the multiplexed real‐time enterprise applications is a keyword search engine where the integrity of user data files and keyword searches are bound to come under cyber hackers. On the one hand, it was found that when a 5G‐based blockchain emulated network gets deployed with intact encryption techniques, the entire system facilitates to give reliable, efficient, and risk‐free keyword search over variegated 5G network data and its complex computational calculations. Consequently, the use of blockchain‐based decentralised cloud orchestration scheme at various levels enabled the architecture to remain incorruptible and protects all the confidential files and keywords in a fully controlled file access environment. The results of the simulation kernel shows that proposed architecture which, when combined with blockchain‐based decentralised cloud orchestration network system, justify all the essential characteristics and effectuates the optimal use of 5G network sharing by each network entity."
pub.1163728956,SciNet: Codesign of Resource Management in Cloud Computing Environments,"The rise of distributed cloud computing technologies has been pivotal for the large-scale adoption of Artificial Intelligence (AI) based applications for high fidelity and scalable service delivery. Systematic resource management is central in maintaining optimal Quality of Service (QoS) in cloud platforms and is divided into three fundamental types: resource provisioning, AI model deployment and workload placement. To exploit the synergy among these decision types, it becomes imperative to concurrently design (co-design) the provisioning, deployment and placement decisions for optimal QoS. As users and cloud service providers shift to non-stationary AI-based workloads, frequent decision making imposes severe time constraints on the resource management models. Existing AI-based solutions often optimize decision types independently and tend to ignore the dependencies across various system performance aspects such as energy consumption and CPU utilization, making them perform poorly in large-scale cloud systems. To address this, we propose a novel method, called SciNet, that leverages a co-simulated digital-twin of the infrastructure to capture inter-metric dependencies and accurately estimate QoS scores. To avoid expensive simulation overheads at test time, SciNet trains a neural network based imitation learner that aims to mimic an oracle, which takes optimal decisions based on co-simulated QoS estimates. Offline model training and online decision making based on the imitation learner, enables SciNet to take optimal decisions while being time-efficient. Experiments with real-life AI-based benchmark applications on a public cloud testbed show that SciNet gives up to 48% lower execution cost, 79% higher inference accuracy, 71% lower energy consumption and 56% lower response times compared to the current state-of-the-art methods."
pub.1093184760,Collaborative cloud service model for delivering multimedia content in mCloud,"Cloud environment is an efficient solution for both computing intensive and data intensive applications and it is a technology paradigm that is ideal for deploying a user centered mobile learning environment. Efficient use of cloud based services is a key component of successful task scheduling and resource allocation in cloud computing environment. In this paper, we propose a collaborative cloud service model for supporting the student's mobile learning process. The proposed collaborative model creates groups of students with joint interests collecting the relevant information from the existing social networks. The spirit of the proposed collaborative model comes from different aspects and influencing factors with respect to Quality of Experience (QoE) process. Experimental results gathered from using the OPNET simulator have verified the benefits of the proposed multimedia content adaptation collaborative model."
pub.1139791529,Multi-Layer Latency Aware Workload Assignment of E-Transport IoT Applications in Mobile Sensors Cloudlet Cloud Networks,"These days, with the emerging developments in wireless communication technologies, such as 6G and 5G and the Internet of Things (IoT) sensors, the usage of E-Transport applications has been increasing progressively. These applications are E-Bus, E-Taxi, self-autonomous car, E-Train and E-Ambulance, and latency-sensitive workloads executed in the distributed cloud network. Nonetheless, many delays present in cloudlet-based cloud networks, such as communication delay, round-trip delay and migration during the workload in the cloudlet-based cloud network. However, the distributed execution of workloads at different computing nodes during the assignment is a challenging task. This paper proposes a novel Multi-layer Latency (e.g., communication delay, round-trip delay and migration delay) Aware Workload Assignment Strategy (MLAWAS) to allocate the workload of E-Transport applications into optimal computing nodes. MLAWAS consists of different components, such as the Q-Learning aware assignment and the Iterative method, which distribute workload in a dynamic environment where runtime changes of overloading and overheating remain controlled. The migration of workload and VM migration are also part of MLAWAS. The goal is to minimize the average response time of applications. Simulation results demonstrate that MLAWAS earns the minimum average response time as compared with the two other existing strategies."
pub.1164151990,Dynamic task offloading and collaborative task execution using three tier edge cloud computing (T2EC2) system for autonomous vehicles,"Automobiles have undergone a transformation during the past two decades due to the merger of the electronics and automotive industries. The combination of autos and electronic sensors has resulted in a new generation of vehicles known as autonomous vehicles (AVs). These AVs have a few hundred thousand sensors, producing an enormous amount of raw data for computation. Data from the vehicular network can be offloaded to existing telecommunication infrastructure to address the problem of processing resources. In order to address vehicular network requirements, large-capacity servers deployed in major telecommunications networks are first used to offload resource-intensive tasks. Mobile Cloud Computing (MCC) is a critical enabling technology for 5 G networks, which has a key feature of offloading to divide application tasks into local and cloud server execution components. This paper proposes a novel Three TierEdge cloud computing (T2 EC2) system which uses an Energy-aware Dynamic Task offloading and collaborative task execution algorithm (EA-DTOCTE) for multilayer vehicular cloud computing networks. The EA-DTOCTE algorithm is included in the decision-making engine in the proposed system, which selects whether to offload the task to the remote environment or implement it locally. EA-DTOCTE focuses on consumption of energy by tasks both locally and remotely since its goal is to efficiently and dynamically split the application into tasks and schedule them on local devices and cloud resources. The proposed T2 EC2 has been evaluated in terms of parameters such as energy consumption, completion time, and throughput. Experimental results indicate that the proposed T2EC2 can save up to 28% of system energy consumption compared with other state-of-art techniques."
pub.1023052080,An efficient framework to handle integrated VM workloads in heterogeneous cloud infrastructure,"When a virtual machine migrates to a host, virtual machine placement is used to estimate the suitability of available host. The virtual machine placement algorithm evaluates the performance data for the host, settings of hardware, resource requirements, and host ratings to provide the best placement on host. Generally, the service providers cannot manage or control the cloud infrastructure including operating system, servers, storage and the placement of service components. This lack of consequence is a deterrent to cloud adoption for some customers. This paper presents an approach for customers to handle the integrated virtual machine workloads. The fuzzy decision making approach, integer linear programming and cost factors such as strategic decision, selection of cloud computing services and its types are considered in this paper. The fuzzy quantifiers are used for selecting the number of virtual machines in the environment and it optimizes the demand of the cloud resources in distributed environment. Integer linear programming constraints have been used to enable infrastructure providers to optimize the placement of virtual machines and algorithms to provide service provider effect. The experimental results show that the proposed approach provides better performance than all other existing techniques such as Markov chain model and optimal crossover genetic algorithm."
pub.1045724454,Autonomic Clouds on the Grid,"Computational clouds constructed on top of existing Grid infrastructure have the capability to provide different entities with customized execution environments and private scheduling overlays. By designing these clouds to be autonomically self-provisioned and adaptable to changing user demands, user-transparent resource flexibility can be achieved without substantially affecting average job sojourn time. In addition, the overlay environment and physical Grid sites represent disjoint administrative and policy domains, permitting cloud systems to be deployed non-disruptively on an existing production Grid. Private overlay clouds administered by, and dedicated to the exclusive use of, individual Virtual Organizations are termed Virtual Organization Clusters. A prototype autonomic cloud adaptation mechanism for Virtual Organization Clusters demonstrates the feasibility of overlay scheduling in dynamically changing environments. Commodity Grid resources are autonomically leased in response to changing private scheduler loads, resulting in the creation of virtual private compute nodes. These nodes join a decentralized private overlay network system called IPOP (IP Over P2P), enabling the scheduling and execution of end user jobs in the private environment. Negligible overhead results from the addition of the overlay, although the use of virtualization technologies at the compute nodes adds modest service time overhead (under 10%) to computationally-bound Grid jobs. By leasing additional Grid resources, a substantial decrease (over 90%) in average job queuing time occurs, offsetting the service time overhead."
pub.1122852628,Stability Analysis of a Statistical Model for Cloud Resource Management,"In this paper, we presented a comprehensive stability analysis of statistical models derived from the network usage data to design an efficient and optimal resource management in a Cloud data centre. In recent years, it has been noticed that network has a significant impact on the HPC and business critical applications when they are run in a cloud environment. The existing VM placement algorithms lack capabilities to deploy such applications in an effective way and cause performance degradation. As a result, there is an urge for a network-aware VM placement algorithm which will consider the application behaviour and system capability. Our approach uses static models based on simple probability distribution concept and partition (number theory) to characterise and predict the resource usage behaviour of the VMs. However, the stability of those models is a key requirement to ensure a persistent placement of the VMs which can prevent their frequent migration and keep the infrastructure rigid. The paper investigates the stability of the models with respect to time. Sticky HDP-HMM method was proven highly capable to model the monitoring data with a certain accuracy. The refined data was further used to estimate the resource consumption of each VM and physical host running in the infrastructure. A stability parameter has been defined to determine the level of steadiness of the models that gives us a clear indication on whether the models can be used further to derive an optimal placement decision for new VMs. The paper ends with a discussion on instance based stability analysis and future work."
pub.1095694523,Towards Security as a Service (SecaaS): On the modeling of Security Services for Cloud Computing,"The security of software services accessible via the Internet has always been a crosscutting non-functional requirement of uttermost importance. The recent advent of the Cloud Computing paradigm and its wide diffusion has given birth to new challenges towards the securing of existing Cloud services, by properly accounting the issues related to their delivery models and their usage patterns, and has opened the way to the new concept of Security as a Service(SecaaS), i.e. the ability of developing reusable software services which can be composed with standard Cloud services in order to offer them the suitable security features. In this context, there is a strong need for methods and tools for the modeling of security concerns, as well as for evaluation techniques, for supporting both the comparison of different design choices and the analysis of their impact on the behavior of new services before their actual realization. This paper proposes a meta-model for supporting the modeling of Security Services in a Cloud Computing environment as well as an approach for guiding the identification and the integration of security services within the standard Cloud delivery models. The proposal is exemplified through a case study."
pub.1117614334,Comparison for Confidential Cryptography in Multimedia Cloud Environment,"The working pattern and lifestyle of people has been changed due to wide adoption of cloud computing in daily life. Cloud is a perfect match for the low configured mobile devices. However, security of crucial data at cloud is always an issue of concern for its widespread applications. In this paper we have discussed the existing cloud security concerns for such crucial media data, along with that we have also reviewed our earlier proposed integrated algorithm for performing encryption and decryption while retrieving and storing multimedia data (images, audio and video files) to/from the cloud. As security of sensitive private media data of a client at cloud server is a big question and multimedia data handling always requires special attention. This proposed integrated security algorithm works in contribution towards this. In this paper we have made a detailed analysis of this algorithm and also compared it with some other proposed hybrid algorithms for a secure multimedia cloud computing environment."
pub.1171452792,Early Detection System for Gas Leakage and Fire in Smart Home Using IOT,"Gas leakage is an important environmental, commercial and residential problem that threatens human health, safety and the environment. Traditional gas detection methods often lack the ability to track time and do not provide timely information, leading to accidents and damage. In this context, Internet of Things (IoT) technology has been found to provide a reliable solution for continuous monitoring and remote management of gas detection systems. This paper presents an IoT-based gas leak detection system to improve safety and reduce the risk of gas leakage. leak. The proposed system integrates gas sensors, microcontrollers, communication modules and cloud computing infrastructure to enable real-time monitoring, data analysis and remote monitoring. Gas detectors used in designated areas indicated the presence of hazardous gases, and data was collected directly from the 's central processing unit for analysis. The system provides rapid rescue and intervention by sending instant notifications via mobile applications in case of a gas leak. The main features of the proposed system are scalability, flexibility and interoperability, allowing integration with existing infrastructure and integration of different protocols. Additionally, the use of a cloud-based platform facilitates data storage, analysis and visualization, allowing partners to gain insight intogasleaks, processes and possible consequences"
pub.1175441168,Distributed Edge Cloud Proposal Based on VNF/SDN Environment,"The integration of lightweight virtualization and distributed management techniques effectively addresses various challenges in edge cloud environments, including optimizing service provision and reducing energy consumption. As the ICT sector transitions from traditional cloud computing to edge cloud to enhance proximity, it has adopted hypervisor-based virtualization. The emergence of beyond 5G systems, characterized by the need for autonomous orchestration, decentralized processing, ultra-low latency, and high throughput, necessitates a shift towards a distributed cloud system with autonomous functionalities. This infrastructure enables proactive control and management of network functions, allowing the delivery of services and microservices through a preconfigured setup at the edge. In regions prone to geographical instability, where network integrity can be compromised by natural disasters like earthquakes and tsunamis, establishing a robust, self-managing network is crucial. Such a network should be capable of independently providing services using local resources, functioning autonomously without external reliance, and effectively managing itself. This paper introduces NerveNet-VNF, a concept focusing on container-based virtualization of network functions. This system is coupled with SDN technology and integrated with a Dynamic Load Balancing (D-LB) system within a distributed cloud architecture. This integrated solution enables the hosting, management, and provisioning of local microservice requirements. Additionally, it introduces a new concept for splitting network functionality based on containers compared to the traditional abstract splitting system. Evaluation of this system demonstrates improvements in latency and throughput. Notably, there is a 20% reduction in NerveNet-VNF request execution time compared to the pre-existing NerveNet network. Moreover, integrating the distributed SDN controller technique into the distributed cloud system (HEC) has reduced latency by 88.91% and improved throughput by 28.48%."
pub.1118641787,Towards Adaptive Compliance,"Mission critical software is often required to comply with multiple
regulations, standards or policies. Recent paradigms, such as cloud computing,
also require software to operate in heterogeneous, highly distributed, and
changing environments. In these environments, compliance requirements can vary
at runtime and traditional compliance management techniques, which are normally
applied at design time, may no longer be sufficient. In this paper, we motivate
the need for adaptive compliance by illustrating possible compliance concerns
determined by runtime variability. We further motivate our work by means of a
cloud computing scenario, and present two main contributions. First, we propose
and justify a process to support adaptive compliance that ex- tends the
traditional compliance management lifecycle with the activities of the
Monitor-Analyse-Plan-Execute (MAPE) loop, and enacts adaptation through
re-configuration. Second, we explore the literature on software compliance and
classify existing work in terms of the activities and concerns of adaptive
compliance. In this way, we determine how the literature can support our
proposal and what are the open research challenges that need to be addressed in
order to fully support adaptive compliance."
pub.1005713246,Towards adaptive compliance,"Mission critical software is often required to comply with multiple regulations, standards or policies. Recent paradigms, such as cloud computing, also require software to operate in heterogeneous, highly distributed, and changing environments. In these environments, compliance requirements can vary at runtime and traditional compliance management techniques, which are normally applied at design time, may no longer be sufficient. In this paper, we motivate the need for adaptive compliance by illustrating possible compliance concerns determined by runtime variability. We further motivate our work by means of a cloud computing scenario, and present two main contributions. First, we propose and justify a process to support adaptive compliance that extends the traditional compliance management lifecycle with the activities of the Monitor-Analyse-Plan-Execute (MAPE) loop, and enacts adaptation through re-configuration. Second, we explore the literature on software compliance and classify existing work in terms of the activities and concerns of adaptive compliance. In this way, we determine how the literature can support our proposal and what are the open research challenges that need to be addressed in order to fully support adaptive compliance."
pub.1025110749,Edge caching with mobility prediction in virtualized LTE mobile networks,"Mobile Edge Computing enables the deployment of services, applications, content storage and processing in close proximity to mobile end users. This highly distributed computing environment can be used to provide ultra-low latency, precise positional awareness and agile applications, which could significantly improve user experience. In order to achieve this, it is necessary to consider next-generation paradigms such as Information-Centric Networking and Cloud Computing, integrated with the upcoming 5th Generation networking access. A cohesive end-to-end architecture is proposed, fully exploiting Information-Centric Networking together with the Mobile Follow-Me Cloud approach, for enhancing the migration of content-caches located at the edge of cloudified mobile networks. The chosen content-relocation algorithm attains content-availability improvements of up to 500% when a mobile user performs a request and compared against other existing solutions. The performed evaluation considers a realistic core-network, with functional and non-functional measurements, including the deployment of the entire system, computation and allocation/migration of resources. The achieved results reveal that the proposed architecture is beneficial not only from the users’ perspective but also from the providers point-of-view, which may be able to optimize their resources and reach significant bandwidth savings."
pub.1026746296,An Enhanced Load Balancing Technique for Efficient Load Distribution in Cloud-Based IT Industries,"The advent of technology has led to the emergence of new technologies such as cloud computing. Evolution of IT industry has oriented towards the consumption of large scale infrastructure and development of optimal software products, thereby demanding heavy capital investment by the organizations. Cloud computing is one of the upcoming technologies that have enabled to allocate apt resources on demand in a pay-go approach. However, the existing techniques of load balancing in cloud environment are not efficient in reducing the response time required for processing the requests. Thus, one of the key challenges of the state-of- art of research in cloud is to reduce the response time, which in turn reduces starvation and job rejection rates. This paper, therefore aims to provide an efficient load balancing technique that can reduce the response time to process the job requests that arrives from various users of cloud. An enhanced Shortest Job First Scheduling algorithm, which operates with threshold (SJFST), is used to achieve the aforementioned objective. The simulation results of this algorithm shows the realization of efficient load balancing technique which has resulted in reduced response time leading to reduced starvation and henceforth lesser job rejection rate. This enhanced technique of SJFST proves to be one of the efficient techniques to accelerate the business performance in cloud atmosphere."
pub.1093783910,"End to End Automation On Cloud with Build Pipeline: The case for DevOps in Insurance Industry Continuous Integration, Continuous Testing, and Continuous Delivery","In modern environment, delivering innovative idea in a fast and reliable manner is extremely significant for any organizations. In the existing scenario, Insurance industry need to better respond to dynamic market requirements, faster time to market for new initiatives and services, and support innovative ways of customer interaction. In past few years, the transition to cloud platforms has given benefits such as agility, scalability, and lower capital costs but the application lifecycle management practices are slow with this disruptive change. DevOps culture extends the agile methodology to rapidly create applications and deliver them across environment in automated manner to improve performance and quality assurance. Continuous Integration (CI) and Continuous delivery (CD) has emerged as a boon for traditional application development and release management practices to provide the capability to release quality artifacts continuously to customers with continuously integrated feedback. The objective of the paper is to create a proof of concept for designing an effective framework for continuous integration, continuous testing, and continuous delivery to automate the source code compilation, code analysis, test execution, packaging, infrastructure provisioning, deployment, and notifications using build pipeline concept."
pub.1009239747,IAM Architecture and Access Token Transmission Protocol in Inter-Cloud Environment,"With the adoption of cloud computing, the number of companies that take advantage of cloud computing has increased. Additionally, various of existing service providers have moved their service onto the cloud and provided user with various cloud-based service. The management of user authentication and authorization in cloud-based service technology has become an important issue. This paper introduce a new technique for providing authentication and authorization with other inter-cloud IAM (Identity and Access Management). It is an essential and easy method for data sharing and communication between other cloud users. The proposed system uses the credentials of a user that has already joined an organization who would like to use other cloud services. When users of a cloud provider try to obtain access to the data of another cloud provider, part of credentials from IAM server will be forwarded to the cloud provider. Before the transaction, Access Agreement must be set for granting access to the resource of other Organization. a user can access the resource of other organization based on the control access configuration of the system. Using the above method, we could provide an effective and secure authentication system on the cloud."
pub.1109927992,Evolution of Enterprise Architecture for Digital Transformation,"The digital transformation of our life changes the way we work, learn, communicate, and collaborate. Enterprises are presently transforming their strategy, culture, processes, and their information systems to become digital. The digital transformation deeply disrupts existing enterprises and economies. Digitization fosters the development of IT systems with many rather small and distributed structures, like Internet of Things, Microservices and mobile services. Since years a lot of new business opportunities appear using the potential of services computing, Internet of Things, mobile systems, big data with analytics, cloud computing, collaboration networks, and decision support. Biological metaphors of living and adaptable ecosystems provide the logical foundation for self-optimizing and resilient run-time environments for intelligent business services and adaptable distributed information systems with service-oriented enterprise architectures. This has a strong impact for architecting digital services and products following both a value-oriented and a service perspective. The change from a closed-world modeling world to a more flexible open-world composition and evolution of enterprise architectures defines the moving context for adaptable and high distributed systems, which are essential to enable the digital transformation. The present research paper investigates the evolution of Enterprise Architecture considering new defined value-oriented mappings between digital strategies, digital business models and an improved digital enterprise architecture."
pub.1061659022,On the Serviceability of Mobile Vehicular Cloudlets in a Large-Scale Urban Environment,"Recently, cloud computing technology has been utilized to make vehicles on roads smarter and to offer better driving experience. Consequently, the concept of mobile vehicular cloudlet (MVC) was born, where nearby smart vehicles were connected to provide cloud computing services locally. Existing researches focus on MVC system models and architectures, and no work to date addresses the critical question of what is the potential, i.e., level of local cloud computing service, achievable by MVCs in real-world large-scale urban environments. This issue is fundamental to the practical implementation of MVC technology. Answering this question is also challenging because MVCs operate in highly complicated and dynamic environments. In this paper, we directly address this challenging issue and we introduce the concept of serviceability to measure the ability of an MVC to provide cloud computing service. In particular, we evaluate this measure in practical environments through a real-world vehicular mobility trace of Beijing. Using the time-varying graph model for mobile cloud computing under different scenarios, we find that the serviceability has a relationship with the delay tolerance of the undertaken computational task, which can be described by two characteristic parameters. The evolution of serviceability through a day and the influence of network congestion are also analyzed. We also portray the spatial distribution of the serviceability and analyze the influence of connectivity and mobility in both MVC and vehicle levels. Our observations are valuable to assist designing vehicular cloud computing systems and applications, as well as to help make offloading decisions."
pub.1144770742,Improved Interaction of BIM Models for Historic Buildings with a Game Engine Platform,"For the purpose of the conservation and representation of cultural heritage, the development of a visual interactive environment to provide an integrated application suitable for collecting a wide range of big data information from historic blocks is regarded as an effective solution. In this study, the existing modeling of wooden building information is imported into a game engine as a practical study for the design of a user-friendly interactive interface design, a Unity integration platform is built, a Revit external application is developed, and cloud services architecture is set to solve the transmission and integration of information models between different platforms. In addition to strengthening the future application of historic building information modeling, this visual information-based interactive environment can help enhance communication and subsequent management of repairs, and bring opportunities for sustainable management and diversified applications to the research field of historic building restoration."
pub.1172534756,Resource Allocation Strategies for Cloud-Edge-End Collaboration in Heterogeneous Edge Computing,"With the continuous innovation of information technology, the Internet of Everything (IoE) is gradually being realized, and more IoT application scenarios have increasingly urgent requirements for high-performance computing and low-latency response. To address the bottleneck of insufficient computing power and energy constraints of terminal devices, edge computing can effectively enhance the task processing capability of terminal devices and improve service quality. However, existing edge computing network architectures, which often provide a single edge server computing capability based on terrestrial networks, cannot meet the task offloading requirements of terminal devices in ubiquitous network environments. This paper designs a cloud-edge-end collaboration space-air-ground integration multi-layer edge computing network architecture (CEEC-SAGIN), and based on this architecture, a heterogeneous network system of terminal devices, multi-layer edge computing services and cloud computing services are constructed. Then, a task processing model is constructed with the optimization objective of minimizing the task processing delay of the system, taking into account the task offloading ratio, computational resources and transmission resources involved in the task processing process of the terminal devices of the system. Further, a multi-layer task offloading and resource allocation algorithm (MTORA) is proposed to realize the optimal allocation of resources in system task processing. Finally, simulation experiments show that the CEEC-AGIN architecture proposed in this paper can meet the requirements of more applications, and the proposed model can improve the system task processing efficiency by up to roughly 15% and reduce the system task processing latency by up to approximately 10% compared with other resource allocation strategies."
pub.1093364165,WebLab-Deployer: Exporting Remote Laboratories as SaaS Through Federation Protocols,"During the last decade, remote laboratories have been extensively used as a primary learning tool in many universities around the world. However, today most of the remote laboratories are still only used by the same institution that provides or even develops them, or by direct partners in federated environments. There are two ways to support this type of federation: a) using a federated authentication system such as Shibboleth or b) installing a remote laboratory management system that supports federation natively. In both cases, the consumer institution must go through a process of deployment or complex configuration. This contribution explores providing access to laboratories using a Cloud Computing approach, considering the federated environments that do not have laboratories attached as a SaaS (Software as a Service) system. This approach not only makes adoption by other institutions easier, but also benefits from the existing features provided by Cloud Computing, such as elasticity to reuse the same resources for different institutions to balance the load."
pub.1094704308,Ecosystem of Cloud Naming Systems: an Approach for the Management and Integration of Independent Cloud Name Spaces,"Cloud computing is a highly dynamic environment where resources can be composed with other ones to provide many kinds of services to clients. In such scenario naming and resource location become critical issues and the existing Domain Name System (DNS), considered alone, is not able to address the new emerging problems. A cloud environment offers a variety of concrete and abstracted entities which need to be identified, whose states can frequently change: a virtual resource, could be allocated, deallocated or moved from a context to another. Moreover, a cloud entity could hold one or more names, identifiers, and representations in various cloud contexts where name alterations could frequently occur. In such environment, the management and integration of independent cloud name spaces is then becoming more compelling. This paper aims to propose a cloud naming system able to address such problems, providing an implementation practice in a cloud federation use case."
pub.1174491324,The Best of Both Worlds: A Practical Guide to Hybrid Disaster Recovery for Virtualized Environments with Hyperconverged Infrastructure and Cloud Services,"Hyperconverged infrastructure and cloud services enable hybrid Disaster Recovery (DR) in virtualized settings, protecting data and keeping enterprises operational. This study examines HCI in virtualized settings, emphasizing easier management, more freedom, and better resource consumption. Comparing old and contemporary disaster recovery approaches shows how difficult it is to handle rising data, speedy recovery, and following the regulations. Mixed DR solutions’ adaptability, scalability, and cost-effectiveness are studied. This solution includes cloud-based DR with on-premises HCI. Disaster recovery is discussed in this in-depth look at disasters and corporate operations. Traditional DR is expensive, complicated, and slow to recover. Modern DR issues include rigorous regulations and speedier healing. This study compares recovery time objectives (RTOs) and recovery point objectives (RPOs) with old and new DR systems using graphs to prove that hybrid DR methods are preferable. Implementing a hybrid disaster recovery strategy requires assessment, planning, architecture, deployment, testing, and validation. A resilient DR plan requires regular tests, security safeguards, thorough recording, and constant monitoring. The paper uses graphs and statistics to discuss hybrid DR’s benefits, including better RTO and RPO, savings, and easier management. Cloud computing, threat changes, automation, and orchestration are examined for disaster recovery’s future. AI-driven disaster recovery solutions with superior failure prediction and recovery optimization are predicted to become more prevalent. The paper concludes that proactive DR planning and ongoing adaptation are essential for firms to be resilient and ready for future shocks."
pub.1093245239,Towards a service mediation framework for dynamic applications,"Since recently, it is accepted that mediation can take many forms depending on the application domains: for example, mediation can be seen as a means to deal with inter-operability between legacy applications in large Information Systems in a variety of environments including Cloud or to synchronize, integrate and route data from sensor networks in pervasive applications. In this paper we present a lightweight, modular, dynamic and distributed framework called Cilia dedicated to mediation. Cilia aims at bringing an homogeneous solution to deal with mediation both in big and little systems and to ensure the seamless integration of the two systems. In this paper we demonstrate the benefits of the Cilia approach through a case study from an RFID project."
pub.1165266625,Data Processing Unit's Entry into Confidential Computing,"One of the biggest transformations in the datacenter in the past decade has been the rapid growth of heterogenous computing involving HW accelerators to allow application specific offloads for power and performance efficiencies. There is a major shift from the monolithic servers to dynamically composable systems consisting of disaggregated memory, storage and compute resources to meet the needs of compute intensive applications such as AI/ML and HPC. At the same time, with the increased adoption of confidential computing in the public cloud, the industry has realized the need to extend the security assurance of trusted execution environment (TEE) from the CPU to devices. While this has resulted in the emergence of new security standards such as TDISP (PCI-SIG's TEE Device Interface Security Protocol), the interface security is only useful if the device also implements a TEE. Data Processing Unit (DPU), the next generation of SmartNICs, have a key role in the new datacenter architecture and must provide high-speed networking, efficient data transfer protocols, and new services while meeting the confidential computing workload needs. In this position paper, we explain how the problem of making a TEE capable DPU differs from those of compute accelerators like GPUs and FPGAs. We describe DPU's threat model which include DPU specific threats, discuss the challenges and suggest design considerations that can help in exploring DPU TEE architecture options."
pub.1169015467,"Dynamic Digital Twins: Challenges, Perspectives and Practical Implementation from a City’s Perspective","Digital twins that serve as virtual representations of real-world objects and structures, are used in various applications for urban environments. Challenges for creating and maintaining digital twins involve data acquisition, fusion of heterogeneous data types, AI-based data analysis, and the integration into existing applications and workflows. In this paper, we present the concept and implementation of a dynamic digital twin from a city’s perspective. The concept avoids explicit modeling to simplify the creation of a comprehensive data basis that can be easily updated frequently. 3D point clouds with semantics are used as representations for static objects and structures, such as buildings, infrastructure and subsurface structures. Dynamic aspects are represented through a time series of sensor data to enable real-time monitoring and change detection applications. A centralized data repository for applications, such as infrastructure monitoring, condition assessment, and inventory management represent a basis to support decisions. We present typical use cases and challenges from the perspective of a city and how the dynamic digital twins can create significant added value."
pub.1100806144,An Innovative Cloud-based Supervision System for the Integration of RPAS in Urban Environments," This paper proposes the outline of a Cloud-based supervision system for Remotely Piloted Aircraft Systems (RPAS), which are operating in urban environments. The novelty of this proposed concept is dual: (i) a Cloud-based supervision system focusing on safety and robustness, (ii) the definition of technical requirements allowing the RPAS to fly over urban areas, as a possible evolution of drone use in future smart cities. A new concept for the regulatory issues is also proposed, compared with existing worldwide regulations. The Cloud framework is intended to be an automated system for path planning and control of RPAS flying under its coverage, and not limited to conventional remote control as if supervised by a human pilot. Future works will be based on the experimental validation of the proposed concept in an urban area of Turin (Italy)."
pub.1139795855,SBAP: A Novel Secure Bucket Access Protocol for Leveraging User’s Data Privacy in Cloud Environment,"With large number of inter-connected of the smart appliances over distributed cloud environment, the user's private information is exposed to higher degree of vulnerability. Existing applications that executes over such devices are mandatorily require an access of user's private information in order to authorize the user to experience the privilege of cloud services rendering declination of data privacy. Therefore, proposed system introduces Secure Bucket Access Protocol (SBAP) that not only offers a secure storage of user's private information but also perform lightweight encryption of such data. The contribution of proposed SBAP is the migration of access control policy from device to cloud to offer higher scope of data privacy. The study outcome exhibits faster response time ideal to secure a communication channel over lethal threats in contrast to frequently used encryption scheme."
pub.1137748149,The importance of nature-inspired meta-heuristic algorithms for solving virtual machine consolidation problem in cloud environments,"Nowadays, cloud computing is known as an internet-based modern area among emerging technologies that brings up an environment, in which computing resources such as hardware, software, storage, etc. can be rented by cloud users based on a pay per use model. Since the size of cloud computing is widely expanding and the number of cloud users is also increasing day by day, high energy consumption becomes a serious concern in the operation of complex cloud data centers. In this regards, Virtual Machine (VM) consolidation plays a vital role in utilizing cloud resources in an efficient manner. It migrates the running VMs from overloaded Physical Machines (PMs) to other PMs considering multiple factors, such as migration overhead, energy consumption, resource utilization, and migration time. Since the VM consolidation issue is known as an NP-hard problem, various nature‐inspired meta-heuristic algorithms aiming to solve this problem have been utilized in recent years. However, a lack of systematic and detailed survey study in this field is obvious. Therefore, this gap motivated us to provide the current paper aiming to highlight the role of nature-inspired meta-heuristic algorithms in the VM consolidation problem, review the existing approaches, offer a detailed comparison of approaches based on important factors, and finally, outline the future directions."
pub.1094261630,An Autonomic Framework for Time and Cost Driven Execution of MPI Programs on Cloud Environments,"This paper gives an overview of a framework for making existing MPI applications elastic, and executing them with user-specified time and cost constraints in a cloud frame-work. Considering the limitations of the MPI implementations currently available, we support adaptation by terminating one execution and restarting a new program on a different number of instances. The key component of our system is a decision layer. Based on the time and cost constraints, this layer decides whether to use fewer or a larger number of instances for the applications, and when appropriate, chooses to migrate the application to a different type of instance. Among other factors, the decision layer also models the redistribution costs."
pub.1123046372,Fog‐cloud distributed intrusion detection and cooperation,"Abstract Cloud computing is becoming a promoted technology. Over the recent years, it has rapidly grown and its adoption is more and more increasing. Many enterprizes and organizations have immigrated their data to the cloud. However, the security of this technology is considered a serious barrier to guarantee the organizations' trust on it. Therefore, intrusion detection has been set as the best solution to secure the cloud platform. Thus, many research studies have been developed for the aim to create an effective and performing intrusion detection system suitable for the cloud environment. In this paper, we propose a new approach for cloud environment–based multilevel cooperation in terms of intrusion detection. It is based on the cooperation and the correlation between fog nodes at the beginning and then between fog and cloud technology. We also evaluate the performance and the efficiency of this proposed prototype in terms of detection rates and detection response to make real‐time detection in the whole cloud platform. The results show that the proposed architecture introduces low false positive rate compared to the existing hybrid intrusion detection approach in cloud computing (HIDCC) framework."
pub.1171398737,ShuffleBench: A Benchmark for Large-Scale Data Shuffling Operations with Distributed Stream Processing Frameworks,"Distributed stream processing frameworks help building scalable and reliable applications that perform transformations and aggregations on continuous data streams. This paper introduces ShuffleBench, a novel benchmark to evaluate the performance of modern stream processing frameworks. In contrast to other benchmarks, it focuses on use cases where stream processing frameworks are mainly employed for shuffling (i.e., re-distributing) data records to perform state-local aggregations, while the actual aggregation logic is considered as black-box software components. ShuffleBench is inspired by requirements for near real-time analytics of a large cloud observability platform and takes up benchmarking metrics and methods for latency, throughput, and scalability established in the performance engineering research community. Although inspired by a real-world observability use case, it is highly configurable to allow domain-independent evaluations. ShuffleBench comes as a ready-to-use open-source software utilizing existing Kubernetes tooling and providing implementations for four state-of-the-art frameworks. Therefore, we expect ShuffleBench to be a valuable contribution to both industrial practitioners building stream processing applications and researchers working on new stream processing approaches. We complement this paper with an experimental performance evaluation that employs ShuffleBench with various configurations on Flink, Hazelcast, Kafka Streams, and Spark in a cloud-native environment. Our results show that Flink achieves the highest throughput while Hazelcast processes data streams with the lowest latency."
pub.1158437542,Design of automatic monitoring system for network information security in cloud computing environment,"Aiming at the problems of incomplete monitoring, slow response speed and low accuracy of the existing network information security automatic monitoring system, the paper designs an automatic network information security monitoring system in a cloud computing environment. Based on the overall system architecture, the design of information collection, information transmission and information security early warning modules has realised the acquisition of network information changes, the transmission and integration of network information, and the risk warning of network abnormalities. Using relative protection entropy as the theoretical basis, the network information security threshold under the cloud computing environment is further set, and the automatic monitoring of network information security is realised by judging the threshold risk coefficient. Experimental results show that the system has a high comprehensive monitoring capability, the response speed is within 0.5s, and the accuracy of information monitoring is as high as 99%."
pub.1171379877,Federated Heterogeneous Compute and Storage Infrastructure for the PUNCH4NFDI Consortium,"PUNCH4NFDI, funded by the Germany Research Foundation initially for five years, is a diverse consortium of particle, astro-, astroparticle, hadron and nuclear physics embedded in the National Research Data Infrastructure initiative. In order to provide seamless and federated access to the huge variety of compute and storage systems provided by the participating communities covering their very diverse needs, the Compute4PUNCH and Storage4PUNCH concepts have been developed. Both concepts comprise state-of-the-art technologies such as a token-based AAI for standardized access to compute and storage resources. The community supplied heterogeneous HPC, HTC and Cloud compute resources are dynamically and transparently integrated into one federated HTCondorbased overlay batch system using the COBalD/TARDIS resource meta-scheduler. Traditional login nodes and a JupyterHub provide entry points into the entire landscape of available compute resources, while container technologies and the CERN Virtual Machine File System (CVMFS) ensure a scalable provisioning of community-specific software environments. In Storage4PUNCH, community supplied storage systems mainly based on dCache or XRootD technology are being federated in a common infrastructure employing methods that are well established in the wider HEP community. Furthermore existing technologies for caching as well as metadata handling are being evaluated with the aim for a deeper integration. The combined Compute4PUNCH and Storage4PUNCH environment will allow a large variety of researchers to carry out resource-demanding analysis tasks. In this contribution we will present the Compute4PUNCH and Storage4PUNCH concepts, the current status of the developments as well as first experiences with scientific applications being executed on the available prototypes."
pub.1103239797,An Approach to Prepare Data to Feed Visual Metaphors in a Multiple View Interactive Environment,"This paper presents an approach to prepare data to feed visual metaphors in a multiple view interactive environment. We implemented a tool that supports programmers and users to prepare datasets from different domains to feed visual metaphors. To analyze the effectiveness of the approach, we conducted a case study with the data of the Brazilian National Health System (known as SUS—Sistema Unico de Saude in Portuguese). The results obtained are an initial evidence of the feasibility of the approach that support the preparation of data to a format suitable to the characteristics of visual metaphors. The case study illustrates scenarios in which both programmers and users are able to prepare datasets from different domains to feed visual metaphors that comprise a multiple view interactive visualization infrastructure."
pub.1171477979,CamoNet: On-Device Neural Network Adaptation With Zero Interaction and Unlabeled Data for Diverse Edge Environments,"Deploying deep learning models to edge devices for low-latency and privacy-preserving applications has become a trend. To adapt to heterogeneous devices and data, it is significant to generate customized models. However, existing model adaptation approaches require edge devices to make interactions (collecting hardware information or local data) with the cloud, which raises privacy concerns, increases communication costs, and burdens the cloud. By contrast, we propose CamoNet, a universal on-device model adaptation framework with zero interaction between devices and the cloud. In CamoNet, a lightweight on-device neural architecture search module is utilized to quickly generate a customized model for subsequent on-device training, followed by an on-device contrastive transfer learning module to effectively leverage unlabeled data for fine-tuning the customized model. Extensive experimental results show that CamoNet can effectively run on various edge devices. Compared with the SOTA model adaptation approaches, CamoNet achieves significant accuracy improvement by 25.2% on average for image classification, 10.1% on average for object detection, and reduces the training memory by 4.8-11.4×. We will open-source our models and tools for edge AI developers."
pub.1145900523,LISP-Based Control Plane for Service Connectivity in Multi-Cluster Cloud Systems,"In mobile networks, the edge cloud environment has emerged to provide various services to users. When the mobile user moves to another location while connecting the edge cloud service, to maintain an optimal service path, it is necessary to migrate the service to a new edge cloud close to the location where the user is newly connected. Several methods have been proposed to integrate edge cloud service migration technology with existing mobility management protocols in mobile networks, and a location/ID separation protocol (LISP)-based service optimal path management method between edge clouds was recently proposed. However, for real implementation of edge cloud with existing platform specified to containerized infrastructure, where the address of service is allocated randomly and locally, it is hard to discover appropriate service from the external client as well as maintain service connectivity when service location is changed. To solve that, this paper proposed the LISP-based service ID management system which provides uniform access for equivalent services running on different edge clouds. To apply the proposed ID management system on real distributed edge cloud systems, the mobility management and edge cloud networking systems were integrated as shown in the testbed implementation. This shows the possibility of combining the isolated mobility management for mobile users and internal identification of edge services using LISP for reducing the interruption and delay of edge service for mobile users. As a result of the experiments conducted on a real testbed, our proposed system was verified to enable a change in the routing path while maintaining a single service ID between different edge clouds, and the delay time for path reconfiguration is reduced compared to the existing method."
pub.1121893509,The NebulaStream Platform: Data and Application Management for the Internet of Things,"The Internet of Things (IoT) presents a novel computing architecture for data
management: a distributed, highly dynamic, and heterogeneous environment of
massive scale. Applications for the IoT introduce new challenges for
integrating the concepts of fog and cloud computing as well as sensor networks
in one unified environment. In this paper, we highlight these major challenges
and outline how existing systems handle them. To address these challenges, we
introduce the NebulaStream platform, a general purpose, endto-end data
management system for the IoT. NebulaStream addresses the heterogeneity and
distribution of compute and data, supports diverse data and programming models
going beyond relational algebra, deals with potentially unreliable
communication, and enables constant evolution under continuous operation. In
our evaluation, we demonstrate the effectiveness of our approach by providing
early results on partial aspects."
pub.1141385764,Recent Advances Delivered in Mobile Cloud Computing's Security and Management Challenges,"Mobile cloud computing provides an opportunity to restrict the usage of huge hardware infrastructure and to provide access to data, applications, and computational power from every place and in any time with the use of a mobile device. Furthermore, MCC offers a number of possibilities but additionally creates several challenges and issues that need to be addressed as well. Through this work, the authors try to define the most important issues and challenges in the field of MCC technology by illustrating the most significant works related to MCC during recent years. Regarding the huge benefits offered by the MCC technology, the authors try to achieve a more safe and trusted environment for MCC users in order to operate the functions and transfer, edit, and manage data and applications, proposing a new method based on the existing AES encryption algorithm, which is, according to the study, the most relevant encryption algorithm to a cloud environment. Concluding, the authors suggest as a future plan to focus on finding new ways to achieve a better integration MCC with other technologies."
pub.1113978652,RF-IoT: A Robust Framework to Optimize Internet of Things (IoT) Security Performance,"Abstract
The current development in data-driven technology has significantly standardized the software defined networking aspects for various cloud enabled streamlined applications. The enormous growth in the statistics associated with the rate of connected devices over computer networks has led to the origins of ides and conceptual notions behind Internet-of-things. The evolution of IoT applications and their potential aspects is witnessed since last 5–6 years, which has made it an active area of research. The diverse and dynamic user requirement instances through on-line services supported with software defined networks (SDN) also leverages the operational cost of IoT. However, the collaboration of different wireless networking components has made IoT vulnerable to different types of security attacks while operated with SDN. The study has introduced a robust security framework namely (Framework for Internet of Things Security) abbreviated as RF-IoT to address security loopholes in IoT networking environment. The secure mechanism in the context of SDN targeted to resist maximum possible attacks in IoT. A numerical simulation environment in created to validate the performance of the RF-IoT in contrast with existing baseline."
pub.1047777765,Dynamic Job Scheduling Using Ant Colony Optimization for Mobile Cloud Computing,"Cloud computing has been considered as one of the important computing paradigm. Its main purpose is to share computing resources. With the current scenario there is no doubting the incredible impact that mobile technologies have had on both in scientific and commercial applications. The integration of emerging cloud computing concept and the potential mobile communication services is together considered as Mobile Cloud Computing (MCC). A prominent challenge by using mobile devices and the mobile cloud [1] is resource constraints of these handheld devices. The computational complexities in mobile devices compared to the desktop computers are due to its smaller screen size, less memory capacity, lower processing capacity and low battery backup. Due to these resource limitations most of the processing and data handlings are carried out in the cloud, which is known as software as a service (SaaS) cloud. The smart phones are used to access could resources by using the browser. Performance of this mobile cloud is impaired by the time varying characteristics such as, latency, jitter and bandwidth of the wireless channel. In this research we propose a modified task scheduling mechanism called Ant Colony Optimization (ACO) to address the issues related to the performance of mobile devices [5] when used in a cloud environment and Hadoop. However there are bottlenecks related to the existing task scheduling techniques in MCC model which uses the built in FIFO algorithm for large amount of tasks. The proposed Ant Colony Optimization algorithm improve the task scheduling process by dynamically scheduling the tasks and improve the throughput and quality of service (QoS) of MCC."
pub.1169524643,ShuffleBench: A Benchmark for Large-Scale Data Shuffling Operations with Distributed Stream Processing Frameworks,"Distributed stream processing frameworks help building scalable and reliable
applications that perform transformations and aggregations on continuous data
streams. This paper introduces ShuffleBench, a novel benchmark to evaluate the
performance of modern stream processing frameworks. In contrast to other
benchmarks, it focuses on use cases where stream processing frameworks are
mainly employed for shuffling (i.e., re-distributing) data records to perform
state-local aggregations, while the actual aggregation logic is considered as
black-box software components. ShuffleBench is inspired by requirements for
near real-time analytics of a large cloud observability platform and takes up
benchmarking metrics and methods for latency, throughput, and scalability
established in the performance engineering research community. Although
inspired by a real-world observability use case, it is highly configurable to
allow domain-independent evaluations. ShuffleBench comes as a ready-to-use
open-source software utilizing existing Kubernetes tooling and providing
implementations for four state-of-the-art frameworks. Therefore, we expect
ShuffleBench to be a valuable contribution to both industrial practitioners
building stream processing applications and researchers working on new stream
processing approaches. We complement this paper with an experimental
performance evaluation that employs ShuffleBench with various configurations on
Flink, Hazelcast, Kafka Streams, and Spark in a cloud-native environment. Our
results show that Flink achieves the highest throughput while Hazelcast
processes data streams with the lowest latency."
pub.1095073990,Environmentally Opportunistic Computing Transforming the Data Center for Economic and Environmental Sustainability,"The United States Environmental Protection Agency forecasts the 2011 national IT electric energy expenditure will grow toward $7.4 billion [1]. In parallel to economic IT energy concerns, the general public and environmental advocacy groups are demanding proactive steps toward sustainable green processes. Our contribution to the solution of this problem is Environmentally Opportunistic Computing (EOC). Our Green Cloud EOC prototype serves as an operational demonstration that IT resources can be integrated with the dominate energy footprint of existing facilities and dynamically controlled to balance process throughput, thermal transfer, and available cooling via process management and migration. The Green Cloud is a sustainable computing technology that complements existing efficiency improvements at the application, operating system and hardware levels. Exhaust heat energy is transferred directly to an adjacent greenhouse facility and cooling is provided by free cooling methods. We will describe the architecture and operation of this successful prototype that has led to its growing use in our production environments."
pub.1133450589,The Virtual Observatory ecosystem facing the European Open Science Cloud,"The International Virtual Observatory Alliance (IVOA) has developed and built, in the last two decades, an ecosystem of distributed resources, interoperable and based upon open shared technological standards. In doing so the IVOA has anticipated, putting into practice for the astrophysical domain, the ideas of FAIR-ness of data and service resources and the Open-ness of sharing scientific results, leveraging on the underlying open standards required to fill the above. In Europe, efforts in supporting and developing the ecosystem proposed by the IVOA specifications has been provided by a continuous set of EU funded projects up to current H2020 ESCAPE ESFRI cluster. In the meantime, in the last years, Europe has realised the importance of promoting the Open Science approach for the research communities and started the European Open Science Cloud (EOSC) project to create a distributed environment for research data, services and communities. In this framework the European VO community, had to face the move from the interoperability scenario in the astrophysics domain into a larger audience perspective that includes a cross-domain FAIR approach. Within the ESCAPE project the CEVO Work Package (Connecting ESFRI to EOSC through the VO) has one task to deal with this integration challenge: a challenge where an existing, mature, distributed e-infrastructure has to be matched to a forming, more general architecture. CEVO started its works in the first months of 2019 and has already worked on the integration of the VO Registry into the EOSC e-infrastructure. This contribution reports on the first year and a half of integration activities, that involve applications, services and resources being aware of the VO scenario and compatible with the EOSC architecture. Within the H2020 ESCAPE project, the ""CEVO"" WP has one task to deal with this challenge of integrating an existing, mature, distributed e-infrastructure to a forming, more general one. CEVO has already worked on the integration of the VO Registry into the EOSC e-infrastructure. This contribution reports on the full first year of integration acitivities."
pub.1124430432,MQTT-based Middleware for Container Support in Fog Computing Environments,"Distributed architectures where the Internet of Things (IoT) and the cloud are efficiently integrated play an increasingly important role for IoT solutions. Among these architectures, there is a growing interest in the ones that support the opportunity of functionality offloading towards either intermediate fog nodes or IoT end devices. Relevant existing research work has mainly focused so far on virtual machine and container migration to intermediate fog nodes and on migration of very simple functions to IoT endpoints (to preserve their limited resources available). In this paper, we originally concentrate on the gap associated with benefitting from fog functionality at resource-powerful IoT endpoints, to create a continuum deployment that glues IoT devices and the cloud. In particular, this paper originally presents a middleware that manages application deployment and life-cycle by simplifying and optimizing management operations such as device configuration and application constraint satisfaction. The proposed solution particularly fits highly articulated scenarios with large numbers of IoT devices and intermediate fog nodes, by supporting the opportunity to offload functionality in a split way between IoT endpoints and edge nodes. The reported experimental results confirm the feasibility of our approach in term of overhead, scalability, and application life-cycle management."
pub.1044736420,Integration of the gUSE/WS-PGRADE and InSilicoLab portals with DIRAC,"The gUSE (Grid User Support Environment) framework allows to create, store and distribute application workflows. This workflow architecture includes a wide variety of payload execution operations, such as loops, conditional execution of jobs and combination of output. These complex multi-job workflows can easily be created and modified by application developers through the WS-PGRADE portal. The portal also allows end users to download and use existing workflows, as well as executing them. The DIRAC framework for distributed computing, a complete Grid solution for a community of users needing access to distributed computing resources, has been integrated into the gUSE/WS-PGRADE system. This integration allows the execution of gUSE workflows in a distributed computing environment, thus greatly expanding the capability of the portal to several Grids and Cloud Computing facilities. The main features and possibilities of the gUSE/WS-PGRADE-DIRAC system, as well as the benefits for users, will be outlined and discussed."
pub.1121232912,User-Centric Interference-Aware Load Balancing for Cloud-Deployed Applications,"VMs deployed in cloud environments are prone to performance interference due to dynamic and unpredictable contention for shared physical resources among colocated tenants. Current provider-centric solutions, such as careful co-scheduling of VMs and/or VM migration, require a priori profiling of customer VMs, which is infeasible in public clouds. Further, such solutions are not always aware of the user's SLO requirements or application bottlenecks. This paper presents DIAL, an interference-aware load balancing framework that can directly be employed by cloud users without requiring any assistance from the provider. The key idea behind DIAL is to infer the demand for contended resources on the physical hosts, which is otherwise hidden from users. Estimates of the colocated load are then used to dynamically shift load away from compromised VMs without violating the application's tail latency SLOs. We implement DIAL for web and online analytical processing applications, and show, via experimental results on OpenStack and AWS clouds, that DIAL can reduce tail latencies by as much as 70 percent compared to existing solutions."
pub.1121430225,Recent Advances Delivered in Mobile Cloud Computing's Security and Management Challenges,"Mobile cloud computing provides an opportunity to restrict the usage of huge hardware infrastructure and to provide access to data, applications, and computational power from every place and in any time with the use of a mobile device. Furthermore, MCC offers a number of possibilities but additionally creates several challenges and issues that need to be addressed as well. Through this work, the authors try to define the most important issues and challenges in the field of MCC technology by illustrating the most significant works related to MCC during recent years. Regarding the huge benefits offered by the MCC technology, the authors try to achieve a more safe and trusted environment for MCC users in order to operate the functions and transfer, edit, and manage data and applications, proposing a new method based on the existing AES encryption algorithm, which is, according to the study, the most relevant encryption algorithm to a cloud environment. Concluding, the authors suggest as a future plan to focus on finding new ways to achieve a better integration MCC with other technologies."
pub.1137356926,Workflow-as-a-Service Cloud Platform and Deployment of Bioinformatics Workflow Applications,"Workflow management systems (WMSs) support the composition and deployment of workflow-oriented applications in distributed computing environments. They hide the complexity of managing large-scale applications, which includes the controlling data pipelining between tasks, ensuring the application’s execution, and orchestrating the distributed computational resources to get a reasonable processing time. With the increasing trends of scientific workflow adoption, the demand to deploy them using a third-party service begins to increase. Workflow-as-a-service (WaaS) is a term representing the platform that serves the users who require to deploy their workflow applications on third-party cloud-managed services. This concept drives the existing WMS technology to evolve toward the development of the WaaS cloud platform. Based on this requirement, we extend CloudBus WMS functionality to handle the workload of multiple workflows and develop the WaaS cloud platform prototype. We implemented the Elastic Budget-constrained resource Provisioning and Scheduling algorithm for Multiple workflows (EBPSM) that is capable of scheduling multiple workflows and evaluated the platform using two bioinformatics workflows. Our experimental results show that the platform is capable of efficiently handling multiple workflows execution and gaining its purpose to minimize the makespan while meeting the budget."
pub.1174242206,Strategic approaches for successful digital transformation in project management across industries,"Digital transformation has become imperative for organizations across industries seeking to enhance operational efficiency, innovate customer experiences, and maintain competitive advantage in a rapidly evolving digital landscape. This paper explores strategic approaches that enable successful digital transformation in project management, addressing key challenges and leveraging opportunities for growth and sustainability. In the current business environment, digital transformation in project management encompasses the adoption of advanced technologies, integration of digital tools, and reengineering of processes to optimize project delivery and outcomes. Organizations are increasingly leveraging cloud computing, artificial intelligence, big data analytics, and Internet of Things (IoT) to streamline project workflows, enhance decision-making capabilities, and facilitate real-time collaboration among stakeholders. Successful digital transformation initiatives in project management are underpinned by strategic planning and leadership commitment. Alignment of digital transformation goals with organizational objectives ensures clarity of purpose and enhances stakeholder engagement throughout the transformation journey. Moreover, fostering a culture of innovation and continuous learning enables teams to adapt to technological advancements and embrace change effectively. Challenges in achieving successful digital transformation include legacy system integration, data security concerns, skill gaps among team members, and resistance to cultural change. Addressing these challenges requires proactive risk management, investment in training and development programs, and collaboration with technology partners to navigate complexities and ensure seamless implementation. Case studies illustrate diverse approaches to digital transformation in project management, highlighting best practices and lessons learned across various industries. Organizations that successfully navigate digital transformation not only achieve operational efficiencies and cost savings but also create value through enhanced customer experiences and improved competitive positioning. Looking forward, the strategic adoption of emerging technologies and agile methodologies will continue to shape the future of digital transformation in project management. Organizations must remain adaptable and responsive to market dynamics, leveraging digital tools to drive innovation, accelerate time-to-market, and sustain long-term growth in an increasingly digital-first world. In conclusion, strategic approaches to digital transformation in project management empower organizations to harness the full potential of digital technologies, driving efficiencies, fostering innovation, and achieving sustainable success across industries."
pub.1152064561,Architecture for Fault Tolerance in Mobile Cloud Computing using Disease Resistance Approach,"The mobile cloud computing (MCC) is one of the emerging fields in the distributed computing. MCC is an integration of both mobile computing and cloud computing. The limitations of the mobile devices are storage, battery and processing proficiency.These sensitive characteristics of mobile devices can be effectively handled with the introduction of cloud computing. The increasing functionality of the cloud and complexity of the applications causes resource failures in the cloud computing and it reduces the overall performance of the MCC environment. On the other hand, the existing approaches for resource scheduling in MCC proposed several architectures and they are only concentrated on the allocation of resources. The existing architectures are lack of fault tolerance mechanism to handle the faulty resources. To overcome the issues stated above, this paper proposes architecture for fault tolerance in MCC using Disease Resistance approach (DRFT). The main aim of the DRFT approach is to effectively handle the faultyVMs in the MCC. This DRFT approach utilizes the human disease resistance mechanism which is used as materials and methods in the proposed model. The DRFT is capable of identifying the faulty virtual machines and reschedules the tasks to the identified suitable virtual machines. This procedure ultimately leads to minimization of makespan value and it improves the overall performance of the scheduling process. To validate the effectiveness of the proposed approach, a series of simulations has been carried out using CloudSim simulator. The performance of the proposed DRFT approach is compared with the Dynamic group based fault tolerance approach (DGFT-approach). The makespan value of DRFT is reduced to 7% and the performance of DRFT is increased when compare to the DGFT approach. The experimental results show the effectiveness of the proposed approach."
pub.1035654562,CIPRNet Training Lecture: Hybrid Simulation of Distributed Large-Scale Critical Infrastructures,"Modern critical infrastructures represent the pivotal assets upon which the current society greatly relies to support welfare, economy, and quality of life. Nowadays, the trend is to re-organize these infrastructures by applying a System of Systems concept, where the sparse islands are progressively interconnected by means of proper middleware solutions through local or wide-area networks. The huge complexity of such systems makes the integration task among components extremely challenging. Indeed, it may introduce unexpected system behaviors, mainly affecting dependability and performance, that usually become evident only during systems operations and, in particular, in presence of stress or unexpected conditions. Additionally, as they cannot be detected earlier, these problems require complex on-site operations resulting in increased maintenance costs and overspending in terms of personnel resources. A promising way to cope with these new complex systems and to reduce maintenance costs, is to reproduce such distributed systems locally, and let them run prior to the actual execution on-site, in order to get knowledge about their real behavior and define mitigation means and improvement actions. On the other hand, the evaluation of this systems requires sophisticated modeling, simulation, and experimentation infrastructure, which needs the integration of existing simulation environments, real sub-systems, and experimental platforms, which have to interact in a coordinated way. Therefore, hybrid and distributed simulation strategies, supported by novel technologies for resources virtualization and working environment reproduction, represent the most promising way to define the needed strategies to actually support such complex paradigms [1, 2]."
pub.1094272100,Towards Accessible Integration and Deployment of Formal Tools and Techniques,"Computer science researchers in the programming languages and formal verification communities, among others, have produced a variety of automated assistance and verification tools and techniques for formal reasoning. While there have been notable successes in utilizing these tools on the development of safe and secure software and hardware, these leading-edge advances remain largely underutilized by large populations of potential users that may benefit from them. In particular, we consider researchers, instructors, students, and other end users that may benefit from instant feedback from lightweight modeling and verification capabilities when exploring system designs or formal arguments. We describe Aartifact, a supporting infrastructure that makes it possible to quickly and easily assemble interacting collections of small domain-specific languages, as well as translations between those languages and existing tools (e.g., Alloy, SPIN, Z3) and techniques (e.g., evaluation, type checking, congruence closure); the infrastructure also makes it possible to compile and deploy these translators in the form of a cloud-based web application with an interface that runs inside a standard browser. This makes more manageable the process of exposing a limited, domain-specific, and logistically accessible subset of the capabilities of existing tools and techniques to end users. This infrastructure can be viewed as a collection of modules for defining interfaces that turn third-party formal modeling and verification tools and techniques into plug-ins that can be integrated within web-based interactive formal reasoning environments."
pub.1149672076,Container-based Service Relocation for Beyond 5G Networks,"<p>With the advent of 5G networks, various research on Multi-access Edge Computing (MEC) to provide high-reliability and ultra-low latency services are being actively conducted. MEC is an intelligent service distributed cloud technology that provides a high level of personal services by deploying cloud servers to edge networks physically closed to users. However, there is a technical issue to be solved, e.g., the service being used by a user does not exist in the new edge network, and there may even be situations in which the service cannot be provided in the new edge network. To address this, the service application must be relocated according to the location of the user’s movement. Various research works are underway to solve this service relocation issue, e.g., cold/live migration studies have been carried in legacy cloud environments. In this paper, we propose a container migration technique that guarantees a smooth service application relocation for mobile users. We design scenarios for adaptive handoff and describe the detailed operation process. In addition, we present our MEC testbed, which has been used to experiment our container migration technique.</p> <p> </p>"
pub.1124548147,Optimization of Microservice Composition Based on Artificial Immune Algorithm Considering Fuzziness and User Preference,"Microservices is a new paradigm in cloud computing that separates traditional monolithic applications into groups of services. These individual services may correlate or cross multi-clouds. Compared to a monolithic architecture, microservices are faster to develop, easier to deploy, and maintain by leveraging modern containers or other lightweight virtualization. To satisfy the requirements of end-users and preferences, appropriate microservices must be selected to compose complicated workflows or processes from within a large space of candidate services. The microservice composition should consider several factors, such as user preference, correlation effects, and fuzziness. Due to this problem being NP-hard, an efficient metaheuristic algorithm to solve large-scale microservice compositions is essential. We describe a microservice composition problem for multi-cloud environments that considers service grouping relations and corresponding correlation effects of the service providers within intra- or inter-clouds. We use the triangular fuzzy number to describe the uncertainty of QoS attributes, the improved fuzzy analytic hierarchy process to calculate multi-attribute QoS, construct fuzzy weights related to user preferences, and transform the multi-optimal problem into a single-optimal problem. We propose a new artificial immune algorithm based on the immune memory clone and clone selection algorithms. We also introduce several optimal strategies and conduct numerical experiments to verify effects and efficiencies. Our proposed method combines the advantages of monoclone, multi-clone, and co-evolution, which are suitable for the large-scale problems addressed in this paper."
pub.1026018362,Elite: Automatic Orchestration of Elastic Detection Services to Secure Cloud Hosting,"Intrusion detection on today’s cloud is challenging: a user’s application is automatically deployed through new cloud orchestration tools (e.g., OpenStack Heat, Amazon CloudFormation, etc.), and its computing resources (i.e., virtual machine instances) come and go dynamically during its runtime, depending on its workloads and configurations. Under such a dynamic environment, a centralized detection service needs to keep track of the state of the whole deployment (a cloud stack), size up and down its own computing power and dynamically allocate its existing resources and configure new resources to catch up with what happens in the application. Particularly in the case of anomaly detection, new application instances created at runtime are expected to be protected instantly, without going through conventional profile learning, which disrupts the operations of the application.To address those challenges, we developed Elite, a new elastic computing framework, to support high-performance detection services on the cloud. Our techniques are designed to be fully integrated into today’s cloud orchestration mechanisms, allowing an o rdinary cloud user to requ est a detection service and specify its parameters conveniently, through the cloud-formation file she submits for deploying her application. Such a detection service is supported by a high-performance stream-processing engine, and optimized for concurrent analysis of a large amount of data streamed from application instances and automatic adaptation to different computing scales. It is linked to the cloud orchestration engine through a communication mechanism, which provides the runtime information of the application (e.g., the types of new instances created) necessary for the service to dynamically configure its resources. To avoid profile learning, we further studied a set of techniques that enable reuse of normal behavior profiles across different instances within one user’s cloud stack, and across different users (in a privacy-preserving way). We evaluated our implementation of Elite on popular web applications deployed over 60 instances. Our study shows that Elite efficiently shares profiles without losing their accuracy and effectively handles dynamic, intensive workloads incurred by these applications."
pub.1149471305,SODALITE in Context,"This chapter will look at the application and integration of the SODALITE toolkit across various platforms in the Cloud, Edge, and HPC domains, with a specific focus on OpenStack, AWS EC2, Kubernetes, and PBS Torque/Slurm. While Chap. 7 provides a deep dive into the operational environment of the individual use cases, this chapter will focus more on general application and deployment patterns of SODALITE alongside existing deployments, demonstrating ways in which SODALITE can be leveraged more generally by SODALITE users in these respective environments."
pub.1092063336,Price forecasting for spot instances in Cloud computing,"Big data applications usually need to rent a large number of virtual machines from Cloud computing providers. As a result of the policies employed by Cloud providers, the prices of spot virtual machine instances behavior stochastically. Spot prices (prices of spot instances) fluctuate greatly or have multiple regimes. Choosing virtual machines according to trends in prices is helpful in decreasing the resource rental cost. Existing price prediction methods are unable to accurately predict prices in these environments. As a result, a dynamic-ARIMA and two markov regime-switching autoregressive model based forecasting methods have been developed in this paper. Experimental results show that the proposals are better than the existing MonthAR in most scenarios."
pub.1135786231,Optimized PSO-EFA Algorithm for Energy Efficient Virtual Machine Migrations,"The expansion of cloud infrastructure follows with increase in number of data centers hosting number of computing nodes and then, it becomes the reason for huge amount of energy consumption across the world. However, benefits of cloud computing industry with its low-price and high productivity keep diverting the attention of organizations from environmental mess and high energy cost incurred by the data centers. Therefore, it becomes very urgent to curtail the increase in requirement of energy for cloud service providers with the provision of sufficient quality of service to end users. The best way to achieve the balance between energy usage and quality of service is workload aware energy efficient Virtual Machine (VM) consolidation. The various parameters are managed to strike the trade-off between energy consumption and cloud services. This paper presents the optimized PSO-EFA algorithm for energy efficiency with workload management in terms of number of migrations and number of systems shut down during migration process of consolidation. This study paved the way forward for energy efficient cloud environment during migration process. The simulation conducted in constrained environment indicated that workload variation has significant impact on different energy consumption allied parameters. The PSO-EFA algorithm outperformed existing base algorithm for energy consumption and other parameters. The proposed algorithm worked in sync with sustainability efforts."
pub.1111907615,A framework for harmonizing internet of things (IoT) in cloud: analyses and implementation,"Internet of Things (IoT) refers to uniquely identifiable entities. Its vision is the world of connected objects. Due to its connected nature the data produced by IoT is being used for different purposes. Since IoT generates huge amount of data, we need some scalable storage to store and compute the data sensed from the sensors. To overcome this issue, we need the integration of cloud and IoT, so that the data might be stored and computed in a scalable environment. Harmonization of IoT in Cloud might be a novel solution in this regard. IoT devices will interact with each other using Constrained Application Protocol (CoAP). In this paper, we have implemented harmonizing IoT in Cloud. We have used CoAP to get things connected to each other through the Internet. For the implementation we have used two sensors, fire detector and the sensor attached with the door which is responsible for opening it. Thus our implementation will be storing and retrieving the sensed data from the cloud. We have also compared our implementation with different parameters. The comparison shows that our implementation significantly improves the performance compared to the existing system."
pub.1090678191,Adaptation and Deployment of PanDA Task Management System for a Private Cloud Infrastructure,"Management of computational infrastructure is a complicated task which, often employs user workloads delivery across multiple clusters. Criteria for such tasks distribution may vary: priority, transport costs, utilization of data, node capabilities, etc.Such process happens to tasks devoted to the simulation and analysis of the results of high-energy physics experiments at CERN. For task distribution on massive data streams obtained during ATLAS experiment, “Production ANd Distributed Analysis system” (PanDA) was developed. It performs management of workloads delivery and execution in a geographically distributed cluster environment. This paper is devoted to the deployment of PanDA server in a private cluster setting.This paper presents architecture and its implementation that allows, to run and embed PanDA system into existing computational solutions. It consists of a container, that isolates PanDA server its dependencies and environment from other system processes and an embedded Web interface which simplifies task management for end-users. In other words, our approach is focused on PanDA system deployment speed up by means of security layer simplification, containerization and stateless client web service implementation. System was tested on a heterogeneous geographically distributed Azure cloud nodes."
pub.1138399207,Data Protection Techniques Over Multi-cloud Environment—A Review,"Data protection is an essential and important process in a multi-cloud environment. Researchers proposed various techniques for data protection. This paper highlights the recent literature review on data protection. The paper showcases various techniques for data protection, i.e., data protection as a service (DPaaS), secure data storage (SCS), live migration, intrusion detection systems (IDS), secret sharing made short, and ESCUDO-CLOUD. It is explored that the machine learning also could be used with existing cloud services like DPaaS which is a dynamic service for protecting data in the cloud. The hybrid or combined approach using machine learning, physical parameter retrieval, and client-based validation is recommended for improvement of various frameworks leveraged across the platforms. In this paper, we analyze various data protection techniques to perform comparative analysis which lead to proposing of a secure client system with code hidden feature in context of multi-cloud environments."
pub.1094284906,TRONE: Trustworthy and Resilient Operations in a Network Environment,"Cloud infrastructures play an increasingly important role for telecom operators, because they enable internal consolidation of resources with the corresponding savings in hardware and management costs. However, this same consolidation exposes core services of the infrastructure to very disruptive attacks. This is indeed the case with monitoring, which needs to be dependable and secure to ensure proper operation of large datacenters and cloud infrastructures. We argue that currently existing centralized monitoring approaches (e.g., relying on a single solution provider, using single point of failure components) represent a huge risk, because a single vulnerability may compromise the entire monitoring infrastructure. In this paper, we describe the TRONE approach to trustworthy monitoring, which relies on multiple components to achieve increased levels of reliance on the monitoring data and hence increased trustworthiness. In particular, we focus on the TRONE framework for event dissemination, on security-oriented diagnosis based on collected events and on fast network adaptation in critical situations based on multi-homing application support. To validate our work, we will deploy and demonstrate our solutions in a live environment provided by Portugal Telecom."
pub.1008693611,Enabling a ubiquitous and cloud manufacturing foundation with field-level service-oriented architecture,"A shift in manufacturing paradigms from centralised to decentralised IT architectures has seen the advancement of cyber physical manufacturing systems towards more interoperable ecosystems with service-oriented architecture (SOA) through ubiquitous and cloud technology. Sociologically decentralised interoperability between systems enables collaboration for local goal satisfaction and global system operation, where synergies can be created by parallel computation and interaction. Economically, SOA technology promotes dynamic reconfigurability, providing manufacturers quick responsiveness to market changes and disturbances, and ultimately enabling enterprises to stay competitive in the marketplace. This reconfigurable nature in decentralised systems is paramount, as entities are no longer bound by strict proprietary peer-to-peer interactive relationships, as data is freely replicable and dispersible throughout a network. Subsequently, from reconfigurability comes the flexibility to provide for environment change, promote system extensibility and allow for the incorporation of legacy technology and the integration of future technology. The focus of this research is to capture the state-of-the-art in field-level SOA approaches in the context of enabling a foundation for ubiquitous and cloud enterprise for manufacturing. The reviewed systems include; the industrial standard OLE for process control–unified architecture (OPC-UA), the emergent universal factory floor communication protocol MTConnect, the National Instrument Shared Variable Engine (SVE) and the data interoperability specification DPWS. The aim of this work is to provide an introduction to SOA systems, modelling techniques and enterprise integration; to provide a comparative topological review of the different field-level manufacturing SOAs; and outline state-of-the-art developments in manufacturing SOA research, contextualised under ubiquitous and cloud enterprise for manufacturing."
pub.1107688713,A Multi-agent System-Based Distributed Intrusion Detection System for a Cloud Computing,"The Cloud security is one of the major obstacles to the adoption of cloud computing services. It requires some solutions such as Intrusion Detection Systems (IDSs) for protecting each user against all malicious. Existing IDS because of lower detection rate and higher false positive rate couldn’t be suitable for a distributed environment such as the cloud. To tackle this problem, we propose a new distributed intrusion detection system based on a multi-agent system to identify and prevent known and unknown attacks in this environment. Carried out experiments demonstrated the performance and efficiency of our proposed system integrated with multi-agent technology."
pub.1093159855,User-Level Runtime Security Auditing for the Cloud,"Cloud computing is emerging as a promising IT solution for enabling ubiquitous, convenient, and on-demand accesses to a shared pool of configurable computing resources. However, the widespread adoption of cloud is still being hindered by the lack of transparency and accountability, which has traditionally been ensured through security auditing techniques. Auditing in cloud poses many unique challenges in data collection and processing (e.g., data format inconsistency and lack of correlation due to the heterogeneity of cloud infrastructures), and in verification (e.g., prohibitive performance overhead due to the sheer scale of cloud infrastructures and need of runtime verification for the dynamic nature of cloud). To this end, existing runtime auditing techniques do not offer a practical response time to verify a wide-range of user-level security properties for a large cloud. In this paper, we propose a runtime security auditing framework for the cloud with special focus on the user-level including common access control and authentication mechanisms e.g., RBAC, ABAC, SSO, and we implement and evaluate the framework based on OpenStack, a widely deployed cloud management system. The main idea towards reducing the response time to a practical level is to perform the costly operations only once, which is followed by significantly more efficient incremental runtime verification. Our experimental results show that runtime security auditing in a large cloud environment is realistic under our approach (e.g., our solution performs runtime auditing of 100,000 users within 500 milliseconds)."
pub.1019897363,Design and implementation of a reliable and cost-effective cloud computing infrastructure: the INFN Napoli experience,"Over the last few years we have seen an increasing number of services and applications needed to manage and maintain cloud computing facilities. This is particularly true for computing in high energy physics, which often requires complex configurations and distributed infrastructures. In this scenario a cost effective rationalization and consolidation strategy is the key to success in terms of scalability and reliability. In this work we describe an IaaS (Infrastructure as a Service) cloud computing system, with high availability and redundancy features, which is currently in production at INFN-Naples and ATLAS Tier-2 data centre. The main goal we intended to achieve was a simplified method to manage our computing resources and deliver reliable user services, reusing existing hardware without incurring heavy costs. A combined usage of virtualization and clustering technologies allowed us to consolidate our services on a small number of physical machines, reducing electric power costs. As a result of our efforts we developed a complete solution for data and computing centres that can be easily replicated using commodity hardware. Our architecture consists of 2 main subsystems: a clustered storage solution, built on top of disk servers running GlusterFS file system, and a virtual machines execution environment. GlusterFS is a network file system able to perform parallel writes on multiple disk servers, providing this way live replication of data. High availability is also achieved via a network configuration using redundant switches and multiple paths between hypervisor hosts and disk servers. We also developed a set of management scripts to easily perform basic system administration tasks such as automatic deployment of new virtual machines, adaptive scheduling of virtual machines on hypervisor hosts, live migration and automated restart in case of hypervisor failures."
pub.1176191337,Revolutionizing Cloud Security: A Novel Framework for Enhanced Data Protection in Transmission and Migration,"This research introduces a novel security framework specifically tailored to enhance data protection during cloud transmission and migration. Our study addresses critical gaps in existing security models by proposing a multi-dimensional system that incorporates advanced encryption techniques, dynamic access control, and continuous security auditing. Notably, this framework excels in ensuring cloud data integrity, confidentiality, and availability—core aspects often compromised under conventional methods. Comparative analysis with existing models in simulated cloud environments reveals that our framework significantly enhances threat detection accuracy, response speed, and resource management efficiency. The findings highlight the system’s capability to reduce security vulnerabilities while optimizing operational overhead, presenting a substantial improvement over traditional security solutions. This innovative approach, marked by improved scalability and flexibility, is poised to revolutionize cloud data security practices across various industries, prompting further research into robust cloud computing security methodologies."
pub.1145705494,Deep Dive on Popular Security Models and Strategies of Cloud Computing,"Cloud computing has been an ever inspiring paradigm transforming people’s lifestyles and working patterns in recent decades. Due to its dynamic provisioning of IT capabilities, cloud computing emerges as a horizon by serving critical applications. But the unavailability of security standards and models makes cloud computing security more puzzling during the transfer of sensitive information across the cloud. Various models such as the multi-tenancy model, risk accumulation model, cloud cube model, and mapping model are some of the existing security approaches of cloud computing. The multi-tenancy model offers multiple applications running on the cloud, in which virtualization isolates viruses, intrusions, and tampered information. Likewise, the risk accumulation model mitigates the inheritance of security risks from SaaS, PaaS to IaaS by satisfying SLA demands, data protection, and compliance. Cloud cube model facilitates figuration description of security attribute, which consists of four model parameters. Each parameter provides security protection at its corresponding levels encompassing interoperability, data protection, and security boundaries. More significantly, the mapping model includes compliance checks, security controls, and ontologies. Although the security models present a lateral view of cloud data protection, some security issues still need to be addressed. Several cloud security strategies are constructed for mitigating the security risks during the migration of businesses to the cloud ecosystem. For instance, secured access of physical and software entities, assessing virtualization security risks, controlling through outsourcing risks, and interoperability are some of the security strategies at the deployment level. Likewise, some of the operational procedures are providing assurance to the business flow, giving alerts proactively, preventing data leakage, notifying security incidents and response, and auditing the security incidents. Some of the security strategies include protecting the data from overspending or misspending on security controls. During security analytics, advanced automation through artificial intelligence techniques has increasingly been receiving attention in recent decades. Typically, authentication through verification and validation can perform security controls over constrained environments. Blockchain models combined with cloud security provide advanced reliability and scalability. Besides, strong security models and strategies need addressing to combat potential hackers. This chapter aims to enlighten some of the novel security models and approaches in cloud computing. This chapter aims to enlighten some of the novel security models and approaches in cloud computing. Traditional access control mechanism comprises three main categories that include discretionary access control mechanism, mandatory access control mechanism, and role-based access control mechanism. The primary purpose of inventing access"
pub.1149692419,[Retracted] Laboratory Virtualization Management Based on Deep Integration of Cloud Desktop,"In order to solve the problems faced in the process of laboratory construction and management, the author proposes a complete laboratory cloud desktop virtualization management platform. The platform combines the school’s existing experimental teaching environment and teaching mode, through actual deployment and online monitoring; it is further verified that the cloud desktop platform not only provides personalized desktop services but also realizes unified management of resources. Experimental results show that the system function tests are all in line with expectations; during the network bandwidth test, the thin client protocol machine is used to access the cloud platform, and the Thunder player is opened to play the movie application, and then, the thin client machine uses RDP to access the cloud platform; when RDP is playing a movie, the traffic is mainly in the range of 1.2~1.6 Mbps, and the network delay is less than 1.3 s. Through research, it is found that the platform enhances students’ learning initiative, improves teaching management level, and has high experimental teaching application value."
pub.1156013487,ECC-reliant secure authentication protocol for cloud server and smart devices in IoT,"The Internet of Things (IoT) designates a network that helps to relate a diversity of heterogeneous devices, various technologies, and other items to the Internet for flexible access and data exchange. Recent smart real-time application design requires the integration of ‘things’ in IoT with cloud infrastructure to offer valuable services to end-users. However, such a combination could raise various security concerns which become the most critical problem nowadays. For protected communication between smart devices interconnected through IoT and cloud servers, authentication becomes one of the crucial security requirements. There exist many strategies specifically for authentic key exchange between smart devices in the IoT environment and the cloud server. But according to the improved Canetti–Krawczyk (xck) rival model which is considered a more appropriate model for evaluating authentication-based security systems, none of the systems is safe and is vulnerable to a variety of assaults. Thus, we explored xck rival model to prove the limitations of the existing approach and presented an Elliptic Curve Cryptographic reliant strategy to overcome such limitations. The soundness and correctness of our approach were evaluated using scyther verification method. The evaluation results confirm that our method is robust and secure under xck model and incurs minimal overhead."
pub.1116668313,Making Sense of Enterprise Apps in Everyday Work Practices,"This paper draws attention to the growing adoption of web and mobile apps in the enterprise, typically supported by digital storage in the cloud. While these developments offer several advantages, they also pose challenges for workers who must make sense of increasingly complex software configurations – with apps accessible from multiple devices (typically supporting different features or capabilities) and used alongside legacy enterprise software. We investigate how workers navigate these environments through a qualitative study of the work practices of employees in an app-enhanced organization. Our findings focus on two sets of practices. The first involves appraising what software programs and software/device combos offer what features, what we refer to as software calculus. The second involves orienting towards data formats and database structures that underlie specific software programs and their interactions with specific devices, what we label data thinking. Building on prior work on the role of materiality in CSCW, our findings set out a call for further attention to the material and lively dimensions of software and the emergent challenges they pose for contemporary work practices."
pub.1103656947,A self-protecting agents based model for high-performance mobile-cloud computing,"Mobile-cloud computing (MCC) allows devices with resource and battery limitations to achieve computation-intensive tasks in real-time. While this new paradigm of computing seems beneficial for real-time mobile computing, existing MCC models mainly rely on keeping full clones of program code at remote sites and unstandardized/uninteroperable environments, hampering wider adoption. Moreover, the security risks arising from offloading data and code to an untrusted platform and the computational overhead introduced by complex security mechanisms stand as deterrents for adoption of MCC at large. In this paper, we present a context-dependent computation-offloading model for MCC, which is based on application segments packed into autonomous agents. This approach only requires isolated execution containers in the cloud to provide a runtime environment for the agents, and minimal involvement of the mobile platform during the computation process. The agents in the proposed model are able to protect themselves from tampering using integrity-checkpointing and an authenticated encryption-based communication mechanism. Experiments with two mobile applications demonstrate the effectiveness of the approach for high-performance, secure MCC."
pub.1106855356,Improving the Performance of Pre-copy Virtual Machine Migration Technique,"Virtualization is the key technology running behind the cloud servers to deploy users services efficiently. So, when a user requests for cloud services, resources are allocated in the form of VM instance. With the tremendous growth in cloud services, the number of applications running over cloud servers is increasing. Therefore, the chances of server overloading and maintenance are quite often. This scenario is handled by live VM migration, that allows migration of VMs from one host to another with minimum disruption to cloud user services. Cloud hypervisors use pre-copy technique to perform the VM migration task. However, it suffers from page-level content redundancy problem, where complete VM memory page is transferred instead of modified contents of the page. Therefore, a technique is proposed which transfers only modified contents of VM memory page. The performance of proposed technique is evaluated with VMs running read-and write-intensive workloads configured on QEMU-KVM virtualized environment. The results obtained demonstrate that for read-intensive workload, compared to existing pre-copy VM migration scheme, our proposed technique minimize downtime by 69.4%, migration time by 73.5%, and reduces network traffic generated by 74.21%. Similarly, for write-intensive workload, downtime, migration time, and network traffic generated are reduced by 47.23, 49.16, and 51.37%. Thus, the proposed technique effectively improves the performance of VM migration scheme."
pub.1154062602,HACM: High Availability Control Method in Container-Based Microservice Applications Over Multiple Clusters,"Emerging cloud-native technologies, such as container runtime and container orchestrator, offer unprecedented agility in developing and running applications, especially when combined with microservice-style architecture. Several commercial Samsung Network products such as Samsung Element Management System (S-EMS), 5G Radio Access Network (RAN) & Core network elements are being redesigned to fit the microservice paradigm. The cloud environment allows enterprises to scale their applications on-demand with minimum cost; however, it is often difficult to use containers without sacrificing the many benefits offered by container technology. S-EMS manages 5G RAN & Core network elements (NEs) deployed nationwide, and systematically stores a huge volume of stateful data per second. Containers are characterized to have an ephemeral state, hence ‘stateful-ness’ aspect of S-EMS makes management more complex. The existing system in a container-based application does not support geo-redundancy where services/data are stateful/state dependent. In this paper, different challenges around geo-redundancy between different independent Kubernetes set up with active and standby modes between the sites where state-dependent data is stored in each site are described. To overcome these challenges, we propose the High Availability Control Method (HACM) - which enables the Kubernetes cluster to be active and standby, where state-dependent data and context-based operations are intrinsically supported by the underlying S-EMS container application. Our approach has been designed to maintain the geo-redundancy philosophy of cloud-native by associating the status of each site using high availability (HA), switching over services based on the health of applications, deciding state when there are conflicts in site state, and the option to auto fallback based on user preference and services are transferred between sites without user intervention with optimized storage that ensures consistency, persistence, reliability, and availability. Through evaluation, we show that HACM with S-EMS can facilitate geo-redundancy HA, while not posing a significant burden on the Cloud."
pub.1151876636,The Effects of High-Performance Cloud System for Network Function Virtualization,"Since ETSI introduced the architectural framework of network function virtualization (NFV), telecom operators have paid more attention to the synergy of NFV and cloud computing. With the integration of the NFV cloud platform, telecom operators decouple network functions from the dedicated hardware and run virtualized network functions (VNFs) on the cloud. However, virtualization degrades the performance of VNF, resulting in violating the performance requirements of the telecom industry. Most of the existing works were not conducted in a cloud computing environment, and fewer studies focused on the usage of enhanced platform awareness (EPA) features. Furthermore, few works analyze the performance of the service function chain on a practical cloud. This paper facilitates the OpenStack cloud with different EPA features to investigate the performance effects of VNFs on the cloud. A comprehensive test framework is proposed to evaluate the verification of functionality, performance, and application testing. Empirical results show that the cloud system under test fulfills the requirements of service level agreement in Rally Sanity testcases. The throughput of OVS-DPDK is up to 8.2 times as high as that of OVS in the performance test. Meanwhile, the hardware-assisted solution, SR-IOV, achieves the throughput at near the line rate in the end-to-end scenario. For the application test, the successful call rate for the vIMS service is improved by up to 14% while applying the EPA features on the cloud."
pub.1031286946,Cloud authorization: exploring techniques and approach towards effective access control framework,"Despite the various attractive features that Cloud has to offer, the rate of Cloud migration is rather slow, primarily due to the serious security and privacy issues that exist in the paradigm. One of the main problems in this regard is that of authorization in the Cloud environment, which is the focus of our research. In this paper, we present a systematic analysis of the existing authorization solutions in Cloud and evaluate their effectiveness against well-established industrial standards that conform to the unique access control requirements in the domain. Our analysis can benefit organizations by helping them decide the best authorization technique for deployment in Cloud; a case study along with simulation results is also presented to illustrate the procedure of using our qualitative analysis for the selection of an appropriate technique, as per Cloud consumer requirements. From the results of this evaluation, we derive the general shortcomings of the extant access control techniques that are keeping them from providing successful authorization and, therefore, widely adopted by the Cloud community. To that end, we enumerate the features an ideal access control mechanisms for the Cloud should have, and combine them to suggest the ultimate solution to this major security challenge — access control as a service (ACaaS) for the software as a service (SaaS) layer. We conclude that a meticulous research is needed to incorporate the identified authorization features into a generic ACaaS framework that should be adequate for providing high level of extensibility and security by integrating multiple access control models."
pub.1138696812,Extending Service-oriented Architectures in Manufacturing towards Fog and Edge Levels,"The further development of today's manufacturing towards an autonomous and automated manufacturing environment has great basic requirements: adaptability of manufacturing equipment and networking, transparency of all systems and their connections, and also security and reliability in data processing and task fulfillment. Common established communication and integration architectures have their limits here. In the future, systems on the office and shop floor must be networked in a reconfigurable manner at any time, whereby logical processes, data formats, and sufficient time and reliability requirements must be implemented. One approach to the flexible linking of different systems is the redesign of current monoliths in production as reconfigurable service-oriented and event-driven architectures. In parallel to the pure reconfigurability, the introduction of Service-oriented Architectures (SoA) often also involves the integration of production systems in global environments such as a factory superordinate cloud, in order to increase transparency across a company. However, known SoA approaches do not meet the special protocol, behavior and reliability requirements of manufacturing environments at control and field level. This work presents the development of an extension of an existing event-driven SoA platform to realize locally deployable protocol-independent and portable communication and integration workflows at the edge of the manufacturing cloud."
pub.1139660692,E-government and digital transformation in Libyan local authorities,"This article reports on e-government in local authorities in Libya, and discusses the issues involved in digital transformation. The study builds upon existing models and frameworks to establish a technology-organisation-process (TOP) maturity model for assessing e-government status in three case studies in Libya, which reveal major problems in adopting e-government in Libya. The current technology deployment remains basic, with inadequate information systems and networks, out of date personal computers and office software, and unreliable access to the internet. Organisational capabilities, skill levels, lack of funding, management support and process inefficiencies are other factors hampering progress in the adoption of e-government. A step-change to digital government that employs emergent technologies such as artificial intelligence, big data, analytics and cloud computing is currently out of reach. The TOP maturity model provides a framework for assessing e-government readiness in a developing world environment and gives a multi-dimensional perspective on local authority capabilities."
pub.1128741505,Evolution of Enterprise Architecture for Intelligent Digital Systems,"Intelligent systems and services are the strategic targets of many current digitalization efforts and part of massive digital transformations based on digital technologies with artificial intelligence. Digital platform architectures and ecosystems provide an essential base for intelligent digital systems. The paper raises an important question: Which development paths are induced by current innovations in the field of artificial intelligence and digitalization for enterprise architectures? Digitalization disrupts existing enterprises, technologies, and economies and promotes the architecture of cognitive and open intelligent environments. This has a strong impact on new opportunities for value creation and the development of intelligent digital systems and services. Digital technologies such as artificial intelligence, the Internet of Things, service computing, cloud computing, blockchains, big data with analysis, mobile systems, and social business network systems are essential drivers of digitalization. We investigate the development of intelligent digital systems supported by a suitable digital enterprise architecture. We present methodological advances and an evolutionary path for architectures with an integral service and value perspective to enable intelligent systems and services that effectively combine digital strategies and digital architectures with artificial intelligence."
pub.1094913709,Beyond Open Source: The Touchdevelop Cloud-Based Integrated Development Environment,"Software engineering tools and environments are migrating to the cloud, enabling more people to participate in programming from many more devices. To study this phenomenon in detail, we designed, implemented and deployed TouchDevelop (www.touchdevelop.com), a cloud-based integrated development environment (CIDE), which has been online for the past three years. TouchDevelop combines a cross-platform browser-based IDE for the creation of mobile+cloud apps, an online program-mer/user community, and an app store. A central feature of TouchDevelop is to track all program edits, versions, runtime information, bugs, as well user comments, questions and feedback in a single cloud-based repository that is available publicly via Web APIs. In this paper, we examine a key feature of TouchDevelop that should be relevant to others creating CIDEs, namely the seamless integration of replicated workspaces, simplified version control and app publishing. An analysis of the TouchDevelop repository shows that this combination of capabilities allows users to easily create new versions of apps from existing apps, make changes to other users' apps, and share their results from a variety of devices, including smartphones, tablets and traditional PCs."
pub.1094478030,Analysis of the Requirements for Offering Industrie 4.0 Applications as a Cloud Service,"Industrie 4.0 introduces a concept of digitalized production by allowing agile and flexible integration of new business models while maintaining manufacturing costs and efficiency at the reasonable level. In addition, cloud computing is one of the IT trends that is used nowadays to offer services on demand from a virtual environment in enterprise and office areas. The use of cloud computing in an industrial automation domain in order to offer on-demand services, such as alarm flood management or control as a service, is a promising solution. This study examines how the cloud-based applications can meet the Industrie 4.0 requirements concerning security, communication, self-configuration, reliability, and asset administration shell. Moreover, research challenges and existing gaps that need further investigation are identified and discussed."
pub.1140322544,Language Support for Secure Software Development with Enclaves,"Confidential computing is a promising technology for securing code and data-in-use on untrusted host machines, e.g., the cloud. Many hardware vendors offer different implementations of Trusted Execution Environments (TEEs). A TEE is a hardware protected execution environment that allows performing confidential computations over sensitive data on untrusted hosts. Despite the appeal of achieving strong security guarantees against low-level attackers, two challenges hinder the adoption of TEEs. First, developing software in high-level managed languages, e.g., Java or Scala, taking advantage of existing TEEs is complex and error-prone. Second, partitioning an application into components that run inside and outside a TEE may break application-level security policies, resulting in an insecure application when facing a realistic attacker. In this work, we study both these challenges. We present JE, a programming model that seamlessly integrates a TEE, abstracting away low-level programming details such as initialization and loading of data into the TEE. JE only requires developers to add annotations to their programs to enable the execution within the TEE. Drawing on information flow control, we develop a security type system that checks confidentiality and integrity policies against realistic attackers with full control over the code running outside the TEE. We formalize the security type system for the JE core and prove it sound for a semantic characterization of security. We implement JE and the security type system, enable Java programs to run on Intel SGX with strong security guarantees. We evaluate our approach on use cases from the literature, including a battleship game, a secure event processing system, and a popular processing framework for big data, showing that we correctly handle complex cases of partitioning, information flow, declassification, and trust."
pub.1173738353,АНАЛІЗ СУЧАСНИХ ТЕНДЕНЦІЙ РОЗРОБКИ ІНФОРМАЦІЙНИХ СИСТЕМ,"The rapid technological development of recent years has significantly affected the field of information systems development. Modern trends in the development of information systems significantly affect the software market. In particular, there is a growing demand for integrated solutions and software that effectively integrates with existing systems. The development of cloud technologies, the use of artificial intelligence, data analytics, Big Data processing technologies, and the expansion of the use of mobile technologies also affect the software market, stimulating innovation and contributing to changes in approaches to the development and delivery of software of various classes of information systems. The material of the article is aimed at the analysis of modern trends in the field of development of information systems of various classes and the identification of the main aspects that determine the direction of development of modeling technologies, design and development of information systems. The purpose of the article is the analysis of modern trends in the development of information systems, the disclosure of their impact on modern business processes, the software market, and the determination of promising directions for the development of modeling technologies, design and development of information systems. Research methods are the analysis of literary sources, statistical data, results of practical use of information systems in various fields. In addition, the influence of the latest technologies, in particular such as artificial intelligence, big data, and cloud technologies, on the development of information systems is analyzed. The novelty of the conducted research is the definition of modern trends and approaches to the development of information systems, which takes into the account not only technical aspects, but also the influence of social, economic and cultural factors on this process. Changes in the requirements for information systems in connection with the growth of digital transformation of modern enterprises are also considered. The conclusion of the conducted research is the need for constant updating and adaptation of enterprises to the modern, rapidly changing information and technological environment. Understanding these trends will enable organizations to effectively use information systems as a tool to achieve their strategic goals."
pub.1167687629,Soft Computing Principles and Integration for Real-Time Service-Oriented Computing,"In recent years, soft computing techniques have emerged as a successful tool to understand and analyze the collective behavior of service-oriented computing software. Algorithms and mechanisms of self-organization of complex natural systems have been used to solve problems, particularly in complex systems, which are adaptive, ever-evolving, and distributed in nature across the globe. What fits more perfectly into this scenario other than the rapidly developing era of Fog, IoT, and Edge computing environment? Service-oriented computing can be enhanced with soft computing techniques embedded inside the Cloud, Fog, and IoT systems. Soft Computing Principles and Integration for Real-Time Service-Oriented Computing explores soft computing techniques that have wide application in interdisciplinary areas. These soft computing techniques provide an optimal solution to the optimization problem using single or multiple objectives. The book focuses on basic design principles and analysis of soft computing techniques. It discusses how soft computing techniques can be used to improve quality-of-service in service-oriented architectures. The book also covers applications and integration of soft computing techniques with a service-oriented computing paradigm. Highlights of the book include: A general introduction to soft computing An extensive literature study of soft computing techniques and emerging trends Soft computing techniques based on the principles of artificial intelligence, fuzzy logic, and neural networks The implementation of service-oriented computing with a focus on service composition and orchestration, quality of service considerations, security and privacy concerns, governance challenges, and the integration of legacy systems The applications of soft computing in adaptive service composition, intelligent service recommendation, fault detection and diagnosis, service level agreement management, and security Such principles underlying service-oriented computing as loose coupling, reusability, interoperability, and abstraction An IoT-based framework for real-time data collection and analysis using soft computing A general introduction to soft computing An extensive literature study of soft computing techniques and emerging trends Soft computing techniques based on the principles of artificial intelligence, fuzzy logic, and neural networks The implementation of service-oriented computing with a focus on service composition and orchestration, quality of service considerations, security and privacy concerns, governance challenges, and the integration of legacy systems The applications of soft computing in adaptive service composition, intelligent service recommendation, fault detection and diagnosis, service level agreement management, and security Such principles underlying service-oriented computing as loose coupling, reusability, interoperability, and abstraction An IoT-based framework for real-time data collection and analysis using soft computi"
pub.1163976916,Secure blockchain assisted Internet of Medical Things architecture for data fusion enabled cancer workflow,"In today’s digital healthcare landscape, numerous clinical institutions collaborate to enhance healthcare quality in a ubiquitous fog and cloud environment. Data fusion plays a vital role in healthcare collaboration, enabling the integration of diverse healthcare sources. The primary advantage is the improvement of healthcare treatments and the availability of services throughout the network. However, despite these benefits, there is room for improvement in addressing various security issues regarding collaboration among clinical healthcare institutions to meet data fusion requirements. The primary challenge lies in processing lung cancer workflow data fusion on heterogeneous nodes while ensuring security in fog cloud networks. As a result, security emerges as a critical issue in the digital healthcare system operating within this ubiquitous environment. We present the secure Blockchain Internet of Medical Things (BIoMT) architecture for lung cancer workflow data fusion processing in fog cloud networks. The BIoMT architecture introduces the Blockchain Data Fusion Secure (BDFS) algorithm framework, which consists of task scheduling and blockchain validation schemes. The study aims to minimize the makespan of the lung workflow tasks based on security and deadline constraints in fog and cloud networks. We consider security at an advanced level, where runtime ransomware attacks are also identified in fog and cloud networks. Simulation results demonstrate that BDFS outperforms all existing BIoMT architectures regarding workflow processing while adhering to the specified constraints. Overall, the BDFS algorithm presented in the BIoMT architecture provides an efficient and secure solution for lung cancer workflow data fusion in fog cloud networks, contributing to the advancement of digital healthcare systems in a ubiquitous environment."
pub.1106130587,QoS in the Cloud Computing: A Load Balancing Approach Using Simulated Annealing Algorithm,"Recently, Cloud computing has known a fast growth in term of applications and the end users. In addition to the growth and evolution of the Cloud environment, many challenges that impact the performances of the Cloud applications emerged. One of these challenges is the Load Balancing between the virtual machines of a Datacenter, which is needed to balance the workload of each virtual machine while hoping to get a better Quality of services (QoS). Many approaches were proposed in hope of offering a good QoS. But due to the fact that the Cloud environment is exponentially evolving, these approaches became outdated. In this axis of research, we are proposing a new approach based on the Simulated Annealing and different parameters that affect the distribution of the tasks between the virtual machines. A simulation is also done to compare our approach with other existing algorithms using Cloudsim."
pub.1111934190,Application of Fog and Cloud Computing for Patient’s Data in the Internet of Things,"Abstract
The last few years have brought a sudden boost in the Internet of Things (IoT). It is considered to be the next big thing in the evolution of the Internet and an integral part of the future Internet. IoT devices that have their own storage and processing capabilities can process and store data at their end. However, the devices which don’t have storage and processing resources, like sensors attached to the patient’s body, collect data from the physical environment and send to some sink for processing and storage. Such sensors generate a huge amount of data, so there is a need to process and store the data efficiently. However, the cloud computing which is used as a platform for IoT has an inherent problem of latency which can cause bad monitoring and patients which need an immediate treatment can be affected. This problem can be considered in every latency sensitive application which requires real-time monitoring and processing. To solve such problems, we need a new platform for IoT related data which offers the same services as a cloud but do not have problems like a cloud. This study proposes a new solution for IoT patient’s data which utilizes an intermediate layer, fog computing with cloud computing, and accelerates the awareness and response to events by removing a round trip delay to the cloud for analysis. It also offloads the gigabytes of network traffic from the core network to the local edge fog network. This work also proposes how energy efficient sensing will be done. Implementation based analysis is performed to demonstrate the performance of the proposed solution with existing solutions. Results show reduction of the delay and energy efficient sensing."
pub.1113269177,Mowgli,"Big Data and IoT applications require highly-scalable database management system (DBMS), preferably operated in the cloud to ensure scalability also on the resource level. As the number of existing distributed DBMS is extensive, the selection and operation of a distributed DBMS in the cloud is a challenging task. While DBMS benchmarking is a supportive approach, existing frameworks do not cope with the runtime constraints of distributed DBMS and the volatility of cloud environments. Hence, DBMS evaluation frameworks need to consider DBMS runtime and cloud resource constraints to enable portable and reproducible results. In this paper we present Mowgli, a novel evaluation framework that enables the evaluation of non-functional DBMS features in correlation with DBMS runtime and cloud resource constraints. Mowgli fully automates the execution of cloud and DBMS agnostic evaluation scenarios, including DBMS cluster adaptations. The evaluation of Mowgli is based on two IoT-driven scenarios, comprising the DBMSs Apache Cassandra and Couchbase, nine DBMS runtime configurations, two cloud providers with two different storage backends. Mowgli automates the execution of the resulting 102 evaluation scenarios, verifying its support for portable and reproducible DBMS evaluations. The results provide extensive insights into the DBMS scalability and the impact of different cloud resources. The significance of the results is validated by the correlation with existing DBMS evaluation results."
pub.1141383089,A Review of Parallel Heterogeneous Computing Algorithms in Power Systems,"The power system expansion and the integration of technologies, such as renewable generation, distributed generation, high voltage direct current, and energy storage, have made power system simulation challenging in multiple applications. The current computing platforms employed for planning, operation, studies, visualization, and the analysis of power systems are reaching their operational limit since the complexity and size of modern power systems results in long simulation times and high computational demand. Time reductions in simulation and analysis lead to the better and further optimized performance of power systems. Heterogeneous computing—where different processing units interact—has shown that power system applications can take advantage of the unique strengths of each type of processing unit, such as central processing units, graphics processing units, and field-programmable gate arrays interacting in on-premise or cloud environments. Parallel Heterogeneous Computing appears as an alternative to reduce simulation times by optimizing multitask execution in parallel computing architectures with different processing units working together. This paper presents a review of Parallel Heterogeneous Computing techniques, how these techniques have been applied in a wide variety of power system applications, how they help reduce the computational time of modern power system simulation and analysis, and the current tendency regarding each application. We present a wide variety of approaches classified by technique and application."
pub.1130169616,Dynamic Scheduling for Stochastic Edge-Cloud Computing Environments Using A3C Learning and Residual Recurrent Neural Networks,"The ubiquitous adoption of Internet-of-Things (IoT) based applications has resulted in the emergence of the Fog computing paradigm, which allows seamlessly harnessing both mobile-edge and cloud resources. Efficient scheduling of application tasks in such environments is challenging due to constrained resource capabilities, mobility factors in IoT, resource heterogeneity, network hierarchy, and stochastic behaviors. Existing heuristics and Reinforcement Learning based approaches lack generalizability and quick adaptability, thus failing to tackle this problem optimally. They are also unable to utilize the temporal workload patterns and are suitable only for centralized setups. However, asynchronous-advantage-actor-critic (A3C) learning is known to quickly adapt to dynamic scenarios with less data and residual recurrent neural network (R2N2) to quickly update model parameters. Thus, we propose an A3C based real-time scheduler for stochastic Edge-Cloud environments allowing decentralized learning, concurrently across multiple agents. We use the R2N2 architecture to capture a large number of host and task parameters together with temporal patterns to provide efficient scheduling decisions. The proposed model is adaptive and able to tune different hyper-parameters based on the application requirements. We explicate our choice of hyper-parameters through sensitivity analysis. The experiments conducted on real-world data set show a significant improvement in terms of energy consumption, response time, Service-Level-Agreement and running cost by 14.4, 7.74, 31.9, and 4.64 percent, respectively when compared to the state-of-the-art algorithms."
pub.1157479634,TOSCA for Microservice Deployment in Distributed Control Systems: Experiences and Lessons Learned,"The OASIS TOSCA language provides means for specifying the deployment of microservices to cloud-platforms in a vendor-neutral way. Designed in a independent of any application domain, it needs to be tailored to the distributed control systems (DCS), which for example manage the automation in chemical refineries, renewables production, and mining applications. There is still a lack of experience reports applying OASIS TOSCA in real-world settings, therefore the benefits and drawbacks of using this technology are still not well understood. In this context, we designed a simple DCS consisting of several microservices modelled in TOSCA and implemented an according TOSCA orchestrator. We executed a case study deploying the microservices to an on-premise and a cloud-based Kubernetes environment. While TOSCA provides a sophisticated object-oriented language, we found a few specification gaps, challenges when creating portable service templates, and challenges for synchronizing TOSCA orchestrators with DCS engineering tools as well as container orchestrators. The adoption of TOSCA in the process automation domain thus requires more work on the specification and tools and remains a mid-term goal."
pub.1129333604,Data Compression Accelerator on IBM POWER9 and z15 Processors : Industrial Product,"Lossless data compression is highly desirable in enterprise and cloud environments for storage and memory cost savings and improved utilization I/O and network. While the value provided by compression is recognized, its application in practice is often limited because it's a processor intensive operation resulting low throughput and high elapsed time for compression intense workloads. The IBM POWER9 and IBM z15 systems overcome the shortcomings of existing approaches by including a novel on-chip integrated data compression accelerator. The accelerator reduces processor cycles, I/O traffic, memory and storage footprint of many applications practically with zero hardware cost. The accelerator also eliminates the cost and I/O slots that would have been necessary with FPGA/ASIC based compression adapters. On the POWER9 chip, a single accelerator uses less than 0.5% of the processor chip area, but provides a 388x speedup factor over the zlib compression software running on a general-purpose core and provides a 13x speedup factor over the entire chip of cores. On a POWER9 system, the accelerators provide an end-to-end 23% speedup to Apache Spark TPC-DS workload compared to the software baseline. The z15 chip doubles the compression rate of POWER9 resulting in even much higher speedup factors over the compression software running on general-purpose cores. On a maximally configured z15 system topology, on-chip compression accelerators provide up to 280 GB/s data compression rate, the highest in the industry. Overall, the on-chip accelerators significantly advance the state of the art in terms of area, throughput, latency, compression ratio, reduced processor utilization, power/energy efficiency, and integration into the system stack. This paper describes the architecture, and novel elements of the POWER9 and z15 compression/decompression accelerators with emphasis on trade-offs that made the on-chip implementation possible."
pub.1120396683,Cloud-Based Digital Twin for Industrial Robotics,"Production systems are becoming more flexible and agile to realize the need for more individualized products. Robotics technology can accomplish these demands, but programming and re-configuration of robots are associated with high costs, especially for small- and medium-sized enterprises. The use of digital twins can significantly reduce these costs by providing monitoring and simulation capabilities for the robot and its environment using real-time data. The integration with an ontology as a knowledge base to describe the robot and its 3d-environment enables an automatic configuration of the digital twin and the particular robot. In this paper, this concept is coupled with cloud-computing to enable an effortless integration as service in existing cloud architectures and easy access using the common web-technology-stack for the end-users. A novel architecture is presented and implemented to incorporate the real system with its digital twin, the ontology and a planner to infer the actual operations from the knowledge base. Finally, the implementation is applied to the industrial manufacturing domain to assemble different THT-Devices on a PCB to evaluate the concept."
pub.1149352036,Architecting IoT based Healthcare Systems Using Machine Learning Algorithms,"The rapid innovations in technologies endorsed the emergence of sensory equipment's connection to the Internet for acquiring data from the environment. The increased number of devices generates the enormous amount of sensor data from diversified applications of Internet of things (IoT). The generation of data may be a fast or real-time data stream which depends on the nature of applications. Applying analytics and intelligent processing over the data streams discovers the useful information and predicts the insights. Decision-making is a prominent process which makes the IoT paradigm qualified. This chapter provides an overview of architecting IoT-based healthcare systems with different machine learning algorithms. This chapter elaborates the smart data characteristics and design considerations for efficient adoption of machine learning algorithms into IoT applications. In addition, various existing and hybrid classification algorithms are applied to sensory data for identifying falls from other daily activities."
pub.1094619814,Communication and Collaboration Service Components For Ubiquitous Communication Applications,"Over the past decades we have undergone a series of significant evolution in the computing and communication technologies and have experienced better quality of life. Among these evolutions, the mobile device technology has changed the life-style of people significantly and made new business opportunities. Especially., the widespread use of smartphones accelerates to make ubiquitous communication world. It is needed to apply new technology to the legacy system rapidly and to introduce new applications to the market based on the new functions quickly. This paper describes the Converged Service Platform which provides a playground of applications for application providers, and introduces service components for a ubiquitous communication and collaboration working environment. The Converged Service Platform is designed to support the process of service delivery mechanism which consists of service creation, service execution, and service management based on the Service-Oriented Architecture. The platform focused on supporting communication and collaboration functionalities in the form of service components to make ubiquitous working environment. The service components utilize various communication means such as e-mail, short message, messengers, wired or wireless telephone call and even social network services, and give knowledge sharing functionalities such as file cloud, wiki and etc."
pub.1108026439,Towards Smart Healthcare: Patient Data Privacy and Security in Sensor‐Cloud Infrastructure,"Nowadays, wireless body area networks (WBANs) systems have adopted cloud computing (CC) technology to overcome limitations such as power, storage, scalability, management, and computing. This amalgamation of WBANs systems and CC technology, as sensor‐cloud infrastructure (S‐CI), is aiding the healthcare domain through real‐time monitoring of patients and the early diagnosis of diseases. Hence, the distributed environment of S‐CI presents new threats to patient data privacy and security. In this paper, we review the techniques for patient data privacy and security in S‐CI. Existing techniques are classified as multibiometric key generation, pairwise key establishment, hash function, attribute‐based encryption, chaotic maps, hybrid encryption, Number Theory Research Unit, Tri‐Mode Algorithm, Dynamic Probability Packet Marking, and Priority‐Based Data Forwarding techniques, according to their application areas. Their pros and cons are presented in chronological order. We also provide our six‐step generic framework for patient physiological parameters (PPPs) privacy and security in S‐CI: (1) selecting the preliminaries; (2) selecting the system entities; (3) selecting the technique; (4) accessing PPPs; (5) analysing the security; and (6) estimating performance. Meanwhile, we identify and discuss PPPs utilized as datasets and provide the performance evolution of this research area. Finally, we conclude with the open challenges and future directions for this flourishing research area."
pub.1152256333,Improving Adversarial Robustness of 3D Point Cloud Classification Models,"Abstract3D point cloud classification models based on deep neural networks were proven to be vulnerable to adversarial examples, with a quantity of novel attack techniques proposed by researchers recently. It is of paramount importance to preserve the robustness of 3D models under adversarial environments, considering their broad application in safety- and security-critical tasks. Unfortunately, existing defenses are not general enough to satisfactorily mitigate all types of attacks. In this paper, we design two innovative methodologies to improve the adversarial robustness of 3D point cloud classification models. (1) We introduce CCN, a novel point cloud architecture which can smooth and disrupt the adversarial perturbations. (2) We propose AMS, a novel data augmentation strategy to adaptively balance the model usability and robustness. Extensive evaluations indicate the integration of the two techniques provides much more robustness than existing defense solutions for 3D classification models. Our code can be found in https://github.com/GuanlinLee/CCNAMS."
pub.1095697038,VirtualTransits: A Flexible Platform for Network Virtualization Across Data Centers,"Network virtualization is crucial in data centers environments for interconnecting all physical and virtualized resources to create a complete semblance of an integrated computing infrastructure. Much of the existing research on data center networking has focused on network mechanisms inside the datacenter. Little attention has been paid to networking mechanisms for integrating multiple data centers. In this paper we introduce VirtualTransits, a flexible platform providing a novel solution to dynamically build and manage virtual networks across multiple datacenters. VirtualTransits can also incorporate several important datacenter networking schemes into a coherent platform that enables seamless integration of intracloud and intercloud traffic. Through our implementation and performance evaluation on a real deployment, we show that our system provides a promising and efficient solution for inter-cloud networking, testbeds federation, and software-defined networking testbeds."
pub.1123934066,Research on Computing Resources Sharing of University Laboratories in Education Cloud Environment,"In view of the common problems of computing resources sharing existing in university laboratories, and combined with the advantages of cloud computing technology in the current education cloud environment, this paper explores the computing resources sharing in colleges and universities. The integration scheme of laboratory computing resources is designed, the sharing mode and operation management mechanism of computing resources are proposed, and the application domain model of computing resources is studied, which is helpful to promote the standardized development of resource sharing, and provides a reference for resource sharing implementation in the education industry."
pub.1137835054,Efficient Load Optimization Method Using VM Migration in Cloud Environment,"Virtualization is the fundamental integral part of CLOUD technology where load balancing among virtual machines (VM) and physical machines (PM) are managed. Heavy workload in PM makes cloud more imbalance and unorganized. To handle load balancing among PMs, VM migration is one of the best approaches. In this paper, a new approach is introduced to explore the best optimal destination PM for VM migration using soft computing based Type2 Fuzzy Logic technique (T2FL). The VM which has minimum migration time gets selected for migration from source PM to optimal destination PM. The simulation is performed using CloudSim tool and java-based fuzzy (Juzzy) packages have been extended for fuzzy system results. The simulated results of proposed VMMLB-FLS has been analyzed with existing Round Robin and CM-eFCFS cloud-based algorithms with respect to performance metrics Transmission Time, Execution Time, MakeSpan, and Number of Migrations. The proposed method results depict the effectiveness of VMMLB-FLS compared to existing methods."
pub.1052832059,"Application performance management using learning, optimization, and control","In the past decade, the IT industry has experienced a paradigm shift as computing resources became available as a utility through cloud based services. In spite of the wider adoption of cloud computing platforms, some businesses and organizations hesitate to move all their applications to the cloud due to performance concerns. Existing practices in application performance management rely heavily on white-box modeling and diagnosis approaches or on performance troubleshooting ""cookbooks"" to find potential bottlenecks and remediation steps. However, the scalability and adaptivity of such approaches remain severely constrained, especially in a highly-dynamic, consolidated cloud environment. For performance isolation and differentiation, most modern hypervisors offer powerful resource control primitives such as reservations, limits, and shares for individual virtual machines (VMs). Even so, with the exploding growth of virtual machine sprawl, setting these controls properly such that co-located virtualized applications get enough resources to meet their respective service level objectives (SLOs) becomes a nearly insoluble task. These challenges present unique opportunities in leveraging the rich telemetry collected from applications and systems in the cloud, and in applying statistical learning, optimization, and control based techniques to developing model-based, automated application performance management frameworks. There has been a large body of research in this area in the last several years, but many problems remain. In this talk, I'll highlight some of the automated and data-driven performance management techniques we have developed, along with related technical challenges. I'll then discuss open research problems, in hope to attract more innovative ideas and solutions from a larger community of researchers and practitioners."
pub.1002508919,RAICB: Robust Authentication Immunity Cloud Bucket for Secure Cloud Storage System,"Data security is one of the prime concerns in cloud storage system inspite of abundant adoption of cloud services. It was seen that inspite of existing research studies towards cloud security; there is little work which has been dedicated toward securing the bucket in the cloud storage system. It is critically important to safeguard bucket as it holds the original IP of the clients and strengthening the authentication system towards virtual machine is the only feasible solution as data resides and is accessed through virtual environments only. Therefore, this paper present a novel security technique termed as RACB i.e. Robust Authentication for Cloud Bucket that potentially strengths the accessibility of virtual machine. The outcome of the study was compared with existing security protocols used to find that proposed RACB offers lightweight and computationally effective technique to ensure data security to buckets."
pub.1173362475,Packet-optical transport network for future radio infrastructure,"The advent of 6G is expected to transform connectivity by necessitating robust and scalable transport networks, capable of managing the escalating demands for enhanced bandwidth, negligible latency, and heightened network automation. The progression towards centralized radio access networks and the cloudification of network functions introduce additional requirements to the transport network. Addressing these demands, the integration of packet and optical systems combines packet flexibility with the high bandwidth and low latency of optical systems, aiming for a balance between performance, efficiency, and functionality. This process considers cost and the reuse of existing infrastructure towards a seamless transition to 6G. The concept of a Mini-ROADM, a cost-effective, energy-efficient optical switch created using silicon photonics, is presented and demonstrated in a ring network application. The role of a transport-aware end-to-end orchestrator in coordinating resources across radio, transport, and cloud domains to ensure a diverse range of quality-of-service levels is also discussed. A system demonstrator that highlights the integration of packet and optical layers and the concrete application of these concepts in a network environment is presented."
pub.1028093738,Enhancing Grid Resource Scheduling Algorithms for Cloud Environments,"Cloud computing is the latest evolution in the distributed computing paradigm and is being widely adopted by enterprises and organizations. The inherent benefits like instant scalability, pay for use, rapid elasticity, cost effectiveness, self-manageable service delivery and broader network access make cloud computing ‘the preferred platform’ for deploying applications and services. However, the technology being in nascent stage needs to be proven. The biggest challenge confronting service providers is effective provisioning and scheduling of cloud services to consumers leveraging the cost benefits of this computing paradigm. This paper attempts to investigate the key concerns for cloud resource management and explores possible alternatives that can be adapted from the existing Grid technology."
pub.1095240111,Cognitive Visual Analytics of Multi-Dimensional Cloud System Monitoring Data,"Hardware virtualization has enabled large scale computational service delivery models with significant cost leverage and has improved resource utilization of cloud computing platforms. This has completely changed the landscape of computing in the last decade. It has enabled very large-scale data analytics through distributed, high performance computing. However, due to the infrastructure complexity, end-users and administrators of cloud platforms can rarely obtain a complete picture of the state of cloud computing systems and data centers. Recent monitoring tools enable users to obtain large amounts of data with respect to many utilization parameters of cloud platforms. However, they often fall short of maximizing the overall insight into the resource utilization dynamics of cloud platforms. Furthermore, existing tools make it difficult to observe large scale patterns making it difficult to learn from the past behavior of cloud system dynamics. New operating platforms for cloud management and service provisioning allow live migration and dynamic resource re-allocation at multiple levels of the hardware virtualization layers. Hence, it has become necessary to provide cognitive visualizing tools for monitoring the activities in an active cloud environment. In this work, we describe a perceptual-based interactive visualization platform that gives users and administrators a cognitive view of cloud computing system dynamics. We define machine states and aggregate states at multiple levels of detail to construct a multiview presentation of the resource utilization according to the scalability and the elasticity features of a cloud computing system."
pub.1093644539,A method for fast switching of redundant networks between data centers,"Last year's earthquake in Eastern Japan sparked a growing demand for redundant structures in Web 3-tier model/servers, NICs and physical lines - which are required by conventional on-premises environments with high reliability and availability - to be directly migrated to cloud environments. In such environments, when applying ICMP connectivity checks and conventional fault detection methods such as IP-SLA, it takes time to recover from faults to allow for coordination between the core network and the user logical networks in the cloud. This paper proposes a method where a correspondence list of user logical networks and core networks is registered beforehand in a cloud management system. When a fault has occurred, the NIC of the virtual machine corresponding to the fault location is closed off, thereby resulting in a rapid switchover to a redundant configuration."
pub.1101630467,Critical Path-Based Ant Colony Optimization for Scientific Workflow Scheduling in Cloud Computing Under Deadline Constraint,"In local computing environment, when we are dealing scientific computation using scientific workflow scheduling environment under deadline constraint, QoS is one of the most challenging tasks for any system used in scientific computing systems. Because when we are focusing on minimizing the workflow execution cost as well as time, we should not forget to consider the user-defined quality of service requirements while minimizing the workflow execution of cost and time. Therefore, to reduce the cost and time, we used cloud environment. Since cloud computing environment is in elastic nature, in which availability of resources is readily available as when and then required, its utilization is another challenge while using cloud computing environment. Therefore, in this paper, we use intelligence ant colony optimization (ACO), in which underutilized VMs allocation is initialized by Pareto distribution. ACO is used to converge the decision of virtual machine (VM) migration by its convergence to minima of cost and time. In our experiments, we have set up a local simulator in stand-alone system; for that workflow simulator, 1.0 has been used, where we have used Java eclipse for executing our program to calculate total execution time (TET) and total execution cost (TEC). In which, we have run our simulator ten times for each scientific workflow application and then average is taken for each workflow application to compare the output in which we found that ACO shows significant performance component when compare to existing genetic algorithm."
pub.1136426486,The Virtual Observatory Ecosystem Facing the European Open Science Cloud,"The International Virtual Observatory Alliance (IVOA) has developed and
built, in the last two decades, an ecosystem of distributed resources,
interoperable and based upon open shared technological standards. In doing so
the IVOA has anticipated, putting into practice for the astrophysical domain,
the ideas of FAIR-ness of data and service resources and the Open-ness of
sharing scientific results, leveraging on the underlying open standards
required to fill the above. In Europe, efforts in supporting and developing the
ecosystem proposed by the IVOA specifications has been provided by a continuous
set of EU funded projects up to current H2020 ESCAPE ESFRI cluster. In the
meantime, in the last years, Europe has realised the importance of promoting
the Open Science approach for the research communities and started the European
Open Science Cloud (EOSC) project to create a distributed environment for
research data, services and communities. In this framework the European VO
community, had to face the move from the interoperability scenario in the
astrophysics domain into a larger audience perspective that includes a
cross-domain FAIR approach. Within the ESCAPE project the CEVO Work Package
(Connecting ESFRI to EOSC through the VO) has one task to deal with this
integration challenge: a challenge where an existing, mature, distributed
e-infrastructure has to be matched to a forming, more general architecture.
CEVO started its works in the first months of 2019 and has already worked on
the integration of the VO Registry into the EOSC e-infrastructure. This
contribution reports on the first year and a half of integration activities,
that involve applications, services and resources being aware of the VO
scenario and compatible with the EOSC architecture."
pub.1144119133,GlobalMatch: Registration of Forest Terrestrial Point Clouds by Global Matching of Relative Stem Positions,"Registering point clouds of forest environments is an essential prerequisite
for LiDAR applications in precision forestry. State-of-the-art methods for
forest point cloud registration require the extraction of individual tree
attributes, and they have an efficiency bottleneck when dealing with point
clouds of real-world forests with dense trees. We propose an automatic, robust,
and efficient method for the registration of forest point clouds. Our approach
first locates tree stems from raw point clouds and then matches the stems based
on their relative spatial relationship to determine the registration
transformation. The algorithm requires no extra individual tree attributes and
has quadratic complexity to the number of trees in the environment, allowing it
to align point clouds of large forest environments. Extensive experiments on
forest terrestrial point clouds have revealed that our method inherits the
effectiveness and robustness of the stem-based registration strategy while
exceedingly increasing its efficiency. Besides, we introduce a new benchmark
dataset that complements the very few existing open datasets for the
development and evaluation of registration methods for forest point clouds. The
source code of our method and the dataset are available at
https://github.com/zexinyang/GlobalMatch."
pub.1004580646,Resource Management for Hybrid Grid and Cloud Computing,"From its start of using supercomputers, scientific computing constantly evolved to the next levels such as cluster computing, meta-computing, or computational Grids. Today, Cloud Computing is emerging as the paradigm for the next generation of large-scale scientific computing, eliminating the need for hosting expensive computing hardware. Scientists still have their Grid environments in place and can benefit from extending them using leased Cloud resources whenever needed. This paradigm shift opens new problems that need to be analyzed, such as integration of this new resource class into existing environments, applications on the resources, and security. The virtualization overheads for deployment and starting of a virtual machine image are new factors, which will need to be considered when choosing scheduling mechanisms. In this chapter, we investigate the usability of compute Clouds to extend a Grid workflow middleware and show on a real implementation that this can speed up executions of scientific workflows."
pub.1094410144,Cross Resource Optimisation of Database Functionality Across Heterogeneous Processors,"Significant application performance improvements can be achieved by heterogeneous compute technologies, such as multi-core CPUs, GPUs and FPGAs. The HARNESS project is developing architectural principles that enable the next generation cloud platforms to incorporate such devices thereby vastly increasing performance, reducing energy consumption, and lowering associated cost profiles. Along with management and integration of such devices in a cloud environment, a key issue is enabling enterprise-level software to make effective use of such compute devices. A major obstacle in adopting heterogeneous compute resources is the requirement that at design time the developer must decide on which device to execute portions of the application. For an interactive application, such as SAP HANA where there are many ongoing tasks and processes, this type of decision is impossible to predict at design time. What is required is the ability to decide, at run-time, the optimal compute device to execute a task. This paper extends upon existing work on SHEPARD to support non-OpenCL devices. SHEPARD decouples application development from the target platform and enables the required run-time allocation of tasks to heterogeneous computing devices. This paper establishes SHEPARD's capability to: (1) select the appropriate compute device to execute tasks, (2) dynamically load the device application code at runtime, and (3) execute the application logic. Experiments demonstrate how SHEPARD optimises the execution of a SAP HANA database management function across heterogeneous compute devices and perform automatic run-time task allocation."
pub.1124431296,Transfer Learning for Cross-Model Regression in Performance Modeling for the Cloud,"Performance characteristics of a complex system in different configurations are expensive to obtain due to the cost of sampling the system performance. We introduce ModelMap, a novel transfer learning technique for the performance modeling of configurable systems. ModelMap captures many explicit and latent types of dynamic system evolution, including configuration changes, scaling and hardware upgrades, by deriving and modeling directly these kinds of incremental transformations between system and/or application instances, over time. Modeling these transformations allows us to build accurate models for new configuration instances with just a few samples, and to interpolate across legacy models to build new models with no new samples at all. We experimentally test our method on a variety of system performance modeling and optimization scenarios. Compared to using conventional direct and incremental modeling techniques, our method achieves higher accuracy by up to an order of magnitude when the sampling budget is extremely limited, in particular between 0% to 5% of an exhaustive sampling budget. We also show how our method can be used to quickly derive an accurate resource allocation split that optimizes a given overall performance goal for co-hosted applications in a virtualized environment."
pub.1001173641,Dynamic content-based cloud data integration system with privacy and cost concern,"Although a lot of research is currently taking place in the cloud computing technology itself, there is an equally urgent need for understanding the cost effective issues surrounding cloud. In this paper we present a dynamic content-based inter-cloud data integration system with privacy and cost concern. Users can evaluate the privacy risk value based on the content (input data or intermediate searching results) which are generated during runtime, in the meanwhile, comparing with existing data sharing techniques, our method is more practical as the cost for technical supporting privacy must be considered in the commoditized cloud computing environment."
pub.1095786141,Enabling User-Policy-Confined VM Migration in Trusted Cloud Computing,"The trusted cloud environment model, based on the trusted computing technology, provides cloud users the capabilities to remotely attest the trustworthiness of cloud service. But unlike physical machine, the virtual machine is migratory among cloud nodes constantly which makes it hard for cloud users to guarantee the Virtual Machine (VM) is running in a trusted cloud node and its migration is consistent with user's policy. Since the migration operation is entirely transparent to cloud users, it is hard for them to launch the remote attestation with the existing remote attestation scheme. To guarantee the migration of VMs is consistent with user's policy, we propose a User-Policy-Confined VM Migration Protocol (UVMP), which provides a new attestation scheme that addresses the limitation and enables VM migration to be controllable and verifiable by cloud users by introducing the Ciphertext-Policy Attribute-Based Encryption(CP-ABE) into attestation field. To demonstrate that UVMP is practical, we incorporate it in the Xen open-source virtualization platform and implement UTVMS: User-Policy-Confined Trusted VM Migration System. Our evaluation suggests that the attestation scheme cannot lead to the obvious performance on the VM migration by analyzing the migration latency. The attestation scheme can provide greater flexibility for cloud users to control their own VM migration."
pub.1128431960,Plug & Play Retrofitting Approach for Data Integration to the Cloud,"Driven by rapid digitalisation, production systems are becoming more flexible and adaptable with the help of emerging concepts like the Internet of Things (IoT) and Industry 4.0. Often, these transformations are not fully implemented in Small and Medium-sized Enterprises (SMEs) due to the replacement cost of existing machines. This paper aims to develop a Plug & Play retrofitting platform, where Industry 4.0 compliant sensor systems can be attached, detected, and configured automatically to the existing production environment. The purpose of the retrofitting is to integrate the sensor system with a cloud platform that would provide persistent storage for sensor data as well as the functionalities to perform monitoring, analysis, and predictive learning."
pub.1167178446,Secure Data Sharing in a Cyber-Physical Cloud Environment,"Cloud computing plays a significant role in digital workplaces and has become an integral part of everyday life. However, the security issues associated with cloud computing remain a major concern that hinders the wider adoption of this technology. In a cyber-physical cloud environment, achieving secure and efficient file storage remains a tough goal. This is particularly the case owing to the wide variety of devices that are being utilised to access the various services and data. Thus, the employment of a secure data sharing protocol is one of the essential techniques to better protect the shared data. A formal agreement between entities or organisations in exchanging personal or business data is called a data sharing protocol. A secure data sharing protocol ensures that data is encrypted and secured when being transferred. There are many different encryption algorithms and protocols in use to devise various secure and efficient data sharing protocols. This paper first reviews the state of the art of some existing data sharing protocols and subsequently implements a secure and efficient data sharing protocol as an application in a cyber-physical cloud environment. The performance analysis conducted on the developed secure data sharing protocol application shows positive results.  "
pub.1095477476,Cloud-Based Load Testing Method for Web Services with VMs Management,"Due to the increased loading the large number of users connected to pervasive web services during the past decade, their load testing and providing the needed resources in low time and cost, requires more attention. In this context cloud computing technology offers new ideas to solve such problems and has reduced the concern of large and complex testing systems. In this research in order to improve the quality and performance of web applications load testing, we proposed a method for web applications load testing based on cloud computing. The proposed method uses the existing facilities in the cloud including pool of computing resources without initial cost, unlimited data storage and cloud computing managerial procedures, containing the actual load generating and multi-user concurrency testing, that lead to improved load testing flexibility, time and operational costs. Moreover, in this load testing method, in order to manage resources and virtual machines, significant improvement is achieved by use of appropriate allocation, reducing performance and unnecessary migration avoiding methods. Through evaluation section of the proposed method through a simulated test environment, it is shown that cloud-based load testing in comparison with traditional methods of load testing, improves factors such as effort, cost and time."
pub.1142048818,HUNTER: AI based holistic resource management for sustainable cloud computing,"The worldwide adoption of cloud data centers (CDCs) has given rise to the ubiquitous demand for hosting application services on the cloud. Further, contemporary data-intensive industries have seen a sharp upsurge in the resource requirements of modern applications. This has led to the provisioning of an increased number of cloud servers, giving rise to higher energy consumption and, consequently, sustainability concerns. Traditional heuristics and reinforcement learning based algorithms for energy-efficient cloud resource management address the scalability and adaptability related challenges to a limited extent. Existing work often fails to capture dependencies across thermal characteristics of hosts, resource consumption of tasks and the corresponding scheduling decisions. This leads to poor scalability and an increase in the compute resource requirements, particularly in environments with non-stationary resource demands. To address these limitations, we propose an artificial intelligence (AI) based holistic resource management technique for sustainable cloud computing called HUNTER. The proposed model formulates the goal of optimizing energy efficiency in data centers as a multi-objective scheduling problem, considering three important models: energy, thermal and cooling. HUNTER utilizes a Gated Graph Convolution Network as a surrogate model for approximating the Quality of Service (QoS) for a system state and generating optimal scheduling decisions. Experiments on simulated and physical cloud environments using the CloudSim toolkit and the COSCO framework show that HUNTER outperforms state-of-the-art baselines in terms of energy consumption, SLA violation, scheduling time, cost and temperature by up to 12, 35, 43, 54 and 3 percent respectively."
pub.1164237171,Life on the Edge from Legacy to Cloud Computing: A Case Study on Insurance Industry,"Cloud is not just a word for the future, it’s here today. The world is witnessing a rapid growth of technologies and the business interest is increasing towards the cloud environments. The secure, on-demand data storage, retrieval, and elasticity of computing power attract more companies to opt for cloud technologies than before. The journey to cloud witnesses pitfalls and opportunities that differ from business to business. This paper serves as a forum to highlight the struggle and journey of the largest insurers in North America in their journey from the legacy world to the cloud. It points out both the theoretical and practical aspects of choosing, designing, building, and managing different cloud-distributed systems. The cloud strategy addresses the real business needs that are specific to industry type and business requirements. Core companies like Insurance and financial institutions are going through the route of continuous transformation that gives them an added advantage over the competitors through a full cloud-strategy lifecycle and innovation. The tested concepts and pilots to scale up the company are highlighted here in this paper."
pub.1154013549,Cloud Computing Synchronization: Case of Late Emails’ Social Impact,"<p>The latest research on cloud computing advances theories and applications in the various existing environments. The adoption of cloud computing is widely in tools as social networks, emails, applications and others. The research discusses the future of a specific type of communication tool -the emails, stating that the cloud computing supporting this tool needs further development considering the content of the email; we discuss the social aspects of these tools related to the impact of mis-synchronization on the user. The research analyzes the variables of the email content -as its type of content ranging from professional to amateur, and proposes theories and hypothesis. The research method is a questionnaire dedicated to the most strategic employees of an organization in the service sector. The results show that there are various point of views on the role of synchronization in the emails and that there is a social impact emerging from it indeed.</p>"
pub.1095558874,PFC: Privacy Preserving FPGA Cloud - a Case Study of MapReduce,"Privacy is one of the critical concerns that hinder the adoption of public cloud. For storage, encryption can be used to protect user's data. But for outsourced data processing, for example MapReduce, there is no satisfying solution. Users have to trust the cloud service providers totally. In this work, we propose PFC, a FPGA cloud for privacy preserving computation in the public cloud environment. PFC leverages the security feature of the existing FPGAs originally designed for bitstream IP protection and proxy re-encryption for preserving user data privacy. In PFC, cloud service providers are not necessarily trusted, and during outsourced computation, user's data is protected by a data encryption key only accessible by trusted FPGA devices. As an important application of cloud computing, we apply PFC to the popular MapReduce programming model and extend the FPGA based MapReduce pipeline with privacy protection capabilities. Proxy re-encryption is employed to support dynamic allocations of trusted FPGA devices as mappers and reducers. Finally, we conduct evaluation to demonstrate the effectiveness of PFC."
pub.1158400966,Security and privacy concerns in cloud-based scientific and business workflows: A systematic review,"Today, the number of data-intensive and compute-intensive applications like business and scientific workflows has dramatically increased, which made cloud computing more popular because of its ability to deliver a large number of computing resources on-demand. Security is a critical issue affecting the wide adoption of cloud technologies, especially for workflows that are mostly dealing with sensitive data and tasks. In this paper, we carry out a review of the state-of-the-art on how security and privacy concerns in scientific and business workflows in cloud environments are being addressed and identify the limitations and gaps in the current body of knowledge in this area. In this extensive literature review, we first present the state-of-the-art security solutions organized according to the phases of the workflow life cycle they target for both business and scientific workflows. The analysis shows that most of the existing literature focuses on the modeling and execution phases, while the monitoring and adaptation phases are not covered adequately by a scarce amount of publications thus leaving a huge gap in the body of knowledge relevant to detection, prevention of and reaction to security violations in cloud-based workflows."
pub.1016447093,Cloudbasierte Infrastruktur für Bibliotheksdaten – auf dem Weg zu einer Neuordnung der deutschen Verbundlandschaft,"In October 2012 the Deutsche Forschungsgemeinschaft (DFG) (German Research Foundation) launched a funding programme for the reorganisation of the information services to encourage the redesign of existing infrastructures, which the Wissenschaftsrat (German Research Council) had called for in its recommendations for the future of library union catalogues in Germany. Under the issue of “Library data infrastructure and local systems” the library consortia of Hesse (HeBIS), Bavaria (BVB), and Berlin-Brandenburg (KOBV) applied successfully for the project “Cloud-based Infrastructure for Library Data” (CIB). The goals of the project aim at the transfer of library workflows and services to cloud environments. Traditional union catalogues and local systems will be replaced stepwise by international service platforms. The work packages of the project also include the integration of German authority data as well as further services into these platforms."
pub.1141882279,HUNTER: AI based Holistic Resource Management for Sustainable Cloud Computing,"The worldwide adoption of cloud data centers (CDCs) has given rise to the
ubiquitous demand for hosting application services on the cloud. Further,
contemporary data-intensive industries have seen a sharp upsurge in the
resource requirements of modern applications. This has led to the provisioning
of an increased number of cloud servers, giving rise to higher energy
consumption and, consequently, sustainability concerns. Traditional heuristics
and reinforcement learning based algorithms for energy-efficient cloud resource
management address the scalability and adaptability related challenges to a
limited extent. Existing work often fails to capture dependencies across
thermal characteristics of hosts, resource consumption of tasks and the
corresponding scheduling decisions. This leads to poor scalability and an
increase in the compute resource requirements, particularly in environments
with non-stationary resource demands. To address these limitations, we propose
an artificial intelligence (AI) based holistic resource management technique
for sustainable cloud computing called HUNTER. The proposed model formulates
the goal of optimizing energy efficiency in data centers as a multi-objective
scheduling problem, considering three important models: energy, thermal and
cooling. HUNTER utilizes a Gated Graph Convolution Network as a surrogate model
for approximating the Quality of Service (QoS) for a system state and
generating optimal scheduling decisions. Experiments on simulated and physical
cloud environments using the CloudSim toolkit and the COSCO framework show that
HUNTER outperforms state-of-the-art baselines in terms of energy consumption,
SLA violation, scheduling time, cost and temperature by up to 12, 35, 43, 54
and 3 percent respectively."
pub.1166866406,Modeling of the process of providing telecommunication services in cloud systems,"The topic discussed in this article is related to the efficient use of resources when providing telecommunications services in a cloud environment. Telecommunications companies pay special attention to testing and introducing new digital services, including data networks. And the use of cloud services is one of the promising tasks, the solution and integration of which with existing systems and equipment will significantly increase the efficiency of the devices involved in favor of expanding the range of services provided both in relation to telecommunication networks within the technological process and in the business management of the entire company. And taking into account the currently ongoing global project to organize a multi-service data transmission network and the widespread modernization of individual regions to organize a high-speed transport data transmission network, it will allow modeling and preliminary assessment of the effectiveness of the services provided in such communication networks at the current stage."
pub.1071981464,Study of Higher Institutes Platform Based on Cloud Computing,"Cloud computing, the future development direction of the IT industry, has the profound influence of cloud computing applications, which is bound to the field of higher education, the construction of university network education resources platform is the integration of all kinds of educational resources of colleges and universities, and to provide fast and convenient resource storage, sharing, learning and computational ability. This article is from the university network education platform of cloud computing based on the advantages of the proposed strategy, construction of information resources of University under the cloud computing environment. This is how the cloud model construction resources platform of network education in Colleges and universities. With the rapid development of Internet, people from all walks of life to and gradually mature ""cloud computing"" model of combining the road to seek an opportunity. However, in the field of education, the application of cloud computing is scanty. Research on the current college network education situation and abuse with according to the characteristics of cloud computing, advantages, and puts forward the conception of network education platform based on cloud computing, and further demonstrates its feasibility. It is the high time to improve the transplantation and service application pattern of cloud computing in the field of education. The main works of this paper are as follows: firstly, the domestic and foreign network education to analyze the situation, reasons and problems existing in domestic university network education problems."
pub.1122960997,A General Framework for Privacy-preserving Computation on Cloud Environments,"While privacy and security concerns dominate public cloud services, Homomorphic Encryption (HE) is seen as an emerging solution that can potentially assure secure processing of sensitive data by third-party cloud vendors. It relies on the fact that computations can occur on encrypted data without the need for decryption, although there are major stumbling blocks to overcome before the technology is considered mature for production cloud environments. This paper examines a proposed technology platform, known as the Homomorphic Encryption Bus (HEB), that leverages HE with data obfuscation methods over a minimal network interaction model, allowing a uniform, flexible and general approach to cloud-based privacy-preserving system integration. The platform is uniquely designed to overcome barriers limiting the mainstream application of existing Fully Homomorphic Encryption (FHE) schemes in the cloud. A client-server interaction model involving ciphertext decryption on the client end is necessary to achieve resetting of 'noisy' ciphertexts in place of a much more inefficient (server only) recryption procedure. Data perturbation techniques are used to obfuscate intermediate data decrypted on the client-side of ciphertext interactions, in a way that is unintelligible to the client. In addition to efficient noise resetting, interactions involving data perturbations also achieve plaintext (binary to integer-based and vice versa) message space swapping, and conversion of accumulated integer-based encodings to a reduced embedded binary form. There appears to be little existing literature that examines these techniques as a means of broadening HE processing capabilities and practical application over the cloud. Interaction performance is examined in terms of timing and multiplicative circuit depth costs, through a simple equation evaluation and against standard recryption."
pub.1168456538,The Cloud-Based Implementation and Standardization of Anthropomorphic Phantoms and Their Applications,"Radiation protection applications often require the creation of a large number of precise simulations of radiation-human body interactions. Our research is focused on creating RadPhantom, a new Geant4 application that constructs voxelized anthropomorphic phantom models. This allows for the standardized and reproducible generation of Geant4 simulations in cloud-based environments. We have incorporated existing and publicly accessible models into Meiga, a framework designed for the integration of Geant4-based applications. To standardize these simulations, guarantee their reproducibility, and adhere to the FAIR principles, we have developed an extended vocabulary schema using metadata and ontologies that align with current standards. By employing virtualization containers, we capitalize on the scalability and adaptability of public and federated clouds. In this paper, we detail our implementation, present some benchmarking results and comparisons with current methodologies, and discuss the potential applications for evaluating doses on commercial flights or assessing radiation shielding in neutron production facilities."
pub.1168696686,A Proposed Framework of Vulnerability Assessment and Penetration Testing (VAPT) in Cloud Computing Environments from Penetration Tester Perspective,"Penetration testing is a process that focuses on finding security vulnerabilities in a target environment that could let an attacker penetrate the network or computer system or steal information. Due to the COVID-19 endemic, most employees still implement working from home or a hybrid approach, even though the number of new cases of COVID-19 is decreasing. However, working from home depends mainly on cloud computing applications that help employees efficiently accomplish their daily work. This situation also increased the number of data generated from various sources, so they may be exposed to different security risks. This research will propose a framework to conduct vulnerability assessment and penetration testing (VAPT) in cloud service models such as SaaS, PaaS, and IaaS from the perspective of penetration testers. This proposed framework is developed through the integration and mapping of existing frameworks and guidelines to conduct VAPT on testing components such as web applications, APIs, network testing, etc. In this proposed framework, the method of conducting VAPT for each cloud service model will be discussed in detail, from the planning and reconnaissance stage until the report is delivered to the cloud subscriber or cloud provider. An advantage of this proposed framework for the penetration tester is that there is still a lack of methods or guidelines for conducting VAPT that cover all the cloud service models in one comprehensive document"
pub.1116851620,An Environment-Aware Market Strategy for Data Allocation and Dynamic Migration in Cloud Database,"Currently, a cloud database is employed to serve on-line query-intensive applications. It inevitably happens that some cloud data nodes storing hot records are facing high frequent query requests while others are rarely visited or even idle. Therefore, how data are dynamically allocated and migrated at runtime has significant impact on query load distribution and system performance. Existing system adopt centralized approaches, and they face two main challenges: (1) Query load on individual node cannot be always balancing even if the data are fairly distributed; (2) For each node, the dynamic changes of configuration resources cannot be captured during the runtime. To this end, this paper presents an environment-aware market strategy based system, named e-MARS, for reasonable data migration to achieve query load balance in cloud database. In e-MARS, cloud database is modeled as a cloudDB market, while data nodes are regarded as intelligent traders and the query load as commodity. Each trader is aware of its local environmental resources, such as computing capacity, disk volume, based on which the trader itself decides how to trade the query load and migrates the corresponding data. In this way the cloudDB market will achieve equilibrium. Experiments are conducted on the real communication data, and e-MARS significantly enhances the efficiency. Compared with HBase Balancer, more than 65% improvement is achieved in terms of query response time."
pub.1095634854,secCloudSim: Secure Cloud Simulator,"Cloud Computing is an emerging field of technology that relies on sharing of resources over the Internet to achieve economy of scales. It has gained momentum among both individuals and corporate sectors due to a variety of applications such as ease of administration, mobility and collaboration, reduction of hardware and licensing cost, and scalability. Researchers are also trying to explore and analyze the Cloud environment, but due to the lack of resources for performing large-scale experiments, they mostly rely on Cloud simulators. In this regards, several Cloud simulators have been developed to analyze different features as evaluation of fetching a virtual machine, power consumption of network equipment's, resource consumptions and allocation of memory. However, according to the best of our knowledge, none of these simulators provide any support for analyzing the security aspects of the Cloud. In this paper, we have designed and implemented a new Cloud simulator, entitled as secure Cloud Simulator (secCloudSim). The secCloudSim is an extension of existing iCanCloud simulator, which provides the basic security features of authentication and authorization. After the successful implementation, we have evaluated the performance of secCloudSim and iCanCloud in terms of packet rate, processing time, response time, and time delay. Our performance evaluation results revealed that secCloudSim has a low performance impact as compared to iCanCloud. This low performance impact of secCloudSim is due to the integration of security features to iCanCloud architecture."
pub.1100550858,Malicious insiders attack in IoT based Multi-Cloud e-Healthcare environment: A Systematic Literature Review,"The emergence of Internet of Things (IoT) has introduced smart objects as the fundamental building blocks for developing a smart cyber-physical universal environment. The IoTs have innumerable daily life applications. The healthcare industry particularly has been benefited due to the provision of ubiquitous health monitoring, emergency response services, electronic medical billing, etc. Since IoT devices possess limited storage and processing power, therefore these intelligent objects are unable to efficiently provide the e-health facilities, or process and store enormous amount of collected data. IoTs are merged with Cloud Computing technology in Multi-Cloud form that basically helps cover the limitations of IoTs by offering a secure and on-demand shared pool of resources i.e., networks, servers, storage, applications, etc., to deliver effective and well-organized e-health amenities. Although the framework based on the integration of IoT and Multi-Cloud is contributing towards better patient care, yet on the contrary, it is challenging the privacy and reliability of the patients’ information. The purpose of this systematic literature review is to identify the top security threat and to evaluate the existing security techniques used to combat this attack and their applicability in IoT and Multi-Cloud based e-Healthcare environment."
pub.1095120510,A Practical Framework for Privacy-Preserving NoSQL Databases,"Cloud infrastructures provide database services as cost-efficient and scalable solutions for storing and processing large amounts of data. To maximize performance, these services require users to trust sensitive information to the cloud provider, which raises privacy and legal concerns. This represents a major obstacle to the adoption of the cloud computing paradigm. Recent work addressed this issue by extending databases to compute over encrypted data. However, these approaches usually support a single and strict combination of cryptographic techniques invariably making them application specific. To assess and broaden the applicability of cryptographic techniques in secure cloud storage and processing, these techniques need to be thoroughly evaluated in a modular and configurable database environment. This is even more noticeable for NoSQL data stores where data privacy is still mostly overlooked. In this paper, we present a generic NoSQL framework and a set of libraries supporting data processing cryptographic techniques that can be used with existing NoSQL engines and composed to meet the privacy and performance requirements of different applications. This is achieved through a modular and extensible design that enables data processing over multiple cryptographic techniques applied on the same database. For each technique, we provide an overview of its security model, along with an extensive set of experiments. The framework is evaluated with the YCSB benchmark, where we assess the practicality and performance tradeoffs for different combinations of cryptographic techniques. The results for a set of macro experiments show that the average overhead in NoSQL operations performance is below 15%, when comparing our system with a baseline database without privacy guarantees."
pub.1137052045,Intrinsic Security and Self-Adaptive Cooperative Protection Enabling Cloud Native Network Slicing,"With the emergence of cloud native technology, the network slicing enables automatic service orchestration, flexible network scheduling and scalable network resource allocation, which profoundly affects the traditional security solution. Security is regarded as a technology independent of the cloud native architecture in the initial design, traditional passive defense such as “reinforced” and “stacked” is relied on to achieve system security protection. The lack of intrinsic security mechanisms makes the system capability insufficient when faces the uncertain threat brought by vulnerabilities and backdoors under the ecosystem of opening-up and sharing. The static nature of existing networks and computing systems makes them easy to be compromised and hard to defend, and thus it is urgent to provide intrinsic security and proactive protection against the unpredictable attacks. To this end, this paper proposes a novel paradigm named intrinsic cloud security (iCS) from the perspective of dynamic defense. The dynamic defense provides component-level security, and has complementary and consistency with the cloud native environment. In particular, iCS introduces mimic defense and moving target defense (MTD), and makes full use of the new features introduced by cloud native to implement an intrinsic and proactive defense mechanism with acceptable costs and efficiency. The iCS paradigm achieves seamless integration and symbiosis evolution between security and cloud native. We implement a trial of iCS based on 5GC commercial system and evaluate its performance on costs, efficiency and attack success. The result shows that the iCS enhanced mode always can provide a better and more stable defense effects."
pub.1122383781,Cloud and Internet of Things Technologies for Supporting In-House Informal Caregivers: A Conceptual Architecture,"Persons in a situation of dependency, or independent but with deficiencies in their autonomy, have specific needs for a better management of their long-term care. New sensing technologies based on real-time location systems, mobile apps, the Internet-of-Things (IoT) paradigm and cloud systems can be used to collect and process information about their activity and their environment in a continuous and truthful way. In this chapter, we analyse current solutions available to support informal caregivers and propose an innovative framework based on the integration of existing IoT products and services of cloud architectures. From the technological point of view, the system we propose is focused on the integration and combination of technologies for providing support for the informal caregiver in long-term care. The differential factor of these technologies is the customization level according to the specific context of the end-users. The main contribution of the proposed systems relies on the intelligence and the management of recorded events to create complex and reliable alerts, and its ability to configure multiple end-user instances and configurations (e.g.: needs, countries, regions, cultures). These type of systems should be sustainable and efficient, and that is why the inclusion of cloud technologies can grant its flexibility and scalability."
pub.1163728859,Secure and Efficient Runtime Environment for Smart Contracts on JointCloud,"Many cloud providers, including Amazon, Google, Microsoft, and Alibaba Cloud, offer support for blockchain cloud services that rely on a runtime environment, such as the Ethereum Virtual Machine (EVM), to execute smart contracts and ensure consistency between participants. However, existing runtime systems suffer from two main limitations. Firstly, traditional runtime systems like EVM cannot guarantee privacy protection as all the data uploaded to the blockchain is visible to all participants. This restricts the use of blockchain in limited scenarios. Secondly, each computation on the runtime system must be synchronized to all nodes in the network, resulting in a significant increase in computational overhead, which can be challenging to implement for more complex applications. One approach to address these limitations is to utilize Trusted Execution Environments (TEE) for blockchain runtime, which can provide privacy protection and mitigate redundant synchronization operations. However, using TEE for blockchain may significantly increase cloud costs. To overcome these challenges, this paper proposes PL-EVM, a new runtime environment for smart contracts that utilizes jointcloud. PL-EVM achieves high-security guarantees by using TEE to protect privacy-sensitive data and incorporates dynamic migration and splitting mechanisms to achieve high efficiency and low costs. Our evaluation results show that PL-EVM can improve performance and reduce costs by 4% to 32.22%."
pub.1027324131,DIaaS: Resource Management System for the Intra-Cloud with On-Premise Desktops,"Infrastructure as a service with desktops (DIaaS) based on the extensible mark-up language (XML) is herein proposed to utilize surplus resources. DIaaS is a traditional surplus-resource integrated management technology. It is designed to provide fast work distribution and computing services based on user service requests as well as storage services through desktop-based distributed computing and storage resource integration. DIaaS includes a nondisruptive resource service and an auto-scalable scheme to enhance the availability and scalability of intra-cloud computing resources. A performance evaluation of the proposed scheme measured the clustering performance time for surplus resource utilization. The results showed improvement in computing and storage services in a connection of at least two computers compared to the traditional method for high-availability measurement of nondisruptive services. Furthermore, an artificial server error environment was used to create a clustering delay for computing and storage services and for nondisruptive services. It was compared to the Hadoop distributed file system (HDFS)."
pub.1044850611,ServBGP: BGP-inspired autonomic service routing for multi-provider collaborative architectures in the cloud,"In this paper we propose the design and implementation of ServBGP, a service routing protocol for managing service collaboration among cloud providers in cloud computing. ServBGP is based on the policy-driven design of the well-known BGP Internet routing protocol to support the different service interaction models currently employed in the cloud particularly the monolithic, composite, and broker-based service models. The main contribution of this work lies in devising a system that autonomously manages the different aspects of service interaction and collaboration among service providers from service discovery and advertisement to service consumption and revocation. The ServBGP routing decision engine is designed to operate by (1) processing cost-bidding service advertisement and revocation messages from the different cloud providers and (2) extracting objective and trustworthy feedback which is maintained by certified network entities and which consists of a set of reputation scores related to quality of service, security, and pricing. The BGP-inspired design choice allows the proposed system to “stand upon the shoulders of giants” by relying on a time-tested protocol that currently supports scalable inter-domain routing services among over 120,000 IPv4 prefixes in the Internet. Moreover, reusing the decision logic of the standard BGP protocol aids in reducing the development and operational costs, hence shortening the time to market, and utilizing a large set of BGP extensions, particularly, in the security domain, that have been incorporated into BGP since its inception in 1989. These qualities are of a chief importance in the cloud community today to ensure the efficient and secure service collaboration among cloud providers and to support the evolution, scalability, and economic feasibility of the overall cloud computing infrastructure. A proof of concept implementation of the ServBGP design is realized on the NetKit network emulation and virtualization platform using the standard BGPv4 protocol. Moreover, the devised ServBGP design is developed from the ground up and deployed in a real cloud computing environment using Amazon EC2, Microsoft Azure, and Google App Engine cloud platforms."
pub.1104157120,Cloud parallel spatial‐temporal data model with intelligent parameter adaptation for spatial‐temporal big data,"Summary With the fast development of earth observation technology and internet of things technology, the spatial‐temporal data can be obtained with higher speed and lower cost, and spatial‐temporal big data management with existing spatial‐temporal data model has become one of the bottlenecks of spatial‐temporal applications such as e‐government construction, digital city, and smart city. Cloud parallel spatial‐temporal data model with intelligent parameter adaptation for spatial‐temporal big data provided in this paper is able to divide a spatial‐temporal problem into a lot of subdivided spatial‐temporal problems and to map the subdivided problems onto different cloud parallel computing nodes to process. This paper includes the concept, division methods, and mathematical formulas of cloud parallel spatial‐temporal data model and provides the method to intelligently find the best parameter of cloud parallel spatial‐temporal data model for solving the problem with highest parallel speedup or highest parallel efficiency in cloud parallel computing environment."
pub.1094798908,A Novel Approach to Enhance Multi Level Security System Using Encryption with Fingerprint in Cloud,"Cloud computing paradigm is still evolving and it is a long dreamed vision as a utility, where users can remotely store their data and enjoy the high quality of on-demand services and applications from a pool of shared configurable resources. It has recently gained tremendous momentum. However, privacy issues and security pose as the key roadblock to its fast adoption. Security is one of the major issues which declines the growth of the cloud computing and complications with the protection of data and privacy continue to plague the market. Cloud service users must be vigilant in data breaches of risk in this new environment. In this paper, a approach of existing different security issues that has emanated due to the service and the idea of implementing the fingerprints in the upcoming models that shows the proposed schemes are provably secure and highly efficient."
pub.1106438650,C-DIDS: A Cooperative and Distributed Intrusion Detection System in Cloud environment,"Cloud Computing is a new promoted technology today. The adoption of the cloud computing has been increased for both enterprises and end-users. But, the major critical problem of this technology is its security that is becoming more and more serious. Thus, intrusion detection has been considered the best solution to secure the Cloud platform. Therefore, the main aim of all new researches is to retrieve an effective and performing intrusion detection system adaptable to cloud environment. In this paper, we propose a new Intrusion Detection System (IDS) for the –+8 rates and detection delay in order to make real time detection in the whole of Cloud platform based on the OpenStack framework. The results show that the C-DIDS is more efficient in term of delay compared to the existing GCCIDS solution."
pub.1100776166,$n$-Dimensional QoS Framework for Real-Time Service-Oriented Architectures,"Service-Orientation has long provided an effective mechanism to integrate heterogeneous systems in a loosely coupled fashion as services. However, with the emergence of Internet of Things (IoT) there is a growing need to facilitate the integration of real-time services executing in non-controlled, non-real-time, environments such as the Cloud. With the need to integrate both cyberphysical systems as hardware-in-the-loop (HIL) components and also with Simulation as a Service (SIMaaS) the execution performance and response-times of the services must be managed. This paper presents a mathematical framework that captures the relationship between the host execution environment and service performance allowing the estimation of Quality of Service (QoS) under dynamic Cloud workloads. A formal mathematical definition is provided and this is evaluated against existing techniques from both the Cloud and Real-Time Service Oriented Architecture (RT-SOA) domains. The proposed approach is evaluated against the existing techniques through simulation and demonstrates a reduction of QoS violation percentage by 22% with respect to response-times as well as reducing the number of Micro-Service $(u\mathbf{S})$ instances with QoS violations by 27%."
pub.1170052966,A Framework Promoting Position Trust Evaluation System in Cloud Environment,"Trust management is one of the biggest obstacles to the adoption and growth of cloud computing. Both consumers and corporations commonly use cloud computing. Protecting the customer’s privacy would not be a simple task due to the sensitive data contained in the interaction between the customer and the trust evaluation service. It might be challenging to defend cloud services against malicious users. The design and development of CloudArmor, a legacy trust evaluation framework that offers a set of functions to provide Trust as a Service, will be discussed in this chapter (TaaS). This chapter also discusses the problems with calculating trust based on input from cloud users. This technology effectively protects cloud services by detecting hostile and inappropriate behavior by the use of trust algorithms, which may recognize on/off assaults and colluding attacks by using different security criteria. In conclusion, the findings demonstrate that the suggested trust model system may deliver high security by lowering security risk and enhancing the decision-making capabilities of cloud users and cloud operators."
pub.1139348098,Computation offloading and content caching and delivery in Vehicular Edge Network: A survey,"The past decade has witnessed the widespread adoption of Cloud Computing (CC) across automotive industries for a myriad of vehicular applications. A vehicular network that solely relies on CC, however, is susceptible to end-to-end latency due to the round-trip between data sources and cloud servers. Alternatively, the computing capability has been considered at the edge of vehicular network to achieve real-time analytics. Despite that, such consideration poses new questions on how data is offloaded and cached among the edge nodes and Autonomous Vehicles (AVs) in the environment of Vehicular Edge Network (VEN). In this paper, we outlined the aspects of VEN, particularly Vehicular Edge Computing (VEC), together with its architecture, layers, communications, and applications that are involved in the computation offloading (ComOf) and content caching and delivery (CachDel) scenarios. We extensively reviewed the existing approaches in solving ComOf and CachDel problems for the respective VEC architecture. The security aspect in ComOf and CachDel were critically discussed as well in the paper. Finally, we highlighted some key challenges, open issues, and future works of ComOf and CachDel in VEC."
pub.1118188115,Multi-Objective Problem Solving With Offspring on Enterprise Clouds,"In this paper, we present a distributed implementation of a network based
multi-objective evolutionary algorithm, called EMO, by using Offspring. Network
based evolutionary algorithms have proven to be effective for multi-objective
problem solving. They feature a network of connections between individuals that
drives the evolution of the algorithm. Unfortunately, they require large
populations to be effective and a distributed implementation can leverage the
computation time. Most of the existing frameworks are limited to providing
solutions that are basic or specific to a given algorithm. Our Offspring
framework is a plug-in based software environment that allows rapid deployment
and execution of evolutionary algorithms on distributed computing environments
such as Enterprise Clouds. Its features and benefits are presented by
describing the distributed implementation of EMO."
pub.1143800743,Accelerating Subsurface Data Processing and Interpretation with Cloud-Based Full Waveform Inversion Systems,"Abstract
                  Understanding the earth's subsurface is critical to the needs of the exploration and production (E&P) industry for minimizing risk and maximizing recovery. Until recently, the industry's service sector has not made many advances in data-driven automated earth model building from raw exploration seismic data. But thankfully, that has now changed. The industry's leading technique to gain an unprecedented increase in resolution and accuracy when establishing a view of the interior of the earth is known as the Full Waveform Inversion (FWI). Advanced formulations of FWI are capable of automating subsurface model building using only raw unprocessed data.
                  Cloud-based FWI is helping to accelerate this journey by encompassing the most sophisticated waveform inversion techniques with the largest compute facility on the planet. This combines to give verifiable accuracy, more automation and more efficiency. In this paper, we describe the transformation of enabling cloud-based FWI to natively take advantage of the public cloud platform's main strength in terms of flexibility and on-demand scalability. We start from lift-and-shift of a legacy MPI-based application designed to be run by a traditional on-prem job scheduler.
                  Our specific goals are to (1) utilize a heterogeneous set of compute hardware throughout the lifecycle of a production FWI run without having to provision them for the entire duration, (2) take advantage of cost-efficient spare-capacity compute instances without uptime guarantees, and (3) maintain a single codebase that can be run both on on-prem HPC systems and on the cloud. To achieve these goals meant transitioning the job-scheduling and ""embarrassingly parallel"" aspects of the communication code away from using MPI, and onto various cloud-based orchestration systems, as well as finding cloud-based solutions that worked and scaled well for the broadcast/reduction operation. Placing these systems behind a customized TCP-based stub for MPI library calls allows us to run the code as-is in an on-prem HPC environment, while on the cloud we can asynchronously provision and suspend worker instances (potentially with very different hardware configurations) as needed without the burden of maintaining a static MPI world communicator.
                  With this dynamic cloud-native architecture, we 1) utilize advanced formulations of FWI capable of automating subsurface model building using only raw unprocessed data, 2) extract velocity models from the full recorded wavefield (refractions, reflections and multiples), and 3) introduce explicit sensitivity to reflection moveout, invisible to conventional FWI, for macro-model updates below the diving wave zone. This makes it viable to go back to older legacy datasets acquired in complex environments and unlock considerable value where FWI until now has been impossible to apply successfully from a poor starting model."
pub.1147860876,CoPaM: Cost-aware VM Placement and Migration for Mobile services in Multi-Cloudlet environment: An SDN-based approach,"Edge Cloud Computing (ECC) is a new approach for bringing Mobile Cloud Computing (MCC) services closer to mobile users in order to facilitate the complicated application execution on resource-constrained mobile devices. The main objective of the ECC solution with the cloudlet approach is mitigating the latency and augmenting the available bandwidth. This is basically done by deploying servers (a.k.a. “Cloudlets”) close to the user’s device on the edge of the cellular network. When considering the users’ mobility along with the limited resource of the cloudlets serving them, the user-cloudlet communication may need to go through multiple hops, which may seriously affect the communication delay between them and the quality of services (QoS). To reduce as much as possible the negative effects of such a QoS degradation, service execution must be dynamically migrated to a better placement. This study proposes a novel Cost-aware Virtual Machine (VM) placement and migration (CoPaM) framework for mobile services in a network of cloudlets. A Mixed Integer Linear Programming (MILP) model is proposed to select the least cost cloudlets for serving mobile users to a given trajectory in advance based on path prediction methods. Since the proposed model is NP-hard, it is not applicable within large-scale environments with a centralized approach and the use of Software Defined Networking (SDN) technology. Therefore, a heuristic algorithm is presented. Experiments are conducted by emulating the proposed framework in Mininet-WiFi with the Floodlight usage as the SDN controller. The simulation results show the superiority of the performance of the proposed architecture in terms of the service rate and the costs imposed on the operator in comparison to existing approaches."
pub.1151856807,Hybrid Meta-heuristic Genetic Algorithm: Differential Evolution Algorithms for Scientific Workflow Scheduling in Heterogeneous Cloud Environment,"The gaint cabailities of cloud computing in providing online services via Internet attract the attention of the distributed sector due to its huge abilities that include storage, processing, software, databases, and servers that are shared simultaneously over the Internet by remote users geographically dispersed. Increasing the enormous amount of generating data through big data platforms and the use of IoT devices connected via the network have exploited the computational power of the cloud. However, the high utilization of the cloud leads to a longer execution time for a specific task. This paper proposing the hybrid strategy of scheduling the workflow in cloud computing called Genetic Algorithm with Differential Evolution (GA-DE). This research aims to investigate how heterogeneous cloud computing affects workflow scheduling. This study is aimed at reducing makespan and verifying if the metaheuristic technology is more suitable for the distributed environment by comparing it to existing heuristics, such as HEFT-Downward Rank,HEFT-Upward Rank,HEFT-Level Rank, and meta-heuristic algorithm GA. The proposed algorithm is validated through extensive experiments compared to three scientific workflows (Epigenomics,Cybershake,and Montage). Based on the simulation result GA-DE algorithm proves its superiority against the other comparing algorithms in term of makespan. Furthermore, the conducted experiment proves that montage scientific workflow is more proper for executing workflow scheduling in heterogeneous cloud computing."
pub.1015753741,Implementation of SLA Management System for QoS Guarantee in Cloud Computing Environment,"Internet is becoming more common with increasing cloud services have been spreading rapidly. A SLA is agreements between service providers and customers of service providers is how to ensure quality of service. SLA in cloud computing environments from the perspective of the IT service providers to customer satisfaction increase for service quality, it will need to differentiate between competing carriers QoS guarantees with SLA is a very important factor. However, the study of SLA in the cloud is staying in its infancy. In this paper, SLA indices for cloud services defining and use them SLA Management System for QoS guarantee has been implemented. The proposed system using monitoring-based migration policy an open source-based cloud computing platform which Cluster Nodes load distribution of virtual machines assigned to their availability, response time, throughput analysis of what affects and is compared with existing cloud."
pub.1173286367,A New Layer Structure of Cyber-Physical Systems under the Era of Digital Twin,"Cyber-Physical Systems (CPS) are new systems designed to support and synthesize sensing, communication, and computing components that interact with physical objects so that the system can sense, monitor, control, and respond to changes occurring in their operating environments. With developing Internet of Things (IoT), edge/cloud computing, and Artificial Intelligence (AI), another new paradigm - Digital Twin (DT), as an enabler for realizing the Metaverse of CPS has emerged recently. The DT provides a virtual replica of the physical world, allowing for real-time monitoring, control, and analysis of safety-critical systems like the CPS. Even though many CPS-related problems can be addressed using DT concepts, it is critical to define DT’s integration clearly to leverage its features to the best possible extent. This paper introduces a new layer called the data layer in the CPS layer architecture to support different DT functionalities. Like the layer and abstract design paradigm of modern operation systems and computer networks, the data layer serves as an abstraction layer, providing immense flexibility and interchangeability of various data sources, supporting applications, and adapting to other components and environments. This also accelerates the growth of smart-world systems and their integration in different application domains (smart grid, smart transportation, smart manufacturing, smart cities, smart healthcare, etc.). To this end, this paper presents a new data layer structure to support CPS applications and the integration of multiple CPS. We propose a hierarchical architecture to synthesize DT techniques as an independent layer in our architecture, deeply integrated into existing CPS. We discuss key issues: leveraging the data layer to support applications in various CPS, incorporating the data layer and physical layer, and building a data layer using distributed computing, naming services, etc. Finally, the paper outlines future research directions based on the presented new data layer architecture for CPS."
pub.1105884690,On the Security of Medical Image Processing in Cloud Environment,"The implementation of cloud computing in the healthcare domain offers an easy and ubiquitous access to off-site solutions to manage and process electronic medical records. In this concept, distributed computational resources can be shared by multiple clients in order to achieve significant cost savings. However, despite all these great advantages, security and privacy remain the major concerns hindering the wide adoption of cloud technology. In this respect, there have been many attempts to strengthen the trust between clients and service providers by building various security countermeasures and mechanisms. Amongst these measures, homomorphic algorithms, Service-Oriented Architecture (SOA) and Secret Share Scheme (SSS) are frequently used to mitigate security risks associated with cloud. Unfortunately, these existing approaches are not able to provide a practical trade-off between performance and security. In the light of this fact, we use a simple method based on fragmentation to support a distributed image processing architecture, as well as data privacy. The proposed methods combine a clustering method, the Fuzzy C-Means (FCM) algorithm, and a Genetic Algorithm (GA) to satisfy Quality of Service (QoS) requirements. Consequently, the proposal is an efficient architecture for reducing the execution time and security problems. This is accomplished by using a multi-cloud system and parallel image processing approach."
pub.1061786662,THEMIS: A Mutually Verifiable Billing System for the Cloud Computing Environment,"With the widespread adoption of cloud computing, the ability to record and account for the usage of cloud resources in a credible and verifiable way has become critical for cloud service providers and users alike. The success of such a billing system depends on several factors: The billing transactions must have integrity and nonrepudiation capabilities; the billing transactions must be nonobstructive and have a minimal computation cost; and the service level agreement (SLA) monitoring should be provided in a trusted manner. Existing billing systems are limited in terms of security capabilities or computational overhead. In this paper, we propose a secure and nonobstructive billing system called THEMIS as a remedy for these limitations. The system uses a novel concept of a cloud notary authority for the supervision of billing. The cloud notary authority generates mutually verifiable binding information that can be used to resolve future disputes between a user and a cloud service provider in a computationally efficient way. Furthermore, to provide a forgery-resistive SLA monitoring mechanism, we devised a SLA monitoring module enhanced with a trusted platform module (TPM), called S-Mon. The performance evaluation confirms that the overall latency of THEMIS billing transactions (avg. 4.89 ms) is much shorter than the latency of public key infrastructure (PKI)-based billing transactions (avg. 82.51 ms), though THEMIS guarantees identical security features as a PKI. This work has been undertaken on a real cloud computing service called iCubeCloud."
pub.1048011722,Learning for test prioritization: an industrial case study,"Modern cloud-software providers, such as Salesforce.com, increasingly adopt large-scale continuous integration environments. In such environments, assuring high developer productivity is strongly dependent on conducting testing efficiently and effectively. Specifically, to shorten feedback cycles, test prioritization is popularly used as an optimization mechanism for ranking tests to run by their likelihood of revealing failures. To apply test prioritization in industrial environments, we present a novel approach (tailored for practical applicability) that integrates multiple existing techniques via a systematic framework of machine learning to rank. Our initial empirical evaluation on a large real-world dataset from Salesforce.com shows that our approach significantly outperforms existing individual techniques."
pub.1093456285,MeDICINE: Rapid Prototyping of Production-Ready Network Services in Multi-PoP Environments,"Virtualized network services consisting of multiple individual network functions are already today deployed across multiple sites, so called multi-PoP (points of presence) environments. This allows to improve service performance by optimizing its placement in the network. But prototyping and testing of these complex distributed software systems becomes extremely challenging. The reason is that not only the network service as such has to be tested but also its integration with management and orchestration systems. Existing solutions, like simulators, basic network emulators, or local cloud testbeds, do not support all aspects of these tasks. To this end, we introduce MeDICINE, a novel NFV prototyping platform that is able to execute production-ready network functions, provided as software containers, in an emulated multi-PoP environment. These network functions can be controlled by any third-party management and orchestration system that connects to our platform through standard interfaces. Based on this, a developer can use our platform to prototype and test complex network services in a realistic environment running on his laptop."
pub.1149670348,Density-Based Machine Learning Scheme for Outlier Detection in Smart Forest Fire Monitoring Sensor Cloud,"<p>Sensor Cloud is an integration of sensor networks with cloud where sensed data is stored and processed in the cloud. The applications of sensor cloud can be seen in forest fire monitoring, healthcare system, and other Internet-of-Things systems. Outliers may present within this data due to malicious activities, low-quality sensors, or node deployment in harsh environments. Such outliers must be detected timely for effective decision making. Many clustering-based machine learning schemes for outlier detection have been devised. However, accuracy of these techniques can be further improved. This paper proposes a density-based machine learning scheme (DBS) for outlier detection which is implemented in Python and executed on the two datasets of different forest fire monitoring networks. DBS makes density-based clusters of all data points where outliers lie in low-density region. The use of a density-based model in the proposed approach improves precision, throughput, and accuracy. DBS outperforms the existing Mean Shift and K Means based clustering schemes with maximum accuracy 98.40%.</p>"
pub.1094218906,Bungee: an Elasticity Benchmark for Self-Adaptive IaaS Cloud Environments,"Today's infrastructure clouds provide resource elasticity (i.e. auto-scaling) mechanisms enabling self-adaptive resource provisioning to reflect variations in the load intensity over time. These mechanisms impact on the application performance, however, their effect in specific situations is hard to quantify and compare. To evaluate the quality of elasticity mechanisms provided by different platforms and configurations, respective metrics and benchmarks are required. Existing metrics for elasticity only consider the time required to provision and de-provision resources or the costs impact of adaptations. Existing benchmarks lack the capability to handle open workloads with realistic load intensity profiles and do not explicitly distinguish between the performance exhibited by the provisioned underlying resources, on the one hand, and the quality of the elasticity mechanisms themselves, on the other hand. In this paper, we propose reliable metrics for quantifying the timing aspects and accuracy of elasticity. Based on these metrics, we propose a novel approach for benchmarking the elasticity of Infrastructure-as-a-Service (I $a$ aS) cloud platforms independent of the performance exhibited by the provisioned underlying resources. We show that the proposed metrics provide consistent ranking of elastic platforms on an ordinal scale. Finally, we present an extensive case study of real-world complexity demonstrating that the proposed approach is applicable in realistic scenarios and can cope with different levels of resource efficiency."
pub.1147071459,Digital Twin Applications: A Survey of Recent Advances and Challenges,"Industry 4.0 integrates a series of emerging technologies, such as the Internet of Things (IoT), cyber-physical systems (CPS), cloud computing, and big data, and aims to improve operational efficiency and accelerate productivity inside the industrial environment. This article provides a series of information about the required structure to adopt Industry 4.0 approaches and a brief review of related concepts to finally identify challenges and research opportunities to envision the adoption of so-called digital twins. We want to pay attention to upgrading older systems aiming to provide the well-known advantages of Industry 4.0 to such legacy systems as reducing production costs, increasing efficiency, acquiring better robustness of equipment, and reaching advanced process connectivity."
pub.1176205628,IBOA: Cost-aware Task Scheduling Model for Integrated Cloud-fog Environments,"Scheduling is an NP-hard problem, and metaheuristic algorithms are often used to find approximate solutions within a feasible time frame. Existing metaheuristic algorithms, such as ACO, PSO, and BOA address this problem either in cloud or fog environments. However, when these environments are combined into a hybrid cloud-fog environment, these algorithms become inefficient due to inadequate handling of local and global search strategies. This inefficiency leads to suboptimal scheduling across the cloud-fog environment because the algorithms fail to adapt effectively to the combined challenges of both environments. In our proposed Improved Butterfly Optimization Algorithm (IBOA), we enhance adaptability by dynamically updating the computation cost, communication cost, and total cost, effectively balancing both local and global search strategies. This dynamic adaptation allows the algorithm to select the best resources for executing tasks in both cloud and fog environments. We implemented our proposed approach in the CloudSim simulator and compared it with traditional algorithms such as ACO, PSO, and BOA. The results demonstrate that IBOA offers significant reductions in total cost, communication cost, and computation cost by 19.65%, 18.28%, and 25.41%, respectively, making it a promising solution for real-world cloud-fog computing (CFC) applications."
pub.1164647359,Data Security Risk Mitigation in the Cloud Through Virtual Machine Monitoring,"Cloud computing offers on demand, pay per use, and remote access to shared pool of resources, because of which several organizations are moving towards cloud. With increased adoption of cloud computing by number of organizations, data security and privacy concerns in cloud have also become increasingly prominent. It is important to maintain data security in the cloud to avoid any kind of data breaches. However, the problem of identifying the optimal solution for data security in cloud environment is very challenging because of various security and performance constraints. The aim of this paper is to emphasize the importance of data security in cloud computing by analyzing various data security techniques. The paper proposes a taxonomy of authentication, verification, and mitigation in multi-agent system cloud monitoring for data security risk mitigation in cloud computing environment. The authentication process will include node authentication, mutual authentication, data authentication and wireless sensor network (WSN) authentication. Verification will be based on the use of digital certificates and access control implementation. Mitigation will involve single sign on solution and live migration approach. The contribution of this work into the wide research area of data security in cloud computing is demonstrating various tools and techniques for data security risk mitigation. We verify the utility of the authentication, verification, and mitigation taxonomy by analyzing 15 research journals. This study will provide a comprehensive insight into already existing techniques for data security risk mitigation in cloud computing."
pub.1126826702,From cloud computing to fog computing: two technologies to serve iot–a review-,"The emergence of IoT (Internet of Things) systems brings several development opportunities to many vital fields such as healthcare, agriculture, smart building, smart cities, environment monitoring, and transport-logistic domain. The amount of data generated by IoT is increasing rapidly, which creates many research challenges in data management chain, furthermore, transmission of such amount of data requires large bandwidth, and produces significant delay. The contribution of this paper is to give a starting point to new searchers interested in IoT data management topic. Thus, we draw up a review of IoT technology, and show its impact on big data evolution, then show how fog computing, as a new paradigm, can contribute to IoT development. Firstly, RFID (Radio Frequency identification) technology is presented, then the integration of big data with IoT systems is discussed, numerous existing cloud IoT infrastructures are inventoried later, followed by a presentation of fog computing advantages and open challenges."
pub.1168211948,A Review Study on Energy Consumption in Cloud Computing,"Cloud computing has become a fundamental technology for a wide range of computing services, yet its increasing energy demands present substantial environmental and economic challenges. With the rapid growth of Cloud services and applications, increasing number of researches have been focused on energy saving. The need to reduce energy costs is a constant challenge of cloud providers and data centers. This paper offers an extensive review of the issues surrounding energy consumption in cloud computing, with a focus on algorithms associated with the situational awareness, consolidation, allocation, placement/migration, and scheduling of virtual machines and containers. We conduct a critical analysis of studies from 2018 to 2023, comparing various methodologies aimed at achieving energy efficiency without sacrificing performance. This review delineates current trends, identifies gaps in existing research, and proposes directions for future investigations. Our study emphasizes the necessity of cultivating sustainable practices in cloud computing and provides valuable insights into the practical implementation of energy-efficient solutions in cloud environments."
pub.1043925744,Open Standards for Service-Based Database Access and Integration,"The Database Access and Integration Services (DAIS) Working Group, working within the Open Grid Forum (OGF), has developed a set of data access and integration standards for distributed environments. These standards provide a set of uniform web service-based interfaces for data access. A core specification, WS-DAI, exposes and, in part, manages data resources exposed by DAIS-based services. The WS-DAI document defines a core set of access patterns, messages and properties that form a collection of generic high-level data access interfaces. WS-DAI is then extended by other specifications that specialize access for specific types of data. For example, WS-DAIR extends the WS-DAI specification with interfaces targeting relational data. Similar extensions exist for RDF and XML data. This chapter presents an overview of the specifications, the motivation for defining them and their relationships with other OGF and non-OGF standards. Current implementations of the specifications are described in addition to some existing and potential applications to highlight how this work can benefit web service-based architectures used in Grid and Cloud computing.The Database Access and Integration Services (DAIS) Working Group, working within the Open Grid Forum (OGF), has developed a set of data access and integration standards for distributed environments. These standards provide a set of uniform web service-based interfaces for data access. A core specification, WS-DAI, exposes and, in part, manages data resources exposed by DAIS-based services. The WS-DAI document defines a core set of access patterns, messages and properties that form a collection of generic high-level data access interfaces. WS-DAI is then extended by other specifications that specialize access for specific types of data. For example, WS-DAIR extends the WS-DAI specification with interfaces targeting relational data. Similar extensions exist for RDF and XML data. This chapter presents an overview of the specifications, the motivation for defining them and their relationships with other OGF and non-OGF standards. Current implementations of the specifications are described in addition to some existing and potential applications to highlight how this work can benefit web service-based architectures used in Grid and Cloud computing."
pub.1152306129,Reinforcement Learning-Based Optimization Framework for Application Component Migration in NFV Cloud-Fog Environments,"By decoupling network functions from the underlying hardware, Network Function Virtualization (NFV) allows application components to be implemented as sets of Virtual Network Functions (VNFs) chained in a specific order, represented by VNF-Forwarding Graphs (VNF-FG). Fog computing is instrumental to tap into the full potential of NFV by deploying VNFs in close proximity to end-users, thus decreasing the latency significantly. However, the mobility of end-users and the fog nodes, and the limited fog nodes coverage results in service discontinuity and may increase application delay. Application component migration offers great potential to address this issue. In this paper, we propose a component migration strategy in an NFV-based hybrid cloud/fog system considering the mobility of both end-users and fog nodes. We use the Gauss-Markov mobility model and a random walk mobility model for fog nodes and end-user devices, respectively. We modeled the problem mathematically, which minimizes the aggregated weighted function of application delay and cost. However, considering the mobility of both end-users and fog nodes makes the problem quite complex. Hence, we propose a Deep Reinforcement Learning (DRL) approach to decide where and when to migrate application components and to achieve rapid decision-making. Simulation results demonstrate that the proposed scheme performs well. It offers favorable convergence and outperforms existing algorithms in terms of application delay and migration costs."
pub.1181742709,Quantum-Inspired Cryptography Protocols for Enhancing Security in Cloud Computing Infrastructures,"As we know, cloud computing plays a vital role in our lives. It also plays a pivotal role in modern business and information technology landscapes. But with this, there is a rise in concerns regarding the security and privacy of all data which is stored in the cloud. To tackle these concerns some effective and traditional cryptographic methods are present but they face potential vulnerabilities due to advent of quantum computing. It also threatens the basis of widely used encryption algorithms and models. This research focuses on the usage of quantum mechanics inspired cryptographic protocols, which will help to strengthen the security of cloud computing models and infrastructures. The research begins with a detailed overview of the current situation and state of security of cloud computing. There is identification of all challenges and vulnerabilities faced due to current, classical and traditional cryptographic techniques and algorithms. As we know that there is a very close arrival of quantum computing, we are surrounded by mighty and latent threats because of quantum computing algorithms that can easily overcome the widely accepted and used encryption standards and models. So, there is a dire need to develop and implement some alternative cryptographic strategies and measures.
 To overcome these challenges, this research proposes the creation and usage of quantum-inspired cryptographic protocols which are designed to resist attacks from both traditional and quantum vulnerabilities. These protocols are inspired from quantum mechanics, like superposition and entanglement. These protocols will help to design and create cryptographic systems and models that offer enhanced security. The research focuses on the theoretical foundations of these quantum mechanics inspired protocols and assesses their practical feasibility within cloud computing infrastructures and environment.This research contains information regarding the challenges associated with integration and deployment of quantum inspired cryptographic solutions in cloud computing. It evaluates the scalability, efficiency, performance and computational overhead of these quantum inspired protocols to ensure their practical feasibility and capability in existing cloud computing environment and infrastructures. This research also includes comparison performed with quantum cryptographic protocols and traditional cryptographic methods. The results aim to provide evidence that quantum cryptographic techniques are better, feasible and secure options to protect data over cloud. In Conclusion, the research aims to provide an insight that development and usage of advanced security measures can withstand and handle evolving threats, ensuring the continued security and protection of data in cloud computing environment by providing theoretical insights, analysis and evolution."
pub.1173377765,Adaptive Container Service: a New Paradigm for Robust and Optimized Bioinformatics Workflow Deployment in the Cloud,"Abstract We propose Adaptive Container Service (ACS), a new paradigm for deploying bioinformatics workflows in cloud computing environments. By encapsulating the entire workflow within a single virtual container, combined with automatic workflow checkpointing and dynamic migration to appropriately scaled containers, ACS-based deployment demonstrates several key advantages over alternative strategies: it enables optimal resource provision to any workflow that comprise of multiple applications with diverse computing needs; it provides protection against application-agnostic out-of-memory (OOM) errors or spot instance interruptions; and it reduces efforts required for workflow development, optimization, and management because it runs workflows with minimal or no code modifications. Proof-of-concept experiments show that ACS avoided both under- and over-provisioning in monolithic single-container deployment. Despite being deployed as a single container, it achieved comparable resource utilization efficiency as optimized Nextflow-managed, multi-modular workflows. Analysis of over 18,000 workflow runs demonstrated that ACS can effectively reduce workflow failures by two-thirds. These findings suggest that ACS frees developers from navigating the complexity of deploying robust workflows and rightsizing compute resources in the cloud, leading to significant reduction in workflow development time and savings in cloud computing costs."
pub.1182119859,Zero Trust Security: Reimagining Cyber Defense for Modern Organizations,"In an era where cyber threats are growing in frequency and sophistication, traditional perimeter-based security models have proven inadequate for protecting modern organizational infrastructures. As digital transformation accelerates, driven by remote work, cloud adoption, and mobile device proliferation, organizations are adopting a new paradigm: Zero Trust Security. Zero Trust is a strategic approach to cybersecurity that assumes all network traffic, both external and internal, may be hostile. This model enforces strict identity verification, limited access, and continuous monitoring of every user, device, and system interaction within an organization’s network. This paper explores the principles and architecture of Zero Trust Security, outlining its core components such as Multi-Factor Authentication (MFA), micro-segmentation, Identity and Access Management (IAM), and least privilege access. By examining why organizations are shifting to this model, the paper highlights how Zero Trust addresses the limitations of conventional security approaches, including their vulnerability to insider threats and unauthorized lateral movement within networks. We discuss the benefits of implementing a Zero Trust strategy, including enhanced security, improved regulatory compliance, and the potential for significant cost savings. Additionally, we provide case studies demonstrating the successful adoption of Zero Trust in various sectors. The paper also addresses the challenges that organizations face when transitioning to a Zero Trust framework, including integration with legacy systems and managing user experience. Finally, we propose metrics for measuring Zero Trust effectiveness and include a cost-benefit analysis comparing traditional and Zero Trust security models over a five-year period. Through this comprehensive examination, the paper emphasizes the role of Zero Trust Security as a reimagined approach for robust cyber defense in today’s complex digital environment, offering actionable insights for organizations looking to modernize their security postures."
pub.1164172560,Unreachable Statements Are Inevitable in Software Testing: Theoretical Explanation,"Often, there is a need to migrate software to a new environment. The existing migration tools are not perfect. So, after applying such a tool, we need to test the resulting software. If a test reveals an error, this error needs to be corrected. Usually, the test also provides some warnings. One of the most typical warnings is that a certain statement is unreachable. The appearance of such warnings is often viewed as an indication that the original software developer was not very experienced. In this paper, we show that this view oversimplifies the situation: unreachable statements are, in general, inevitable. Moreover, a wide use of above-mentioned frequent view can be counterproductive: developers who want to appear more experienced will skip potentially unreachable statements and thus, make the software less reliable."
pub.1175055308,Hierarchical Service Composition via Blockchain-enabled Federated Learning,"In recent years, the transformative evolution of cloud computing has reshaped organizational practices by enabling the outsourcing of web service applications. This shift has led to the emergence of the cloud environment, characterized by the involvement of Cloud Service Providers (CSPs) and intelligent applications. Cloud Service Composition (CSC) has become pivotal in this context, playing a crucial role in enhancing efficiency, Quality of Service (QoS), and customer satisfaction through the aggregation of diverse Cloud Services (CSs) to create composite services. However, the vast array of available CSs presents a challenge in efficiently addressing specified QoS requirements, turning CSC into a recognized NP-hard problem. Existing solutions, often involving third-party brokers, struggle with scalability in large-scale systems and overlook crucial security concerns. To address these limitations, we propose the Hierarchical Service Composition (HSC) approach, leveraging blockchain and federated learning to minimize computational complexity. The integration of Blockchain-enabled Federated Learning (BFL) facilitates machine learning model training with decentralized data, ensuring practicality and fairness. HSC comprises an initialization phase and two selection layers. The first selection layer enables each CSP to efficiently select services using a pre-trained model, while the second selection layer employs a blockchain-based QoS-aware mechanism for the final composition result, addressing privacy concerns. HSC introduces a novel framework, collaborative service selection methods, and a smart selection algorithm, demonstrating remarkable composition efficiency in extensive simulations compared to the baseline approach."
pub.1174193426,Hybrid Grey Wolf and Particle Swarm Optimized (HGW-PSO) Scheduling in Containerized Cloud Computing Environment,"The concept of containerization consists of encapsulating applications with their libraries with the help of operating system level virtualization, ensuring they are separated from other processes on the same machine. Containers gained popularity in several data centers due to their lightweight nature and simplified deployment. Application containerization technology is a fundamental technology that enables the deployment of numerous containers on a single physical node. A single physical node has the capability to offer several services to users. The adoption of microservice architecture is on the rise as it offers several benefits over monolithic designs, such as simplified deployment, enhanced versatility, and improved accessibility. The rise of containerization has significantly increased the significance of microservices, enhancing their scalability and portability. Many researchers have developed theories and solutions for the microservice container scheduling problem, yet limits remain. The algorithm's tendency to become stuck in local optimization and its slow optimal solution search are examples. In this work we present a proposal for grey wolf and particle swarm optimized (GW-PSO) scheduling in a Containerized Cloud Computing Environment. During the first phase of the search, the GW's global search can direct the PSO away from local optima, preventing early convergence. In the last phases of the search, the PSO can improve GW search along the Pareto optimum frontier. The research findings demonstrate, in comparison to the other 3 methods, the algorithm achieves a 19.06% reduction in network transmission cost, a 15.57% improvement in load balancing performance, and a 6.67% enhancement in service stability."
pub.1117337190,When Data Management Meets Project Management,"Complex projects that collect, curate and analyse biodiversity data are often presented with the challenge of accommodating diverse data types, various curation and output workflows, and evolving project logistics that require rapid changes in the applications and data structures. At the same time, sustainability concerns and maintenance overheads pose a risk to the long term viability of such projects. We advocate the use of flexible, multiplatform tools that adapt to operational, day-to-day challenges while providing a robust, cost efficient, and maintainable framework that serves the needs data collectors, managers and users. EarthCape is a highly versatile platform for managing biodiversity research and collections data, associated molecular laboratory data (Fig. 1), multimedia, structured ecological surveys and monitoring schemes, and more. The platform includes a fully functional Windows client as well as a web application. The data are stored in the cloud or on-premises and can be accessed by users with various access and editing rights. Ease of customization (making changes to user interface and functionality) is critical for most environments that deal with operational research processes. For active researchers and curators, there is rarely time to wait for a cycle of development that follows a change or feature request. In EarthCape, most of the changes to the default setup can be implemented by the end users with minimum effort and require no programming skills. High flexibility and a range of customisation options is complemented with mapping to Darwin Core standard and integration with GBIF, Geolocate, Genbank, and Biodiversity Heritage Library APIs. The system is currently used daily for rapid data entry, digitization and sample tracking, by such organisations as Imperial College, University of Cambridge, University of Helsinki, University of Oxford. Being an operational data entry and retrieval tool, EarthCape sits at the bottom of Virtual Research Environments ecosystem. It is not a software or platform to build data repositories, but rather a very focused tool falling under ""back office"" software category. Routine label printing, laboratory notebook maintenance, rapid data entry set up, or any other of relatively loaded user interfaces make use of any industry standard relational database back end. This opens a wide scope for IT designers to implement desired integrations within their institutional infrastructure. APIs and developer access to core EarthCape libraries to build own applications and modules are under development. Basic data visualisation (charts, pivots, dashboards), mapping (full featured desktop GIS module), data outputs (report and label designer) are tailored not only to research analyses, but also for managing logistics and communication when working on (data) papers. The presentation will focus on the software platform featuring most prominent use cases from two areas: ecological research (managing complex net"
pub.1155031399,GlobalMatch: Registration of forest terrestrial point clouds by global matching of relative stem positions,"Registering point clouds of forest environments is an essential prerequisite for LiDAR applications in precision forestry. State-of-the-art methods for forest point cloud registration require the extraction of individual tree attributes, and they have an efficiency bottleneck when dealing with point clouds of real-world forests with dense trees. We propose an automatic, robust, and efficient method for the registration of forest point clouds. Our approach first locates tree stems from raw point clouds and then matches the stems based on their relative spatial relationship to determine the registration transformation. The algorithm requires no extra individual tree attributes and has quadratic complexity to the number of trees in the environment, allowing it to align point clouds of large forest environments. Extensive experiments on forest terrestrial point clouds have revealed that our method inherits the effectiveness and robustness of the stem-based registration strategy while exceedingly increasing its efficiency. Besides, we introduce a new benchmark dataset that complements the very few existing open datasets for the development and evaluation of registration methods for forest point clouds. The source code of our method and the dataset are available at https://github.com/zexinyang/GlobalMatch."
pub.1167138305,Toward optimizing scientific workflow using multi-objective optimization in a cloud environment,"Scientific workflows are a common and critical part of scientific computing, involving complex computations and oversized and distributed computing resources. Efficient workflow execution requires scheduling algorithms considering task dependencies, resource requirements, and deadlines. Cloud computing provides an innovative architecture for extensive heterogeneous computing services. However, scheduling hybrid cloud resources with deadline restrictions while observing QoS standards is an NP-complete task. Mapping workflow tasks to virtual machines and determining the optimal schedule order is a challenging aspect of cloud computing. By executing task requests on the most advantageous virtual machine in the resource pool, energy consumption, overall execution time, and computing costs can be reduced. This research aims to identify the best location to process applications using user’s demand and priority. A multi-objective genetic algorithm is proposed to achieve this objective, which considers conflicting objectives such as time, energy, cost, and deadline. The algorithm initializes an efficient ranking heuristic approach and predicts the earliest finish time (PEFT) using the Bayesian approach to improve the Pareto fronts. This approach enhances the VM migration of cloud-based tasks and optimizes the search space for conflicting objectives. Experimental findings show that the proposed approach reduces cost by 5–6% and time delay by 8% compared to existing approaches. The proposed approach offers an effective solution for scheduling scientific workflows on cloud computing resources while considering various QoS standards. The results demonstrate the potential of multi-objective genetic algorithms for optimizing workflow scheduling in cloud computing environments."
pub.1091493492,ARAAC: A Rational Allocation Approach in Cloud Data Center Networks,"The expansion of telecommunication technologies touches almost all aspects life that we are living nowadays. Indeed, such technologies have emerged as a fourth essential utility alongside the traditional utilities of electricity, water, and gas. In this context, Cloud Data Center Networks (cloud-DCNs) have been proposed as a promising way to cope with such a high-tech era and with any expected trends in future computing networks. Resources of cloud-DCNs are leased to the interested users in the form of services, such services come in different models that vary between software, platform, and infrastructure. The leasing process of any service model starts with the users (i.e., service tenants). A tenant asks for the service resources, and the cloud-provider allocates the resources with a charge that follows a predefined cost policy. Cloud resources are limited, and those cloud providers have profit objectives to be satisfied. Thus, to comply with the aforementioned promise, the limited resources need to be carefully allocated. Existing allocation proposals in the literature dealt with this problem in varying ways. However, none proposes a win-win allocation model that satisfies both the providers and tenants. This work proposes A Rational Allocation Approach in Cloud Data Center Networks (ARAAC) that efficiently allocates the available cloud resources, in a way that allows for a win-win environment to satisfy both parties: the providers and tenants. To do so, ARAAC deploys the Second Best-Price (SBP) mechanism along with a behavioral-based reputation model. The reputation is built according to the tenants’ utilization history throughout their previous service allocations. The reputation records along with the adoption of the SBP mechanism allows for a locally free-equilibrium approach that allocates the available cloud-DCN resources in an efficient and fair manner. In ARAAC, through an auction scenario, tenants with positive reputation records are awarded by having the required resources allocated at prices that are lower than what they have offered. Compared to other benchmark models, simulation results show that ARAAC can efficiently adapt the behavior of those rational service-tenants to provide for better use of the cloud resources, with an increase in the providers’ profits."
pub.1030109385,On the interplay of Internet of Things and Cloud Computing: A systematic mapping study,"The Internet of Things (IoT) is a novel paradigm relying on the interaction of smart objects (things) with each other and with physical and/or virtual resources through the Internet. Despite the recent advances that have made IoT a reality, there are several challenges to be addressed towards exploiting its full potential and promoting tangible benefits to society, environment, economy, and individual citizens. Recently, Cloud Computing has been advocated as a promising approach to tackle some of the existing challenges in IoT while leveraging its adoption and bringing new opportunities. With the combination of IoT and Cloud Computing, the cloud becomes an intermediate layer between smart objects and applications that make use of data and resources provided by these objects. On the one hand, IoT can benefit from the almost unlimited resources of Cloud Computing to implement management and composition of services related to smart objects and their provided data. On the other hand, the cloud can benefit from IoT by broadening its operation scope to deal with real-world objects. In spite of this synergy, the literature still lacks of a broad, comprehensive overview on what has been investigated on the integration of IoT and Cloud Computing and what are the open issues to be addressed in future research and development. The goal of this work is to fill this gap by systematically collecting and analyzing studies available in the literature aiming to: (i) obtain a comprehensive understanding on the integration of IoT and Cloud Computing paradigms; (ii) provide an overview of the current state of research on this topic; and (iii) identify important gaps in the existing approaches as well as promising research directions. To achieve this goal, a systematic mapping study was performed covering papers recently published in journals, conferences, and workshops, available at five relevant electronic databases. As a result, 35 studies were selected presenting strategies and solutions on how to integrate IoT and Cloud Computing as well as scenarios, research challenges, and opportunities in this context. Besides confirming the increasing interest on the integration of IoT and Cloud Computing, this paper reports the main outcomes of the performed systematic mapping by both presenting an overview of the state of the art on the investigated topic and shedding light on important challenges and potential directions to future research."
pub.1144951401,"Machine Learning, Deep Learning-Based Optimization in Multilayered Cloud","The ongoing COVID-19 pandemic has resulted in the loss of lives and economic losses. In this scenario, social distancing is the only way to protect ourselves. In such a scenario, to boost the economy, a large number of industries and businesses have shifted their system to cloud, for example education, shipping, training and many more globally. To support this transition cloud services are the only solution to provide reliable and secure services to the user to sustain their business. Due to this, the load on the existing cloud infrastructure has drastically increased. So it is the responsibility of the cloud to manage the load on the existing infrastructure to maintain reliability and provide high-quality services to the user. Task allocation in the cloud is one of the key features to optimize the performance of cloud infrastructure. In this work, we have proposed a prediction-based technique using a pre-trained neural network to find a reliable resource for a task based on previous training and the history of cloud and its performance to optimize the performance in overloaded and underloaded situations. The main aim of this work is to reduce faults and provide high performance by reducing scheduling time, execution time, average start time, average finish time and network load. The proposed model uses the Big Bang–Big Crunch algorithm to generate huge datasets for training our neural model. The accuracy of the BB–BC ANN model is improved with 98% accuracy. This chapter discusses the proactive, predictive power-aware fault-tolerant efficient scheduling technique which is based on a hybrid approach using the BB-BC algorithm and feedforward neural network. Evolution is a set of steps repeated in every evolution which include selection, crossover, mutation and the big crunch phase. The proposed algorithm tries to improve the fault tolerance of the cloud environment and to improve the performance of the system at the same time using machine learning techniques. The results and discussions section illustrates that BB-BC ANN performs better than existing meta-heuristic techniques. The ongoing COVID-19 pandemic has resulted in the loss of lives and economic losses. In this scenario, social distancing is the only way to protect ourselves. In this generation of growing need for fast computing power, cloud computing is the best solution to fulfill the upcoming needs and manage the performance of the data centers at the same time."
pub.1094163400,ENERGY AWARE CLOUD SERVICE PROVISIONING APPROACH FOR GREEN COMPUTING ENVIRONMENT,"As cloud computing becomes more emerging technology, there is increasing attention being paid to the energy consumption pattern across the entire information and communication technology (ICT) sector. With advent of cloud computing, computing functions and storage processing are migrating to remote resources like virtual servers stored in systems which are mostly hosted in the data centers located in various places. Virtualization technology has been used widely in modern data center in order to improve its energy efficiency. Virtual machine (VM) migration has recently emerged technique used as an essential building block for data center and storage systems, mainly due to its service provisioning and energy aware consolidation. We analysed the existing energy consumption model for various types of services in cloud computing environment and we present a new energy-aware provisioning approach by considering energy efficiency as a key factor. Our model uses pre-processed data about the service usage of cloud used for initiating the live migration. Our design encompasses the component called as trigger engine which initiates automatic migration of VM to preserve the computing environment green and energy friendly."
pub.1181994995,Cloud Accounting: A Theoretical Overview,"The literature on the topic of the Use and Integration of Accounting Software is quite extensive, but less broad in the context of challenges faced by SMEs, and even more and narrow when we look at the limited literature on the numerous challenges faced in a geographically dispersed location such as Oman. This literature review analyzes and investigates existing research on the challenges faced by SMEs in using and integrating accounting software, to find out the most significant and relevant variables previously identified through previous studies. The aim being to apply a select few of those variables and apply them to the context of Oman to identify the specific obstacles that Omani SMEs face. The literature review involved a methodical exploration of diverse academic databases, employing specific search terms such as “SMEs,” “accounting software,” “challenges,” “adoption,” “integration,” and “small business.” The selection criteria focused on peer-reviewed articles and subsequently, a comprehensive evaluation of abstracts and full-texts led to the identification of about 20 pertinent studies for in-depth analysis. The research looked upon coordinate with the central and fundamental themes and dimensions of the topic, and mainly consists of secondary data including peer reviewed journals on the importance of accounting software and the benefits for SMEs in comparison with traditional or manual accounting. Moreover, the main obstacles faced by SMEs in the adoption of accounting software is reviewed as well. The review of the selected literature has revealed that although there are numerous factors or challenges that influence the proper use and integration of accounting software, the studies all found certain key variables that stand out in relation to others. Furthermore, the most significant challenges faced seem to be affected by other variables such as the level of development of the economy or country or special circumstances such as the Covid-19 Pandemic. This means that this research which is carried out in less developed economy, and in a post-pandemic environment is necessary to reflect the actual circumstances of Omani SMEs implementing accounting software."
pub.1143840675,"Azure Arc-Enabled Kubernetes and Servers, Extending Hyperscale Cloud Management to Your Datacenter","Welcome to this introductory guide to using Microsoft’s Azure Arc service, a new multi-cloud management platform that belongs in every cloud or DevOps estate. As many IT pros know, servers and Azure Kubernetes Service drive a huge amount of consumption in Azure—so why not extend familiar management tools proven in Azure to on-premise and other cloud networks? This practical guide will get you up to speed quickly, with instruction that treads light on the theory and heavy on the hands-on experience to make setting up Azure Arc servers and Kubernetes across multiple clouds a lot less complex. Azure experts and MVPs Buchanan and Joyner provide just the right amount of context so you can grasp important concepts, and get right to the business of using and gaining value from Azure Arc. If your organization has resources across hybrid cloud, multi-cloud, and edge environments, then this book is for you. You will learn how to configure and use Azure Arc to uniformly manage workloads across all of these environments. What You Will Learn Introduces the basics of hybrid, multi-cloud, and edge computing and how Azure Arc fits into that IT strategy Teaches the fundamentals of Azure Resource Manager, setting the reader up with the knowledge needed on the technology that underpins Azure Arc Offers insights into Azure native management tooling for managing on-premises servers and extending to other clouds Details an end-to-end hybrid server monitoring scenario leveraging Azure Monitor and/or Azure Sentinel that is seamlessly delivered by Azure Arc Defines a blueprint to achieve regulatory compliance with industry standards using Azure Arc, delivering Azure Policy from Azure Defender for Servers Explores how Git and GitHub integrate with Azure Arc; delves into how GitOps is used with Azure Arc Empowers your DevOps teams to perform tasks that typically fall under IT operations Dives into how to best use Azure CLI with Azure Arc This book is for DevOps, system administrators, security professionals, and IT workers responsible for servers both on-premises and in the cloud. Some experience in system administration, DevOps, containers, and use of Git/GitHub is helpful. Steve Buchanan is a Director, Azure Platform Lead & Containers Services Lead on a Cloud Transformation team with a large consulting firm. He is a 10-time Microsoft MVP, Pluralsight author, and the author of six technical books. He has presented at tech events, including DevOpsDays, Midwest Management Summit (MMS), Microsoft Ignite, BITCon, Experts Live Europe, OSCON, Inside Azure management, and user groups. He stays active in the technical community and enjoys blogging about his adventures in the world of IT at www.buchatech.com. John Joyner is Senior Director, Technology at AccountabilIT, a managed services provider of 24x7 Network Operations and Security Operations Center (NOC & SOC) services. As an Azure Solutions Architect Expert, he designs and builds modern management and security solutions base"
pub.1174648566,A Development of Design Strategy of OL Network System for IOT Applications,"The advancement in low power wide area (LPWA) mechanism has revolutionized this existing landscape of Internet of Things (IoT) use cases, particularly through emergence from Long Range (LoRa) - proposed framework networks. Operating on absence of license industrial, scientific and medical (ISM) band, proposed mechanism server provide longer range connection for users operating from power saving mode and enhance this protocol established from the proposed mechanism network. This article provides an overview of proposed mechanism server architecture, focusing on proposed mechanism nodal point, gateways, network servers, and application servers. The challenges of global deployment and integration with current cloud loT platforms are addressed by the proposed solution, which emphasizes flexibility, scalability, and performance evaluation. Proposed mechanism pathway hardware design and implementation and proposed mechanism server architecture optimization are detailed. By analyzing test results that evaluate coverage performance and network server capacity, this can understand the deployment and performance of proposed mechanism server in urban environments."
pub.1122669091,Toward scalable cloud data center simulation using high‐level architecture,"Summary Existing simulators are designed to simulate a few thousand nodes due to the tight integration of modules. Thus, with limited simulator scalability, researchers/developers are unable to simulate protocols and algorithms in detail, although cloud simulators provide geographically distributed data centers environment but lack the support for execution on distributed systems. In this paper, we propose a distributed simulation framework referred to as CloudSimScale. The framework is designed on top of highly adapted CloudSim with communication among different modules managed using IEEE Std 1516 (high‐level architecture). The underlying modules can now run on the same or different physical systems and still manage to discover and communicate with one another. Thus, the proposed framework provides scalability across distributed systems and interoperability across modules and simulators."
pub.1175455600,Delving into the Potential of Deep Learning Algorithms for Point Cloud Segmentation at Organ Level in Plant Phenotyping,"Three-dimensional point clouds, as an advanced imaging technique, enable researchers to capture plant traits more precisely and comprehensively. The task of plant segmentation is crucial in plant phenotyping, yet current methods face limitations in computational cost, accuracy, and high-throughput capabilities. Consequently, many researchers have adopted 3D point cloud technology for organ-level segmentation, extending beyond manual and 2D visual measurement methods. However, analyzing plant phenotypic traits using 3D point cloud technology is influenced by various factors such as data acquisition environment, sensors, research subjects, and model selection. Although the existing literature has summarized the application of this technology in plant phenotyping, there has been a lack of in-depth comparison and analysis at the algorithm model level. This paper evaluates the segmentation performance of various deep learning models on point clouds collected or generated under different scenarios. These methods include outdoor real planting scenarios and indoor controlled environments, employing both active and passive acquisition methods. Nine classical point cloud segmentation models were comprehensively evaluated: PointNet, PointNet++, PointMLP, DGCNN, PointCNN, PAConv, CurveNet, Point Transformer (PT), and Stratified Transformer (ST). The results indicate that ST achieved optimal performance across almost all environments and sensors, albeit at a significant computational cost. The transformer architecture for points has demonstrated considerable advantages over traditional feature extractors by accommodating features over longer ranges. Additionally, PAConv constructs weight matrices in a data-driven manner, enabling better adaptation to various scales of plant organs. Finally, a thorough analysis and discussion of the models were conducted from multiple perspectives, including model construction, data collection environments, and platforms."
pub.1094302333,Cost Optimization in Multi-Site Multi-Cloud Environments with Multiple Pricing Schemes,"The rapid adoption of cloud computing has led to the proliferation of cloud providers, along with increasing options offered by them. Currently cloud providers offer a wide range of services (machine sizes, availability modes, storage etc.) with complex pricing schemes (spot pricing, reservation pricing, etc.). At the same time, customers require distributed deployments in order to meet their own SLA commitments. This has led to the concept of multi-site multi-cloud deployment schemes that allows enterprises to deploy highly performant distributed applications such as gaming using multiple cloud providers. Existing research has solved the problem of cost optimization of such deployments BUT under the assumption of a single pricing scheme for all providers. The introduction of varied and dynamic pricing introduces several complexities that needs a fresh look at this problem. In this paper we present a solution for mapping the compute and storage requirements of a set of related enterprise sites onto cloud providers who offer multiple, dynamic pricing schemes. Our goal is to optimize the cost of a multi-site deployment consisting of compute and storage components at each site while meeting SLA requirements of the application. We show that our approach can achieve cost reductions of up to 22% for the customer even for medium sized deployments consisting of tens of sites over the scheme that uses either a single cloud provider or fixed pricing schemes."
pub.1091667546,Adaptive Resource Allocation and Provisioning in Multi-Service Cloud Environments,"In the current cloud business environment, the cloud provider (CP) can provide a means for offering the required quality of service (QoS) for multiple classes of clients. We consider the cloud market where various resources such as CPUs, memory, and storage in the form of Virtual Machine (VM) instances can be provisioned and then leased to clients with QoS guarantees. Unlike existing works, we propose a novel Service Level Agreement (SLA) framework for cloud computing, in which a price control parameter is used to meet QoS demands for all classes in the market. The framework uses reinforcement learning (RL) to derive a VM hiring policy that can adapt to changes in the system to guarantee the QoS for all client classes. These changes include: service cost, system capacity, and the demand for service. In exhibiting solutions, when the CP leases more VMs to a class of clients, the QoS is degraded for other classes due to an inadequate number of VMs. However, our approach integrates computing resources adaptation with service admission control based on the RL model. To the best of our knowledge, this study is the first attempt that facilitates this integration to enhance the CP's profit and avoid SLA violation. Numerical analysis stresses the ability of our approach to avoid SLA violation while maximizing the CP's profit under varying cloud environment conditions."
pub.1118595914,MeDICINE: Rapid Prototyping of Production-Ready Network Services in Multi-PoP Environments,"Virtualized network services consisting of multiple individual network
functions are already today deployed across multiple sites, so called multi-PoP
(points of presence) environ- ments. This allows to improve service performance
by optimizing its placement in the network. But prototyping and testing of
these complex distributed software systems becomes extremely challenging. The
reason is that not only the network service as such has to be tested but also
its integration with management and orchestration systems. Existing solutions,
like simulators, basic network emulators, or local cloud testbeds, do not
support all aspects of these tasks. To this end, we introduce MeDICINE, a novel
NFV prototyping platform that is able to execute production-ready network func-
tions, provided as software containers, in an emulated multi-PoP environment.
These network functions can be controlled by any third-party management and
orchestration system that connects to our platform through standard interfaces.
Based on this, a developer can use our platform to prototype and test complex
network services in a realistic environment running on his laptop."
pub.1105538565,An Approach for Minimizing Energy Consumption in Cloud Environment,"Background/Objectives: Cloud computing could be considered of vital paradigms in IT which allows services to be delivered to the users via the internet on demand and on pay as you go basis. The growing demand on cloud computing environments increasing the number of datacenters which in turn increase the amount of power consumption in datacenters along with cooling equipment. Load balancing is considered a major challenge affecting in cloud performance Methods: An existing problem is how to allocate Virtual Machines (VMs) to Physical Machines (PMs) or hosts. This process is called VM placement. An algorithm is proposed that can reduce power consumption. Findings: The proposed algorithm assigns VMs onto PMs based on first fit decreasing algorithm and improves an existed one through reducing power consumption by turning-off some under load hosts if available and migrating their VMs to other active hosts. Application: The presented approach could decrease significantly energy consumption in comparison with the existing one through migrating VMs from underload hosts and turns them off. Keywords: Cloud Computing, Energy Consumption, Load Balancing, Migration, VM Placement"
pub.1169648491,Notebook-as-a-VRE (NaaVRE): From private notebooks to a collaborative cloud virtual research environment,"Studying many scientific problems, such as environmental challenges or cancer diagnosis, requires extensive data, advanced models, and distributed computing resources. Researchers often reuse assets (e.g. data, AI models, workflows, and services) from different parties to tackle these issues. This requires effective collaborative environments that enable advanced data science research: discovery access, interoperation and reuse of research assets, and integration of all resources into cohesive observational, experimental, and simulation investigations with replicable workflows. Such use cases can be effectively supported by Virtual Research Environments (VREs). Existing VRE solutions are often built with preconfigured data sources, software tools, and functional components for managing research activities. While such integrated solutions can effectively serve a specific scientific community, they often lack flexibility and require significant time investment to use external assets, build new tools, or integrate with other services. In contrast, many researchers and data scientists are familiar with notebook environments such as Jupyter.  We propose a VRE solution for Jupyter to bridge this gap: Notebook-as-a-VRE (NaaVRE). At its core, NaaVRE allows users to build functional blocks by containerizing cells of notebooks, composing them into workflows, and managing the lifecycle of experiments and resulting data. The functional blocks, workflows, and resulting datasets can be shared to a common marketplace, enabling the creation of communities of users and customized VREs. Furthermore, NaaVRE integrates with external sources, allowing users to search, select, and reuse assets such as data, software, and algorithms. Finally, NaaVRE natively works with modern cloud technologies, making it possible to use compute resources flexibly and cost-effectively. We demonstrate the versatility of NaaVRE by building several customized VREs that support legacy scientific workflows from different communities. This includes the derivation of ecosystem structure from Light Detection and Ranging (LiDAR) data, the tracking of bird migrations from radar observations, and the characterization of phytoplankton species. The NaaVRE is also being used to build Digital Twins of ecosystems in the Dutch NWO LTER-LIFE project."
pub.1171682353,Privacy and Security using Double Signature Based Cryptocraphy using DS-SHA256 in Cloud Cimputing,"Cloud computing is a popular technology that offers clients various services remotely daily. Nowadays, cloud computing is becoming a fast-growing industry that contains the potential to provide extensible services via the internet using software and hardware virtualization. It has become difficult for cloud companies to manage the enormous volume of data and diverse resources in the cloud due to its rapid growth. Most clients range from individual users to large corporations or business houses. To prevent over-provisioning and under-provisioning problems, it is essential to ensure the availability of the resources to the end user with the least amount of administration. As a result, resources must be equitably distributed among various stakeholders without sacrificing the organization’s profitability or customer satisfaction. The enormous load from dynamic sources with various formats makes resource allocation and scheduling a complex process. It also leads to several security issues, which include privacy, interoperability, credibility, following critical standards, and collaborations become a significant concern in cloud computing. We proposed cloud service provider dynamic scheduling (CSPDS) to address these issues. The proposed system is efficiently designed to handle various formats and huge loads in the cloud environment. The cloud service provider (CSP) plays a primary role in our proposed methodology (CSPDS), which controls and monitors the entire network. The CSPDS enhances the network’s QoS (quality of service) in resource monitoring, resource utilization, and resource allocation. Next, the migration model provides the best resource utilization, and the cryptography modules improve the users’ overall data security in the cloud. Finally, in the experimental section, a comparison work is conducted between the proposed CSPDS with existing resource allocation& task scheduling using a hybrid machine learning (RATS-HM) [13] technique and Resource Allocation Algorithm (RAA)[14]. In the comparison work, these algorithms are analyzed with evaluation metrics such as utilization, system efficiency, migration time, and security performance. The obtained results state in all the metrics that the proposed CSPDS performance is more efficient than the others."
pub.1093753813,Spinner: Scalable Graph Partitioning in the Cloud,"In this paper, we present a graph partitioning algorithm to partition graphs with trillions of edges. To achieve such scale, our solution leverages the vertex-centric Pregel abstraction provided by Giraph, a system for large-scale graph analytics. We designed our algorithm to compute partitions with high locality and fair balance, and focused on the characteristics necessary to reach wide adoption by practitioners in production. Our solution can (i) scale to massive graphs and thousands of compute cores, (ii) efficiently adapt partitions to changes to graphs and compute environments, and (iii) seamlessly integrate in existing systems without additional infrastructure. We evaluate our solution on the Facebook and Instagram graphs, as well as on other large-scale, real-world graphs. We show that it is scalable and computes partitionings with quality comparable, and sometimes outperforming, existing solutions. By integrating the computed partitionings in Giraph, we speedup various real-world applications by up to a factor of 5.6 compared to default hash-partitioning."
pub.1172583512,Cloud Scaling Policies Verifier System driven by STORM Model Checker,"<p>Interpreting the stochastic behavior of cloud scaling policies formalized and verified with a model checker, using an independent visualization tool can be time-consuming as several manual steps need to be taken to visualize the behavior. This is because the existing model checker presents verification results through text-based descriptions and line charts depicting probabilities or rewards against investigated variables. However, with the line charts, the user can only analyze the probabilities or rewards instead of the behavior. To address this, we propose a cloud scaling policies verifier system that utilizes the extensible STORM model checker to automatically visualize the behavior of the verified model and the verification results. Additionally, the system architecture integrates the desktop local environment with the Docker container environment using open-source technologies including <em>StormPy</em>, <em>PyQt</em>, <em>Networkx</em>, and <em>Matplotlib</em> to enable replication and innovation for future research. Moreover, the system has undergone evaluation through functional and integration testing, demonstrating its effectiveness. Finally, it has been demonstrated that the system can be utilized not only for checking and verifying various cloud scaling policies but also for other decision-making policies beyond cloud scaling.</p>"
pub.1139578978,A comprehensive study of the role of cloud computing on the information technology infrastructure library (ITIL) processes,"
                    Purpose
                    Information technology infrastructure library (ITIL) is a commonly utilized IT service management execution technique that helps IT services to be planned, designed, selected, operated and continuously improved. ITIL procedures are utilized to measure the efficiency of IT service management procedures and their association with the accelerated system development of cloud systems. The challenges faced in IT deployment and maintenance management significantly restrict cloud computing services' reliability. Therefore, this article aims to review a comprehensive study of the role of cloud computing on the ITIL processes.
                  
                  
                    Design/methodology/approach
                    Each enterprise strives to stay competitive in the market and offers the services its consumers are looking for, all in line with cost-effectiveness and client needs. The ITIL framework provides best practice guidance for IT service management that includes a collection of ample publications supplying detailed guidelines on the management of IT functions, processes, responsibilities and roles associated with IT service management. On the other hand, the way companies employ IT services with an effect on the role of enterprise infrastructure is altered by cloud computing. Hence, the investigation makes utilization of a systematic literature review (SLR) detailing crucial success factors of cloud computing execution in ITIL. The authors have recognized 35 valuable contributions, providing a comprehensive view of study in this field, of which 22 papers were found according to some filters that have been analyzed in this article. Selected articles are presented in two groups, including cloud service and cloud service providers.
                  
                  
                    Findings
                    Owing to the overall expense of execution and problems with combining the ITIL approach with the existing organizational IT strategic strategy, ITIL adoption has begun to wane over the last few years. An established methodology for ITIL deployment that will assure long-term success for those wanting to use private cloud procurement will be the most important inference that can be taken from this article. ITIL offers a perfect platform to execute and support cloud applications effectively. IT will prevent cloud sprawl and instability, reduce the likelihood of service interruption and optimize customer loyalty by merging humans, procedures and technologies into hybrid environments.
                  
                  
                    Research limitations/implications
                    This survey is more aimed at specialists such as IT experts; so, further evaluations must also be carried out in order to understand the company's views on the risks and advantages of adopting ITIL. In addition, non-English articles are not discussed in this article.
                  
     "
pub.1181109085,Efficient virtual machine placement in cloud computing environment using BSO-ANN based hybrid technique,"Cloud computing has revolutionized the way businesses and individuals access and utilize computing resources. Efficient virtual machine placement is a critical aspect of optimizing resource utilization, reducing operational costs, energy consumption, service level agreement and minimum virtual machine migrations, execution time, and ensuring the overall performance of cloud services. This manuscript introduces a novel approach that combines the creative problem-solving capabilities of brainstorming with the computational power of Artificial Neural Networks (ANN) to address the virtual machine placement problem in cloud environments. In this study, we propose a hybrid technique that leverages the collective intelligence of human brainstorming to generate a diverse set of placement strategies. These strategies are then evaluated, refined, and optimized using an ANN model trained on historical cloud resource allocation workload logs. By integrating the human creative process with the data-driven predictive capabilities of ANN, our approach aims to overcome the limitations of traditional virtual machine placement algorithms, which often struggle to adapt to dynamic workloads and changing resource requirements. The manuscript provides a detailed description of the hybrid technique, including the process of brainstorming for generating placement strategies, data collection and preprocessing, ANN model development, and the integration of these components into an efficient placement system. We present experimental results demonstrating the effectiveness of our approach in optimizing resource allocation, improving service performance, and optimizing resource utilization, reducing energy consumption, service level agreement and minimum virtual machine migrations, reducing execution time compared to existing static, and meta-heuristic methods. The proposed Brain Storming with ANN based Hybrid Technique offers a promising solution for enhancing the efficiency of virtual machine placement in cloud computing environments. BSO-ANN outperforms the existing techniques using performance metrics (energy consumption(Kwh), execution time(ms), SLA violations, and number of migrations). It combines human ingenuity with data-driven insights to adapt to the ever-changing dynamics of cloud workloads. This manuscript contributes to the ongoing research in cloud resource management, offering a practical approach for cloud service providers and organizations to better utilize their resources and enhance the overall quality of cloud-based services. © 2012 Published by Elsevier Ltd. Selection and/or peer-review under responsibility of Global Science and Technology Forum Pte Ltd."
pub.1123578822,Mobile fog based secure cloud-IoT framework for enterprise multimedia security,"With the dawn of computing era in the enterprise business, the methodologies for business have changed significantly. The enterprise information in the form of multimedia data is now being created and extensively shared over the internet. On the other side, the rapid and continuous growth of enterprises is causing the computing systems to evolve rapidly and continuously to match the pace of growth. This has led to the evolution of new technologies related to the enterprise environment. The advent of Internet of Things (IoT) and Cloud Computing in the enterprise environment is the result of its rapid and continuous requirement. But, their use has increased the complexity of enterprise computing systems especially in the domain of Enterprises Multimedia Security (EMS). In Cloud-IoT environment, the Enterprise Multimedia Data (EMD) stored at remote Cloud premises and subject to various kinds of vulnerabilities like data leakage, data modification and confidentiality breach. Hence, this paper proposes a distinctive EMS approach using Fog and Cloud to store different types of data and maintains their co-relation to prevent various EMD security breaches. In this paper, a Mobile Fog based Cloud-IoT framework for EMS has been designed in which only encrypted data is stored at the Cloud and its keys are securely stored at the enterprise premises on the Fog layer. It uses Advanced Encryption Standard (AES) for encryption, Message Authentication Code (MAC) for data integrity, Secure Socket Layer (SSL) for safe data transmission, tokenizer, hashing and data pointers for easy data search on Fog and Cloud. It stores the encryption/decryption and MAC keys encrypted with Master Encryption Key (MEK) securely at the enterprise premises on the Fog layer. For assured security of keys, the framework doesn’t store MEK at any place on the network. The experimental result analyses acknowledge the highest level of security, better scalability and storage management as well as searching time efficiency in the proposed framework. The overall results analysis show that the proposed framework is highly secure to protect the multimedia data of an enterprise from the inside and outside threats and access of data is provided to the authorized enterprise users."
pub.1121430885,Architecting IoT based Healthcare Systems Using Machine Learning Algorithms,"The rapid innovations in technologies endorsed the emergence of sensory equipment's connection to the Internet for acquiring data from the environment. The increased number of devices generates the enormous amount of sensor data from diversified applications of Internet of things (IoT). The generation of data may be a fast or real-time data stream which depends on the nature of applications. Applying analytics and intelligent processing over the data streams discovers the useful information and predicts the insights. Decision-making is a prominent process which makes the IoT paradigm qualified. This chapter provides an overview of architecting IoT-based healthcare systems with different machine learning algorithms. This chapter elaborates the smart data characteristics and design considerations for efficient adoption of machine learning algorithms into IoT applications. In addition, various existing and hybrid classification algorithms are applied to sensory data for identifying falls from other daily activities."
pub.1173382122,A Novel Framework for Cloud Data Security with Blockchain Technology and Distributed Virtual Machine Agents,"Even though cloud computing has advanced over the years, the two major challenges currently faced by cloud computing are data security and trusted computing. In cloud computing applications, blockchain technology can enhance data security and trusted computing when paired with virtual machine agents and mobile agent technologies. A decentralized new framework for distributed computing is blockchain. A potential area of study is the integration of blockchain technology with cloud computing, leveraging the former's security mechanisms to enhance the latter's secure computing and storage capabilities. The usage of the proposed framework builds on the inherent security features of blockchain technology in an effort to secure data and information integrity, confidentiality and authentication in cloud environments. Through the introduction of distributed virtual machine agents, the framework increases the scalability and effectiveness of data protection mechanisms for virtual data environments that can be used to store information protected by cloud security measures. Thus, the proposed study substantiates the applicability and efficiency of the existing and emerging problems of cloud data security and this approach develops satisfactory data protection solutions."
pub.1175164155,Transfer Learning Based Multi-Objective Evolutionary Algorithm for Dynamic Workflow Scheduling in the Cloud,"Managing scientific applications in the Cloud poses many challenges in terms of workflow scheduling, especially in handling multi-objective workflow scheduling under quality of service (QoS) constraints. However, most studies address the workflow scheduling problem on the premise of the unchanged environment, without considering the high dynamics of the Cloud. In this paper, we model the constrained workflow scheduling in a dynamic Cloud environment as a dynamic multi-objective optimization problem with preferences, and propose a transfer learning based multi-objective evolutionary algorithm (TL-MOEA) to tackle the workflow scheduling problem of dynamic nature. Specifically, an elite-led transfer learning strategy is proposed to explore effective parameter adaptation for the MOEA by transferring helpful knowledge from elite solutions in the past environment to accelerate the optimization process. In addition, a multi-space diversity learning strategy is developed to maintain the diversity of the population. To satisfy various QoS constraints of workflow scheduling, a preference-based selection strategy is further designed to enable promising solutions for each iteration. Extensive experiments on five well-known scientific workflows demonstrate that TL-MOEA can achieve highly competitive performance compared to several state-of-art algorithms, and can obtain triple win solutions with optimization objectives of minimizing makespan, cost and energy consumption for dynamic workflow scheduling with user-defined constraints."
pub.1174751632,A Novel Framework for Cloud Data Security with Blockchain Technology and Distributed Virtual Machine Agents,"Even though cloud computing has advanced over the years, the two major challenges currently faced by cloud computing are data security and trusted computing. In cloud computing applications, blockchain technology can enhance data security and trusted computing when paired with virtual machine agents and mobile agent technologies. A decentralized new framework for distributed computing is blockchain. A potential area of study is the integration of blockchain technology with cloud computing, leveraging the former's security mechanisms to enhance the latter's secure computing and storage capabilities. The usage of the proposed framework builds on the inherent security features of blockchain technology in an effort to secure data and information integrity, confidentiality and authentication in cloud environments. Through the introduction of distributed virtual machine agents, the framework increases the scalability and effectiveness of data protection mechanisms for virtual data environments that can be used to store information protected by cloud security measures. Thus, the proposed study substantiates the applicability and efficiency of the existing and emerging problems of cloud data security and this approach develops satisfactory data protection solutions."
pub.1154205659,Where do countries stand in cloud computing readiness? A country-level analysis of capacity and potential,"Cloud computing has become relevant due to the increasing need for computing, storage, and communication over the internet. This growth, however, is not same across the geographies. While developed economies have adopted cloud-based services, emerging economies are still lagging. As digitization has become core to policy-level strategies, a slow adoption of cloud-based Information and Communications Technology (ICT) may impact growth. The adoption and growth of cloud-based services depends on the readiness of the broader ecosystem. This study evaluates the factors influencing the cloud computing readiness of selected countries. In the process, the study extends the existing understanding of the ranking system by providing additional insights into the countries’ performances against their potentials by using technical efficiency analysis. The study identifies and categorizes countries into four groups: aspirants, initiators, performers, and achievers. Results suggest that political-regulatory-business environment, investing in research and development, tertiary education, and knowledge-worker significantly impact cloud computing readiness."
pub.1171279343,Study and Investigation of Cloud Based Security Policies Using Machine Learning Techniques,"In recent days, most organizations have started to use cloud environments and various cloud services. To secure and secure the operations that an organization performs in the cloud, it is important to provide a secure and stable environment solution in cloud wind. Existing methods such as linear regression and support vector machines attempt to improve cybersecurity in the marketplace by proving the static behavior of the cloud to predict threats. There are security concerns with CC and edge computing, including client-side vulnerabilities and authentication organizations slowing the adoption of fast computing models. In this review article, researchers examine CC security threats, issues, and solutions implemented for one or more machine learning algorithms. The main purpose of this report is to develop next generation cloud security using machine learning algorithm like KNN which can provide an automatic and precise way to improve the cloud security environment. Machine learning not only focuses on finding and recognizing patterns in sensitive data, but can also provide solutions with holistic algorithms to protect business data in any cloud application. Researchers analyzed various machine learning algorithms to tackle air safety issues, including supervised, unsupervised, and semi-supervised, and then compared the performance advantages of each system against its features, strengths and weaknesses. However, in the future, techniques such as data mining and data clustering are used to improve classification, while more advanced techniques can be used to analyze traditional data. After reviewing a few algorithms, it's best to use encryption and cryptography techniques along with recommended algorithms to improve the performance and security of your data."
pub.1172244758,A Deep Analysis of Performance Metrics and Comparative Assessment of Network Telemetry Tools in Linux Environments,"As cyber-attacks targeting public cloud infrastructure increase in severity, it is essential to have strong network security measures for Linux machines. [1] Recent statistics underscore the severity of the situation, with a significant 39% of businesses experiencing security breaches within their cloud environments in 2022. This data shows a notable 35% increase in security attacks from the previous year. These breaches affected around 400 million individuals, emphasizing the urgent need for action.
 As organizations increasingly migrate their operations to the cloud, addressing security risks is paramount. This needs a comprehensive approach to cloud security, focusing on monitoring and surveillance of cloud infrastructure usage by customers. Effective security observability requires deploying monitoring and alerting systems capable of promptly detecting and mitigating potential threats in real-time. [2] The Linux community has embraced Berkeley Packet Filter (BPF) technology as a cornerstone in this effort. BPF's flexibility and extensibility have led to the development of sophisticated tools, offering unparalleled capabilities in enhancing security observability and response mechanisms. This study begins by examining legacy solutions like auditd, which help auditing of all aspects of Linux machines. It also explores the origins and evolution of BPF within the Linux ecosystem, highlighting its transformative impact.
 The study further delves into BPF-based monitoring tools tailored for scrutinizing Linux system processes. It elucidates their functionalities and meticulously assesses the performance of select tools and technologies. Rigorous experimental method, involving virtual machines with identical specifications subjected to network load simulations, ensures reliable and unbiased performance evaluations. Through this experimentation, valuable insights into resource consumption patterns for each tool are gleaned, aiding informed decision-making in tool selection and deployment strategies."
pub.1095189978,Policing As A Service in the Cloud,"Security and Privacy are fundamental concerns in cloud computing both in terms of legal complications and user trust. Cloud computing is a new computing paradigm, aiming to provide reliable, customized, and guaranteed computing dynamic environment for end-users. However, the existing security and privacy issues in the cloud still present a strong barrier for users to adopt cloud computing solutions. This paper investigates the security and privacy challenges in cloud computing in order to explore methods that improve the users' trust in the adaptation of the cloud. Policing As A Service can be offered by the cloud providers with the intention of empowering the users to monitor and guard their assets in the cloud. This service is beneficial both to the cloud providers and the users. However, at first, the cloud providers may only be able to offer basic auditing services due to undeveloped tools and applications. Similar to other services delivered in the cloud, this service can be purchased by the users to gain some control over their data. The subservices of the proposed service can be Privacy As A Service and Forensics As A Service. These services give the cloud users a sense of transparency and having control over their data in the cloud while better security and privacy safeguards are sought."
pub.1158159174,Towards end-to-end deep RNN based networks to precisely regress of the lettuce plant height by single perspective sparse 3D point cloud,"Nowadays, 3D point cloud is supposed to be the most direct and effective data form for studying plant morphology structure. However, automatic and high-throughput acquisition of accurate individual plant height traits from 3D point cloud remains an urgent challenging problem. Summarizing the related research results in recent years, the factors limiting its application mainly come from these aspects: (1) Many existing methods require spatial auxiliary information such as ground control points (GCP), digital terrain models (DTM) and digital surface models (DSM) to obtain accurate plant height; (2) For 3D point cloud data in different environments, specialized modeling and careful parameter fine-tuning are usually required; (3) Sometimes, the point cloud processing involves the combined utilization of multiple programming languages and software, which is difficult for system integration. Focusing on these challenges, firstly, we proposed a novel end-to-end deep Recurrent Neural Network (RNN) based regression network framework called DRN, which consists of three parts: point cloud feature extraction network, deep RNN and regression network. The convolution operations-based point cloud feature extraction network is function as filtering noise, outliers and redundant information; The deep RNN network with long and short-term memory (LSTM) ability is used to learn the relationships between the feature points on the high-dimensional feature sequence separated by a certain distance; regression network is used to regress the output from deep RNN to plant height value. Experiments results on the 3rd Greenhouse Growing Challenge datasets show that DRN can directly regress the plant height of a single plant effectively without manual operations and the participation of spatial auxiliary information with an R2 of 0.948 and a relative root mean square error (RRMSE) of 10.06% in four different varieties of lettuce at different growth period. After studying the influence of the weights of the x, y, z coordinate of the input 3D point cloud on the regression result, then, we design a Dimension Attention (DA) module at the front end of the feature extraction network to learning the characteristic coordinate weight for every input point cloud sample. The DRN network with a DA module is called D-DRN, experiment results indicate D-DRN tend to achieve better result (R2 = 0.960; RRMSE = 8.680%) than DRN. Considering the end-to-end-based DRN and D-DRN network capable of ease of integration and their considerable prediction accuracy on public datasets, we believe they has a certain complementary effect on the existing study methods of obtaining plant morphological structure phenotype by point cloud data."
pub.1146612647,C-SCALE: A new Data and Compute Federation for Earth Observation,"<p>Through the provision of massive streams of high-resolution Earth Observation (EO) data, the EU Copernicus programme has established itself globally as the predominant spatial data provider. These data are widely used by research communities to monitor and address global challenges, such as environmental monitoring and climate change, supporting European policy initiatives, such as the Green Deal and others. To date, there is no single European data sharing and processing infrastructure that serves all datasets of interest, and Europe is falling behind international developments in Big Data analytics and computing.</p><p>The C-SCALE (Copernicus - eoSC AnaLytics Engine, https://c-scale.eu) project federates European EO infrastructure services, such as ESA’s Sentinel Collaborative Ground Segment, the Copernicus DIASes (Data and Information Access Services under the EC), independent nationally-funded EO service providers, and European Open Science Cloud (EOSC) e-infrastructure providers. It capitalises on EOSC's capacity and capabilities to support Copernicus research and operations with large and easily accessible European computing environments. The project will implement and publish the C-SCALE Federation in the EOSC Portal as a suite of complementary services that can be easily exploited. It will consist of a Data Federation, a service providing access to a large EO data archive, a Compute Federation, and analytics tools.</p><p>The C-SCALE Data Federation aims at making EO data providers under EOSC findable, their metadata databases searchable, and their product storage accessible. While a centralised, monolithic, complete Copernicus data archive may not be feasible, some organisations maintain various archives for limited areas of their interest. C-SCALE, therefore, integrates these heterogeneous resources into a “system of systems” that will offer the users an interface that, in most cases, provides similar functionality and quality of service as a centralised, monolithic data archive would. The federation is built on existing technologies, avoiding redundancy and replication of functions and not disrupting existing usage patterns at participating sites, instead only adding a simple layer for improved discovery and seamless access.</p><p>At the same time, the C-SCALE Compute Federation provides access to a wide range of computing providers (IaaS VMs, container orchestration platforms, HPC and HTC systems) to enable the analysis of Copernicus and EO data under EOSC. The design of the federation allows users to deploy their applications using federated authentication mechanisms, find their software under a common catalogue, and have access to data using C-SCALE Data Federation tools. The federation relies on existing tools and services already compliant with EOSC, thus facilitating the integration into the larger EOSC ecosystem.</p><p>By making such a scalable Big Copernicus Data Analytics federated services available through EOSC and its Por"
pub.1137040640,Advance Cloud Data Analytics for 5G Enabled IoT,"The mobile cellular networks such as 5G are evolving from the existing 4G networks and will continue to provide better services to the end-users. Over time, the number of Internet of Things (IoT) devices will be linked with the 5G networks to provide support for low latency and ultra-reliable communication. The main drawback is the decision process management and handling vast amounts of data associated with the IoT and 5G based systems. Hence the adoption of Cloud Computing (CC), 5G, and IoT is an important keyword for its implementation. The market interest in the IoT is increased because of the improvements in the 5G and CC technologies. The 5G can cater to present requirements like smart energy applications, etc. and many to come in the future timeline. The 5G users can be categorized into latency calculation, enhanced mobile broadband (eMBB), and critical communications for massive IoT clusters. CC will help to handle the information volumes generated by IoT, as 5G boosts the network capacity. These combinations of technologies such as CC, IoT, and 5G will be a boon to industries such as automotive and mobility, media and content, Public/smart city, healthcare, manufacturing, energy, and utility. The contribution of this chapter is the benefits the 5G enabled IoT system will get by integrating with the Cloud ecosystem. The outcome of the case study reflects the benefits of using the Cloud ecosystem for the 5G based IoT infrastructure for an effective decision-making process via intelligent communication mechanism and handling security in the IoT framework. abstract environment."
pub.1181401701,Self-Manageable System Architecture Design for Distributed Intelligent Automation,"A major challenge of extant industrial systems designed under traditional engineering techniques and running on legacy automation platforms is that these systems are unable to automatically discover alternative solutions, flexibly coordinate reconfigurable modules, and actively deploy corresponding functions, to quickly respond to frequently changes and intelligently adapt to evolving requirements in dynamic environments. This paper extends our research on the design of industrial cyber-physical systems to introduce a multi-layer self-manageable system architecture for IEC 61499 based distributed intelligent automation. The proposed architecture is designed to enable system-level run-time intelligence in the cloud and device-level real-time adaptation at the edge by integrating multi-agent modelling and IEC 61499 function block modelling. The proposed architecture is demonstrated and evaluated through various simulation tests on the agent-based simulation model of an automated conveyor system. The results show the ability of the proposed architecture to adapt the system autonomously to respond to frequent changes and evolving requirements typical of modern industrial environments in Industry 4.0."
pub.1000030638,Towards Lightweight and Swift Storage Resource Management in Big Data Cloud Era,"Workload IO behavior in modern data centers is fluctuating and unpredictable due to the rapidly adopted, public cloud environment. Nevertheless, existing storage resource management systems, such as VMware SDRS, are incapable of performing real time policy-based storage management due to the high cost of migrating large size virtual disks. Hence, the traditional storage management schemes become ineffective due to the lack of quick response to the frequent IO bursts and the inaccurate storage latency prediction in the light of a highly fluctuating environment. To address the aforementioned issues, we propose LightSRM, which can work properly in a time-variant cloud environment. To mitigate the storage migration cost, we leverage copy-on-write/read snapshots to redirect the IO requests without moving the virtual disk. To support snapshots in storage management, we also build a performance model specifically for snapshots. We employ exponentially weighted moving average with adjustable sliding window to provide quick and accurate performance prediction. Furthermore, we propose a hybrid management scheme, which can dynamically choose either snapshot or migration for fastest performance tuning. We build our prototype in a QEMU/KVM based virtualized environment. Our empirical evaluation results show that snapshot can redirect IO requests in a faster manner than migration can do when the virtual disk size is large. Besides, snapshot method has less disk performance impact on the applications. By employing hybrid snapshot/migration method, LightSRM yields less overall latency, better load balance, and less IO traffic overhead."
pub.1031843444,Energy-Aware Virtual Machine Consolidation in IaaS Cloud Computing,"With immense success and rapid growth within the past few years, cloud computing has been established as the dominant paradigm of IT industry. To meet the increasing demand of computing and storage resources, infrastructure cloud providers are deploying planet-scale data centers across the world, consisting of hundreds of thousands, even millions of servers. These data centers incur very high investment and operating costs for the compute and network devices as well as for the energy consumption. Moreover, because of the huge energy usage, such data centers leave large carbon footprints and thus have adverse effects on the environment. As a result, efficient computing resource utilization and energy consumption reduction are becoming crucial issues to make cloud computing successful. Intelligent workload placement and relocation is one of the primary means to address these issues. This chapter presents an overview of the infrastructure resource management systems and technologies and detailed description of the proposed solution approaches for efficient cloud resource utilization and minimization of power consumption and resource wastages. Different types of server consolidation mechanisms are presented along with the solution approaches proposed by the researchers of both academia and industry. Various aspects of workload reconfiguration mechanisms and existing works on workload relocation techniques are described."
pub.1120773119,Improving Industrial Computing Capacity with Fog Computing and Smart Systems,"Fog computing, edge devices and cloud computing together can provide an impressive amount of resources for ongoing industrial projects by improving the utilization of existing edge resources. This paper will examine fog computing architecture, it's characteristics and its interoperability to edge computing and cloud computing. The aim is to investigate the essential knowledge necessary to support adoption of fog computing in challenging conditions which arise in smart factories and smart agriculture. The present work was motivated by recent developments in Artificial Intelligence indicating that there is great potential for the combination of fog computing and AI to maximize the potential of intelligent systems. This paper aims to present earlier findings which will support an undergoing research project concerning a novel generation of Artificial Intelligent Agents created to self-discover and orchestrate existing resources, actually present in industrial environments."
pub.1093286912,Information-Flow Control for Building Security and Privacy Preserving Hybrid Clouds,"A hybrid cloud is a cloud computing environment in which an organization provides and manages some internal resources (private cloud) while the other resources are provisioned externally (public cloud). Rapid deployment of hybrid clouds for utility, cost, effectiveness and flexibility has made it necessary to assure the security and privacy of hybrid clouds as it transcends different domains. Further, successful hybrid cloud implementation requires a well-structured architecture supporting the functionalities of both private and public clouds and the seamless transitions between them. One of the challenges in a hybrid cloud is securing resource access, in particular, enforcing that the owner's policy never gets violated even when the data gets consumed and processed in multiple domains. Existing mechanisms for achieving this, including industry standards such as XACML, SAML, and OAuth, are vulnerable to indirect information leaks as they do not keep track of information flow. The Readers-Writers Flow Model (RWFM) is a novel security model with an intuitive security policy that tracks and controls the flow of information in a decentralized system. In this paper, we present an approach to building a hybrid cloud that preserves the given security and privacy policy by integrating an RWFM security module into a cloud service manager. An advantage of RWFM is that it provides a uniform solution for securing various kinds of hybrid cloud architectures ranging from the simple pairwise federation to the complex interclouds, and supporting varying degrees of flexibility in workload placement ranging from a simple static placement to fully dynamic migration. Further, RWFM framework is forensic-ready by design, because the labels of data and services readily provide the necessary forensic information."
pub.1150274874,Towards End-to-End Deep RNN Based Networks to Precisely Regress of the Lettuce Plant Height by Single Perspective Sparse 3D Point Cloud,"Nowadays, 3D point cloud is supposed to be the most direct and effective data form for studying plant morphology structure. However, automatic and high-throughput acquisition of accurate individual plant height traits from 3D point cloud remains an urgent challenging problem. Summarizing the related research results in recent years, the factors limiting its application mainly come from these aspects: (1) Many existing methods require spatial auxiliary information such as ground control points (GCP), digital terrain models (DTM) and digital surface models (DSM) to obtain accurate plant height; (2) For 3D point cloud data in different environments, specialized modeling and careful parameter fine-tuning are usually required; (3) Sometimes, the point cloud processing involves the combined utilization of multiple programming languages and software, which is difficult for system integration. Focusing on these challenges, firstly, we proposed a novel end-to-end deep RNN(Recurrent Neural Network) based regression network framework called DRN, which consists of three parts: point cloud feature extraction network, deep RNN and regression network. The convolution operations-based point cloud feature extraction network is function as filtering noise, outliers and redundant information; The Deep RNN network with long and short-term memory (LSTM) ability is used to learn the relationships between the feature points on the high-dimensional feature sequence separated by a certain distance; regression network is used to regress the output from Deep RNN to plant height value. Experiments results on the 3rd Greenhouse Growing Challenge datasets show that DRN can directly regress the plant height of a single plant effectively without manual operations and the participation of spatial auxiliary information with an R 2 of 0.948 and a relative root mean square error (RRMSE) of 10.06% in four different varieties of lettuce at different growth period. After studying the influence of the weights of the x, y, z coordinate of the input 3D point cloud on the regression result, then, we design a Dimension Attention (DA) module at the front end of the feature extraction network to learning the characteristic coordinate weight for every input point cloud sample. The DRN network with a DA module is called D-DRN, experiment results indicate D-DRN tend to achieve better result(R 2 =0.960 ; RRMSE=8.680%) than DRN. Considering the end-to-end-based DRN and D-DRN network capable of ease of integration and their considerable prediction accuracy on public datasets, we believe they has a certain complementary effect on the existing study methods of obtaining plant morphological structure phenotype by point cloud data."
pub.1173642255,FEDGE: An Interference-Aware QoS Prediction Framework for Black-Box Scenario in IaaS Clouds with Domain Generalization,"Public cloud providers embrace multi-tenancy as a strategy to enhance the utilization and efficiency of resources. However, co-located virtual machines (VMs) suffer from qualityof-service (QoS) degradation caused by shared resource interference. Existing solutions for predicting QoS degradation often rely on the assumption of online access to application-level information. However, in a production environment, this assumption proves invalid as the VMs are black boxes to the providers. This intrinsic characteristic of the IaaS cloud necessitates the prediction model to generalize to unfamiliar applications and imposes specific criteria on the monitorable metrics. To meet the black-box scenario under Infrastructure as a Service (IaaS) cloud computing, we present a novel framework, FEDGE, that can predict interference-aware QoS (IA-QoS) of co-located VMs using only low-level monitorable metrics before migration. Specifically, FEDGE utilizes a stochastic gates layer to select the most informative features from the high-dimensional resource and hardware metrics, which helps to reduce the monitoring overhead. Furthermore, we design a multi-domain MMD-based adversarial denoising autoencoder to regularize the learned hidden representations and prevent over-fitting on the source domains. Next, we employ a multi-layer perceptron (MLP) to accurately predict complex QoS degradation using the learned representations with domain generalization. Experimental results demonstrate that FEDGE outperforms other state-of-the-art methods in terms of both generalizability and effectiveness."
pub.1091357998,Management of Service Level Agreements for Cloud Services in IoT: A Systematic Mapping Study,"Cloud computing and Internet of Things (IoT) are computing technologies that provide services to consumers and businesses, allowing organizations to become more agile and flexible. Therefore, ensuring quality of service (QoS) through service-level agreements (SLAs) for such cloud-based services is crucial for both the service providers and service consumers. As SLAs are critical for cloud deployments and wider adoption of cloud services, the management of SLAs in cloud and IoT has thus become an important and essential aspect. This paper investigates the existing research on the management of SLAs in IoT applications that are based on cloud services. For this purpose, a systematic mapping study (a well-defined method) is conducted to identify the published research results that are relevant to SLAs. This paper identifies 328 primary studies and categorizes them into seven main technical classifications: SLA management, SLA definition, SLA modeling, SLA negotiation, SLA monitoring, SLA violation and trustworthiness, and SLA evolution. This paper also summarizes the research types, research contributions, and demographic information in these studies. The evaluation of the results shows that most of the approaches for managing SLAs are applied in academic or controlled experiments with limited industrial settings rather than in real industrial environments. Many studies focus on proposal models and methods to manage SLAs, and there is a lack of focus on the evolution perspective and a lack of adequate tool support to facilitate practitioners in their SLA management activities. Moreover, the scarce number of studies focusing on concrete metrics for qualitative or quantitative assessment of QoS in SLAs urges the need for in-depth research on metrics definition and measurements for SLAs."
pub.1160378716,Toward Network-Slicing-Enabled Edge Computing: A Cloud-Native Approach for Slice Mobility,"Network slicing is a key enabler for 5G and beyond networks that permits operators to provide scalable, flexible, and dedicated networks over a common physical infrastructure. To cope with the rising demand for ultrareliable and low-latency communication (URLLC) in beyond 5G networks, the provision of dedicated secure networks closer to the users is essential. Multiaccess edge computing (MEC) is a promising technology that provides data and computational resources closer to mobile users. However, MEC servers are resource-constrained, and offering dedicated service-specific network slices at the edge in a highly dynamic and mobile environment is challenging. Network slicing and MEC are being evolved by two different standardization bodies that limit their integration and raise mobility challenges that deserve more attention. We propose a cloud-native microservices architecture for network slice mobility management in MEC that permits each MEC slice to be distributed as stateless and independently deployable microservices. The proposal separates the MEC slice operational data and the user context, as each network function in a MEC slice stores the context in a separate shared database. The proposed architecture leverages new SDN extended federation modules in compliance with the ETSI requirements for inter-MEC system coordination. The federation modules support a more flexible and scalable creation of network slices at MEC servers, efficient resource utilization, and mobility of network slices across MEC servers. The simulation results show that our proposed architecture outperforms the existing SDN-based approaches for network slicing in MEC by achieving high slice acceptance rates and reduced slice migration delay."
pub.1173163332,"Enabling federated learning across the computing continuum: Systems, challenges and future directions","In recent years, as the boundaries of computing have expanded with the emergence of the Internet of Things (IoT) and its increasing number of devices continuously producing flows of data, it has become paramount to boost speed and to reduce latency. Recent approaches to this growing complexity and data deluge aim to integrate seamlessly and securely diverse computing tiers and data environments, spanning from core cloud to edge — the Computing Continuum (or Edge-to-Cloud Continuum). Typically, the cloud is used for resource-intensive computations while the edge for low-latency tasks. This provides an opportunity to run complex AI-enabled applications across multiple tiers specifically facilitating the training of Machine Learning (ML) models at the “edge” of the Internet (i.e., beyond centralized computing facilities such as cloud datacenters). Federated Learning (FL) represents a novel ML paradigm for collaborative training, capitalizing on processing capabilities at the edge for training purposes while addressing privacy concerns. A set of clients (i.e., edge devices) collaboratively train a shared model under the supervision of a centralized server without exchanging personal data. However, several challenges arise from the decentralized nature of FL in the Computing Continuum context: statistical heterogeneity (data drift between parties), system heterogeneity (due to the nature of the environment), volatility (e.g., client dropouts), security threats and, persistently, privacy (although no personal data is transmitted, the shared model updates include information about data used for training). As opposed to previous studies dedicated to federated learning (typically on homogeneous, edge-based infrastructures), this survey aims to present a systematic overview of the existing literature addressing how state-of-the-art Federated Learning (FL) systems contend with the challenges previously outlined within the edge-to-cloud Computing Continuum, in particular heterogeneity, volatility and large-scale distribution. We analyze representative tools for implementing, monitoring, configuring and deploying such systems. We highlight significant efforts made to overcome statistical heterogeneity and security problems in FL. We specifically analyze the quality of the experimental evaluation done for existing systems and the relevant benchmarks. Finally, we discuss some open issues and future directions (e.g., lack of experiments in realistic environments) to support the broader adoption of FL across the continuum and to eventually fulfill the vision of the AI and edge computing convergence — the edge intelligence."
pub.1144691946,FETCH: A Deep Learning-Based Fog Computing and IoT Integrated Environment for Healthcare Monitoring and Diagnosis,"These days cloud-based infrastructure is facing many challenges, out of which the major issue is their syncing data before cutover and data migration. Due to the limited scalability in terms of security concerns of cloud computing, the need for a centralized IoTs based environment has been constrained to a limited extent. The sensitivity of device latency emerged during healthy systems such as health monitoring, etc. is the main reason, because healthy systems require computing operations on high-volume data. Fog computing provides an innovative solution to improve the performance of cloud computing, providing the ability to take the necessary resources and those that are closer to the end-users. Existing fog computing models retain several limitations, such as either considering result accuracy or overestimating response time, but managing both together impairs system compatibility. FETCH is a proposed framework that integrates with edge computing devices to work on deep learning technology and automated monitoring and offers a highly useful framework for real-life health care systems such as heart disease and more. The proposed Fog-enabled cloud computing framework uses FogBus, which demonstrates utility in the form of consumption of power, network bandwidth, jitter, latency, process execution time, and their accuracy as well."
pub.1101062617,Energy Efficient Scheduling of Scientific Workflows in Cloud Environment,"High end scientific applications in the form of workflows are being executed in cloud for various benefits. But with an increase in the processing capabilities of the cloud system, the energy consumption has also increased significantly. Thus energy efficient execution of these scientific workflows in cloud becomes essential. Existing research on energy efficient scheduling of scientific workflows in cloud mainly focus on reducing only the dynamic energy consumption of the compute nodes and uses DVFS technique. In this paper, we have proposed six different energy efficient scheduling approaches for a set of online scientific workflows in a cloud system considering both static and dynamic energy consumption of the compute nodes. These approaches are divided into two categories: non-splittable allocation of VMs on single host, and splittable allocation on multiple hosts. We have compared the performance of our proposed policies with state-of-art energy efficient scheduling policy, EnReal and found that the policies perform better than EnReal. All three scheduling policies with non-splittable VM allocation perform at par with EnReal in energy consumption but they do not require any migration of VMs. And all policies under splittable VM category perform significantly better than EnReal with an average energy reduction of 70%."
pub.1012210329,Policing as a Service in the Cloud,"Security and privacy are fundamental concerns in cloud computing both in terms of legal complications and user trust. Cloud computing is a new computing paradigm, aiming to provide reliable, customized, and guaranteed computing dynamic environment for end users. However, the existing security and privacy issues in the cloud still present a strong barrier for users to adopt cloud computing solutions. This paper investigates the security and privacy challenges in cloud computing in order to explore methods that improve the users’ trust in the adaptation of the cloud. Policing as a Service can be offered by the cloud providers with the intention of empowering users to monitor and guard their assets in the cloud. This service is beneficial both to the cloud providers and the users. However, at first, the cloud providers may only be able to offer basic auditing services due to undeveloped tools and applications. Similar to other services delivered in the cloud, users can purchase this service to gain some control over their data. The subservices of the proposed service can be Privacy as a Service and Forensics as a Service. These services give users a sense of transparency and control over their data in the cloud while better security and privacy safeguards are sought."
pub.1139005298,European Weather Cloud at the service of the European Meteorological Infrastructure (EMI) ,"<p>Since 2019, ECMWF (European Centre for Medium-Range Weather Forecasts) together with EUMETSAT (European Organisation for the Exploitation of Meteorological Satellites) initiated a project named “<strong>European Weather Cloud</strong>” (https://www.europeanweather.cloud/) expected to become operational in 2022. The strategic goal of this initiative is to build and offer a <strong>community cloud infrastructure</strong> on which Member and Co‐operating States of both organizations can create and manage on demand virtual resources enabling access to the ECMWF’s Numerical Weather Predication (NWP) products and EUMETSAT’s satellite data in a timely, efficient, and configurable fashion. Moreover, one of the main goals is to involve more entities in this initiative in a joint effort to form a federation of clouds/data offered from our Member States, for the maximum benefit of the European Meteorological Infrastructure.</p><p>During the current pilot phase of the project several use cases have been defined, mostly aimed at service developers own organisations. These broad categories of use cases are:</p><ul><li>Web services exploring hosted datasets.</li>
<li>Infrastructure allowing the running of an atmospheric dispersion model on ECMWF forecast data.</li>
<li>Platform to support the training of machine learning models on archive datasets.</li>
<li>Platform to support workshops and training courses (DWD/ICON model training, various ECMWF training courses)</li>
<li>Environment facilitating research in collaboration with external partners.</li>
</ul><p>Some examples of the use cases currently developed at the European Weather Cloud are:</p><ul><li>The Royal Meteorological Institute of Belgium prepares ECMWF forecast data for use in a local atmospheric dispersion model.</li>
<li>The German weather service, which is already feeding maps generated by a server it deployed on the cloud into its public GeoPortal service.</li>
<li>The Royal Netherlands Meteorological Institute hosts a climate explorer web application based on KNMI climate explorer data and ECMWF weather and climate reanalyses.</li>
<li>EUMETSAT Numerical Weather Prediction Satellite Application Facility (NWP SAF) develops a training module will develop a training module for a fast radiative transfer model (RTTOV) based on ERA5 reanalysis data.</li>
<li>EUMETSAT and ECMWF joint use case assess bias correction schemes for the assimilation of radiance data based on several satellite data time series.</li>
</ul><p>During the current pilot phase of the project, both organizations have organised user and technical workshops to actively engage with the meteorological community to align the evolution of the European Weather Cloud to reflect and satisfy their goals and needs.</p><p>In this presentation, the status of the project will be analysed describing the existing infrastructure, the offered services and how these are accessed by the end-users along with examples of the existing use cases. The p"
pub.1103960965,Digital Transformation and IIoT for Oil and Gas Production,"Abstract
                  Oil and gas producers are challenged with addressing long-term business needs such as: improving health, safety, and environment; extending life of production equipment; increasing reliability to reduce maintenance cost; reducing losses; improving operability to reduce personnel cost; improving productivity; increasing production; and improving governance and regulatory compliance.
                  To meet these business needs, operating companies are once again investing, now in digital transformation of how their sites are run and maintained. Personnel are digitally enabled. For safety this includes digital distress calls, digital safety checks for situational awareness, digital mustering and geolocating. For reliability, new ways of working include digital inspection and predictive maintenance of more of the equipment, digital collaboration with experts, digital documents on-the-go, and digital integrity management. For operations, new best practices include digital operator rounds including on EOR injection wells and digital logbooks. Also, instead of having people at site; some personnel instead work from a city office or equipment monitoring is outsourced to a service provider as Industrial Internet of Things (IIoT) -based connected services.
                  This digital transformation is made possible with new technologies such as wireless sensor networks, analytics software, and cloud computing making condition monitoring for a broader scope of process equipment practical, and enable new business models without capital outlay. Wireless, non-intrusive sensors along with readymade software apps enable easy and rapid deployment of these solutions at low cost.
                  Digital transformation with IIoT and advanced software does not require costly and time-consuming custom programming, but can be accomplished through purpose-built, readymade apps with embedded subject matter expert knowledge. Using platform-agnostic apps, operating companies can use their existing middleware instead of replacing it, and without adding another expensive platform layer, thus protecting existing investment.
                  Results from digital transformation include: extended life of production equipment to ensure continued service of facilities and operations, maintenance cost reduction, operations cost reduction, improved productivity, and increased production. Further, all departments benefit from the same digital infrastructure, which also enhances integration across disciplines."
pub.1095295320,Adaptive and Cost-effective Service Placement,"In this paper we will discuss the problem of placing service replicas in the cloud network to fulfil customers' demand. Such a problem hass been well studied in ‘Facility Location’ topic; however, in this area opening a new facility is costly and its migration is an expensive decision which makes the solution static. In cloud network infrastructures, services are pieces of software that can be migrated from one server to another in an easier way and therefore the dynamic service placement should be part of the solution. This paper aims to propose a new scheme of optimal dynamic service placement in the context of changing network environment. This scheme reinforces the existing solutions proposed in “Facility Location”, allowing at each period of time to find the best locations for services given the network topology, resource availability and customers' demand. The scheme considers the efficiency of the new locations versus the necessary modifications that have to be made to obtain them. The solution has been evaluated by simulation and has shown that small modification in service deployment can achieve high performance close to optimal solutions."
pub.1061715900,Whispers in the Hyper-Space: High-Bandwidth and Reliable Covert Channel Attacks Inside the Cloud,"Privacy and information security in general are major concerns that impede enterprise adaptation of shared or public cloud computing. Specifically, the concern of virtual machine (VM) physical co-residency stems from the threat that hostile tenants can leverage various forms of side channels (such as cache covert channels) to exfiltrate sensitive information of victims on the same physical system. However, on virtualized x86 systems, covert channel attacks have not yet proven to be practical, and thus the threat is widely considered a “potential risk.” In this paper, we present a novel covert channel attack that is capable of high-bandwidth and reliable data transmission in the cloud. We first study the application of existing cache channel techniques in a virtualized environment and uncover their major insufficiency and difficulties. We then overcome these obstacles by: 1) redesigning a pure timing-based data transmission scheme, and 2) exploiting the memory bus as a high-bandwidth covert channel medium. We further design and implement a robust communication protocol and demonstrate realistic covert channel attacks on various virtualized x86 systems. Our experimental results show that covert channels do pose serious threats to information security in the cloud. Finally, we discuss our insights on covert channel mitigation in virtualized environments."
pub.1130733533,Enhancing the Access Privacy of IDaaS System Using SAML Protocol in Fog Computing,"Fog environment adoption rate is increasing day by day in the industry. Unauthorized accessing of data occurs due to the preservation of Identity and information of the users either at the endpoints or at the middleware. This paper proposes a methodology to protect and preserve the Identity during data transmission of the users. It uses fog computing for storage against security issues in the cloud and database environment. Cloud and database architectures failed to protect the data and Identity of users but the Fog computing based Identity management as a service (IDaaS) system can handle it with Security Assertion Mark-up Language (SAML) protocol and Pentatope based Elliptic Curve Crypto cipher. A detailed comparative study of the proposed and existing techniques is investigated by considering multi-authentication dialogue, security services, service providers, Identity, and access management."
pub.1018789552,Hierarchical Attribute-Role Based Access Control for Cloud Computing,"With the rapid and wide adoption of cloud computing, data outsourcing in cloud storage is gaining attention due to its cost effectiveness, reliability and availability. However, data outsourcing introduces new data security and privacy issues, therefore access control and cryptography are essential ingredients in a cloud computing environment to assure the confidentiality of the outsourced data. Existing access control systems suffer from manual user role and role permission assignments that impose online and computational burdens on the data owner in large scale cloud systems. In this paper, a hierarchical attribute driven role based access control system is proposed, such that the user role assignments can be automatically constructed using policies applied on the attributes of users and roles. The proposed access control system consequently solves the scalability and key management problems in cloud storage systems."
pub.1134340574,A review of edge computing: Features and resource virtualization,"With the advent of Internet of Things (IoT) connecting billions of mobile and stationary devices to serve real-time applications, cloud computing paradigms face some significant challenges such as high latency and jitter, non-supportive location-awareness and mobility, and non-adaptive communication types. To address these challenges, edge computing paradigms, namely Fog Computing (FC), Mobile Edge Computing (MEC) and Cloudlet, have emerged to shift the digital services from centralized cloud computing to computing at edges. In this article, we analyze cloud and edge computing paradigms from features and pillars perspectives to identify the key motivators of the transitions from one type of virtualized computing paradigm to another one. We then focus on computing and network virtualization techniques as the essence of all these paradigms, and delineate why virtualization features, resource richness and application requirements are the primary factors for the selection of virtualization types in IoT frameworks. Based on these features, we compare the state-of-the-art research studies in the IoT domain. We finally investigate the deployment of virtualized computing and networking resources from performance perspective in an edge-cloud environment, followed by mapping of the existing work to the provided taxonomy for this research domain. The lessons from the reviewed are that the selection of virtualization technique, placement and migration of virtualized resources rely on the requirements of IoT services (i.e., latency, scalability, mobility, multi-tenancy, privacy, and security). As a result, there is a need for prioritizing the requirements, integrating different virtualization techniques, and exploiting a hierarchical edge-cloud architecture."
pub.1160409852,Interactive Object Segmentation in 3D Point Clouds,"We propose an interactive approach for 3D instance segmentation, where users can iteratively collaborate with a deep learning model to segment objects directly in a 3D point cloud. Current methods for 3D instance segmentation are generally trained in a fully-supervised fashion, which requires large amounts of costly training labels, and does not generalize well to classes unseen during training. Few works have attempted to obtain 3D segmentation masks using human interactions. Existing methods rely on user feedback in the 2D image domain. As a consequence, users are required to constantly switch between 2D images and 3D representations, and custom architectures are employed to combine multiple input modalities. Therefore, integration with existing standard 3D models is not straightforward. The core idea of this work is to enable users to interact directly with 3D point clouds by clicking on desired 3D objects of interest (or their background) to interactively segment the scene in an open-world setting. Specifically, our method does not require training data from any target domain and can adapt to new environments where no appropriate training sets are available. Our system continuously adjusts the object segmentation based on the user feedback and achieves accurate dense 3D segmentation masks with minimal human effort (few clicks per object). Besides its potential for efficient labeling of large-scale and varied 3D datasets, our approach, where the user directly interacts with the 3D environment, enables new AR/VR and human-robot interaction applications."
pub.1172101791,A modular and scalable web platform for computational phylogenetics,"Phylogenetic analysis, which allow to understand the evolution of bacterial
and viral epidemics, requires large quantities of data to be analysed and
processed for knowledge extraction. One of the major challenges consists on the
integration of the results from typing and phylogenetic inference methods with
epidemiological data, namely in what concerns their integrated and simultaneous
analysis and visualization. Numerous approaches to support phylogenetic
analysis have been proposed, varying from standalone tools to integrative web
applications that include tools and/or algorithms for executing the common
analysis tasks for this kind of data. However, most of them lack the capacity
to integrate epidemiological data. Others provide the ability for visualizing
and analyzing such data, allowing the integration of epidemiological data but
they do not scale for large data analysis and visualization. Namely, most of
them run inference and/or visualization optimization tasks on the client side,
which becomes often unfeasible for large amounts of data, usually implying
transferring data from existing databases in order to be analysed. Moreover,
the results and optimizations are not stored for reuse. We propose the PHYLOViZ
Web Platform, a cloud based tool for phylogenetic analysis, that not only
unifies the features of both existing versions of PHYLOViZ, but also supports
structured and customized workflows for executing data processing and analyses
tasks, and promotes the reproducibility of previous phylogenetic analyses. This
platform supports large scale analyses by relying on a workflow system that
enables the distribution of parallel computations on cloud and HPC
environments. Moreover, it has a modular architecture, allowing easy
integration of new methods and tools, as well as customized workflows, making
it flexible and extensible."
pub.1032889255,Towards a Smartphone based Multimode Sensing,"With rapid strides being made in mobile computing technology, today smart phones are not just devices for mobile communications and small computations, but may also be considered as a full scale PC embedded with a rich set of sensors such as accelerometers, GPS, Microphone, Camera and Ambient Light Sensor. Application based these integrated sensors are being developed for various domains. Even though a range of sensors available on smart phones are sufficient for several applications, but some applications like monitoring air quality or pollutants, would need external sensors along with existing sensors. This paper this presents brief description about architecture for seamless integration of external sensors with the smartphone, Tri-Coder Framework. We develop easily deployable sensor cloud enabled smartphone applications to record and retrieve sensor data in a dynamically changing environment that could potentially increase situational awareness."
pub.1129457073,Condition-based maintenance for major airport baggage systems,"
                    Purpose
                    The aim of this paper is to develop a contribution to knowledge that adds to the empirical evidence of predictive condition-based maintenance by demonstrating how the availability and reliability of current assets can be improved without costly capital investment, resulting in overall system performance improvements
                  
                  
                    Design/methodology/approach
                    The empirical, experimental approach, technical action research (TAR), was designed to study a major Middle Eastern airport baggage handling operation. A predictive condition-based maintenance prototype station was installed to monitor the condition of a highly complex system of static and moving assets.
                  
                  
                    Findings
                    The research provides evidence that the performance frontier for airport baggage handling systems can be improved using automated dynamic monitoring of the vibration and digital image data on baggage trays as they pass a service station. The introduction of low-end innovation, which combines advanced technology and low-cost hardware, reduced asset failures in this complex, high-speed operating environment.
                  
                  
                    Originality/value
                    The originality derives from the application of existing hardware with the combination of edge and cloud computing software through architectural innovation, resulting in adaptations to an existing baggage handling system within the context of a time-critical logistics system.
                  "
pub.1148189717,IoT Network Used in Fog and Cloud Computing,"The ever-changing rapid paradigm shift from one technology to another has led to extensive research in interdisciplinary fields. The different technologies are enhancing and transmuting at such an exponential rate that it has become important to effectively manage all the data and related environments associated with them. The aim of this chapter is to contribute to understanding the significance of fog and cloud computing and the related Internet of Things (IoT) networks supporting these technologies in real-time applications. This chapter provides insights on how the IoT frameworks provide increased scalability, increased performance, and immediate pay-as-you-go payment options to both the providers and consumers. This chapter aims to provide detailed information about IoT networks which can be employed in cloud and fog computing. The chapter seeks to provide the readers a brief description on introduction of IoT, IoT networks, cloud computing, and fog computing and their integration with IoT. A detailed comparison between cloud computing and fog computing paradigms throws light on the distinctions between the two paradigms. This chapter highlights the implementation of fog and cloud computing using IoT in detail and also highlights the security issues, threats and concerns related to the discussed technologies and their implementations. We have also explored the IoT network protocols used for the implementation of fog computing and cloud computing using IoT, along with few case studies which underline the applications and examples with respect to the technologies. Since the power and potential of IoT is being realized, major business giants are investing heavily in the technology as the gains are long termed. As per recent reports, the fog computing market is expected to reach an approximate 18 to 20 billion $ by 2022. Also, fog computing has become a necessity only for IoT and cloud, but also for embedded artificial intelligence, it has become an integral part of major interdisciplinary works. And since the technology is acting as the backbone of the industries, it cannot be ignored now and it is here to stay. It is expected that fog computing is anticipated to embark into already existing software, devices as Fog as Service (FaaS)."
pub.1147185887,Interactive Object Segmentation in 3D Point Clouds,"We propose an interactive approach for 3D instance segmentation, where users
can iteratively collaborate with a deep learning model to segment objects in a
3D point cloud directly. Current methods for 3D instance segmentation are
generally trained in a fully-supervised fashion, which requires large amounts
of costly training labels, and does not generalize well to classes unseen
during training. Few works have attempted to obtain 3D segmentation masks using
human interactions. Existing methods rely on user feedback in the 2D image
domain. As a consequence, users are required to constantly switch between 2D
images and 3D representations, and custom architectures are employed to combine
multiple input modalities. Therefore, integration with existing standard 3D
models is not straightforward. The core idea of this work is to enable users to
interact directly with 3D point clouds by clicking on desired 3D objects of
interest~(or their background) to interactively segment the scene in an
open-world setting. Specifically, our method does not require training data
from any target domain, and can adapt to new environments where no appropriate
training sets are available. Our system continuously adjusts the object
segmentation based on the user feedback and achieves accurate dense 3D
segmentation masks with minimal human effort (few clicks per object). Besides
its potential for efficient labeling of large-scale and varied 3D datasets, our
approach, where the user directly interacts with the 3D environment, enables
new applications in AR/VR and human-robot interaction."
pub.1148833180,Deep reinforcement learning for energy and time optimized scheduling of precedence-constrained tasks in edge–cloud computing environments,"The wide-spread embracement and integration of Internet of Things (IoT) has inevitably lead to an explosion in the number of IoT devices. This in turn has led to the generation of massive volumes of data that needs to be transmitted, processed and stored for efficient interpretation and utilization. Edge computing has emerged as a viable solution which complements cloud thereby enabling the integrated edge–cloud paradigm to successfully satisfy the design requirements of IoT applications. A vast majority of existing studies have proposed scheduling frameworks for individual tasks and only very few works have considered the more challenging problem of scheduling complex workloads such as workflows across edge–cloud environments. Workflow scheduling is an NP hard problem in distributed infrastructures. It is further complicated when scheduling framework needs to coordinate workflow executions across resource constrained and highly distributed edge–cloud environments. In this work, we leverage Deep Reinforcement Learning for designing a workflow scheduling framework capable of overcoming the aforementioned challenges. Different from all existing works we have designed a novel hierarchical action space for promoting a clear distinction between edge and cloud nodes. Coupled with this a hybrid actor–critic based scheduling framework enhanced with proximal policy optimization technique is proposed to efficiently deal with the complex workflow scheduling problem in edge–cloud environments. Performance of the proposed framework was compared against several baseline algorithms using energy consumption, execution time, percentage of deadline hits and percentage of jobs completed as evaluation metrics. Proposed Deep Reinforcement Learning technique performed 56% better with respect to energy consumption and 46% with respect to execution time compared to time and energy optimized baselines, respectively. This was achieved while also maintaining the energy efficiency in par with the energy optimized baseline and execution time in par with the time optimized baseline. The results thus demonstrate the superiority of the proposed technique in establishing the best-trade off between the conflicting goals of minimizing energy consumption and execution time."
pub.1135563497,Providing Consistent State to Distributed Storage System,"In cloud storage systems, users must be able to shut down the application when not in use and restart it from the last consistent state when required. BlobSeer is a data storage application, specially designed for distributed systems, that was built as an alternative solution for the existing popular open-source storage system-Hadoop Distributed File System (HDFS). In a cloud model, all the components need to stop and restart from a consistent state when the user requires it. One of the limitations of BlobSeer DFS is the possibility of data loss when the system restarts. As such, it is important to provide a consistent start and stop state to BlobSeer components when used in a Cloud environment to prevent any data loss. In this paper, we investigate the possibility of BlobSeer providing a consistent state distributed data storage system with the integration of checkpointing restart functionality. To demonstrate the availability of a consistent state, we set up a cluster with multiple machines and deploy BlobSeer entities with checkpointing functionality on various machines. We consider uncoordinated checkpoint algorithms for their associated benefits over other alternatives while integrating the functionality to various BlobSeer components such as the Version Manager (VM) and the Data Provider. The experimental results show that with the integration of the checkpointing functionality, a consistent state can be ensured for a distributed storage system even when the system restarts, preventing any possible data loss after the system has encountered various system errors and failures."
pub.1170510784,Effective use of cloud technologies in corporate governance,"Аннотация: проблема эффективного использования облачных технологий в корпоративном управлении занимает важное место в современной деловой и информационной среде. В эпоху цифровизации и постоянного технологического прогресса, облачные решения предоставляют компаниям новые возможности для повышения эффективности, гибкости и конкурентоспособности. Они позволяют организациям масштабировать свои ресурсы, оптимизировать затраты и улучшать управленческие процессы, при этом обеспечивая высокий уровень доступности и безопасности данных. Однако, несмотря на значительные преимущества, использование облачных технологий сопряжено с рядом трудностей, которые включают в себя вопросы безопасности и конфиденциальности данных, интеграцию с существующими системами и инфраструктурой, а также управление и контроль над облачными ресурсами. Кроме того, эффективное внедрение облачных технологий требует изменений в организационной культуре и подходах к управлению. Объектом исследования являются облачные технологии и их применение в корпоративном управлении. Целью исследования является анализ и определение наиболее эффективных способов использования облачных технологий в корпоративном управлении. Методы исследования: литературный обзор, сравнительный анализ, общенаучные методы. Научная новизна исследования: исследование фокусируется на актуальных и быстро развивающихся облачных технологиях, особенно в контексте их влияния на корпоративное управление, и направлено на выявление конкретных стратегий и практик, которые могут быть использованы компаниями для повышения эффективности и конкурентоспособности благодаря облачным технологиям. Abstract: the problem of effective use of cloud technologies in corporate management occupies an important place in the modern business and information environment. In the era of digitalization and constant technological progress, cloud solutions provide companies with new opportunities to increase efficiency, flexibility and competitiveness. They enable organizations to scale their resources, optimize costs and improve management processes, while ensuring high levels of data availability and security. However, despite the significant benefits, the use of cloud technologies is associated with a number of challenges, which include issues of data security and privacy, integration with existing systems and infrastructure, and management and control of cloud resources. In addition, effective adoption of cloud technologies requires changes in organizational culture and management approaches. The object of the study is cloud technologies and their application in corporate management. The purpose of the study is to analyze and determine the most effective ways to use cloud technologies in corporate management. Research methods: literature review, comparative analysis, general scientific methods. Scientific novelty of the study: The study focuses on current and rapidly developing cloud technologies, especially in the context of their impact on corporat"
pub.1167566812,Privacy-Preserving Data in IoT-based Cloud Systems: A Comprehensive Survey with AI Integration,"As the integration of Internet of Things devices with cloud computing
proliferates, the paramount importance of privacy preservation comes to the
forefront. This survey paper meticulously explores the landscape of privacy
issues in the dynamic intersection of IoT and cloud systems. The comprehensive
literature review synthesizes existing research, illuminating key challenges
and discerning emerging trends in privacy preserving techniques. The
categorization of diverse approaches unveils a nuanced understanding of
encryption techniques, anonymization strategies, access control mechanisms, and
the burgeoning integration of artificial intelligence. Notable trends include
the infusion of machine learning for dynamic anonymization, homomorphic
encryption for secure computation, and AI-driven access control systems. The
culmination of this survey contributes a holistic view, laying the groundwork
for understanding the multifaceted strategies employed in securing sensitive
data within IoT-based cloud environments. The insights garnered from this
survey provide a valuable resource for researchers, practitioners, and
policymakers navigating the complex terrain of privacy preservation in the
evolving landscape of IoT and cloud computing"
pub.1094360572,Evaluating Risks in Cloud Computing: Security Perspective,"The cloud computing provide resources and service as and when required over the internet. In cloud computing environment users share highly important data, maintaining centralizing storage of data and resource sharing which tents to extends financial benefit, rather managing their own systems. World is still not very open to cloud computing platform due to many security, privacy and policy related issues that are responsible for limiting the adoption of cloud computing. However, legal challenges and policies issues of cloud computing are the major cause in slow down the growth of cloud computing. In this paper potential of cloud computing with reference to Security, Privacy and Policy issues are examined as a part of major concern which makes the computing potential puny and review of existing literature for security challenges and policies issues in cloud computing."
pub.1147191510,FAML: Fog Descriptor Language for Fog Service Development and Deployments,"Cloud-centric services are experiencing major challenges due to the tsunami of computations and data streaming. Some of these challenges are bandwidth, latency, uninterrupted requests, continuous data streaming by the end devices, and computation dependability. Fog computing is a new computation paradigm that attempts to address delay and latency challenges in cloud services, mainly by offloading computations to computing nodes near the consumers. One of the main challenges to this computation model adoption is the lack of standardized software development kits (SDKs), tools, and run-time environment. This paper introduces a new descriptor and annotation language that can be used to develop an offloadable fog service from an already existing cloud service. FAML is an abbreviation for Fog Annotation and Meta Language; a language that can beused to develop a fog service. The FAML compiler has been implemented to work with Golang cloud service implementations. The generated fog services have been offloaded into fog nodes in the same local network of the service consumer as part of the experiment's evaluation. The paper presents the usage of the proposed language in three different service development use cases: image enhancement service, weather forecast service, and bulk update service."
pub.1181620528,Privacy preserving spatio-temporal attribute-based encryption for cloud applications,"Cloud computing offers scalable implementation of applications by sharing internet-based storage and computing resources. However, its ubiquitous nature introduces the security and privacy risks to sensitive data. Existing encryption techniques often rely on access control mechanisms to allow selective sharing of encrypted data. However, they don’t efficiently support secure integration of space and time constraints in the authorization mechanism, rendering them unsuitable for dynamic cloud environments. In this paper, we propose a privacy-preserving spatio-temporal attribute-based access control technique for cloud-based applications. Our approach utilizes ciphertext policy attribute-based encryption (CP-ABE) with distributed key generation, geohashes for proximity detection, and fog server-based verification. The proposed cryptosystem generates decryption keys based only on the user’s static attributes eliminating the need to manage user revocation due to frequent contextual changes. Time and location constraints are enforced through spatio-temporal locks in the access policy. Geohash enables defining authorized geographic areas while preserving user location privacy. Additionally, our system supports multiple attribute authorities for key generation, enhancing security by limiting user identity leakages and preventing key escrow attacks. Most of the decryption-related computations are outsourced to fog servers, thus, making the decryption independent of the number of attributes in the policies. The analysis of security and performance demonstrates the effectiveness of our scheme in practical cloud-based applications, enabling precise control over real-time data access while satisfying user privacy."
pub.1144335372,The Application of K-Cluster Algorithm Method in the Legalization of Grass-Roots Governance,"Big data promotes a new round of technological change. It collects massive data and analyzes the value of obtaining information based on the Internet of things and cloud computing, which is the premise of effective analysis and scientific decision making for decision makers. As a new way to update management concepts, social governance models and improve the governance capacity of the government, big data can promote the modernization of national governance system. In the era of big data, this paper mainly discusses how to promote the innovation of grass-roots governance from the aspects of rights operation, network construction, and legal system construction in the traditional grass-roots governance system. At the same time, it uses the thinking of big data governance to innovate the grass-roots governance mode, governance structure, governance ability, governance process and governance mode, and further optimize the innovation environment of grass-roots governance, we will continue to optimize and innovate grass-roots governance, promote the deep integration of new big data technologies and grass-roots social governance, and explore and improve the innovation of government governance capacity."
pub.1173009474,"Evaluate Canary Deployment Techniques Using Kubernetes, Istio, and Liquibase for Cloud Native Enterprise Applications to Achieve Zero Downtime for Continuous Deployments","To cater to the changing needs of the businesses, enterprises are adopting processes that allow rapid iteration and feedback loop. Today, development teams work closely with the business leveraging agile methods to gather feedback, assess the impact of the changes and deploy changes in a short duration. By taking advantage of the microservices architecture (MSA), large monolithic code is logically broken down into microservices that can be developed, deployed and scaled independently. Applications are leveraging containerization and orchestration technologies along with microservices architecture to package, deploy and manage the code across different environments. The underlying infrastructure and agile processes need to be supported with robust methods to perform code integration and deployments without any service disruption. This paper provides a qualitative assessment of the following code deployment techniques (i) Recreating Deployments (ii) Rolling Deployment (iii) Blue-Green Deployment (iv) Canary Deployment. This assessment can guide enterprises to identify the right code deployment strategy that can be adopted based on the business use case. Next, the paper dives deeper on how the in-built capabilities of Kubernetes along with open-source tools like Istio can be leveraged successfully to implement canary deployments for service changes. The paper presents a novel technique for performing canary deployments whereby service and database changes can be promoted to production by leveraging Istio and Liquibase along with load balancer without incurring any downtime for the application. The paper provides a complete canary deployment reference architecture that can be adopted by enterprises pursuing zero downtime for continuous deployments."
pub.1042634576,Middleware for efficient and confidentiality-aware federation of access control policies,"Software-as-a-Service (SaaS) is a type of cloud computing in which a tenant rents access to a shared, typically web-based application hosted by a provider. Access control for SaaS should enable the tenant to control access to data that are located at the provider side, based on tenant-specific access control policies. Moreover, with the growing adoption of SaaS by large enterprises, access control for SaaS has to integrate with on-premise applications, inherently leading to a federated set-up. However, in the state of the art, the provider completely evaluates all policies, including the tenant policies. This (i) forces the tenant to disclose sensitive access control data and (ii) limits policy evaluation performance by having to fetch this policy-specific data. To address these challenges, we propose to decompose the tenant policies and evaluate the resulting parts near the data they require as much as possible while keeping sensitive tenant data local to the tenant environment. We call this concept policy federation. In this paper, we motivate the need for policy federation using an in-depth case study analysis in the domain of e-health and present a policy federation algorithm based on a widely-applicable attribute-based policy model. Furthermore, we show the impact of policy federation on policy evaluation time using the policies from the case study and a prototype implementation of supporting middleware. As shown, policy federation effectively succeeds in keeping the sensitive tenant data confidential and at the same time improves policy evaluation time in most cases."
pub.1173793980,Cybersecurity Frameworks for Cloud Computing Environments,"Purpose: The general objective of this study was to explore cybersecurity frameworks for cloud computing environments.
 Methodology: The study adopted a desktop research methodology. Desk research refers to secondary data or that which can be collected without fieldwork. Desk research is basically involved in collecting data from existing resources hence it is often considered a low cost technique as compared to field research, as the main cost is involved in executive’s time, telephone charges and directories. Thus, the study relied on already published studies, reports and statistics. This secondary data was easily accessed through the online journals and library.
 Findings: The findings reveal that there exists a contextual and methodological gap relating to explore cybersecurity frameworks for cloud computing environments. The study emphasized the necessity of robust, comprehensive security measures to address the unique challenges of cloud infrastructures. It highlighted the importance of advanced security measures like encryption, multi-factor authentication, and continuous monitoring to mitigate risks. The research underscored the need for holistic and adaptable frameworks that integrate technological solutions and human factors, while also stressing regulatory compliance. The findings had significant implications for cloud service providers, businesses, regulatory bodies, and cybersecurity professionals, suggesting a focus on new technologies like AI and blockchain for future research.
 Unique Contribution to Theory, Practice and Policy: The Diffusion of Innovations Theory, Technology Acceptance Model (ATM) and Socio-Technical Systems Theory may be used to anchor future studies on cybersecurity frameworks for cloud computing environments. The study made significant theoretical, practical, and policy recommendations. It emphasized the need for an integrated theoretical approach, the adoption of multi-layered security practices, and regular security assessments. The study also advocated for standardized and specific regulatory frameworks tailored to cloud environments and international cooperation for consistent global cybersecurity policies. These recommendations aimed to enhance the understanding, implementation, and governance of cloud security, ultimately contributing to a more resilient and secure cloud computing ecosystem."
pub.1164733956,Agile FDP of Complex Brown Field Fast-Tracked by Digital Cloud Technologies,"Abstract PETRONAS D Field, discovered in 1981, is a brownfield located offshore Terengganu, Malaysia in the West central part of Malay Basin. The field has been on production for over 30 years. It is an E-W trending anticline consisting of vertically stacked sandstone reservoirs segmented into several fault blocks, forming structural traps. The field has been producing primarily under a natural depletion drive supplemented by water and gas injection as an IOR strategy. Several development campaigns have been carried out in the past where additional infill wells were drilled to improve recovery. Agile FDP has demonstrated the potential to recover an additional >50MMSTB of oil. The key challenges to overcome were to 1) deliver the project in a tight timeframe which required comprehensive evaluation of infill proposals to ensure reserves attainability; 2) assess the risk in proposed infill wells in view of communication between fault blocks due to juxtaposition of stacked sands across fault planes 3) achieve a reasonable history-match in the presence of a large number of subsurface uncertainties and obtain a probabilistic forecast; 4) insufficient time to investigate full range of development scenarios due to the constraints of on-premises infrastructure. These challenges are addressed by the PETRONAS LiveFDP digital transformation program, through deployment of digital cloud technologies and solutions with scalable High-Performance Computing (HPC) environment. The cloud-based native and Petrotechnical applications enable remote work, ensure full data auditability in an integrated E&P cognitive environment, enable large-scale probabilistic studies, and streamline the automated integration from Reservoir Engineering workflows to Economics Studies. The agile FDP workflows, enabled by unlimited HPC power, accelerate the subsurface studies and facilitate evaluation of a broad spectrum of development scenarios in an accelerated manner. The agile Field Development Planning studies completed within two months using HPC cloud solutions and workflows compared to 1-year timeframe of using on-premises infrastructure. Utilizing cloud solutions and ensemble probabilistic approach, the team has: Achieved project milestone of delivering first-oil one week ahead of the committed date and saved US$5 Million. Project delivered within sanctioned P80 cost and avoided NPT during drilling campaign. Performed detailed investigation into the impact of pressure depletion due to communication with adjacent fault blocks and with nearby ""S"" field through aquifer. Improved recovery of 0.7MMstb (~US$50 million) by optimizing well location and IOR injection scheme which improved sweep and pressure maintenance in the primary reservoirs. Conducted probabilistic studies of 40 uncertainty parameters by running 250 cases per ensemble in 2 days and significantly improved history-match (over 90% matched quality). Safeguarded 11 MMstb reserves with total project investment of US$328 mill"
pub.1093986199,Wireless Positioning Sensor Network Integrated with Cloud for Industrial Automation,"Automation of modern industrial plants require real-time tracking of object locations and sensing of local and ambient parameters for variety of applications such as counting and tracking of objects in assembly line, detection and positioning of failures of machines etc. Mostly, discrete Real Time Location System (RTLS) performs object tracking in existing industrial automation without its integration with the sensing and control network, which constrains application's responsiveness. In this paper, we propose an integrated solution of Wireless Positioning Sensor Network (WPSN) that is designed for accuracy, reliability, scalability and optimal network operation. The proposed WPSN is applicable for harsh indoor industrial environments for not only monitoring and control of plant operations but also for identification, localization and tracking of assets and inventory in industrial warehouses. The indoor industrial environment poses challenging conditions for radio signal propagation that adversely affects reliability of communication of sensing and location data. We approach reliability by incorporating redundancy and making our network reconfigurable through adaptive intelligent learning process. We employ adaptive clustering technique to address the need of scalable deployment for varied industrial scenarios. We include hybrid localization scheme to provide high precision positioning but with fallback reduced precision operation to deal with longterm channel impairment. The WPSN is connected with backend cloud infrastructure for low cost monitoring and control."
pub.1112688171,Improved Availability Using I_RRect Algorithm in Cloud Environment,"In our day to day life we are generating a lots of data, which has become a major problem to store those data in a local server or personal computer. In order to resolve this problem, we need many numbers of servers and networking which are more expensive of buying and which creates more complexity. To resolve this problem we are using the method called cloud computing, it provides IT resources as the service to the user over the internet and can use it on the basis of pay per usage. As it is a researching area of computer network technology, many of huge companies are providing the cloud services. According to the financial perspective, migration of data to the cloud remains trending now a days, but there are several other aspects that must be taken in to consideration before it is decided to do, among them the most important problem is security. Cloud computing security refers to the set of polices, technologies and controls over, applications and infrastructure of cloud computing. This paper tells us how to establish the data confidentiality, availability, security and integrity of shared data in multi cloud environment. So while providing the storage for cloud storage the providers repeatedly make sure the data confidentiality, by encrypting the content of the file by using encryption algorithm and certify the data integrity by cross checking the hash values of the file. Despite, when the cloud storage service fails, the availability of the user data cannot be certified and the cloud sharing function of the user data also cannot be supported. To overcome this users have to give full trust in existing schemes to the provider of the cloud storage service. If the provider system get hacked or becomes untrustworthy, the confidentiality of the user data will be exposed. In order to solve these problems, this paper proposes a scheme for securely storing and sharing data based on the improved proxy re-encryption algorithm in the multi cloud environment. Here the multi cloud storage is constructed to counter the failure of any single cloud, the symmetric encryption algorithm is used to encrypt the user files. The encryption key is accurately distributed with the Shamir's threshold secret sharing scheme by the general RSA algorithm. Finally the improved proxy re-encryption algorithm is advantage to support the sharing of encrypted user data via clouds. The pattern of the design is performed in the java development environment and is assessed under the assumed multi cloud environment. The experimental results of this design shows that, the time cost of the Shamir's secret partitioning process and symmetric encryption process about be negligible if the key size is greater than the 384 bytes, and the improved proxy re-encryption process takes about 1.5 seconds in an average."
pub.1119736710,A Novel Approach Toward Enhancing the Quality of Life in Smart Cities Using Clouds and IoT-Based Technologies,"The smart city means using information technologies as per the needs of citizens in order to improve their day-to-day activities with high efficiency and decrease the living cost. The development of the smart city is the process of urbanization which can further improve the efficiency, reliability, and security of a city. The integration of communication and information technologies with the Internet of Things (IoT) and artificial intelligence (AI) techniques will be helpful for the urban/metro city areas in the overall management of schools, colleges, universities, libraries, power plants, transportation systems, waste management, hospitals, water supply, law enforcement, and other community services. The information and digital technologies will be used by end users and office administrations for the overall management of the things related to urban/metro city areas. The information and communication technologies (ICT) will allow officials of the city to interact/communicate directly with social communities and the infrastructure of the city will be available to the city officials on their fingertips. This chapter describes the economic benefits, implementation costs, and challenges toward the development of a smart city and its integration with cloud computing, IoT, and AI technologies. In this research work, we have tried to study the existing technologies, and we have proposed a novel architecture of a smart city which incorporates IoT, AI, and distributed cloud computing technologies and the smart city will have its own independent self-management system for managing almost everything related to the needs of our daily life. The proposed work will be helpful in maintaining the ecological system of the earth and the use of clean solar energy is making it friendly to the environment."
pub.1132395550,A container-based cloud-native architecture for the reproducible execution of multi-population optimization algorithms,"Splitting a population into multiple instances is a technique used extensively in recent years to help improve the performance of nature-inspired optimization algorithms. Work on those populations can be done in parallel, and they can interact asynchronously, a fact that can be leveraged to create scalable implementations based on, among other methods, distributed, multi-threaded, parallel, and cloud-native computing. However, the design of these cloud-native, distributed, multi-population algorithms is not a trivial task. Using as a foundation monolithic (single-instance) solutions, adaptations at several levels, from the algorithmic to the functional, must be made to leverage the scalability, elasticity, (limited) fault-tolerance, reproducibility, and cost-effectiveness of cloud systems while, at the same time, conserving the intended functionality. Instead of an evolutive approach, in this paper, we propose a cloud-native optimization framework created from scratch, that can include multiple (population-based) algorithms without increasing the number of parameters that need tuning. This solution goes beyond the current state of the art, since it can support different algorithms at the same time, work asynchronously, and also be readily deployable to any cloud platform. We evaluate this solution’s performance and scalability, together with the effect other design parameters had on it, particularly the number and the size of populations with respect to problem size. The implemented platform is an excellent alternative for running locally or in the cloud, thus proving that cloud-native bioinspired algorithms perform better in their “natural” environment than other algorithms, and set a new baseline for scaling and performance of this kind of algorithms in the cloud."
pub.1101696369,GPU Power Modeling of HPC Applications for the Simulation of Heterogeneous Clouds,"Hardware accelerators have been widely used in the scientific community, as the gain in the performance of HPC applications is significant. Hardware accelerators have been used in cloud computing as well, though existing cloud simulation frameworks do not support modeling and simulation of such hardware. Models for the estimation of the power consumption of accelerators have been proposed by many researchers, but they require large number of inputs and computations, making them unsuitable for hyper scale simulations. In previous work, a generic model for the estimation of the power consumption of accelerators has been proposed, that can be combined with generic CPU power models suitable for integration in hyper scale simulation environments. This paper extends this work by providing models for the energy consumption of GPUs and CPU-GPU pairs, that are experimentally validated with the use of different GPU hardware models and GPU intensive applications. The relative error between the actual and the estimated energy consumption is low, thus the proposed models provide accurate estimations and can be efficiently integrated into cloud simulation frameworks."
pub.1162690442,Resource Management allocation in the CC Environments with Multi-Objective Optimization system,"These days, the topic of cloud computing has become a very popular paradigm for providing services online. For the cloud data center to deliver cloud infrastructure services in response to user demand, a set of virtualized resources are required. In order to lower the degree as a result of resource loss and SLA violations, the necessity for optimum resource management becomes a crucial problem in cloud data centers. These issues are classified as NP-hard, and meta heuristics approaches are well suited to solving them. The article develops a multi-objective hybrid fruit fly optimization (MOHFO)-based method for SLA-conscious dynamic resource management in cloud data centers. For the fruit fly optimization technique, the Bald Eagle Search (BES) optimization behavior search capabilities. To balance resource waste and SLA breaches, the proposed solution employs dynamic consolidation and deployment of virtual machines (VMs) mechanism. The suggested technique utilizing Clouds to simulate different data center designs, using QoS measures to assess the experimental outcomes. The proposed MOHFO technique is assessed and contrasted to existing optimization strategies in terms of resource wastage, energy use, communication costs, and migration frequency in order to provide cloud computing with QoS-aware optimal resource provisioning."
pub.1094912139,DeepSpotCloud: Leveraging Cross-Region GPU Spot Instances for Deep Learning,"Cloud computing resources that are equipped with GPU devices are widely used for applications that require extensive parallelism, such as deep learning. When the demand of cloud computing instance is low, the surplus of resources is provided at a lower price in the form of spot instance by AWS EC2. This paper proposes DeepSpotCloud that utilizes GPU-equipped spot instances to run deep learning tasks in a cost efficient and fault-tolerant way. Thorough analysis about spot instance price history logs reveals that GPU spot instances show more dynamic price change pattern than other general types of cloud computing resources. To deal with the price dynamicity of the GPU spot instance, DeepSpotCloud utilizes instances in different regions across continents as a single resource pool. This paper also proposes a task migration heuristic by utilizing a checkpointing mechanism of existing deep learning analysis platform to conduct fast task migration when a running spot instance is interrupted. Extensive experiments using real AWS services prove that the proposed task migration method is effective even in a WAN environment with limited network bandwidth. Comprehensive simulations by replaying AWS EC2 price history logs reveal that DeepSpotCloud can achieve 13% more cost gain than a state-of-the-art interrupt-driven scheduling policy. The prototype of DeepSpotCloud is implemented using various cloud computing services provided by AWS to serve real deep learning tasks."
pub.1156377218,Experience with an Interdisciplinary Approach to Removing Barriers Related to IT Personalized Support for Teachers in the Creation and Transmission of Educational Content,"Research on IT integration into teaching is an interdisciplinary field that has both educational (didactics) and informatics components. In particular, the situation with the Covid 19 pandemic has forced a push to address personal IT support for teachers in distance education. However, this runs into the problem of the lack of personal educational software, so that in practice the teacher has to adapt to existing technology and test how it can be used for teaching. In this context, the work of a university teacher requires the mass creation of educational content, its transfer between offline computers (laptop, classroom computers) and online environments (web, virtual learning environments, academic information systems, clouds, networks). Given the nature of university teaching, IT support solutions for self-study also face a challenge. However, no single technology covers such a broad scope, so there is a lack of universal solutions. The authors minimize this gap by programming universal software tailored to the needs of the teacher and by building a combined offline/online IT infrastructure on which to conduct the research. Collaborative research by an international team using the infrastructure is a solution to automate the creation of educational packages, including the multi-lingual support. The article clarifies the categories of barriers that the team had to overcome, either from a didactic or an informatics perspective. Here, a new paradigm using a specific data structure (called virtual knowledge) for the rapid reduction and concentration of educational content was proven to simulate virtually any teacher activity. Therefore, the goal of further research is to use the results and experiences to date to build a multilingual learning portal."
pub.1144628469,An efficient IDS in cloud environment using feature selection based on DM algorithm,"Cloud Computing provides the use of a wide array of applications to a designated server outside one’s personal computer. In the current technological era with the evolution of the Internet, it is being used on a wider range. With such popularity and wide use comes a threat to its security. Intrusion Detection System (IDS) helps to secure the cloud environment from intruders by classifying the packets as an attack or normal. The datasets used for such purpose are very large which contains many features hence takes a huge time in computation. It is important to choose pertinent features to feed into the model which can give better results than using all the features and take less computational time. The authors proposed a nature-inspired Dolphin Mating (DM) algorithm to determine pertinent features from the dataset. For this purpose, the authors have used the NSL-KDD dataset and Kyoto dataset. The selected features are trained and tested using several machine learning algorithms. The result obtained is compared with several existing algorithms and it was found that the proposed DM algorithm selects the most relevant feature subset which made the IDS efficient in the Cloud environment."
pub.1133342741,A Cloud Model‐Based Risk Assessment Methodology for Tunneling‐Induced Damage to Existing Tunnel,This study presents a cloud model‐based approach for risk assessment of existing tunnels in tunneling construction environments where the cloud model provides a basis for uncertainty transformation between its qualitative concepts and quantitative expressions. An evaluation index system is established for risk assessment of existing tunnels based on the tunnel‐induced failure mechanism analysis. The assessment result is obtained through the correlation with the cloud model of each risk level. Risk assessment for existing Guangzhou‐Shenzhen‐Hong Kong Railway Tunnel in the tunneling environment of Shenzhen Metro Line 6 is shown in a case study. Comparisons between Fuzzy Analytic Hierarchy Process (FAHP) methods are further discussed according to results. The proposed evaluation method is verified to be more competitive as the fuzziness and randomness of uncertainties in the risk assessment system can be considered comprehensively. This method can serve as a decision‐making tool for other similar project risk assessment methods to increase the likelihood of a successful project in an uncertain environment.
pub.1094995722,Localization of Health Center Assets Through an IoT Environment (LoCATE),"The rapid advances in modern wireless technology opens the door for new applications using the Internet of Things (IoT) technology. In the medical field, staff members of a certain hospital are in need for a system that tracks where patients/medical staff/devices are at any given time. LoCATE, which is Localization of Health Center Assets Through an IoT Environment, provides a near-real time tracking tool for medical systems using the existing 802.11 WiFi infrastructure. The primary goal of this system is to track assets and personnel at any hospital (e.g., Sentara® RMH hospital) and continuously log a real-time location data on a cloud computing platform such as Amazon Web Services (AWS). Using LoCATE, administrators can view the location of doctors, patients, and assets in real-time via a web UI or a mobile app, within the organization. The collected data, stored and processes on a Cloud Storage platform, is then analyzed to expose inefficiencies in daily operations and improve the health care system. Low-level functionality of the LoCATE system is unlike that of typical Radio-frequency identification (RFID) technologies. The spirit of the IoT paradigm employed by LoCATE makes the system both flexible and scalable, by leveraging collaboration between embedded and cloud systems. This flexibility will allow for the future support of additional applications such as hardware integration (e.g., New hardware components). This can include data acquisition such as usage statistics and historical patient health data. Compiling this data might pave the way for future research into disease vectors or could be used to optimize care delivered for specific conditions. While implications for an IoT system such as LoCATE are wide-ranging; its primary objective is to provide an easy to use, low-cost solution to track the location of medical assets in real-time."
pub.1045209341,Experimental evaluation of a flexible I/O architecture for accelerating workflow engines in ultrascale environments,"The increasing volume of scientific data and the limited scalability and performance of storage systems are currently presenting a significant limitation for the productivity of the scientific workflows running on both high-performance computing (HPC) and cloud platforms. Clearly needed is better integration of storage systems and workflow engines to address this problem. This paper presents and evaluates a novel solution that leverages codesign principles for integrating Hercules—an in-memory data store—with a workflow management system. We consider four main aspects: workflow representation, task scheduling, task placement, and task termination. The experimental evaluation on both cloud and HPC systems demonstrates significant performance and scalability improvements over existing state-of-the-art approaches."
pub.1159268576,Feature Consistent Point Cloud Registration in Building Information Modeling,"Point Cloud Registration contributes a lot to measuring, monitoring, and simulating in building information modeling(BIM). In BIM applications, the robustness and generalization of point cloud features are particularly important due to the huge differences in sampling environments. We notice two possible factors that may lead to poor generalization, the normal ambiguity of boundaries on hard edges leading to less accuracy in transformation; and the fact that existing methods focus on spatial transformation accuracy, leaving the advantages of feature matching unaddressed. In this work, we propose a boundary-encouraging local frame reference, the PyramidFeature(PMD), consisting of point-level, line-level, and mesh-level information to extract a more generalizing and continuous point cloud feature to encourage the knowledge of boundaries to overcome the normal ambiguity. Furthermore, instead of registration guided by spatial transformation accuracy alone, we suggest another supervision to extract consistent hybrid features. A large number of experiments have demonstrated the superiority of our PyramidNet(PMDNet), especially when the training(ModelNet40) and testing(BIM) sets are very different, PMDNet still achieves very high scalability."
pub.1174178682,Embarking on a digital journey to cater for future growth,"Only a third of all strategic initiatives are successfully implemented. This leaves huge untapped potential that can be unleashed by rethinking the way the organisation executes on their plans. Organisations need to adapt to a new approach to ensure their strategy works to enable better choices, higher engagement and a greater impact. However, before embarking on a strategic initiative, one must acknowledge that conducting a strategic transformation is an infinite exercise of keeping up with requirements, assessing and adapting to trends and development, whilst engaging and empowering the employees. Nordea Asset Management set out on a strategic journey with the overarching vision of being ‘Client driven, Investment led and Tech enabled’. This implied investigating how to adopt modern technology principles, reduce dependencies on legacy infrastructure and free up time with employees. This, to enable the organisation to focus on application functionality, utilise cloud-native platforms, exploit elasticity and scalability of cloud computing and make data available in a structured and non-complex manner. Making applications interoperable through a robust IT infrastructure and having data sources integrated and accessible — making the vision of ‘tech enabled’ come true. Nordea Asset Management made the strategic choice of embarking on a digital transformation to accelerate growth and strategically drive the business and the organisation forward. The choice was made after considerable investigations and market research. The key to making the strategic initiative come alive was securing the ownership of the transformation with the business, the business being the driver of customer value and thereby owners of the problem to be solved. This enabled Nordea Assets Management to ensure that the solutions being built were both fit for the customer and the growth of the business, as well as for the future digital services and data solutions. With a well-designed and resilient Technology base, companies can scale efficiently and enable the organisation to deliver even more value to customers, faster, while continuously increasing the experience for employees. Engaging the individuals and igniting the vision for the future is a key trait of an organisation adapting to disruption. And a prerequisite for a successful transformation is the ability to ‘keep the lights on’, having the legacy systems working and maintaining the strong relations, while at the same time bridging the complexity and creating an environment of trust, respect and curiosity to build the future."
pub.1175315524,Fintech network evolution: The strategic migration to advanced networking technologies,"The rapid evolution of fintech has necessitated a parallel advancement in network architecture to meet the growing demands for speed, security, and scalability. This review explores the strategic migration of fintech companies to advanced networking technologies, such as Software-Defined Networking (SDN), cloud computing, and 5G. These technologies are transforming the way fintech companies operate by providing enhanced flexibility, improved security measures, and the ability to handle vast amounts of data in real-time. The transition to advanced networking technologies is not merely a technical upgrade but a strategic move that aligns with broader business objectives, including improved customer experiences, regulatory compliance, and cost efficiency. This migration is driven by the need for networks that can support complex financial transactions, provide seamless connectivity across global markets, and ensure robust protection against increasingly sophisticated cyber threats. The review also examines the challenges faced during this migration, including the need for significant investment, the complexity of integrating new technologies with existing systems, and the ongoing management of security risks. Despite these challenges, the strategic benefits of adopting advanced networking technologies are compelling. Fintech companies that successfully migrate to these networks can achieve a competitive edge by offering faster, more reliable services, enhancing data analytics capabilities, and ensuring greater compliance with regulatory standards. Furthermore, this migration is expected to play a crucial role in the future of fintech, as the industry continues to innovate and expand into new markets. By embracing advanced networking technologies, fintech firms can position themselves as leaders in the digital financial landscape, ready to capitalize on emerging opportunities and navigate the challenges of a rapidly changing technological environment. The review concludes by highlighting the importance of strategic planning and cross-functional collaboration in ensuring a smooth and successful transition to these advanced networking solutions.
 Keywords: Fintech, Network Evolution, Strategic Migration, Advanced, Networking Technologies."
pub.1147150834,Enhancing Secure Mutual Authentication in Mobile Fog Computing Environment,"Fog computing that extends the cloud nearby to the IoT devices. The advent of IoT technology generates a huge amount of data and is processed in the cloud, this increases the latency. Many time-sensitive applications are affected by delayed response. Fog computing solves the latency problem by processing near the data generating device. Because of the nature of fog computing mobility support, dynamic environment, geographic distribution, location awareness existing schemes do not provide better security. To address these issues, this paper provides mutual secure authentication between each entity in the fog like between the cloud and fog node, between fog node and gateway, between gateway and IoT device. This security mechanism improves the authentication in the fog network. Mobility-based authentication providing mechanism can be implemented with the MobFogSim mobility dataset that is collected from the Luxembourg traffic. Migration of the node is considered in mobility secure authentication."
pub.1142519132,Time-aware Data Spaces - A key Computing Unit in the Edge-to-Cloud Continuum,"Digital transformation requires new views and strategies for edge computing for Industrial Internet of Things applications. In this work, we elaborate the importance of time-aware data spaces as a conceptual entity for a digital and scaleable approach in a data-driven world. This work introduces these small data spaces deployed and distributed through edge analytic boxes. The concept time-aware data space is more suitable than single sensors for this type of application. It is particularly suitable in the edge-to-cloud continuum to build robust solutions and allow a new view on these types of applications. Moreover, we use this approach in a production environment. The requirements are a suitable application area for our time-aware data spaces. They can be used to monitor the environment, the machines or the production processes. Such an approach can also be integrated into other existing solutions with reasonable efforts."
pub.1133382751,Psychological Dimensions in Trust-Based Models in Cloud Computing,"Cloud computing has been rapidly evolving and various organizations are updating their infrastructure and services to the cloud computing system. However, being a relatively new technology, users of cloud computing are facing various issues that need to be addressed. Identifying the risks associated and establishing trust in that is one such major issue that needs to be addressed. This article has focused on the various concepts related to trust in the cloud environment, technicalities of trust being used and evaluated, the trust models with an objective of identifying the inclusion of psychological dimensions and attributes in the assessment, evaluation, and management process. The results from the review have indicated that as such there are no models found which contribute to incorporating the psychological attributes. As an extension of the assessment, an integration of two important aspects are recommended in the existing trust model. The inclusion of psychological attributes that consist of motivation, perception, learning, and personality attributes; and an initial stage of assessment model in a top-down approach scheme for identifying the effectiveness of cloud service providers based on the collaborative trust information-sharing mechanism and by using techniques such as rankings and ratings."
pub.1127297686,Private STaaS with OpenStack Cinder Volumes for Hybrid/Multi-cloud,"Storage as a Service (STaaS) is in a very advanced stage and 80-90% of the users trust and use public cloud storage. However, enterprises and communities are still skeptical and are concerned with data safety and theft, and are investing to build their own private storage system, where cost of storage space is not the most important concern. This limits resource (compute and network) scale at the peak on demand in hybrid/multi-cloud environment as one cannot keep buying and upgrading computing hardware resources to support peak workloads in private cloud [9]. There are commercial solutions offered by few public cloud service providers/brokers to maintain the data in a private cloud in a hybrid environment, but this may have vendor/broker lock-in and portability/migration issues. In this paper, we explore OpenStack cloud for private Storage as a Service in hybrid/multi-cloud environment and demonstrate with our framework (middleware), which is built upon OpenStack [16], an open source cloud and by leveraging existing OpenStack functionalities."
pub.1109783960,PPK-Means: Achieving Privacy-Preserving Clustering Over Encrypted Multi-Dimensional Cloud Data,"Clustering is a fundamental and critical data mining branch that has been widely used in practical applications such as user purchase model analysis, image color segmentation, outlier detection, and so on. With the increasing popularity of cloud computing, more and more encrypted data are converging to cloud computing platforms for enjoying the revolutionary advantages of the cloud computing paradigm, as well as mitigating the deeply concerned data privacy issues. However, traditional data encryption makes existing clustering schemes no more effective, which greatly obstructs effective data utilization and frustrates the wide adoption of cloud computing. In this paper, we focus on solving the clustering problem over encrypted cloud data. In particular, we propose a privacy-preserving k-means clustering technology over encrypted multi-dimensional cloud data by leveraging the scalar-product-preserving encryption primitive, called PPK-means. The proposed technique is able to achieve efficient multi-dimensional data clustering as well to preserve the confidentiality of the outsourced cloud data. To the best of our knowledge, our work is the first to explore the privacy-preserving multi-dimensional data clustering in the cloud computing environment. Extensive experiments in simulation data-sets and real-life data-sets demonstrate that our proposed PPK-means is secure, efficient, and practical."
pub.1120098374,Ontology Based Information Creation Approach for Digital Twins: Early-Stage Findings,"Abstract
Digitalization and consistency of information about existing buildings, held by the owners, facility managers, local and national authorities, is a common issue for asset management or any other building life-cycle analysis or optimization. The digital twin concept could provide the solution; however, the creation of appropriate input data has been the main roadblock in adoption and application. The information needs for creation of digital twins of existing buildings are vast and as such none of the existing research approaches and/or algorithms are capable of holistic data collection to be used as input. A prevailing method for gathering geometrical data of existing assets is laser scanning, where a point cloud with high precision and high data volume is produced. The detection of geometrical and/or semantic information from the point cloud data has been recently a goal of many research initiatives; however they were usually focused on single purpose data extraction, thus limiting wider application. In the paper, an ontology-based approach of information creation is presented that minimizes human assistance, interaction with raw data and processing resources while increasing expressive power of the asset’s digital twin model for more efficient reasoning."
pub.1152348100,FDP Simulation Studies for Green Fields Cluster Development in Less than 30 Days Utilizing Cloud Technologies,"Abstract
                  BD Cluster green fields development located offshore Sabah, Malaysia, consists of three multi-stacked turbidite fields, namely A, B and C, encompassing thick and thin bed sands. Due to the lack of existing infrastructure in close proximity, a wellhead platform (WHP) will be installed on top of Field A. Fields B and C will be developed with a respective 8 and 7km subsea tie back to this WHP. Gas will be exported from the WHP to Facility-1 situated 5km away, whereas oil from a single thin oil rim reservoir in Field A will be exported to Facility-2 50km away.
                  The challenges faced by the Reservoir Engineering (RE) Team was delivering an extensive number of dynamic simulations while adhering to the Field Development Planning (FDP) submission deadline: 1) uncertainty analysis and probabilistic modelling for 9 models, 2) construction of coupled reservoir models 3) screening alternative oil and gas export routes, and 4) optimizing capex phasing by determining the optimum startup sequence of the fields. Delivering the FDP work on time with the limited software licenses and computing infrastructure available on-premise appeared to be a ""bridge too far"".
                  The limitations were addressed by PETRONAS LiveFDP digital transformation initiative commenced in 2019, through deployment of digital cloud technologies and solutions with scalable High-Performance Computing (HPC) environment. A total of 9 geological models were delivered to REs for dynamic simulation studies. Probabilistic modelling was then employed to obtain the dynamic P10, P50 and P90 models for each field. The Reservoir Coupling facility and Extended Network option were used in the numerical simulator to couple the standalone models in order to honor the overall facility constraints and incorporate the pipeline effects. Utilizing the coupled network model, multiple studies including condensate banking, determining optimum field sequencing and export route scenario were performed.
                  The FDP subsurface development simulation runs were completed within 1 month using HPC cloud solutions and workflows compared to 9 months if using on-premise infrastructure. It provided the necessary tools to allow the team: 1) accurately assess the impact of condensate banking on well productivity, 2) executed over 1200 cases for probabilistic modelling for the 9 models in 24 hours of simulation time, 3) reduced the number of wells derived from a previous study from 14 to 9 yielding a saving of ~US$115 million, 4) ~US$50 million savings as a result of capex phasing by optimizing the field start up sequence, and 5) US$130 million savings by establishing the lowest cost oil and gas export route scenario."
pub.1163835446,Impacts of Vehicle-to-Everything Enabled Applications: Literature Review of Existing Studies,"As communication technology is developing at a rapid pace, connected vehicles (CVs) can potentially enhance vehicle safety while reducing vehicle energy consumption and emissions via data sharing. Many researchers have attempted to quantify the impacts of such CV applications and vehicle-to-everything (V2X) communication, or the instant and accurate communication among vehicles, devices, pedestrians, infrastructure, network, cloud, and grid. Cellular V2X (C-V2X) has gained interest as an efficient method for this data sharing. In releases 14 and 15, C-V2X uses 4G LTE technology, and in release 16, it uses the latest 5G new radio (NR) technology. Among its benefits, C-V2X can function even with no network infrastructure coverage; in addition, C-V2X surpasses older technologies in terms of communication range, latency, and data rates. Highly efficient information interchange in a CV environment can provide timely data to enhance the transportation system's capacity, and it can support applications that improve vehicle safety and minimize negative impacts on the environment. Achieving the full benefits of CVs requires rigorous investigation into the effectiveness, strengths, and weaknesses of different CV applications. It also calls for deeper understanding of the communication protocols, results with different CV market penetration rates (MPRs), CV- and human-driven vehicle interactions, integration of multiple applications, and errors and latencies associated with data communication. This paper includes a review of existing literature on the safety, mobility, and environmental impacts of CV applications; gaps in current CV research; and recommended directions for future research. The results of this paper will help shape future research for CV applications to realize their full potential."
pub.1155491650,Preserving Resource Handiness and Exigency‐Based Migration Algorithm (PRH‐EM) for Energy Efficient Federated Cloud Management Systems,"On-demand computing ability and efficient service delivery are the major benefits of cloud systems. The limitation in resource availability in single data centers causes the extraction of additional resources from the cloud providers group. The federation scheme dynamically increases resource availability in response to service requests. The dynamic increase in resource count leads to excessive energy consumption, maximum cost, and carbon footprints emission. Hence, the reduction of resources is the major requirement to construct the optimized cloud source models for profit maximization without considering energy mix and CO2. This paper proposes the novel migration method to reduce carbon emissions and energy consumption. The initial stage in the proposed work is the categorization of data centers based on the MIPS and cost prior to job allocation offers scalable and efficient services and resources to the cloud user. Then, the job with the maximum size is allotted to the VM only if its capacity is less than the cumulative capacity of data centers. A novel migration based on overutilized and underutilized levels provides the services to the user even if the particular VM fails. The proposed work offers efficient maintenance of resource availability and maximizes the profit of the cloud providers associated with the federated cloud environment. The comparative analysis of the proposed algorithm with the existing methods regarding the response time, accuracy, profit, carbon emission, and energy consumption assures the effectiveness in a confederated cloud environment."
pub.1095428780,Self Adaptations in Access Control Models in Context of Cloud Computing,"Access control models are widely adopted by IT industry as they restrict access to the digital data and resources. While different access control models have been presented in literature, there is an issue which has not been investigated so far i.e. the need for self-adaptation in access control models in distributed computing environment. We have presented in-depth analysis, which demonstrates that the existing traditional and non-traditional access control models are unable to cope up with the challenges of dynamic adaptations in the system due to unprecedented changes which are not identified at requirement engineering time. Due to runtime changes either in the operational environment of a system or in user requirements, system changes its behaviour dynamically. Hence for a system that has been deployed in cloud computing environment where user requirements change dynamically, a useful access control model is the one that can handle the authorization requests along with the runtime changes in the system. This paper presents the analysis of various access control models and frameworks in order to demonstrate the need of self-adaptation in access control models particularly in the domain of cloud computing."
pub.1141525171,Research and Practice of Container System,"Container technology has been widely used in various real-world situations, like cloud platforms, CI/CD, and DevOps. By enabling a layered image system and OS-level virtualization, container technology can provide agile deployment and isolate execution environment for applications. However, existing container systems fail to support containers efficiently and securely. On the one hand, coarse-grained image management makes the deployment and update of applications time-consuming when the corresponding images need to be delivered in network. On the other hand, shared OS kernel may arise resource contention and security issues. This talk shows our research and practice of container systems. Specifically, I will introduce approaches of image management for fast container deployment, OS kernel isolation for secure and high-performance container execution environment, and container live migration for mitigating resource contention."
pub.1173419981,Leveraging Meta-Learning for Dynamic Anomaly Detection in Zero Trust Clouds,"In the rapidly evolving landscape of cloud computing, ensuring the security of data and services remains an imperative challenge. The Zero Trust framework, advocating continuous verification and access control, presents a pivotal paradigm to mitigate risks. This research introduces a pioneering approach named “DeepMetaGuard” for addressing dynamic anomaly detection within Zero Trust cloud environments. By amalgamating the Model-Agnostic Meta-Learning (MAML) and Variational Autoencoders (VAEs)–a Deep Anomaly Detection model, DeepMetaGuard stands as a promising innovation. DeepMetaGuard harnesses the potential of meta-learning through MAML, which expedites the model's adaptation to diverse cloud scenarios, thereby enhancing its adaptability to anomalous behaviours. Simultaneously, its integration with VAEs equips the model to identify anomalies across various cloud environments by acquiring generalized knowledge while accommodating distinct traits. To assess DeepMetaGuard's efficacy, a comprehensive simulation analysis is conducted, comparing its performance against existing anomaly detection algorithms. The evaluation encompasses a spectrum of simulation metrics, including Area Under Curve–Precision Recall Metric (AUC-PR), Detection Time, Precision-Recall Gain Curves, and Matthews Correlation Coefficient (MCC). AUC-PR gauges precision-recall trade-offs, Detection Time measures response speed, Precision-Recall Gain Curves visualize incremental performance gains, and MCC balances overall model performance. In this pioneering study, DeepMetaGuard emerges as a proficient contender in dynamic anomaly detection within Zero Trust cloud environments. The amalgamation of meta-learning and deep anomaly detection techniques, as evidenced through the comprehensive evaluation, underscores its potential in redefining cloud security. By introducing DeepMetaGuard and substantiating its effectiveness against established benchmarks, this research contributes to the advancement of cybersecurity strategies in the realm of cloud systems."
pub.1091990238,Global Registration of 3D LiDAR Point Clouds Based on Scene Features: Application to Structured Environments,"Acquiring 3D data with LiDAR systems involves scanning multiple scenes from different points of view. In actual systems, the ICP algorithm (Iterative Closest Point) is commonly used to register the acquired point clouds together to form a unique one. However, this method faces local minima issues and often needs a coarse initial alignment to converge to the optimum. This paper develops a new method for registration adapted to indoor environments and based on structure priors of such scenes. Our method works without odometric data or physical targets. The rotation and translation of the rigid transformation are computed separately, using, respectively, the Gaussian image of the point clouds and a correlation of histograms. To evaluate our algorithm on challenging registration cases, two datasets were acquired and are available for comparison with other methods online. The evaluation of our algorithm on four datasets against six existing methods shows that the proposed method is more robust against sampling and scene complexity. Moreover, the time performances enable a real-time implementation."
pub.1117705746,Бізнес-моделі хмарного надання ІТ-послуг,"Aim of the article. Application of cloud technologies extends possibilities of delivering the IT-services ""on demand"", makes cooperation between the participants of business more rapid, more flexible and more open. However, methods of successful organization of businesses on the basis of cloud technologies appear to be rather the result of certain business-initiatives than of the analysis of advantages and drawbacks of different business models with further reasonable choice of the most suitable model. So the systematization and classification of business models of modern cloud businesses, analysis of their strengths and weaknesses seems to be an actual task. Numerous publications considering the business models of cloud services mainly don't go beyond the levels of serviced IT-resources or methods of granting the access to them without reasoning the choice of both of them with taking into account target consumers, key resources, pricing models, service content etc. The purpose of the article is to investigate the typology of business models of cloud businesses and to reveal the terms of successful introduction of certain models. The results of the analyses. A business model is traditionally considered as a structural model of value creation and commercialization by the business system. Special features of cloud services determine the particularities of constructing the business models of cloud service providers. In all classifications of business models of cloud ecosystems two invariants are taken into account: method of deploying the IT-services (private, public, community and hybrid clouds) and level of the virtually (through a cloud) rendered services determined by virtually used IT-resource called the layer of IT-services (Infrastructure as a Service, IaaS; Platform as a Service, PaaS; Software as a Service, SaaS; Business Process as a Service, BPaaS). Evident additional business model elements are represented by pricing model (mainly pay per use or subscription) and value proposal which ranges in different typologies from the level of the virtually rendered services to the content of each special service (cloud storage, cloud service brokerage, cloud service integration, security as a service etc.). Each of the models has different market maturity and prospects of market growth, needs special resources for deployment and provides different advantages for users. To choose а model for cloud service providing we suggest comparing the provider resources and goals with success driving factors for each model. For instance, the SaaS level provides the easiest market entry, the best opportunities for innovations, the least need for venture capital and lots of successful business models, but the technology/vendor lock-in is very high and competition is very strong. This model provides lots of advantages for a consumer: scaling, robustness, economy and optimization of costs, reliability etc. As the model of service consumption, it is extremely attrac"
pub.1143073239,Agile Support Vector Machine for Energy-efficient Resource Allocation in IoT-oriented Cloud using PSO," Over the years cloud computing has seen significant evolution in terms of improvement in infrastructure and resource provisioning. However the continuous emergence of new applications such as the Internet of Things (IoTs) with thousands of users put a significant load on cloud infrastructure. Load balancing of resource allocation in cloud-oriented IoT is a critical factor that has a significant impact on the smooth operation of cloud services and customer satisfaction. Several load balancing strategies for cloud environment have been proposed in the past. However the existing approaches mostly consider only a few parameters and ignore many critical factors having a pivotal role in load balancing leading to less optimized resource allocation. Load balancing is a challenging problem and therefore the research community has recently focused towards employing machine learning-based metaheuristic approaches for load balancing in the cloud. In this paper we propose a metaheuristics-based scheme Data Format Classification using Support Vector Machine (DFC-SVM), to deal with the load balancing problem. The proposed scheme aims to reduce the online load balancing complexity by offline-based pre-classification of raw-data from diverse sources (such as IoT) into different formats e.g. text images media etc. SVM is utilized to classify “n” types of data formats featuring audio video text digital images and maps etc. A one-to-many classification approach has been developed so that data formats from the cloud are initially classified into their respective classes and assigned to virtual machines through the proposed modified version of Particle Swarm Optimization (PSO) which schedules the data of a particular class efficiently. The experimental results compared with the baselines have shown a significant improvement in the performance of the proposed approach. Overall an average of 94% classification accuracy is achieved along with 11.82% less energy 16% less response time and 16.08% fewer SLA violations are observed. "
pub.1094710615,Embracing the Cloud for Better Cyber Security,"The future of cyber security is inextricably tied to the future of computing. Organizational needs and economic factors will drive computing outcomes. Cyber security researchers and practitioners must recognize the path of computing evolution and position themselves to influence the process to incorporate security as an inherent property. The best way to predict future computing trends is to look at recent developments and their motivations. Organizations are moving towards outsourcing their data storage, computation, and even user desktop environments. This trend toward cloud computing has a direct impact on cyber security: rather than securing user machines, preventing malware access, and managing removable media, a cloud-based security scheme must focus on enabling secure communication with remote systems. This change in approach will have profound implications for cyber security research efforts. In this work, we highlight existing and emerging technologies and the limitations of cloud computing systems. We then discuss the cyber security efforts that would support these applications. Finally, we discuss the implications of these architectural changes, in particular with respect to malware and social engineering."
pub.1095337680,Extending Grids with Cloud Resource Management for Scientific Computing,"From its start using supercomputers, scientific computing constantly evolved to the next levels such as cluster computing, meta-computing, or computational Grids. Today, Cloud Computing is emerging as the paradigm for the next generation of large-scale scientific computing, eliminating the need of hosting expensive computing hardware. Scientists still have their Grid environments in place and can benefit from extending them by leased Cloud resources whenever needed. This paradigm shift opens new problems that need to be analyzed, such as integration of this new resource class into existing environments, applications on the resources and security. The virtualization overheads for deployment and starting of a virtual machine image are new factors which will need to be considered when choosing scheduling mechanisms. In this paper we investigate the usability of compute Clouds to extend a Grid workflow middleware and show on a real implementation that this can speed up executions of scientific workflows."
pub.1095660348,A Cloud HUB for Brokering Business Processes as a Service,"The management and coordination of crossenterprise collaboration is changing due to globalization, specialization and innovation; furthermore, it is changing also due to the adoption of service-oriented cloud computing (SOCC). SOCC provides means for value-chain automation that is supported by many service-oriented industry standards. In this new environment, enterprises face new challenges. Firstly, they need to semi-automatically find collaboration parties and learn about their identity, services, and reputation. Second, they need to source Business-Processes-as-a-Service (BPaaS) from these newly found third parties to provide them service value that they do not create in-house due to lack of resources or know-how. The contribution of this paper is twofold. First, it proposes a BPaaS- HUB architecture that enables speedy business-partner discovery and support for on-the-fly background checking. The architecture addresses the domain-independent requirements of the functionality, data-exchange protocol, and system behavior. Second, it indicates how to populate the architecture with existing services, thus, illustrating a rapid domain-specific instantiation of the BPaaS-HUB. The existence of BPaaS- HUB-type of services as “rendezvous locations” is required for the broad adoption of cross-enterprise collaboration (CeC) in a SOCC environments."
pub.1154730086,ВИКОРИСТАННЯ СУЧАСНИХ ДЕЦЕНТРАЛІЗОВАНИХ ТЕХНОЛОГІЙ ДЛЯ РОЗМЕЖУВАННЯ ДОСТУПУ В ХМАРНОМУ СЕРЕДОВИЩІ,"The work solves the actual scientific and technical problem of developing a method of demarcating access to cloud services using dynamically generated filtering rules for virtual firewalls. The model proposed in the work takes into account the dynamic nature of the allocation of allocated resources and the characteristics of network interaction protocols. The input of the model receives a stream of network packets that are sent to the firewall of the protection system in the cloud environment in real time. The model divides packets into virtual connections, and defines subsets of filtering rules for all information connections that allow filtering network interaction to comply with access policie. The integration of access control functions into the components of the cloud environment reduces its performance, provided that the firewalls that control information interaction use the hardware resources of the regular hypervisor. The virtual connection classification algorithm proposed in the work uses the existing technologies of parallel computing and the structure of the TCP/IP stack, and is implemented using the Netgraph network subsystem. This makes it possible to increase the performance of firewalls and more efficiently use the computing power of existing hardware platforms. This reduces the cost of access delimitation tools in the cloud environment. The developed algorithms and method expand the possibilities of using the technology of inter-network shielding. The interaction of virtual machines within the framework of one hypervisor is carried out without the use of physical communication lines and is ensured by a software method, for example through the use of smart contracts."
pub.1168614227,The synthesis method for specifications and requirements in the process of it project reengineering,"In this study, the aim is to create and improve a methodology for synthesizing requirements and specifications for the re-engineering of IT projects with maximum efficiency and business orientation. The main task is to adapt outdated IT systems to the changing technical environment, in particular to cloud technologies and security system requirements. To achieve these goals, the proposed methodology uses the analysis of archaic systems, the reverse engineering method, expert surveys, documentation analysis, and black-box modeling. The application of these methods allows for the identification and revision of requirements and specifications, ensuring a high level of quality and efficiency in the process of re-engineering IT projects. The article further discusses the practical aspects of applying the methodology, prospects for further development, and the peculiarities of using various statistical methods in the process of improving re-engineering results. The operating principles of the method are described along with the main approaches and techniques that promote the analysis of existing IT systems, the synthesis of requirements and specifications, quality control, and successful project implementation. The individual components of the method include the collection of data about the existing system and the analysis of archaic systems to restore the definition of requirements. The use of the black-box model for testing the developed system is discussed, including the analysis of the obtained results, correction of requirements, and improvement of specifications. The methodology includes documentation analysis tools, reverse engineering, surveys and data visualization tools, as well as analytical techniques such as a formula for parallel testing, a formula for requirement traceability matrix, and a formula for forecasting requirements based on discrepancy rate analysis. As a result of implementing the IT project reengineering method, successful transition from old to new technologies can be achieved, the IT industry can be optimized, and conditions can be created for adaptation to modern technical environments, ensuring stability and reliability of the implemented reengineering projects. Based on the analysis of modern sources, previous experience, and conducted research, it can be asserted that the method for synthesizing specifications and requirements in the process of reengineering IT projects is of great importance and relevance for the modern development of information technology and business processes."
pub.1004911524,A hybrid solution for privacy preserving medical data sharing in the cloud environment,"Storing and sharing of medical data in the cloud environment, where computing resources including storage is provided by a third party service provider, raise serious concern of individual privacy for the adoption of cloud computing technologies. Existing privacy protection researches can be classified into three categories, i.e., privacy by policy, privacy by statistics, and privacy by cryptography. However, the privacy concerns and data utilization requirements on different parts of the medical data may be quite different. The solution for medical dataset sharing in the cloud should support multiple data accessing paradigms with different privacy strengths. The statistics or cryptography technology alone cannot enforce the multiple privacy demands, which blocks their application in the real-world cloud. This paper proposes a practical solution for privacy preserving medical record sharing for cloud computing. Based on the classification of the attributes of medical records, we use vertical partition of medical dataset to achieve the consideration of different parts of medical data with different privacy concerns. It mainly includes four components, i.e., (1) vertical data partition for medical data publishing, (2) data merging for medical dataset accessing, (3) integrity checking, and (4) hybrid search across plaintext and ciphertext, where the statistical analysis and cryptography are innovatively combined together to provide multiple paradigms of balance between medical data utilization and privacy protection. A prototype system for the large scale medical data access and sharing is implemented. Extensive experiments show the effectiveness of our proposed solution."
pub.1154202059,"Interactive, Cloud-Native Workflows on HPC Using KNoC","Cloud and HPC platforms differentiate by many aspects, but both can run applications in identical contexts using containers. In this paper we present KNoC, an open-source virtual node (kubelet) for Kubernetes that transparently manages the container lifecycle on a remote HPC cluster using Slurm and Singularity. Our goal is on one hand to allow HPC users to leverage existing cloud-native tools, such as the popular Argo Workflows language to express complex data-processing pipelines, while on the other hand enabling Cloud setups to exploit computing resources available in HPC centers. KNoC bridges Cloud and HPC, transforming Argo to a cross-environment, portable solution, which allows the combination of Cloud-based tools and HPC steps into the same workflow, controlled and monitored through an interactive frontend. Deploying KNoC requires only a secure shell connection to the cluster’s login node. We describe the design and implementation of KNoC, and evaluate the integration using several proof-of-concept workflows."
pub.1173871023,"A Journey into Security Certification, From the Cloud to Artificial Intelligence","This book, authored by some of the pioneers in the security certification domain, provides a foundational knowledge base for designing and deploying effective security certification schemes for cloud-edge distributed systems. It gives readers unique and essential insight into the history of security certification and its evolution from static to dynamic models. The book helps readers to understand the importance of security certification across distributed system evolution from cloud-based to artificial intelligence-machine learning systems. It covers in full the certification of cloud-based composite services, discussing the role of multi-factor certification. It also emphasizes processes for continuously certifying services across system and context changes, as well as the certification of the deployment environment and development process and their impact on the overall cloud service certification. The book considers and challenges existing security certification schemes for cloud services, discussing issues and gaps when applied to modern systems in the cloud-edge continuum and built on artificial intelligence. Readers will become familiar with the challenges in certifying cloud-edge distributed systems and machine learning-based systems. This book also provides actionable insight to readers who manage modern security systems. Security controls have grown in size and complexity, requiring reliable and sustainable certification schemes to support varying quality standards and produce compliant products. It covers the logic behind security standards and the evaluation criteria of main certification standards, further discussing the evolution and corresponding certification of distributed systems towards cloud, cloud-edge, and artificial intelligence. The book teaches readers how to build faster, scalable, and more reliable cloud-edge distributed systems and beyond."
pub.1153134744,Mitigating Yo-Yo attacks on cloud auto-scaling,"In recent years, global businesses have witnessed a significant cloud adoption that provides considerable value compared to traditional data centers, achieving greater scalability, cost efficiency, and improved performance. Cloud auto-scaling is a cloud service feature that copes with variations in the workload by spinning up or down instances on the fly. Attackers may exploit auto-scaling mechanisms to transform the traditional DDoS attacks into Economic Denial of Sustainability attacks (EDoS). In this perspective, a new type of attack, called Yo-Yo attack, has been recently reported in the literature where the attackers send a burst of traffic periodically to oscillate the auto-scaling system between scale-out and scale-in status inducing economic loss to the tenant. These new types of attacks cause are harder to detect compared with traditional DDoS, and they require fewer resources from the attacker. In this paper, we present a simple solution that is capable of detecting a Yo-Yo attack and mitigating it. In this quest, a legacy approach named Trust-based Adversarial Scanner Delaying (TASD) [1] is implemented and tested in a cloud production environment. The TASD method assigns a trust value number as a Quality of Service (QoS) value to each user. The original TASD system used an Additive Decrease method to update the trust value. Inspired by the TCP rate control mechanisms, we introduce two variants of TASD, ADAI (Additive Decrease/Additive Increase) and MDAI (Multiplicative Decrease/Additive Increase). The devised TASD approaches are deployed on Amazon Web Services (AWS). The experiment evaluations show that our proposed TASD variants can efficiently detect and mitigate Yo-Yo attacks in an actual cloud application."
pub.1169803054,The integration strategy of information system based on artificial intelligence big data technology in metaverse environment,"The concept of the meta-universe is still in its early stages, but many leading tech companies have invested heavily in research and development for this technology. The development of meta-smart cities is a significant trend. In the meta-universe environment, integrating information systems is crucial for analyzing AI big data. Establishing an integrated platform for medical information systems is key to advancing information technology. In the context of the meta-universe, creating an efficient and unified integration platform to eliminate medical information silos and reduce system integration costs has become a pressing issue in medical informatization. This paper proposes a medical information system integration method based on an integration platform and utilizing cloud computing technology as a data center. The core business layer uses the integration software “Ensemble” as the integration platform. The underlying data center employs a Hadoop storage cluster with distributed data storage and parallel computing technology, and the existing scheduling algorithm is studied and analyzed to enhance the resource scheduling algorithm for medical small file data. The effectiveness of the algorithm is simulated and verified on an experimental platform, demonstrating improved efficiency in resource scheduling."
pub.1172268921,Computer Big Data Analysis and Cloud Computer Networking Technology in Marketing,"Under the background of high-speed development of network information technology, the type of computer data storage tends to be diversified, and the amount of storage has increased significantly, which puts a higher demand on data storage and processing efficiency. In recent years, with the rise of digital technologies such as cloud computing, edge computing, 5G, and artificial intelligence, various industries have begun to accelerate the digitalization process. Energy and other digital technologies have emerged, various industries have begun to accelerate the digitalization process, and the global the real economy worldwide has invested in digital transformation. Marketing environment has changed from a single way of marketing to digital marketing, only to continue to improve the “online” and “offline” degree of integration and digitalization, to be able to occupy a place in the marketing market, and in practice, into the marketing management of digital marketing. Marketing market to occupy a place, and in practice, to promote the marketing management of digitalization is to promote the upgrading of the marketing system. In practice, promoting the digitalization of marketing management is a key move to boost the upgrading of the marketing system. Therefore, by combining the understanding of AI marketing in existing literature, this paper discusses the application of AI in the field of marketing from three aspects: the true meaning of AI marketing, its application, and users’ reaction to AI, so as to provide reference for the application of AI in the field of marketing."
pub.1061395543,Human mobility models for opportunistic networks,"Mobile ad hoc networks enable communications between clouds of mobile devices without the need for a preexisting infrastructure. One of their most interesting evolutions are opportunistic networks, whose goal is to also enable communication in disconnected environments, where the general absence of an end-to-end path between the sender and the receiver impairs communication when legacy MANET networking protocols are used. The key idea of OppNets is that the mobility of nodes helps the delivery of messages, because it may connect, asynchronously in time, otherwise disconnected subnetworks. This is especially true for networks whose nodes are mobile devices (e.g., smartphones and tablets) carried by human users, which is the typical OppNets scenario. In such a network where the movements of the communicating devices mirror those of their owners, finding a route between two disconnected devices implies uncovering habits in human movements and patterns in their connectivity (frequencies of meetings, average duration of a contact, etc.), and exploiting them to predict future encounters. Therefore, there is a challenge in studying human mobility, specifically in its application to OppNets research. In this article we review the state of the art in the field of human mobility analysis and present a survey of mobility models. We start by reviewing the most considerable findings regarding the nature of human movements, which we classify along the spatial, temporal, and social dimensions of mobility. We discuss the shortcomings of the existing knowledge about human movements and extend it with the notion of predictability and patterns. We then survey existing approaches to mobility modeling and fit them into a taxonomy that provides the basis for a discussion on open problems and further directions for research on modeling human mobility."
pub.1023290172,Storing Long-Lived Concurrent Schema and Data Versions in Relational Databases,"Although there is a strong focus on NoSQL databases for cloud computing environments, traditional relational data bases are still an integral part of many computing services in the cloud. Two significant issues in managing a relational database in a cloud environment are handling the inevitable evolution of the database schema and managing changes to system configuration and other data stored in the database as the system evolves over time. Techniques for handling these issues in on-premise databases are much less feasible in cloud computing environments, which demand efficiency, elasticity, and scalability. We propose a versioning system that can be used in relational databases to allow new versions of the database schema and data to be maintained within the same database as the production data. Past research on versioning either handles data versioning but not schema changes, or handles both but is focused on OLAP or XML databases. In this paper, we describe a mechanism for storing concurrent versions of data in an OLTP database. We explore two different implementation alternatives for versioned data storage and evaluate their relative merits given different workloads. We provide a concrete description of how this can be implemented within the InnoDB storage engine, which is the default data store for MySQL databases, and we present a quantitative comparison of the two implementations in InnoDB."
pub.1172913599,An Optimal Novel Approach for Dynamic Energy-Efficient Task Offloading in Mobile Edge-Cloud Computing Networks,"The rapid evolution of mobile devices has greatly advanced secure medical image transmission, yet challenges persist due to resource limitations and security concerns inherent to these devices. In response, this paper introduces a Dynamic Energy-Efficient Offloading Algorithm (DEEO), seamlessly integrated into the Mobile Edge-Cloud Computing (MECC) environment. DEEO empowers mobile devices to efficiently offload computationally intensive secure image transmission tasks to the nearest edge server or fog access point. This integration optimizes resource utilization, minimizes energy consumption, and ensures the confidentiality and integrity of sensitive medical image data. Through rigorous evaluations and comparative analysis, our approach demonstrates clear superiority over existing solutions. This integrated framework is poised to significantly enhance healthcare applications, offering heightened efficiency, elevated security, and an overall improved user experience."
pub.1109785762,Magnum opus of an efficient hospitality technique for load balancing in cloud environment,"Summary The load balancing approach in the cloud environment is an open issue in the recent days. It helps obtain better resource utilization in cloud environment. It also improves the performance of the system. A good load balancing algorithm in cloud ensures no under loading or over loading in a single host. This paper proposes an algorithm named Hospitality Load Balancing (HLB) algorithm, which aims to balance the load among virtual machines (VM) for increasing the performance and throughput. The proposed algorithm handles preemptive or nonpreemptive tasks, which may be independent. It helps to reduce make span and obtain low task migration. The HLB algorithm has been analyzed and compared with plenty of load balancing technique. An experimental result shows that it is an excellent and effective algorithm when compared with the existing load balancing algorithm. It also illustrates the performance of proposed load balancing scheme with analysis and proves that it attains low makespan and low task migration."
pub.1144793354,BULWARK: A Framework to Store IoT Data in User Accounts,"The explosive growth of the Internet of Things (IoT) devices raises serious concerns for a user’s privacy and security because the existing software framework on these devices often support various default features and generate large data sets. Moreover, many IoT devices incorporate a manufacturer-owned cloud-based back-end support to process and store the generated data while simultaneously sharing with third parties. Clearly, in such an industry-driven environment with the desire to use the IoT data as a revenue stream, it is a challenge for users to control IoT data. Device manufacturers utilize an opaque software design where user data is generated and stored with little transparency. Manufacturers use EULAs as a legal construct to protect a manufacturer’s legal standing and to explain a device’s behavior, however this explanation is vague and lacks the necessary details for a user to determine a device’s acceptable use and it has become increasingly difficult for users to secure and maintain their data. Fortunately, as the privacy minded user base of IoT devices grows, the manufacturers will be forced to implement a new framework that can enable users to have more control on the creation of their IoT data, and to store/disseminate such data in a secure and private manner. In this paper, we address this lack of transparency from manufacturers and address the issues of privacy and security by proposing a new framework called Bulwark, for manufacturer use on IoT devices and mobile applications. Proposed framework enables the user to generate and manage a set of data controlling rules, and store the result in their personal cloud account, while providing a dashboard data reporting tool enabling data transparency and supporting good user choices. The user’s ability to access, disseminate and secure IoT generated data, is now available within our proposed framework. Using reverse engineering, simulation and implementation of open source solutions, we demonstrate support for a set of common devices. Each device executed the framework, while communicating with a mobile application and cloud services. Rules were generated for each message and telemetry was returned to the mobile application for dashboard rendering. We stored generated data in the cloud using our own account, while maintaining the free tier for each of the cloud services. Network usage increased between 4% and 9% while storage size grew between 0% and 2% larger, as compared to using the device without the framework. Our framework demonstrates support for a multitude of devices, by either open source or support for similar feature sets. This framework is easy to integrate and we anticipate wide spread adoption."
pub.1129582760,Magellan,"Entity matching (EM) finds data instances that refer to the same real-world entity. In 2015, we started the Magellan project at UW-Madison, jointly with industrial partners, to build EM systems. Most current EM systems are stand-alone monoliths. In contrast, Magellan borrows ideas from the field of data science (DS), to build a new kind of EM systems, which is ecosystems of interoperable tools for multiple execution environments, such as on-premise, cloud, and mobile. This paper describes Magellan, focusing on the system aspects. We argue why EM can be viewed as a special class of DS problems and thus can benefit from system building ideas in DS. We discuss how these ideas have been adapted to build <code>PyMatcher</code> and <code>CloudMatcher</code>, sophisticated on-premise tools for power users and self-service cloud tools for lay users. These tools exploit techniques from the fields of machine learning, big data scaling, efficient user interaction, databases, and cloud systems. They have been successfully used in 13 companies and domain science groups, have been pushed into production for many customers, and are being commercialized. We discuss the lessons learned and explore applying the Magellan template to other tasks in data exploration, cleaning, and integration."
pub.1061755169,AppBooster: Boosting the Performance of Interactive Mobile Applications with Computation Offloading and Parameter Tuning,"Interactive mobile applications attract lots of attentions recently. They utilize complex algorithms (e.g., machine learning) to provide advanced functions (e.g., object recognition), thus lead to long response time while running on mobile devices. To reduce the response time, researchers propose offloading some compute-intensive parts of mobile applications onto cloud. Existing works aim to optimize general performance (e.g., response time), but ignore the enhancement of application quality (e.g., recognition accuracy), which is also critical to user experience. In this paper, we develop AppBooster, a mobile cloud platform which boosts both general performance and application quality for interactive mobile applications. AppBooster jointly leverages the quality adaptation, computation offloading and parallel speedup to boost the comprehensive performance, which is defined by developers based on the metrics of application quality and general performance. Through combining history-based platform-learned knowledge, developer-provided information and the platform-monitored environment conditions (e.g., workload, network), AppBooster manages applications with optimal computation partitioning scheme and tunable parameter setting thus obtain high comprehensive performance. We evaluate AppBooster with an object recognition application in various network conditions and show AppBooster can significantly boost application performance and obtain 1.3 to 3.5 times better performance than existing strategies."
pub.1167930228,Hypervisor-Level Ransomware Detection in Cloud Using Machine Learning,"Ransomware attack incidences have been on the rise for a few years. The attacks have evolved over the years. The severity of these attacks has only increased in the cloud era. This article discusses the evolution of ransomware attacks targeting cloud storage and explores existing ransomware detection solutions. It also presents a methodology for generating a dataset for detecting ransomware in the cloud and discusses the results, including feature selection and normalization. The article proposes a system for detecting attacks in virtualized environments using machine learning models and evaluates the performance of different classification models. The proposed system is shown to have high accuracy of 96.6% in detecting ransomware attacks in virtualized environments at the hypervisor level."
pub.1149653341,CSO-ILB: chicken swarm optimized inter-cloud load balancer for elastic containerized multi-cloud environment,"The dynamic nature of the cloud environment increases the complexity of managing its resources and the distribution of user workload between the available containers in the data center. However, the workload must be balanced to improve the cloud system’s overall performance. Generally, most of the existing load balancing techniques suffer from performance degradation due to the communication overheads among the containers. Moreover, less attention is given to stabilize the load in a multi-cloud environment. Therefore, to overcome this problem, there is a need to develop an elastic load balancing method to improve the performance of cloud systems. This paper proposed an autonomic CSO-ILB load balancer to ensure the elasticity of the cloud system and balance the user workload among the available containers in a multi-cloud environment. The concept of multi-loop has been utilized in our approach to enabling efficient self-management before load balancing. The tasks are scheduled to the containers using an extended scheduling algorithm called Deadline-Constrained Make-span Minimization for Multi-Task Scheduling (DCMM-MTS). Based on the task scheduling, the load in each container is computed and then balanced using the proposed load balancer algorithm CSO-ILB. The proposed approach is evaluated in the Container CloudSim platform, and the performance is compared with the existing meta-heuristic algorithms such as Ant Colony Optimization, Bee Colony Optimization, Shuffled Frog Leaping Algorithm and Cat Swarm Optimization (CSO). The simulations proved that the proposed approach outperformed the other approaches in terms of reliability, CPU utilization, make-span, energy utilization, response time, execution cost, idle time, and task migration."
pub.1164148169,Ganos Aero: A Cloud-Native System for Big Raster Data Management and Processing,"The development of Earth Observation technology contributes to the production of massive raster data. It is vital to manage and conduct analytical tasks on the raster data. Existing solutions employ dedicated systems for the raster data management and processing, respectively, incurring problems such as data redundancy, difficulty in updating, expensive data transferring and transformation, etc. To cope with these limitations, this demonstration presents Ganos Aero, a cloud-native system for big raster data management and processing. Ganos Aero proposes a unified raster data model for both the data management and processing, which stores a single copy of the raster data and without performing an expensive tiling procedure, and thus achieves significant improvement in the storage and updating efficiency. To enable efficient query and batch task processing, Ganos Aero implements an on-the-fly tile production mechanism, and optimizes its performance using the cloud features including decoupling compute from storage and pushing costly operations closer to the storage layer. Since deployed in Alibaba Cloud in 2022, Ganos Aero has been playing a critical role in many real applications including the modern agriculture, environment monitoring and protection, et al."
pub.1095534473,Reinforcement Learning Based Service Provisioning for a Greener Cloud,"Cloud computing is an emerging distributed computing model consisting of massive datacenters for making different services available to the users. In the current scenario where energy consumption and wastage in the IT field is looked upon with growing apprehension, Green Computing encourages the design of energy-efficient computing approaches that can be applied to cloud computing to address and reduce the factors which influence power consumption alias energy cost. Other evolving technologies like Virtualization and VM (Virtual Machine) migration technologies are employed widely for energy efficient consolidation of resources. The existing work on green cloud service provisioning aids energy aware cloud service provisioning by incorporating the Trigger Engine Agent which uses the static pre-processed information of service usage to initiate live VM migration. This paper proposes to take the dynamic environment into consideration to substantiate the decisions made by the existing model and incorporate learning agents into the model. Our model comprises of two parts: a dynamic Post-Processing Agent and a Learning Agent. The Post-Processing Agent verifies the migration decisions made by the Trigger Engine and corrects them if they do not conform to the energy-aware provisioning approach. The significant portion of this work is the Learning Agent, which will learn the optimal policy to follow in the current environment by incorporating the actions of the Post-Processing agent into the preprocessed data of the existing system using Q-Iearning methodology."
pub.1103851791,A Cloud-IoT Model for Reconfigurable Radio Sensing: The Radio.Sense Platform,"In this paper we elaborate on the challenges that emerge when designing open IoT models and methods to enable passive “radio vision” functions within a cloud Platform-as-a-Service (PaaS) environment. Radio vision allows to passively detect and track any moving/fixed object or people, by using radio waves as probe signals that encode a 2D/3D view of the environment they propagate through. View reconstruction from the received radio signals is based on data analytic tools, that combine multiple radio measurements from possibly heterogeneous IoT networks. The goal of the study is to define the baseline specifications that are necessary to integrate this new technology into a cloud-IoT architecture. Following emerging semantic interoperability concepts, we propose an expressive ontology model to represent the radio vision concept and allow for interoperability with other systems. For accelerated integration of radio vision functions the open Radio. Sense platform is designed as compliant with existing models (oneM2M based ontologies)."
pub.1143042742,BeFaaS: An Application-Centric Benchmarking Framework for FaaS Platforms,"Following the increasing interest and adoption of FaaS systems, benchmarking frameworks for determining nonfunctional properties have also emerged. While existing (microbenchmark) frameworks only evaluate single aspects of FaaS platforms, a more holistic, application-driven approach is still missing. In this paper, we design and present BeFaaS, an extensible application-centric benchmarking framework for FaaS environments that focuses on the evaluation of FaaS platforms through realistic and typical examples of FaaS applications. BeFaaS includes a built-in e-commerce benchmark, is extensible for new workload profiles and new platforms, supports federated benchmark runs in which the benchmark application is distributed over multiple providers, and supports a fine-grained result analysis. Our evaluation compares three major FaaS providers in single cloud provider setups and shows that BeFaaS is capable of running each benchmark automatically with minimal configuration effort and providing detailed insights for each interaction."
pub.1000822393,Active ITS Infrastructure Management Strategy for Enhanced ITS Service,"In this study, we analyzed the next generation ITS (C-ITS) technology trends, focusing on the national and international C-ITS projects. Based on the promotion practices of developed countries, we pointed out the lack of linkages with the existing ITS infrastructure. As a way to overcome this problem, we proposed the three-direction to enable the existing ITS infrastructure corresponding to the C-ITS. First one is developing a technique to improve the performance of the existing ITS infrastructure and automate the performance management (Performance-enhanced ITS). Second, developing active sensors or fusion sensor which along with V2X communication technology implement of an active safety driving support system (Safety-enhanced ITS). Third, we need to develop a technology that generate the new advanced traffic data by integrating the collected data from existing ITS infrastructure and nomadic device (Cloud-ITS). By improving the function of the existing ITS infrastructure for adaptation to the new V2X communication environment, we enhanced the efficiency of maintenance performance and would maximize the benefit of the introduction of C-ITS."
pub.1168883739,GIS-LIKE ENVIRONMENTS AND HBIM INTEGRATION FOR ANCIENT VILLAGES MANAGEMENT AND DISSEMINATION,"Abstract. Over the past two decades, we have witnessed significant progress in the digitization of artistic and cultural heritage, which was initially aimed at conservation and analysis but has since expanded its scope to emphasize the dissemination and knowledge of artworks and sites of historical, archaeological, and architectural interest. For ancient villages management and dissemination, there are numerous existing tools, among these Building Information Model (BIM), Geographic Information System (GIS), virtual reality, augmented reality, and mixed reality apps and online platforms. These design and analysis systems have, however, interoperability and integration problems. In attempt to solve these problems, in this context, the research activity involves the creation of a 3D model, and HBIM of Precacore complex starting from the points cloud obtained by images acquired by DJI mini 3 pro. The procedure for importing the 3D HBIM model in a GIS environment using Autodesk Infraworks software will be described, followed by the creation of the Virtual/Augmented/Mixed reality app using the Unity 3D platform and Microsoft Hololens for the purpose of attempting to solve the problems relating the different data format deriving from the use of the commercial software from a virtual point of view. Finally, the methodology for the creation of a hybrid and experimental system will be proposed that, by exploiting the Cesium ion platform, allows the possible (although not complete) effective integration of the multi-scale information contained in both software used for the modeling HBIM and GIS."
pub.1093992059,Seamless Context-aware Voice Service in the Cloud for Heterogeneous Network Environment,"The full coverage of continuous voice cloud services is still an open issue in an overlapping network, particularly with regard to heterogeneously public communication network infra-structures. However, hardly anyone of today's service providers offers seamless voice call continuity across these networks due to the high technical integration effort and economic cost. Given this backdrop, we present a feasible Virtual Room based Vertical Handoff (VOOH) solution. In contrast to its counterparts, VOOH is a network operator independent solution that only uses the today's existing wireless access technologies for connectivity purposes. The advantage of VOOH is that, e.g., mobile service providers can immediately provide advantage of seamless voice cloud service without any modifications in the underlying network layers. Besides illustrating the system architecture, we elaborate the operational details of the proposed solution and also analyze its performance based on real implementation over a public network infrastructure. The experimental results demonstrate that the handoff performance of the VOOH solution is not only competitive, but also less expensive compared to similar approaches. Furthermore, a Markov Chain model has been developed to show the effect of an enhanced VOOH solution on the handoff performance as QoS for different traffic loads."
pub.1168957423,Blockchain-based cloud-fog collaborative smart home authentication scheme,"With the rapid development of IoT technology, smart homes have emerged. At the same time, data security and privacy protection are also of great concern. However, the traditional centralized authentication scheme has defects such as single point of failure, poor scalability, center dependence, vulnerability to attacks, etc., and is not suitable for the distributed and dynamically changing smart home environment. Thus, many researchers have proposed decentralized authentication schemes based on blockchain technology. Although many characteristics of blockchain technology such as decentralization, non-tampering, and solving single point of failure have good application scenarios in authentication, the mature integration of the two applications has to be further explored. For example, the introduction of blockchain also brings security issues; the balance between security and performance in most blockchain-based authentication schemes remains to be investigated; and resource-constrained IoT devices tend to perform a large number of intensive computations, which is clearly inappropriate. Consequently, this paper introduces fog computing in blockchain-based authentication schemes, proposes a network architecture in which cloud and fog computing work together, and investigates the security and performance issues of authentication schemes under this architecture. Finally, formal and informal security analysis show that our scheme has multiple security properties, and our scheme has better performance than existing solutions."
pub.1092576192,Authentication Framework for Cloud Machine Deletion,"Today Digital Investigation on the cloud platform is a challenging task. As cloud follows notion of pay as you demand people are attracted to adopting this technology. But an unclear understanding of control, trust and transparency are the challenges behind the less adoption by most companies. Investigation in the cloud platform is hard to collect the strong evidences because resource allocation, dynamic network policy are facilitated on demand request fulfillment. This is why, it is difficult to perform forensic analysis in such a virtualized environment because the state of the system changes frequently. Even to prevent the deletion of system on cloud is a tough task. This paper will cover all the theoretical concepts of cloud along with the challenges presented in NIST guidelines. Through this paper, we explore the existing frameworks, loopholes, and suggest some possible solutions that will be a roadmap for forensic analysis in future."
pub.1016874526,Home-Appliance Control using Mobile Cloud Technology in Web2.0 Platform,"Cloud computing has been evolved as a key computing platform for sharing resources and services. People should have a relatively convenient environment for handling Home-appliances. Existing Home-Appliance control systems are not providing complete control over Home-Appliances and also difficult to control from distant places. This project proposes a Framework, which is composed of mobile users, Home-appliances and the cloud environment. Mobile that the user is going to use should contain Internet facility. A mobile user can use a smart phone with internet connection to control and handle Home-appliances through Web2.0 Blog-based interfaces in Web2.0 Platform. Mobile User can control the Home-appliances, using the Device Profile of Web Services in the cloud environment and can control completely by not only switching on and off but also can change settings of the devices and also from any far places."
pub.1135752629,BeFaaS: An Application-Centric Benchmarking Framework for FaaS Platforms,"Following the increasing interest and adoption of FaaS systems, benchmarking
frameworks for determining non-functional properties have also emerged. While
existing (microbenchmark) frameworks only evaluate single aspects of FaaS
platforms, a more holistic, application-driven approach is still missing. In
this paper, we design and present BeFaaS, an extensible application-centric
benchmarking framework for FaaS environments that focuses on the evaluation of
FaaS platforms through realistic and typical examples of FaaS applications.
BeFaaS includes a built-in e-commerce benchmark, is extensible for new workload
profiles and new platforms, supports federated benchmark runs in which the
benchmark application is distributed over multiple providers, and supports a
fine-grained result analysis. Our evaluation compares three major FaaS
providers in single cloud provider setups and shows that BeFaaS is capable of
running each benchmark automatically with minimal configuration effort and
providing detailed insights for each interaction."
pub.1133882818,Hybrid Approach to HPC Cluster Telemetry and Hardware Log Analytics,"The number of computer processing nodes and processor cores in cluster systems is growing rapidly. Discovering, and reacting to, a hardware or environmental issue in a timely manner enables proper fault isolation, improves quality of service, and improves system up-time. In the case of performance impacts and node outages, RAS policies can direct actions such as job quiescence or migration. Additionally, power consumption, thermal information, and utilization metrics can be used to provide cluster energy and cooling efficiency improvements as well as optimized job placement. This paper describes a highly scalable telemetry architecture that allows event aggregation, application of RAS policies, and provides the ability for cluster control system feedback. The architecture advances existing approaches by including both programmable policies, which are applied as events stream through the hierarchical network to persistence storage, and treatment of sensor telemetry in an extensible framework. This implementation has proven robust and is in use in both cloud and HPC environments including the Summit system of 4,608 nodes at Oak Ridge National Laboratory [5]."
pub.1146476343,The Behaviors of Intraseasonal Cloud Organization During DYNAMO/AMIE,"Abstract This study investigates the organization of tropical convection associated with the Madden‐Julian Oscillation (MJO) during the Dynamics of the MJO/Atmospheric Radiation Measurement MJO Investigation Experiment field campaign. While it is known that tropical clouds can organize and impact the large‐scale environment, how this occurs, and its underlying mechanism are not fully understood. Application of several existing cloud organization indices showed inconsistent evolutions in the measured degree of organization with the MJO. The inconsistency arises from the varying definitions and assumptions of cloud organization behaviors that are applied to each index. While these indices often combine different properties of clouds, such as their number density, size, and distance between them, the analysis of these properties separately provided further understanding of how clouds organize with the MJO. Using the rainfall clusters identified from the S‐Polka radar, we find that deep convective rainfall clusters begin to increase their number density before the arrival of MJO enhanced convective center, which is accompanied by increased proximity (shorter distance to each other) and followed by growth in their size. However, the nonrandomness in the spatial distribution of rainfall clusters maximizes as MJO convection decays. Deep convective clusters become the least randomly distributed as the clusters decay because of the suppression and decay of isolated deep convective cells, while clustered deep convective cells exist longer. This evolution of cloud organization is analogous to mesoscale convective systems, indicating that the duration and frequency of their organization stages are altered by the large‐scale environmental perturbations associated with the MJO.
Plain Language Summary Tropical cumulus clouds often tend to form close to one another and merge into larger cloud formations with a horizontal dimension greater than 100 km, a phenomenon known as “cloud organization.” While we know that cloud organization plays an important role by affecting the surrounding atmosphere through altering humidity and temperature, we do not fully understand why and how clouds organize. This study specifically examines how cloud organization varies on the intraseasonal timescale (30–90 days) that is specifically important to improving long‐range prediction skills in the tropics and globally. One finding of this study indicates that we do not have a universally agreed definition of cloud organization that can be used to quantify it, therefore we conduct further analysis to understand how cloud organization occurs. We find that the increased environmental humidity by the MJO first increases the number of cumulus clouds, which naturally leads to their higher proximity, followed by their mergers and growth in size. As the environment dries, scattered and isolated cumulus clouds weaken preferentially, while clustered cumulus clouds live longer, leading to the mo"
pub.1182027495,An efficient deep reinforcement learning based task scheduler in cloud-fog environment,"Efficient task scheduling in cloud and fog computing environments remains a significant challenge due to the diverse nature and critical processing requirements of tasks originating from heterogeneous devices. Traditional scheduling methods often struggle with high latency and inadequate processing times, especially in applications demanding strict computational efficiency. To address these challenges, this paper proposes an advanced fog-cloud integration approach utilizing a deep reinforcement learning-based task scheduler, DRLMOTS (Deep Reinforcement Learning based Multi Objective Task Scheduler in Cloud Fog Environment). This novel scheduler intelligently evaluates task characteristics, such as length and processing capacity, to dynamically allocate computation to either fog nodes or cloud resources. The methodology leverages a Deep Q-Learning Network model and includes extensive simulations using both randomized workloads and real-world Google Jobs Workloads. Comparative analysis demonstrates that DRLMOTS significantly outperforms existing baseline algorithms such as CNN, LSTM, and GGCN, achieving a substantial reduction in makespan by up to 26.80%, 18.84, and 13.83% and decreasing energy consumption by up to 39.60%, 30.29%, and 27.11%. Additionally, the proposed scheduler enhances fault tolerance, showcasing improvements of up to 221.89%, 17.05%, and 11.05% over conventional methods. These results validate the efficiency and robustness of DRLMOTS in optimizing task scheduling in fog-cloud environments."
pub.1157031178,Deep Meta Q-Learning Based Multi-Task Offloading in Edge-Cloud Systems,"Resource-constrained edge devices can not efficiently handle the explosive growth of mobile data and the increasing computational demand of modern-day user applications. Task offloading allows the migration of complex tasks from user devices to the remote edge-cloud servers thereby reducing their computational burden and energy consumption while also improving the efficiency of task processing. However, obtaining the optimal offloading strategy in a multi-task offloading decision-making process is an NP-hard problem. Existing Deep learning techniques with slow learning rates and weak adaptability are not suitable for dynamic multi-user scenarios. In this article, we propose a novel deep meta-reinforcement learning-based approach to the multi-task offloading problem using a combination of first-order meta-learning and deep Q-learning methods. We establish the meta-generalization bounds for the proposed algorithm and demonstrate that it can reduce the time and energy consumption of IoT applications by up to 15%. Through rigorous simulations, we show that our method achieves near-optimal offloading solutions while also being able to adapt to dynamic edge-cloud environments."
pub.1140299411,Big Data for Smart Education,"Advanced technologies are allowing students to cram more information more effectively and in a versatile manner rather than contentedly. Learners use smart devices to access digital resources over wireless networks, then customize the seamless learning experience. Smart education, which includes learning on a digital platform, is becoming more popular. This chapter introduces smart education and provides logical foundations for it. The system of smart pedagogies, as well as a key feature of the smart learning environment, is designed for adoptive smart learners who need mastery of information and skills in 21st-century learning. A smart pedagogics system integrates simple personalized learning and mass procreative learning, as well as class-based discriminated guidance. SES (smart education system) is a step forward from existing educational information systems, and it is powered by cloud computing, Internet of things (IoT), and mobile internet. It also relies on a pervasive network environment and needs a cloud computing data center as well as a multidimensional IoT-sensing device. SES is based on—and challenged by—the implementation of Big Data (BD). This chapter will look into the implementation of BD in SES, recommending the transformation of universities’ knowledge portals into services portals. There are number of opportunities and challenges associated with the implementation of a higher education. In recent years, there has been an increase in the customization of learning management system (LMS) within the educational system. Smartphones are now being used by students to access online content. As a result, when opposed to conventional learning, online practices will result in a massive reduction in the amount of unexploited data waste. Processing is not something that analytics is capable of. BD innovations and resources are disseminated in the educational system. As a result, current implementations of BD technologies in the field of education, as well as a summary of the literature on educational learning analytics, are presented here. In addition, the technical architecture of a SES will be proposed, emphasizing the importance of smart computing. The tri-tier design and core roles are already in place, and smart education issues will be addressed at the end. This chapter concludes by laying out upcoming recommendations for developing and implementing a BD-related institutional project. Advanced technologies are allowing students to cram more information more effectively and in a versatile manner rather than contentedly. Large amounts of data on the current status and behavior of each component mutually together will be organized through Big Data Analytics to existing development designs and placed into smart application. To support convergence, increase ease, and encourage smart device and learning, open system architectures must be improved. The current technology architecture of smart education environment is focused on smart comput"
pub.1173899427,Importance of Machine Learning and Data Science in Modern Business ,"The article explores the vital role of machine learning (ML) and data science in advancing business efficiency, especially under crisis conditions like the ongoing conflict in Ukraine. It discusses how digital transformation through these technologies is crucial for maintaining competitiveness and operational resilience. As part of the research, it was conducted deep analysis of existing works. It was identified gap of comprehensive studies on the strategic application of cloud-based ML and data science solutions during crises. This study Highlights the increasing accessibility of ML and data science tools due to technological advancements, fostering a competitive business landscape. The study emphasizes the democratization of advanced technologies facilitated by cloud platforms like Microsoft Azure, Google Cloud Platform (GCP), and Amazon Web Services (AWS), making sophisticated tools accessible to smaller companies. Article concludes that strategic use of ML and data science significantly bolsters business resilience and efficiency, especially in challenging environments like Ukraine. Article examines ML tools and services provided by AWS, Azure, and GCP. As an assessment criterion it was chosen features, integration capabilities, innovation, pricing structures, computing capabilities, and security measures. In scope of this research it was defined that each platform offers robust ML solutions with unique strengths tailored to different business needs. For example, AWS excels in specialized tools, Azure in integration within its ecosystem, and GCP in sustainability and advanced technologies. Article provides recommendations for selecting cloud-based ML and data science solutions that align with operational strategies and crisis management needs. It encourages ongoing research to explore the long-term impacts of these technologies on business innovation and market dynamics. Highlights the need for further studies into the socio-economic impacts of ML and data science, including addressing privacy, security, and ethical concerns. Article provides tailored advice on choosing appropriate ML and data science tools to support their specific needs during the ongoing crisis. Also, it suggests broader adoption of cloud-based ML and data science technologies for enhanced decision-making and operational efficiency."
pub.1119488134,"Cloud Forensics: A Meta-Study of Challenges, Approaches, and Open Problems","In recent years, cloud computing has become popular as a cost-effective and
efficient computing paradigm. Unfortunately, today's cloud computing
architectures are not designed for security and forensics. To date, very little
research has been done to develop the theory and practice of cloud forensics.
Many factors complicate forensic investigations in a cloud environment. First,
the storage system is no longer local. Therefore, even with a subpoena, law
enforcement agents cannot confiscate the suspect's computer and get access to
the suspect's files. Second, each cloud server contains files from many users.
Hence, it is not feasible to seize servers from a data center without violating
the privacy of many other users. Third, even if the data belonging to a
particular suspect is identified, separating it from other users' data is
difficult. Moreover, other than the cloud provider's word, there is usually no
evidence that links a given data file to a particular suspect. For such
challenges, clouds cannot be used to store healthcare, business, or national
security related data, which require audit and regulatory compliance. In this
paper, we systematically examine the cloud forensics problem and explore the
challenges and issues in cloud forensics. We then discuss existing research
projects and finally, we highlight the open problems and future directions in
cloud forensics research area. We posit that our systematic approach towards
understanding the nature and challenges of cloud forensics will allow us to
examine possible secure solution approaches, leading to increased trust on and
adoption of cloud computing, especially in business, healthcare, and national
security. This in turn will lead to lower cost and long-term benefit to our
society as a whole."
pub.1126121799,Hash Based Integrity Verification for Vehicular Cloud Environment,"Vehicular Cloud Computing (VCC) facilitates the customers to share resources ranging from storage to computing power to renting it to other users over the internet. VCC technology is combination of VANET and cloud computing. However, this integration of vehicular network and cloud computing introduces new challenges for the research community. For security reasons, users would like to verify the integrity of their data once storing it in the VCC. In this paper, we will introduce a scheme for the integrity checking with the support of the Road Side Unit (RSU) for dynamic vehicular cloud. The hash functions and signatures are employed for integrity checking of messages which are uploaded from the vehicle to the cloud. We have analysed effectiveness of proposed scheme against typical attacks like tampering attack, node impersonation and application attack etc. Extensive simulations are conducted to verify the performance of the proposed scheme and the results are compared with existing research based on ID-based cryptographic scheme. The proposed scheme performs better in terms of computation cost and communication cost."
pub.1138915797,Clustering-based data placement in cloud computing: a predictive approach,"Nowadays, cloud computing environments have become a natural choice to host and process a huge volume of data. The combination of cloud computing and big data frameworks is an effective way to run data-intensive applications and tasks. Also, an optimal arrangement of data partitions can improve the tasks executions, which is not the case in most big data frameworks. For example, the default distribution of data partitions in Hadoop-based clouds causes several problems, which are mainly related to the load balancing and the resource usage. In addition, most existing data placement solutions are static and lack precision in the placement of data partitions. To overcome these issues, we propose a data placement approach based on the prediction of the future resources usage. We exploit Kernel Density Estimation (KDE) and Fuzzy FCA techniques to, first, forecast the workers’ and tasks’ future resource consumption and, second, cluster data partitions and intensive jobs according to the estimated resource usage. Fuzzy FCA is also used to exclude partitions and jobs that require less resources, which will reduce the needless migrations. To allow monitoring and predicting the workers’ states and the data partitions’ consumption, we modeled the big data cluster as an autonomic service-based system. The obtained results have shown that our solution outperformed existing approaches in terms of migrations rate and resource consumption."
pub.1117550319,Improving efficiency and availability in Smart Classroom environments,"The improvement of the teaching and learning process today is possible through the integration of day-to-day technologies. Among the existing technological methods for learning, there are those focused on content, like educational games and virtual environments, and those centered on the student and the environment, like adaptive, ubiquitous and context-aware environments. In this last scenario, you will find smart classrooms smart, which must have the support of a Fog Computing infrastructure to operate effectively. The objective of this work is to demonstrate methods of identification, prevention, and failure treatment in this of cloud computing infrastructure, to achieve greater efficiency of the ubiquitous and adaptive environment of Smart Classroom. The proposal is not only limited to this application, but is applicable regardless of the type of Internet of Things (IoT) environment implemented. To verify these methods effectiveness, a Smart Classroom is simulated on the iFogSim platform. The results obtained regarding the level of availability and operability of the components were promising, evidencing proper levels of methods applicability."
pub.1144357186,Spider Monkey Optimization based Energy-Efficient Resource Allocation in Cloud Environment,"The origin of Cloud computing is from the principle of utility computing, which is characterized as a broadband service providing storage and computational resources. It provides a large variety of processing options and heterogeneous tools, allowing it to meet the needs of a wide range of applications at different levels. As a result, resource allocation and management are critical in cloud computing. In this work, the Spider Monkey Optimization (SMO) is used for attaining an optimized resource allocation. The key parameters considered to regulate the performance of SMO are its application time, migration time, and resource utilization. Energy consumption is another key factor in cloud computation which is also considered in this work. The Green Cloud Scheduling Model (GCSM) is followed in this work for the energy utilization of the resources. This is done by scheduling the heterogeneity tasks with the support of a scheduler unit which schedules and allocates the tasks which are deadline-constrained enclosed to nodes which are only energy-conscious. Assessing these methods is formulated using the cloud simulator programming process. The parameter used to determine the energy efficiency of this method is its energy consumption. The simulated outcome of the proposed approach proves to be effective in response time, makespan, energy consumption along with resource utility corresponding to the existing algorithms."
pub.1172510352,Importance of Machine Learning and Data Science in Modern Business,"The article explores the vital role of machine learning (ML) and data science in advancing business efficiency, especially under crisis conditions like the ongoing conflict in Ukraine. It discusses how digital transformation through these technologies is crucial for maintaining competitiveness and operational resilience. As part of the research, it was conducted deep analysis of existing works. It was identified gap of comprehensive studies on the strategic application of cloud-based ML and data science solutions during crises. This study Highlights the increasing accessibility of ML and data science tools due to technological advancements, fostering a competitive business landscape. The study emphasizes the democratization of advanced technologies facilitated by cloud platforms like Microsoft Azure, Google Cloud Platform (GCP), and Amazon Web Services (AWS), making sophisticated tools accessible to smaller companies. Article concludes that strategic use of ML and data science significantly bolsters business resilience and efficiency, especially in challenging environments like Ukraine. Article examines ML tools and services provided by AWS, Azure, and GCP. As an assessment criterion it was chosen features, integration capabilities, innovation, pricing structures, computing capabilities, and security measures. In scope of this research it was defined that each platform offers robust ML solutions with unique strengths tailored to different business needs. For example, AWS excels in specialized tools, Azure in integration within its ecosystem, and GCP in sustainability and advanced technologies. Article provides recommendations for selecting cloud-based ML and data science solutions that align with operational strategies and crisis management needs. It encourages ongoing research to explore the long-term impacts of these technologies on business innovation and market dynamics. Highlights the need for further studies into the socio-economic impacts of ML and data science, including addressing privacy, security, and ethical concerns. Article provides tailored advice on choosing appropriate ML and data science tools to support their specific needs during the ongoing crisis. Also, it suggests broader adoption of cloud-based ML and data science technologies for enhanced decision-making and operational efficiency."
pub.1034390707,Fog Forecasting for the Southern Region: A Conceptual Model Approach,"The prediction of fog occurrence, extent, duration, and intensity remains difficult despite improvements in numerical guidance and modeling of the fog phenomenon. This is because of the dependency of fog on microphysical and mesoscale processes that act within the boundary layer and that, in turn, are forced by the prevailing synoptic regime. Given existing and new technologies and techniques already available to the operational forecaster, fog prediction may be improved by the development and application of a simple conceptual model. A preliminary attempt at such a model is presented for the southern region of the United States (gulf coastal states) and requires information regarding cloud condensation nuclei, moisture availability (or saturation), and dynamic forcing. Each of these factors are assessed with regard to their extent and evolution with time. An illustration, and potential application, of how the model could be used is detailed as no extensive operational testing has yet been completed. Instead, the model is applied in hindcast to verify its application. Successful use of the model will require an operational forecaster to assimilate all available tools including climatology, numerical guidance, sounding analysis, model diagnostic software, and satellite imagery. These must be used to characterize and quantify the nature of the local and regional boundary layer in the forecast region according to macroscale forcing and moisture availability, the initial local settings and boundary layer, qualitative assessment of cloud condensation nuclei, and the interaction of these in time and space. Once identified, the evolution of the boundary layer may be forecast with regard to the overall environment for fog occurrence, its likely extent, intensity, and duration."
pub.1110385432,Liquid Silicon-Monona,"With the recent trend of promoting Field-Programmable Gate Arrays (FPGAs) to first-class citizens in accelerating compute-intensive applications in networking, cloud services and artificial intelligence, FPGAs face two major challenges in sustaining competitive advantages in performance and energy efficiency for diverse cloud workloads: 1) limited configuration capability for supporting light-weight computations/on-chip data storage to accelerate emerging search-/data-intensive applications. 2) lack of architectural support to hide reconfiguration overhead for assisting virtualization in a cloud computing environment. In this paper, we propose a reconfigurable memory-oriented computing fabric, namely Liquid Silicon-Monona (L-Si), enabled by emerging nonvolatile memory technology i.e. RRAM, to address these two challenges. Specifically, L-Si addresses the first challenge by virtue of a new architecture comprising a 2D array of physically identical but functionally-configurable building blocks. It, for the first time, extends the configuration capabilities of existing FPGAs from computation to the whole spectrum ranging from computation to data storage. It allows users to better customize hardware by flexibly partitioning hardware resources between computation and memory, greatly benefiting emerging search- and data-intensive applications. To address the second challenge, L-Si provides scalable multi-context architectural support to minimize reconfiguration overhead for assisting virtualization. In addition, we provide compiler support to facilitate the programming of applications written in high-level programming languages (e.g. OpenCL) and frameworks (e.g. TensorFlow, MapReduce) while fully exploiting the unique architectural capability of L-Si. Our evaluation results show L-Si achieves 99.6% area reduction, 1.43× throughput improvement and 94.0% power reduction on search-intensive benchmarks, as compared with the FPGA baseline. For neural network benchmarks, on average, L-Si achieves 52.3× speedup, 113.9× energy reduction and 81% area reduction over the FPGA baseline. In addition, the multi-context architecture of L-Si reduces the context switching time to - 10ns, compared with an off-the-shelf FPGA (∼100ms), greatly facilitating virtualization."
pub.1101711747,Liquid Silicon-Monona,"With the recent trend of promoting Field-Programmable Gate Arrays (FPGAs) to first-class citizens in accelerating compute-intensive applications in networking, cloud services and artificial intelligence, FPGAs face two major challenges in sustaining competitive advantages in performance and energy efficiency for diverse cloud workloads: 1) limited configuration capability for supporting light-weight computations/on-chip data storage to accelerate emerging search-/data-intensive applications. 2) lack of architectural support to hide reconfiguration overhead for assisting virtualization in a cloud computing environment. In this paper, we propose a reconfigurable memory-oriented computing fabric, namely Liquid Silicon-Monona (L-Si), enabled by emerging nonvolatile memory technology i.e. RRAM, to address these two challenges. Specifically, L-Si addresses the first challenge by virtue of a new architecture comprising a 2D array of physically identical but functionally-configurable building blocks. It, for the first time, extends the configuration capabilities of existing FPGAs from computation to the whole spectrum ranging from computation to data storage. It allows users to better customize hardware by flexibly partitioning hardware resources between computation and memory, greatly benefiting emerging search- and data-intensive applications. To address the second challenge, L-Si provides scalable multi-context architectural support to minimize reconfiguration overhead for assisting virtualization. In addition, we provide compiler support to facilitate the programming of applications written in high-level programming languages (e.g. OpenCL) and frameworks (e.g. TensorFlow, MapReduce) while fully exploiting the unique architectural capability of L-Si. Our evaluation results show L-Si achieves 99.6% area reduction, 1.43× throughput improvement and 94.0% power reduction on search-intensive benchmarks, as compared with the FPGA baseline. For neural network benchmarks, on average, L-Si achieves 52.3× speedup, 113.9× energy reduction and 81% area reduction over the FPGA baseline. In addition, the multi-context architecture of L-Si reduces the context switching time to - 10ns, compared with an off-the-shelf FPGA (∼100ms), greatly facilitating virtualization."
pub.1092141165,Dynamic Threshold-Based Dynamic Resource Allocation Using Multiple VM Migration for Cloud Computing Systems,"As compared to traditional distributed computing systems, cloud computing systems are more reliable, dynamic, and scalable. In recent trend the challenge is managing the resources to maintain the scalability in dynamic environment. The need is to improve the performance of cloud computing systems by provisioning and allocation of on-demand resources to reduce the time. Some of the existing methods are based on static parameters such as CPU utilization threshold, resources, and workload that give less efficient results and there is lack in handling the over-provisioning and under-provisioning situations. In this paper we propose resource allocation model on the basis of dynamic parameters. The proposed method, dynamic threshold-based dynamic resource allocation can optimize the resource utilization and time. The proposed model is implemented on CloudSim and experimental results show the proposed model can improve resource utilization and time."
pub.1086070716,One‐on‐one contract game–based dynamic virtual machine migration scheme for Mobile Edge Computing,"Abstract In the evolution toward fifth‐generation networks, Mobile Edge Computing (MEC) is an emerging paradigm, conceived to meet the ever‐increasing computational demands of mobile applications. Within the access range of mobile devices, the MEC technique promises the enablement of efficient Mobile Cloud Computing services. In an MEC system, virtual machine (VM) migration is a key issue; VM migration is the process of moving a VM from an edge node to another edge node. To improve service quality and system performance, the VM migration method has a dual focus on the MEC system's computation and communication resources. In this study, we formulate the VM migration problem as a one‐on‐one contract game model and develop a learning‐based price control mechanism to effectively handle the MEC's resource. By using the game methodology and learning process, our approach is able to capture the dynamics of MEC systems, and it interacts continually with an unknown system environment. Finally, extensive simulation results are provided to demonstrate the capability of the proposed approach in achieving, with respect to existing MEC schemes, both higher resource utilization and system throughput, as well as reduced service drop ratio and reduced service delay."
pub.1095033231,Cloud Computing Interoperability: The State of Play,"Cloud computing is a promising IT paradigm which enables the Internet's evolution into a global market of collaborating services. Cloud computing semantic interoperability plays a key role in making this a reality. Towards this direction, a comprehensive and systematic survey of Cloud computing interoperability efforts by standardization groups, industry and research community is carried out. The main objective of this survey is to derive an initial set of semantic interoperability requirements to be supported by existing as well as next generation Cloud systems. Ôhe survey motivates and encourages the Cloud community to adopt a common Cloud computing interoperability framework with core dimensions the creation of a common data model and a standardized Cloud interface (API), which will constitute the base for the development of a semantically interoperable Cloud environment."
pub.1169284874,Implementing Tactile Internet Using 5G Network for Cloud Manufacturing in a PLC-Driven Water Bottling Plant,"Key characteristics of a smart manufacturing environment are system automation, data storage, remote monitoring, prediction of bottlenecks, and the ability to interrupt a physical system using digital twins. Over the years, automation of smart factories has been achieved through the implementation of Tactile Internet using 3G/4G/LTE network. However, high latency, low bandwidth, limited network access, and minimal device density are some of the barriers which increase production time and in turn limit the adaptation of smart factories in time-sensitive applications. Research shows that the introduction of 5G as a network link between the physical system and cloud storage could address long-existing challenge of high latency, low bandwidth, minimal device density, and limited network access. However, there is limited research on the feasibility of this implementation and the actual results thereof. This article showcases an experimental setup to implement a 5G network link between the cloud and a water bottling plant driven by Programmable Logic Controllers, which is currently operating on a 4G network. The hypothesis is that, with the implementation of 5G, there will be a decreased production time owing to minimal latency, improved data transmission capacity, and a broader network access though cloud storage."
pub.1135933994,Reality Capture of Buildings Using 3D Laser Scanners,"The urgent need to improve performance in the construction industry has led to the adoption of many innovative technologies. 3D laser scanners are amongst the leading technologies being used to capture and process assets or construction project data for use in various applications. Due to its nascent nature, many questions are still unanswered about 3D laser scanning, which in turn contribute to the slow adaptation of the technology. Some of these include the role of 3D laser scanners in capturing and processing raw construction project data. How accurate are the 3D laser scanner or point cloud data? How does laser scanning fit with other wider emerging technologies such as building information modeling (BIM)? This study adopts a proof-of-concept approach, which in addition to answering the aforementioned questions, illustrates the application of the technology in practice. The study finds that the quality of the data, commonly referred to as point cloud data, is still a major issue as it depends on the distance between the target object and 3D laser scanner’s station. Additionally, the quality of the data is still very dependent on data file sizes and the computational power of the processing machine. Lastly, the connection between laser scanning and BIM approaches is still weak as what can be done with a point cloud data model in a BIM environment is still very limited. The aforementioned findings reinforce existing views on the use of 3D laser scanners in capturing and processing construction project data."
pub.1045330267,A personalized IPTV channel-recommendation mechanism based on the MapReduce framework,"Internet protocol television viewers spend considerable time browsing through the many existing channels, which is inefficient and time consuming. Although the recommendation system can solve the channel-switching problem, its performance is slow unless it is adapted to read a large amount of data sets. This study proposes a novel cloud-assisted channel-recommendation system under a cloud computing environment, channel association rules (CARs), to speed up the performance of channel switching, thereby help users to find their favorite channels in less time. The CARs algorithm is compared with the conventional (COV) solution and the most frequently selected (MFS) algorithm based on MovieLens data sets. The experimental results indicate that the predictive accuracy of CARs is superior to that of the COV and MFS algorithms. In addition, CARs use parallel computing in MapReduce to distribute large amounts of user history logs across multiple computers for processing. The experimental results show that the proposed algorithm can be employed to efficiently handle big data in a finite time when a huge of cloud servers are rented from commercial cloud providers such as Amazon Elastic Compute Cloud (EC2), Microsoft HDinsight."
pub.1011999312,A Design of Web Log Integration Framework Using NoSQL,"Webservice is a software technology as the representative method of information communication currently used to create a dynamic system environment that is configured to fulfill its users’ needs. Therefore, analyzing log data that occurred at provision is being used as the significant basic data in webservice research. Thanks to development of Cloud computing technology, it has resulted in centralized points from which data is generated and data enlargement. A research is now implemented to create information from collecting, processing and converting flood of data and to obtain the new various items of information. Against this backdrop, it is justified that collection, storage and analysis of web log data in the existing conventional RDBMS system may be inadequate to process the enlarged log data. This research propose a framework which to integrate web log for storage using HBase, a repository of the Cloud computing- based NoSQL. In addition, data validation must be completed in the pre-process when collecting web log. The validated log is stored in the modeling structure in which takes features of web log into account. According to the results, introduction of NoSQL system is found to integrate the enlargement of log data in more efficient manner. By comparisons with the existing RDBMS in terms of data processing performance, it was proved that the NoSQL- based database had a superior performance."
pub.1033577582,A Multi-Level Privacy Scheme for Securing Data in a Cloud Environment,"Privacy concern is often cited as one of the key factors that impede large-scale adoption of the cloud computing paradigm by enterprise customers. Existing solutions to privacy issue with respect to cloud computing, commonly through encryption mechanisms, often result in performance problem. This paper proposes and presents a multi-level privacy support scheme for addressing the trade-off between privacy of user’s data stored in the cloud and system performance. This is achieved by using encryption algorithms with varying strengths to protect the privacy of different categories of user’s data depending on their privacy sensitivity. Simulation results, using Jindael AES encryption algorithm as case study, lends credence to the efficacy of the proposed privacy scheme."
pub.1169581436,"NASA’s Open Science Platform VEDA (Visualization, Exploration and Data Analytics)","VEDA is an open-source science cyberinfrastructure for data processing, visualization, exploration, and geographic information systems (GIS) capabilities (https://www.earthdata.nasa.gov/esds/veda, https://www.earthdata.nasa.gov/dashboard/). NASA has always had open data policies, so data has always been openly accessible for anyone, but NASA hasn’t constantly exposed it in friendly interfaces or analytics platforms. VEDA attempts to make NASA’s Earth data mean more As VEDA supplies data and computing services through its dashboard and JupyterHub applications and engages with communities such as EGU, it is a critical component of NASA’s open science initiative. VEDA’s adoption of existing and emerging standards such as STAC, Cloud-Optimized GeoTIFFs, Zarr, the Features API, and the Tiles API ensures interoperability and reusability. In the past year, VEDA has expanded its impact in 3 ways: (1) the reuse of its infrastructure to stand up the multi-agency Greenhouse Gas Center (https://earth.gov/ghgcenter, announced at COP28) and NASA’s Earth Information Center (https://earth.gov/), (2) the reuse of data APIs across applications, such as VEDA data in NASA’s Enterprise GIS, and (3) the generalization of the data system architecture into a free and open source framework called eoAPI.  VEDA has also maintained and deepened its connections to the Multi-Mission Algorithm and Analysis Platform (MAAP). MAAP is a research data infrastructure (RDI) for above-ground biomass estimation. MAAP is reusing and contributing to the eoAPI data system and plans to integrate the analytics components (JupyterHub and data processing system) further. Now that VEDA has manifested GHG Center and EIC, VEDA is a project where innovation happens. The VEDA team, composed of NASA project leads, scientists, designers, and developers, constantly works to resolve old and new challenges in managing EO architectures. For example, the team designs and implements interfaces to manage STAC metadata. eoAPI is a result of this innovative environment. eoAPI is a new, open-source, installable combination of data catalog and associated services for earth observation and related data with a cloud-computing infrastructure first approach. eoAPI combines STAC data ingestion, data hosting (pgSTAC), and querying services (stac-fastapi) with raster (Titiler) and vector services (TiPg). eoAPI is built for reuse and has been used beyond VEDA, GHG, and EIC to deliver MS Planetary Computer and AWS ASDI’s data catalog and applications for the International Federation of the Red Cross and MercyCorps. This presentation will demo the current capabilities of eoAPI and VEDA and discuss how these capabilities were designed and architected with the central goals of science delivery, reproducible science, and interoperability to support the re-use of data and APIs across the Earth Science ecosystem of tools. The presentation will close with VEDA and eoAPI’s plans."
pub.1171781155,Mobile-Aware Service Function Chain Intelligent Seamless Migration in Multi-access Edge Computing,"With the improvement of service delay and quality requirements for new applications such as unmanned driving, internet of vehicles, and virtual reality, the deployment of network services is gradually moving from the cloud to the edge. This transition has led to the emergence of multi-access edge computing (MEC) architectures such as distributed micro data center and fog computing. In the MEC environment, network infrastructure is distributed around users, allowing them to access the network nearby and move between different service coverage locations. However, the high mobility of users can significantly affect service orchestration and quality, and even cause service interruption. How to respond to user mobility, dynamically migrate user services, and provide users with a continuous and seamless service experience has become a huge challenge. This paper studies the dynamic migration of service function chain (SFC) caused by user mobility in MEC environments. First, we model the SFC dynamic migration problem in mobile scenarios as an integer programming problem with the goal of optimizing service delay, migration success rate, and migration time. Based on the above model, we propose a deep reinforcement learning-driven SFC adaptive dynamic migration optimization algorithm (DRL-ADMO). DRL-ADMO can perceive the underlying network resources and SFC migration requests, intelligently decide on the migration paths of multiple network functions, and adaptively allocate bandwidth, achieving parallel and seamless SFC migration. Performance evaluation results show that compared with existing algorithms, the proposed algorithm can optimize 7% service delay and 20% migration success rate at the cost of sacrificing a small amount of migration time."
pub.1176233181,Concept of SME Business Model for Industry 4.0 Environment,"Ongoing technological development pushes industry towards the so called fourth industrial revolution. Considering new technology as a determinant of future business environment, we find it necessary to examine how platforms such as Industry 4.0 will change enterprises organization and business models. Designed model should serve as guidance for new and also already existing enterprises for implementing of Industry 4.0 required attributes especially in early stage. Main emphasis is given on software and cloud solutions that will become necessary despite the fact that in recent industrial SMEs they do not play significant role. Such transformation will raise crucial questions about funding new technologies."
pub.1153231917,Towards Automating the Integration of Legacy IEDs into Edge-Supported Internet of Smart Grid Things,"The prominence of the SG-to­Cloud continuum will pave the way towards advanced Smart Grid (SG) ecosystems and will enable cutting Edge applications and servers into the power energy vertical at unprecedented innovation levels. During the design of future Smart Grid ecosystems, legacy Intelligent Eletronic Device (IED) cannot be left behind, whereby their full integration into the Internet of Smart Grid Things(IoSGT) reveals itself as a continuous issue. In an attempt to tackle this challenge, we are introducing the Legacy Smart Grid to IoT Integration Approach(SG2IoT), which automates the integration of multiple legacy IEDs in a scalable and flexible environment made possible by the IoSGT. Aside from that, the SG2IoT establishes an SG­to­Cloud continuum for provisioning architectural modular components for running in a distributed approach at Cloud facilities spread in Edge and central datacenters. Finally, the SG2IoT impact estimation was made up of harnessing a prototype running atop a lab­premised testbed that features real­world technologies. Outcome analysis proves the viability of the SG2IoT lightweight approach by establishing an SG­to­Cloud continuum to afford low times responses and affordable IoSGT scalability."
pub.1094484464,An ACO-Based Scheduling Strategy on Load Balancing in Cloud Computing Environment,"Overload balance of cloud data centers is a matter of great concern. Live migration of virtual machines presents an effective method to realize load balancing and to optimize resources utilization. With the rapidly increasing scale of cloud data centers, traditional centralized migration strategy begins to show lack of scalability and reliability. In this paper, we propose a novel distributed VM migration strategy based on a metaheuristic algorithm called Ant Colony Optimization. In our ACO-VMM Strategy, local migration agent autonomously monitors the resource utilization and launchs the migration. At monitoring stage, it takes both the previous and current system condition into account to avoid unnecessary migrations. Besides, it adopts two different traversing strategies for ants in order to find the near-optimal mapping relationship between virtual machines (VMs) and physical machines (PMs). Experimental results show that ACO-VMM outperforms the existing migration strategies by achieving load balance of whole system, as well as reducing the number of migrations and maintaining the required performance levels."
pub.1136340439,Fuzzy Logic-based Robust Failure Handling Mechanism for Fog Computing,"Fog computing is an emerging computing paradigm which is mainly suitable for
time-sensitive and real-time Internet of Things (IoT) applications. Academia
and industries are focusing on the exploration of various aspects of Fog
computing for market adoption. The key idea of the Fog computing paradigm is to
use idle computation resources of various handheld, mobile, stationery and
network devices around us, to serve the application requests in the Fog-IoT
environment. The devices in the Fog environment are autonomous and not
exclusively dedicated to Fog application processing. Due to that, the
probability of device failure in the Fog environment is high compared with
other distributed computing paradigms. Solving failure issues in Fog is crucial
because successful application execution can only be ensured if failure can be
handled carefully. To handle failure, there are several techniques available in
the literature, such as checkpointing and task migration, each of which works
well in cloud based enterprise applications that mostly deals with static or
transactional data. These failure handling methods are not applicable to highly
dynamic Fog environment. In contrast, this work focuses on solving the problem
of managing application failure in the Fog environment by proposing a composite
solution (combining fuzzy logic-based task checkpointing and task migration
techniques with task replication) for failure handling and generating a robust
schedule. We evaluated the proposed methods using real failure traces in terms
of application execution time, delay and cost. Average delay and total
processing time improved by 56% and 48% respectively, on an average for the
proposed solution, compared with the existing failure handling approaches."
pub.1153430091,TinyML for Ultra-Low Power AI and Large Scale IoT Deployments: A Systematic Review,"The rapid emergence of low-power embedded devices and modern machine learning (ML) algorithms has created a new Internet of Things (IoT) era where lightweight ML frameworks such as TinyML have created new opportunities for ML algorithms running within edge devices. In particular, the TinyML framework in such devices aims to deliver reduced latency, efficient bandwidth consumption, improved data security, increased privacy, lower costs and overall network cost reduction in cloud environments. Its ability to enable IoT devices to work effectively without constant connectivity to cloud services, while nevertheless providing accurate ML services, offers a viable alternative for IoT applications seeking cost-effective solutions. TinyML intends to deliver on-premises analytics that bring significant value to IoT services, particularly in environments with limited connection. This review article defines TinyML, presents an overview of its benefits and uses and provides background information based on up-to-date literature. Then, we demonstrate the TensorFlow Lite framework which supports TinyML along with analytical steps for an ML model creation. In addition, we explore the integration of TinyML with network technologies such as 5G and LPWAN. Ultimately, we anticipate that this analysis will serve as an informational pillar for the IoT/Cloud research community and pave the way for future studies."
pub.1094659395,A Hardware/Software Approach for Mitigating Performance Interference Effects in Virtualized Environments Using SR-IOV,"Single Root I/O Virtualization (SR-IOV) is an extension to the PCI Express (PCIe) standard that allows virtual machines (VMs) to directly access shared I/O devices without host involvement. This enabled SR-IOV to become the best-performing solution for virtual I/O to date, which lead to its commercial adoption, e.g., in the Amazon EC2. On the downside, a malicious VM can exploit the direct access to an SR-IOV device by flooding it with PCle packets. This results in a congestion on the PCle interconnect, which leads to performance interference effects between the malicious VM, concurrent VMs and even the host. In this paper, we present a hardware/software approach that detects and mitigates such Denial-of-Service (DoS) attacks. On the hardware side, we propose monitoring extensions within SR-IOV devices that distinguish legal device use from malicious device use by observing the rate of incoming PCIe transactions at VM granularity. Malicious VMs are reported to the host via interrupts. On the software side, performance interference effects can then be mitigated by dynamically adjusting the host's scheduling of the malicious VM or even shutting it down. We implement a prototype with a commercial off-the-shelf SR-IOV Ethernet controller and an FPGA board. On it, we demonstrate that appropriate scheduling of malicious VMs successfully mitigates interference effects for three cloud-relevant benchmarks. For example, Memcached is restored to 99.4% of baseline performance (compared to 61.8% without our extensions). In contrast to QoS features proposed in the PCle 3.0 standard, our solution is more flexible. Additionally, it can be realized as an add-on to existing misuse detection hardware like the Intel Malicious Driver Detection (MDD)."
pub.1138987649,"Cloud manufacturing architecture: a critical analysis of its development, characteristics and future agenda to support its adoption"," Purpose In the last decade, cloud manufacturing (CMfg) has attracted considerable attention from academia and industry worldwide. It is widely accepted that the design and analysis of cloud manufacturing architecture (CMfg-A) are the basis for developing and applying CMfg systems. However, in existing studies, analysis of the status, development process and internal characteristics of CMfg-A is lacking, hindering an understanding of the research hotspots and development trends of CMfg-A. Meanwhile, effective guidance is lacking on the construction of superior CMfg-As. The purpose of this paper is to review the relevant research on CMfg-A via identification of the main layers, elements, relationships, structure and functions of CMfg-A to provide valuable information to scholars and practitioners for further research on key CMfg-A technologies and the construction of CMfg systems with superior performance.   Design/methodology/approach This study systematically reviews the relevant research on CMfg-A across transformation process to internal characteristics by integrating quantitative and qualitative methods. First, the split and reorganization method is used to recognize the main layers of CMfg-A. Then, the transformation process of six main layers is analysed through retrospective analysis, and the similarities and differences in CMfg-A are obtained. Subsequently, based on systematic theory, the elements, relationships, structure and functions of CMfg-A are inductively studied. A 3D printing architecture design case is conducted to discuss the weakness of the previous architecture and demonstrate how to improve it. Finally, the primary current trends and future opportunities are presented.   Findings By analyzing the transformation process of CMfg-A, this study finds that CMfg-A resources are developing from tangible resources into intangible resources and intelligent resources. CMfg-A technology is developing from traditional cloud computing-based technology towards advanced manufacturing technology, and CMfg-A application scope is gradually expanding from traditional manufacturing industry to emerging manufacturing industry. In addition, by analyzing the elements, relationships, structure and functions of CMfg-A, this study finds that CMfg-A is undergoing a new generation of transformation, with trends of integrated development, intelligent development, innovative development and green development. Case study shows that the analysis of the development trend and internal characteristics of the architecture facilitates the design of a more effective architecture.   Research limitations/implications This paper predominantly focuses on journal articles and some key conference papers published in English and Chinese. The reason for considering Chinese articles is that CMfg was proposed by the Chinese and a lot of Chinese CMfg-A articles have been published in recent years. CMfg is suitable for the development of China’s manufacturing industry becau"
pub.1092535292,ShareRender,"Cloud gaming is promising to provide high-quality game services by outsourcing game execution to cloud so that users can access games via thin clients (e.g., smartphones or tablets). However, existing cloud gaming systems su er from low GPU utilization in the virtualized environment. Moreover, GPU resources are scheduled in units of virtual machines (VMs) and this kind of coarse-grained scheduling at the VM-level fails to fully exploit GPU processing capacity. In this paper, we present ShareRender, a cloud gaming sys- tem that o oads graphics workloads within VMs directly to GPUs, bypassing GPU virtualization. For each game running in a VM, ShareRender starts a graphics wrapper to intercept frame rendering requests and assign them to render agents responsible for frame rendering on GPUs. Thanks to the exible workload assignment among multiple render agents, ShareRender enables ne-grained resource sharing at the frame-level to signi cantly improve GPU utilization. Further more, we design an online algorithm to determine workload assignment and migration of render agents, which considers the tradeo between minimizing the number of active server and low agent migration cost. We conduct experiments on real deployment and trace-driven simulations to evaluate the performance of ShareRender under di erent system settings. The results show that ShareRender outperforms the existing video-streaming-based cloud gaming system by over 4 times."
pub.1105445262,3 Dimensional point cloud registration using global optimization techniques,"3-dimensional (3D) measurement and modeling, latterly has become an important issue to facilitate the analysis of a real object or an environment. However, obtained data at a time by existing methods such as laser scanning, structured light and stereo vision, cannot represent the entire object or the environment due to the reflections and limited field of view of the sensors. In order to generate a full 3D model, it is necessary to make measurements from different positions and locate all of them in a common coordinate system. This procedure called as point cloud registration. In this study, to registration of point clouds without any prior information, four basic affine transformation parameters are calculated by global optimization techniques such as Differential Evolution (DE) and Particle Swarm Optimization (PSO). Algorithm performances are compared with a fast, deterministic algorithm called Iterative Closest Point (ICP) that highly used in 3D registration literature. The studies show that, global optimization algorithms are quite successful in."
pub.1160611883,A Fault tolerant Multimedia Cloud Framework to guarantee Quality of Experience (QoE) in Live Streaming,"The massive scale of Cloud computing has enabled the popularity of the internet and growth of multimedia streaming. Live streaming applications have greatly benefitted on deploying it in the cloud environment due to its abundant availability of resources and features that handle scalability with agility. Managing Device heterogeneity is critical and affects user experience drastically. Response time and bandwidth are other issues to be focused on. The streaming services involve both desktop users and mobile users. High-definition video applications are often challenging for mobile devices due to their limited processing capability and bandwidth-constrained network connection. Cloud computing environments are elastic in nature which balances the load according to the fluctuations in the network. It is also easy to re-commission the required services in case of failures. But downgrade of services is possible. The reasons for the downgrade of services are many however hardware failure is one of the chief causes and queue overflow during re-commissioning is another. Whatever the cause, the effect of downgrade is drastic on the customer. To meet up with the problem of resource allocation, bandwidth allocation and fault tolerance issues and at the same time to guarantee the desired level of Quality of Experience (QoE) to the end-users, an entire framework is proposed with novel algorithms for all the addressed issues. The resource allocation, performed at the cloud end needs to be dynamic. Our framework incorporates the proposed Guess Fit algorithm to provision virtual machines dynamically based on priority scores calculated using probabilities as a result of the combined Naïve Bayes algorithm with association rule mining. The scores also take into account the hit ratios and the penalty values. It is found to perform better than the existing Fit algorithms. On the client end, an efficient cluster bandwidth allocation algorithm (CBA algorithm) is proposed to share bandwidth resources among the fixed device and the mobile devices in a cluster. The framework also incorporates a switching table-based fault tolerance module. The switching table is entirely built based on AND-OR logic and help in desktop migration for uninterrupted streaming."
pub.1121477172,Interference and Topology-Aware VM Live Migrations in Software-Defined Networks,"Live migration of virtual machines (VMs) serves as the key technique that enables the resource management, server maintenance, and load balancing in cloud datacenters, while introducing a sufficient amount of performance degradation to both the source and destination physical machines (PMs) during and after the migration. Existing works generally focus on either minimizing the VM migration time (or downtime) or reducing the performance degradation for an individual VM migration from the host aspect. Jointly optimizing the performance of multiple concurrent VM migrations and the performance interference among VMs from the network aspect, however, receives comparably little attention. Leveraging the benefits of Software-Defined Network (SDN), in this paper, we propose ITEM, an Interference and Topology-awarE VM live Migration strategy to minimize the overall VM migration cost, which jointly optimizes the VM migration time during the migration and the VM performance interference after the migration. Specifically, our ITEM strategy seeks to decide the optimal migration destination PM, the migration path, and the allocated network bandwidth for a single migration (ITEM-S) and a set of concurrent migrations (ITEM-M). Extensive experiments using OpenFlow in the Mininet virtual network environment show that our ITEM strategy can significantly reduce the migration cost and improve the network bandwidth utilization, in comparison to the state-of-the-art VM migration strategies, yet with acceptable runtime overhead."
pub.1155766650,PANGEO multidisciplinary test case for Earth and Environment Big data analysis in FAIR-EASE Infra-EOSC project,"Earth observation and modelling is a major challenge for research and a necessity for environmental and socio-economic applications. It requires voluminous and heterogeneous data from distributed and domain-dependent data sources, managed separately by various national and European infrastructures.
In a context of unprecedented data wealth and growth, new challenges emerge to enable inter-comparison, inter-calibration and comprehensive studies and uses of earth system and environmental data.
To this end, the FAIR-EASE project aims to provide integrated and interoperable services through the European Open Science Cloud to facilitate the discovery, access and analysis of large volumes of heterogeneous data from distributed sources and from different domains and disciplines of Earth system science.
This presentation will explain how the PANGEO stack will be used within FAIR EASE to improve data access, interpolation and analysis, but will also explore its integration with existing services (e.g. Galaxy) and underlying IT infrastructure to serve multidisciplinary research uses."
pub.1116623717,"Security, Privacy, and Trust in Cloud Computing","Cloud computing is revolutionizing the cyberspace by enabling convenient, on-demand network access to a large shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released [1]. While cloud computing is gaining popularity, diverse security, privacy and trust issues are emerging, which hinders the rapid adoption of this new computing paradigm. However, the development of defense solutions is lagging behind. To identify the limitations of existing solutions and to envision the future research directions are essential for ensuring a secure and trustworthy cloud environment. This chapter introduces important concepts, models, key technologies, and unique characteristics of cloud computing, which helps readers better understand the fundamental reasons for current security, privacy, and trust issues in cloud computing. Furthermore, critical security, privacy and trust challenges, and the corresponding state-of-the-art solutions are categorized and discussed in detail, followed by future research directions, which concludes this chapter. Cloud computing is revolutionizing the cyberspace by enabling convenient, on-demand network access to a large shared pool of configurable computing resources that can be rapidly provisioned and released. Cloud computing is defined as a service model that enables convenient, on-demand network access to a large shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction. Cloud users have different options to deploy the cloud based on their budget and security requirements. Cloud computing is revolutionizing cyberspace through its distinct characteristics such as broad-based network access, resource pooling, rapid elasticity, pay-as-you-go services, and federated environment. Diverse technologies have been developed in cloud computing scenarios to provide high quality and cost-efficient solutions for cloud users. Although cloud computing brings numerous attractive features, it does not come without costs. Serving as an essential component of the cloud foundation, virtualization technology receives extensive security attacks from different aspects."
pub.1172218367,Performance Evaluation of various Load Balancing Techniques in Cloud Computing,"The survival of cloud computing (CC) depends on a load balancing (LB) algorithm that is more reliable, efficient, and scalable. LB is one of the most difficult aspects of CC since it distributes changing workloads over numerous nodes while guaranteeing that no one node is overburdened. Resource consumption is kept to a minimum with efficient LB, allowing for scalability, avoiding bottlenecks, and overprovisioning, among other benefits. A systematic review of existing LB approaches currently used in CC was conducted in this research. According to the findings, existing strategies are mostly focused on minimising reaction time, completion time, cost, and throughput. Neither technique was able to reveal efficient task scheduling LB in a single or federated cloud system. Existing techniques, on the other hand, do not take into account studies such as energy LB, server consolidation, or VM migration. Future research will concentrate on creating a multi-objective job scheduling system that improves service quality in a homogeneous and federated heterogeneous cloud environment."
pub.1175429127,"Incorporating Privacy by Design Principles in the Modification of AI Systems in Preventing Breaches across Multiple Environments, Including Public Cloud, Private Cloud, and On-prem","The rapid integration of artificial intelligence (AI) across various sectors has significantly amplified privacy concerns, particularly with the growing reliance on cloud environments. Existing methods often fall short of effectively preventing privacy breaches due to inadequate risk assessment and mitigation strategies. These limitations highlight the necessity for more robust solutions, indicating the importance of Privacy by Design (PbD) principles. This study addresses these gaps by proposing a comprehensive approach to incorporating PbD principles into AI systems to prevent breaches across public, private, and on-prem environments. The proposed work utilizes logistic regression analysis to identify significant predictors of privacy breaches, revealing that both the environment (B = -1.142, p < .001) and severity of vulnerabilities (B = 0.932, p < .01) play crucial roles. Additionally, a strong positive correlation (r = 0.791) between breach detection rates and PbD effectiveness is observed, indicating the need for enhanced detection mechanisms. To support the empirical findings, this study also reviews existing case studies. It conducts a thematic analysis to provide a deeper understanding of the practical challenges and solutions associated with PbD implementation, particularly in healthcare and smart city applications. These analyses serve to supplement the empirical evidence and demonstrate the effectiveness of PbD over other existing methods. The study concludes that implementing PbD principles is critical for achieving robust privacy protection, and the study recommends prioritizing advanced breach detection mechanisms, comprehensive privacy impact assessments, continuous stakeholder engagement, and investment in privacy-enhancing technologies to address privacy risks effectively."
pub.1129693571,Increasing the quality of services and resource utilization in vehicular cloud computing using best host selection methods,"One of the technology for increasing the safety and welfare of humans in roads is Vehicular Cloud Computing (VCC). This technology can utilize cloud computing advantages in the Vehicular Ad Hoc Network (VANET). VCC by utilizing modern equipment along with current vehicles, can play a significant role in the area of smart transportation systems. Given the potential of this technology, effective methods for managing existing resources and providing the expected service quality that is essential for such an environment are not yet available as it should. One of the most important barriers to providing such solutions seems to be resource constraints and very high dynamics in vehicles in VCC. In this article, based on virtualization and taking into account the environment with these features, we propose simple ways to manage resources better and improve the quality of service. We were able to achieve better results in simulation than previous methods by providing a flexible data structure to control the important data in the environment effectively. To illustrate the impact of the proposed methods, we compared them with some of the most important methods in this context, and we used SUMO 1.2.0 and MATLAB R2019a software to simulate them. Simulation results indicate that the proposed methods provide better results than previous methods in terms of resource efficiency, Quality of Service (QoS), and load balancing."
pub.1158119784,Pace v0.2: a Python-based performance-portable atmospheric model,"Abstract. Progress in leveraging current and emerging high-performance computing infrastructures using traditional weather and climate models has been
slow. This has become known more broadly as the software productivity gap. With the end of Moore's law driving forward rapid specialization of
hardware architectures, building simulation codes on a low-level language with hardware-specific optimizations is a significant risk. As a
solution, we present Pace, an implementation of the nonhydrostatic FV3 dynamical core and GFDL cloud microphysics scheme which is entirely
Python-based. In order to achieve high performance on a diverse set of hardware architectures, Pace is written using the GT4Py domain-specific
language. We demonstrate that with this approach we can achieve portability and performance, while significantly improving the readability and
maintainability of the code as compared to the Fortran reference implementation. We show that Pace can run at scale on leadership-class
supercomputers and achieve performance speeds 3.5–4 times faster than the Fortran code on GPU-accelerated supercomputers. Furthermore, we
demonstrate how a Python-based simulation code facilitates existing or enables entirely new use cases and workflows. Pace demonstrates how a
high-level language can insulate us from disruptive changes, provide a more productive development environment, and facilitate the integration with
new technologies such as machine learning."
pub.1164215655,A Novel Framework for VM Selection and Placement in Cloud Environment,"In spite of various research that has been conducted in the past but there are some challenges that are still into existence related to balancing of workload in cloud applications. There has been a great need for efficient allocation of resources that is handling all the data center, servers & various virtual machines connected with cloud applications. It is the responsibility of cloud facility providers to confirm high facility delivery in an unavoidable situation. All such type of hosts is overloaded or underloaded based on their execution time and throughput. Task scheduling helps in balancing the load of resources and on the other hand task scheduling adheres to the requirement of service level agreement. SLA parameters such as deadlines are concentrated on the Load Balancing algorithm. This paper proposes algorithm which optimizes cloud resources and improves the balancing of load based on migration, SLA and energy efficiency. Proposed load-balancing algorithm discourses all states and focuses on existing research gaps by focusing on the literature gaps. Task scheduling is mainly concentrating on balancing the load and task scheduling mainly adheres to SLA. SLA is one of the documents offered by the service provider to the user. There are various parameters of load balancing such as deadlines which are discussed in the load balancing algorithm. The key focus of proposed process identifies optimize method of resources and improved Load Balancing based on QoS, priority migration of VMs and resource allocations. This proposed algorithm addressed these issues based on the literature review findings."
pub.1181662672,Automatic Coarse Registration of Urban Point Clouds Using Line-Planar Semantic Structural Features,"Point cloud registration is essential for constructing comprehensive geometric information of a scene. In feature-based point cloud registration, the extraction of stable features and accurate feature matching significantly enhance the algorithm’s robustness and overall effectiveness. This article introduces an automatic coarse registration method for large-scale urban point clouds utilizing line-planar semantic structural features (LSSFs), specifically designed for structured urban environments. The proposed method begins by extracting plane and line features from the point cloud and constructing LSSFs based on their geometric relationships. Then, it establishes correspondences between the source and target point clouds utilizing a hash table and the four line-planar spatial information constraints (FLSICs). The method then groups potential semantic features through a four-point geometric consistency (FPGC) approach, from which it selects the most reliable feature group to estimate the initial spatial transformation parameters. The registration result is further optimized through a global maximum consensus set (GMCS), achieving accurate registration of the point clouds. Experiments conducted on six large-scale urban point cloud datasets demonstrate the effectiveness of the proposed method in achieving pairwise point cloud registration, with a rotation error of merely 0.07° and a translation error of 0.11 m. In comparison to existing methods, this approach significantly enhances registration efficiency and accuracy, particularly when dealing with high-density point clouds and in the presence of noise. The method offers a reliable and efficient point cloud registration solution for complex urban environments, indicating potential for widespread application."
pub.1121059120,An efficient system model to minimize signal interference and delay in mobile cloud environment,"The integration of mobile computing with cloud computing has increased advantages to the users to access the applications from anywhere at any time. The traditional cellular system is overlaid by number of low power nodes which allows users to attain services at any time. But the increase of femto cells causes intra-tier, inter-tier interference and delay in the network. To overcome the bottleneck a sub-modular optimizing based offloading algorithm is proposed in this paper where the task is divided into two functions and processed. The main aim is to optimize execution time and interference. The simulation results show that the execution time has been reduced by 57.89% and performs well to obtain optimum solution compared to other existing methods."
pub.1164296206,LoRa-Enabled Smart RS485 Data Logger and MQTT Gateway for Industrial IoT Applications Using ESP32,"Industry 4.0 has driven the need for efficient data gathering and analysis to enhance business performance. However, collecting data from assets in challenging environments with uncertain network conditions remains a major obstacle. In this paper, we propose the Smart Data Logger, a gateway designed to reliably collect data from various sensors and devices and transmit it to the cloud or a server. We address the challenge of converting data from different devices and protocols to enable Industrial Internet of Things (IIoT) readiness. Our gateway utilizes an ESP32 microcontroller with a Wi-Fi board and an RS485 to TTL add-on module for communication. It supports bidirectional communication between field devices and cloud applications and provides a Modbus RTU interface on RS485 for onsite device connectivity. The gateway incorporates a GSM A7670 4G LTE module for internet connectivity and includes a built-in SD card for data storage during connectivity loss. Furthermore, an onboard LoRa E32 868MHz module allows multiple nodes to connect and send data to the centralized gateway. The data logger gateway features a two-column data storage pattern with timestamps and data packets, ensuring accurate data recording. A realtime clock (RTC) DS3231 with SPI communication is used for timestamping. Data stored on the SD card is transmitted to the cloud via MQTT when connectivity is restored. Our gateway architecture includes a smart agent for data conversion and supports configuration through embedded web pages. It provides an easy and secure solution for IIoT applications, allowing integration with existing operational technology. Overall, our Smart Data Logger gateway enables robust data collection and analysis in challenging industrial environments."
pub.1181101708,"Incorporating Privacy by Design Principles in the Modification of AI Systems in Preventing Breaches across Multiple Environments, Including Public Cloud, Private Cloud, and On-prem","The rapid integration of artificial intelligence (AI) across various sectors has significantly amplified privacy concerns, particularly with the growing reliance on cloud environments. Existing methods often fall short of effectively preventing privacy breaches due to inadequate risk assessment and mitigation strategies. These limitations highlight the necessity for more robust solutions, indicating the importance of Privacy by Design (PbD) principles. This study addresses these gaps by proposing a comprehensive approach to incorporating PbD principles into AI systems to prevent breaches across public, private, and on-prem environments. The proposed work utilizes logistic regression analysis to identify significant predictors of privacy breaches, revealing that both the environment (B =-1.142, p < .001) and severity of vulnerabilities (B = 0.932, p < .01) play crucial roles. Additionally, a strong positive correlation (r = 0.791) between breach detection rates and PbD effectiveness is observed, indicating the need for enhanced detection mechanisms. To support the empirical findings, this study also reviews existing case studies. It conducts a thematic analysis to provide a deeper understanding of the practical challenges and solutions associated with PbD implementation, particularly in healthcare and smart city applications. These analyses serve to supplement the empirical evidence and demonstrate the effectiveness of PbD over other existing methods. The study concludes that implementing PbD principles is critical for achieving robust privacy protection, and the study recommends prioritizing advanced breach detection mechanisms, comprehensive privacy impact assessments, continuous stakeholder engagement, and investment in privacy-enhancing technologies to address privacy risks effectively."
pub.1147007945,An Efficient Secure Data Deduplication and Portability In Distributed Cloud Server Using Whirlpool-Hct And Lf-Wdo,"Distributed Cloud Computing Storage has come up as a service that can expedite data owners (DO) to store their data remotely according to their application or data or file environment. However, insecure data storage, high uploading bandwidth, integration issues of DCS has breached the trustworthiness of the user to store data. In order to
conquer the challenge, the work has developed a data Deduplication and portability-based secure data storage in DCS. The work aids to remove unwanted data and selects the most relevant features to avoid data loss by using GK-QDA Feature Reduction Method and HFG feature selection method. The selected cloud server for the respective data or application is analyzed for redundant data by data duplication using a whirlpool hashing algorithm followed by a hash chaining algorithm. Finally, to minimize the integration issues while moving the encrypted data between the DCS, the work has developed an LF-WDO technique. An experimental analysis has showed an enormous result by achieving a
computation time of 2987 ms as compared to the existing methods"
pub.1129885083,Load Balancing Algorithm to Reduce Make Span in Cloud Computing by Enhanced Firefly Approach,"Cloud computing provides essential environments and services to the organizations through cloud service and its often splendid choice for individual to enterprises. Cloud Servers places an important role to integrate the environment may remotely located in several areas owned by somebody else and own. These are fully managing environment to execute applications in easy, elasticity and dynamic way, either remotely located or employed by users keep with their requirements. Today researchers provide numerous ways to unravel additional challenges in Resource sharing and load Balancing. Load Balancing allows distributing the work load among all the cloud servers evenly to form sure optimization of resource allocation. Load Balancing algorithm developed by using Bio-inspired intelligent firefly approach through the strong ability of worldwide searching. Firefly approach used to develop a solution for Meta heuristic problem like load balancing. Enhanced Firefly algorithm developed by adopting the essential concepts of Firefly approach. Search capability increased to understand global optimized solution by enhanced firefly feedback mechanism and communications. Enhanced Firefly Approach (EFA) evaluated in Cloudsim simulation environment to retrieve execution time, make span time and number of migrations etc. less variance value proves its efficiency. The Results are compared with existing traditional firefly load balancing algorithms, supported the result value of objective function meets the user's requirements and proved its performance."
pub.1094698165,Security Services Lifecycle Management in On-Demand Infrastructure Services Provisioning,"Modern e-Science and high technology industry require high-performance and complicated network and computer infrastructure to support distributed collaborating groups of researchers and applications that should be provisioned on-demand. The effective use and management of the dynamically provisioned services can be achieved by using the Service Delivery Framework (SDF) proposed by TeleManagement Forum that provides a good basis for defining the whole services life cycle management and supporting infrastructure services. The paper discusses conceptual issues, basic requirements and practical suggestions for provisioning consistent security services as a part of the general e-Science infrastructure provisioning, in particular Grid and Cloud based. The proposed Security Services Lifecycle Management (SSLM) model extends the existing frameworks with additional stages such as “Reservation Session Binding” and “Registration and Synchronisation” that specifically target such security issues as the provisioned resources restoration, upgrade or migration and provide a mechanism for remote executing environment and data protection by binding them to the session context. The paper provides a short overview of the existing standards and technologies and refers to the ongoing projects and experience in developing dynamic distributed security services."
pub.1148765443,EFGHNet: A Versatile Image-to-Point Cloud Registration Network for Extreme Outdoor Environment,"We present an accurate and robust image-to-point cloud registration method that is viable in urban and off-road environments. Existing image-to-point cloud registration methods have focused on vehicle platforms along paved roads. Therefore, image-to-point cloud registration on UGV platforms for off-road driving remains an open question. Our objective is to find a versatile solution for image-to-point cloud registration. We present a method that stably estimates a precise transformation between an image and a point cloud using a two-phase method that aligns the two input data in the virtual reference coordinate system (virtual-alignment) and then compares and matches the data to complete the registration (compare-and-match). Our main contribution is the introduction of divide-and-conquer strategies to image-to-point cloud registration. The virtual-alignment phase effectively reduces relative pose differences without cross-modality comparison. The compare-and-match phase divides the process of matching the image and point cloud into the rotation and translation steps. By breaking down the registration problem, it is possible to develop algorithms that can robustly operate in various environments. We performed extensive experiments on four datasets (Rellis-3D, KITTI odometry, nuScenes, and KITTI raw). Experiments cover a variety of situations in which image-to-point cloud registration is applied, from image-based localization in off-road environments to camera-LiDAR extrinsic calibration in urban environments. The experiments demonstrate that the proposed method outperforms the existing methods in accuracy and robustness."
pub.1139212521,The Cross-Domain Identity Authentication Scheme Has no Trusted Authentication Center in the Cloud Environment,"This paper analyzes the existing problems of cross-domain authentication models and proposes a new cross-domain authentication model because there is no trusted authentication center in the cloud environment. The model is based on the certificateless public-key cryptosystem. It takes advantage of the transformability of heavy signature; A resigned proxy is introduced between two different domains for identity transformation. The authentication work is mainly concentrated in the key generation center KA and Proxy in different domains to reduce information interaction and computation between users and cloud service providers. At the same time, this message’s identity information is added to each request message of the user, and check the identification information in the received reply message to confirm whether it is a reply to the application message. This method can prevent malicious attacks and improve security. The results show that the scheme satisfies key security, message unforgeability, and communication information confidentiality."
pub.1181149498,AccessFlex: Flexible Attribute Based Access Control Scheme for Sharing Access Privileges in Cloud Storage,"With the rapid growth of cloud storage adoption, ensuring efficient access control and tamper verification has become a critical concern. This paper presents AccessFlex, a Flexible Attribute-Based Access Control Scheme for Sharing Access Privileges in Cloud Storage. The proposed system is developed using Python and MySQL database servers to achieve robust and secure cloud storage management. AccessFlex introduces three key actors in the system: Admin, Data Owner, and Data User. Admin manages cryptography keys and data consumer attributes. Data Owners can securely upload files to the cloud storage server and define access details for Data Users. Data Users are granted access to authorized files based on their attributes. The AccessFlex system guarantees confidentiality, integrity, and accountability, providing a comprehensive solution for secure file management in cloud storage environments. By leveraging attribute-based access control, masking, and encryption techniques, the system offers flexibility and scalability, catering to diverse access requirements while safeguarding sensitive data from unauthorized access and modifications. For data security existing RSA encryption techniques are used."
pub.1175388063,Pose Estimation of Mobile Robot Using Image and Point-Cloud Data,"In Simultaneous Localization and Mapping (SLAM) techniques, the precise estimation of the initial pose of a mobile robot presents a significant challenge. The initial pose is crucial as it can significantly reduce the accumulated errors in SLAM. Despite various advancements in pose estimation, accurately determining the initial pose in different application scenarios continues to be a complex task, often requiring techniques such as loop closure. However, loop closure detection may not always be feasible, as it is time-consuming and relies on the robot revisiting previous paths or its initial starting position. In essence, most localization methods such as SLAM critically require the initial pose to estimate the mobile robot’s pose and the more precise initial pose makes the localization process to converge quickly. Addressing these persistent challenges, this paper proposes a novel method that utilizes both image and point cloud data, allowing for easy adaptation across diverse and dynamic environments. The method integrates well-known technologies such as NetVLAD, RootSIFT, 5-Point, and Iterative Closest Point (ICP) algorithms. This approach not only addresses the initial pose estimation problem but also provides an alternative to existing landmarks, enhancing adaptability to diverse and dynamic environments. NetVLAD is utilized to find the most similar image data in stored images by comparing the image captured by the mobile robot with the stored images with pose data. The relative pose is estimated by applying the RootSIFT and 5-Point algorithm, and ICP algorithm to the found image and point cloud data, respectively. This method determines the final pose of the mobile robot by combining each relative pose extracted from image data and point cloud data through weighted integration. The effectiveness of the proposed method is verified by comparing it with existing deep learning-based pose estimation methods. This method can accurately estimate poses, including the initial pose, using much less data than existing deep learning methods, even in diverse and dynamic environments. Furthermore, this method is applicable not only when using image data alone but also when both image and point cloud data are available."
pub.1072776143,Efficient parallel implementation of the SHRiMP sequence alignment tool using MapReduce,"With the advent of ultra high-throughput DNA sequencing technologies used in Next-Generation Sequencing (NGS) machines, we are facing a daunting new era in petabyte scale bioinformatics data. The enormous amounts of data produced by NGS machines lead to storage, scalability, and performance challenges. At the same time, cloud computing architectures are rapidly emerging as robust and economical solutions to high performance computing of all kinds. To date, these architectures have had limited impact on the sequence alignment problem, whereby sequence reads must be compared to a reference genome. In this research, we present a methodology for efficient transformation of one of the recently developed NGS alignment tools, SHRiMP, into the cloud environment based on the MapReduce programming model. Critical to the function and performance of our methodology is the implementation of several techniques and mechanisms for facilitating the task of porting the SHRiMP sequence alignment tool into the cloud. These techniques and mechanisms allow the ""cloudified"" SHRiMP to run as a black box within the MapReduce model, without the need for building new parallel algorithms or recoding this tool from scratch. The approach is based on the MapReduce parallel programming model, its open source implementation Hadoop, and its underlying distributed file system (HDFS). The deployment of the developed methodology utilizes the cloud infrastructure installed at Qatar University. Experimental results demonstrate that multiplexing large-scale SHRiMP sequence alignment jobs in parallel using the MapReduce framework dramatically improves the performance when the user utilizes the resources provided by the cloud. In conclusion, using cloud computing for NGS data analysis is a viable and efficient alternative to analyzing data on in-house compute clusters. The efficiency and flexibility of the cloud computing environments and the MapReduce programming model provide a powerful version of the SHRiMP sequence alignment tool with a considerable boost. Using this methodology, ordinary biologists can perform the computationally demanding sequence alignment tasks without the need to delve deep into server and database management, without the complexities and hassles of running jobs on grids and clusters, and without the need to modify the existing code in order to adapt it for parallel processing."
pub.1111058301,Smart integrated IoT healthcare system for cancer care,"The emergence of the internet of things (IoT) has drastically influenced and shaped the world of technology in the contexts of connectivity, interconnectivity, and interoperability with smart connected sensors, objects, devices, data, and applications. In fact, IoT has brought notable impacts on the global economy and human experience that span from industry to industry in a variety of application domains, including healthcare. With IoT, it is expected to facilitate a seamless interaction and communication of objects (devices) with humans in the environment. Therefore, it is imperative to embrace the potentials and benefits of IoT technology in healthcare delivery to ensure saving lives and to improve the quality of life using smart connected devices. In this paper, we focus on the IoT based healthcare system for cancer care services and business analytics/cloud services and also propose the adoption and implementation of IoT/WSN technology to augment the existing treatment options to deliver healthcare solution. Here, the business analytics/cloud services constitute the enablers for actionable insights, decision making, data transmission and reporting for enhancing cancer treatments. Furthermore, we propose a variety of frameworks and architectures to illustrate and support the functional IoT-based solution that is being considered or utilized in our proposed smart healthcare solution for cancer care services. Finally, it will be important to understand and discuss some security issues and operational challenges that have characterized the IoT-enabled healthcare system."
pub.1033032290,Incentivising resource sharing in social clouds,"Summary Social Clouds provide the capability to share resources among participants within a social network—leveraging on the trust relationships already existing between such participants. In such a system, users are able to trade resources between each other rather than make use of capability offered at a (centralized) data center. Although such an environment has significant potential for improving resource utilization and making available additional capacity that remains dormant, incentives for sharing remain an important hurdle limiting its effective. In this paper, we utilize the socioeconomic model proposed by Silvio Gesell to demonstrate how a ‘virtual currency’ can be used to incentivise sharing of resources within a ‘community’. We subsequently demonstrate, through simulations, the benefit provided to participants within such a community using a variety of economic (such as overall credits gained) and technical (number of successfully completed transactions) metrics. Further, we describe our implementation of such a Social Cloud using CometCloud. CometCloud is an autonomic computing engine for cloud and grid environments. It supports highly heterogeneous and dynamic federated cloud/Grid infrastructures, integration of public/private clouds and autonomic cloudbursts. We demonstrate the implementation of two designs on the basis of the master/worker approach: (i) one tuple space per cluster and (ii) one coordination tuple space and multiple transient spaces—one per each cluster. Finally, we discuss an extended version of our Social Cloud model where intermediary relay nodes take on more active roles as traders in a transaction. Copyright © 2013 John Wiley & Sons, Ltd."
pub.1159753879,Knowledge discovery of suspicious objects using hybrid approach with video clips and UAV images in distributed environments: a novel approach,"The current video surveillance systems that employ manual face detection and automatic face recognition in unmanned aerial vehicles (UAVs) have limited accuracy, typically below 90%. This is due to the utilization of a small number of Eigenfaces for principal component analysis transformation. Detecting faces in cloud-based Internet of Things (IoT) video frames involves separating video/image windows into two classes: one with faces (to train the surroundings) and the other with matches (in the foreground). The face detection process is further complicated by geometries, inconsistent image/video qualities, and lighting conditions, as well as the possibility of partial occlusion and disguises. Moreover, a fully automated iris image-based face recognition and detection system could prove useful in surveillance applications such as automated teller machine user security, whereas an automated face recognition system using UAV video frames in a cloud-integrated-IoT-based distributed computing environment is better suited for mug-shot matching and surveillance of distrustful objects. This is because controlled conditions are present when capturing mug shots. The proposed hybrid approach was rigorously tested, and the experimental results suggest that its real-world performance will be far more accurate than existing systems. Intelligent surveillance knowledge databases contain vast amounts of information on landmarks, terrain, events, activities, and entities that need to be efficiently and accurately processed and disseminated. Therefore, discovering the appropriate knowledge to detect distrustful objects plays a crucial role in future analysis. The experimental findings indicate that the proposed hybrid approach has high accuracy, low overall and average error rates, and very high average recall rates for benchmark and self-generated datasets. These results demonstrate the robustness, efficiency, and reliability of the authors’ choices. Although further improvements in results are possible, the proposed approach is sufficient for detecting distrustful objects."
pub.1119912245,Network Defragmentation in Virtualized Data Centers,"Network virtualization is an extensively used approach to allow multiple tenants with different network architectures and services to coexist on a shared data center infrastructure. Core to its realization is the mapping (or embedding) of virtual networks onto the underlying substrate infrastructure. Existing approaches are not suitable for cloud environments as they lack its most fundamental requirement: elasticity. To address this issue, we introduce two new network primitives –expand and contract– which allow virtual networks to scale up and down. Mapping and scaling virtual networks over time, however, introduces fragmentation in the substrate network. This is akin to fragmentation in a file system where files are not laid out in contiguous physical blocks of the storage device. This problem impacts network performance and reliability for tenants and their applications. Instead of further improving embedding algorithms to tackle this problem, in this work, we present a yet unexplored approach: leveraging network migration techniques to defragment the network. We introduce network defragmentation as a new management primitive and propose algorithms to materialize it. We show through extensive simulations that our techniques significantly improve network performance while maintaining high utilization of the infrastructure, thus increasing provider revenue. On average, using defragmentation leads to 20% reduction in path length and utilization and cuts the number of very long paths (longer than half of the network diameter) between 52% and 62%. Moreover, it doubles the number of servers utilized by 50% or less as a result of consolidation."
pub.1161675103,Design and Implementation of an IoT-Integrated Smart Home System with End-to-End Security Using Blockchain Technology,"Smart homes with IoT and cloud integration have many security challenges and scalability issues. The reason behind security issues is that the environment is distributed in nature and thousands of devices may participate in the network. When any device is compromised or when security credentials are stolen, the whole system will be exposed to security risks. The existing solutions to the problem of security in smart homes are not scalable and they do have loopholes like lack of standards, prone to DDoS attacks and other attacks. They are not able to provide end-to-end solutions to the transactions in IoT-enabled smart homes. There is a need for end-to-end security in such systems. Since blockchain is the distributed ledger of transactions that is accessible to all legitimate devices, it can help devices to be smart enough to prevent any security attacks. In other words, the smart home with all its participating devices and data are protected with blockchain technology integration. This chapter presents the design and implementation of an IoT-enabled smart home system with blockchain technology for end-to-end security, irrespective of the make and platform of applications and devices that participate in distributed computing. It presents experimental results in terms of latency for block creation and data retrieval. Smart homes with Internet of Things (IoT) and cloud integration have many security challenges and scalability issues. This chapter presents the design and implementation of an IoT-enabled smart home system with blockchain technology for end-to-end security, irrespective of the make and platform of applications and devices that participate in distributed computing. It also presents experimental results in terms of latency for block creation and data retrieval. The usage of an open-standard distributed IoT solution can solve many problems that are associated with centralized approaches. Smart homes with IoT and cloud integration have many security challenges and scalability issues. The reason behind security issues is that the environment is distributed in nature and thousands of devices may participate in the network. The IoT-based smart home–related transactions are saved to a cloud-based distributed ledger in blockchain. The system has a provision to create a hash and encrypt the transactions prior to sending to the blockchain."
pub.1008760926,Feedback based Trust Management for Cloud Environment,"Trust is most challenging issue for the growth and adoption of cloud computing but the trust management systems are becoming apparent and promising technology to better the e-commerce providers and consumers relationship. Various trust models are proposed for managing trust feedbacks but reliability of trust feedback is generally ignored. It is very difficult to maintain trust feedback due to dynamic nature of cloud and unpredictable number of cloud users for service. Although it is very important to find the reliability of trust feedbacks because unreliable trust feedback can generate inaccurate trust results. This paper introduces the challenges and threats that can compromise the trust system. We proposed the trust management framework that can effectively filter out the unreliable feedback based on the behavior of cloud users, ageing factor, majority feedbacks and exogenous method and measure the trustworthiness of service provider. This proposed framework can produce accurate trust results and valuated and compared with existing system."
pub.1169390988,Impact of 5G on Internet of Things and its Challenges,"In this era of rapid technological growth, the Internet of Things (IOT) refers to a system of interrelated, internet-connected objects that are able to collect and transfer data over a wireless network without human intervention. We see the IOT as billions of smart, connected things (a sort of ―universal global neural network in the cloud) that will encompass every aspect of our lives, and its foundation is the intelligence that embedded processing provides. The IOT is comprised of smart machines interacting and communicating with other machines, objects, environments and infrastructures. As a result, huge volumes of data are being generated, and that data is being processed into useful actions that can ―command and control things to make our lives much easier and safer—and to reduce our impact on the environment. The creativity of this new era is boundless, with amazing potential to improve our lives. The following thesis is an extensive reference to the possibilities, utility, applications and the evolution of the Internet of Things. The rollout of commercial 5G cellular networks has commenced, marking a significant milestone in the adoption of 5G and the Internet of Things (IoT). This adoption is propelled by various factors, such as rising consumer and enterprise demands, coupled with the availability of cost-effective devices. Notably, substantial investments by operators in 5G technology, spectrum allocation, and infrastructure development, alongside the establishment of global standards, are key drivers fuelling growth and generating heightened market interest in the IoT. The substantial investments made by operators in 5G technology, spectrum allocation, and infrastructure, coupled with the adoption of global standards, play a pivotal role in stimulating growth and generating heightened market interest in the Internet of Things (IoT). The ongoing evolution of 5G mobile cellular networks, derived from existing 4G frameworks, underscores their adaptability to diverse use cases, both present and future-oriented. With a forward- looking approach, 5G infrastructure is poised to address current demands like smart energy applications while proactively accommodating forthcoming innovations such as autonomous vehicles. In navigating this technological transition, mobile operators face the challenge of ensuring that their networks remain adept at supporting a spectrum of present and future use cases. Strategic investment management is imperative for operators to seamlessly transition their networks to 5G while upholding the continuity of service for their customers Key Words: Scalability, safety, security, rapid growth, interoperability, devices, interconnections, 5G with Internet of things, healthcare, smart city"
pub.1154208268,Enabling Methodologies for Renewable and Sustainable Energy,"This book aims to provide practical aspects of, and an introduction to, the applications of various technological advancement tools, such as AI, machine learning to design, big data, cloud computing, and IoT, to model, characterize, optimize, forecast, and do performance prediction of renewable energy exploitation. It further discusses new avenues for energy sources such as hydrogen energy generation and energy storage technologies including existing policies and case studies for a better understanding of renewable energy generation. Features: Covers technologies considered to explore, predict, and perform operation and maintenance of renewable energy sources. Aids in the design and use of renewable energy sources, including the application of artificial intelligence in a real-time environment. Includes IoT, cloud computing, big data, smart grid, and different optimization techniques for resource forecasting, installation, operation, and optimization of energy. Discusses the principle of integration/hybridization of renewable energy sources along with their optimization based on energy requirements. Reviews the concepts and challenges involved in the implementation of smart grids. Covers technologies considered to explore, predict, and perform operation and maintenance of renewable energy sources. Aids in the design and use of renewable energy sources, including the application of artificial intelligence in a real-time environment. Includes IoT, cloud computing, big data, smart grid, and different optimization techniques for resource forecasting, installation, operation, and optimization of energy. Discusses the principle of integration/hybridization of renewable energy sources along with their optimization based on energy requirements. Reviews the concepts and challenges involved in the implementation of smart grids. This book is aimed at researchers and graduate students in renewable energy engineering, computer and mechanical engineering, novel technologies, and intelligent systems."
pub.1100140874,Cloudysme: An Ontological Framework for Aiding SMEs Adoption of SaaS in a Cloud Environment,"Adoption techniques are widely applied in and for cloud service usage to improve the slow acceptance rate of cloud services by SMEs. In such context, a well-understood problem is finding a suitable service from the vast number of services offering similar packages to satisfy user requirements such as security, cost, trust and operating systems compatibility has become a big challenge. However, a major drawback of existing techniques such as frameworks, web search, decision support tools, management models, ontology models and agent technology is that they are restricted to a specific task or they replicate service provider offerings. In this paper, we present Cloudysme a cloud service adoption solution, a middleware that is capable of aiding the decision making process for SMEs adoption of cloud services. Using a case study of SaaS storage services offerings by cloud providers, we introduce a new formalism for judging the superiority of one service attribute over another, we propose an extended version of pairwise comparison and Analytical hierarchical Process (AHP) which is a traditional multi-criteria decision method (MCDM) in solving complex comparisons. We solve the issue of service recommendation by introducing an acceptable standard for each service attribute and propose a protocol using rational relationships for aiding cloud service ranking process. We tackle the issue of specific tasking by using a set of concepts and associated semantic rules to rank and retrieve user requirements. We promote a knowledge engineering approach for natural language processing by using terms and conditions in translating human sentences to machine readable language. Finally, we implement our system using 30 SMEs as a pivotal study. We prove that the use of semantic rules within an ontology can tackle the issue of specific tasking."
pub.1137832326,Spaceborne differential absorption radar water vapor retrieval capabilities in tropical and subtropical boundary layer cloud regimes,"Differential absorption radar (DAR) near the 183 GHz water vapor absorption line is an emerging measurement technique for humidity profiling inside of clouds and precipitation with high vertical resolution, as well as for measuring integrated water vapor (IWV) in clear air regions. For radar transmit frequencies on the water line flank away from the highly attenuating line center, the DAR system becomes most sensitive to water vapor in the planetary boundary layer (PBL), which is a region of the atmosphere that is poorly resolved in the vertical by existing spaceborne humidity and temperature profiling instruments. In this work, we present a high-fidelity, end-to-end simulation framework for notional spaceborne DAR instruments that feature realistically achievable radar performance metrics, and apply this simulator to assess DAR's PBL humidity observation capabilities. Both the assumed instrument parameters and radar retrieval algorithm leverage recent technology and algorithm development for an existing airborne DAR instrument. To showcase the capabilities of DAR for humidity observations in a variety of relevant PBL settings, we implement the instrument simulator in the context of large eddy simulations (LES) of 5 different cloud regimes throughout the trade-wind subtropical-to-tropical cloud transition. Three distinct DAR humidity observations are investigated: IWV between the top of the atmosphere and the first detected cloud bin or Earth's surface; in-cloud water vapor profiles with 200 meter vertical resolution; and IWV between the last detected cloud bin and the Earth's surface, which can provide a precise measurement of the sub-cloud humidity. We provide a thorough assessment of the systematic and random errors for all 3 measurement products for each LES case, and analyze the humidity precision scaling with along-track measurement integration. While retrieval performance depends greatly on the specific cloud regime, we find generally that for a radar with cross-track scanning capability, in-cloud profiles with 200 m vertical resolution and 10–20 % uncertainty can be retrieved for horizontal integration distances of 100–200 km. Furthermore, column IWV can be retrieved with 10 % uncertainty for 10–20 km of horizontal integration. Finally, we provide some example science applications of the simulated DAR observations, including estimating near-surface relative humidity using the cloud-to-surface column IWV, and inferring in-cloud temperature profiles from the DAR water vapor profiles by assuming a fully saturated environment."
pub.1148159668,An Implementation of Modified Blowfish Technique with Honey Bee Behavior Optimization for Load Balancing in Cloud System Environment,"Cloud computing is a system that allows data to be saved in the cloud on a virtual worker. Outsiders and virtual machines in the cloud worker supplier played a critical part in efficiently storing and accessing information. Security, access control, and load balancing are critical challenges in cloud engineering. In the past, various solutions for adjusting cloud load have been proposed. Operator-based burden adjustment calculation surpassed all other offered CPU use, cost, and idle time strategies. The productivity of the specialist-based load adjustment computation decreased when any of the client hubs changed regions. Experimental outcomes show that Modified-HBB-LB performs better than the existing load balancing strategies such as HBB-LB, DLB, FCFS, WRR, HDLB, and FIFO by achieving the load balance of the complete system. The Modified-HBB-LB technique reduces the number of migrations tasks (30%, 25% and 20%) as compared to HDLB, DLB, and HBB-LB. The proposed Modified-HBB-LB technique maintains the 3-5% higher performance levels on makespan, completion, and response time as compared to existing comparative techniques."
pub.1123225710,Modelling and simulation of security-aware task scheduling in cloud computing based on Blockchain technology,"Although a lot of work has been done in the domain, tasks scheduling and resource allocation in cloud computing remain the challenging problems for both industry and academia. Security in scheduling in highly distributed computing environments is one of the most important criteria in the era of personalization of the cloud services. Blockchain became recently a promising technology for integration with the cloud clusters and improvement of the security of cloud transactions and access to data and application codes. In this paper, we developed a new model of the cloud scheduler based on the blockchain technology. Differently to the other similar models, we tried to offload the implementation of the blockchain modules. We developed a novel ’proof–of–schedule’ consensus algorithm (instead of ’proof–of–work’) and used the Stackelberg games for the improvement of the approval of the generated schedules. The developed model has been experimentally simulated and validated by using the new original cloud simulator. The proposed Blockchain Scheduler was also compared with other selected cloud schedulers. The experiments shows that the applied approach improved significantly the efficiency of prepared schedules, in most cases, simulator returns a schedule with better makespan than existing individual scheduling modules."
pub.1154091332,An Improved Approach for Load Balancing among Virtual Machines in Cloud Environment,"Cloud computing has shown noteworthy evolution in information technology. Users can enjoy various services of cloud technology only if internet connection is available. In cloud computing, load balancing considered as a fundamental issue that has confronted researchers in this domain. Load balancing basically works by allotting fair and efficient work among computing resources which ultimately achieve high user satisfaction and raises systems productivity. Many load-balancing techniques made efforts to resolve this problem using metaheuristics algorithm, and amplify the operation and efficiency of systems. In this paper, existing load balancing techniques has been discussed and research gaps in existing lirture hasbeeb discussed. Also a new technique has been proposed called IG-GWO - Inquisitive Genetic Grey Wolf Optimization algorithm using combination of Grey wolf optimization (GWO) algorithm and Genetic algorithm."
pub.1142770910,Integration of micro-services as components in modeling environments for low code development,"Low code development environments are gaining attention due to their potential as a development paradigm for very large scale adoption in the future IT. In this paper, we propose a method to extend the (application) Domain Specific Languages supported by two low code development environments based on formal models, namely DIME (native Java) and Pyro (native Python), to include functionalities hosted on heterogeneous technologies and platforms. For this we follow the analogy of micro services. After this integration, both environments can leverage the communication with pre-existing remote RESTful and enterprise systems’ services, in our case Amazon Web Services (AWS) (but this can be easily generalized to other cloud platforms). Developers can this way utilize within DIME and Pyro the potential of sophisticated services, potentially the entire Python and AWS ecosystems, as libraries of drag and drop components in their model driven, low-code style. The new DSLs are made available in DIME and Pyro as collections of implemented SIBs and blocks. Due to the specific capabilities and checks underlying the DIME and Pyro platforms, the individual DSL functionalities are automatically validated for semantic and syntactical errors in both environments."
pub.1129932118,ERP Migration Challenges and Solution Approach for Digital Transformation To SAP S/4HANA For SAP Customers,"Dynamic IT environment, increased spending on cloud-based applications and ever- growing power of digital transformation is forcing companies to embrace change. Digital transformation is the process of a company undergoing multiple changes internal and external leveraging digital technologies to become more agile, optimized and efficient in its business operations. Digital business transformation is ubiquitous and ERP market is no exception. SAP, the market leader(in revenue terms) has decided to phase out support for SAP ECC in next 5 years and its existing install base customers are bewildered in deciding how they should migrate from earlier versions to S/4HANA. Migration gives the organization a chance to streamline its processes, rationalize interfaces, decommission custom code, enforce standard SAP modules to become more efficient. However, migration from SAP ECC to SAP S/4HANA comes with its own set of unique challenges from organization perspective, vendor perspective, end user perspective and the consultant’s perspective. This paper deals with challenges from organization perspective in migrating to SAP S/4HANA and what approach should it take in making the transition to SAP S/4HANA seamless. This paper elaborates how an organization should formulate the SAP S/4HANA migration plan to ensure data integrity is maintained."
pub.1170552579,"Beyond Automation: Exploring the Synergy of Cloud, AI, Machine Learning, and IoT for Intelligent Systems","In the rapidly evolving landscape of Industry 4.0 and beyond, the amalgamation of Cloud Computing, Artificial Intelligence (AI), Machine Learning (ML), and the Internet of Things (IoT) has emerged as a transformative force capable of elevating traditional automation to intelligent systems. This research paper delves into the profound potential of synergizing these advanced technologies, aiming to surpass the limitations of rule-based automation and foster a new era of adaptability, efficiency, and innovation.
 The study begins by articulating the escalating demand for intelligent systems that can dynamically respond to complex and ever-changing environments. The integration of Cloud, AI, ML, and IoT is posited as a solution to the constraints of conventional automation, offering the ability to process vast datasets, make informed decisions, and continuously learn from interactions.
 A comprehensive review of existing approaches and related works forms the foundation of this research. The analysis encompasses diverse applications, ranging from smart manufacturing to healthcare, showcasing the ways in which individual technologies have been leveraged. By scrutinizing these approaches, the study aims to distill the strengths and weaknesses, paving the way for a novel methodology that harnesses their collective power.
 Identifying the limitations of current approaches, such as scalability challenges, real-time processing bottlenecks, and interoperability issues, serves as a critical precursor to the proposed methodology. The paper presents a holistic strategy that intricately weaves together Cloud, AI, ML, and IoT into a unified framework. The architectural design, data flow, and interaction mechanisms are elucidated to demonstrate how this synergy can overcome existing challenges, providing adaptability and innovation in diverse domains.
 Empirical results derived from the implementation of the proposed methodology are presented and rigorously analyzed in the Results and Discussion section. Performance metrics, efficiency gains, and the impact on decision-making processes are thoroughly examined. Real-world case studies exemplify the effectiveness of the integrated approach, offering tangible evidence of its potential applications.
 Concluding remarks encapsulate the key findings, emphasizing the significance of the research in shaping the trajectory of intelligent systems. The broader implications of the proposed methodology across various industries are discussed, and avenues for future work are suggested. As technologies continue to evolve, the proposed methodology serves as a foundation for ongoing exploration, adaptation, and integration with emerging technologies.
 In essence, this research paper offers a detailed exploration into the synergy of Cloud, AI, ML, and IoT, paving the way for a new era of intelligent systems that transcend the limitations of traditional automation, fostering adaptability, efficiency, and innovation in an ever-chang"
pub.1141712834,Spaceborne differential absorption radar water vapor retrieval capabilities in tropical and subtropical boundary layer cloud regimes,"Abstract. Differential absorption radar (DAR) near the 183 GHz water vapor absorption line is an emerging measurement technique for humidity profiling inside of clouds and precipitation with high vertical resolution, as well as for measuring integrated water vapor (IWV) in clear-air regions. For radar transmit frequencies on the water line flank away from the highly attenuating line center, the DAR system becomes most sensitive to water vapor in the planetary boundary layer (PBL), which is a region of the atmosphere that is poorly resolved in the vertical by existing spaceborne humidity and temperature profiling instruments. In this work, we present a high-fidelity, end-to-end simulation framework for notional spaceborne DAR instruments that feature realistically achievable radar performance metrics and apply this simulator to assess DAR's PBL humidity observation capabilities. Both the assumed instrument parameters and radar retrieval algorithm leverage recent technology and algorithm development for an existing airborne DAR instrument. To showcase the capabilities of DAR for humidity observations in a variety of relevant PBL settings, we implement the instrument simulator in the context of large eddy simulations (LESs) of five different cloud regimes throughout the trade-wind subtropical-to-tropical cloud transition. Three distinct DAR humidity observations are investigated: IWV between the top of the atmosphere and the first detected cloud bin or Earth's surface; in-cloud water vapor profiles with 200 meter vertical resolution; and IWV between the last detected cloud bin and the Earth's surface, which can provide a precise measurement of the sub-cloud humidity. We provide a thorough assessment of the systematic and random errors for all three measurement products for each LES case and analyze the humidity precision scaling with along-track measurement integration. While retrieval performance depends greatly on the specific cloud regime, we find generally that for a radar with cross-track scanning capability, in-cloud profiles with 200 m vertical resolution and 10 %–20 % uncertainty can be retrieved for horizontal integration distances of 100–200 km. Furthermore, column IWV can be retrieved with 10 % uncertainty for 10–20 km of horizontal integration. Finally, we provide some example science applications of the simulated DAR observations, including estimating near-surface relative humidity using the cloud-to-surface column IWV and inferring in-cloud temperature profiles from the DAR water vapor profiles by assuming a fully saturated environment."
pub.1042993694,A Cloud-Manager-Based Re-Encryption Scheme for Mobile Users in Cloud Environment: a Hybrid Approach,"Cloud computing is an emerging computing paradigm that offers on-demand, flexible, and elastic computational and storage services for the end-users. The small and medium-sized business organization having limited budget can enjoy the scalable services of the cloud. However, the migration of the organizational data on the cloud raises security and privacy issues. To keep the data confidential, the data should be encrypted using such cryptography method that provides fine-grained and efficient access for uploaded data without affecting the scalability of the system. In mobile cloud computing environment, the selected scheme should be computationally secure and must have capability for offloading computational intensive security operations on the cloud in a trusted mode due to the resource constraint mobile devices. The existing manager-based re-encryption and cloud-based re-encryption schemes are computationally secured and capable to offload the computationally intensive data access operations on the trusted entity/cloud. Despite the offloading of the data access operations in manager-based re-encryption and cloud-based re-encryption schemes, the mobile user still performs computationally intensive paring-based encryption and decryption operations using limited capabilities of mobile device. In this paper, we proposed Cloud-Manager-based Re-encryption Scheme (CMReS) that combines the characteristics of manager-based re-encryption and cloud-based re-encryption for providing the better security services with minimum processing burden on the mobile device. The experimental results indicate that the proposed cloud-manager-based re-encryption scheme shows significant improvement in turnaround time, energy consumption, and resources utilization on the mobile device as compared to existing re-encryption schemes."
pub.1174676038,"A Comprehensive System Architecture using Field Programmable Gate Arrays Technology, Dijkstra's Algorithm, and Edge Computing for Emergency Response in Smart Cities","Efficient emergency response systems are crucial for smart cities. But their
implementation is highly challenging, particularly in regions like Chad where
infrastructural constraints are prevalent. The urgency for optimized response
times and resource allocation in emergency scenarios is magnified in these
contexts, yet existing solutions often assume robust infrastructure and
uninterrupted connectivity, which is not always available. Most of the time,
they are based on system architectures pre-designed for other purposes. This
paper addresses these critical challenges by proposing a comprehensive system
architecture that integrates Field Programmable Gate Arrays (FPGAs), Dijkstra's
algorithm, and Edge Computing. The objective is to enhance emergency response
through accelerated route planning and resource allocation, addressing gaps
left by traditional cloud-based systems. Methodologically, key characteristics
of the desired system are identified, then its components are described and
their integration is explained; the system leverages FPGA-based computations
and a distributed implementation of Dijkstra's algorithm to compute the
shortest paths rapidly, while Edge Computing ensures decentralized and
resilient processing. A theoretical analysis highlights promising improvements
in response times and resource management. The proposed system architecture not
only enhances emergency response efficiency but is also adaptable to
infrastructural constraints of Chadian-like environments."
pub.1169270559,"Seamless Decision-Making in the Big Data Era: A Modular Approach to Integrating IoT, Cloud Computing, and Data Lakes","The integration of big data with cutting-edge technologies like IoT and Cloud Computing has profoundly influenced various aspects of modern life, including everyday service processes. To facilitate data-driven decision-making, big data analytics—focused on identifying patterns, trends, and correlations in large data sets—is indispensable. While traditional statistical techniques are useful, new tools and infrastructures such as Hadoop, Spark, and NoSQL are essential to tackle big data challenges. However, modifying the existing environment can be impractical, especially in production settings, due to the need for significant investment and specialized expertise. This article presents a novel computational paradigm that adds a decision-making layer atop existing systems for data analysis, eliminating the need to alter the environment. The approach treats the current information system as a data lake and introduces a new data recovery layer through web services, drawing inspiration from big data technologies like MapReduce. This system offers the advantage of being modular, reusable, and universally compatible, making it an independent decisional framework that can work with any information system or data source."
pub.1141081530,Verifiable Access Technology of Hybrid Database in Distributed System Under Big Data,"Targeting at the problem that the existing hybrid database solutions can not simultaneously realize the client’s verification and access function to the encrypted data in the public cloud database and the cross database query function in the heterogeneous database environment, this paper designs an access and integration middleware to realize the client’s verification and access function to the heterogeneous hybrid database in the public cloud and the cross database query function. Access and integration middleware is composed of verification access component and heterogeneous database data integration method. The verification and access component reconstructs the protocol interaction process based on MySQL protocol, and encrypts and decrypts the encrypted data in the public cloud database through the existing hybrid database scheme, so as to realize the client’s verification and access to the encrypted data in the cloud database. The heterogeneous database data integration method unifies the data from different types of databases, and then integrates them into the same database, and uses this database for query operation to realize cross database query. This paper first analyzes the hybrid database system model, then designs from the aspects of MySQL protocol analysis, verification access, data integration, and finally analyzes the access and integration middleware from the aspects of functionality and security."
pub.1094599907,Online Adaptation Models for Resource Usage Prediction in Cloud Network,"Cloud computing provides rapid on-demand access to shared services over the Internet. Elasticity is a key feature of cloud that allows the system to dynamically adapt to workload changes such that the resource allocation sufficiently models the resource usage as close as possible. It is a highly challenging job as a number of users enter and leave the cloud environment dynamically. Predicting the usage of different resources in advance helps the service providers in better capacity planning and satisfy the needs of users. In this paper, we analyze the multi-step ahead CPU usage predictions of different existing time series prediction models. To model the highly varying cloud workloads and counter the effect of error propagation in iterative multi-step ahead predictions we proposed and analyzed online adaptation of time series CPU usage prediction models. We also use fractional differencing to capture long range dependence in the data. We evaluate the proposed adaptive models on Google cluster trace."
pub.1136260507,"Energy, performance and cost efficient cloud datacentres: A survey","In major Information Technology (IT) companies such as Google, Rackspace and Amazon Web Services (AWS), virtualization and containerization technologies are usually used to execute customers’ workloads and applications — as part of their cloud computing services offering. The computational resources are provided through large-scale datacentres, which consume substantial amount of energy and, consequently, affect our environment with global warming. Cloud datacentres have become a backbone for today’s business and economy, which are the fastest-growing electricity consumers, globally. Numerous studies suggest that ∼ 30% of the US datacentres are comatose and the others are grossly less-utilized, which make it possible to save energy through technologies like virtualization and containerization. These technologies provide support for allocation and consolidation of workloads on appropriate resources. However, consolidation comprises migrations of virtual machines (VMs), containers and/or applications, depending on the underlying virtualization method; that are expensive in terms of energy consumption, performance degradation, and therefore, costs which is mostly not accounted for in many existing models, and, possibly, it could be more energy and performance efficient not to consolidate. This paper describes energy consumption and performance, therefore, cost issues of large-scale datacentres. Besides, we cover various methods for energy and performance efficient distributed systems, clouds and datacentres. We elaborate energy efficiency methods at three different levels: hardware; resource management; and applications. Besides these, different performance management techniques are mapped onto taxonomies and described in details. In last, energy, performance and cost management techniques, at geographically distributed and multi-access edge computing platforms, are described along with critical discussion."
pub.1181963168,Cloud guard: Optimizing intrusion detection for fortifying privacy defenses with an optimized self adaptive physics-informed neural network,"The rapid expansion of cloud computing is fueled by its vast processing capabilities and ability to support data-intensive applications, which highlights the urgent demand for advanced and reliable security solutions. Despite its benefits, the persistent security problems prevent wider use, particularly when it comes to handling sensitive data. To address these, effective intrusion detection is essential for maintaining data confidentiality and integrity. The intricacy of dynamic cloud environments surpasses the capacity of existing methodologies, which underscoring the necessity for advanced solutions. Therefore, a novel approach called Cloud Guard: Optimizing Intrusion Detection for Fortifying Privacy Defenses with an Optimized Self Adaptive Physics-Informed Neural Network (CG-FPD-SAPINN) is proposed in this paper to enhance the intrusion detection in cloud environments. This method leverages Self Adaptive Physics-Informed Neural Network (SAPINN) optimized with the Piranha Search Optimization Algorithm (PFOA). The process begins with data acquisition from the BOT-IoT dataset and proceeds through pre-processing and feature selection phases. Hierarchical Manta Ray Foraging Optimization (HMRFO) is used for optimal feature selection. Then, the SAPINN is used for classification, and its network's weights are optimized by PFOA to improve the efficacy of intrusion detection. The proposed CG-FPD-SAPINN technique is executed in Python using performance metrics, like accuracy, precision, recall, specificity, f-Score, computation time, error rate, latency, throughput, scalability, and resource consumption. The comparative analysis demonstrates that the CG-FPD-SAPINN method outperforms other existing approaches. It exhibits higher recall and Receiver Operating Characteristic (ROC) curve while achieving reduced computation time and latency. The proposed CG-FPD-SAPINN method provides a feasible solution for increasing the safety in cloud computing environments, particularly in intrusion detection. By using advanced techniques, such as SAPINN and PFOA, the method demonstrates better performance in detecting intrusions, thus addressing security concerns and facilitating the secure adoption of cloud technologies."
pub.1170950867,"A CRITICAL REVIEW OF ERP SYSTEMS IMPLEMENTATION IN MULTINATIONAL CORPORATIONS: TRENDS, CHALLENGES, AND FUTURE DIRECTIONS","Enterprise Resource Planning (ERP) systems have become indispensable tools for managing complex business processes in multinational corporations (MNCs). This paper presents a critical review of the trends, challenges, and future directions associated with ERP systems implementation in the context of MNCs. The study synthesizes existing literature to offer a comprehensive understanding of the evolving landscape of ERP adoption in a globalized business environment. The trends in ERP implementation within MNCs underscore the increasing reliance on integrated software solutions to enhance operational efficiency and facilitate seamless communication across diverse geographical locations. Notably, the integration of emerging technologies such as artificial intelligence, machine learning, and blockchain into ERP systems has gained traction, presenting new opportunities and complexities for MNCs. However, the implementation of ERP systems in MNCs is not without challenges. Cultural diversity, varying regulatory frameworks, and differences in business practices across regions pose significant obstacles to successful deployment. Additionally, the sheer scale and complexity of MNCs' operations demand careful consideration of customization needs, scalability, and alignment with organizational goals. The critical review highlights the need for a strategic approach to ERP implementation in MNCs. Successful cases often involve top management commitment, extensive user training, and a phased implementation strategy that accommodates regional variations. Furthermore, the study emphasizes the importance of post-implementation evaluation and continuous improvement to address evolving business requirements. Looking ahead, future directions in ERP systems implementation within MNCs are shaped by the rapid evolution of technology and the dynamic nature of global business. The integration of cloud-based solutions, the advancement of Industry 4.0 technologies, and the increasing emphasis on sustainability are anticipated to influence the next wave of ERP adoption in MNCs. This critical review contributes valuable insights into the current state of ERP systems implementation in multinational corporations, addressing trends, challenges, and future directions. It provides a roadmap for practitioners, researchers, and policymakers to navigate the complexities of ERP adoption in the ever-evolving landscape of global business.
 Keywords: ERP, MNCs, Industry 4.0, Multinational, Corporation, Review."
pub.1106338987,Cuckoo Optimization Algorithm Based Job Scheduling Using Cloud and Fog Computing in Smart Grid,"The integration of Smart Grid (SG) with cloud and fog computing has improved the energy management system. The conversion of traditional grid system to SG with cloud environment results in enormous amount of data at the data centers. Rapid increase in the automated environment has increased the demand of cloud computing. Cloud computing provides services at the low cost and with better efficiency. Although problems still exists in cloud computing such as Response Time (RT), Processing Time (PT) and resource management. More users are being attracted towards cloud computing which is resulting in more energy consumption. Fog computing is emerged as an extension of cloud computing and have added more services to the cloud computing like security, latency and load traffic minimization. In this paper a Cuckoo Optimization Algorithm (COA) based load balancing technique is proposed for better management of resources. The COA is used to assign suitable tasks to Virtual Machines (VMs). The algorithm detects under and over utilized VMs and switch off the under-utilized VMs. This process turn down many VMs which puts a big impact on energy consumption. The simulation is done in Cloud Sim environment, it shows that proposed technique has better response time at low cost than other existing load balancing algorithms like Round Robin (RR) and Throttled."
pub.1172099148,Building Custom Spreadsheet Functions with Python: End-User Software Engineering Approach,"End-user computing empowers non-developers to manage data and applications, enhancing collaboration and efficiency. Spreadsheets, a prime example of end-user programming environments widely used in business for data analysis. However, Excel functionalities have limits compared to dedicated programming languages. This paper addresses this gap by proposing a prototype for integrating Python’s capabilities into Excel through on-premises desktop to build custom spreadsheet functions with Python. This approach overcomes potential latency issues associated with cloud-based solutions. This prototype utilizes Excel-DNA and IronPython. Excel-DNA allows creating custom Python functions that seamlessly integrate with Excel’s calculation engine. IronPython enables the execution of these Python (CSFs) directly within Excel. C# and VSTO add-ins form the core components, facilitating communication between Python and Excel. This approach empowers users with a potentially open-ended set of Python (CSFs) for tasks like mathematical calculations, statistical analysis, and even predictive modeling, all within the familiar Excel interface. This prototype demonstrates smooth integration, allowing users to call Python (CSFs) just like standard Excel functions. This research contributes to enhancing spreadsheet capabilities for end-user programmers by leveraging Python’s power within Excel. Future research could explore expanding data analysis capabilities by expanding the (CSFs) functions for complex calculations, statistical analysis, data manipulation, and even external library integration. The possibility of integrating machine learning models through the (CSFs) functions within the familiar Excel environment."
pub.1135050105,Information Reuse and Stochastic Search," Many software systems operate in environments of change and uncertainty. Techniques for self-adaptation allow these systems to automatically respond to environmental changes, yet they do not handle changes to the adaptive system itself, such as the addition or removal of adaptation tactics. Instead, changes in a self-adaptive system often require a human planner to redo an expensive planning process to allow the system to continue satisfying its quality requirements under different conditions; automated techniques must replan from scratch. We propose to address this problem by reusing prior planning knowledge to adapt to unexpected situations. We present a planner based on genetic programming that reuses existing plans and evaluate this planner on two case-study systems: a cloud-based web server and a team of autonomous aircraft. While reusing material in genetic algorithms has been recently applied successfully in the area of automated program repair, we find that naively reusing existing plans for self- * planning can actually result in a utility loss. Furthermore, we propose a series of techniques to lower the costs of reuse, allowing genetic techniques to leverage existing information to improve utility when replanning for unexpected changes, and we find that coarsely shaped search-spaces present profitable opportunities for reuse. "
pub.1163331031,DEPS: A Demand-Oriented Framework for Edge Intelligent Production System,"In many cases, demands of Intelligent Manufacturing are generated locally and served locally, not necessarily dependent on the cloud. We believe in the distributed intelligent production system, cloud and edge systems should act as equal processing nodes for service migration and cooperation. This paper focuses on the edge nodes and expands the scope of edge computing, aimed at building an independent and complete edge system - DEPS based on demand-orientation. To adapt to the edge environment, DEPS adopts a hierarchical structure to provide a relatively complete system framework for multiple parallel services. To meet the demands of industrial production, DEPS takes two measures: 1) integrates a variety of existing industrial control systems; 2) uses flexible modular design to allow experts to participate in the design, enhancing the professionalism of the system which can be designed more on-demand. We use short and continuous industrial production data to evaluate DEPS and propose a method to measure its processing performance versus current systems: 1) as for a cloud only system, DEPS has superior performance servicing local demands in real-time, securely and of lower cost; 2) as for current edge systems, partial comparison of them in applications and performances is discussed."
pub.1122068897,Design and Analysis of Sustainable and Seasonal Profit Scaling Model in Cloud Environment,"Cloud is a widely used platform for intensive computing, bulk storage, and networking. In the world of cloud computing, scaling is a preferred tool for resource management and performance determination. Scaling is generally of two types: horizontal and vertical. The horizontal scale connects users’ agreement with the hardware and software entities and is implemented physically as per the requirement and demand of the datacenter for its further expansion. Vertical scaling can essentially resize server without any change in code and can increase the capacity of existing hardware or software by adding resources. The present study aims at describing two approaches for scaling, one is a predator-prey method and second is genetic algorithm (GA) along with differential evolution (DE). The predator-prey method is a mathematical model used to implement vertical scaling of task for optimal resource provisioning and genetic algorithm (GA) along with differential evolution(DE) based metaheuristic approach that is used for resource scaling. In this respect, the predator-prey model introduces two algorithms, namely, sustainable and seasonal scaling algorithm (SSSA) and maximum profit scaling algorithm (MPSA). The SSSA tries to find the approximation of resource scaling and the mechanism for maximizing sustainable as well as seasonal scaling. On the other hand, the MPSA calculates the optimal cost per reservation and maximum sustainable profit. The experimental results reflect that the proposed logistic scaling-based predator-prey method (SSSA-MPSA) provides a comparable result with GA-DE algorithm in terms of execution time, average completion time, and cost of expenses incurred by the datacenter."
pub.1095530306,Dynamic Fault Tolerant Scheduling Mechanism for Real Time Tasks in Cloud Computing,"Now-a-days cloud makes a success in various fields such as financial transaction, scientific computing and so on. Reliability and Availability are most important between cloud provider and user. Fault tolerance plays a major role for using cloud services for industry and scientific specific purposes. In the present work, we focus on fault tolerance and resource allocation policies for cloud environments. In the existing system, Primary Backup (PB) model is used, but it does not contain any dynamic resource allocating mechanism. We propose a dynamic resource allocating mechanism with fault tolerance to improve resource utilization. We incorporate a backup overlapping mechanism and efficient VM migration strategy for designing novel Dynamic Fault Tolerant Scheduling Mechanism for Real Time Tasks in cloud computing. Our proposed model aims at achieving both fault tolerance and high resource utilization in the cloud. The experiments are evaluated using random synthetic workload and Google cloud trace logs to test the efficiency of our proposed model."
pub.1147718319,Load balancing in cloud environment using enhanced migration and adjustment operator based monarch butterfly optimization,"In the decades before the advent of computers, humans tend to make mistakes while calculating and remembering tasks. Distributed computing helped to reduce the workload of each computer by distributing the workload evenly among computers connected in the network. Cloud computing have eradicated most of the problems that occurred in distributed computing but were also prone to different types of issues. Major issues in cloud computing relate to security and load balancing. Load balance of a node relates to two important parameters namely request time and response time. Meta heuristics algorithms can be used to provide proper load balancing techniques in cloud. This paper provides a mechanism namely EMAMBO to ensure that each node is properly load-balanced in cloud. Based on different metrics considered, it could be inferred that the proposed system fares better when compared to different benchmarked existing systems."
pub.1104274094,BAHS: A Bandwidth-Aware Heterogeneous Scheduling Approach for SDN-Based Cluster Systems,"Software Defined Networking(SDN) is a novel network architecture proposed by Stanford in 2008, which decouples control plane and data plane separately. SDN comes with logically centralized view of the whole network status and flexible control of packet switching, providing more possibilities of manipulating network in large-scale cloud systems. Scheduling algorithms impact the network performance significantly, especially for SDN-based cluster systems. However, finding an appropriate scheduling algorithm for heterogeneous network environments is an NP problem, where Quality-of-Service(QoS) needs to be ensured efficiently in these systems. Existing solutions rarely take into account of fine-grained bandwidth information and the lack of global network information also reasons why bandwidth is not widely used in scheduling. To address this problem, we proposed a new scheduling approach called BAHS, which is a comprehensive scheduling approach built in SDN environment. The main idea of BAHS is choosing proper tasks and their corresponding paths with the lowest interference. To estimate potential path conflicts, BAHS incorporates the state-of-the-art solution such as Dynamic Online Routing Algorithm(DORA). To demonstrate the effectiveness of our proposed approach, we conduct several simulations based on Mininet. Compared to previous approaches, BAHS improves the completion time and local rate of all tasks with QoS guarantee by up to 14.49% and 12.76%, respectively."
pub.1132360038,"Building Cyber Physical Systems – Design Challenges, Techniques","A cyber physical system (CPS) is a “system of systems”, where complex and heterogeneous systems interact in a continuous manner and proper regulation of it necessitates careful co-design of the overall architecture of CPS. CPS are a new class of engineered systems that offer close interaction between cyber and physical components. This field of CPS has been identified as a key area of research and CPS are expected to play a major role in the design and development of future embedded products. CPS has become an advanced technology which paves the way for the interconnection, computation and control of all the physical devices through embedded processors. CPS has transformed the way in which the physical things in the world interact, compute and control processes. The complete design architecture of CPS has three major parts: (i) embedded and real-time systems, (ii) distributed sensor systems and (iii) control and monitoring systems. This chapter, Investigations of Design Challenges, Techniques and Applications of Cyber Physical Systems, explores the engineering system perspective needed to design and build complex cyber physical systems. The contents also focus on techniques for the design of real-time control and adaptation to achieve real-time dynamic control and behavioural adaptation in diverse environments, such as clouds, and in network-challenged spaces. It addresses the foundational issues across CPS to design secure and resilient systems for rapidly evolving applications. CPS has applications in almost all sectors, such as medical, agriculture, defence, transport, design and construction of smart buildings, manufacturing technology, industry automation and aerospace. The objective of this chapter is to address the design challenges in CPS, such as the selection of suitable embedded processors and their interfacing with the sensors, selection of low-cost sensors for different applications, and the selection of efficient control and monitoring systems. In the near future, CPS will revolutionize how human will interact with, control and monitor the physical system around us. This chapter discusses the design challenges and techniques and applications of cyber physical systems (CPS), and explores the engineering system perspective needed to design and build complex CPS. CPS is a mechanism by which physical systems are controlled or monitored by software algorithms. CPSs require interaction between widespread sensors and actuators in the physical systems, networked to the existing Internet. The physical part is that aspect of the system which is realised with computers. It includes mechanical parts, and biological or chemical processes, depending on the applications. A CPS includes one or more computational platforms, with wireless sensor nodes as inputs and actuators as outputs, and computers with real-time operating systems specific for individual applications. The end-to-end connectivity between the physical device and the edge devices dete"
pub.1104266802,Energy conscious multi-site computation offloading for mobile cloud computing,"In mobile devices, one of the major reasons for battery consumption is due to computation of complex applications. To provide high performance of execution on the mobile devices, the concept of mobile cloud computing (MCC) is used. MCC allows the computation complex modules to offload onto a cloud from a mobile device, which helps to remove the resource constraint condition of the mobile devices. Computations can also be offloaded to nearby clouds called multi-sites, which may have different resources, access delays, computation capability or service charges. The mobile users can specify their priority such as total completion time, cost or energy saving of the application execution in MCC environment. But most of the existing research is focused to optimize only one objective, i.e., either total completion time or cost or energy. But when offloading computation modules onto cloud-based multi-sites, a tradeoff solution is required to strike a balance between the total completion time and energy savings. Further, the entire computational execution on the cloud is to be served efficiently with optimal power utilization. Various algorithms are developed to reduce power consumption, and one such algorithm is dynamic voltage and frequency scaling (DVFS) algorithm. In this paper, new algorithms known as cost and time constraint task partitioning and offloading algorithm (CTTPO), multi-site task scheduling algorithm (MTS) based on teaching, learning-based optimization and the energy saving on multi-sites (ESM) using DVS technique are proposed. CTTPO deals with trade-off between time and cost for the task partitioning and offloading. The MTS algorithm deals with time efficient scheduling on multi-sites, and ESM algorithm saves the energy on the multi-sites by switching the sites from high voltage to low voltage during ideal time. The simulation study demonstrates that the proposed algorithms outperformed the existing techniques based on time, cost and energy parameters."
pub.1159429117,Container Allocation in Cloud Environment Using Multi-Agent Deep Reinforcement Learning,"Nowadays, many computation tasks are carried out using cloud computing services and virtualization technology. The intensive resource requirements of virtual machines have led to the adoption of a lighter solution based on containers. Containers isolate packaged applications and their dependencies, and they can also operate as part of distributed applications. Containers can be distributed over a cluster of computers with available resources, such as the CPU, memory, and communication bandwidth. Any container distribution mechanism should consider resource availability and their impact on overall performance. This work suggests a new approach to assigning containers to servers in the cloud, while meeting computing and communication resource requirements and minimizing the overall task completion time. We introduce a multi-agent environment using a deep reinforcement learning-based decision mechanism. The high action space complexity is tackled by decentralizing the allocation decisions among multiple agents. Considering the interactions among the agents, we introduce a new cooperative mechanism for a state and reward design, resulting in efficient container assignments. The performances of both long short term memory (LSTM) and memory augmented-based agents are examined, for solving the challenging container assignment problem. Experimental results demonstrated an improvement of up to 28% in the execution runtime compared to existing bin-packing heuristics and the common Kubernetes industrial tool."
pub.1174047992,Reliable Resource Optimization Model for Cloud Using Adversarial Neural Network,"This book chapter presents a comprehensive study on the development and implementation of a reliable resource optimization model for cloud computing using an adversarial neural network (ANN). Optimization and efficient resource allocation have become crucial with the increasing adoption of cloud computing to ensure optimal performance and meet user demands. This chapter addresses these challenges by proposing a unique approach that leverages the capabilities of ANN to optimize resource allocation in cloud environments. The chapter begins with an introduction to cloud computing and its significance in modern IT infrastructures. It emphasizes the need for effective resource allocation strategies to maximize resource utilization while adhering to service level agreements (SLAs). The limitations of existing resource allocation models are discussed, highlighting the necessity for a more reliable and efficient solution. The proposed model introduces a pioneering architecture founded on an adversarial neural network, which comprises a generator network and a discriminator network. The generator network is responsible for generating resource allocation plans, while the discriminator network assesses the quality of these plans based on predefined metrics. By means of an adversarial training process, the generator network acquires knowledge and expertise in generating optimized resource allocation strategies that surpass the capabilities of the discriminator network, thus resulting in improved reliability and performance. The chapter provides detailed insights into the system design and working of the ANN-based resource optimization model. It discusses the architectural considerations, hyperparameters, and training methodology employed. Furthermore, it addresses the challenges associated with training the ANN, such as mode collapse and training instability, and presents effective strategies to mitigate these issues. Various optimization algorithms and loss functions are explored to ensure efficient convergence and the generation of high-quality resource allocation plans. To evaluate the efficacy of the proposed model, extensive experimental evaluations will be conducted. Performance benchmarks will be established, and comparisons will be made against conventional resource allocation approaches. The experimental results will demonstrate the superior reliability and optimization achieved by the ANN-BPSO-RF model. Furthermore, the model will exhibit remarkable adaptability to changing workload demands and showcases its scalability in large-scale cloud environments."
pub.1165910546,Robot Localization and Reconstruction based on 3D Point Cloud,"The 3D point cloud is widely used in robot fields because of its accurate positioning results and dense environment information. However, most of the existing methods are real-time positioning and 3D reconstruction in unknown environments. In some scenes that require multiple regular operations, such as robot patrol and maintenance, the stability of the system is slightly insufficient. At present, the methods in this field are mostly based on fixed starting points or manual positioning, with an insufficient degree of automation. In this paper, a real-time robot localization and reconstruction system based on 3D vision is proposed, which includes pose estimation, environment reconstruction, and relocalization based on a 3D point cloud. First, a more accurate pose estimation method is applied for 3D environment reconstruction, using the coordinate transformation of the point cloud and the point cloud matching of the key frames. Then, a new point cloud segmentation method is proposed for local map maintenance to realize point cloud map display and human-robot interaction under real-time network transmission. Finally, a new robot relocalization method is proposed for map updating when the mapping is interrupted or repeated. The M2DGR dataset and real robot test were used to verify the accuracy and effect of the system, where the results showed that our method had a good performance."
pub.1173286094,The Significance of Porches in Urban Applications: A Method for Automated Modeling and Integration,"Abstract. Porches, as defined by the Art & Architecture Thesaurus, serve as vital transitional spaces linking indoor and outdoor environments. Despite their historical and contemporary significance, porches lack explicit representation in prevalent standards like CityGML and IndoorGML, posing challenges for comprehensive spatial modeling and its application. This paper proposes a method for modeling porches that aligns with the existing OGC standard CityGML 3.0, ensuring accuracy and compatibility. Drawing upon geomatics techniques, the method aims to bridge the gap in representing these spaces, critical for applications such as navigation systems, urban planning, and energy simulations. By integrating geometric, machine learning, and informative modeling approaches, this method seeks to provide a robust foundation for various practical applications. The paper outlines a comprehensive state-of-the-art review, describes the proposed method from digitalization to random forest (RF)-based point cloud classification and vectorization, presents case studies and results, and offers critical discussions and conclusions. Through this endeavor, the paper contributes to enhancing the representation and understanding of porches within the digital spatial landscape."
pub.1166836207,Health Block: A Blockchain Based Secure Healthcare Data Storage and Retrieval System for Cloud Computing,"Data in healthcare domain is highly sensitive in nature. Besides, there is need for maintaining integrity of such data. Blockchain technology has emerged to solve the problem of data integrity and non-repudiation with immutable storage in distributed repository. Thus secure data storage and retrieval in cloud environments is made possible using blockchain implementation. There are many existing healthcare systems with blockchain integration found in the literature. However, there is need for a system that supports complete set of operations that are governed by smart contracts. Another important consideration is that end users should be able to operate healthcare system without the need for knowledge of blockchain technology. Towards this end, in this paper, we proposed a Blockchain based secure healthcare data storage and retrieval system known as HealthBlock for cloud computing environments. We defined smart contract with underlying structures and functions using Solidity language for Ethereum blockchain platform. We also proposed and implemented an algorithm known as Healthcare Transactions over Blockchain (HToB). This algorithm supports secure blockchain based data storage and retrieval governed by smart contracts. Our system is evaluated using user-friendly web based client application. The experimental results showed that our system is able to ensure data integrity and non-repudiation besides reaping all benefits of blockchain technology."
pub.1170301655,inSēquio: A Programmable 3D CAD Application for Designing DNA Nanostructures,"Abstract  DNA nanotechnology is evolving rapidly, paralleling the historic trajectory of the 1970s electronics industry. However, current DNA nanostructure (DN) design software limits users to either manual design with minimal automation or a constrained range of automated designs. inSēquio Design Studio, developed by Parabon ® NanoLabs, bridges this gap as a programmable 3D computer-aided design (CAD) application, integrating a domain-specific graphical editor with a Python API for versatile DN design.   Developed in C++ for Windows ® and Macintosh ® systems, inSēquio features a user-friendly GUI with extensive CAD tools, capable of managing complex designs and offloading computational tasks to the cloud. It supports various DNA design formats, PDB molecule integration, residue modifications, and includes preloaded designs and thorough documentation.  With its combination of features, inSēquio enables a code-centric design (CCD) approach, enhancing DN construction with improved precision, scalability, and efficiency. This approach is elucidated through a streptavidin barrel cage designed via Python notebook and a spheroid origami case study. Marking a significant advance in DN design automation, inSēquio, the first fully programmable 3D CAD tool for DN design, enables both manual and programmatic 3D editing. This fusion of features establishes inSēquio as a transformative tool, poised to significantly enhance designer productivity and expand the scope of possible designs.  Extended Abstract Advances in DNA nanotechnology have positioned the field at a juncture reminiscent of the pivotal growth phase of the electronics industry in the 1970s. The evolution of software for designing DNA nanostructures (DNs) is following a similar historical trajectory and dozens of software packages have been developed for creating them. Existing software options, however, require users to choose between manual design with minimal automation support or selecting from a limited set of designs, typically wireframe, that can be generated from a high-level structural description. Here, we introduce the inSēquio Design Studio, a programmable 3D computer-aided design (CAD) application that effectively bridges this gap. By integrating a domain-specific, freeform graphical editor with a Python application programming interface (API), inSēquio provides a comprehensive and extensible platform for designing complex nucleic acid (NA) nanostructures.  The inSēquio desktop application, developed in C++, runs on Windows ® and Macintosh ® operating systems. Its graphical user interface (GUI) features multiple synchronized view panels and a diverse set of CAD and NA-specific editing tools. Its optimized graphics pipeline enables editing of designs with >2M nucleotides, and it includes an integrated service infrastructure for offloading heavy computations to cloud servers. The software also supports import and export of various DNA design file formats, integration of arbitrary PDB m"
pub.1173236018,A Holistic Approach with Behavioral Anomaly Detection (BAD) for Mitigating Insider Threats in Cloud Environments,"Insider threats remain a critical concern in cloud environments, necessitating robust strategies for detection and mitigation. This paper proposes a holistic approach that integrates Behavioral Anomaly Detection (BAD) with existing security measures to mitigate insider threats effectively. BAD focuses on identifying abnormal behaviors that deviate from established patterns, making it a valuable tool for detecting insider threats. The holistic approach combines BAD with other security mechanisms such as access control, encryption, and user monitoring to create a comprehensive defense strategy. The key advantage of this approach is its ability to adapt to the dynamic nature of insider threats. By continuously analyzing user behavior and identifying anomalies, the system can detect potential threats early, allowing for timely intervention. Moreover, the holistic approach reduces the number of false positives compared to using BAD in isolation, thereby improving the efficiency of threat detection. To demonstrate the effectiveness of the approach, we provide a detailed implementation plan and showcase case studies from real-world cloud environments. Our evaluation results indicate that the holistic approach significantly enhances the security posture of cloud environments against insider threats. By leveraging BAD and integrating it into a holistic security framework, organizations can better protect their sensitive data and infrastructure from insider threats. This research contributes to the field of cloud security by providing a practical and effective strategy for mitigating insider threats. It also opens up avenues for future research, such as exploring the integration of machine learning techniques to enhance BAD's detection capabilities further."
pub.1095586894,Efficient Management of Privacy Issues in Mobile Cloud Environment,"Mobile application developers and users can feel a direct advantage within mobile based cloud computing for overcoming the inherent constraints present in mobile devices - be it battery life, memory space or processing power. However, ubiquitous adoption of programmable smart mobile devices and connecting them to unsecured public domain of Internet raises newer privacy and security challenges across enterprises. Both mobile and cloud environment where sensitive personal data can be stored, are susceptible to security attacks that can compromise users' data privacy. Smartphones and Tablets are not only storing users' private data but also the private data of the collaborators including friends, family members, customers, vendors or any other individual. The private data of other involvers are primarily communicated, received and stored in address book, SMS, WhatsApp, emails of the users' Smartphone. We have identified several privacy plus security threats. In the backdrop of services and support of open source platform, we have enumerated existing best practices and our recommendations as preventive as well as measures on occurrence."
pub.1134902580,Digital Transformation Enables Automated Real-Time Torque-and-Drag Modeling,"This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 199670, “Digital Transformation Strategy Enables Automated Real-Time Torque-and-Drag Modeling,” by Dingzhou Cao, Occidental Petroleum; Don Hender, SPE, IPCOS; and Sam Ariabod, Apex Systems, et al., prepared for the 2020 IADC/SPE International Drilling Conference, Galveston, Texas, 3-5 March. The paper has not been peer reviewed. Automated real-time torque-and-drag (RT-T&D) analysis compares real-time measurements with evergreen models to monitor and manage downhole wellbore friction, improving drilling performance and safety. Enabling RT-T&D modeling with contextual well data, rig-state detection, and RT-interval event filters poses significant challenges. The complete paper presents a solution that integrates a physics-based T&D stiff/soft string model with a real-time drilling (RTD) analytics system using a custom-built extract, transform, and load (ETL) translator and digital-transformation applications to automate the T&D modeling work flow. Methodology A T&D representational state transfer (REST) application program interface (API) was integrated with an RTD analytics system capable of receiving and processing both real-time (hookload, torque, and rig-state) and digitized (drillstring and casing components, trajectory profiles, and mud-property) well data across multiple platforms. This strategy consists of four parts: Digital transformation apps, ETL, and translator Physics-based stiff/soft string T&D model API Pre-existing data infrastructure RTD analytics system The data-flow architecture reveals a flexible design in the sense that it can accommodate different types of T&D models or any other physics-based REST API models (e.g., drillstring buckling or drilling hydraulics) and can be accessed offline for prejob/post-job planning. Drilling engineers can also leverage the RTD systems’ historical database to perform recalculations, comparative analysis, and friction calibrations. The RT-T&D model also can be deployed in a cloud environment to ensure that horizontal scalability is achieved."
pub.1145407588,Band-Area Application Container and Artificial Fish Swarm Algorithm for Multi-Objective Optimization in Internet-of-Things Cloud,"Container virtualization methods based on application deployment levels have been widely adopted in cloud-computing environments to implement application construction, deployment, and migration. However, most application containers focus on the interface between the applications and hosts and lack collaboration between application containers. This study proposes a new application container model that contains users, application services, documents, and messages, called Band-area Application Container. A salient feature of the Band-area is that it can express a variety of things in reality, such as organizations or individuals. End users can build a complex and changeable application system through cooperation between the Band-areas. However, the resource allocation of non Internet-of-Thing and Internet-of-Thing tasks from the application container is an open issue. The resource allocation method of tasks should not only improve the quality of the user experience, but also reduce energy consumption by improving the resource utilization of the server. To solve this problem, an artificial fish swarm algorithm is proposed to optimize container-based task scheduling. The algorithm considers not only the reliability, processing time overhead, and energy consumption of the task, but also the resource utilization of the servers. Experimental evaluation shows that, compared with the existing three algorithms, the algorithm obtains a better improvement rate in task processing time overhead, energy consumption, reliability, and cluster load balancing."
pub.1141186876,Towards cloud‐based unobtrusive monitoring in remote multi‐vendor environments,"Abstract Nowadays, many complex multi‐vendor production environments, such as telecom infrastructures in smart cities or on‐board passenger information systems in trains, are based on micro‐services and deployed in the cloud. From a service integrator viewpoint, building new solutions for these environments, which can host a large number of externally designed and developed micro‐services, is often complex and error‐prone. This is in part due to undocumented behaviour or undocumented architectural specifications of such systems. Advanced service monitoring can offer a solution to quickly detect anomalies or unexpected service interaction behaviour during on‐site integration. However, the monitoring service should not have an impact on the production environment itself. Therefore, this article proposes an agent‐based unobtrusive monitoring platform, capable of monitoring both internally developed and externally developed services through the use of sidecar containers. It monitors state, metrics and network traffic at micro‐service level and the research was conducted as part of the DynAMo research project, a collaboration with various industry partners. Prototype evaluation proves that our solution has a negligible impact (below 0.02% CPU usage on average) on an existing micro‐service environment just as other monitoring systems like Prometheus while offering additional functionality focused on multi‐vendor service integration. This makes it suitable to be deployed in complex production domains to further aid on‐site integration and quickly find potential new anomalies."
pub.1128898211,Adaptive Privacy Preservation Approach for Big Data Publishing in Cloud using k-anonymization," Background: Big data is an emerging technology that has numerous applications in the fields, like hospitals, government records, social sites, and so on. As the cloud computing can transfer large amount of data through servers it has found its importance in big data. Hence, it is important in cloud computing to protect the data so that the third party users cannot access the information from the users.   Methods: This paper develops an anonymization model and adaptive Dragon Particle Swarm Optimization (adaptive Dragon-PSO) algorithm for privacy preservation in cloud environment. The development of proposed adaptive DragonPSO incorporates the integration of adaptive idea in the dragon-PSO algorithm. The dragon-PSO is the integration of Dragonfly Algorithm (DA) and Particle Swarm Optimization (PSO) algorithm. The proposed method derives the fitness function for the proposed adaptive Dragon-PSO algorithm to attain the higher value of privacy and utility. The performance of the proposed method was evaluated using the metrics, such as information loss and classification accuracy for different anonymization constant values.   Conclusion: The proposed method provided a minimal information loss and maximal classification accuracy of 0.0110 and 0.7415, respectively when compared with the existing methods. "
pub.1134694736,Reality Capture of Buildings Using 3D Laser Scanners,"The urgent need to improve performance in the construction industry has led to the adoption of many innovative technologies. 3D laser scanners are amongst the leading technologies being used to capture and process assets or construction project data for use in various applications. Due to its nascent nature, many questions are still unanswered about 3D laser scanning, which in turn contribute to the slow adaptation of the technology. Some of these include the role of 3D laser scanners in capturing and processing raw construction project data. How accurate is the 3D laser scanner or point cloud data? How does laser scanning fit with other wider emerging technologies such as Building Information Modelling (BIM)? This study adopts a proof-of-concept approach, which in addition to answering the afore-mentioned questions, illustrates the application of the technology in practice. The study finds that the quality of the data, commonly referred to as point cloud data is still a major issue as it depends on the distance between the target object and 3D laser scanner’s station. Additionally, the quality of the data is still very dependent on data file sizes and the computational power of the processing machine. Lastly, the connection between laser scanning and BIM approaches is still weak as what can be done with a point cloud data model in a BIM environment is still very limited. The aforementioned findings reinforce existing views on the use of 3D laser scanners in capturing and processing construction project data."
pub.1133428020,I/O Strength-Aware Credit Scheduler for Virtualized Environments,"With the evolution of cloud technology, the number of user applications is increasing, and computational workloads are becoming increasingly diverse and unpredictable. However, cloud data centers still exhibit a low I/O performance because of the scheduling policies employed, which are based on the degree of physical CPU (pCPU) occupancy. Notably, existing scheduling policies cannot guarantee good I/O performance because of the uncertainty of the extent of I/O occurrence and the lack of fine-grained workload classification. To overcome these limitations, we propose ISACS, an I/O strength-aware credit scheduler for virtualized environments. Based on the Credit2 scheduler, ISACS provides a fine-grained workload-aware scheduling technique to mitigate I/O performance degradation in virtualized environments. Further, ISACS uses the event channel mechanism in the virtualization architecture to expand the scope of the scheduling information area and measures the I/O strength of each virtual CPU (vCPU) in the run-queue. Then, ISACS allocates two types of virtual credits for all vCPUs in the run-queue to increase I/O performance and concurrently prevent CPU performance degradation. Finally, through I/O load balancing, ISACS prevents I/O-intensive vCPUs from becoming concentrated on specific cores. Our experiments show that compared with existing virtualization environments, ISACS provides a higher I/O performance with a negligible impact on CPU performance."
pub.1142824797,What The Future Upholds!,"As the world is changing with better infrastructure, fiber, clouds, networks, data centers have become a source of providing information around the globe, providing us one of the biggest businesses, with a large amount of spending done over them. By the end of 2021, “Global Information Technology” is calculated to be spending $3.8 trillion over the companies shifting to the cloud. [1] The various networks of the software, hardware, infrastructure, databases: all are highly developed for making the IT to boom up for the incoming eras. Arthur C Clarke, once said, “any sufficiently advanced technology is indistinguishable from magic.” Everything going to the advanced level, the shared amount of data relies on fast connectivity. There is a lot of talking going on the capabilities provided by the 5G from past few years, which is finally out with its first batch of 5G handsets have been announced hitting the market soon.[2]The market opportunities of various companies are as bigger as they have ever been. Organizations such as DARPA, IBM, Shipchain, Paxful, Circle [3] are few of the companies working on providing number of applications to the technological world. Companies such as Honeywell, Microsoft, AWS, Spunk, Google are actively working in order to update the computing capabilities which we are accessing upon by providing us with quantum world. [4]Quantum computing provides us a redefine form of what the computer is all about. It gives us the computing power which is billions of times more renowned and powerful than the computing environment we are existing in today. Subdermal technologies are overpowered with new rulings neuro link. In today’s world, people working on completely virtual spaces with power assist fabrics (Digital fabrics) [5]. It provides human the evolution of added strength and gaining mobility to another level. United Nations have developed a universal translator, which in turn reduces the requirement of human interpreters and translators"
pub.1173979660,Blockchain-assisted multi-authority searchable encryption scheme in cloud environment,"With the deep integration of new technology concepts such as big data, cloud storage, the Internet of Things with enterprise management, power grid infrastructure has also entered a period of rapid development, and the realization of safe and reliable data sharing has emerged as an urgent issue that requires resolution. Firstly, the existing attribute-based encryption schemes are mostly deployed in the cloud environment, and it could be challenging for users to achieve fine-grained access control to ciphertext while taking into account the search query function. Secondly, current attribute-based encryption schemes face the single-point bottleneck problem issue brought on by a single authority managing the whole attribute set, making it tough to guarantee data confidentiality and security. In view of the above situation, this paper designs a blockchain-assisted searchable attribute-based encryption scheme, which uses the consortium blockchain's consensus nodes to generate system parameters and user's key through (t, n) secret sharing, besides that, the user's access requests and operations are recorded on the blockchain, realizing the traceability of transactions. The system entirely conceals the access policy and ciphertext's keyword characteristics while also supporting fine-grained access control to users. The security analysis and performance evaluation on the proposed scheme demonstrate its reliability and system-level robustness."
pub.1181655270,A Novel Approach to Data Security in Cloud Storage using Erasure Coding and Re-Encryption,"The widespread adoption of cloud storage systems raises concerns regarding data confidentiality, particularly when data is stored on external clouds. Encryption techniques can protect data privacy but often impose limitations on storage system functionality, especially in decentralized distributed systems. This research study proposes a novel solution to address these challenges by combining a re-encryption algorithm with decentralized erasure coding. The proposed system enables users to forward their data to others without requiring decryption, ensuring reliable and secure data storage and retrieval. The primary technical contribution of this research lies in the development of a re-encryption system that facilitates both encryption and forwarding operations over encrypted and decrypted communications using the Blowfish algorithm. This innovative approach enhances data confidentiality and flexibility in decentralized cloud storage environments. By addressing the limitations of existing encryption techniques, this research contributes to the development of more secure and efficient cloud storage solutions, providing users with greater control over their data and ensuring its privacy and integrity."
pub.1181794160,KubeRosy: A Dynamic System Call Filtering Framework for Containers,"With the rapid adoption of cloud environments, container technology has become crucial for the efficient operation of large-scale applications. Although container technology offers high efficiency and scalability through low-level isolation via shared host operating systems, it also introduces security vulnerabilities, such as container escape and privilege escalation attacks through system call exploitation. Seccomp-BPF, one of the most widely used system call filtering mechanisms, supports container environments but cannot update system call policies while containers are running. To address these limitations, we propose KubeRosy, a system call filtering framework that allows dynamic modification of system call policies without downtime, even during container runtime. KubeRosy leverages eBPF and LSM hooks to support fine-grained system call policies while ensuring compatibility with existing Seccomp-BPF environments. This approach enables the application of customized, granular system call policies tailored to container environments, thereby reducing the attack surface. Our evaluation shows that KubeRosy incurs an additional overhead of only 722 ns compared to traditional Seccomp-BPF, which is negligible. Furthermore, KubeRosy allows for dynamic policy modification without container downtime and provides precise argument-based filtering, demonstrating its practicality and efficiency."
pub.1095277226,Cloud Radio Access Networks (C-RAN) in Mobile Cloud Computing Systems,"Cloud computing will have profound impacts on wireless networks. On one hand, the integration of cloud computing into the mobile environment enables mobile cloud computing (MCC) systems; on the other hand, the powerful computing platforms in the cloud for radio access networks lead to a novel concept of cloud radio access networks (C-RAN). In this paper, we study the topology configuration and rate allocation problem in C-RAN with the objective of optimizing the end-to-end performance of MCC users in next generation wireless networks. An intrinsic issue related to such system is that only sub-optimal decisions can be made due to the fact that the channel state information is outdated. We employ a decision-theoretic framework to tackle this issue, and maximize the system throughput with constraints on the response latency experienced by each MCC user. Using simulation results, we show that, with the emergence of MCC and C-RAN technologies, the design and operation of future mobile wireless networks can be significantly affected by cloud computing, and the proposed scheme is capable of achieving substantial performance gains over existing schemes."
pub.1084754936,Analyzing Requirements Engineering for Cloud Computing,"Cloud computing is a business paradigm, where cloud providers offer resources (e.g., storage, computing, network) and cloud consumers use them after accepting a specific service level agreement. Cloud requirements can rapidly change over time, so organizations need to count with rapid methods to elicit, analyze, specify, verify, and manage dynamic requirements in a systematic and repeatable way. The existing works of this field are generally focused in a limited number of requirements and capabilities for cloud services. This chapter aims to provide a comprehensive and systematic literature review of academic researches done in requirements engineering for cloud computing area. During this study, some approaches for cloud computing were found that considered a limited number of characteristics (e.g., security, privacy, performance) and few activities involving diverse stakeholders. Generally, cloud stakeholders have got neither guidelines nor standards to manage multiple aspects of services in cloud environments. Thus, a literature review was first conducted and five dimensions are discussed (i.e., Contractual, Compliance, Financial, Operational, and Technical) in order to classify cloud characteristics, specify requirements, and support cloud contracts. Different specialists and experts may be requested to evaluate particular dimensions in the service level agreement and cloud service adoption. Finally, a simple sample is given to illustrate how to identify the cloud dimensions."
pub.1168078133,Towards Sustainable Smart Living: Cloud-Based IoT Solutions for Home Automation,"In recent times, the realm of home automation systems has garnered significant attention, thanks to the ever-evolving landscape of communication technology. The concept of a smart home, essentially an application of the Internet of Things (IoT), leverages the power of the internet to oversee and employ household appliances through a sophisticated automation infrastructure. Nevertheless, challenges persist within the existing home automation systems, such as constrained wireless transmission reach, a deficiency in backup power management, and the substantial financial outlay involved. Addressing these limitations, our study introduces an economical and resilient solution that combines cloud based IoT with an uninterrupted power management system, making a cutting-edge home automation prototype. This system relies on a microcontroller unit, specifically the ESP-32, which functions as a Wi-Fi-enabled gateway for connecting a variety of sensors and transmitting their data to the Blynk IoT cloud server. The data assembled from a multitude of sensors, including vibration sensors and voltage detectors, becomes readily accessible on users' devices, be it smartphones or laptops, irrespective of their geographical location. The system is further strengthened by a set of relays that link the ESP-32 with household appliances, allowing for centralized control. Structurally, the design uses a control box that can be seamlessly integrated into a real home environment, offering the means to both monitor and govern an array of household devices. This IoT-based home automation solution not only efficiently manages internet-connected appliances but also provides an effective emergency power management system, enabling remote initiation and deactivation of backup generators. It represents a innovative leap in the evolution of home automation systems, steering in convenience, efficiency, and cost-effectiveness."
pub.1164763241,СУЧАСНІ ПЕРСПЕКТИВИ ЗАСТОСУВАННЯ КОНЦЕПЦІЇ ZERO TRUST ПРИ ПОБУДОВІ ПОЛІТИКИ ІНФОРМАЦІЙНОЇ БЕЗПЕКИ ПІДПРИЄМСТВА,"Modern businesses have undergone significant changes as a result of digital advances and the recent COVID-19 pandemic. In particular, there has been an increase in the number of employees working remotely, using personal digital devices alongside corporate devices, and the enterprise itself moving business processes to the cloud or using hybrid environments that combine both cloud and on-premises services. Taken together, this leads to increased interaction between devices and services over open networks, creating new risks of cyber-attack. It is this situation that has led to the relevance and direction of this research. The paper analyzes the current state of effectiveness of the application of enterprise information security policy, in particular, identifies the main limitations associated with the difficulty, and sometimes impossibility, to control the behavioral aspects of enterprise employees to comply with the basic provisions of security policy and general information security. The basic principles of the Zero Trust conceptual approach are analyzed and the main advantages of its application in the formation of the security policy as a strategic approach to ensuring the information security of the enterprise in the conditions of dynamic growth of new threats and transformation of modern business are determined. At the same time, it is established that one of the key components of the Zero Trust architecture is the access control system. As a result, forming the prospects of applying the concept of Zero Trust in the construction and implementation of the information security policy, the necessity of conducting an accompanying study of the effectiveness of modern mechanisms of identification/authentication of access subjects was determined."
pub.1041935636,Distributed Application Runtime Environment (DARE): A Standards-based Middleware Framework for Science-Gateways,"Gateways have been able to provide efficient and simplified access to distributed and high-performance computing resources. Gateways have been shown to support many common and advanced requirements, as well as proving successful as a shared access mode to production cyberinfrastructure such as the TG/XSEDE. There are two primary challenges in the design of effective and broadly-usable gateways: the first revolves around the creation of interfaces that catpure existing and future usage modes so as to support desired scientific investigation. The second challenge and the focus of this paper, is concerned about the requirement to integrate the user-interfaces with computational resources and specialized cyberinfrastructure in an interoperable, extensible and scalable fashion. Currently, there does not exist a commonly usable middleware to that enables seamless integration of different gateways to a range of distributed and high-performance infrastructures. The development of multiple similar gateways that can work over a range of production cyberinfrastructures, usage modes and application requirements is not scalable without a effective and extensible middleware. Some of the challenges that make using production cyberinfrastructure as a collective resource difficult are also responsible for the absence of middleware that enables multiple gateways to utilize the collective capabilities. We introduce the SAGA-based, Distributed Application Runtime Environment (DARE) framework, using which gateways that seamlessly and effectively utilize scalable distributed infrastructure can be built. We discuss the architecture of DARE-based gateways, and show using several different prototypes—DARE-HTHP, DARE-NGS, how gateways can be constructed by utilizing the DARE middleware framework."
pub.1117307802,Adaptive User-managed Service Placement for Mobile Edge Computing: An Online Learning Approach,"Mobile Edge Computing (MEC), envisioned as a cloud extension, pushes cloud resource from the network core to the network edge, thereby meeting the stringent service requirements of many emerging computation-intensive mobile applications. Many existing works have focused on studying the system-wide MEC service placement issues, personalized service performance optimization yet receives much less attention. Thus, in this paper we propose a novel adaptive user-managed service placement mechanism, which jointly optimizes a user’s perceived-latency and service migration cost, weighted by user preferences. To overcome the unavailability of future information and unknown system dynamics, we formulate the dynamic service placement problem as a contextual Multi-armed Bandit (MAB) problem, and then propose a Thompson-sampling based online learning algorithm to explore the dynamic MEC environment, which further assists the user to make adaptive service placement decisions. Rigorous theoretical analysis and extensive evaluations demonstrate the superior performance of the proposed adaptive user-managed service placement mechanism."
pub.1128997268,HDFS-based parallel and scalable pattern mining using clouds for incremental data,"Increased usage of internet led to the migration of large amount of data to the cloud environment which uses Hadoop and map reduce framework for managing various mining applications in distributed environment. Earlier research activity in distributed mining comprises of solving complex problems using distributed computational techniques and new algorithmic designs. But as the nature of the data and user requirement becomes more complex and demanding, the existing distributed algorithms fails in multiple aspects. In our work, a new distributed frequent pattern algorithm, named Hadoop-based parallel frequent pattern mining (HPFP) has been proposed to optimally utilise the clusters efficiently and mine repeated patterns from large databases very effectively. The empirical evaluation shows that HPFP algorithm improves the performance of mining operation by increasing the level of parallelism and execution efficacy. HPFP achieves complete parallelism and delivers superior performance to become an efficient algorithm in HDFS, than existing distributed pattern mining algorithms."
pub.1142518092,Autonomous Temporal Time Zone Management,"With the increase in the amount of data, their heterogeneity, and scope, there is a legitimate demand to transform existing database systems to provide a robust, comprehensive, and performance-balanced environment to meet the needs of intelligent information systems. The principles and core part are significantly focused on moving existing systems to the cloud environment, which ensures performance and scalability. The complexity of the information system, decision making, and analytics perspective requires storing not only current valid data, but the whole evolution should be covered by proposing techniques of temporal databases. A significant problem can be, however, identified during the transformation related to the time zones. This paper deals with the temporal databases and covers the performance of the time zone management related to the autonomous transaction profiles."
pub.1029027077,Integrating Complex Events for Collaborating and Dynamically Changing Business Processes,"Business processes must become agile, respond to changes in the business environment in a timely manner and quickly adapt themselves to new conditions. Event-Driven Business Process Management (ED-BPM) is an enhancement of Business Process Management (BPM) by concepts of Service Oriented Architecture (SOA) and Complex Event Processing (CEP). The most important enhancement is the integration of services accessible via the Internet that fire events into global event clouds. The events can be processed by event processing platforms for aggregating the information into higher value complex business events. These events can be modeled in a business process execution language within a process driven Business Process Management System (BPMS) to trigger changes in control flow of a process or start other services. A reference model and a reference architecture for ED-BPM are presented, based on the NEXOF Reference Architecture. A taxonomy for classifying changes to process flow is proposed. Enhancements have to be applied to the existing standards in the BPM field, including both the design-time and the runtime. A scenario from the banking domain illustrates the main concepts and principles."
pub.1117611806,Energy-aware Virtual Machine Selection and Allocation Strategies in Cloud Data Centers,"These days, information technologies are expanding exponentially so the need for high-speed processors and huge storage space are developing quickly. As a result of increasing requests, more resources are required to satisfy the client's necessities. Thus, in a cloud environment, the large number of resources consumes a lot of energy during their operation, which is turned into a key issue nowadays and demands a critical discussion in the present scenario. This research paper investigates and explores the literature on the assignment of virtual machines to hosts in a data center according to variable workload requests of different cloud consumers application executing on the virtual machines. The choice of ideal virtual machines and their placement on host prompts to limit the energy utilization. This paper proposes an algorithm for improving virtual machine selection and allocation strategies in cloud data centers. The proposed algorithm is then compared with existing algorithms on the basis of performance metrics like energy consumption and VM migrations using different threshold values. As a result, the proposed algorithm emerged to be the optimized one in enhancing the use of cloud resources by lessening the energy utilization of datacenter."
pub.1128644749,A Hybrid Model for Load Balancing in Cloud Using File Type Formatting,"Maintaining accuracy in load balancing using metaheuristics is a difficult task even with the help of recent hybrid approaches. In the existing literature, various optimized metaheuristic approaches are being used to achieve their combined benefits for proper load balancing in the cloud. These approaches often adopt multi-objective QoS metrics, such as reduced SLA violations, reduced makespan, high throughput, low overload, low energy consumption, high optimization, minimum migrations, and higher response time. The cloud applications are generally computation-intensive and can grow exponentially in memory with the increase in size if no proper effective and efficient load balancing technique is adopted resulting in poor quality solutions. To provide a better load balancing solution in cloud computing, with extensive data, a new hybrid model is being proposed that performs classification on the number of files present in the cloud using file type formatting. The classification is performed using Support Vector Machine (SVM) considering various file formats such as audio, video, text maps, and images in the cloud. The resultant data class provides high classification accuracy which is further fed into a metaheuristic algorithm namely Ant Colony Optimization (ACO) using File Type Formatting FTF for better load balancing in the cloud. Frequently used QoS metrics, such as SLA violations, migration time, throughput time, overhead time, and optimization time are evaluated in the cloud environment and comparative analysis is performed with recent metaheuristics, such as Ant Colony Optimization-Particle Swarm Optimization (ACOPS), Chaotic Particle Swarm Optimization (CPSO), Q- learning Modified Particle Swarm Optimization (QMPSO), Cat Swarm Optimization (CSO) and D-ACOELB. The proposed algorithm outperforms them and provides good performance with scalability and robustness."
pub.1155036959,FMMEHO Based Workflow Scheduling in Virtualized Cloud Environment for Smart Irrigation System,"Extensive and exhaustive water utilization for agriculture, industries and ground water consumption for domestic purposes has heavily deterioted the water bodies. Cloud and sensor technology is widely deployed in a several real-time applications, especially in agriculture. The transformation of data obtained from large sensor networks into a valuable knowledge and assests for applications can effectively leverage the techniques like Cloud Computing (CC). In CC, scheduling the workflow is the major concern that focuses on comprehensive execution of workflows without compromising the Quality of Service (QoS). But workflow scheduling augmented with resource allocation is extremely challenging task because of its inherent computational intensity, task dependencies, and heterogeneous cloud resources. In this article, a novel Optimum Energy and Resource Aware Workflow Scheduling (OERES) scheme that is motivated by popular Fuzzy Membership Mutation Elephant Herding Optimization (FMMEHO) algorithm is proposed, that aims to schedule the task workflow to Virtual Machines (VMs) that are involved in computation. This also concentrates on dynamically deploying and un-deploying the VMs pertaining to the task requirements. The FMMEHO algorithm is a popular nature inspired technique, which is rooted on herding patterns of the giant mammals, the elephants. This algorithm employs a clan operator that updates the location and distance of elephants depending resource and energy usage of each clan in the context of matriarch elephant. The proposed OERES schema elevates the resource utilization and simultaneously mitigates the energy usage without compromising the dependency and deadline constraints. This work uses the famous Cloud Sim simulator to simulate the underlying cloud environment to investigate the effectiveness of proposed model. The efficacy of the scheduling methods is examined based on important parameters like mean Resource Utilization (RU), Energy utilization or Consumption/ Task (ECT), Total Energy Consumption (TEC), Makespan and Execution Time per Task (ETT). The results very well portray the effectiveness of proposed OERES algorithm against already existing methods."
pub.1169921248,Machine Learning Protocols for Enhanced Cloud Network Security,"In fortifying the security of an organization's cloud environment, it is crucial to align security strategies with specific objectives and threats unique to the cloud landscape. This paper proposes an Integrated Augmented Intelligence approach, amalgamating Artificial Intelligence (AI) technologies such as machine learning, natural language processing, and behavioral analytics. The methodology involves thorough evaluation of existing security protocols, identification of AI components based on organizational needs, and integration of cloud-native security tools. Data from diverse cloud sources are processed, and features are extracted for machine learning models. The proposed system includes three key algorithms: Threat Detection using Machine Learning, Real-time Incident Response, and Explainable AI for Cloud Security. The Threat Detection algorithm employs machine learning for real-time threat identification. The Real-time Incident Response algorithm automates responses based on threat severity, significantly reducing response time compared to traditional manual methods. Lastly, the Explainable AI algorithm enhances interpretability, crucial for ensuring human trust in AI-driven security decisions. A comprehensive comparative analysis is performed against six traditional methods, revealing the proposed system's superiority in threat detection accuracy, rapid incident response, scalability, interpretability, and resource efficiency. The integration of augmented intelligence offers continuous improvement, learning from emerging threats."
pub.1155196857,"Cybersecurity Threats, Vulnerabilities, Mitigation Measures in Industrial Control and Automation Systems: A Technical Review","Cyberattacks on Industrial Control and Automation Systems (ICAS) have significantly increased in recent years due to IT and OT convergence. Traditionally, ICAS were isolated systems running proprietary protocols on specialised software and hardware. However, to improve business processes and efficiency, ICAS vendors are adopting smart technologies such as Industrial Internet of Things (IIOT), Machine to Machine (M2M), Digital Twin, cloud computing, and Artificial Intelligence (AI). This integration presents new vulnerabilities in ICAS that can be exploited by threat actors. ICAS are utilised in critical infrastructure and widely used in power, nuclear plant, water, oil, natural gas, and manufacturing industries. Therefore, cyberattacks on these systems can pose a significant threat to humans and the environment, disrupt social services, cause financial losses, and threaten national security. Because of these threats, numerous mitigation measures are being implemented to protect ICAS from cyberattacks. However, security experience and expertise have demonstrated that we can never fully protect a system and one should never propose that their solution will fully protect. Rather one can claim that their solution/mitigation technique adds a layer to the defence in depth approach. This paper discusses the different cybersecurity standards and frameworks for ICAS, investigates the existing threats and vulnerabilities, and methods of securing ICAS"
pub.1073147440,Performance Evaluation of Power Aware VM Consolidation using Live Migration,"Power Efficiency is the efficient use of power and is a crucial come forth in cloud computing environment. Green Computing is nothing but is a cloud computing with efficient use of power and green refers to make the environment friendly to the user by saving heat and power. Data centre power efficiency in cloud environment will be reduced when virtualization is used as contrary to physical resource deployment to book adequate to grant all application requests. Nevertheless, in any case of the resource provisioning approximation, occasion remains in the way in which they are made attainable and workload is scheduled. The objective of this research work is therefore to pack workload into servers, selected as a function of their cost to operate, to achieve (or as close to) the utmost endorsed employment in a cost-efficient manner, avoiding occurrences where devices are under-utilized and management cost is acquired inefficiently. This work has enhanced the existing work by introducing the dynamic wake up calls either to shut down the active servers or restart the passive server. The wakeup calls has been initiated dynamically. The overall objective is to decrease the response time of users which will be increased during wakeup time in existing research work."
pub.1175173659,An electricity price and energy-efficient workflow scheduling in geographically distributed cloud data centers,"The cloud computing platform has become a favorable destination for running cloud workflow applications. However, they are primarily complicated and require intensive computing. Task scheduling in cloud environments, when formulated as an optimization problem, is proven to be NP-hard. Thus, efficient task scheduling plays a decisive role in minimizing energy costs. Electricity prices fluctuate depending on the vending company, time, and location. Therefore, optimizing energy costs has become a serious issue that one must consider when building workflow applications scheduling across geographically distributed cloud data centers (GD-CDCs). To tackle this issue, we have suggested a dual optimization approach called electricity price and energy-efficient (EPEE) workflow scheduling algorithm that simultaneously considers energy efficiency and fluctuating electricity prices across GD-CDCs, aims to reach the minimum electricity costs of workflow applications under the deadline constraints. This novel integration of dynamic voltage and frequency scaling (DVFS) with energy and electricity price optimization is unique compared to existing methods. Moreover, our EPEE approach, which includes task prioritization, deadline partitioning, data center selection based on energy efficiency and price diversity, and dynamic task scheduling, provides a comprehensive solution that significantly reduces electricity costs and enhances resource utilization. In addition, the inclusion of both generated and original data transmission times further differentiates our approach, offering a more realistic and practical solution for cloud service providers (CSPs). The experimental results reveal that the EPEE model produces better success rates to meet task deadlines, maximize resource utilization, cost and energy efficiencies in comparison to adapted state-of-the-art algorithms for similar problems."
pub.1135752508,Adapting legacy robotic machinery to industry 4: a ciot experiment version 1,"This paper presents an experimental adaptation of a non-collaborative robot
arm to collaborate with the environment, as one step towards adapting legacy
robotic machinery to fit in industry 4.0 requirements. A cloud-based internet
of things (CIoT) service is employed to connect, supervise and control a
robotic arm's motion using the added wireless sensing devices to the
environment. A programmable automation controller (PAC) unit, connected to the
robot arm receives the most recent changes and updates the motion of the robot
arm. The experimental results show that the proposed non-expensive service is
tractable and adaptable to higher level for machine to machine collaboration.
The proposed approach in this paper has industrial and educational
applications. In the proposed approach, the CIoT technology is added as a
technology interface between the sensors to the environment and the robotic
arm. The proposed approach is versatile and fits to variety of applications to
meet the flexible requirements of industry 4.0. The proposed approach has been
implemented in an experiment using MECA 500 robot arm and AMAX 5580
programmable automation controller and ultrasonic proximity wireless sensor."
pub.1113661033,Revolutionizing Subsea Field Development Planning Through System Integration and Advanced Diagnostics,"Abstract Field development planning (FDP) is a multidisciplinary process that is complex, time consuming, and inefficient. Existing workflows are domain centric and involve many discrete software programs, making the process linear, limiting optimization, and often preventing cross-domain collaboration. Although a few software tools exist for parts of the field layout design, no existing software integrates the equipment selection, systems knowledge, field layout, economics, and software simulations spanning the entire planning process from feasibility studies to detail design and optimization. This paper demonstrates how a novel FDP software package revolutionizes integrated concept development within the industry, supporting the process from reservoir engineering through to definition of the subsea system. In early FDP stages, including feasibility and concept selection, operators plan for the development of a field that will produce for 20 years or more, making critical decisions with very limited reservoir information and a high level of uncertainty. Because these early stages are short, operators rarely have the time or resources necessary to evaluate all possible development scenarios, and decisions are delivered with less conviction than is desirable. Hence, operators are in constant search for tools and processes that enable evaluation of more options to help them make better-quality decisions in these early stages, where reducing uncertainty and making decisions are critical to achieving first oil faster. The paper describes an innovative FDP methodology that utilizes the industry's first cloud-based collaborative subsea FDP software environment and provides efficiency and optimization by combining full-field economic calculations with reservoir engineering, production assurance, systems engineering, subsea technology selection and installation knowledge spanning the entire planning process in one visually rich environment. The paper demonstrates how the solution provides the ability to evaluate technical feasibility and economic viability of hundreds of development scenarios during early FDP stages more accurately and in less time compared with traditional methods. The impacts on the industry are numerous and include efficiency gains, risk reduction, and significant system cost savings, thereby improving cash flow and reducing payback periods."
pub.1152868194,A Provably Secure Data Sharing Scheme for Smart Gas Grid in Fog Computing Environment,"The smart gas grid (SGG) implies to enhance current gas distribution grids by establishing continuous on-request and bidirectional data interchange between metering devices, gas flow instrument’s, utilities, and end clients. SGG integrates IoT in gas distribution system and improves the management in hazard minimization for gas infrastructure. As a result, the security and privacy of this type of vital infrastructure are frequently overlooked during the design process. Cloud reserves are frequently researched to store data from smart meters. Regardless of SGG’s exemplary service, this raises concerns about the unpredictable and long-term connections between cloud data hubs and smart meters. To address this issue, the integration of fog computing with SGG emerges as a promising solution. This chapter proposed a hyperelliptic curve (HYEC)-based proxy signcryption scheme based on identities of data owners and data users in fog computing environment (IBPSC-SGG-FCE). The use of fog computing layer provides excellent response times, reliability, and enhanced privacy. HYEC enhances network computation efficiency. Formal security analysis is used to assess the toughness of security measures. Under OFMC and CL-Atse backend, the simulation study using Automated Validation of Internet Security-sensitive Protocols and Applications (AVISPA) tool shows that the proposed scheme is safe. The computation and communication costs of the proposed scheme have also been compared to those of the relevant existing schemes in the performance analysis. The security and performance evaluations show that the proposed scheme is superior."
pub.1164230171,Data Analytics for Internet of Things Infrastructure,"This book provides techniques for the deployment of semantic technologies in data analysis along with the latest applications across the field such as Internet of Things (IoT). The authors focus on the use of the IoT and big data in business intelligence, data management, Hadoop, machine learning, cloud, smart cities, etc. They discuss how the generation of big data by IoT has ruptured the existing data processing capacity of IoT and recommends the adoption of data analytics to strengthen solutions. The book addresses the challenges in designing the web based IoT system, provides a comparative analysis of different advanced approaches in industries, and contains an analysis of databases to provide expert systems. The book aims to bring together leading academic scientists, researchers, and research scholars to exchange and share their experiences and research results on all aspects of IoT and big data analytics. Provides deployment of semantic technologies in dataanalysis along with the latest applications in Internet of Things; Familiarizes readers with the data analysis environment so they can apply it in Internet of Things; Addresses the challenges in designing web based IoT systems."
pub.1130861304,Automated tools for the development of microservice compositions for hybrid scientific computations,"In recent years, a significant amount of research is focused on the development of tools for creating composite web-services for solving both business and scientific complex problems. This study discusses tools for building compositions or ensembles of microservices (depending on the method of integration) developed based on the HPCSOMAS framework. These tools are oriented on the application in a package of applied microservices for solving computationally complex problems of structural analysis and parametric synthesis of controlled dynamic systems in a heterogeneous high-performance computing environment. In particular, binary dynamic systems are studied using the Boolean constraint method for both their qualitative analysis and synthesis of laws to control these systems. Creating and executing composite services is carried out on a semantic peer-to-peer network of agents. The HPCSOMAS framework supports two modes of these processes, both the static creation and application of a composite service based on the procedural formulation of the problem and dynamic, based on the declarative formulation. In the first case, agents deployed on the network perform hierarchical control over the execution of the composition of microservices, in the second case, decentralized asynchronous management of the ensemble of microservices. Both operating modes are automated, and the validity of the resulting composite service is checked based on a logical approach. The tools are aimed both at a professional programmer and the end-user, a specialist in the subject domain. The HPCSOMAS framework supports the execution of composite microservices in a hybrid computing infrastructure, which includes both cloud and on-premises resources."
pub.1157430666,Research on Unified Cloud Authentication and Centralized Management of Power Internet of Things,"At present, the user identity management and authentication of each system are completed in each business system. Meanwhile the users of the application system is extended to non-power company users, such as application developers, network maintenance providers, and other temporary users. The disadvantages of the this management model under the rapid increase in the number of business systems and the increasing number of users are increasingly prominent. Identity authentication and management is not a new issue, but as the security boundaries change, identity management vulnerabilities create increasing security problems for the entire system. This paper aims to study the unified cloud authentication and centralized management of the power Internet of Things. This paper first analyzes the research status and related technologies of the Internet of Things and the unified authentication, and points out that the existing unified authentication system cannot meet the needs of the authentication and security in the Internet of Things environment, and there are complex problems of the authorization management in the unified authorization. Through the analysis of the application system security requirements, combined with the access control and unified authorization related methods, according to the idea of the application system integration, the hierarchical authentication and authorization method is proposed. This method is mainly divided into three parts: hierarchical access policy, user security policy, and unified authorization policy. In the hierarchical access strategy, on the basis of existing authentication, it increases the verification of user dynamic authentication, and calculates the user security level, which dynamically reflects user security. In the unified authorization strategy, it effectively reduces the complexity of authority management in the unified authorization. Finally, the prototype system is designed and implemented in the case to verify the feasibility of the proposed method."
pub.1181660929,Application and Development of Digital Twins in Smart Cities,"This paper synthesizes existing research to explore the synergistic relationship between the advancement of digital twin technology and the evolving landscape of smart cities. It delves into the mechanisms by which digital twins are integrated into smart city frameworks, positioning digital twins as pivotal components in the evolution of these urban environments. Understanding the influence of digital twin technology on smart cities is crucial for fostering the deployment of digital technologies in future urban settings. Moving forward, the seamless integration and application of 5G, cloud computing, the Internet of Things (IoT), blockchain, artificial intelligence, digital twins, and 3R technologies (virtual reality, augmented reality, mixed reality) will be essential for the development and management of smart cities. The investigation into the utilization of digital twin technology within smart cities aims to enhance the efficiency of resource allocation and urban governance."
pub.1094904641,Hybrid Spot Instance Based Resource Provisioning Strategy in Dynamic Cloud Environment,"Utilization of resources to the maximum extent in large scale distributed cloud environment is a major challenge due to the nature of cloud. Spot Instances in the Amazon Elastic Compute Cloud (EC2) are provisioned based on highest bid with no guarantee of task completion but incurs the overhead of longer task execution time and price. The paper demonstrates the last partial hour and cost overhead that can be avoided by the proposed strategy of Hybrid Spot Instance. It aims to provide reliable service to the ongoing task so as to complete the execution without abruptly interrupting the long running tasks by redefining the bid price. The strategy also considers that on-demand resource services can be acquired when spot price crosses on-demand price and thereby availing high reliability. This will overcome the overhead involved during checkpointing, restarting and workload migration as in the existing system, leading to efficient resources usage for both the providers and users. Service providers revenue is carefully optimized by eliminating the free issue of last partial hour which is a taxing factor for the provider. Simulation carried out based on real time price of various instances considering heterogenous applications shows that the number of out-of-bid scenarios can be reduced largely which leads to the increased number of task completion. Checkpointing is also minimized maximally due to which the overhead associated with it is reduced. This resource provisioning strategy aims to provide preference to existing customers and the task which are nearing the execution completion."
pub.1094504708,Digital Rights Management and Its Evolution in the Context of IPTV Platforms in the Web Domain,"This paper introduces a novel mechanism for content protection in cloud-based IPTV. The architecture presented here integrates Digital Rights Management into the web-based landscape, where the IPTV user interface is being remotely hosted or executed. The platforms for the user interface (UI) execution studied here are browser-based execution approaches, where the execution environment is provided by the browser. The browser resides on the client-side in the context of browser-based UI execution, whereas in a fully cloud-based execution the browser is shifted into the cloud. The client-side implementation for these platforms is done on the same Set-Top Box hardware to demonstrate the response time difference for both approaches. The response time considered in this paper refers to the UI execution and video encryption for unicast and multicast. This paper also introduces a breakdown of response time by introducing a method for load time measurements in the context of remote IPTV middleware. Moreover the paper presents the evolution of Digital Rights Management systems from currently existing proprietary approaches to web-based Encrypted Media Extensions in the IPTV domain, its impact on the browser-and cloud - based execution of the IPTV user interface, challenges and open issues."
pub.1123271977,Toward Fog-Based Mobile Crowdsensing Systems: State of the Art and Opportunities,"MCS is an emerging paradigm that leverages the pervasiveness of mobile, wearable, and vehicle-mounted devices to collect data from urban environments for ubiquitous service provisioning. In order to manage MCS application data streams efficiently, a scalable computing infrastructure hosting heterogeneous and distributed resources is critical. FC, as a geo-distributed computing paradigm, is a key enabler for this requirement as it bridges cloud servers and smart mobile devices. Research on the integration of MCS with FC has recently started to be explored, recognizing the requirements of MCS and their coexistence with cyber-physical systems. In this article, we analyze the state of the art of FC solutions in MCS systems. After a brief overview of MCS, we emphasize the link between MCS and FC. We then investigate the existing fog-based MCS architectures in detail by focusing on their building blocks, as well as the challenges that remain unaddressed. Our detailed review on the subject results in a taxonomy of FC solutions in MCS systems. In particular, we highlight the node structures, the information exchanged, the resource and service management, and the type of solutions adopted concerning privacy and security. Moreover, we provide a thorough discussion on the open issues and challenges by reporting useful insights for researchers in MCS and FC."
pub.1021931207,Linking ERP and e-Business to a Framework of an Integrated e-Supply Chain,"Recent developments have created an opportunity for organizations to leverage Web-based technologies. Such organizational initiatives need to be supported by sound existing infrastructures based on well-functioning Enterprise Resource Planning (ERP) systems. Also, business processes in multiple organizations across the supply chain need to be integrated to forge tighter links, from raw materials to customers. This chapter examines the evolving relationship between ERP and e-Business. We study how organizations can gain competitive advantage by leveraging the complementarities between these two technologies. We present a framework of e-Supply Chain Management (e-SCM) which facilitates the integration of business processes across the supply chain. We also discuss the recent developments in the area of cloud computing and its impact on the Internet-enabled supply chain environment."
pub.1026886981,"On the design of scalable, self-configuring virtual networks","Virtual networks (VNs) provide methods that simplify resource management, deal with connectivity constraints, and support legacy applications in distributed systems, by enabling global addressability of VN-connected machines through either a common layer 2 Ethernet or a NAT-free layer 3 IP network. This paper presents a novel VN design that supports dynamic, seamless addition of new resources with emphasis on scalability in a unified private IP address space. Key features of this system are: (1) Scalable connectivity via a P2P overlay with the ability to bypass overlay routing in LAN communications, (2) support for static and dynamic address allocation in conjunction with virtual nameservers through a distributed data store, and (3) support for transparent migration of IP endpoints across widearea networks. The approach is validated by a prototype implementation which has been deployed in grid and cloud environments. We present both a quantitative and qualitative discussion of our findings."
pub.1140633348,TZ-Container: protecting container from untrusted OS with ARM TrustZone,"Containers are widely deployed on cloud platforms because of their low resource footprint, fast start-up time, and high performance, especially compared with its counterpart virtual machines. However, the Achilles’ heel of container technology is its weak isolation. For an attacker, jailbreaking into a host OS from a container is relatively easier than attacking a hypervisor from a virtual machine, because of its notably larger attack surface and larger trusted computing base (TCB). Researchers have proposed various solutions to protect applications from untrusted OS; yet, few of them focus on protecting containers, especially those hosting multiple applications and shared by multiple users. In this paper, we first identify several new attacks that cannot be prevented using the existing solutions. Furthermore, we systematically analyze the security properties that should be maintained to defend against these attacks and protect a full-fledged container from a malicious host OS. We then present the TZ-Container, a TrustZone-based secure container mechanism that can keep all these security properties. The TZ-Container specifically leverages TrustZone to construct multiple isolated execution environments (IEEs). Each IEE has a memory space isolated from the underlying OS and any other processes. By interposing switching between the user and the kernel modes, IEEs enforce security checks on each system call according to its semantics. We have implemented TZ-Container on the Hikey development board ensuring that it can support running unmodified Docker images downloaded from existing repositories such as https://hub.docker.com/. The evaluation results demonstrate that the TZ-Container has a performance overhead of approximately 5%."
pub.1171197280,Cloud-Edge Collaborative Continual Adaptation for ITS Object Detection,"In the field of Intelligent Transportation Systems (ITS), the challenge of performance degradation in lightweight object detection models on edge devices is significant. This issue primarily arises from environmental changes and shifts in data distribution. The problem is twofold: the limited computational capacity of edge devices, which hinders timely model updates, and the inherent limitations in the generalization capabilities of lightweight models. While large-scale models may have superior generalization, their deployment at the edge is impractical due to computational constraints. To address this challenge, we propose a cloud-edge collaborative continual adaptation learning framework, specifically designed for the DETR model family, aimed at enhancing the generalization ability of lightweight edge models. This framework uses visual prompts to collect and upload data from the edge, which helps to fine-tune cloud-based models for improved target domain generalization. The refined knowledge is then distilled back into the edge models, enabling continuous adaptation to diverse and dynamic conditions. The effectiveness of this approach has been validated through extensive experiments on two datasets for traffic object detection in dynamic environments. The results indicate that our learning method outperforms existing techniques in continual adaptation and cloud-edge collaboration, highlighting its potential in addressing the challenges posed by dynamic environmental changes in ITS."
pub.1136564484,Energy-makespan optimization of workflow scheduling in fog–cloud computing,"The rapid evolution of smart services and Internet of Things devices accessing cloud data centers can lead to network congestion and increased latency. Fog computing, focusing on ubiquitously connected heterogeneous devices, addresses latency and privacy requirements of workflows executing at the network edge. However, allocating resources in this paradigm is challenging due to the complex and strict Quality of Service constraints. Moreover, simultaneously optimizing conflicting objectives, e.g., energy consumption and workflow makespan increases the complexity of the scheduling process. We investigate workflow scheduling in fog–cloud environments to provide an energy-efficient task schedule within acceptable application completion times. We introduce a scheduling algorithm, Energy Makespan Multi-Objective Optimization, that works in two phases. First, it models the problem as a multi-objective optimization problem and computes a tradeoff between conflicting objectives while allocating fog and cloud resources, and schedules latency-sensitive tasks (with lower computational requirements) to fog resources and computationally complex tasks (with low latency requirements) on cloud resources. We adapt the Deadline-Aware stepwise Frequency Scaling approach to further reduce energy consumption by utilizing unused time slots between two already scheduled tasks on a single node. Our evaluation using synthesized and real-world applications shows that our approach reduces energy consumption, up to 50%, as compared to existing approaches with minimal impact on completion times."
pub.1160136596,A novel multi-level hybrid load balancing and tasks scheduling algorithm for cloud computing environment,"Today cloud computing is at the heart of all information technologies. This prodigious technological paradigm relies on a very simple concept defined as the ability to deliver hardware and software resources as service directly over internet. A set of mechanisms cooperate to maintain the cloud reliability and to allow continuous delivery of these services while guaranteeing the same quality of service (QoS) and respecting the service-level agreement (SLA) for each client. Load balancing is one of those mechanisms and it ensures a crucial service, it can be defined as the ability of the system to ensure fairness in the distribution of workload over all servers. The most recent load balancing techniques are hybrid methods involving in major cases the combination between static and dynamic approaches, in other cases it can go further by integrating other mechanisms in order to improve the overall efficiency of the systems. The performance can be evaluated by parameters which generally refers to the degree of compliance with SLA and QoS. In order to enhance load balancing and tasks scheduling in cloud environment we propose in this paper a different hybrid approach which allows the decomposition of the problem and to operate on two levels by going through two stages : (i) first clusters are built for each datacenter grouping together sub-sets of servers that have close utilization rates, (ii) then tasks scheduling and load balancing operate at the datacenter level to deal with distribution over clusters and at the cluster level to ensure fairness between servers of the same cluster. Our method allows hot-deployment in already operating cloud environments and an excellent scalability. It also offers decoupling of missions and strong interoperability between the different mechanisms. To prove its validity, we implemented it on the standard cloudSim plus simulator, before carrying out a comparative study which shows better results than existing approaches in terms of makespan, reduces reaction time, number of migrations required and SLA violations."
pub.1144994814,"Digital Twins in Built Environments: An Investigation of the Characteristics, Applications, and Challenges","The concept of digital twins is proposed as a new technology-led advancement to support the processes of the design, construction, and operation of built assets. Commonalities between the emerging definitions of digital twins describe them as digital or cyber environments that are bidirectionally-linked to their physical or real-life replica to enable simulation and data-centric decision making. Studies have started to investigate their role in the digitalization of asset delivery, including the management of built assets at different levels within the building and infrastructure sectors. However, questions persist regarding their actual applications and implementation challenges, including their integration with other digital technologies (i.e., building information modeling, virtual and augmented reality, Internet of Things, artificial intelligence, and cloud computing). Within the built environment context, this study seeks to analyze the definitions and characteristics of a digital twin, its interactions with other digital technologies used in built asset delivery and operation, and its applications and challenges. To achieve this aim, the research utilizes a thorough literature review and semi-structured interviews with ten industry experts. The literature review explores the merits and the relevance of digital twins relative to existing digital technologies and highlights potential applications and challenges for their implementation. The data from the semi-structured interviews are classified into five themes: definitions and enablers of digital twins, applications and benefits, implementation challenges, existing practical applications, and future development. The findings provide a point of departure for future research aimed at clarifying the relationship between digital twins and other digital technologies and their key implementation challenges."
pub.1165778390,Prism: Revealing Hidden Functional Clusters from Massive Instances in Cloud Systems,"Ensuring the reliability of cloud systems is critical for both cloud vendors and customers. Cloud systems often rely on virtualization techniques to create instances of hardware resources, such as virtual machines. However, virtualization hinders the observability of cloud systems, making it challenging to diagnose platform-level issues. To improve system observability, we propose to infer functional clusters of instances, i.e., groups of instances having similar functionalities. We first conduct a pilot study on a large-scale cloud system, i.e., Huawei Cloud, demonstrating that instances having similar functionalities share similar communication and resource usage patterns. Motivated by these findings, we formulate the identification of functional clusters as a clustering problem and propose a non-intrusive solution called Prism. Prism adopts a coarse-to-fine clustering strategy. It first partitions instances into coarse-grained chunks based on communication patterns. Within each chunk, Prism further groups instances with similar resource usage patterns to produce fine-grained functional clusters. Such a design reduces noises in the data and allows Prism to process massive instances efficiently. We evaluate Prism on two datasets collected from the real-world production environment of Huawei Cloud. Our experiments show that Prism achieves a v-measure of ~0.95, surpassing existing state-of-the-art solutions. Additionally, we illustrate the integration of Prism within monitoring systems for enhanced cloud reliability through two real-world use cases."
pub.1112644018,A Model For Policy-aware Execution of Business Process in Federated Clouds,"Today, Organization's daily operations rely on automatic business processes running on IT infrastructures. On the other hand, fast-changing business environments along with the costly process of in-house development of business process management systems drives the businesses to outsource their business processes. Cloud computing paradigm has become increasingly important for executing business processes as users can exploit the economic and technical advantages of this computing. The Business Process as a Service (BPaaS) paradigm is a new concept in using specific business processes as an intermediary to align business and information technology. The management and deployment of business processes on the existing heterogeneous Cloud providers, however, is still a challenge for the organizations due to interoperability concerns. This paper suggests a Federated BPaaS model, a service aggregation concept characterized by interoperability specifications, to address the integration challenges and the vendor lock-in problem. The analysis and comparison of Policy-Based Approaches shows that our model provides a complete solution for the Process as a Service based on architectural concepts."
pub.1110503433,Multi-user Multi-provider Resource Allocation in Cloud Computing**This work is in part supported by the National Natural Science Foundation of China under Grants 61472005 and 61201252 and CERNET Innovation Project under Grant NGII20160207.,"In a cloud computing environment, cloud service providers offer cloud services to users. How to achieve reasonable and efficient service resource allocation to meet a user's demands is an important problem. Considering a multi-user and multi-provider environment, we propose a service resource allocation framework to optimize an objective function of load and completion time. We design an improved differential evolution algorithm for optimal resource allocation given a batch of multiple tasks. Experimental results show that proposed method can better balance loads among computing resources while achieving better optimized results for the entire system than some existing methods."
pub.1163316966,Prism: Revealing Hidden Functional Clusters from Massive Instances in Cloud Systems,"Ensuring the reliability of cloud systems is critical for both cloud vendors
and customers. Cloud systems often rely on virtualization techniques to create
instances of hardware resources, such as virtual machines. However,
virtualization hinders the observability of cloud systems, making it
challenging to diagnose platform-level issues. To improve system observability,
we propose to infer functional clusters of instances, i.e., groups of instances
having similar functionalities. We first conduct a pilot study on a large-scale
cloud system, i.e., Huawei Cloud, demonstrating that instances having similar
functionalities share similar communication and resource usage patterns.
Motivated by these findings, we formulate the identification of functional
clusters as a clustering problem and propose a non-intrusive solution called
Prism. Prism adopts a coarse-to-fine clustering strategy. It first partitions
instances into coarse-grained chunks based on communication patterns. Within
each chunk, Prism further groups instances with similar resource usage patterns
to produce fine-grained functional clusters. Such a design reduces noises in
the data and allows Prism to process massive instances efficiently. We evaluate
Prism on two datasets collected from the real-world production environment of
Huawei Cloud. Our experiments show that Prism achieves a v-measure of ~0.95,
surpassing existing state-of-the-art solutions. Additionally, we illustrate the
integration of Prism within monitoring systems for enhanced cloud reliability
through two real-world use cases."
pub.1174381261,An Agent Based Model for Ransomware Detection and Mitigation in a Cloud System,"The increasing trend toward dematerialization and digitalization has prompted a surge in the adoption of IT service providers, offering cost-effective alternatives to traditional local services. Consequently, cloud services have become prevalent across various industries. While these services offer undeniable benefits, they face significant threats, particularly concerning the sensitivity of the data they handle. Many existing mathematical models struggle to accurately depict the complex scenarios of cloud systems. In response to this challenge, this paper proposes a behavioral model for ransomware propagation within such environments. In this model, each component of the environment is defined as an agent responsible for monitoring the propagation of malware. Given the distinct characteristics and criticality of these agents, the impact of malware can vary significantly. Scenario attacks are constructed based on real-world vulnerabilities documented in the Common Vulnerabilities and Exposures (CVEs) through the National Vulnerability Database. Defender actions are guided by an Intrusion Detection System (IDS) guideline. This research aims to provide a comprehensive framework for understanding and addressing ransomware threats in cloud systems. By leveraging an agent- based approach and real-world vulnerability data, our model offers valuable insights into detection and mitigation strategies for safeguarding sensitive cloud-based assets."
pub.1149989166,Interactive Visualization of Protein RINs using NetworKit in the Cloud,"Network analysis has been applied in diverse application domains. We consider an application from protein dynamics, specifically residue interaction networks (RINs). While numerous RIN visualization tools exist, there are no solutions that are both easily programmable and as fast as optimized network analysis toolkits. In this work, we use NetworKit - an established package for network analysis - to build a cloud-based environment that enables domain scientists to run their visualization and analysis workflows on large compute servers, without requiring extensive programming and/or system administration knowledge. To demonstrate the versatility of this approach, we use it to build a custom Jupyter-based widget for RIN visualization. In contrast to existing RIN visualization approaches, our widget can easily be customized through simple modifications of Python code, while both supporting a comprehensive feature set and providing near real-time speed. Due to its integration into Jupyter notebooks, our widget can easily interact with other popular packages of the Python ecosystem to build custom analysis pipelines (e.g., pipelines that feed RIN data into downstream machine learning tasks)."
pub.1130926518,"Advanced Platform Development with Kubernetes, Enabling Data Management, the Internet of Things, Blockchain, and Machine Learning","Leverage Kubernetes for the rapid adoption of emerging technologies. Kubernetes is the future of enterprise platform development and has become the most popular, and often considered the most robust, container orchestration system available today. This book focuses on platforming technologies that power the Internet of Things, Blockchain, Machine Learning, and the many layers of data and application management supporting them. Advanced Platform Development with Kubernetes takes you through the process of building platforms with these in-demand capabilities. You'll progress through the development of Serverless, CICD integration, data processing pipelines, event queues, distributed query engines, modern data warehouses, data lakes, distributed object storage, indexing and analytics, data routing and transformation, query engines, and data science/machine learning environments. You’ll also see how to implement and tie together numerous essential and trending technologies including: Kafka, NiFi, Hive, Keycloak, Cassandra, MySQL, Zookeeper, Mosquitto, Elasticsearch, Logstash, Kibana, Presto, Mino, OpenFaaS, Ethereum, WireGuard, MLflow and Seldon Core. The book uses Golang and Python to demonstrate the development integration of custom container and Serverless functions, including interaction with the Kubernetes API. The exercises throughout teach Kubernetes through the lens of platform development, expressing the power and flexibility of Kubernetes with clear and pragmatic examples. Discover why Kubernetes is an excellent choice for any individual or organization looking to embark on developing a successful data and application platform. What You'll Learn · Configure and install Kubernetes and k3s on vendor-neutral platforms, including generic virtual machines and bare metal · Implement an integrated development toolchain for continuous integration and deployment · Use data pipelines with MQTT, NiFi, Logstash, Kafka and Elasticsearch · Install a serverless platform with OpenFaaS · Explore blockchain network capabilities with Ethereum · Support a multi-tenant data science platform and web IDE with JupyterHub, MLflow and Seldon Core · Build a hybrid cluster, securely bridging on-premise and cloud-based Kubernetes nodes"
pub.1095872182,Research on Evolution and Visualization Analysis of Application of Big Data,"Based on literatures relevant to application of big data included in the Web of Science database as the data sources, this paper used CiteSpace as the research tool to visualize the distribution of keywords, evolution of hot spots and evolution rules. After that, it analyzed the hot spots, contexts, tool technologies and application fields of application of big data, so as to reveal the current situation of researches. The study indicates that existing researches involve a wide range of disciplines and technology is still the center of current researches. At present, the majority of the studies all focused on the field of management, network, information, medicine, health, environment, energy and etc. Overall, technology is still the important part in current research on application of big data. Technical tools, such as data mining, algorithms, cloud computing, mapreduce, hadoop, machine learning all provide strong supports for the practical application of big data."
pub.1094150354,Modeling Correlation between QoS Attributes for Trust Computation in Cloud Computing Environments,"This paper introduces an approach that handles with the trustworthy cloud service selection issue in Cloud computing environments. Despite the fact that most of the existing trust systems consider several QoS attributes for trust computing, none of them did consider the correlation that may exist among these attributes. However, we demonstrate in this paper that the integration of correlation between QoS attributes in trust computing significantly helps to solve many issues such as predicting missing assessment, detecting malicious feedbacks and improving the accuracy of trust values. The main goal of this paper is to show the significance of correlation between those attributes for trust computing. To do that, we propose combining both the popular Nave Bayes model and the n-gram Markov model to design a more efficient trust model for cloud environments. Our proposed trust model also takes into account the user's requirements, aggregates both the qualitative and quantitative assessment, and considers several sources when computing cloud services' trust values. Experimental results show that our proposed approach outperforms the traditional Nave Bayes trust models."
pub.1156380585,Exploring Multi-Criteria Decision-Making Methods in ERP Selection,"Enterprise resource planning (ERP) adoption literature has a consensus that selecting the right ERP system is one of the most critical success factors in the ERP adoption lifecycle. While choosing a non-fitting ERP system may lead to adoption failures, however very few papers focus solely on this selection phase. Hence, given the criticality of the ERP selection phase, this paper aims to identify and review the different ERP selection methods in extant literature. This research also presents the factors and variables included in each identified selection method in ERP literature. As a result, each method identified was reviewed, analyzed, and summarized. Our main findings suggest that ERP selection is a multi-criteria decision-making (MCDM) problem, with various methods and techniques that can be utilized for such problems. Several MCDM methods have been used in literature, but often complementing more than one method combined at a time. This is since some methods excel in considering factors in uncertain environments, and other methods are best in evaluating qualitative and quantitative factors. Finally, while there are some methods that were used for cloud-ERP selections, there is no clear consensus in extant literature if some methods could best fit specifically cloud-ERP contexts in contrast to on-premises counterparts."
pub.1116852189,Enhancing the Reliability of NFV with Heterogeneous Backup,"Virtual network function provides tenant with flexible and scalable end-to-end service chaining in the cloud computing and data center environments. However, comparing with traditional hardware network devices, the uncertainty caused by software and virtualization of Network Function Virtualization expands the attack surface, making the network node vulnerable to a certain types of attacks. The existing approaches for solving the problem of reliability are able to reduce the impact of failure of physical devices, but pay little attention to the attack scenario, which could be persistent and covert. In this paper, a heterogeneous backup strategy is brought up, enhancing the intrusion tolerance of NFV SFC by dynamically switching the VNF executor. The validity of the method is verified by simulation and game theory analysis."
pub.1170010887,SLICES Data Management Infrastructure for Reproducible Experimental Research on Digital Technologies,"This paper presents the ongoing research effort related to the design of the Data Management Infrastructure (DMI) to support experimental research on digital technologies with application to the ESFRI SLICES scientific instrument. We consider the experiment documentation and data collection across the whole continuum of access network, IoT, edge, cloud, and data processing workflow. The paper includes the requirements analysis for DMI to enable research reproducibility of complex and large-scale experimentation. We provide an analysis of data collected and processed in SLICES and explain approaches and solutions used in SLICES for experimental research reproducibility, primarily based on the plain orchestration service and supported by metadata collection tools. The proposed multi-layer DMI includes: data (storage) access, data processing, data ingest, experiment management, and virtual research environment. The paper also provides recommendations for the selection of existing standards and tools for data and metadata management, in particular those developed by EOSC and supported by the RDA community to ensure wide compatibility and integration."
pub.1109868415,Development and Field Trial of the World's 1st Cloud Connected Wireless Intelligent Completion System,"Abstract This paper describes the development and field trials of a wireless intelligent completion system that enables long term monitoring and interval control to enhance production management within new or existing wellbores by connecting the user wirelessly from the desktop to the downhole component of the system, inflow control valves (ICVs). The effectiveness of intelligent completions for production optimisation and improving reservoir management is well established, yet the use of this technology remains limited to high value, single bore wells. The cost and complexity of these solutions coupled with limitations in well types, interval quantity and poor system interfaces are all factors which have prevented broader application. An intelligent completion that communicates wirelessly within the wellbore provides dual benefits in completion operations and production management. Eliminating the requirement for control lines addresses many of the limitations of traditional systems, and the all-electric technology provides greater scope for digital well management and integration with surface systems. A slimline inflow control valve (ICV) has been developed which provides infinitely variable choking capability and multiple integrated sensors. Qualification has been completed to Statoil's/Equinor's existing standard for the qualification of interval control valves adapted and augmented to reflect the differences between the wireless solution and conventional technology. Several field installations have been performed with the system, two of which are detailed within this paper to demonstrate the performance of the technology. In the first, operations were performed with local only access to surface data which demonstrated the ability to communicate effectively but highlighted some mechanical limitations in the tool itself. Following implementation of design improvements, a second installation was performed which also included a surface management system with wireless interpretation software and cloud connectivity. The system has been proven to provide robust communication using pressure pulses within the flowing well stream to provide operating instructions to the ICV, and for the ICV to operate as instructed and communicate effectively to the wellhead. Data from the system has been digitally managed and remotely accessed demonstrating the ability to change tool settings remotely. Integrating a semi-duplex pressure pulse system which operates in compressive fluid environments to provide direct communication between a downhole device and wellhead provides a low energy communication method compatible with extended service life completion tools. The system has been demonstrated with ICV technology in both multiphase and gas environments. In field trial this has been integrated with an intelligent system effectively deploying the world's 1st cloud connected wireless intelligent completion."
pub.1117014130,SAVE: self-adaptive consolidation of virtual machines for energy efficiency of CPU-intensive applications in the cloud,"In virtualized data centers, consolidation of virtual machines (VMs) on minimizing the number of total physical machines (PMs) has been recognized as a very efficient approach. This paper considers the energy-efficient consolidation of VMs in a cloud datacenter. Concentrating on CPU-intensive applications, the objective is to schedule all requests non-preemptively, subjecting to constraints of PM capacities and running time interval spans, to make the total energy consumption of all PMs is minimized (called MinTE for abbreviation). The MinTE problem is NP-complete in general. We propose a self-adaptive approach called SAVE. The approach makes decisions of the assignment and migration of VMs by probabilistic processes and is based exclusively on local information. Both simulation and real environment test show that our proposed method SAVE can reduce energy consumption about 30%$$30\%$$ against VMWare DRS and 10–20% against ecoCloud on average. Extensive experiments show that our method outperforms the existing method and achieves significant energy savings and high utilization."
pub.1061541881,Tier-Centric Resource Allocation in Multi-Tier Cloud Systems,"In IT service delivery and support, the cloud paradigm has introduced the problem of resource over-provisioning through rapid automation (or orchestration) of manual IT operations. Due to the elastic nature of cloud computing, this shortcoming ends up significantly reducing the real benefit, viz., the cost-effectiveness of cloud adoption for Cloud Service Consumers (CSC). Similarly, detecting and eliminating such over-provisioning of cloud resources without affecting the quality of service (QoS) is extremely difficult for Cloud Service Providers (CSPs) since they have visibility only into the state of the IT services (cloud resources) but none into the actual performance of business services. In this paper, we propose Tier-centric Business Impact and Cost Analysis (T-BICA), a tier-centric optimal resource allocation algorithm, to address the problem of rapid provisioning of IT resources in modern enterprise cloud environments, through extensive data gathering and performance analyses of business services in a simulated environment emulating a mature cloud service provider. We have derived improved analytics to address the issues and to accelerate real cloud adoption for large enterprises within the context of meeting (or exceeding) business service level objectives (SLOs) and minimizing the cloud subscription cost (OpEx) for the business. While investigating the problem, we consider the time and the cost of delivering business service in mediumto large-size enterprise environments, quantifying the negative impact of IT resource over-provisioning (due to highly mature IT services centric orchestration capabilities) on the business, and indicate how the suggested cloud analytics could assist in reducing total cost of ownership (TCO) of the business service. From our analysis of the test data, we have observed that our suggested approach and analytic reduces the cost of delivering business services by 65.19 percent, and improves the performance (total time to deliver) by 74.18 percent when compared to the existing modern cloud management and resource provisioning approach. Using T-BICA dramatically reduces upfront costs (CapEx) for CSPs (from the capacity procurement and management points of view) through efficient on-demand resource de-provisioning, without affecting business SLOs and IT service level agreements (SLAs). The improved dynamic allocation of resources also makes for better efficiency of utilization, which in turn has desirable consequences for sustainability, and makes this an approach for “Green” IT."
pub.1101178516,A Review on Security Aspects in 5G Systems: VLSI Perspective,"In the present scenario, security is the prime requirement because of the increasing usage of the internet or public cloud for storing the data. It aims at preserving the integrity, confidentiality, availability of the information system resources. The trend towards ubiquitous computing environment can be envisioned by addressing the 5G system. On one hand, the integration of multiple existing progressive technologies can target the objective, but, may lead to vast security challenges in the future 5G mobile networks. Nowadays, security in the public environment is necessary for preserving the data secrecy. The available private environments are more expensive than public area. As a result, everyone is agreeable with internet i.e. public cloud. Though several cryptographic techniques are proposed in Very Large Scale Integration with the consideration in area, power and speed. This paper concentrates on the survey of various cryptographic algorithms for existing systems of 3G, 4G and analyze the necessity for additional security aspects for 5G and suggests the choice of the encryption scheme that fits in the 5G pack to improve its performance."
pub.1095219260,Big Data Security,"Technology today has progressed to an extent wherein collection of data is possible for every granular aspect of a business, in real time. Electronic devices, power grids and modern software all generate huge volumes of data, which is in the form of petabytes, exabytes and zettabytes. It is important to secure existing big data environments due to increasing threats of breaches and leaks from confidential data and increased adoption of cloud technologies due to the ability of buying processing power and storage on-demand. This is exposing traditional and new data warehouses and repositories to the outside world and at the risk of being compromised to hackers and malicious outsiders and insiders. In this paper the current big data scenario has been summarized along with challenges faced and security issues that need attention. Also some existing approaches have been described to illustrate current and standard directions to solving the issues."
pub.1132670766,Managing the CERN Batch System with Kubernetes,"The CERN Batch Service faces many challenges in order to get ready for the computing demands of future LHC runs. These challenges require that we look at all potential resources, assessing how efficiently we use them and that we explore different alternatives to exploit opportunistic resources in our infrastructure as well as outside of the CERN computing centre. Several projects, like BEER, Helix Nebula Science Cloud and the new OCRE project, have proven our ability to run batch workloads on a wide range of non-traditional resources. However, the challenge is not only to obtain the raw compute resources needed but how to define an operational model that is cost and time efficient, scalable and flexible enough to adapt to a heterogeneous infrastructure. In order to tackle both the provisioning and operational challenges it was decided to use Kubernetes. By using Kubernetes we benefit from a de-facto standard in containerised environments, available in nearly all cloud providers and surrounded by a vibrant ecosystem of open-source projects. Leveraging Kubernetes’ built-in functionality, and other open-source tools such as Helm, Terraform and GitLab CI, we have deployed a first cluster prototype which we discuss in detail. The effort has simplified many of the existing operational procedures we currently have, but has also made us rethink established procedures and assumptions that were only valid in a VM-based cloud environment. This contribution presents how we have adopted Kubernetes into the CERN Batch Service, the impact its adoption has in daily operations, a comparison on resource usage efficiency and the experience so far evolving our infrastructure towards this model."
pub.1140296781,Noncontact Sleep Monitoring System Under a Mattress,"Sleep quality and duration are critical indicators of individual health. Traditional methods of monitoring them either were limited to the application environment or resulted in discomfort. To overcome these existing limitations, we propose a noncontact sleep monitoring system placed under the mattress, named SleepMatrix, to provide users with a comprehensive picture of their real-time to long-term health status. SleepMatrix is a sleep monitoring system that consists of two parts: a front-end sensor array and a cloud-edge data processing system. The data-driven system, based on the Internet of Things, can be flexibly deployed in environments from homes to transportation, and a variety of centralized control and monitoring functions can be flexibly deployed under different application scenarios. At the same time, users can view real-time and historical data through supporting applications and understand the sleep monitoring results. SleepMatrix is a complete and systematic sleep monitoring tool that can help people understand their health status and its evolution from a new perspective and therefore may contribute greatly to informing users of potential illnesses."
pub.1042512398,Cloud2Bubble,"In recent years the mass adoption of mobile devices and increasingly ubiquitous connectivity have contributed to a radical change in the way people interact with computer systems. Moreover cloud computing infrastructures have paved the way for the development of smart systems in such settings, whose goal is to provide a service to enhance user experience based on environment and user sensed data. In this context, there is a clear disconnection between the two streams that flow continuously between user and cloud-based systems. On the one hand, user- and environment-generated data is being, for the most part, disregarded by service providers. On the other hand, services offered do not address users' specific needs and preferences. In addition, service discovery is a cognitive demanding process and it may have detrimental consequences in user experience. In this paper we propose a user-centric framework that addresses the disconnection between these two streams: Cloud2Bubble. The framework facilitates the design and development of smart systems. It aims at leveraging existing technology, such as environment sensors and personal devices, to aggregate localised user-related data - defined as a bubble - into the cloud. This aggregation later supports the delivery of personalised services, contextually relevant to users. The delivery of services with such characteristics has the potential to enhance quality of experience and influence user behaviour. A first iteration of the platform was developed and an evaluation in a simulated environment was performed with encouraging results. Thus, the platform will be further expanded for instantiation and evaluation in the context of urban public transports. We intend to investigate the effects of relevant service delivery in terms of enhancement of quality of experience and influencing user behaviour. The delivery of a service with these characteristics presents benefits for both users and service providers."
pub.1181456833,Stochastic measurement-based multi-cloud consolidation for efficient resource distribution,"In distributed cloud systems, due to high computational complexity, existing methods cannot infinitely reduce the measurement interval to obtain the mean demand. However, considering both the unpredictability of demand and the diversity of pricing adds significant complexity to the problem. In response, we introduce an efficient algorithm called Stochastic Demand-oriented Resource consolidation based on Meta-heuristics (SDRM). The algorithm introduces the stochastic demand model and formulates a cross-cloud demand consolidation and resource scheduling problem, and tries to solve it efficiently by combining differential evolution algorithm with dynamic programming optimization. The results are provided for experiments utilizing both simulated and real-world data. It demonstrates that compared with traditional algorithms, increasing gain by up to 53%, SDRM just incurs an affordable time expense (i.e., 1.10-2.51 times that of existing algorithms). Therefore, SDRM stands as a compelling solution for resource distribution in heterogeneous cloud environments."
pub.1122153951,PERFoRM: Industrial Context and Project Vision,"The increasing global competition, the rapid introduction of new products, and quick changes in customer needs and demands (Cala et al. 2016) led the market to be more unpredictable than before and, consequently, new challenges in manufacturing sectors are required. On the client side, the demand for more customized, cheaper, and higher-quality products is driving the manufacturing companies to search for innovative development and production approaches to match those requirements. On the company side, disturbances such as delays or shortages from suppliers or resources breakdowns may have a deep impact on company performances. In addition, there is the constant need for reduction of production costs imposing processes to be more efficient and sustainable (Cala et al. 2016). Therefore, it is clear that the traditional production systems based on rigid predefined functionalities are no longer suited to cope with this competitive market and with its variability and unpredictability. The current manufacturing control systems based on centralized or hierarchical control structures do not support efficiently the requirements of flexibility and reconfigurability, since they present a weak response to change to production needs and to highly dynamic variations (Boschi et al. 2016). The solution to such issues will force companies to redesign their manufacturing systems that, using smarter manufacturing equipment, not only produce higher-quality products at lower costs, but are also able to quickly and effectively respond to rapid changes in their environment, even enabling production to continue despite the failure of single components (Boschi et al. 2016). Thus, a migration from traditional production systems characterized by vertical applications, centralized approach, and rigidity to agile plug-and-produce systems that are dynamically adaptable to changing production conditions open to new features and functions, flexible to different processing tasks, modular to enable quick and economical changes, and able to support new business processes and go-to-market strategies is needed (Delsing et al. 2012), see also Chapter 6 of (ETSI world class standards 2009). To face these challenges, the manufacturing industry, eager as well to recover from the crisis started in 2008, is addressing innovative paradigms, namely industrial Internet of Things (IoT), Industry 4.0, and Industrial Cyber-Physical System (ICPS). They represent different areas of action and research aiming to move from traditional approaches toward intelligent manufacturing control and automation systems that are reconfigurable, adaptable to changing production environment, and flexible to support business needs. These paradigms are supported by the implementation of new enabling technologies and intelligent approaches such as Multi-Agent Systems (MAS), Service-oriented Architecture (SOA), Plug-and-Produce systems, and Edge/Cloud technologies that have been developed in a variety of research f"
pub.1095101293,Towards a MOLGENIS based computational framework,"High-throughput bioinformatics research is complex and requires the combination of multiple experimental approaches each producing large amounts of diverse data. The analysis and evaluation of these data are equally complex requiring specific integrations of various software components into complex workflows. The challenge is to provide less technically involved bioinformaticians with simple interfaces to specify the workflow of commands they need while at the same time scale up to hundreds of jobs to get the terabytes of genetic data processed by recent methods. Here, we present a computational framework for bioinformatics which enables data and workflow management in a distributed computational environment. Firstly, we propose a new data model to specify workflow execution logic on available network resources and components. Our model extends existing generic workflow and bioinformatics models to describe workflows compactly and unambiguously. Secondly, we present the implementation of our computational framework, which is constructed as a computational cloud for bioinformatics using open source off-the-shelf components. Finally, we demonstrate applications of the framework on complex real-world bioinformatics tasks."
pub.1154716588,An Energy-Aware Agent-Based Resource Allocation Using Targeted Load Balancer for Improving Quality of Service in Cloud Environment,"In order to manage the load on dispersed data centers and cut down on energy established on time usage, agent-based resource allocation is given attention. Using a targeted load balancer (TLB), we suggest an energy-aware agent-based resource allocation in this research to enhance quality of service in a cloud setting. This agent is first set up to keep track of the resource load resulting from the request that has been assigned a job. Cloud watch also keeps an eye on energy levels to determine the typical payload size of resource execution. The TLB establishes new instance state to assign the resource based on the payload weight. To shorten the execution time, the dynamic hyper switching model develops a balancing mechanism. The suggested system achieves high performance in resource management by creating load balancer that is efficiently targeted to cut down on computation time and cost depending on energy levels. In comparison to existing techniques, the suggested parallelized homogeneous job in the cloud environment produces greater performance up to 95.5% while maintaining the time execution utilizing switching state of execution. This maintains the reduced CPU consumption, which dependent on the lowering of temporal complexity."
pub.1039043578,HOPE,"The functional scope of today's software-defined data centers (SDDC) has expanded to such an extent that servers face a growing amount of critical background operational tasks like load monitoring, logging, migration, and duplication, etc. These ancillary operations, which we refer to as management operations, often nibble the stringent data center power envelope and exert a tremendous amount of pressure on front-end user tasks. However, existing power capping, peak shaving, and time shifting mechanisms mainly focus on managing data center power demand at the ""macro level"" -- they do not distinguish ancillary background services from user tasks, and therefore often incur significant performance degradation and energy overhead. In this study we explore ""micro-level"" power management in SDDC: tuning a specific set of critical loads for the sake of overall system efficiency and performance. Specifically, we look at management operations that can often lead to resource contention and energy overhead in an IaaS SDDC. We assess the feasibility of this new power management paradigm by characterizing the resource and power impact of various management operations. We propose HOPE, a new system optimization framework for eliminating the potential efficiency bottleneck caused by the management operations in the SDDC. HOPE is implemented on a customized OpenStack cloud environment with heavily instrumented power infrastructure. We thoroughly validate HOPE models and optimization efficacy under various user workload scenarios. Our deployment experiences show that the proposed technique allows SDDC to reduce energy consumption by 19%, reduce management operation execution time by 25.4%, and in the meantime improve workload performance by 30%."
pub.1061411887,Cloud computing meets mobile wireless communications in next generation cellular networks,"In next generation cellular networks, cloud computing will have profound impacts on mobile wireless communications. On the one hand, the integration of cloud computing into the mobile environment enables MCC systems. On the other hand, the powerful computing platforms in the cloud for radio access networks lead to a novel concept of C-RAN. In this article we study the topology configuration and rate allocation problem in C-RAN with the objective of optimizing the end-to-end performance of MCC users in next generation cellular networks. We use a decision theoretical approach to tackle the delayed channel state information problem in C-RAN. Simulation results show that the design and operation of future mobile wireless networks can be significantly affected by cloud computing, and the proposed scheme is capable of achieving substantial performance gains over existing schemes."
pub.1171681647,Enriched Resourceful Weighted Round Robin For Optimal Balancing of Loads in Cloud Environment,"Load balancing is considered as one of the most critical task of cloud computing as it requires an effective load balancing system to balance the network loads based on parameters including latency, throughput, execution time, response time, quality of services, fault tolerance, load balance, scalability, and resource utilization. In this paper, we develop a load-balancing model using an Enriched Resourceful Weighted Round Robin approach that aims at balancing the cloud based on several input parameters. The parameters including Response time, Processing cost and Resource Utilization helps in finding the intensity of loads in cloud environment. These inputs define the dynamic nature of load balancing in clouds and the round robin approach estimates the load and allocates the resources based on this. The simulation is conducted on CloudSim tool to test the efficacy of the model and the results of simulation show proposed model achieves higher rate of resource utilization with minimum overhead, reduced migration time and increased throughput that shows improved network performance than existing methods."
pub.1000222021,A healthcare information sharing scheme in distributed cloud networks,"Current information & communication technology infrastructures are struggling for a wide variety of devices, services, and business and technology evolution. Cloud computing is becoming a promising platform and has developed rapidly. In contrast to traditional enterprise IT solution, it enables enterprises to procure computing resources on demand basis and delegate management of all the resources to the cloud service provider. Healthcare information networks have emerged as one of the major research area in the cloud networks. Healthcare information is communication and information systems and technology that facilitate quality patient care, progressive medical education, and innovative research. Healthcare information networks must ensure the reliability and efficiency because it transfers highly personal data. Thus, it guarantees and keeps a certain level of transfer efficiency. In the distributed cloud environments, management costs for the network and computing resources are solved fundamentally through the integrated management system. It can increase the cost savings to solve the traffic explosion problem of core network via a distributed micro datacenters (DCs). However, traditional flooding methods may cause a lot of traffic because it had to send data to all the neighbor DCs. Restricted Path Flooding algorithms have been proposed for this purpose. In large networks, there is still the disadvantage that may occur traffic. In this paper, we develope lightweight path flooding algorithm to improve existing flooding algorithm using hop count restriction. This paper also investigates the problem of partial information sharing."
pub.1059480700,iMIG: Toward an Adaptive Live Migration Method for KVM Virtual Machines,"With the energy and power costs increasing alongside the growth of the IT infrastructures, achieving workload concentration and high availability in cloud computing environments is becoming more and more complex. Virtual machine (VM) migration has become an important approach to address this issue, particularly; live migration of the VMs across the physical servers facilitates dynamic workload scheduling of the cloud services as per the energy management requirements, and also reduces the downtime by allowing the migration of the running instances. However, migration is a complex process affected by several factors such as bandwidth availability, application workload and operating system configurations, which in turn increases the complications in predicting the migration time in order to negotiate the service-level agreements in a real datacenter. In this paper, we propose an adaptive approach named improved MIGration (iMIG), in which we characterize some of the key metrics of the live migration performance, and conduct several experiments to study the impacts of the investigated metrics on the Kernel-based VM (KVM) functionalities, as well as the energy consumed by both the destination and the source hosts. Our results reveal the importance of the configured parameters: speed limit, TCP buffer size and max downtime, along with the VM properties and also their corresponding impacts on the migration process. Improper setting of these parameters may either incur migration failures or causes excess energy consumption. We witness a few bugs in the existing Quick EMUlator (QEMU)/KVM parameter computation framework, which is one of most widely used KVM frameworks based on QEMU. Based on our observations, we develop an analytical model aimed at better predictions of both the migration time and the downtime, during the process of VM deployment. Finally, we implement a suite of profiling tools in the adaptive mechanism based on the qemu-kvm-0.12.5 version, and our experiment results prove the efficiency of our approach in improving the live migration performance. In comparison with the default migration approach, our approach achieves a 40% reduction in the migration latency and a 45% reduction in the energy consumption."
pub.1170531539,Modified tabu-based ant colony optimisation algorithm for energy-efficient cloud computing systems,"The widespread adoption of Cloud Computing (CC) and rapid rise in capacity and scale of data centres to host and provide services via internet results in a significant increase in electricity usage and increased carbon footprints. One of the challenging research problems is to motivate green CC by reducing datacentres energy consumption. The well-known Task Scheduling (TS) considered as NP-hard problem needs to be addressed to facilitate green CC which influences the overall efficiency of cloud system. This paper presented a modified novel Tabu-based Ant Colony Optimisation Algorithm (TACO) energy efficient TS approach for heterogeneous cloud environment. TACO maintains a Long-Term Memory (LTM) in terms of aspiration list and Short-Term Memory (STM) in terms of tabu list. TACO is evaluated in CloudSim 3.0.3 to measure its performance with existing energy-based metaheuristics Particle Swarm Optimisation (PSO) and Genetic Algorithm (GA) which shows its outperformance in energy consumption, makespan and resource utilisation."
pub.1120153773,ESBL: Design and Implement A Cloud Integrated Framework for IoT Load Balancing,"The continuous growth in wireless communication, the demand for sophisticated, simple and low-cost solutions are also increasing. The demand motivated the researchers to indulge into inventing suitable network solutions ranging from wireless sensor networks to wireless ad-hoc networks to Internet of Things (IoT). With the inventions coming from the researchers, the demand for further improvements into the existing researchers have also growth upbound. Initially the network protocols were the demand for research and further improvements. Nevertheless, the IoT devices are started getting used in various fields and started gathering a huge volume of data using complex application. This invites the demands for research on load balancing for IoT networks. Several research attempts were made to overcome the communication overheads caused by the heavy loads on the IoT networks. Theses research attempts proposed to manage the loads in the network by equally distributing the loads among the IoT nodes. Nonetheless, in the due course of time, the practitioners have decided to move the data collected by the IoT nodes and the applications processing those data in to the cloud. Hence, the challenge is to build an algorithm for cloud-based load balancer matching with the demands from the IoT network protocols. Hence, this work proposes a novel algorithm for managing the loads on cloud integrated IoT network frameworks. The proposed algorithm utilizes the analytics of loads on cloud computing environments driven by the physical host machines and the virtual environments. The major challenge addressed by this work is to design a load balancer considering the low availability of the energy and computational capabilities of IoT nodes but with the objective to improve the response time of the IoT network. The proposed algorithm for load balancer is designed considering the low effort integrations with existing IoT framework for making the wireless communication world a better place."
pub.1132621708,An adaptive fault detector strategy for scientific workflow scheduling based on improved differential evolution algorithm in cloud,"With the increasing popularity and acceptance of cloud computing, it is being applied in services like executing large-scale applications, where cloud environment is selected by the scientific associations to easily execute the computation intensive workflows. However, cloud computing can have higher failure rates due to the larger number of servers and components filled with the intensive workloads. These failures may lead to the unavailability of virtual machines (VMs) for computation. Hence, this issue of fault occurrences can be tolerated by adopting an effective and efficient fault tolerant strategy. The goal of our research in this paper is to develop an adaptive fault detector strategy based on Improved Differential Evolution (IDE) algorithm in cloud computing that can minimize the energy consumption, the makespan, the total cost and, at the same time, tolerate up faults when scheduling scientific workflows. This proposed work applies an adaptive network-based fuzzy inference system (ANFIS) prediction model to proactively control resource load fluctuation that increases the failure prediction accuracy before fault/failure occurrence. In addition, it applies a reactive fault tolerance technique for when a processor fails and the scheduler must allocate a new VM to execute the workflow tasks. The experimental results show that compared with existing techniques, the proposed approach significantly improves the overall scheduling performance, achieves a higher degree of fault tolerance with high HyperVolume (HV) compared with the ICFWS, IDE, and ACO algorithms, minimizes the makespan, the energy consumption and task fault ratio, and reduces the total cost."
pub.1110762966,Summary and Future Work,"The adoption of cloud computing by the U.S. government, including the Department of Defense, is proceeding quickly [1, 4, 8, 9] and is likely to become widespread [5]. As government becomes more comfortable with the technology, mission‐oriented cloud computing seems inevitable. However, security remains a top threat to the use of clouds for dependable and trustworthy computing [6], even as FedRAMP [7] and other standards converge to a common set of requirements, as discussed in this chapter. The cloud computing environment is maturing, but we are observing the rise of new aspects of cloud computing – like mobiles interconnected into clouds, real‐time concerns, edge computing, and machine learning – that are challenging the existing techniques for testing, validation, verification, robustness, and resistance to attack. As reflected in this book, academia and industry are attempting to respond quickly to rapidly changing cloud technologies, as driven by the value of these technologies in today's society.The preceding chapters of this book have touched on many of the concerns arising from cloud technology: survivability, risks, benefits, detection, security, scalability, workloads, performance, resource management, validation and verification, theoretical problems, and certification. In this last chapter, we will consider what has been learned since 2007 and what issues and obstacles remain in any mission‐critical system deployment on cloud computing."
pub.1119180173,Leveraging OpenStack and Ceph for a Controlled-Access Data Cloud,"While traditional HPC has and continues to satisfy most workflows, a new
generation of researchers has emerged looking for sophisticated, scalable,
on-demand, and self-service control of compute infrastructure in a cloud-like
environment. Many also seek safe harbors to operate on or store sensitive
and/or controlled-access data in a high capacity environment.
  To cater to these modern users, the Minnesota Supercomputing Institute
designed and deployed Stratus, a locally-hosted cloud environment powered by
the OpenStack platform, and backed by Ceph storage. The subscription-based
service complements existing HPC systems by satisfying the following unmet
needs of our users: a) on-demand availability of compute resources, b)
long-running jobs (i.e., $> 30$ days), c) container-based computing with
Docker, and d) adequate security controls to comply with controlled-access data
requirements.
  This document provides an in-depth look at the design of Stratus with respect
to security and compliance with the NIH's controlled-access data policy.
Emphasis is placed on lessons learned while integrating OpenStack and Ceph
features into a so-called ""walled garden"", and how those technologies
influenced the security design. Many features of Stratus, including tiered
secure storage with the introduction of a controlled-access data ""cache"",
fault-tolerant live-migrations, and fully integrated two-factor authentication,
depend on recent OpenStack and Ceph features."
pub.1105578172,Leveraging OpenStack and Ceph for a Controlled-Access Data Cloud,"While traditional HPC has and continues to satisfy most workflows, a new generation of researchers has emerged looking for sophisticated, scalable, on-demand, and self-service control of compute infrastructure in a cloud-like environment. Many also seek safe harbors to operate on or store sensitive and/or controlled-access data in a high capacity environment. To cater to these modern users, the Minnesota Supercomputing Institute designed and deployed Stratus, a locally-hosted cloud environment powered by the OpenStack platform, and backed by Ceph storage. The subscription-based service complements existing HPC systems by satisfying the following unmet needs of our users: a) on-demand availability of compute resources; b) long-running jobs (i.e., > 30 days); c) container-based computing with Docker; and d) adequate security controls to comply with controlled-access data requirements. This document provides an in-depth look at the design of Stratus with respect to security and compliance with the NIH's controlled-access data policy. Emphasis is placed on lessons learned while integrating OpenStack and Ceph features into a so-called ""walled garden"", and how those technologies influenced the security design. Many features of Stratus, including tiered secure storage with the introduction of a controlled-access data ""cache"", fault-tolerant live-migrations, and fully integrated two-factor authentication, depend on recent OpenStack and Ceph features."
pub.1114505911,Adapting the Machine Learning Grid Prediction Models for Forecasting of Resources on the Clouds,"As the grid is loosely coupled congregation of geographically distributed and heterogeneous resources, the efficient scheduling of their resources require the support of a sound Performance Prediction System (PPS). The performance prediction of resources is helpful for both Resource Management Systems and users to make optimized resource usage decisions. In this paper, it is discussed about the ways of several Machine Learning (ML) PPSs that span over several grid resources in several dimensions. Taxonomy is used to categorize and identify approaches which are followed in the implementation of the existing PPSs for grids and for their adaptation in the clouds. A test framework is proposed to evaluate their capability of serving in the clouds environment."
pub.1132093701,Automated migration of EuGENia graphical editors to the web,"Domain-specific languages (DSLs) are languages tailored for particular domains. Many frameworks and tools have been proposed to develop editors for DSLs, especially for desktop IDEs, like Eclipse. We are witnessing the advent of low-code development platforms, which are cloud-based environments supporting rapid application development by using graphical languages and forms. While this approach is very promising, the creation of new low-code platforms may require the migration of existing desktop-based editors to the web. However, this is a technically challenging task. To fill this gap, we present ROCCO, a tool that migrates Eclipse-based graphical modelling editors to the web, to facilitate their integration with low-code platforms. The tool reads a meta-model annotated with EuGENia annotations, and generates a web editor using the DPG web framework used by the UGROUND company. In this paper, we present the approach, including tool support and an evaluation based on migrating nine editors created by third parties, which shows the usefulness of the tool."
pub.1090294862,Developing a wind turbine planning platform: Integration of “sound propagation model–GIS-game engine” triplet," In this study, we propose an interactive information system for wind turbine siting, considering its visual and sound externalities. This system is an integration of game engine, GIS and analytical sound propagation model in a unified 3D web environment. The game engine–GIS integration provides a 3D virtual environment where users can navigate through the existing geospatial data of the whole country and place different wind turbine types to explore their visual impact on the landscape. The integration of a sound propagation model in the game engine–GIS supports the real-time calculation and feedback regarding wind turbine sound at the surrounding buildings. The platform's GIS component enables massive (on-the-fly) georeferenced data utilization through tiling techniques as well as data accessibility and interoperability via cloud-based architecture and open geospatial standard protocols. The game engine, on the other hand, supports performance optimization for both data display and sound model calculations."
pub.1061541813,Aggressive Resource Provisioning for Ensuring QoS in Virtualized Environments,"Elasticity has now become the elemental feature of cloud computing as it enables the ability to dynamically add or remove virtual machine instances when workload changes. However, effective virtualized resource management is still one of the most challenging tasks. When the workload of a service increases rapidly, existing approaches cannot respond to the growing performance requirement efficiently because of either inaccuracy of adaptation decisions or the slow process of adjustments, both of which may result in insufficient resource provisioning. As a consequence, the Quality of Service (QoS) of the hosted applications may degrade and the Service Level Objective (SLO) will be thus violated. In this paper, we introduce SPRNT, a novel resource management framework, to ensure high-level QoS in the cloud computing system. SPRNT utilizes an aggressive resource provisioning strategy which encourages SPRNT to substantially increase the resource allocation in each adaptation cycle when workload increases. This strategy first provisions resources which are possibly more than actual demands, and then reduces the over-provisioned resources if needed. By applying the aggressive strategy, SPRNT can satisfy the increasing performance requirement in the first place so that the QoS can be kept at a high level. The experimental results show that SPRNT achieves up to 7.7× speedup in adaptation time, compared with existing efforts. By enabling quick adaptation, SPRNT limits the SLO violation rate up to 1.3 percent even when dealing with rapidly increasing workload."
pub.1149691358,Deployment of a LoRa-based Network and Web Monitoring Application for a Smart Farm,"The integration of Industry 4.0 technologies in agriculture will reduce the increasing challenges of agricultural process around the globe. The real-time farm management with high degree of automation will greatly improve productivity, agri-food supply chain efficiency and food safety. This paper describes a fully customized LoRa-based IoT system that aims for a low-cost, low power and wide range wireless sensor network targeted for smart farms. The presented system integrates already existing Programmable Logic Controllers (PLC) typically used to control multiple processes and devices such as water pumps, certain machinery, etc. along with a newly developed network of wireless LoRA sensors distributed over the farm. A Telegram bot is also included as novelty for automated user communication via this mobile phone messaging application. The network structure was deployed and tested. The developed integrated system also includes a cloud-based monitoring application to provide remote visualization and control for all the farming environment."
pub.1129510239,"A Seven-layered Model Architecture, Network Model, Protocol Stack, Security, Application, Issues and Challenges in Internet of Vehicle"," Background: Recently, the Internet of Things (IoT) has brought various changes in the existing research field by including new areas such as smart transportation, smart home facilities, smart healthcare, etc. In smart transportation systems, vehicles contain different components to access information related to passengers, drivers, vehicle speed, and many more. This information can be accessed by connecting vehicles with the Internet of Things leading to new fields of research known as Internet of Vehicles. The setup of the Internet of Vehicle (IoV) consists of many sensors to establish a connection with several other sensors belonging to different environments by exploiting different technologies. The communication of the sensors faces a lot of challenging issues. Some of the critical challenges are to maintain security in information exchanges among the vehicles, inequality in sensors, quality of internet connection, and storage capacity.   Objective: To overcome the challenging issues, we have designed a new framework consisting of seven-layered architecture, including the security layer, which provides seamless integration by communicating with the devices present in the IoV environment. Further, a network model consisting of four components such as Cloud, Fog, Connection, and Clients has been designed. Finally, the protocol stack which describes the protocol used in each layer of the proposed seven-layered IoV architecture has been shown.   Methods: In this proposed architecture, the representation and the functionalities of each layer and types of security have been defined. Case studies of this seven-layer IoV architecture have also been performed to illustrate the operation of each layer in real-time. The details of the network model including all the elements inside each component have also been shown.   Results: We have discussed some of the existing communication architectures and listed a few challenges and issues occurring in present scenarios. Considering these issues, we have developed the seven-layered IoV architecture and the network model with four essential components known as the cloud, fog, connection, and clients.   Conclusion: This proposed architecture provides a secure IoV environment and provides life safety. Hence, safety and security will help to reduce the cybercrimes occurring in the network and thereby provide good coordination and communication of the vehicles in the network. "
pub.1118790282,TrustShadow: Secure Execution of Unmodified Applications with ARM TrustZone,"The rapid evolution of Internet-of-Things (IoT) technologies has led to an
emerging need to make it smarter. A variety of applications now run
simultaneously on an ARM-based processor. For example, devices on the edge of
the Internet are provided with higher horsepower to be entrusted with storing,
processing and analyzing data collected from IoT devices. This significantly
improves efficiency and reduces the amount of data that needs to be transported
to the cloud for data processing, analysis and storage. However, commodity OSes
are prone to compromise. Once they are exploited, attackers can access the data
on these devices. Since the data stored and processed on the devices can be
sensitive, left untackled, this is particularly disconcerting.
  In this paper, we propose a new system, TrustShadow that shields legacy
applications from untrusted OSes. TrustShadow takes advantage of ARM TrustZone
technology and partitions resources into the secure and normal worlds. In the
secure world, TrustShadow constructs a trusted execution environment for
security-critical applications. This trusted environment is maintained by a
lightweight runtime system that coordinates the communication between
applications and the ordinary OS running in the normal world. The runtime
system does not provide system services itself. Rather, it forwards requests
for system services to the ordinary OS, and verifies the correctness of the
responses. To demonstrate the efficiency of this design, we prototyped
TrustShadow on a real chip board with ARM TrustZone support, and evaluated its
performance using both microbenchmarks and real-world applications. We showed
TrustShadow introduces only negligible overhead to real-world applications."
pub.1125680921,Novel Sustainable and Heterogeneous Offloading Management Techniques in Proactive Cloudlets,"Cloudlet-based mobile cloud offloading is an emerging technology designed to augment mobile elements by migrating resource-hungry components to adjacent local resource pooling. However, the Cloudlet resources are usually limited in terms of the computing utility, storage and network bandwidth. In this scenario, the remote Cloud infrastructure can provide additional computing and storage utility during run-time; the heterogeneous offloading methods for different mobile applications and diverse offloading resources complicate the Cloudlet-based offloading and resource allocation process. As a result, a considerable amount of delay is caused by setting up the execution environment, communication overhead and waiting in the queue, which significantly downgrade the QoS and the usability of such systems. In this paper, we propose a novel hybrid offloading model to solve the heterogeneous resource-constraint offloading issues in the Cloudlet, concerning the offloading energy and execution efficiency. A queue-based offloading framework is developed to formulate and analyze the mixed migration-based and partition-based offloading behaviors in the Cloudlet. The execution and energy-aware heterogeneous offloading resource allocation problem is formalized, and a Particle Swarm Optimization heuristic solution is presented. A SARIMA-based load prediction model is designed in the Cloudlet to achieve fine-grain proactive resource allocation. Experimental results reveal that the proposed framework can effectively reduce the offloading energy cost and execution time, compared to currently existing solutions."
pub.1152548686,Industry 4.0: a systematic review of legacy manufacturing system digital retrofitting,"Industry 4.0 technologies and digitalised processes are essential for implementing smart manufacturing within vertically and horizontally integrated production environments. These technologies offer new ways to generate revenue from data-driven services and enable predictive maintenance based on real-time data analytics. They also provide autonomous manufacturing scheduling and resource allocation facilitated by cloud computing technologies and the industrial Internet of Things (IoT). Although the fourth industrial revolution has been underway for more than a decade, the manufacturing sector is still grappling with the process of upgrading manufacturing systems and processes to Industry 4.0-conforming technologies and standards. Small and medium enterprises (SMEs) in particular, cannot always afford to replace their legacy systems with state-of-the-art machines but must look for financially viable alternatives. One such alternative is retrofitting, whereby old manufacturing systems are upgraded with sensors and IoT components to integrate them into a digital workflows across an enterprise. Unfortunately, to date, the scope and systematic process of legacy system retrofitting, and integration are not well understood and currently represent a large gap in the literature. In this article, the authors present an in-depth systematic review of case studies and available literature on legacy system retrofitting. A total of 32 papers met the selection criteria and were particularly relevant to the topic. Three digital retrofitting approaches are identified and compared. The results include insights common technologies used in retrofitting, hardware and software components typically required, and suitable communication protocols for establishing interoperability across the enterprise. These form an initial basis for a theoretical decision-making framework and associated retrofitting guide tool to be developed."
pub.1090377689,TrustShadow: Secure Execution of Unmodified Applications with ARM TrustZone,"The rapid evolution of Internet-of-Things (IoT) technologies has led to an emerging need to make them smarter. A variety of applications now run simultaneously on an ARM-based processor. For example, devices on the edge of the Internet are provided with higher horsepower to be entrusted with storing, processing and analyzing data collected from IoT devices. This significantly improves efficiency and reduces the amount of data that needs to be transported to the cloud for data processing, analysis and storage. However, commodity OSes are prone to compromise. Once they are exploited, attackers can access the data on these devices. Since the data stored and processed on the devices can be sensitive, left untackled, this is particularly disconcerting. In this paper, we propose a new system, TrustShadow that shields legacy applications from untrusted OSes. TrustShadow takes advantage of ARM TrustZone technology and partitions resources into the secure and normal worlds. In the secure world, TrustShadow constructs a trusted execution environment for security-critical applications. This trusted environment is maintained by a lightweight runtime system that coordinates the communication between applications and the ordinary OS running in the normal world. The runtime system does not provide system services itself. Rather, it forwards requests for system services to the ordinary OS, and verifies the correctness of the responses. To demonstrate the efficiency of this design, we prototyped TrustShadow on a real chip board with ARM TrustZone support, and evaluated its performance using both microbenchmarks and real-world applications. We showed TrustShadow introduces only negligible overhead to real-world applications."
pub.1106072491,Cloud Strife,"Infrastructure-as-a-Service (IaaS), more generally the ""cloud,"" changed the landscape of system operations on the Internet. Clouds' elasticity allow operators to rapidly allocate and use resources as needed, from virtual machines, to storage, to IP addresses, which is what made clouds popular. We show that the dynamic component paired with developments in trust-based ecosystems (e.g., TLS certificates) creates so far unknown attacks. We demonstrate that it is practical to allocate IP addresses to which stale DNS records point. Considering the ubiquity of domain validation in trust ecosystems, like TLS, an attacker can then obtain a valid and trusted certificate. The attacker can then impersonate the service, exploit residual trust for phishing, or might even distribute malicious code. Even worse, an aggressive attacker could succeed in less than 70 seconds, well below common time-to-live (TTL) for DNS. In turn, she could exploit normal service migrations to obtain a valid certificate, and, worse, she might not be bound by DNS records being (temporarily) stale. We introduce a new authentication method for trust-based domain validation, like IETF's automated certificate management environment (ACME), that mitigates staleness issues without incurring additional certificate requester effort by incorporating the existing trust of a name into the validation process. Based on previously published work [1]. [1] Kevin Borgolte, Tobias Fiebig, Shuang Hao, Christopher Kruegel, Giovanni Vigna. February 2018. Cloud Strife: Mitigating the Security Risks of Domain-Validated Certificates. In Proceedings of the 25th Network and Distributed Systems Security Symposium (NDSS '18). Internet Society (ISOC). DOI: 10.14722/ndss.2018.23327. URL: https://doi.org/10.14722/nd"
pub.1147053338,Hybrid Genetic Algorithm and Modified-Particle Swarm Optimization Algorithm (GA-MPSO) for Predicting Scheduling Virtual Machines in Educational Cloud Platforms,"Cloud computing is expanding gradually as the number of educational applications is rapidly increasing. To get Educational cloud services, internet connectivity is predominantly important and Cloud Environment uses one of the basic technology to manage the Physical servers effectively ie; Virtualization Technology. In Cloud Computing, the data centers host numerous Virtual Machines (VMs) on top of the Servers. Due to the rapid growth of Educational platforms, the workload of the VM is computationally getting increased. In the Cloud Educational platforms, to execute the jobs IT resources are provisioned over the network. Since the data generated from the client-side is dynamic in nature, it is difficult to allocate the computational resources efficiently. So to enhance the energy efficiency and to provide the resources in an optimized way, a VM Scheduling mechanism with Hybrid Genetic Algorithm-Modified Particle Swarm Optimization (GA-MPSO) is proposed in this work to achieve QoS parameters like reduced Energy consumption, SLA violation, and cost reduction over the heterogeneous environments. The Hybrid G-MPSO develops the optimal range and improves the best range of scheduling the Virtual resources to VMs from Physical Machines (PMs). The proposed approach, when compared to other VM scheduling algorithms, it intensifies the energy consumption to 105KWH, SLA violation rate of 0.08%, reduces the migrations count to 2122, and consumes the overall cost of 2567.68$. The different scheduling methods for VMs are evaluated against the results, which show that the Hybrid GA-MPSO method is far better than the existing algorithms."
pub.1112783088,"Foundations and Trends in Smart Learning, Proceedings of 2019 International Conference on Smart Learning Environments","This book focuses on the interplay between pedagogy and technology, and their fusion for the advancement of smart learning environments. It discusses various components of this interplay, including learning and assessment paradigms, social factors and policies, emerging technologies, innovative application of mature technologies, transformation of curriculum and teaching behavior, transformation of administration, best infusion practices, and piloting of new ideas. The book provides an archival forum for researchers, academics, practitioners and industry professionals interested and/or engaged in reforming teaching and learning methods by promoting smart learning environments. It also facilitates discussions and constructive dialogue among various stakeholders on the limitations of existing learning environments, the need for reform, innovative uses of emerging pedagogical approaches and technologies, and sharing and promoting best practices, leading to the evolution, design and implementation of smart learning environments."
pub.1112924552,"iSTEP, an integrated Self-Tuning Engine for Predictive maintenance in Industry 4.0","The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper."
pub.1168247835,Fog Computing in Vehicular Ad Hoc Network Applications: A Survey of Challenges and Scope," Abstract: The use of computationally intensive applications requiring substantial storage resources in vehicular ad hoc networks is on the rise due to the technology shift from smart, intelligent vehicles to autonomous vehicles. Cloud computing addresses this problem to some extent by providing computational and storage facilities for the significant amount of data generated by vehicular nodes. However, it still cannot meet the stringent requirements of highly dynamic vehicular nodes in a real-time environment. The delay-sensitive applications of vehicular ad hoc networks instill the need for computing facilities in the closest possible proximity to the vehicular nodes. Therefore, fog computing has found potential in vehicular ad hoc network applications. Fog computing brings storage and computing resources closer to vehicles. Fog-based vehicular ad hoc networks can help improve the issues of first-generation vehicular ad hoc networks, such as latency, location perception, and concurrent response. Based on existing research, using fog computing decreased the delay by 50-60% and 60-70% in vehicular ad hoc network safety and entertainment and commercial applications, respectively, compared to cloud computing. An extensive study of the amalgamation of fog computing and vehicular ad hoc networks should be conducted to understand the underlying challenges of fog-based vehicular ad hoc networks. Existing surveys only include specific applications of this technology. However, we present an extensive survey focusing on the architecture of these two technologies, their integration, use cases of fog vehicular ad hoc networks, simulation tools, important performance metrics, the challenges in multiple contexts, and the gaps that need to be addressed, as well as suggest prominent journals in this field. This paper will not only impart knowledge to researchers in this area but also provide them with insights to identify the challenges and pursue their work further in fog-based vehicular ad hoc networks. "
pub.1061755115,Cloudde: A Heterogeneous Differential Evolution Algorithm and Its Distributed Cloud Version,"Existing differential evolution (DE) algorithms often face two challenges. The first is that the optimization performance is significantly affected by the ad hoc configurations of operators and parameters for different problems. The second is the long runtime for real-world problems whose fitness evaluations are often expensive. Aiming at solving these two problems, this paper develops a novel double-layered heterogeneous DE algorithm and realizes it in cloud computing distributed environment. In the first layer, different populations with various parameters and/or operators run concurrently and adaptively migrate to deliver robust solutions by making the best use of performance differences among multiple populations. In the second layer, a set of cloud virtual machines run in parallel to evaluate fitness of corresponding populations, reducing computational costs as offered by cloud. Experimental results on a set of benchmark problems with different search requirements and a case study with expensive design evaluations have shown that the proposed algorithm offers generally improved performance and reduced computational time, compared with not only conventional and a number of state-of-the-art DE variants, but also a number of other distributed DE and high-performing evolutionary algorithms. The speedup is significant especially on expensive problems, offering high potential in a broad range of real-world applications."
pub.1145274043,Environmental and Safety Impacts of Vehicle-to-Everything Enabled Applications: A Review of State-of-the-Art Studies,"With the rapid development of communication technology, connected vehicles
(CV) have the potential, through the sharing of data, to enhance vehicle safety
and reduce vehicle energy consumption and emissions. Numerous research efforts
are quantifying the impacts of CV applications, assuming instant and accurate
communication among vehicles, devices, pedestrians, infrastructure, the
network, the cloud, and the grid, collectively known as V2X
(vehicle-to-everything). The use of cellular vehicle-to-everything (C-V2X), to
share data is emerging as an efficient means to achieve this objective. C-V2X
releases 14 and 15 utilize the 4G LTE technology and release 16 utilizes the
new 5G new radio (NR) technology. C-V2X can function without network
infrastructure coverage and has a better communication range, improved latency,
and greater data rates compared to older technologies. Such highly efficient
interchange of information among all participating parts in a CV environment
will not only provide timely data to enhance the capacity of the transportation
system but can also be used to develop applications that enhance vehicle safety
and minimize negative environmental impacts. However, before the full benefits
of CV can be achieved, there is a need to thoroughly investigate the
effectiveness, strengths, and weaknesses of different CV applications, the
communication protocols, the varied results with different CV market
penetration rates (MPRs), the interaction of CVs and human driven vehicles, the
integration of multiple applications, and the errors and latencies associated
with data communication. This paper reviews existing literature on the
environmental, mobility and safety impacts of CV applications, identifies the
gaps in our current research of CVs and recommends future research directions."
pub.1105408232,A survey study on virtual machine migration and server consolidation techniques in DVFS-enabled cloud datacenter: Taxonomy and challenges,"Cloud Computing is a promising paradigm in comparison with traditional information technology approaches, in which organizations and enterprises adopt elastic cloud services on a pay-per-use model in order to reduce costs. Virtualization is a technique pervasively applied in modern datacenters (DCs) to maximize resource utilization, reduce greenhouse gas emissions, and lower overall cost. Virtual machine (VM) migration is widely exploited within and across DCs to meet a variety of needs of the virtualized cloud environment. For instance, server consolidation needs VM migration for power management. Also, load balancing, fault tolerance, system maintenance, and minimizing the service-level agreement (SLA) violation rate require live VM migration. The VM migration process is very resource intensive and requires intelligent approaches to avoid saturating network bandwidth and to keep server downtime to a minimum. In addition, in Dynamic Voltage Frequency Scaling (DVFS) in cluster servers in intrinsically heterogeneous DCs the voltage-frequency is varied based on workload to reduce power consumption, which is a large part of the overall cost. Although miscellaneous cloud computing studies have been presented in the literature concerning economic, privacy, and security issues, etc., there is a clear lack of survey studies on VM migration, server consolidation, and DVFS techniques. To fill this gap, we present different schemes to classify commonalities and discrepancies between the perspectives of researchers, based on metrics derived from the literature. Finally, open issues, challenges, and future directions are discussed for improving existing schemes and approaches."
pub.1182029707,Cloud based IoT platforms for Home Automation System,"This research project focuses on developing and implementing a Cloud-based IoT platform for smart home automation, focusing on enhancing user experience and remote control functionality. The study uses hardware components like the Pir Sensor, MQ-2 Sensor, and DHT11 Sensor to construct easy to use and intelligent home environment. The research explores the existing knowledge on IoT in smart home automation, cloud-based IoT platforms, and user-centric aspects of such systems. The hardware components are thoroughly described and integrated into the smart home system, emphasizing connectivity and communication protocols. The research investigates the design of a user-friendly interface, advanced remote control functionalities, and the interlinking of AI-ML to automate smart home operations. Measures are taken to ensure data privacy and security within the cloud-based IoT environment, including data encryption, access control, and authorization mechanisms. The results and findings section showcases improvements in user experience and remote control functionalities, while the discussion section identifies potential limitations and challenges. The research project contributes to the evolution of smart home automation and underscores significant enhancements in user experience, remote control functionality, and data privacy within cloud-based IoT"
pub.1138522163,An IoT-oriented Multiple Data Replicas Placement Strategy in Hybrid Fog-Cloud Environment,"The growing adoption of Fog computing for the sensitive-time IoT applications allows to facilitate the real-time actions and to enhance their efficiency and performance. In fact, keeping the data in the distributed Fog network brings the advantages and power of the Cloud closer to where data are generated while saving network bandwidth and reducing latency and operational costs. However, due to the diversity of the Fog nodes, IoT system distribution and data sharing, how and where to place the produced data with low latency is a main challenge. Moreover, a data placement based on a single replica cannot meet the data access requirements of all data consumers that have different topology positions. Thus, in this paper, we propose a multi-objective optimization data placement model in a hybrid Fog-Cloud environment based on multiple data replicas. It aims to find better distributed data storage while optimizing the overall system latency and the used storage space by minimizing the data replicas and following full and partial data replication methods. Further, we propose a greedy algorithm $iFogDP_h$ which uses a refined method to find a solution for assigning the IoT data to the appropriate data hosts in polynomial time by reducing the time required to transfer data for storage, access and replication. We conducted the experiments on iFogSim, a toolkit for modeling and simulation of Fog environments. The experimental results show the effectiveness of our proposed solution in terms of latency, storage overhead and the number of data replicas compared to the existing strategies."
pub.1146080066,A wholistic optimization of containerized workflow scheduling and deployment in the cloud–edge environment,"Digital Twin in Industry 4.0 utilizes Internet of Things (IoT) to collect real-life data and combine it with simulation models for product design and development. The simulation process can be executed as a workflow, consisting of tasks with precedence constraints. In a container-based workflow execution system, each task in the workflow is executed in a container within a virtual machine (VM). In this paper, a three-step scheduling model is proposed to combine scheduling of container-based workflows and the deployment of containers on a cloud–edge environment. In the first step, virtual CPU (vCPU) is allocated for each container to enable vCPU sharing among different containers. Next, two-step resource deployment is used to schedule the containers onto VM, and VM onto the physical machines in either edge or cloud environment. Multiple objectives are considered, including minimizing makespan, load imbalance, and energy consumption, from the perspective of cloud–edge resources as well as containerized workflows. To obtain a set of non-dominated solutions, three evolution strategies are designed and combined with two multi-objective algorithm frameworks — co-evolution strategy (CES), basic non-co-evolution strategy (B-NCS), and hybrid non-co-evolution strategy (H-NCS). Simulation results demonstrate that the proposed model outperforms the existing two-step scheduling model and H-NCS performs better than other strategies."
pub.1171945814,Optimizing Cloud Computing Resources: An Energy Efficient Multi-QoS Factor-Based VM Placement Strategy,"Cloud computing is experiencing unprecedented demand, offering scalable and flexible resources for a wide range of applications. However, this surge in demand has raised concerns about energy consumption and the need for environmentally sustainable solutions. Green computing has emerged as a critical consideration in this context. Virtual Machine Placement (VMP) is a key component of optimizing cloud resources, aiming to allocate virtual machines efficiently while minimizing energy consumption, cost, and load balancing. This paper addresses the VMP problem by introducing a novel approach based on multifactor optimization, specifically the Diversity Constraint Digger Snake Optimizer (DCD-SO). It offers an innovative perspective on optimizing virtual machine placement by considering energy efficiency, load balancing, and resource utilization simultaneously with the aim to reduce VM migration count, time and cost. Proposed method provides a more comprehensive and sustainable solution, aligning with the principles of green computing. Through extensive simulations and experiments, we have rigorously evaluated the performance of DCD-SO in comparison to traditional optimization techniques such as Particle Swarm Optimization (PSO) and Snake Optimization. In our analysis of actual cloud environments, we compared the results of our method with existing state-of-the-art techniques. Result outcomes determine showed that proposed approach has reduced migration count of 5 and 3 for scheduling 42VMs and 84VMs on 16 and 32 host units respectively than traditional MOGANS, GA-S, GA-N and GA-NN methods. This comprehensive evaluation reinforces the effectiveness and practicality of our approach in addressing the intricate challenges of Virtual Machine Placement (VMP) in dynamic cloud computing settings. As cloud computing continues to evolve, our study contributes to more sustainable and efficient resource management, addressing both current demands and future needs."
pub.1143903162,A FHIR based architecture of a multiprotocol IoT Home Gateway supporting dynamic plug of new devices within instrumented environments,"Internet of Things (IoT) technologies have become a milestone advancement in the digital healthcare domain, since the number of IoT medical devices is grown exponentially, and it is now anticipated that by 2020 there will be over 161 million of them connected worldwide. Therefore, in an era of continuous growth, IoT healthcare faces various challenges, such as the collection over multiple protocols (e.g, Bluetooth, MQTT, CoAP, ZigBEE etc.) the interpretation, as well as the harmonization of the data format that derive from the existing huge amounts of heterogeneous IoT medical devices. In this respect this study aims at proposing an advanced Home Gateway architecture that offers a unique data collection module, supporting direct data acquisition over multiple protocols (i.e. BLE, MQTT) and indirect data retrieval from cloud health services (i.e, GoogleFit). Moreover the solution propose a mechanism to automatically convert the original data format, carried over BLE, in HL7 FHIR by exploiting device capabilities semantic annotation implemented by means of FHIR resource as well. The adoption of such annotation enables the dynamic plug of new sensors within the instrumented environment without the need to stop and adapt the gateway. This simplifies the dynamic devices landscape customization requested by the several telemedicine applications contexts (e.g. CVD, Diabetes) and demonstrate, for the first time, a concrete example of using the FHIR standard not only (as usual) for health resources representation and storage but also as instrument to enable seamless integration of IoT devices. The proposed solution also relies on mobile phone technology which is widely adopted aiming at reducing any obstacle for a larger adoption."
pub.1131079799,Robust 3-Dimension Point Cloud Mapping in Dynamic Environment Using Point-Wise Static Probability-Based NDT Scan-Matching,"Graph-based simultaneous localization and mapping (SLAM) is one of the methods to generate point cloud maps which are used for various applications in autonomous vehicles. Graph-based SLAM represents the pose of the vehicle as a node and the odometry between two different nodes as an edge. Among the edge generating methods, scan matching, light detection and ranging (LiDAR) based method, can provide an accurate pose between two nodes based on the high distance accuracy of the LiDAR. However, the point cloud in real driving situations contains numerous moving objects, which degrade the scan-matching performance. Therefore, this article defines the static probability which means the likelihood that an acquired point is from a static object, and proposes the weighted normal distribution transformation (NDT), which is achieved by modifying NDT. Weighted NDT is a scan-matching algorithm which can reflect the static probability of each point as a weight. The odometry from the weighted NDT is utilized for graph construction to generate a robust point cloud map even in a dynamic environment. Finally, the proposed algorithm was compared with the existing object removal algorithms in two areas: dynamic object classification and scan-matching performance. Based on the scan-matching results, the accuracy of the point cloud map generated by the proposed algorithm was evaluated with a reference map using high-performance global navigation satellite system (GNSS). It was confirmed that the proposed algorithm has higher classification accuracy and lower scan-matching error compared with other dynamic object removal methods. The proposed algorithm was able to generate a point cloud map, despite the presence of many dynamic objects, that was similar to a map generated in the absence of dynamic objects in the same environment."
pub.1138369741,A modelling & Simulation via CloudSim for Live Migration in Virtual Machines,"Abstract
                  In growing era of cloud computing, one major utility is better and improved performance. For which Cloud Computing is a model which provides various resources available in a pool from where the users can get available the same when required. In the overall process there can be an important issue that is Fault tolerance to provide the high availability for the process which is being required by the user. And simultaneously if any fault occurs then user might not found any server downtime and other kind of delay in getting the response. There are many compute nodes such as Virtual machines further controlled by the Hypervisor. For transferring the load among compute nodes can be done by Cold Migration earlier there was a technique Non-Live migration. But at the same time there were lot many issues and challenges with the same that user might get the server downtime for infinite delay. For overcoming that there is an on-going technique which is known as Hot Migration or Live migration. It allows system administrators to shuffle the load available on an operating system instance to other Guest Operating system instance without any intervention on hosted service. This approach benefits to provide and efficient online system where no downfall in server maintenance. It also cures the load balancing, reconfiguration of data centers. The simulation of above aforesaid concept has been done via Cloudsim a tool for modelling and simulation in Cloud computing environment for live migration of virtual machines without intervention of cloud services. This paper simulated the work and gives the efficient results in comparison to the existing techniques."
pub.1125755610,Probabilistic analysis of security attacks in cloud environment using hidden Markov models,"Summary The rapidly growing cloud computing paradigm provides a cost‐effective platform for storing, sharing, and delivering data and computation through internet connectivity. However, one of the biggest barriers for massive cloud adoption is the growing cybersecurity threats/risks that influence its confidence and feasibility. Existing threat models for clouds may not be able to capture complex attacks. For example, an attacker may combine multiple security vulnerabilities into an intelligent, persistent, and sequence of attack behaviors that will continuously act to compromise the target on clouds. Hence, new models for detection of complex and diversified network attacks are needed. In this article, we introduce an effective threat modeling approach that has the ability to predict and detect the probability of occurrence of various security threats and attacks within the cloud environment using hidden Markov models (HMMs). The HMM is a powerful statistical analysis technique and is used to create a probability matrix based on the sensitivity of the data and possible system components that can be attacked. In addition, the HMM is used to provide supplemental information to discover a trend attack pattern from the implicit (or hidden) raw data. The proposed model is trained to identify anomalous sequences or threats so that accurate and up‐to‐date information on risk exposure of cloud‐hosted services are properly detected. The proposed model would act as an underlying framework and a guiding tool for cloud systems security experts and administrators to secure processes and services over the cloud. The performance evaluation shows the effectiveness of the proposed approach to find attack probability and the number of correctly detected attacks in the presence of multiple attack scenarios."
pub.1121036382,MDRAA: A Multi-decisive Resource Allocation Approach to Enhance Energy Efficiency in a Cloud Computing Environment,"With the development of cloud environment which is serving user requests, storing data etc., energy consumption has become a big issue. Increased energy data consumption of data centers emit a large amount of CO2 and also has made the IT industry to worry about when we think of green computing. As more tasks are running in the datacenter, minimizing the energy consumption becomes a challenge. Technologies like virtualization, migration, and DVFS (Dynamic Voltage and Frequency Scaling) and workload consolidation are the appreciating solutions and hence used in our work to reduce energy consumption and power without affecting the progress rate of jobs. Virtualization is a technology in which physical machines are partitioned into multiple virtual machines (VMs). Techniques like Fuzzy logic and Linear Regression are also used for the host discovery and allocation of VM identified for migration. We have also compared our proposed mechanism with existing systems in various dimensions. To understand this, a prior knowledge of cloud’s energy consumption is required."
pub.1120698309,Multi-Objective Service Placement Scheme Based on Fuzzy-AHP System for Distributed Cloud Computing,"With the rapid increase in the development of the cloud data centers, it is expected that massive data will be generated, which will decrease service response time for the cloud data centers. To improve the service response time, distributed cloud computing has been designed and researched for placement and migration from mobile devices close to edge servers that have secure resource computing. However, most of the related studies did not provide sufficient service efficiency for multi-objective factors such as energy efficiency, resource efficiency, and performance improvement. In addition, most of the existing approaches did not consider various metrics. Thus, to maximize energy efficiency, maximize performance, and reduce costs, we consider multi-metric factors by combining decision methods, according to user requirements. In order to satisfy the user’s requirements based on service, we propose an efficient service placement system named fuzzy- analytical hierarchical process and then analyze the metric that enables the decision and selection of a machine in the distributed cloud environment. Lastly, using different placement schemes, we demonstrate the performance of the proposed scheme."
pub.1101401231,"Challenges and Solutions in Smart Learning, Proceeding of 2018 International Conference on Smart Learning Environments, Beijing, China","This book focuses on the interplay between pedagogy and technology, and their fusion for the advancement of smart learning environments. It discusses various components of this interplay, including learning and assessment paradigms, social factors and policies, emerging technologies, innovative application of mature technologies, transformation of curriculum and teaching behavior, transformation of administration, best infusion practices, and piloting of new ideas. The book provides an archival forum for researchers, academics, practitioners and industry professionals interested and/or engaged in reforming teaching and learning methods by promoting smart learning environments. It also facilitates discussions and constructive dialogue among various stakeholders on the limitations of existing learning environments, the need for reform, innovative uses of emerging pedagogical approaches and technologies, and sharing and promoting best practices, leading to the evolution, design and implementation of smart learning environments."
pub.1135015727,A lightweight cryptographic algorithm for the transmission of images from road environments in self-driving,"With the large-scale application of 5G in industrial production, the Internet of Things has become an important technology for various industries to achieve efficiency improvement and digital transformation with the help of the mobile edge computing. In the modern industry, the user often stores data collected by IoT devices in the cloud, but the data at the edge of the network involves a large of the sensitive information, which increases the risk of privacy leakage. In order to address these two challenges, we propose a security strategy in the edge computing. Our security strategy combines the Feistel architecture and short comparable encryption based on sliding window (SCESW). Compared to existing security strategies, our proposed security strategy guarantees its security while significantly reducing the computational overhead. And our GRC algorithm can be successfully deployed on a hardware platform."
pub.1105797226,Learning non-deterministic impact models for adaptation,"Many adaptive systems react to variations in their environment by changing their configuration. Often, they make the adaptation decisions based on some knowledge about how the reconfiguration actions impact the key performance indicators. However, the outcome of these actions is typically affected by uncertainty. Adaptation actions have non-deterministic impacts, potentially leading to multiple outcomes. When this uncertainty is not captured explicitly in the models that guide adaptation, decisions may turn out ineffective or even harmful to the system. Also critical is the need for these models to be interpretable to the human operators that are accountable for the system. However, accurate impact models for actions that result in non-deterministic outcomes are very difficult to obtain and existing techniques that support the automatic generation of these models, mainly based on machine learning, are limited in the way they learn non-determinism. In this paper, we propose a method to learn human-readable models that capture non-deterministic impacts explicitly. Additionally, we discuss how to exploit expert's knowledge to bootstrap the adaptation process as well as how to use the learned impacts to revise models defined offline. We motivate our work on the adaptation of applications in the cloud, typically affected by hardware heterogeneity and resource contention. To validate our approach we use a prototype based on the RUBiS auction application."
pub.1001246999,Cloud voice service as over-the-top solution for seamless voice call continuity in a heterogeneous network environment,"Despite the current introduction of the 4G mobile network technology Long Term Evolution (LTE) and the existing heterogeneous network infrastructure, hardly any service provider supports seamless cloud voice service owing to the high integration cost.In order to address this challenging issue, we present a cost-effective Virtual Room based vertical handoff (VOOH) as an Over-the-top (OTT) solution, which is network provider independent and only uses existing wireless access technologies (e.g., LTE, 2G/3G, WiFi) for connectivity purposes. That means no changes in the underlying network technologies are required that result in lower integration effort. Besides providing handoff of ongoing Voice over IP (VoIP) calls seamlessly between WiFi and LTE, unlike related solutions (e.g., Circuit-Switched Fall Back (CSFB)) VOOH presents a cost-effective OTT solution to support voice call continuity between LTE and the circuit-switched channel of 2G/3G networks. In the long term, VOOH also enables data-offloading (e.g., via WiFi) to lower the traffic load of cellular base stations.While our previous work has mainly focused on proof of concepts and experimental performance evaluations, this manuscript proposes a novel analytical and simulation model to analyze the scalability of the VOOH solution that is discussed for specific use case examples. Moreover, the impact of an enhanced VOOH (eVOOH) approach is presented to improve the system performance in terms of blocking probability. Based on the validated models, the results illustrate that eVOOH outperforms the original solution that is shown by lower call blocking probabilities."
pub.1128037806,Architecting Intelligent Digital Systems and Services,"Our paper gives first answers on a fundamental question: How can the design of architectures of intelligent digital systems and services be accomplished methodologically? Intelligent systems and services are the goals of many current digitalization efforts today and part of massive digital transformation efforts based on digital technologies. Digital systems and services are the foundation of digital platforms and ecosystems. Digitalization disrupts existing businesses, technologies, and economies and promotes the architecture of open environments. This has a strong impact on new value-added opportunities and the development of intelligent digital systems and services. Digital technologies such as artificial intelligence, the Internet of Things, services computing, cloud computing, big data with analytics, mobile systems, and social enterprise networks systems are important enablers of digitalization. The current publication presents our research on the architecture of intelligent digital ecosystems of products and services influenced by the service-dominant logic. We present original methodological extensions and a new reference model for digital architectures with an integral service and value perspective to model intelligent systems and services that effectively align digital strategies and architectures with artificial intelligence as main elements to support intelligent digitalization."
pub.1158532407,Look-Ahead Task Offloading for Multi-User Mobile Augmented Reality in Edge-Cloud Computing,"Mobile augmented reality (MAR) blends a real scenario with overlaid virtual
content, which has been envisioned as one of the ubiquitous interfaces to the
Metaverse. Due to the limited computing power and battery life of MAR devices,
it is common to offload the computation tasks to edge or cloud servers in close
proximity. However, existing offloading solutions developed for MAR tasks
suffer from high migration overhead, poor scalability, and short-sightedness
when applied in provisioning multi-user MAR services. To address these issues,
a MAR service-oriented task offloading scheme is designed and evaluated in
edge-cloud computing networks. Specifically, the task interdependency of MAR
applications is firstly analyzed and modeled by using directed acyclic graphs.
Then, we propose a look-ahead offloading scheme based on a modified Monte Carlo
tree (MMCT) search, which can run several multi-step executions in advance to
get an estimate of the long-term effect of immediate action. Experiment results
show that the proposed offloading scheme can effectively improve the quality of
service (QoS) in provisioning multi-user MAR services, compared to four
benchmark schemes. Furthermore, it is also shown that the proposed solution is
stable and suitable for applications in a highly volatile environment."
pub.1147859031,Smart Technologies in Food Manufacturing,"The concept of “smart food factory” is also known as “food industry 4.0” or “connected food industry” which relies on the up-gradation of existing facilities to modern technologies and their integration with the internet, cyber physical system, artificial intelligence, big data, cloud computing, etc. Smart food industry is a digitally connected, automated food processing environment where the real-time monitoring of physical operations, collection and sharing of data throughout the processing line, storing and processing the data through neural networking and algorithm, communicating and cooperating with humans in real time, along with precise control over the operation through actuators and robots are done simultaneously. As compared to others, food industries are lagging behind in adoption of modern technologies. However, increasing market requirements and strict compliance of food safety regulation in food industries around the world like, grain processing industry, fruits and vegetables processing industry, meat, fish and poultry processing industry, dairy and beverage industry, etc., upgraded their existing facilities at different levels with modern technologies. This chapter illustrates a deep insight on transformation of different food processing industries from a traditional processing environment to a digitally connected modern technologies based environment over the decades."
pub.1172529177,Intrusion detection and secure data storage in the cloud were recommend by a multiscale deep bidirectional gated recurrent neural network,"Data communication security is developing very day with the creation of cloud computing. The imperative for robust data communication security has become increasingly evident with the pervasive adoption of cloud computing. However, challenges persist due to the inherent complexities and limited security measures in cloud environments, particularly in transmitting and storing sensitive information. Previous studies have underscored the intricate nature of intrusion detection systems (IDS) in cloud-based settings. In this research, a Multi scale Deep Bidirectional Gated Recurrent Neural Network and Optimal Encryption Scheme espoused Intrusion Detection and Secure Data Storage in the Cloud (MDBGRNN-ID-SCESOA) is proposed. Leveraging the KDD CUP 99 dataset and DS2OS Dataset, initial data preprocessing involves Domain Transform Filtering (DTF) for tokenization, dimension reduction, and semantic analysis. Subsequently, MDBGRNN is employed to discern intrusion from non-intrusion data. Furthermore, a two-way encryption mechanism, integrating Elliptical Curve Cryptography (ECC) with the Sine Cosine Egret Swarm Optimization Algorithm (ECC-SCESOA), enhances data security while minimizing computational overhead. To safeguard encrypted data at rest in the cloud, a steganography process is devised, effectively concealing sensitive content. Performance evaluation metrics, including accuracy, specificity, sensitivity, encryption/decryption time, execution time, memory usage, and Matthews correlation coefficient (MCC), demonstrate the efficacy of MDBGRNN-ID-SCESOA. Comparative analysis with existing techniques reveals notable enhancements in computational efficiency and data security. This comprehensive approach addresses critical security concerns in cloud computing, offering a promising avenue for safeguarding sensitive data in cloud environments."
pub.1165183921,Look-Ahead Task Offloading for Multi-User Mobile Augmented Reality in Edge-Cloud Computing,"Mobile augmented reality (MAR) blends a real scenario with overlaid virtual content, which has been envisioned as one of the ubiquitous interfaces to the Metaverse. Due to the limited computing power and battery life of MAR devices, it is common to offload the computation tasks to edge or cloud servers in close proximity. However, existing offloading solutions developed for MAR tasks suffer from high migration overhead, poor scalability, and short-sightedness when applied in provisioning multi-user MAR services. To address these issues, a MAR service-oriented task offloading scheme is designed and evaluated in edge-cloud computing networks. Specifically, the task interdependency of MAR applications is firstly analyzed and modeled by using directed acyclic graphs. Then we propose a look-ahead offloading scheme based on a modified Monte Carlo tree (MMCT) search, which can run several multi-step executions in advance to get an estimate of the long-term effect of immediate action. Experiment results show that the proposed off-loading scheme can effectively improve the quality of service (QoS) in provisioning multi-user MAR services, compared to four benchmark schemes. Furthermore, it is also shown that the proposed solution is stable and suitable for applications in a highly volatile environment."
pub.1173479079,Security Automation in next-generation Networks and Cloud environments,"In the next generation networks and cloud systems, administrators should only need to define their intentions through simple high-level intents, leaving the system to autonomously implement them in the best way possible. The adoption of automation enables the possibility to create reactive systems that can reconfigure themselves in response to unpredictable events, such as network attacks. Nowadays, such solutions are far from being achieved. The enforcement of security requirements continues to heavily rely on manual efforts and tools requiring non-negligible expertise to be used. This results in frequent misconfiguration errors or the complete absence of default security measures due to their high implementation complexity. This paper introduces the research that will be carried out within my Ph.D. program, focusing on network security automation. The objective is to bridge existing gaps in the literature, on one side developing novel automated and intent-based approaches for security enforcement in cloud environments, ensuring formal correctness and optimization, and on the other side researching new solutions for the design of security reaction mechanisms for modern networks in response to network attacks."
pub.1182212373,Analysis of application of FPGA technologies in IoT,"The subject of study in this article and work is the modern technologies of programmable logic devices (PLD) classified as FPGA, and the peculiarities of its application in Internet-of-Things domain at different architectural layers of the implementation, as well as the elements of decision making during choosing of appropriate solutions based on FPGA in the context of implementation of IoT tasks with the requirements of intensive computations. The goal is to analyze the possibilities and peculiarities of the application of technology of programmable logic devices as part of the modern architecture of the Internet of Things with the improvement of decision making process during the choosing of appropriate technical solutions for the use of FPGA with taking into account the types of tasks. Tasks : to analyze the current state of demands regarding the use of FPGA technology in IoT projects; to analyze the challenges and limitations for the application of FPGA technology at different layers during the prototyping of IoT infrastructure; to propose ways of the use of FPGA technology in the IoT infrastructure with the use of high-speed computations; to analyze the advantages and disadvantages of the proposed approaches of integration of FPGA technology; to provide recommendations and determine the further direction of the research of the use of FPGA technology as a part of the IoT infrastructure considering of AI/ML tasks; to provide a practical example of the use of FPGA technology during the creation of a high-performance IoT system. According to the tasks, the following results were obtained. The analysis of the current state of the use of FPGA technology in IoT projects is performed, that showed the significant potential of these technologies for the implementation of high-speed computations in Internet of Things systems. A classification of the ways of applying FPGA in the tasks of real-time data processing and implementation of neural networks is proposed. The set of limitations of the possibility of FPGA technology applying at different layers of IoT infrastructure is defined. It includes high equipment costs, the need for specific design skills, limitations in energy consumption, and the complexity of the integration with existing systems. The practical example of the use of the IoT system with the implementation of high-intensive computations is provided. Conclusions. The main contribution and scientific novelty of the obtained results is that the conducted analysis allows to make a decision about the possibility of applying programmable logic in the construction of IoT projects and home automation systems. Based on the proposed classification of types of IoT tasks, there is a possibility of making a decision on the application of FPGA technology as a separate integrated circuit in the system or using a complete instance of FPGA as a Service in the cloud environment."
pub.1093773464,WFE-enabled Web Page Transformation: Generating Real-Time Collaborative Editing Systems from Exisiting Web Pages,"We have been developing Web Flexible Editing (WFE), a system for Real-Time Collaborative Editing (RTCE) for existing Web pages. We propose three transformation approaches to enable several users to modify existing Web pages simultaneously on their browsers, while modified contents are reflected to other users in real-time. The three approaches (a server-side, a local proxy., and a companion server) involve trade-offs among URL transparency, server environment invasiveness, and user environment invasiveness. We present one of the optimal implementations to realize our system as a service in a cloud computing environment. Our comparison experiments show that we can achieve this by using push technology to perform synchronization."
pub.1130733746,Multi-Container Application Migration with Load Balanced and Adaptive Parallel TCP,"Application migration in Wide Area Network (WAN) is needed in many scenarios: disaster recovery and service hand-off in edge cloud. Modern distributed applications are virtualized with multiple virtual machines or containers. This paper focuses on parallel multi-container migration in WAN environments by utilizing multiple TCP connections over a single direct path (a.k.a parallel TCP). Our application migration middleware framework utilizes a feedback controller to determine a proper number of parallel container migration (i.e., parallel window) based on changing network bandwidth. Then a middleware’s scheduler selects migration requests for the parallel window to load balance multiple pairs of hosts. The goal of our migration is to achieve the best possible balance between optimizing the total migration time and average individual migration time. This differs from most existing live migration works that attempt to optimize mainly the down time. Our proposed framework is generalized and not restricted to any particular virtualization technology implementation. For performance evaluation, we conducted a WAN-emulated experiment in static and dynamic network settings. The performance evaluation results show that the total migration time using our feedback controller is less than that of the sequential migration by 32.7% in the static network, and 43.9% in the dynamic network. Moreover, while achieving total migration time comparable to that of the best fixed parallel migration window, our approach can reduce the average individual migration time by 62.7% in the dynamic network setting."
pub.1033162252,Next generation content networks,"Content Delivery Networks (CDNs) [1] have emerged to overcome the inherent limitations of the Internet in terms of user perceived Quality of Service (QoS) when accessing Web content. They offer infrastructure and mechanisms to deliver content and services in a scalable manner, and enhance users' Web experience. However, modern applications do not just perform retrieval or access operations on content but also create content, modify and manage content, and actively place content at appropriate locations. In order to deal with such new requirements, along with the proliferation, formation, and consolidation of the CDN landscape, new forms of Content Networks (CNs) are coming into picture. Thus, distribution and management of content is introducing new challenges in this domain through raising new issues in the architecture, design and implementation of CNs. Moreover, the evolution of next-generation CNs in a large-scale heterogeneous environment demands for a paradigm shift within the research community in terms of the technologies used. Therefore, the integrated uses of existing emerging as well as stable technologies (e.g. agent, P2P, grid, data mining) are anticipated to augment the effectiveness and boost the efficiency of future CN infrastructures [2, 3, 4]. The aim of this panel session is to discuss about new ideas and results in the content networking domain. It will capture the state-of-the-art in content networking domain in terms of organizational structure, content distribution mechanisms, request redirection techniques, and performance measurement methodologies. It will also identify potential research directions and technologies that will drive innovations within this domain. In particular, the talks will be focused on agents for content management and delivery, P2P-based CNs, mobile multimedia CDNs, integration between CDNs and cloud computing, and content delivery on wireless networks."
pub.1169000211,An Energy Efficient Virtual Machine Placement Scheme for Intelligent Resource Management at Cloud Data Center,"In cloud computing environments, virtualization optimally allocates PM (physical machine) resources to users through VM (virtual machines). Operating these PMs leads to significant energy usage, and PM overloading due to high resource demands can breach SLA (Service Level Agreement). Dynamic VM consolidation techniques mitigate this by migrating VMs to enhance resource usage and reduce energy consumption. However, excessive migrations can hamper application performance due to runtime overhead. Effective resource management is crucial to improve cloud operational efficiency. An EE-VMP (Energy Efficient VM Placement) model is proposed to tackle these challenges. It utilizes an evolutionary NN (Neural Network) to predict VM resource needs, optimized by the Stochastic Gradient Descent with Momentum algorithm. Optimal VM allocations are based on these predictions. Experiments using the GCD (Google Cluster Data Set) highlight the model’s efficacy. Compared to existing methods, the proposed model exhibits substantial reductions in power consumption by up to 56.89%, the number of active servers by 37%, and improvement in resource utilization by up to 64.03%."
pub.1158312983,Self-attention convolutional neural network optimized with season optimization algorithm Espoused Chronic Kidney Diseases Diagnosis in Big Data System,"Nowadays, Internet of Things (IoT) and cloud platforms are broadly employed in numerous healthcare applications. Instead of using the limited storage and processing power found in mobile devices, the vast amount of data generated by internet of things devices in healthcare sector can be evaluated on a cloud platform. In this article, the Self-Attention Convolutional Neural Network optimized with Season Optimization Algorithm is proposed for Chronic Kidney Disease Diagnosis using IoT and cloud computing in Smart Medical Big Data health care system (SACNN-SOA-CKD-IoT-CC). IoT devices, like wearable and sensors perform data acquisition process. For chronic kidney disease (CKD) diagnostic model, the self-Attention convolutional neural network (SACNN) is applied. But, the SACNN not divulge any optimization systems adoption to calculate the optimal parameters and to make sure the exact categorization of Chronic Kidney Disease (CKD). Therefore, the season optimization algorithm (SOA) is used to optimize SACNN. The proposed approach is implemented in python language its performance is analyzed with performance metrices, like sensitivity, accuracy, recall, f-measure, specificity, network latency, scalability, response time, delay, and accuracy. The proposed SACNN-SOA-CKD-IoT-CC method achieves higher accuracy in CKD dataset of 15.66 %, 21.65 % and 9.64 %, lower error rate of 11.27 %, 8.35 %, and 21.06 % compared to the existing methods, like intelligent internet of things with cloud centric clinical decision support scheme for the prediction of CKD (LR-AME-CKD-IoT-CC), diagnostic prediction method for CKD in internet of things (MLP-SVM-CKD-IoT-CC) and ensemble of deep learning based clinical decision support system for CKD detection in medical internet of things environment (EDL-CDSS-CKD-IoT-CC)."
pub.1149358838,Energy efficient task allocation and consolidation in multicast cloud network,"Information Technology is now a revolution due to its user dependence for various services available over Internet through cloud computing (CC). The usability is now reached to nearly 60% of globally population. The number is increasing rapidly everyday due to its services in low cost. It is becoming a challenge to manage the load, speed, computing, processing with respect to cost in CC. The resources generate enormous heat in computing and effecting the environment adversely. Hence authors have adopted multicasting, load balancing and greening environment using energy efficiency policies to get minimum consumption of energy. In this work we focus on local communication between Simple Storage Service (S3) and Elastic Computing Cloud (EC2) considering the load of virtual machines (VMs). We suggest two algorithms (i) energy-aware task consolidation (EaTC) and (ii) Low Energy Saving Task Consolidation (LESTC) considering dynamic download for different servers. The proposed mechanism also reduces migration cost and the migration mechanism performs better. Overall, the experimental study concludes that multicasting among local servers even in heterogeneous environments open doors for energy efficient optimized load to improve the performance of the system. The proposed mechanism is compared with various existing mechanism of its class and found better."
pub.1128734507,A Review of Cognitive Radio Smart Grid Communication Infrastructure Systems,"The cognitive smart grid (SG) communication paradigm aims to mitigate quality of service (QoS) issues in obsolete communication architecture associated with the conventional electrical grid. This paradigm entails the integration of advanced information and communication technologies (ICTs) into power grids, enabling a two-way flow of information. However, due to the exponential increase in wireless applications and services, also driven by the deployment of the Internet of Things (IoT) smart devices, SG communication systems are expected to handle large volumes of data. As a result, the operation of SG networks is confronted with the major challenge of managing and processing data in a reliable and secure manner. The existing works in the literature proposed architectures with the objective to mitigate the underlying QoS issues such as latency, bandwidth, data congestion, energy efficiency, etc. In addition, a variety of communication technologies have been analyzed for their capacity to support stringent QoS requirements for diverse SGs environments. This notwithstanding, a standard architecture designed to mitigate the aforementioned issues for SG networks remains a work-in-progress. The main objective of this paper is to investigate the emerging technologies such as cognitive radio networks (CRNs) as part of the Fifth-Generation (5G) mobile technology for reliable communication in SG networks. Furthermore, a hybrid architecture based on the combination of fog computing and cloud computing is proposed. In this architecture, real-time latency-sensitive information is given high priority, with fog edge based servers deployed in close proximity to home area networks (HANs) for preprocessing and analyzing of information collected from smart IoT devices. In comparison to the recent works in the literature, which are mainly based on CRNs and 5G separately, the proposed architecture in this paper incorporates the combination of CRNs and 5G for reliable and efficient communication in SG networks."
pub.1155754877,Morphological exploration of Arctic rivers using Google Earth Engine,"Climate change is already altering the hydrological regime of Arctic rivers. However, still little is known about fluvial morphological processes and trajectories in permafrost environments. In such iced floodplains, both hydrological and thermal regimes affect sediment transport and riverine morphological processes. Remote sensing represents a powerful approach to investigate fluvial systems in those isolated areas. Nevertheless, its application presents challenges linked to ice seasonality and the limited time window of the morphological activity, alongside the complex permafrost/river spatial patterns and related spectral signatures, which imply significant computational efforts. Addressing this, we propose an improved integration of existing tools for the spatio-temporal extraction of fluvial morphological indicators, combining in a unique working environment the cloud computing capability of Google Earth Engine (GEE) and a process-based tool for riverine multitemporal planform analysis (PyRIS). Fluvial morphological metrics have been extracted from a set of meandering rivers in the Arctic region, outlining the potential of anisotropic image filtering and image segmentation to enhance active channel detection in complex spatial-pattern areas. A 20-40% refinement in small object removal in river mask detection emerges. The synergy among existing instruments enhances the observation of natural river systems in permafrost environments, setting the basis for further studies on morphological processes and the evolution of such pristine and climatically-sensitive river systems."
pub.1032451213,Privacy Preserving Computation in Cloud Using Noise-Free Fully Homomorphic Encryption (FHE) Schemes,"With the wide adoption of cloud computing paradigm, it is important to develop appropriate techniques to protect client data privacy in the cloud. Encryption is one of the major techniques that could be used to achieve this goal. However, data encryption at the rest alone is insufficient for secure cloud computation environments. There is also the need for efficient techniques to carry out computation over encrypted data. Fully homomorphic encryption (FHE) and garbled circuits are naturally used to process encrypted data without leaking any information about the data. However, existing FHE schemes are inefficient for processing large amount of data in cloud and garbled circuits are one time programs and cannot be reused. Based on quaternion/octonion algebra and Jordan algebra over finite rings Zq$$\mathbb {Z}_q$$, this paper designs efficient fully homomorphic symmetric key encryption (FHE) schemes without bootstrapping (that is, noise-free FHE schemes) that are secure in the weak ciphertext-only security model assuming the hardness of solving multivariate quadratic equation systems and solving univariate high degree polynomial equation systems in Zq$$\mathbb {Z}_q$$. The FHE scheme designed in this paper is sufficient for privacy preserving computation in cloud."
pub.1149303769,Knowledge engineering–based DApp using blockchain technology for protract medical certificates privacy,"In the Industry 4.0 era, an inherited featured technology, blockchain, plays a vital role in knowledge engineering applications. Blockchain provides privacy to sensitive data as an intelligent agent, so its adoption rate increases in all the advanced domains. Especially in the health care department, blockchain technology usage helps avoid attacks like the Wannacry ransomware attack during 2017. Therefore, this paper described a decentralised application (DApp) expert system using public blockchain to create and maintain official health documents, especially medical certificates. Current existing systems, either paper‐based or database or clouds to save the medical certificates, have more scope to do attacks. Hence, proposed a blockchain‐based DApp that acts as an interface between intelligent agents, blockchains, and system related to the medical certificates. The main strength of this paper is implementation results, which are not among the maximum literary works currently available. The associate cost for conducting distributed application operations on the blockchain in terms of Gas comprehensively presented here. Furthermore, it consists of comparing the system's non‐functional functions by considering blockchain and non‐blockchain environments. Also, presented the simulation results with the performance results compared with the existed systems."
pub.1181288888,SELECTED ASPECTS OF DIGITAL REPRESENTATION OF INFORMATION SYSTEMS,"From the viewpoint of the evolution of computing devices and corresponding use cases, the article structures an information systems timeline progressing through early electromechanical devices, electronic vacuum tubes, solid-state and integrated circuit electronics, the advent of microprocessors, personal and remote computing, and distributed systems of numerous autonomous devices, while exploring different approaches to a digital representation of computational entities. By establishing the research scope and organizing scholarly sources chronologically, the article reveals and selects connections between individual events, provides an overview and critical analysis, and highlights expected future expansions and shifts in approaches to digital representation. Thus, the article examines a shift from operations and operands, their further complication into code and data structures, and the transition from procedural to structured and object-oriented programming (OOP). Client-server applications implemented with static OOP and relational data management are examined as the pinnacle of monolithic architecture. The subsequent domain-driven design (DDD) and microservices architecture are examined as contemporary methods for remote cloud computing environments. The article then discusses the rise of the Internet of Things (IoT), the emergence of smart things and digital twins, describes advanced and novel use cases of global digitalization, such as Industry 5.0 ideas, and reveals the limitations of extant methods for corresponding digital representation. Ultimately, the article introduces a novel method for digital representation employing the post-non-classical paradigm in computer science, which eschews predefined structures in favor of dynamic, interaction-based representations, enabling flexible and adaptive design of distributed systems. Future research directions include the formal specification of this approach and the development of tools for its implementation in complex distributed systems. Keywords : digital representation, solution architecture, programming paradigms, distributed systems, internet of things, digital twin, industry 5.0"
pub.1168586931,"Strategic Implementation of AWS Security Services: A Focus on Best Practices, SSM and Secrets Manager","Purpose: As organizations increasingly migrate to the cloud, ensuring robust data security becomes paramount. This technical paper explores the purpose, methodology, findings, and unique contributions in cloud security, focusing on AWS Systems Manager (SSM) and Secrets Manager. The objective is to provide a comprehensive guide for engineers and architects seeking to enhance data security in their cloud environments.
 Methodology: The comparative study conducted involves analyzing the features, capabilities, and integration possibilities of AWS SSM and Secrets Manager. Real-world use cases, best practices, and security measures associated with these services are explored through a methodology that includes hands-on exploration, case studies, and a survey of existing literature to distill critical insights.
 Findings: The paper uncovers a rich landscape of security measures facilitated by AWS SSM and Secrets Manager. Findings highlight the strengths and limitations of each service, emphasizing the importance of their integration for a holistic security approach. Automated rotation of credentials, encryption options, IAM policies, and monitoring strategies emerge as critical findings, contributing to the overall understanding of secure cloud data management.
 Unique contributor to theory, policy, and practice: This work uniquely provides a detailed comparative analysis that empowers engineers and architects to make well-informed decisions. The paper offers insights into the nuances of AWS SSM and Secrets Manager, enabling professionals to tailor their security strategies based on specific use cases and requirements. The focus on real-world scenarios and identifying best practices make this contribution practical and applicable, serving as a valuable resource for those navigating the complex landscape of cloud security engineering and architecture."
pub.1095796728,Starling: Minimizing Communication Overhead in Virtualized Computing Platforms Using Decentralized Affinity-Aware Migration,"Virtualization is being widely used in large-scale computing environments, such as clouds, data centers, and grids, to provide application portability and facilitate resource multiplexing while retaining application isolation. In many existing virtualized platforms, it has been found that the network bandwidth often becomes the bottleneck resource, causing both high network contention and reduced performance for communication and data-intensive applications. In this paper, we present a decentralized affinity-aware migration technique that incorporates heterogeneity and dynamism in network topology and job communication patterns to allocate virtual machines on the available physical resources. Our technique monitors network affinity between pairs of VMs and uses a distributed bartering algorithm, coupled with migration, to dynamically adjust VM placement such that communication overhead is minimized. Our experimental results running the Intel MPI benchmark and a scientific application on a 7-node Xen cluster show that we can get up to 42% improvement in the runtime of the application over a no-migration technique, while achieving up to 85% reduction in network communication cost. In addition, our technique is able to adjust to dynamic variations in communication patterns and provides both good performance and low network contention with minimal overhead."
pub.1106122106,Building a Conceptual Framework for E-Learning Based on Cloud Computing In Egyptian Universities,"In order to have an intelligent education, we have to make use of all modern education techniques. One of these techniques is the integration of information and communication technologies in education according to the global trend. In other words, to prepare a creative environment using means of existing web tools, techniques, and services to provide Browser-Based-Application. Nowadays Arab countries have a big interest in E-learning techniques and put it into the form of services within Services Oriented Architecture Technique (SOA), and mixing its input and outputs within the components of the Education Business Intelligence and enhance it to simulate reality by educational virtual worlds. This paper presents an idea about tools, instruments, techniques to enhance the educational process in the way that it could reach maximum uses of intelligent education modern technique."
pub.1093370910,An Adaptive Network Intrusion Detection Approach for the Cloud Environment,"As Internet attacks grow rapidly, firewalls or network intrusion systems are indispensable. Existing approaches usually use attack signatures, machine learning or data mining algorithms to detect and stop anomaly or malicious flow. Machine learning algorithms need a set of labeled data to train the detection model, while the labeled data set is not always available. In this paper, we proposed an anomaly detection approach that is adaptive to the ever-changing network environment. The approach constructs a decision tree-based detection model for intrusion detection from unlabeled data by using an unsupervised learning algorithm called spectral clustering. And the system can easily be deployed on the cloud environment. In the experiments with the DARPA 2000 data set and the KDD Cup 1999 data set, our system shows notable improvement on the detection performance after the adaptation procedure."
pub.1094610722,Review on Pre-fetching for Mobile Cloud Computing,"Improvement of our technology on mobile devices and the significant of cloud computing (CC) services has led to the integration of mobile devices with a CC service called as Mobile Cloud Computing (MCC). It provides many benefits for users. However, the increasing numbers of users utilising the MCC service leads to the latency on the network. Besides, there are several constraints on mobile devices including battery capacity, storage and bandwidth. Hence, this work proposes intelligent mobile web pre-fetching for the cloud environment in order to handle this critical problem. In order to increase understanding of MCC, we conducted an extensive study, previous work on the pre-fetching technique and Machine Learning, which were used to enhance the efficiency and effectiveness of their performance. Our study corroborates that existing systems for the pre-fetching technique focuses on regular websites but it still narrows the study of cloud computing especially for mobile devices and still has limitations."
pub.1153965633,Fault Tolerant Load Balancing with Quadruple Osmotic Hybrid Classifier and Whale Optimization for Cloud Computing,"Cloud Computing (CC) environment is developing as a recently discovered caliber for computing applications over the network. Fault tolerance is one of the foremost issues in CC environment. Since the negligence in resource have a profound effect on job execution, throughput, response time and performance of the entire network. In this work, in order to address the issue, Quadruple Osmotic Hybrid Classification and Whale Optimization (QOHC-WO) is introduced to fault-tolerance under the requirement of different user request tasks. Initially, Quadruple Fault Tolerance Level is applied to allocate the fault tolerance level. Followed by, Hybrid Vector Classifier is used to categorize the user request tasks (task) and cloud server nodes (node). Then, the Osmotic function is employed for performing the migration among virtual machines with lesser response time. This helps to solve the maximum level of fault issue. Finally, Improved Whale Optimization Algorithm is applied to find the optimal allocation of tasks with the corresponding node. In addition, the Bandit function and Whale optimization are used to address the trade-off between exploitation and exploration. Experimental setup of the proposed QOHC-WO method and existing methods are carried out with different factors such as task response time, the number of VM migrations, and percentage of fault detected rate with respect to a number of tasks. The analyzed results validate that the proposed QOHC-WO method achieves a higher fault detection rate with minimum response time as well as task migration than the state-of-the-art methods."
pub.1172130538,CLOUD-BASED DATA INTRUSION DETECTION SYSTEM USING ML TECHNIQUES,"Cloud computing (CC) stands as a groundbreaking technology, streamlining access to network and computer resources while offering a plethora of services, such as storage and data management, all with the aim of optimizing system functionality. Despite its array of advantages, cloud providers grapple with notable security challenges, particularly concerning the safeguarding of resources and services. Addressing these concerns and bolstering security measures necessitates vigilant monitoring of resources, services, and networks to promptly detect and respond to potential attacks. Central to this effort is the implementation of an advanced mechanism known as an intrusion detection system (IDS), which plays a pivotal role in regulating network traffic and identifying anomalous activities. This paper introduces an innovative cloud-based intrusion detection model that harnesses the power of the random forest (RF) algorithm alongside cutting-edge feature engineering techniques. Specifically, the integration of the RF classifier aims to enhance the accuracy (ACC) of the detection model significantly. The efficacy of the proposed model is rigorously evaluated using the NSL-KDD dataset, demonstrating a remarkable 99.99% ACC. This performance surpasses that of existing methodologies, underscoring the effectiveness and robustness of the proposed approach in addressing security challenges within cloud environments. Keywords —Accuracy, Anomaly detection, Cloud security, Feature Engineering, Intrusion Detection System (IDS), Random forest (RF)"
pub.1111934189,An Enhanced Knowledge Integration of Association Rules in the Privacy Preserved Distributed Environment to Identify the Exact Interesting Pattern,"Numerous research works are carried out in the field of data mining, especially in the areas of association rule mining, knowledge integration in the distributed data mining and privacy intense data mining. In the distributed data mining environment, the local data mining systems distributed across the environment. The way these local mining systems distributed in the environment, plays a major role in the process of knowledge integration. If all the local data mining systems are deployed in an organization, there will not be any impact. If the local data mining systems distributed across multiple organizations, that would cause a major impact in the process of knowledge integration. The problems are caused due to the privacy related issues and the agreement between those organizations. Though there are existing generic approaches to integrate the knowledge in the distributed mining, focus of this paper is to propose an enhanced algorithm specific to integration of association rules in the privacy protected distributed data mining environment and to find the interesting rules which are sub sets of an actual rule."
pub.1173671117,GIJA:Enhanced geyser‐inspired Jaya algorithm for task scheduling optimization in cloud computing,"Abstract Task scheduling optimization plays a pivotal role in enhancing the efficiency and performance of cloud computing systems. In this article, we introduce GIJA (Geyser‐inspired Jaya Algorithm), a novel optimization approach tailored for task scheduling in cloud computing environments. GIJA integrates the principles of the Geyser‐inspired algorithm with the Jaya algorithm, augmented by a Levy Flight mechanism, to address the complexities of task scheduling optimization. The motivation for this research stems from the increasing demand for efficient resource utilization and task management in cloud computing, driven by the proliferation of Internet of Things (IoT) devices and the growing reliance on cloud‐based services. Traditional task scheduling algorithms often face challenges in handling dynamic workloads, heterogeneous resources, and varying performance objectives, necessitating innovative optimization techniques. GIJA leverages the eruptive dynamics of geysers, inspired by nature's efficiency in channeling resources, to guide task scheduling decisions. By combining this Geyser‐inspired approach with the simplicity and effectiveness of the Jaya algorithm, GIJA offers a robust optimization framework capable of adapting to diverse cloud computing environments. Additionally, the integration of the Levy Flight mechanism introduces stochasticity into the optimization process, enabling the exploration of solution spaces and accelerating convergence. To evaluate the efficacy of GIJA, extensive experiments are conducted using synthetic and real‐world datasets representative of cloud computing workloads. Comparative analyses against existing task scheduling algorithms, including AOA, RSA, DMOA, PDOA, LPO, SCO, GIA, and GIAA, demonstrate the superior performance of GIJA in terms of solution quality, convergence rate, diversity, and robustness. The findings of GIJA provide a promising solution quality for addressing the complexities of task scheduling in cloud environments (95%), with implications for enhancing system performance, scalability, and resource utilization."
pub.1043224375,An Efficient Approach for Storage Migration of Virtual Machines Using Bitmap,"Cloud computing is an emerging technique to provide computing environment and services on-demand over web. Virtualization is the key technology used by cloud computing to employ virtual machines to satisfy the user demands for computing resources dynamically. The migration of the virtual machines in between physical hosts is necessary for balancing the load and minimizing the service disruption during system maintenance. The virtual machines with local storage have advantages of higher availability, improved performance and higher security over the virtual machines sharing centralized storage. So, the virtual machines need to have local storage and hence the storage need to be migrated along with virtual machine migration across physical hosts. The existing approaches of storage migration suffers from increased total migration time and reduced service performance. In this paper, we propose a five phase storage migration approach that will migrate virtual machines with storage across physical hosts. Our objective is to improve the service performance and reduce the total migration time during the storage migration of virtual machines. We have used the block bitmap to synchronize the virtual machine storage during migration. The synchronization time during the memory transfer is absent in our approach unlike previous approaches."
pub.1094651380,Energy Optimized VM Placement in Cloud Environment,"Large energy hungry datacenters are increasing at an alarming rate, making energy management as one of the key design constraints for the cloud datacenters. Energy consumption in cloud datacenters can be reduced by employing VM consolidation technique which is the process of packing VMs on the least number of servers and VM consolidation uses VM migration technique for this purpose. But unnecessary migrations may lead to performance degradation, service level agreement violations, service down time. Efficiency of system that employs virtualization greatly depends on the technology used to allocate VMs to the appropriate hosts. This gives significant importance to VM Placement Problem (VMPP). The aim of VM placement problem is to put the idle server to low-power consuming mode and to reduce the number of overall migrations in a system. In this paper, an algorithm based on MBFD has been proposed for VM placement. The goal is to reduce the number of active servers and to obtain a stable host for each VM such that the number of unnecessary migrations and total power consumption can be reduced. The performance of proposed algorithm is compared to that of an existing MBFD placement algorithm. The result shows that the proposed algorithm saves more energy than the method compared to."
pub.1094296664,Analysis of Data Interchange Formats for Interoperable and Efficient Data Communication in Clouds,"Efficient mechanisms for data structuring and formatting are indispensable for managing data traffic between and within federated Cloud environments to avoid excessive bandwidth cost and to ensure portability and interoperability. This facilitates provider-agnostic communication, which is essential for interoperable inter-Cloud deployments and portable integration of service components, both with one another and with the underlying Cloud platform. The existing data interchange formats for structuring and serialising data have not yet been analysed in the context of data communication in Clouds. Thus, to address this issue, the determination of an appropriate data interchange format for Clouds is necessary. In this paper, we present a performance analysis of some selected data interchange formats to assess their efficiency in terms of their usability in realising a common messaging format for communicating data in Clouds. We first describe the characteristics of each data format for clear understanding. As a basis for the analysis, we introduce a Cloud use case scenario comprising the transmission of monitored data as messages. The communication means is achieved with a novel message bus system designed to integrate the data interchange formats. We present some evaluations of the formats and compare their compactness for supporting efficient data transmission in Clouds."
pub.1095846100,Why Should Only Your Home be Smart? - A Vision for the Office of Tomorrow,"The digitization of the physical world and the daily life becomes more and more interesting for research and industry. The key technologies for the digitization can be found in the fields of Internet of Things (IoT) and Cloud Computing. One of the different settings for IoT containing a high potential to create value for users and industry is Smart Office. The existing applications comprise a large quantity of systems and services, but there are still application scenarios, which cannot be covered by the existing solutions. Examples for uncovered applications are shared office cleaning services or a flexible in-room coffee service. This paper provides definitions and differentiations in the fields of Smart Buildings, Smart Home and Smart Office, but its main contribution is a concept for the integration, the management and the control of service devices in a Smart Office environment as well as for the use of these devices through a network calendar such as MS Exchange and a voice service like Amazon Alexa."
pub.1147050110,A fuzzy‐based method for cloud service migration using a shark smell optimization algorithm,"Abstract Computation should develop and become more powerful and flexible as a result of the expansion of apps and the incorporation of novel consumers into the realm of computing systems. The potential of live virtual machine (VM) migration among various clouds is one of the growing study fields in cloud computing. It can be more important when the cloud service quality that a user is presently using deteriorates or when a new cloud service is launched that is superior in performance, quality, and pricing to the existing services. Because this problem is NP‐Hard in nature, this article proposes a method to migrate the load from an over‐loaded VMs to the active machine that is least loaded using a fuzzy‐based shark smell optimization algorithm. This algorithm is based on a shark's ability to discover prey as a strong hunter in nature, based on the shark's smell sense and motion to the odor source. The suggested optimization technique mathematically models diverse shark behaviors in the search environment, which is seawater. The CloudSim is utilized to illustrate the efficiency of the approach compared to others. The outcomes reveal that energy consumption and execution time are better than Genetic Algorithm and Particle Swarm Optimization algorithms."
pub.1140964160,Resource integrity-aware flexible resource scaling approach over sensor-cloud,"Massive internet of things (IoT) framework deployments increase edge devices usage and dependently increase the generation of data. The traditional elastic asset scheduling approach is phenomenally suitable to a single cloud environment. The prognosticative asset demand is not sufficient. The existing methods are neglecting billing mechanisms to scale up and down the asset scheduling actions. Consequently, we propose an adaptive workload prediction algorithm to schedule the resource and asset migration algorithm to accomplish low leased costs. The predictive model ensures assets scheduling at cluster-edge to reduce the latency. The migration algorithm regulates data reliability with moderate workload balancing. The simulation results exhibit an adaptive system performance such as leased cost curb, essential data integrity, and workload balancing."
pub.1093276300,A Ubiquitous Sensor Network for Domestic Energy Monitoring and Energy Consumption Optimization,"The current energy dilemma facing the world today expatriates the need for smart grid development, effective home power management & realizing automated smart load distribution systems. In this paper, we introduce you to “Slug” (Smart Plug) sensor network. The “Slug” system is designed for use in occupational and domestic environments as a device that is able to identify the domestic appliance based on the behavior of the current patterns consumed by the appliance. The “Slug” system acts as both a sensor network and the individual node in the network acts as a functioning power strip. The nodes communicate with each other using the existing Power Line architecture of the environment. The use of Power Lines Communication (PLC) makes the system a seamless part of the environment. The issues related to the excessive noise and potential loss of data packets in PLC is solved by using the flood transmission protocol. We believe that the seamless integration of sensor network in its environment is important for being successful in the realm of successful ubiquitous computing. In this paper we discuss the hardware and software architectures of the “Slug” system which look at certain test appliances and explain by what we mean by current consumption patterns and current profiles of appliances. We try our best to come with a possible hypothesis of this behavior and explain the use of classifier algorithm which needs zero training but only a dynamically upgradeable database, thus creating a need of a cloud database connected to all the home servers. The system implementations and description of protocols developed for the appliance controls will also be explained in the paper. Finally we present the need for Flood transmission protocols in PLC and an encryption scheme to deal with security threats in the system."
pub.1163971449,Hybrid Pelican and Archimedes optimization algorithm fostered energy aware task scheduling in heterogeneous virtualized cloud computing,"Abstract Energy reduction is a key issue of virtualized cloud computing (CC) schemes, because it provide significant benefits, like lower operating costs, improving system effectiveness, securing the environment. Energy‐efficient task scheduling is a feasible mode to attain these objectives. Nevertheless, it is difficult to match cloud resources to user requests in a way that improves performance while meeting a user‐defined deadline for energy consumption reduction. Therefore, hybrid Pelican and Archimedes optimization algorithm fostered energy‐aware task scheduling in heterogeneous virtualized cloud computing (TS‐HVCC‐Hyb‐POA‐AOA) is proposed in this article. A hybrid Pelican and Archimedes optimization algorithm (Hyb‐POA‐AOA) is used to lessen the task duration and the consumption of power in the cloud. For evaluation, the cloud environment uses the provided environment hybrid Pelican and Archimedes optimization algorithm for execute a few common workloads at the simulated data center. The performance metrics, like makespan right skewed analysis, makespan left skewed analysis, energy consumption right skewed analysis, energy consumption left skewed analysis, availability and resource utilization is considered. The performance of the proposed TS‐HVCC‐Hyb‐POA‐AOA method provides 23.69%, 29.50%, and 36.78% higher resource utilization; 38.23%, 31.35%, and 26.19% lower consumption of energy left skewed analysis and 34.52%, 30.28%, and 23.54% lower makespan for right skewed analysis compared with existing methods such as; task scheduling in heterogeneous cloud environment with gray wolf optimization algorithm (TS‐HVCC‐GWOA), hybrid whale optimization approach and differential evolution optimization for multiple objective virtual machine programming at cloud services (TS‐HVCC‐WOA), energy and performance planning in TasterkHeterek virtualized cloud computing using particle swarm optimization algorithm (TS‐HVCC‐PSOA)."
pub.1043679391,Introduction,"In recent years, the emergence and evolution of large scale parallel systems, grids, cloud computing environments, and multi-core architectures have prompted the performance community to push its boundaries. Whether system scale is achieved by coupling processors with a large number of cores that are tightly coupled or by massive numbers of loosely coupled processors, many systems will contain hundreds of thousands of processors on which millions of computation threads solve ever larger and more complex problems. At the same time, the coverage of the term ’performance’ has constantly broadened to include reliability, robustness, energy consumption, and scalability in addition to classical performance-oriented evaluations of system functionalities. In response to these two new sets of challenges, our community has the mission to develop a range of novel methodologies and tools for performance modeling, evaluation, prediction, measurement, benchmarking, and visualization of existing and emerging large scale parallel and distributed systems."
pub.1107643572,CBase: Fast Virtual Machine storage data migration with a new data center structure,"Live Virtual Machine (VM) migration within a data center is an important technology for cloud management, and has brought many benefits to both cloud providers and users. With the development of cloud computing, across-data-center VM migration is also desired. Normally, there is no shared storage system between data centers, hence the storage data (disk image) of a VM will be migrated to the destination data center as well. However, the slow network speed of the Internet and the comparatively large size of VM disk image make VM storage data migration become a bottleneck for live VM migration across data centers. In this paper, based on a detailed analysis of VM deployment models and the nature of VM image data, we design and implement a new migration system, called CBase. The key concept of CBase is a newly introduced central base image repository for reliable and efficient data sharing between VMs and data centers. With this central repository, further performance optimizations to VM storage data migration are made possible. Two migration mechanisms (data deduplication and Peer-to-Peer (P2P) file sharing) are utilized to accelerate base image migration, and a strategy is designed to elevate the synchronization of newly-written disk blocks. The results from an extensive experiment show that CBase significantly outperforms existing migration mechanisms under different conditions regarding total migration time and total network traffic. In particular, CBase with data deduplication is better than P2P file sharing for base image migration in our experimental environment."
pub.1122979532,Research Scope and Tools for Workflow Scheduling in Cloud Environment,"Workflow is a prototype that executes the behavior of scientific and engineering applications for which the sequence of tasks needs to be automated based on the input parameters specified. Difficulties arise for CSPs primarily during the execution environment due to its direct impact on various QoS parameters. Existing workflow scheduling techniques has research focus with dimensions that include undetermined demands, task failure,l delay cost, ambiguous deadlines, bandwidth, cache inclusion, scheduler policies, VM cycles, QoS impact, OS support, fault-tolerant and virtualization level. The comparative analysis made in this paper for workflow scheduling strategies and tools used in the cloud environment towards QoS parameters would have a greater impact on industry task automation which in turn provides the researchers to set their objectives and tools to be used in-order to bring forth new approaches and solutions. This paper can be extended considering the research scope given in the comparative analysis and the data size pertaining to each task in workflow and data migration issues which cannot be done without cost, legal policies and technical issues."
pub.1130538760,Streaming service provisioning in IoT‐based healthcare: An integrated edge‐cloud perspective,"Abstract Bio‐sensor data streaming and analytics is a key component of smart e‐healthcare. However, existing Internet of Things (IoT) ecosystem is unable to materialize the real‐time bio‐sensor data streaming and analytics within resource constrained environment. Moreover, traditional solutions fail to mitigate the edge‐cloud integration within a single sub‐system under IoT periphery which lead to investigate how edge‐cloud hybridization could be realized via similar set of tools. The objective of this article is to implement an integrated dual‐mode edge‐cloud system to serve streaming and analytics in real‐time. This study aims to achieve the aforesaid goal by presenting two different experiments that deals with the real‐time pulse sensor data streaming and analytics while utilizing light‐weight IoT‐supported JavaScript frameworks that includes Node.js, Johnny‐Five, Serialport.js, Plotly client, Flot.js, jQUERYy, Express Server, and Socket.io. Firstly, a standalone IoT‐edge system is developed and later, an integrated IoT‐based edge‐cloud system is developed to compare between the effectiveness of the systems. The implementation results show near correlation between the standalone edge and dual‐mode edge system. However, the dual‐mode edge‐cloud system provides more flexibility and capability to counter the bio‐sensor data streaming and analytics services within the constrained framework."
pub.1095080492,Load-Based Controlling Scheme of Virtual Machine Migration,"Playing a key role in system online maintenance and upgrade, dynamic resource management, fault tolerance and power management, virtual machine migration has attracted significant attention in recent years. This paper proposes a load-based controlling scheme of virtual machine migration to achieve a better system performance than existing methods in cloud computing environment. It concludes three parts, migration triggering module judging whether the migration process should be activated, host machine selecting module deciding which machine in the physical host should be migrated, destination host selecting module determining which network node is selected for migration destination. Simulations show that the proposed scheme efficiently avoids unnecessary migration, and reduces the amount of data transferred, as well as improves the system performance of load balancing across physical system."
pub.1101770972,Secure Assignment Strategy of Virtual Machines Based on Operating System Diversity,"The co-resident attack across virtual machines is a serious security challenge to the resource sharing mechanism in cloud computing. The existing defensive solutions, mainly aiming at improving the isolation and dynamics of virtual machines, require modification of the monitor or frequent migration of virtual machines, which are hard to be deployed in large scale scenarios. Considering the strong dependence of co-resident attacks on the type of operating system, a diversified assignment strategy of operating systems for virtual machines is proposed to mitigate the attack by increasing the cost of attackers. Specifically, a diversified operating system mapping model is proposed, and an interactive workflow process is established between the user and the cloud service provider. The simulation results show that the proposed method can reduce the attack efficiency by at least 33.46%, which provides more security guarantee for the cloud environment when compared with the assignment strategy of single operating system."
pub.1128199667,milliEgo: Single-chip mmWave Radar Aided Egomotion Estimation via Deep Sensor Fusion,"Robust and accurate trajectory estimation of mobile agents such as people and
robots is a key requirement for providing spatial awareness for emerging
capabilities such as augmented reality or autonomous interaction. Although
currently dominated by optical techniques e.g., visual-inertial odometry, these
suffer from challenges with scene illumination or featureless surfaces. As an
alternative, we propose milliEgo, a novel deep-learning approach to robust
egomotion estimation which exploits the capabilities of low-cost mmWave radar.
Although mmWave radar has a fundamental advantage over monocular cameras of
being metric i.e., providing absolute scale or depth, current single chip
solutions have limited and sparse imaging resolution, making existing
point-cloud registration techniques brittle. We propose a new architecture that
is optimized for solving this challenging pose transformation problem.
Secondly, to robustly fuse mmWave pose estimates with additional sensors, e.g.
inertial or visual sensors we introduce a mixed attention approach to deep
fusion. Through extensive experiments, we demonstrate our proposed system is
able to achieve 1.3% 3D error drift and generalizes well to unseen
environments. We also show that the neural architecture can be made highly
efficient and suitable for real-time embedded applications."
pub.1132889898,milliEgo,"Robust and accurate trajectory estimation of mobile agents such as people and robots is a key requirement for providing spatial awareness for emerging capabilities such as augmented reality or autonomous interaction. Although currently dominated by optical techniques e.g., visual-inertial odometry these suffer from challenges with scene illumination or featureless surfaces. As an alternative, we propose milliEgo, a novel deep-learning approach to robust egomotion estimation which exploits the capabilities of low-cost mm Wave radar. Although mmWave radar has a fundamental advantage over monocular cameras of being metric i.e., providing absolute scale or depth, current single chip solutions have limited and sparse imaging resolution, making existing point-cloud registration techniques brittle. We propose a new architecture that is optimized for solving this challenging pose transformation problem. Secondly, to robustly fuse mmWave pose estimates with additional sensors, e.g. inertial or visual sensors we introduce a mixed attention approach to deep fusion. Through extensive experiments, we demonstrate our proposed system is able to achieve 1.3% 3D error drift and generalizes well to unseen environments. We also show that the neural architecture can be made highly efficient and suitable for real-time embedded applications."
pub.1124071263,"Connected Vehicles in the Internet of Things, Concepts, Technologies and Frameworks for the IoV","This book presents an overview of the latest smart transportation systems, IoV connectivity frameworks, issues of security and safety in VANETs, future developments in the IoV, technical solutions to address key challenges, and other related topics. A connected vehicle is a vehicle equipped with Internet access and wireless LAN, which allows the sharing of data through various devices, inside as well as outside the vehicle. The ad-hoc network of such vehicles, often referred to as VANET or the Internet of vehicles (IoV), is an application of IoT technology, and may be regarded as an integration of three types of networks: inter-vehicle, intra-vehicle, and vehicular mobile networks. VANET involves several varieties of vehicle connectivity mechanisms, including vehicle-to-infrastructure (V2I), vehicle-to-vehicle (V2V), vehicle-to-cloud (V2C), and vehicle-to-everything (V2X). According to one survey, it is expected that there will be approximately 380 million connected carson the roads by 2020. IoV is an important aspect of the new vision for smart transportation. The book is divided into three parts: examining the evolution of IoV (basic concepts, principles, technologies, and architectures), connectivity of vehicles in the IoT (protocols, frameworks, and methodologies), connected vehicle environments and advanced topics in VANETs (security and safety issues, autonomous operations, machine learning, sensor technology, and AI). By providing scientific contributions and workable suggestions from researchers and practitioners in the areas of IoT, IoV, and security, this valuable reference aims to extend the body of existing knowledge."
pub.1166297807,Secure data access using blockchain technology through IoT cloud and fabric environment,"Abstract In the current landscape, staying abreast of the latest technological advancements is a formidable challenge, especially given the deluge of data inundating the internet. The realization has dawned that effectively managing the surge in emerging data necessitates the integration of multiple technologies. In pursuit of this objective, the Internet of Things (IoT), renowned for its sensor‐based data capture capabilities, is frequently coupled with blockchain technology to ensure secure data storage and access. This amalgamation, in turn, leverages the cloud environment when data volume surpasses a machine's processing capacity, thus mitigating infrastructure and maintenance costs. This study endeavors to optimize data storage within the blocks of the blockchain (BCT) by storing an index that points to the actual data. This innovative approach not only conserves storage space but also enhances operational efficiency. Furthermore, it simplifies the task of identifying malicious or faulty nodes deployed at different locations for data capture within the prescribed time frame. To exemplify this implementation, a case study is presented, focusing on securing user votes through the creation of contracts. The results showcased underscore the preference for a permissioned blockchain, such as Fabric, over a permissionless one, like Ethereum, particularly in the context of security considerations. The findings reveal that as the number of operations (in this case, votes cast) increases, Ethereum's performance deteriorates, while Fabric exhibits exceptional robustness. Additionally, the study analyzes sensor data simulated via IoT nodes before and after the application of security algorithms to underscore the significance of the proposed Secure Cloud‐Based Blockchain (SCB2) model. The analysis encompasses various facets, including the creation, validation, and computation times of transactions and blocks within a node, and positions the model favorably in comparison to existing literature."
pub.1155471164,Implementing Open-Source Information Systems for Assessing and Managing the Seismic Vulnerability of Historical Constructions,"The characterisation of the seismic vulnerability of historical constructions represents a complex problem in which the typological variability, the difficulty of performing reliable large-scale assessments and dealing with a large database all play a role. Nevertheless, reducing the uncertainty regarding the structural vulnerability of the existing building stock (mostly for small and/or isolated human settlements) is key for risk assessment and management. The present work proposes a novel approach based on the integration of a series of open-source tools for assembling a vulnerability-oriented database that is linked to a series of external services for increasing its capabilities. The database was implemented in a Geographical Information System (GIS) environment and contains the survey of a seismic vulnerability index for masonry constructions based on an adapted version of the GNDT-II approach. A customised Python-based software for reading, managing and editing the database is herein presented. This program allows the execution of the most typical operations with no assistance from the GIS environment, facilitating user interaction. Furthermore, the calculations regarding the vulnerability index and levels of damage have been implemented in this program. Alternatives for distributing the database are implemented and discussed, such as cloud-based distribution and the use of the Transactional Web Feature Service (WFS-T) protocol for its virtual publishing. The entire framework herein presented is a replicable and feasible workflow that can be set even with reduced infrastructure, allowing a progressive enlargement."
pub.1129673721,Intrusion Detection Based on Dynamic Gemini Population DE-K-mediods Clustering on Hadoop Platform,"In view of the fact that the existing intrusion detection system (IDS) based on clustering algorithm cannot adapt to the large-scale growth of system logs, a K-mediods clustering intrusion detection algorithm based on differential evolution suitable for cloud computing environment is proposed. First, the differential evolution algorithm is combined with the K-mediods clustering algorithm in order to use the powerful global search capability of the differential evolution algorithm to improve the convergence efficiency of large-scale data sample clustering. Second, in order to further improve the optimization ability of clustering, a dynamic Gemini population scheme was adopted to improve the differential evolution algorithm, thereby maintaining the diversity of the population while improving the problem of being easily trapped into a local optimum. Finally, in the intrusion detection processing of big data, the optimized clustering algorithm is designed in parallel under the Hadoop Map Reduce framework. Simulation experiments were performed in the open source cloud computing framework Hadoop cluster environment. Experimental results show that the overall detection effect of the proposed algorithm is significantly better than the existing intrusion detection algorithms."
pub.1169343316,Advancing Enterprise Data Strategies: Integration and Optimization of SAP HANA for Backup and Disaster Recovery,"This research paper delves into the integration and optimization of SAP HANA, in enterprise data centers with a focus on strategies to ensure backup and disaster recovery (DR). SAP HANA, which is an in memory database brings about a transformation in how data's stored and processed. It offers real time analytics, predictive modeling and advanced analytics capabilities. The paper discusses the utilization of SAP HANA in both on premise and cloud environments while highlighting the need to strike a balance between speed, scalability and challenges such as data security and system integration. Emphasizing the importance of backup and DR protocols it underscores their role in protecting data integrity and ensuring uninterrupted business operations even during high risk situations. Through exploration of methods and technologies for backup and recovery this study aims to enhance efficiency in handling data within the realm of SAP HANA. By shedding light on implications related to deployment of SAP HANA – regarding data security measures and operational resilience – this research contributes to the ongoing discourse surrounding, in memory databases within enterprise settings."
pub.1130743999,Towards computer-network architectures for the digital transformation of large-scale systems,"На основе математического обобщения классической модели универсального компьютера Дж. фон Неймана в статье предложен общий подход к устранению причин воспроизводства разнородности аппаратных, программных и информационных ресурсов в глобальной компьютерной среде (ГКС). Обобщённая модель позволяет бесшовно расширять свойство универсальной программируемости с внутрикомпьютерных ресурсов на сколь угодно большие сети. При этом кибербезопасность кардинального повышается за счёт аппаратного воплощения ""тяжёлых"" системных функций, программно выполняемых в операционных системах.
                  Анализ тенденций развития больших распределённых систем показал, что существующие технологии функциональной интеграции разнородных ресурсов ГКС (Grid, Cloud, пиринговые сети) приблизились к пределам своих возможностей увеличения масштабов таких систем. Дальнейшее увеличение их размеров требует неприемлемых затрат на преодоление крайней разнородности и обеспечение кибербезопасности.
                  Причины разнородности ГКС фундаментальны. Они скрыты в логике фоннеймановских оснований микропроцессорных архитектур. Сетевые протоколы TCP/IP в глобальных масштабах легализуют разнородность, а также эвристические методы интеграции разнородных ресурсов.
                  Первоначальные принципы формирования ГКС не предназначались для системно-целостного решения проблем создания сколь угодно больших распределённых систем в ГКС. Предложенная модель открывает возможности для воплощения в ГКС математически-однородного, универсального, бесшовно программируемого и кибербезопасного алгоритмического пространства распределённых вычислений. С устранением разнородности на уровне массовых приложений кардинально снижаются затраты на создание/развитие всего разнообразия сколь угодно больших распределённых систем.
Based on the mathematical generalization of the classical model of the universal computer by J. von Neumann, the article proposes a new approach to eliminating the causes of reproduction of heterogeneity of hardware, software, and information resources in the global computer environment (GCE). The generalized model allows seamlessly extending the property of universal programmability from internal computers’ resources to arbitrarily large networks. The hardware implementation of ""heavy"" system functions performed programmatically within operating systems cardinally increase cybersecurity. Analysis of trends in the development of large distributed systems has shown that existing technologies for functional integration of heterogeneous GCE resources (Grid, Cloud, peer-to-peer networks) have come close to the limits of their ability to increase the scale of such systems. Further increase in their size requires unacceptable costs to overcome extreme heterogeneity and ensure cybersecurity. The reasons for the heterogeneity of GCE are fundamental. They are hidden in the logic of the von Neumann bases of microprocessor architectures. TCP/IP network protocols legalize heterogeneit"
pub.1153796354,Multi-Client Boolean File Retrieval With Adaptable Authorization Switching for Secure Cloud Search Services,"Secure cloud search services provide a cost-effective way for resource-constrained clients to search encrypted files in the cloud, where data owners can customize search authorization. Despite providing fine-grained authorization, traditional attribute-based keyword search (ABKS) solutions generally support single keyword search. Towards expressive queries over encrypted data, multi-client searchable symmetric encryption (MC-SSE) was introduced. However, current search authorizations of existing MC-SSEs: (i) cannot support dynamic updating; (ii) are (semi-)black-box implementations of attribute-based encryption; (iii) incur significant cost during system initialization and file encryption. To address these limitations, we present AasBirch, an MC-SSE system with fast fine-grained authorization that supports adaptable authorization switching from one policy to any other one. AasBirch achieves constant-size storage and lightweight time cost for system initialization, file encryption and file searching. We conduct extensive experiments based on Enron dataset in real cloud environment. Compared to state-of-the-art MC-SSE with fine-grained authorization, AasBirch achieves 30$\sim 200\times$∼200× smaller public parameter and secret key size, with the assumed least frequent keyword in a query ($s$s-term) as 21. Moreover, it runs 10$\sim 20\times$∼20× faster for file encryption and $>20\times$>20× faster for file searching. In addition, AasBirch outperforms 80,000× (resp. 7,850×) faster with $s$s-term=1 (resp. =21), as compared to classic dynamic ABKS system."
pub.1155749874,European Weather Cloud: A community cloud tailored for big Earth modelling and EO data processing,"The European Centre for Medium-Range Weather Forecasts (ECMWF) together with the European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT) have worked together to offer to their Member States a new paradigm to access and consume weather data and services. The “European Weather Cloud-(EWC)” (https://www.europeanweather.cloud/), concluded its pilot phase and is expected to become operational during the first months of 2023.This initiative aims to offer a community cloud infrastructure on which Member and Co‐operating States of both organizations can create on demand virtual compute (including GPUs) and storage resources to gain easy and high throughput access to the ECMWF’s Numerical Weather Predication (NWP) and EUMETSAT’s satellite data in a timely and configurable fashion. Moreover, one of the main goals is to involve more National Meteorological Services to jointly form a federation of clouds/data offered from their Member States, for the maximum benefit of the European Meteorological Infrastructure (EMI). During the pilot phase of the project, both organizations have jointly hosted user and technical workshops to actively engage with the meteorological community and align the evolution of the EWC to reflect and satisfy their operational goals and needs.The EWC, in its pilot phase hosted several use cases, mostly aimed at users in the developers’ own organisations. These broad categories of these cases are:Web services to explore hosted datasets
Data processing applications
Platforms to support the training of machine learning models on archive datasets
Workshops and training courses (e.g., ICON model training, ECMWF training etc)
Research in collaboration with external partners
World Meteorological Organization (WMO) support with pilots and PoC.
Some examples of the use cases currently developed at the EWC are:The German weather service DWD, which is already feeding maps generated by a server it deployed on the cloud into its public GeoPortal service.
EUMETSAT and ECMWF joint use case assesses bias correction schemes for the assimilation of radiance data based on several satellite data time series
the Royal Netherlands Meteorological Institute (KNMI) hosts a climate explorer web application based on KNMI climate explorer data and ECMWF weather and climate reanalyses
The Royal Meteorological Institute of Belgium prepares ECMWF forecast data for use in a local atmospheric dispersion model.
NordSat, a collaboration of northern European countries which is developing and testing imagery generation tools in preparation for the Meteosat Third Generation (MTG) satellite products.
UK Met Office with the DataProximateCompute use case, which distributes compute workload close to data, with the automatic creation and disposal of Dask clusters, as well as the data plane VPN network, on demand and in heterogeneous cloud environments.
In this presentation, the status of the project, the offered services and how these are accessed by "
pub.1135619934,A Novel Classified Ledger Framework for Data Flow Protection in AIoT Networks,"The edge computing node plays an important role in the evolution of the artificial intelligence-empowered Internet of things (AIoTs) that converge sensing, communication, and computing to enhance wireless ubiquitous connectivity, data acquisition, and analysis capabilities. With full connectivity, the issue of data security in the new cloud-edge-terminal network hierarchy of AIoTs comes to the fore, for which blockchain technology is considered as a potential solution. Nevertheless, existing schemes cannot be applied to the resource-constrained and heterogeneous IoTs. In this paper, we consider the blockchain design for the AIoTs and propose a novel classified ledger framework based on lightweight blockchain (CLF-LB) that separates and stores data rights at the source and enables a thorough data flow protection in the open and heterogeneous network environment of AIoT. In particular, CLF-LB divides the network into five functional layers for optimal adaptation to AIoTs applications, wherein an intelligent collaboration mechanism is also proposed to enhance the across-layer operation. Unlike traditional full-function blockchain models, our framework includes novel technical modules, such as block regenesis, iterative reinforcement of proof-of-work, and efficient chain uploading via the system-on-chip system, which are carefully designed to fit the cloud-edge-terminal hierarchy in AIoTs networks. Comprehensive experimental results are provided to validate the advantages of the proposed CLF-LB, showing its potentials to address the secrecy issues of data storage and sharing in AIoTs networks."
pub.1142509765,Design and implementation of endogenous security container based on union file system,"In order to solve the increasing attacks on container file system and the IO errors of containers in big data processing scenarios in cloud computing environment, a scheme based on the idea of heterogeneous redundancy in endogenous security and transformation of container union file system was proposed to improve the security and fault tolerance of containers. Based on the above scheme, experiments are carried out on Docker, the most popular container technology, and OverlayFS, the most representative union file system. The experimental results show that this scheme can improve the security and fault tolerance of containers on the premise of ensuring availability, and realize the endogenous security of containers."
pub.1027630084,Design and Usage of a Process-Centric Collaboration Methodology for Virtual Organizations in Hybrid Environments,"<p>This article describes a collaboration methodology for virtual organizations where the processes can be automatically executed using a hybrid web service, grid or cloud resources. Typically, the process of deriving executable workflows from process models is cumbersome and can be automated only in part or specific to a particular distributed system. The approach introduced in this paper, exemplified by the construction industry field, integrates existing technology within a process-centric framework. The solution on the basis of a hybrid system architecture in conjunction with semantic methods for consistency saving and the framework for modeling VO processes and their automated transformation and execution are discussed in detail.</p>"
pub.1094077401,Adaptive Dynamic Priority Scheduling for Virtual Desktop Infrastructures,"Virtual Desktop Infrastructures (VDIs) are gaining popularity in cloud computing by allowing companies to deploy their office environments in a virtualized setting instead of relying on physical desktop machines. Consolidating many users into a VDI environment can significantly lower IT management expenses and enables new features such as “available-anywhere” desktops. However, barriers to broad adoption include the slow performance of virtualized I/OCPU scheduling interference problems, and shared-cache contention. In this paper, we propose a new soft real-time scheduling algorithm that employs flexible priority designations (via utility functions) and automated scheduler class detection (via hypervisor monitoring of user behavior) to provide a higher quality user experience. We have implemented our scheduler within the Xen virtualization platform, and demonstrate that the overheads incurred from colocating large numbers of virtual machines can be reduced from 66% with existing schedulers to under 2% in our system. We evaluate the benefits and overheads of using a smaller scheduling time quantum in a VDI setting, and show that the average overhead time per scheduler call is on the same order as the existing SEDF and Credit schedulers."
pub.1133283747,An optimal SLA based task scheduling aid of hybrid fuzzy TOPSIS-PSO algorithm in cloud environment,"Cloud computing is an important technology for bulk data storage, resource mobilization and online access to computer services. In the cloud computing platform there are plenty of resources, but the foremost challenge is to allocate tasks. The vital issue in scheduling is how the entire task could be allocated with maximized resource utilization to a resultant virtual machine, existing scheduling algorithms mainly focused on minimization of the task execution time while ignoring the SLA (Service Level Agreement) and QoS (Quality of Service) assurance. This paper thrives to accomplish the total task with eminent resource utilization, minimum migration cost, and minimum utilization of energy. For attaining these objectives, the proposed method utilizes the hybrid algorithm such as Fuzzy-TOPSIS and particle swarm optimization (PSO) approach. Initially the available task and the no. of VMs (virtual machines) are optimized by PSO algorithm. In the cloud, the multi-objective SLA based task scheduling problem is solved by the Fuzzy TOPSIS which uses the weighted sum of energy, cost and execution time as an objective function. Fuzzy TOPSIS method is work positively for many applications and impacts liberally on real world decision making concerns. The proposed approach is investigated based on the distinct evaluation metrics. Our proposed algorithm outperforms the existing approaches and achieves better results in terms of QoS parameters. The implementation had done using JAVA with CloudSim simulator."
pub.1137147747,Predicting LiDAR Data From Sonar Images,"Sensors using ultrasonic sound have proven to provide accurate 3D perception in difficult environments where other modalities fail. Several industrial sectors need accurate and reliable sensing in these harsh conditions. The conventional LiDAR/camera approach in many state-of-the-art autonomous navigation methods is limited to environments with optimal sensing conditions for visual modalities. The use of other sensing modalities can thus improve reliability and usability and increase the application potential of autonomous agents. Ultrasonic measurements provide, compared to LiDAR, a much sparser representation of the environment, making a direct replacement of the LiDAR sensor difficult. In this work, we propose a method to predict LiDAR point cloud data from an in-air acoustic sonar sensor using a convolutional stacked autoencoder. This provides a robotic system with high-resolution measurements and allows for easier integration into existing systems to safely navigate environments where visual modalities become unreliable and less accurate. A video of our predictions is available at https://youtu.be/jlx1S-tslmo."
pub.1132544643,Scheduling of Time Constrained Workflows in Mobile Edge Computing,"Mobile edge computing is an augmentation of cloud computing, and helps to reduce latency and network traffic. It has become a promising solution for real-time or data-intensive mobile applications. A large amount of mobile applications, such as smart city application, are workflow application. Therefore, workflow scheduling in edge computing environment become one of the key issues in the management of workflow execution. We need to allocate suitable edge resources to workflow task so that the workflow task can be completed within the time constraint specified by end user. We will address this issue in this paper. We formulate the time constrained workflow scheduling problem in mobile edge computing as an integer programming. A workflow scheduling algorithm for mobile edge computing is derived by extending Differential Evolution Algorithm. We conduct simulation experiments by comparing our algorithm with existing algorithms. The results show the effectiveness of our algorithm."
pub.1036892268,PRACTICAL IMPLEMENTATION OF SEMI-AUTOMATED AS-BUILT BIM CREATION FOR COMPLEX INDOOR ENVIRONMENTS,"Abstract. In recent days, for efficient management and operation of existing buildings, the importance of as-built BIM is emphasized in AEC/FM domain. However, fully automated as-built BIM creation is a tough issue since newly-constructed buildings are becoming more complex. To manage this problem, our research group has developed a semi-automated approach, focusing on productive 3D as-built BIM creation for complex indoor environments. In order to test its feasibility for a variety of complex indoor environments, we applied the developed approach to model the ‘Charlotte stairs’ in Lotte World Mall, Korea. The approach includes 4 main phases: data acquisition, data pre-processing, geometric drawing, and as-built BIM creation. In the data acquisition phase, due to its complex structure, we moved the scanner location several times to obtain the entire point clouds of the test site. After which, data pre-processing phase entailing point-cloud registration, noise removal, and coordinate transformation was followed. The 3D geometric drawing was created using the RANSAC-based plane detection and boundary tracing methods. Finally, in order to create a semantically-rich BIM, the geometric drawing was imported into the commercial BIM software. The final as-built BIM confirmed that the feasibility of the proposed approach in the complex indoor environment."
pub.1130148366,Simulation of Continental Shallow Cumulus Populations Using an Observation‐Constrained Cloud‐System Resolving Model,"Abstract Continental shallow cumulus (ShCu) clouds observed on 30 August 2016 during the Holistic Interactions of Shallow Clouds, Aerosols, and Land‐Ecosystems (HI‐SCALE) field campaign are simulated by using an observation‐constrained cloud‐system resolving model. On this day, ShCu forms over Oklahoma and southern Kansas and some of these clouds transition to deeper, precipitating convection during the afternoon. We apply a four‐dimensional ensemble‐variational (4DEnVar) hybrid technique in the Community Gridpoint Statistical Interpolation (GSI) system to assimilate operational data sets and unique boundary layer measurements including a Raman lidar, radar wind profilers, radiosondes, and surface stations collected by the U.S. Department of Energy's (DOE) Atmospheric Radiation Measurement (ARM) Southern Great Plains (SGP) atmospheric observatory into the Weather Research and Forecasting (WRF) model to ascertain how improved environmental conditions can influence forecasts of ShCu populations and the transition to deeper convection. Independent observations from aircraft, satellite, as well as ARM's remote sensors are used to evaluate model performance in different aspects. Several model experiments are conducted to identify the impact of data assimilation (DA) on the prediction of clouds evolution. The analyses indicate that ShCu populations are more accurately reproduced after DA in terms of cloud initiation time and cloud base height, which can be attributed to an improved representation of the ambient meteorological conditions and the convective boundary layer. Extending the assimilation to 18 UTC (local noon) also improved the simulation of shallow‐to‐deep transitions of convective clouds.
Plain Language Summary Accurate prediction of life cycle of shallow convective clouds is very challenging for the existing weather and climate models since they have difficulties in reproducing realistic atmospheric structure within a shallow layer near the Earth surface (also called boundary layer, roughly below 2‐km height in daytime). To tackle this fundamental problem, the observational data collected for operational weather prediction as well as unique boundary layer observations measured near north‐central Oklahoma are integrated to constrain the behavior of cloud‐system resolving model across different scales with an emphasis on boundary layer. The results show that the model biases in atmospheric conditions, especially humidity within boundary layer, are reduced with the modification informed by observations. As a consequence, shallow convective clouds are well reproduced in terms of cloud evolution in time and space which are verified by various cloud measurements. It also suggests that surface observation can be used to correct cold pool intensity which is closely related to the maintenance of deep convective clouds that are transitioned from shallow convective clouds.
Key Points    Observations are utilized to constrain cloud‐system resolving mo"
pub.1113703513,Smart Process Communication for Small and Medium-Sized Enterprises,"The transformation process towards new industrial paradigms in the course of I4.0 places new demands on interoperable communication. Entities as part of cyber-physical production systems are required to interact autonomously with their environment. This can constitute a challenge for small- and medium-sized companies, since adapting existing manufacturing structures to the needs of I4.0 often involves major obstacles due to the high degree of innovation. Thus, the objective of this paper is to suggest approaches for the application of smart communication solutions to connect entities within the fully connected value network and show how components and be classified upon two criteria in order to determine whether these fulfil requirements on I4.0 applications. Especially, it provides an overview of the most forward-looking communication protocols that allow the share of information from the field level up to cloud applications and across company boundaries."
pub.1145466158,Enhance Robotic Process Automation Using a Rule-Based Approach,"Robotic Process Automation (RPA) is an important research area in the revolution of industrial 4.0 throughout the world. RPA technology can increase manufacturing productivity and reduce cost in the long term. One of the most important facts that exist in existing RPA systems are they are platform dependent. It must be used in a general-purpose operating system environment and cannot be applied to other software systems such as embedded software or BIOS. RPA then captures a screenshot of the desktop graphical user interface display, analyzes the image, determines the next steps or process and then prompts the user to act as keyboard and mouse. This proposed an RPA solution with the intelligence to offload the high processing task via edge or cloud, based on a set of rules. The ability to decide when to offload tasks is the key factor to increase the adoption rate of the industry. This process will be done based on a set of rules. This type of programable rules-based RPA system is essential to provide a wide range of industry needs. The prototype RPA operated on a rules-based approach has demonstrated that it is possible to apply it to different operating industries.The proposed rule-based RPA algorithm shows the RPA had the intelligence to route th task to the most suitable RPA processing system."
pub.1071809977,"Embedded System and Robotic Education in a Blended Learning Environment Utilizing Remote and Virtual Labs in the Cloud, Accompanied by ‘Robotic HomeLab Kit’","It is impossible to imagine everyday life without embedded devices and robotic applications, as they are utilized in almost every nowadays technical product. And there is a frantic need of well-educated developers, designers and programmers to handle and further evolve this existing technology. The domain itself is in a big change because the borders of pure ICT and embedded system are fusing and according to this process new methods for teaching these disciplines are necessary. It is important that ICT education will become more and more to real systems education, instead of just computer software programming, but in most curricula these two domains are still separated. The paper addresses a novel and implemented solution for teaching and learning of Robotics and embedded systems, while setting in remote labs and modern Internet technology into overall learning process. The proposed concept builds the bridge for a simple and logical study process by utilizing ICT for controlling and understanding real word processes and situations. The introduced blended learning concept covers several educational levels, starting from first and second level education up to university education and life-long learning. The solution is covered with hands-on mobile hardware kits, collaborative e-tools and remote labs. The focus in this paper is on the integration of the overall concept and an evaluation of the given courses."
pub.1113579172,Power and Resource-Aware VM Placement in Cloud Environment,"Cloud computing provides various services to the cloud consumers based on demand and pay per use basis. To improve the system performance (such as energy efficiency, resource utilization (RU), etc.) more than one virtual machine (VM) can be deployed on a server. Efficient VM placement policy increases the system performance by utilizing all the computing resources at their maximum threshold limit and reduce the probability to become a server overloaded/underloaded. Overloaded/underloaded servers consume more energy and increase the number of VM migration in comparison to the server which is in a normal state. In this paper, Energy and Resource-Aware VM Placement (ERAP) algorithm is presented. This algorithm considers both, energy as well as central processing unit (CPU) utilization to deploy the VMs on the servers. CloudSim toolkit is used to analyze the behavior of the ERAP algorithm. The effectiveness of the ERAP algorithm is tested on real workload traces of Planet Lab. Results show that ERAP algorithm performs better in comparison to the existing algorithm on the account of the number of VM migrations, total energy consumption, number of servers shutdowns, and average service level agreement (SLA) violation rate. Results show that on average 13.12% energy consumption is minimized in contrast to the existing algorithm."
pub.1125114284,"Hyper-local, efficient extreme heat projection and analysis using machine learning to augment a hybrid dynamical-statistical downscaling technique"," This paper describes a scalable system for quantifying hyper-local heat stress in urban environments and its expected response within the changing climate. A hybrid dynamical-statistical downscaling approach links Global Climate Models (GCMs) with dynamically downscaled extreme heat events using the Weather Research and Forecasting model (WRF). Downscaled historical simulations in WRF incorporate urban canopy physics to better describe localized land surface details in the urban environment relevant to extreme heat. This downscaled library is then used in an analog-based approach. This contribution reports a series of enhancements to existing analog-based methods which can efficiently produce more detailed results. The system here uses advanced statistical methods and simple machine learning (ML) techniques to optimize analog selection, perform spatially-consistent bias correction, and decompose patterns of extreme heat into dynamic components such as the land-sea contrast and inland sea-breeze penetration. Hindcast projections are validated against observational data from in-situ weather observing stations. The results demonstrate the scalability and efficiency of this system as it is deployed in cloud-based architectures with parallelized code. Downscaled predictions are equally applicable to heat stress at weather and climate time scales, supporting infrastructure resilience and adaptation, and emergency response."
pub.1153942818,Digital Transformation for Automated Test Systems,"Automatic Test Equipment (ATE) have been key in the laboratory testing, calibration, and maintenance of Unit Under Test (UUT) and Line Replaceable Components (LRC), maintaining a high standard in the sustainment of complex systems. In a wide market full of unique capabilities, spanning from legacy systems to cutting-edge technologies, it only makes sense to evolve the ATE world adopting a modular architecture from inception to deployment. A modular mentality applied to every aspect of a system design, shifts away from a rigid perspective and towards a flexible environment where customer ideas and goals can thrive. From utilizing AGILE strategies in execution of design and documentation, taking “vertical slices” on complex designs, utilizing Model-Based Systems Engineering to derive system requirements, using collaborative, cloud-based tools to maximize productivity and shorten program-execution schedules, developing full kit-based solutions, to the virtualization of the ATE design, test and manufacturing plans; All in support of a product that offers confidence during design and execution, maximizes future upgradeability, obsolescence management and field-sustainment, while minimizing non-recurring costs experienced on a traditional, fixed, waterfall ATE design environment. This paper will describe how Digital Transformation in a modular test system makes this possible and enables customers to achieve and maintain success for years to come."
pub.1035492140,A Strategy for Adopting Server Virtualization in the Public Sector: NIPA Computer Center,"Many public organizations have been adopting and operating various servers. These servers run on Windows, Unix, and Linux operating systems that generally use less than 10% of their capacity. For migrating a public organization to cloud computing, we must first virtualize the server environment. This article proposes a strategy for server virtualization that the National IT Industry Promotion Agency (NIPA) has done and describes the effects of a public organization migrating to cloud computing. The NIPA Computer Center planned an effective virtualization migration on various servers. This project of virtualization migration was conducted with the existing policy of separate x86 servers and Unix servers. There are three popular approaches to server virtualization: a virtual machine model, a paravirtual machine model, and virtualization at the operating system layer. We selected a VMware solution that uses the virtual machine model. We selected servers for virtualization in the following manner. Servers were chosen that had the highest rate of service usage and CPU usage and had been operating for five years or more. However, we excluded servers that require 80% or greater rates of CPU usage. After adopting the server virtualization technique, we consolidated 32 servers into 3 servers. Virtualization is a technology that can provide benefits in these areas: server consolidation and optimization, infrastructure cost reduction and improved operational flexibility, and implementation of a dual computing environment."
pub.1146572145,Synergy of Digital Universality of the Global Computer Environment,"The global computer environment (GCE) as a whole is considered an object of research. The system-wide as­pects of implementing the digital transformation of the sociotechnosphere through the GCE are analyzed. Factors of the destructive influence of the fundamental system-technical laws of the spontaneous growth of the GCE on the functioning/development of the sociotechnosphere are presented. Special attention is paid to the study of the features of the apperance of a positive synergetic network effect, expressed by Metcalfs law, in the conditions of the initially open intra-system heterogeneity of the network resources of the existing GCE. It is established that with the increase in the size of large distributed systems implemented in the GCE, the factor of heterogeneity of network resources becomes a source of negative (system-destructive) network synergy, which devalues the positive synergy of Metcalfs law. Examples of ecosystems implemented in the GCE through cloud technologies are considered. It is shown that in the conditions of initially open heterogeneity of network resources, the further increase in the size of large distributed systems leads to an insurmountable increase in the complexity of the system-functional integra­tion of network resources and an uncontrolled decrease in the stability of the sociotechnosphere. The principles of conceptual reengineering the GCE are presented to eliminate the causes of systemic heterogeneity and seamless/ cybersecurity distribution of algorithmic universality, closed in intracomputer resources, to any arbitrarily large subset of GCE computers. Such reengineering will allow neutralizing the negative synergy and maximizing the positive synergy of the Metcalf effect in large distributed systems without restrictions on their size."
pub.1017673424,A review of Internet of Things for smart home: Challenges and solutions,"Although Internet of Things (IoT) brings significant advantages over traditional communication technologies for smart grid and smart home applications, these implementations are still very rare. Relying on a comprehensive literature review, this paper aims to contribute towards narrowing the gap between the existing state-of-the-art smart home applications and the prospect of their integration into an IoT enabled environment. We propose a holistic framework which incorporates different components from IoT architectures/frameworks proposed in the literature, in order to efficiently integrate smart home objects in a cloud-centric IoT based solution. We identify a smart home management model for the proposed framework and the main tasks that should be performed at each level. We additionally discuss practical design challenges with emphasis on data processing, as well as smart home communication protocols and their interoperability. We believe that the holistic framework ascertained in this paper can be used as a solid base for the future developers of Internet of Things based smart home solutions."
pub.1120301123,Factors Influencing Fog Computing Adoption Based on Quality of Results (QoR) for Heterogeneous Data Analysis: A Proposed Framework,"The rapid increase of data generated has brought challenges on data quality level. Fog computing in general has been supporting the requirements of end user devices that could not be met by cloud computing solution and it is acknowledged to have a major impact on how an organisation decides to adopt for preprocessing a huge amount of data being generated by the devices. Since IoT devices generating very heterogeneous and dynamic data, there are challenges for the level of data quality. The limitation has hindered the development of fog systems framework that capable operating the dynamic execution of edge devices that handling generation and collection large amounts of data on-premise and off-premise. Thus,sufficient operations of identifying Quality of Result enable user to detect any problems when conducting the decision making. The aim of this paper is to address the factors that perceived likely to influence the adoption of fog computing in evaluating the data analysis on data transmitted from the ever increases devices.A conceptual framework has been constructed considering attributes such as heterogeneous data analysis (on-premise and off-premise) and Quality of Results (quality indicators, quality control, validity outcome and reliability outcome).Potential benefits from the implementation of this framework to organisation is it enable to provide greater value and benefits to the business process. The framework of this study could also be influencing and inhibiting the adoption of fog computing.Quality of result has higher chances to satisfy the defined industrial’s requirement. In addition, fog-computing adoption is important for serving an environment for industry to execute, monitor, and analyze a large form of data in a fog landscape."
pub.1129691204,Architecting Digital Products and Services,"Enterprises are currently transforming their strategy, processes, and their information systems to extend their degree of digitalization. The potential of the Internet and related digital technologies, like Internet of Things, services computing, cloud computing, artificial intelligence, big data with analytics, mobile systems, collaboration networks, and cyber physical systems both drives and enables new business designs. Digitalization deeply disrupts existing businesses, technologies and economies and fosters the architecture of digital environments with many rather small and distributed structures. This has a strong impact for new value producing opportunities and architecting digital services and products guiding their design through exploiting a Service-Dominant Logic. The main result of the book chapter extends methods for integral digital strategies with value-oriented models for digital products and services which are defined in the framework of a multi-perspective digital enterprise architecture reference model."
pub.1121433565,Meta-Heuristic and Non-Meta-Heuristic Energy-Efficient Load Balancing Algorithms in Cloud Computing,"The number of users of cloud computing services is drastically increasing, thereby increasing the size of data centers across the globe. In virtue of it, the consumption of power and energy is a major concern for system designers and developers. Their goal is now to develop power and energy-efficient products at the same time maintaining the quality and cost of products and services. For managing the power and efficiency, several aspects are taken into consideration in cloud computing paradigm. Load balancing, task scheduling, task migration, resource allocation are some of the techniques, which need to be efficiently employed to minimize the energy consumption. This chapter represents the detailed survey of the existing solutions and approaches for energy-efficient load balancing algorithms used in cloud environments. The research challenges as well as future research directions are also discussed in this chapter."
pub.1148629602,WSN Security,"The Influence of computing technology is so dominant that it is affecting and capturing every aspect of human life and providing more and more convenience by automation. Development of automatic systems in every arena reduces the problems of unavailability of the services and disparity in providing the services; it also decreases the time complexity, space complexity, need for human intervention, and many more. Every convenience comes at a cost, and the cost is exposure to cyber-physical threats, so security becomes the most prominent issue, which prevails whether it is in terms of the physical or digital environment. Connectivity to the internet creates an inward–outward connection for a system, which opens the door for the adversary. In terms of wireless sensor networks (WSNs), the enormous production of data from sensors and smart and intelligent radio nodes and the integration of it with cloud computing is putting the data at stake, creating privacy issue. Protecting a system from intruders to maintain confidentiality, integrity and availability (CIA), should be the utmost priority in an intrusion detection system (IDS), which plays a key role in the security infrastructure. An IDS monitors network traffic and reports unusual activity and prevents attacks. This chapter deals with how the security can be compromised using various intrusion techniques and how that could be prevented with using machine learning in a wireless sensor network. The Influence of computing technology is so dominant that it is affecting and capturing every aspect of human life and providing more and more convenience by automation. This chapter deals with how the security can be compromised using various intrusion techniques and how that could be prevented with using machine learning in a wireless sensor network. The intrusion detection system works on pattern analysis of network and resource usage. It may be based on the combination of hardware and software. In an insider attack the attacker is within the premises, such as when a dissatisfied or malicious employee or attacker successfully breaches physical security. Vulnerability refers to a system's existing weakness caused by physical, environmental, economic, and social factors. Machine learning is a branch of artificial intelligence, which provides the capability to the machine to acquire knowledge from learning the system and environment without the need of hardcode program knowledge."
pub.1095716070,A System for Supporting Migration to Overlay OpenFlow Network Using OpenStack,"OpenFlow network technologies has attracted attention along with popularization of cloud environment and server virtualization since it enables flexible network configuration. It is expected that existing networks of many organizations will migrate to OpenFlow networks in the future. Since it takes a huge cost to replace all existing network devices to OpenFlow switches, many organizations consider to employ overlay OpenFlow networks where virtual OpenFlow devices are introduced at endpoint servers. In overlay OpenFlow networks, the cost for migration increases according to the number of servers. In this paper, we propose a system for supporting migration to an overlay OpenFlow network. Our proposed system acquires settings of servers in the existing network endpoints and reflects them to the overlay OpenFlow network. Through experimental evaluations, we confirm the behavior of the configuration acquisition function of our proposed system."
pub.1148121547,Intelligent home temperature and light control system based on the cloud platform,"Compared with traditional home appliances, the high cost of purchase and maintenance of intelligent home appliances is becoming the fatal defect of the popularization and development of intelligent home appliances. In this paper, Intelligent Control System based on Alibaba Cloud platform is designed for traditional home appliances. The master control MCU chip HaaS1000 is equipped with dual-core Cortex-M33, combined with the temperature and humidity sensor to perceive the home environment. The infrared module installed with XK2233 chip and air conditioning coding library is used to control the air conditioning. The relay is used to control the lighting and the motor is used to control the curtain in the home. The system accesses to Internet via WI-FI and communicates with the cloud platform using MQTT protocol. Data is transmitted in JSON format between the MCU and the cloud platform. Users can browse the home environment data and control the working state of smart home through APP. The experimental results show that the system can run stably, which is a low-cost solution to the intelligent transformation of traditional home, and to some extent can solve the problem of high replacement price of existing smart home appliances."
pub.1153683623,"M³LVI: a multi-feature, multi-metric, multi-loop, LiDAR-visual-inertial odometry via smoothing and mapping","
                    Purpose
                    
                      The purpose of this paper is to propose a multi-feature, multi-metric and multi-loop tightly coupled LiDAR-visual-inertial odometry, M
                      3
                      LVI, for high-accuracy and robust state estimation and mapping.
                    
                  
                  
                    Design/methodology/approach
                    
                      M
                      3
                      LVI is built atop a factor graph and composed of two subsystems, a LiDAR-inertial system (LIS) and a visual-inertial system (VIS). LIS implements multi-feature extraction on point cloud, and then multi-metric transformation estimation is implemented to realize LiDAR odometry. LiDAR-enhanced images and IMU pre-integration have been used in VIS to realize visual odometry, providing a reliable initial guess for LIS matching module. Location recognition is performed by a dual loop module combined with Bag of Words and LiDAR-Iris to correct accumulated drift. M³LVI also functions properly when one of the subsystems failed, which greatly increases the robustness in degraded environments.
                    
                  
                  
                    Findings
                    
                      Quantitative experiments were conducted on the KITTI data set and the campus data set to evaluate the M
                      3
                      LVI. The experimental results show the algorithm has higher pose estimation accuracy than existing methods.
                    
                  
                  
                    Practical implications
                    The proposed method can greatly improve the positioning and mapping accuracy of AGV, and has an important impact on AGV material distribution, which is one of the most important applications of industrial robots.
                  
                  
                    Originality/value
                    
                      M
                      3
                      LVI divides the original point cloud into six types, and uses multi-metric transformation estimation to estimate the state of robot and adopts factor graph optimization model to optimize the state estimation, which improves the accuracy of pose estimation. When one subsystem fails, the other system can complete the positioning work independently, which greatly increases the robustness in degraded environments.
                    
                  "
pub.1095651089,Enhancing Access-Control with Risk-Metrics for Collaboration on Social Cloud-Platforms,"Cloud computing promotes the exchange of information, resources and tasks between different organizations by facilitating the deployment and adoption of centralized collaboration platforms: Professional Social Networking (PSN). However, issues concerning security management are preventing their widespread use, as organizations still need to protect some of their sensitive data. Traditional access control policies, defined over the triplet (User, Action, Resource) are difficult to put in place in such highly dynamic environments. In this paper, we introduce risk metrics in existing access control systems to combine the fine-grained policies defined at the user level, with a global risk-policy defined at the organization's level. Experiments show the impact of our approach when deployed on traditional systems."
pub.1144639232,Environmental and Safety Impacts of Vehicle-to-Everything Enabled Applications: A Review of State-of-the-Art Studies,"With the rapid development of communication technology, connected vehicles (CV) have the potential, through the sharing of data, to enhance vehicle safety and reduce vehicle energy consumption and emissions. Numerous research efforts have been conducted to quantify the impacts of CV applications, assuming instant and accurate communication among vehicles, devices, pedestrians, infrastructure, the network, the cloud, and the grid, collectively known as V2X (vehicle-to-everything). The use of cellular vehicle-to-everything (C-V2X), to share data is emerging as an efficient means to achieve this objective. C-V2X releases 14 and 15 utilize the 4G LTE technology and release 16 utilizes the new 5G new radio (NR) technology. C-V2X can function without network infrastructure coverage and has a better communication range, improved latency, and greater data rates compared to older technologies. Such highly efficient interchange of information among all participating parts in a CV environment will not only provide timely data to enhance the capacity of the transportation system but can also be used to develop applications that enhance vehicle safety and minimize negative environmental impacts. However, before the full benefits of CV can be achieved, there is a need to thoroughly investigate the effectiveness, strengths, and weaknesses of different CV applications, the communication protocols, the varied results with different CV market penetration rates (MPRs), the interaction of CVs and human driven vehicles, the integration of multiple applications, and the errors and latencies associated with data communication. This paper reviews existing literature on the environmental, mobility and safety impacts of CV applications, identifies the gaps in our current research of CVs and recommends future research directions. The results of this paper will help shape the future research direction for CV applications to realize their full potential benefits."
pub.1092884568,DISINTEGRATION OF LARGE BALLS OF WATER-BASED LIQUIDS IN FREE FALL THROUGH HIGH-TEMPERATURE GASES,"This paper presents an experimental study of the transformation (i.e., deformation and disintegration) of liquid balls in free fall through a gaseous environment at temperatures from 300 to 1100 K. The experiments involve the use of the following liquids: water, NaCl solution, and a suspension with carbon particles; the initial volume of liquid balls ranges from 0.05 l to 1 l. We apply high-speed video cameras, Phantom V411 and Phantom Miro M310 (up to 6 &times; 10<sup>5</sup> frames per second), and Tema Automotive software, providing the ability to track objects continuously. The study establishes the mechanism of deformation and disintegration of liquid balls. The following factors influence characteristics of these processes: the initial volume of liquid balls, their component composition, the velocity of the side airflow (wind), and the temperature of gases (i.e., air and combustion products). In addition, the Weber numbers are determined for the transitions between the transformation stages of the liquid balls. The determination of Weber numbers is necessary for illustrating the limiting conditions at which the breakup of liquid balls and the transition from one stage of deformation to another occur. The research illustrates the limitations of an increase in transverse and longitudinal sizes of liquid balls during deformation under conditions corresponding to extinguishing fires. The results substantiate the necessity of spraying water for covering a larger fire area. In the case of the local water discharge, batches of liquids transform to clouds consisting of droplets with small transverse sizes even after falling several dozens of meters. Combustion products heated up to high temperatures essentially decelerate a transition from the monolithic ball of water to the droplet cloud and limit its transverse size. This limitation is associated with the intensive evaporation and the formation of buffer vapor zones between combustion products and the droplet cloud."
pub.1094988711,Secured Geospatial Data Storage and Retrieval Using Spatial Hadoop Framework in Cloud Environment,"Due to recent advancement in satellite remote sensing technology, the volume of remote sensing image data grows exponentially, while the processing capability of existing computer system is hard to satisfy the requirements of remote sensing image data accessing. Technological advancements of Geospatial Information Systems (GIS) and cloud did the process in a simpler and acceptable way to share the fact and figures that makes complicated and time consuming decision making with more assurance level. Later on, increase in an IT reflected the storage, management, integration and correlation of huge data which has an impact in the efficiency of the operations growth rate. It is more important to secure the geospatial data which is stored in the public cloud to ensure who is accessing the data to reduce risks to information, national security, or perhaps for other reasons. This research proposes an efficient way of storing the geospatial data in public cloud using Spatial Hadoop mechanism in cloud computing environment. The Spatial Hadoop-GIS is a scalable and high performance spatial data warehousing system for running large scale spatial queries on Spatial Hadoop. It will support multiple types of spatial queries on MapReduce through spatial partitioning, customizable spatial query engine. However, the associations are hesitant to store their touchy data on the cloud because of different protection and character following dangers. In the previous couple of years, a great deal of innovative work endeavors has been considered to list out the features and security for unified nodes for the development of makeup of data in cloud environment. At the outset, the model need to be refined with a similarity as the main concern. The article shows verification and approval protocol that pictorize the primary components of mysterious correspondence to be used for the cloud."
pub.1134952122,Leveraging Existing Business Processes for Digital Transformation in Manufacturing Industry,"Digital transformation is the process of undergoing multiple internal and external changes by an organization leveraging digital technologies to become more agile, optimized and efficient in its business operations. Dynamic IT environment, the urge to leverage emerging technologies, and the unpreparedness to function at scale in throes of the global pandemic are driving increased spend on cloud-based applications and further pushing the case for digital transformation. IT and business goals alignment to channelize resources in creation of digital products and experiences which can drive growth in current markets has become the norm driving manufacturing companies to focus on transformation, optimizing operations, enhancing customer-centricity, managing risk, finding new use cases to deploy digital technology and in turn increase revenues. This article provides a deep dive into what is ‘Digital Transformation’ and how can various existing business processes be leveraged to explore opportunities that can help drive organizational growth in manufacturing industry."
pub.1144400718,Random Forest Algorithm in Grass-Roots Management Platform of West China,"Big data promotes a new round of technological change. It collects massive data and analyzes the value of obtaining information based on the Internet of things and cloud computing, which is the premise of effective analysis and scientific decision-making for decision makers. As a new way to update management concepts, social governance models and improve the governance capacity of the government, big data can promote the modernization of national governance system. In the era of big data, this paper mainly discusses how to promote the innovation of grass-roots governance from the aspects of rights operation, network construction and legal system construction in the traditional grass-roots governance system. At the same time, it uses the thinking of big data governance to innovate the grass-roots governance mode, governance structure, governance ability, governance process and governance mode, and further optimize the innovation environment of grass-roots governance, we will continue to optimize and innovate grass-roots governance, promote the deep integration of new big data technologies and grass-roots social governance, and explore and improve the innovation of government governance capacity."
pub.1170758999,Scalable MDC-Based Volumetric Video Delivery for Real-Time One-to-Many WebRTC Conferencing,"The production and consumption of video content has become a staple in the current day and age. With the rise of virtual reality (VR), users are now looking for immersive, interactive experiences which combine the classic video applications, such as conferencing or digital concerts, with newer technologies. By going beyond 2D video into a 360 degree experience the first step was made. However, a 360 degree video offers only rotational movement, making interaction with the environment difficult. Fully immersive 3D content formats, such as light fields and volumetric video, aspire to go further by enabling six degrees-of-freedom (6DoF), allowing both rotational and positional freedom. Nevertheless, the adoption of immersive video capturing and rendering methods has been hindered by their substantial bandwidth and computational requirements, rendering them in most cases impractical for low latency applications. Several efforts have been made to alleviate these problems by introducing specialized compression algorithms and by utilizing existing 2D adaptation methods to adapt the quality based on the user's available bandwidth. However, even though these methods improve the quality of experience (QoE) and bandwidth limitations, they still suffer from high latency which makes real-time interaction unfeasible. To address this issue, we present a novel, open source [32], one-to-many streaming architecture using point cloud-based volumetric video. To reduce the bandwidth requirements, we utilize the Draco codec to compress the point clouds before they are transmitted using WebRTC which ensures low latency, enabling the streaming of real-time 6DoF interactive volumetric video. Content is adapted by employing a multiple description coding (MDC) strategy which combines sampled point cloud descriptions based on the estimated bandwidth returned by the Google congestion control (GCC) algorithm. MDC encoding scales more easily to a larger number of users compared to performing individual encoding. Our proposed solution achieves similar real-time latency for both three and nine clients (163 ms and 166 ms), which is 9% and 19% lower compared to individual encoding. The MDC-based approach, using three workers, achieves similar visual quality compared to a per client encoding solution, using five worker threads, and increased quality when the number of clients is greater than 20."
pub.1168126964,1 Integrated schematic design method for shear wall structures: a practical application of generative adversarial networks,"The intelligent design method based on generative adversarial networks (GANs) represents an emerging structural design paradigm where design rules are not artificially defined but are directly learned from existing design data. GAN-based methods have exhibited promising potential compared to conventional methods in the schematic design phase of reinforced concrete (RC) shear wall structures. However, for the following reasons, it is challenging to apply GAN-based approaches in the industry and to integrate them into the structural design process: (1) The data form of GAN-based methods is heterogeneous from that of the widely used computer-aided design (CAD) methods, and (2) GAN-based methods have high requirements on the hardware and software environment of the user's computer. As a result, this study proposes an integrated schematic design method for RC shear wall structures, providing a workable GAN application strategy. Specifically, (1) a preprocessing method of architectural CAD drawings is proposed to connect the GAN with the upstream architectural design; (2) a user-friendly cloud design platform is built to reduce the requirements of the user's local computer environment; and (3) a heterogeneous data transformation method and a parametric modeling procedure are proposed to automatically establish a structural analysis model based on GAN's design, facilitating downstream detailed design tasks. The proposed method makes it possible for the entire schematic design phase of RC shear wall structures to be intelligent and automated. A case study reveals that the proposed method has a heterogeneous data transformation accuracy of 97.3% and is capable of generating shear wall layout designs similar to the designs of a competent engineer, with 225 times higher efficiency."
pub.1174841306,Self-localization method combining laser devices and image sensors in non-GNSS environments,"In the promotion of Information and Communication Technology (ICT) in construction projects (i-Construction), as well as in the digitization of existing infrastructure (DX; Digital Transformation,) many improved distance measurement technologies enhance the efficient development and management of societal infrastructure. Infrastructure maintenance and management incorporate such technologies as small unmanned aerial vehicles (drones) and 3D laser scanners. Recently, the utilization of Mobile Mapping Systems (MMS) equipped with laser scanners has facilitated the acquisition of three-dimensional data in areas surrounding roads. MMS proves effective in measuring the three-dimensional shape of roadways as it acquires cloud point data. The data, represented by three-dimensional coordinates (x, y, z) aligned with the world geodetic system, can be seamlessly integrated into cyberspace on a computer. MMS utilizes a laser beam from a scanner installed on the vehicle, resulting in a measurement error of a few centimeters in the acquired cloud point data. However, the accuracy of the cloud point data significantly decreases in tunnels due to the absence of satellite positioning correction, leading to potential errors exceeding 1 meter. Presently, long distance [1 m to 30 m] measurement commonly relies on the Doppler effect. However, notable issues are the requirement for a stationary state and several seconds of measurement time to ensure accuracy. Hence, when mounted on a moving vehicle, poor quality data would be obtained. Similarly, the use of beacons or radar yields unreliable measured values for the vehicle movement. Even with autonomous navigation systems employing a gyro sensor and a speed sensor mounted on the vehicle to determine the point of travel, the cumulative error in position estimation gradually increases, particularly in locations where catching GPS satellites is challenging. Some essential sites on and around roads, such as tunnels and areas under elevated railway tracks, remain inaccessible for position information through GNSS (Global Navigation Satellite System,) limiting the full utilization of ICT technology. This study proposes a novel self-positioning system that integrates two laser devices and an image sensor to accurately acquire position information even in non-GNSS environments."
pub.1083924770,When ERM Met Alma: The Intricacies of Content Management in a Shared Consortia Landscape,"In 2013, after nearly two decades of operating in a distributed legacy Integrated Library System (ILS) environment on local servers, the Orbis Cascade Alliance, a consortium of public and private academic libraries in Washington, Oregon, and Idaho, began a two-year-long process to migrate its 37 members to a shared implementation of Ex Libris's cloud-based Alma library management system (LMS) and Primo discovery interface. Although much has been written on electronic resource management (ERM) functionality at an institution level, little has been written on serials and ERM functionality and workflows within a shared consortial environment. This article discusses the challenges and opportunities of implementing a consortial-based LMS, with particular emphasis on serials and ERM functionality. Key migration issues related to serials control, acquisitions, licensing, administration, cataloging, statistics, and interoperability are examined at the institutional and large-scale networked levels. Benefits and limitations of using a shared consortial cloud-based LMS are explored, and the overall capabilities of the Alma LMS for electronic resource management are reviewed."
pub.1157699970,"Transitioning to Microsoft Power Platform, An Excel User Guide to Building Integrated Cloud Applications in Power BI, Power Apps, and Power Automate","Welcome to this step-by-step guide for Excel users, data analysts, and finance specialists. It is designed to take you through practical report and development scenarios, including both the approach and the technical challenges. This book will equip you with an understanding of the overall Power Platform use case for addressing common business challenges. While Power BI continues to be an excellent tool of choice in the BI space, Power Platform is the real game changer. Using an integrated architecture, a small team of citizen developers can build solutions for all kinds of business problems. For small businesses, Power Platform can be used to build bespoke CRM, Finance, and Warehouse management tools. For large businesses, it can be used to build an integration point for existing systems to simplify reporting, operation, and approval processes. The author has drawn on his 15 years of hands-on analytics experience to help you pivot from the traditional Excel-basedreporting environment. By using different business scenarios, this book provides you with clear reasons why a skill is important before you start to dive into the scenarios. You will use a fast prototyping approach to continue to build exciting reporting, automation, and application solutions and improve them while you acquire new skill sets. The book helps you get started quickly with Power BI. It covers data visualization, collaboration, and governance practices. You will learn about the most practical SQL challenges. And you will learn how to build applications in PowerApps and Power Automate. The book ends with an integrated solution framework that can be adapted to solve a wide range of complex business problems. What You Will Learn Develop reporting solutions and business applications Understand the Power Platform licensing and development environment Apply Data ETL and modeling in Power BI Use Data Storytelling and dashboard design to better visualize data Carry out data operations with SQL and SharePoint lists Develop useful applications using Power Apps Develop automated workflows using Power Automate Integrate solutions with Power BI, Power Apps, and Power Automate to build enterprise solutions"
pub.1166640937,From Lab to Field: Real-World Evaluation of an AI-Driven Smart Video Solution to Enhance Community Safety,"This article adopts and evaluates an AI-enabled Smart Video Solution (SVS)
designed to enhance safety in the real world. The system integrates with
existing infrastructure camera networks, leveraging recent advancements in AI
for easy adoption. Prioritizing privacy and ethical standards, pose based data
is used for downstream AI tasks such as anomaly detection. Cloud-based
infrastructure and mobile app are deployed, enabling real-time alerts within
communities. The SVS employs innovative data representation and visualization
techniques, such as the Occupancy Indicator, Statistical Anomaly Detection,
Bird's Eye View, and Heatmaps, to understand pedestrian behaviors and enhance
public safety. Evaluation of the SVS demonstrates its capacity to convert
complex computer vision outputs into actionable insights for stakeholders,
community partners, law enforcement, urban planners, and social scientists.
This article presents a comprehensive real-world deployment and evaluation of
the SVS, implemented in a community college environment across 16 cameras. The
system integrates AI-driven visual processing, supported by statistical
analysis, database management, cloud communication, and user notifications.
Additionally, the article evaluates the end-to-end latency from the moment an
AI algorithm detects anomalous behavior in real-time at the camera level to the
time stakeholders receive a notification. The results demonstrate the system's
robustness, effectively managing 16 CCTV cameras with a consistent throughput
of 16.5 frames per second (FPS) over a 21-hour period and an average end-to-end
latency of 26.76 seconds between anomaly detection and alert issuance."
pub.1102351776,"Regulating the Cloud: More, Less or Different Regulation and Competing Agendas","Cloud computing challenges existing regulatory paradigms in a variety of ways. This paper, which differentiates among cloud services, services hosted on cloud platforms and ‘cloud-enhanced’ services delivered with the aid of cloud-based storage, processing and other functions, seeks to identify and analyse challenges to existing regulation arising from the spread of cloud and cloud-hosted services and areas where new regulatory intervention may be necessary (especially to meet the competition and consumer protection obligations of telecommunications regulators). The paper further tries to distinguish between problems specific to the cloud (e.g. the IPR issues associated with content-matching for cloud-hosted content) and those that are simply more noticeable or less tractable there (e.g. issues of data location). The first part of the paper develops a framework for analysing these issues, taking into account i) cloud features (e.g. service, contractual and deployment models, B2B vs. B2C offerings); ii) specific mechanisms for creating or preventing citizen, consumer and competitive harm; and iii) regulatory mechanisms or relations (ex ante/ex post, rule- or principle-based, competition vs. utility). The regulatory issues can be divided among: a) bypass (where regulated activity escapes regulation by going ‘via the cloud’); b) direct regulation (of cloud services); and c) indirect regulation (e.g. regulation of cloud-hosted services or use of telecom or direct cloud regulation to address issues arising in cloud-hosted or ‘cloud-enhanced’ services. The analysis identifies (for European telecom regulators) whether the issues require an extension of the existing mandate or duties and whether they are likely to be transitional (solved by market developments) or amenable to self- or co-regulation. This framework is populated by an inventory of issues and recommendations regarding such specific cases as customer mobility, data location and migration and the telecom regulatory implications of communications-as-a-service. The inventory of policy issues is divided between regulatory concerns arising from the statutory duties of telecoms regulators and implementation issues arising from the distributed and internationally peripatetic nature of the cloud. However, it raises more questions than it answers; while Section 4.2 indicates some areas where existing justifications for regulatory intervention can be applied to the cloud and Section 4.3 discusses how regulation may or may not work, it is useful to consider whether distributed and delocalised computing as a service calls for a reconsideration of the regulatory heritage of communications and/or broadcast content distribution. To partially address, this, the final section provides a prolegomena to an alternative theoretical analysis. It concentrates on economic regulation of cloud services, specifically cloud platform competition and neutrality. To the extent that computational services complement many f"
pub.1175917817,A robust correspondence-based registration method for large-scale outdoor point cloud,"Large-scale outdoor point cloud registration is essential for the 3D reconstruction of outdoor scenes. Its central objective is to achieve accurate point cloud registration by determining accurate spatial transformation parameters. While feature-based methods eliminate the need for initial position estimation, they encounter challenges in handling high outlier rates. Therefore, a method capable of effectively managing outliers is crucial for enhancing the efficiency and accuracy of large-scale outdoor point cloud registration. This paper introduces the maximal clique with adaptive voting (MCAV) method, which leverages graph-based inlier compatibility to optimize potential matches. MCAV employs adaptive parameter voting (APV) to enhance computational efficiency, demonstrating significant speedup characteristics in datasets with a significant number of inliers. To further reduce outliers in potential matches, we integrate Black-Rangarajan Duality (BRD) and graduated non-convexity (GNC) into the truncated least squares (TLS) framework (BG-TLS). Accordingly, we propose the efficient BG-TLS (EBG-TLS) method for computing the registration model. Comparative analyses with traditional and deep learning-based methods across various real-world environments demonstrate that the proposed method outperforms existing algorithms in terms of rotation error, translation error, and efficiency, particularly in complex, high-noise settings. This method finds broad applications in geospatial mapping and surveying, autonomous navigation, and environmental monitoring."
pub.1094925797,Elimination Based Fault Localization in Shared Resource Environments,"Fault Localization is the process to identify the component(s) that is the exact source of failure given a set of observed failure indications. Despite being a focus of research for a long time, fault localization is still deemed to be a challenge due to the complexity of current distributed environment. Growing adoption of cloud computing wherein multiple applications share multiple resources increases the complexity of the problem. Existing probing techniques are found to be inefficient due to large number of applications and resources. Availability and utilization of such shared resource environment trigger the need for finding other novel techniques to fault localization. In this paper, we present an elimination-based fault localization method that leverages shared resources among applications. Shared resources are used as ‘Readily Available Probes’ to find the real-time state of applications. These probes are used to eliminate non-faulty resources leaving minimal subset of resources that are likely to the faulty components. We show this method significantly reduces the effort required to design and implement probes. Various experiments demonstrate that our method reduces time taken and increases efficiency and accuracy of problem determination."
pub.1150473488,Integrated Schematic Design Method for Shear Wall Structures: A Practical Application of Generative Adversarial Networks,"The intelligent design method based on generative adversarial networks (GANs) represents an emerging structural design paradigm where design rules are not artificially defined but are directly learned from existing design data. GAN-based methods have exhibited promising potential compared to conventional methods in the schematic design phase of reinforced concrete (RC) shear wall structures. However, for the following reasons, it is challenging to apply GAN-based approaches in the industry and to integrate them into the structural design process. (1) The data form of GAN-based methods is heterogeneous from that of the widely used computer-aided design (CAD) methods, and (2) GAN-based methods have high requirements on the hardware and software environment of the user’s computer. As a result, this study proposes an integrated schematic design method for RC shear wall structures, providing a workable GAN application strategy. Specifically, (1) a preprocessing method of architectural CAD drawings is proposed to connect the GAN with the upstream architectural design; (2) a user-friendly cloud design platform is built to reduce the requirements of the user’s local computer environment; and (3) a heterogeneous data transformation method and a parametric modeling procedure are proposed to automatically establish a structural analysis model based on GAN’s design, facilitating downstream detailed design tasks. The proposed method makes it possible for the entire schematic design phase of RC shear wall structures to be intelligent and automated. A case study reveals that the proposed method has a heterogeneous data transformation accuracy of 97.3% and is capable of generating shear wall layout designs similar to the designs of a competent engineer, with 225 times higher efficiency."
pub.1181867329,KubeAegis: A Unified Security Policy Management Framework for Containerized Environments,"Recently, containers have become the standard for cloud-native service delivery, ensuring scalability and reliability. However, they are also prime targets for various security attacks that exploit vulnerabilities. In particular, deploying security policies in dynamic cloud-native environments presents significant challenges, such as misconfigurations arising from the heterogeneity of different security policies. Despite numerous attempts to address these challenges, existing solutions often lack a unified framework for consistently managing and enforcing heterogeneous security policies across network, system, and cluster layers. Current approaches typically focus on isolated aspects of security rather than providing a comprehensive policy management solution. This fragmentation leads to inconsistencies, inefficiencies, and potential security gaps. To address these challenges, in this paper, we propose KubeAegis, an advanced and unified policy management framework designed to manage the integration, verification, and enforcement of heterogeneous security policies at the network, system, and cluster levels. Our framework enables centralized management of security policies, simplifying the integration of new security tools through an adapter-based approach and API recommendation mechanisms. We also incorporate a pre-validation process to detect potential misconfigurations before policy enforcement and to enable real-time tracking of policies applied to containers. Our evaluation demonstrates the effectiveness of KubeAegis in integrating and managing network, system, and cluster security policies in real cloud-native environments, providing extensive coverage and achieving a minimal translation delay of approximately 17ms."
pub.1094584618,Reducing service failures by failure and workload aware load balancing in SaaS clouds,"SLA violations are typically viewed as service failures. If service fails once, it will fail again unless remedial action is taken. In a virtualized environment, a common remedial action is to restart or reboot a virtual machine (VM). In this paper we present, a VM live-migration policy that is aware of SLA threshold violations of workload response time, physical machine (PM) and VM utilization as well as availability violations at the PM and VM. In the migration policy we take into account PM failures and VM (software) failures as well as workload features such as burstiness (coefficient of variation or CoV >1) which calls for caution during the selection of target PM when migrating these workloads. The proposed policy also considers migration of a VM when the utilization of the physical machine hosting the VM approaches its utilization threshold. We propose an algorithm that detects proactive triggers for remedial action, selects a VM (for migration) and also suggests a possible target PM. We show the efficacy of our proposed approach by plotting the decrease in the number of SLA violations in a system using our approach over existing approaches that do not trigger migration in response to non-availability related SLA violations, via discrete event simulation of a relevant case study."
pub.1153664161,An Energy-Efficient Dynamic Scheduling Method of Deadline-Constrained Workflows in a Cloud Environment,"With the rapid development of cloud applications, the computing requests of cloud data centers have increased significantly, consuming a lot of energy, making cloud data centers unsustainable, which is very unfavorable from both the cloud provider’s point of view and the environmental point of view. Therefore, it is crucial to minimize energy consumption and improve resource utilization while ensuring user service quality constraints. In this paper, we propose a hybrid workflow scheduling algorithm (Online Hybrid Dynamic Scheduling, OHDS), which aims to minimize the energy consumption of tasks and maximize service resource utilization while satisfying the sub-deadline and data dependency constraints of workflow tasks. Firstly, the data dependencies between workflow tasks are considered for multi-task merging, and sub-deadline constraints are assigned to workflow tasks based on task priority. Secondly, based on the independent nature of the tasks of different workflows, a hybrid scheduling of multiple workflows is performed to reduce service idle time. Then, the workflow task scheduling priority and its sub-deadlines are dynamically adjusted, and the service status is sensed by the CPU utilization of the service, and the workload on the overloaded/underloaded service is balanced by dynamic migration of virtual machines. Finally, the OHDS method is compared with three existing scheduling methods to verify its better performance in terms of scheduling energy consumption, scheduling success rate and service resource utilization."
pub.1150379068,GIS application in environmental monitoring and risk assessment,"GIS techniques are becoming the mainstream tool in different disciplines such as assessments of biomass resources, mineral resource analysis, groundwater, and air quality investigation. This study explored its application in solving environmental problems monitoring and risk assessment. GIS application in environmental monitoring has mainly been divided into three aspects: water, soil, and atmosphere. Based on ArcGIS 10.1 software and ArcGIS 9.3.1 version, GIS has been applied in water supply system monitoring and soil heavy metal concentration monitoring, respectively. In addition, it can achieve real-time geographic location information transmission accurately and monitor in various fields by combining the Alibaba Cloud elastic computing service server, user management development environment, real-time data display, wireless sensor network, Arduino microcontroller, and a series of sensors. Combined with the Radial Basis Function Network model and spatial data, GIS technology could monitor and assess the degree of soil wind erosion hazard by quantifying the various indicators of soil wind erosion. GIS can also be applied to assess the environmental risks from water, land, and atmosphere. Based on geological and geomorphological data, the integration of remote sensing and GIS can complete the assessment of flash flood disasters, groundwater exploration, and groundwater pollution. By using the spatial analysis and data processing capabilities of GIS and combined with other technologies or methods such as digital elevation model and Pollution Index, topographic changes, soil properties, and heavy metal pollution can be assessed. GIS provides the ability to query spatial data and translates existing spatial patterns into measurable targets with its built-in analytical tools. It can be used to predict and assess air quality prediction. Through these approaches, relevant authorities can manage different areas rationally and targeted manner, which has practical and long-term implications."
pub.1131962893,Integration of recurrent convolutional neural network and optimal encryption scheme for intrusion detection with secure data storage in the cloud,"Abstract Data communication security is growing day after day with the proliferation of cloud computing. It is primarily because of the few security constraints and challenges occurring in the cloud environment during data transmission. Existing research has shown that the intrusion detection system (IDS) centered on the cloud is more complicated. In this article, we address the above issues by proposing an attention‐based recurrent convolutional neural network (RCNN). This proposed RCNN is used to detect whether the text data are intrusion or nonintrusion. The nonintrusion text information is then used for further processing and encrypted using a two‐way encryption scheme. We introduce the elliptical curve cryptography (ECC) approach to increase the security‐level performance of nonintrusion data. Moreover, the integration of ECC with the modified flower pollination algorithm (MFP‐ECC) creates the two‐way encryption scheme, and it is used to produce an optimal private key. The encrypted data are then stored in a cloud environment by steganography and the data with the sensitive information are replaced by some other text, thus providing security to the data at rest. The proposed MFP‐ECC approach shows maximum breaking time results and can also withstand different classical attacks when compared with other methods. As a result, the proposed intrusion detection and secure data storage mechanism is highly secured and it is never affected by any kinds of conspiracy attacks."
pub.1145900840,Review of existing variants of Grey Wolf Optimization algorithm handling Load Balancing in Clouds,"Cloud computing has earned lot of awarenes in the Information Technology and now appeared as the next level in the evolution of Internet. But now it has been observed that the Load balancing (LB) is one among various challenges in cloud computing that needs to be resolved to perform the accurate operations on cloud and also to obtain the rapid development in the the sphere of cloud computing. The demand of various customers all over the world for the services used to increase the load rapidly. Therefore load balancing required to equally distribute the workload over each and every virtual machine in the cloud system hence as a result the throughput increases and the response time minimizes. The aim of load balancing is to build the client satisfaction, resource utilization maximisation and improvement in the cloud system performance leads to reduction in energy consumption and heat dissipation. In the present paper, the standard Grey Wolf Optimisation algorithm for load balancing is demonstrated for the cloud environment. Also the other versions of Grey wolf optimisation has been studied to know the issues related to them and additional functionality required by them to achieve the higher system performance. Furthermore, according to the surveyed research papers it has been seen that now the performance of the proposed hybrid Grey Wolf Optimisation algorithms is simulated by using Cloudsim simulator on the basis of different parameters such as throughput and response time etc."
pub.1134281080,Attack and intrusion detection in cloud computing using an ensemble learning approach,"The distributed and decentralized nature of cloud computing facilitates its adoption and expansion in different sectors of society such as education, government, information technology, business, and entertainment, etc. Cloud Computing provides a wide information technology landscape. Its existence in every section of society makes this computing paradigm prone to intrusions and attacks. A huge volume of data stored on cloud computing poses a high risk of security and privacy [6]. Therefore it is important to build a network intrusion detection system using an anomaly detection approach for a cloud computing network which can identify as many threats as possible with better assault identification level and less false positives. This paper discusses an effective network-based intrusion detection model utilizing an ensemble-based machine learning approach using four classifiers i.e., Boosted tree, bagged tree, subspace discriminant, and RUSBooted along with a voting scheme. The voting algorithm is incorporated into the framework to obtain a consolidated final prediction. Standard dataset and simulator namely, CICIDS 2017 and CloudSim were used for simulation and testing of the suggested model. The implementation results obtained by employing individual classifiers and combined result of all the four classifiers is compared along with comparing the proposed model with respect to existing Intrusion detection models. Results of implementation demonstrate the ability of the proposed model in the identification of intrusions in the cloud environment with a higher rate of detection and generation of minimal false alarm warnings, which suggests its dominance relative to state-of-the-art approaches. The implementation results show an accuracy of 97.24%."
pub.1124068386,An Adaptive Multilevel Feedback based Learning Management Framework on Cloud Environment,"Internet becomes the key source of communication to engage with everyone and everywhere around the world at any time. The advent of internet provides enormous way to transform an existing system into digital form. The impact of this adoption reflected back in all the domains, notably in healthcare, education, communication and so on. Information and Communication Technology (ICT) is a collective term, represents the components and infrastructure that enables the modern day computing. Cloud Computing is an essential part of ICT that delivers the computing services and resources based on demand and availability. It provides flexibility to the stakeholders to scale their resources virtually. All these technologies make the activities of day-to-day life easier and comfortable. E-Learning becomes an emerging concept that inherits the functionalities of modern day computing approaches. To makes the learner more engaging and interactive with the courses, new frameworks are developed online. With the help of cloud platform, delivering education-related services becomes handy. In this paper, a new concept is introduced with a thematic framework in theoretical aspects to improve the way to deliver personalized recommendations to the user based on the feedback provided on each course enrolled by the learner. This system helps the learners to select a course that is more relevant to their domain. Moreover, this feedback system helps the instructor of the course to improve their standard by providing some suggestions based on the learners comments. This system could be beneficial for both the learner as well as the instructor by providing personalized recommendations"
pub.1020333791,Design of Integrated Medical Information System Based on The Cloud,"Today, the medical information system has evolved in the way of integrated healthcare IT information systems. Therefore, it is trying to build advanced U-Healthcare service. Though the U-Healthcare environments is exchanged the information between systems in many cases, however since the each system is different, the integration and exchange of data is difficult. To overcome this problem, in this paper it proposes that we suggests a possible DBaaS(DataBase as a Service) for the heterogeneous integration of medical information management and data exchange. First, the proposed system builds DBaaS cloud by integrating the meta-DB Schema level and DB Schema for each hospital. And, the mapping the schema data and the existing hospital information system is possible using the International Standard HL7. By applying the proposed method to the hospital system, it comes true the efficient exchange of information between the patients, doctors, staffs through the data mapping of the one to multi-system."
pub.1040102894,Making existing production systems Industry 4.0-ready,"This paper presents an approach to how existing production systems that are not Industry 4.0-ready can be expanded to participate in an Industry 4.0 factory. Within this paper, a concept is presented how production systems can be discovered and included into an Industry 4.0 (I4.0) environment, even though they did not have I4.0-interfaces when they have been manufactured. 
The concept is based on a communication gateway and an information server. Besides the concept itself, this paper presents a validation that demonstrates applicability of the developed concept."
pub.1173413438,Reinforcement Learning-based Task Scheduling for Heterogeneous Computing in End-Edge-Cloud Environment,"The End-Edge-Cloud (EEC) computing framework can offer low-latency, high-quality services to users of diverse demands by leveraging pervasive resources. However, the inherent disparities in task requirements and the strong heterogeneity of computational resources in these systems make it non-trivial for scheduler design, particularly in high load scenarios (e.g. burst of tasks). This also complicates the adaptation of traditional cloud-oriented schedulers considering their limited support of heterogeneous processors and accelerators (e.g., CPUs, GPUs and NPUs). In light of this, we first present a system framework for task scheduling in the EEC architecture. In the framework we adopt a reinforcement learning (RL)-based scheduler tailored for reducing task completion time and waiting time. Our method integrates task characteristics and environmental constraints within matrices, based on which an adapted Q-Learning agent is employed for decision making. We then introduce the implementation of our framework that features Kubernetes and Rancher-based coordination with extended support for heterogeneous processing units. Experimentally we built a real-world EEC testbed comprising PC, Atlas 200 DK, and Raspberry PI devices. Evaluation results of our algorithm demonstrate a 278\% enhancement in performance compared to existing algorithms in the context of burst-arrival task queues, which underscores the efficacy of our solution in realistic scenarios."
pub.1151692940,Energy Efficient Optimization with Threshold Based Workflow Scheduling and Virtual Machine Consolidation in Cloud Environment,"Cloud computing provides users with usage-based IT services on-demand basis. In these cloud centers, physical machines (PMs) are combined with virtual machines (VMs). Improper planning in workflow scheduling and VM consolidation disturbs the load balancing capability of the system thereby reducing the overall energy of the system with rapid increase in execution time. In this paper, the energy-efficient multi-objective adaptive Manta ray foraging optimization (MAMFO) is proposed for efficient workflow planning. It also optimizes the multi-objective factors such as energy consumption and resource utilization, i.e., CPU and memory. Dynamic Threshold with Enhanced Search and Rescue (DT-ESAR) is introduced for the VM Consolidation System. The dynamic threshold identifies the hosts that are underutilized, overutilized, and normalized. ESAR migrates the VMs from one host to another based on the threshold number. The proposed framework improves energy efficiency and minimizes the time span of the process flow. The experimental results show the efficiency of the proposed approach in terms of energy consumption, makespan, number of migrations and overall SLA. The proposed framework energy consumption is 0.234 kWh, the makespan is 107.25, the number of VM migrations performed is 51, and the overall SLA is 5.23. To determine whether the proposed MAMFO/DT-ESAR method is effective, the findings are compared with the existing methods. Utilizing CloudSim for the experimental evaluation, it is found that the suggested approach significantly improved resource utilization and energy efficiency."
pub.1092309630,A federated collaborative care cure cloud architecture for addressing the needs of multi-morbidity and managing poly-pharmacy (c3-cloud project),"Introduction: (comprising context and problem statement) There is an increasing need to organise the care around the patient and not the disease, taking into account his or her multiple physical and psycho-social conditions. An integrated, patient-centred care and cure delivery architecture needs to be developed considering the realities of multi-morbidity and poly-pharmacy. This needs to address the medical, technological, organisational and socio-economical challenges of creating a collaboration environment for all of the stakeholders involved in the holistic continuum of care.Short description of practice change implemented: The project C3-Cloud (H2020, PHC-25-2015, 689181) aims to enable the development of personalised care plans for multi-morbid conditions supported by innovative ICT components to improve delivery of integrated care services to elderly patients with multi-morbidity through continuous coordination of patient-centred care activities by a multidisciplinary care team (MDT). It is undertaking the design and development of: (i) Personalised Care Plan Development Platform & Coordinated Care and Cure Delivery Platform managed by a coordinated multidisciplinary team, (ii) Clinical Decision Support Modules enabling risk stratification, poly-pharmacy management and goal setting and monitoring and (iii) Patient Empowerment Platform facilitating and fostering the involvement of the patient and his informal care givers.Aim and theory of change: The applicability of this C3-Cloud integrated care approach and supporting set of innovative ICT components will be demonstrated in varying clinical, technological and organisational settings by piloting in three European regions (South Warwickshire, Basque Country and Region Jämtland Härjedalen) with quite different health and social care systems and ICT landscapes. Then new organisational models for addressing multi-morbidity will be proposed by identifying the best practices in different deployment settings.Targeted population and stakeholders: The target population for the C3-Cloud pilot applications is elderly (65+) patients, having at least two among these four chronic diseases: heart failure, renal failure, diabetes and depression.In total in the 3 pilot sites, 150 patients for intense evaluation in exploratory trial, 600/600 patients for resource monitoring to support large-scale impact assessment and 62 multidisciplinary care team members composed of health professionals, social care workers and homecare providers will be involved in pilot operation and evaluation activities.Timeline: The project is running for 48 months, from May 2016 to April 2020. The main phases are:Conceptual design of the C3-Cloud System Architecture according to pilot applications’ requirements, May 2016-December 2016Exploration of new patient pathways and organisational models for improved delivery of integrated care: May 2016-April 2017Development and testing of ICT components: January 2017-June 2018Preparation of"
pub.1154092870,Adaptive Storage Optimization Scheme for Blockchain-IIoT Applications Using Deep Reinforcement Learning,"Blockchain-IIoT integration into industrial processes promises greater security, transparency, and traceability. However, this advancement faces significant storage and scalability issues with existing blockchain technologies. Each peer in the blockchain network maintains a full copy of the ledger which is updated through consensus. This full replication approach places a burden on the storage space of the peers and would quickly outstrip the storage capacity of resource-constrained IIoT devices. Various solutions utilizing compression, summarization or different storage schemes have been proposed in literature. The use of cloud resources for blockchain storage has been extensively studied in recent years. Nonetheless, block selection remains a substantial challenge associated with cloud resources and blockchain integration. This paper proposes a deep reinforcement learning (DRL) approach as an alternative to solving the block selection problem, which involves identifying the blocks to be transferred to the cloud. We propose a DRL approach to solve our problem by converting the multi-objective optimization of block selection into a Markov decision process (MDP). We design a simulated blockchain environment for training and testing our proposed DRL approach. We utilize two DRL algorithms, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO) to solve the block selection problem and analyze their performance gains. PPO and A2C achieve 47.8% and 42.9% storage reduction on the blockchain peer compared to the full replication approach of conventional blockchain systems. The slowest DRL algorithm, A2C, achieves a run-time 7.2 times shorter than the benchmark evolutionary algorithms used in earlier works, which validates the gains introduced by the DRL algorithms. The simulation results further show that our DRL algorithms provide an adaptive and dynamic solution to the time-sensitive blockchain-IIoT environment."
pub.1063270098,Abstract 5282: A cloud-enabled open source data management platform supporting a federated research and development organization,"Abstract Biopharmaceutical R&D organizations characterize drug candidate target effects and modes of action and create molecular models of target diseases. These data-intensive activities are informed by vast data resources including publicly available data, internally generated data and partnered private data collections. However, rapid evolution in computing, data management tools, analytical and visualization methods, the complexity of data types and the data volumes that must be accommodated present significant technical and logistic hurdles to overcome. It is particularly difficult for a geographically dispersed R&D organization to make data resources easily available to scientists for search, visualization and exploration. Nevertheless, this is required for R&D scientists to gain insight into disease and drug mechanisms and to capture the knowledge needed to sustain the scientific enterprise. Standardized commercial solutions to R&D data challenges are unattractive since they require significant resource investment in platform configuration, user-training and system maintenance. This strategy necessarily creates delay in adopting newly emerging technologies and provides incentive not to adopt alternatives due to investment in existing systems. In contrast, our solution to R&D data demands was to build a cloud-deployed data platform using state of the art tools developed and maintained by the open source software community at the Apache Software Foundation. Partnering with academic data scientists, we selected the best available tools to fit our specific needs. We integrated them into a platform accessible to our federated R&D scientific community while allowing the system to be freely modified and updated on demand to meet evolving user requirements. Priorities for our data platform are to ingest, secure and index R&D source data of all types, make these indexed data assets available to computational scientists for analysis and provide faceted search capability based on a comprehensive metadata model. Three products: LabKey server, Apache OODT and ISATools have all been combined into a scientific data management system to provide a unified data resource enhanced by a search platform powered by Apache Solr. The platform supports both internally generated data and data imported from public, contracted or partnered sources. All data are available for interactive exploration by our R&D community, accessed via integrated search, analysis and visualization tools. Deployment of this system to our R&D organization has been met with enthusiastic adoption. Feedback for improvement or requests for system enhancements and additional capabilities are rapidly addressed in this open source environment, leading to further adoption among the R&D scientists and providing the basis for accessible, stable institutional knowledge collections. Citation Format: Lauren Intagliata, Selina Chu, Garth McGrath, Giuseppe Totaro, Daniel Civello, Nipurn Doshi, Shivika Th"
pub.1145992931,Performance Analysis of Hybrid BAT Algorithm and Cuckoo Search Algorithm [HB-CSA] for Task Scheduling in Mobile Cloud Computing,"In the recent research year, there has been lot researcher‟s research in the field of Mobile Cloud Computing (MCC). In this research paper to utilize for reducing load imbalance degree between the machines to resolve problems generated due to the evolution of technology and guarantee superior quality of service. This investigation spotlights on two significant factors: scheduling the tasks based on requested resources and partitioning task considering the priority with respect to meta-heuristic Hybrid BAT and Cuckoo Search algorithm (HB-CSA). Task allocation is primarily based on priority levels provided by bat algorithm using mathematical functions and partition the system into number of virtual machines with almost equal performance. Also, contribution of this investigation is extended with the analysis of cuckoo search algorithm based on improving the switching factors. The results are validated and compared with existing methods respectively. The anticipated method is executed in cloudsim environment, which shows better trade off in contrast to other Meta heuristic algorithms."
pub.1100989313,"The Industrial Internet of Things as an enabler for a Circular Economy Hy-LP: A novel IIoT protocol, evaluated on a wind park’s SDN/NFV-enabled 5G industrial network"," Smart interconnected devices, including Cyber-Physical Systems (CPS), permeate our lives and are now an integral part of our daily activities, paving the way towards the Internet of Things (IoT). In the industrial domain, these devices interact with their surroundings and system operators, while often also integrating industrial cloud applications. This 4th Industrial Revolution guides new initiatives, like the introduction of 5th Generation Mobile Networks (5G), to implement flexible, efficient, QoS- and energy- aware solutions that are capable of serving numerous heterogeneous devices, bringing closer the vision of a sustainable, Circular Economy. However, the lack of interoperable solutions that will accommodate the integration, use and management of the plethora of devices and the associated services, hinders the establishment of smart industrial environments across the various vertical domains. Motivated by the above, this paper proposes the Hy-LP - a novel hybrid protocol and development framework for Industrial IoT (IIoT) systems. Hy-LP enables the seamless communication of IIoT sensors and actuators, within and across domains, also facilitating the integration of the Industrial Cloud. The proposed solution is compared with existing standardised solutions on a common application, working around the protocols’ intrinsic characteristics and features to produce each variant. The developed systems are evaluated on a common testbed, demonstrating that the proposed solution is around 10 times faster for the same CPU usage level, while consuming 7 times less memory. Moreover, the applicability of the proposed solutions is validated in the context of a real industrial setting, analyzing the network characteristics and performance requirements of an actual, operating wind park, as a representative use case of industrial networks."
pub.1127637048,The Impact of Government Support on the Adoption of IaaSBEL by University’s Top Management,"Due to globalization and advancement in technological innovations, cloud-based e-learning has become an important paradigm in providing education for all. Numerous Higher Education institutions have realized the significance of adopting innovation that will make learning easier and affordable. Thus, Infrastructure as-a Service-based e-learning has been recognized as an important technology to improve the existing e-learning systems, and thus, is increasingly gaining the attention of practitioners and researchers in both developed and developing countries. The purpose of this study is to identify the factors that influence the intention to adopt Infrastructure as-a Service-based e-learning among ICT directorates top managers in the Nigerian HEIs. The study further deploys the Technology, Organization, Environment, and Diffusion of Innovation theory. Data was collected from 454 ICT directorate’s Top Managers in the Nigerian HEIs using disproportionate stratified random sampling technique. The findings indicate that Relative Advantage, Cost Savings, Service Provider Support, and Government Support are the key predictors of Infrastructure as Service-Based E-learning intention to adopt model in Nigerian HEIs."
pub.1023131512,Task-oriented access model for secure data sharing over cloud,"Cloud computing has become a prevalent technology and with its increased maturity more and more data including sensitive and non sensitive, is being centralized into it. While outsourcing the sensitive data into public cloud, its prior encryption is strongly recommended. Provisioning of encryption and existing work that guarantee security and privacy concerns on sensitive data, have removed the holdouts against cloud adoption at a large. One of the main issue with this data in cloud environment is to manage user access and its auto revocation in a controlled and flexible way. The issue becomes more complex when privacy on user access has to be ensured as well to hide additional leakage of information. For automatic revocation over cloud data, access can be bounded within certain anticipated time limit so that the access expires beyond effective time period as proposed by one of the existing system as time based proxy re-encryption. This time-oriented approach is more rigid and not a one-size-fits-all solution. In certain circumstances exact time anticipation is not an easy choice. Instead, the alternate solution could be task-oriented to restrict user beyond certain number of permissible attempts to access the data. In this paper we have proposed a system that allows authorized users to access encrypted data for predefined attempts rather pre-defined time. Our approach allows user to avail permissible attempts without time restriction and at the same time also preserves the privacy aspect of user access by concealing access limit until availed. Performance analysis revealed that the cost of operations performed are within the range of .097 to .278 $ per 1000 requests."
pub.1113887076,"Special Issue on Recent Trends and Future of Fog and Edge Computing, Services and Enabling Technologies","Recent Trends and Future of Fog and Edge Computing, Services, and Enabling Technologies
Cloud computing has been established as the most popular as well as suitable computing infrastructure providing on-demand, scalable and pay-as-you-go computing resources and services for the state-of-the-art ICT applications which generate a massive amount of data. Though Cloud is certainly the most fitting solution for most of the applications with respect to processing capability and storage, it may not be so for the real-time applications. The main problem with Cloud is the latency as the Cloud data centres typically are very far from the data sources as well as the data consumers. This latency is ok with the application domains such as enterprise or web applications, but not for the modern Internet of Things (IoT)-based pervasive and ubiquitous application domains such as autonomous vehicle, smart and pervasive healthcare, real-time traffic monitoring, unmanned aerial vehicles, smart building, smart city, smart manufacturing, cognitive IoT, and so on. The prerequisite for these types of application is that the latency between the data generation and consumption should be minimal. For that, the generated data need to be processed locally, instead of sending to the Cloud. This approach is known as Edge computing where the data processing is done at the network edge in the edge devices such as set-top boxes, access points, routers, switches, base stations etc. which are typically located at the edge of the network. These devices are increasingly being incorporated with significant computing and storage capacity to cater to the need for local Big Data processing. The enabling of Edge computing can be attributed to the Emerging network technologies, such as 4G and cognitive radios, high-speed wireless networks, and energy-efficient sophisticated sensors.
Different Edge computing architectures are proposed (e.g., Fog computing, mobile edge computing (MEC), cloudlets, etc.). All of these enable the IoT and sensor data to be processed closer to the data sources. But, among them, Fog computing, a Cisco initiative, has attracted the most attention of people from both academia and corporate and has been emerged as a new computing-infrastructural paradigm in recent years. Though Fog computing has been proposed as a different computing architecture than Cloud, it is not meant to replace the Cloud. Rather, Fog computing extends the Cloud services to network edges for providing computation, networking, and storage services between end devices and data centres. Ideally, Fog nodes (edge devices) are supposed to pre-process the data, serve the need of the associated applications preliminarily, and forward the data to the Cloud if the data are needed to be stored and analysed further.
Fog computing enhances the benefits from smart devices operational not only in network perimeter but also under cloud servers. Fog-enabled services can be deployed anywhere in the network, and "
pub.1145549523,The Integration of Blockchain With IoT in Smart Appliances,"An amazing utilization of internet of things (IoT)-based smart technologies has been observed in recent years. Internet of things (IoT) became smarter and autonomous, leading the way by interconnecting and interacting with the environment. Existing technologies such as big data, AI, and cloud computing have been supported by IoT but with some limitations. Blockchain is the better solution to access and share data with the decentralized system. IoT spreads its applications in healthcare, industrial internet of things, supply chain management, traffic management, etc., out of which smart cities take special attention in recent days by integrating with IoT to create a sustainable ecosystem by using information and communication technology (ICT). Smart cities can offer various smart applications for boosting the life prominence of society. These characteristics of blockchain are beneficial to the creation of smart cities. In this regard, the authors have conducted a systematic review of articles. This study highlights the integration of blockchain with IoT."
pub.1165179231,Double Deep Q-Network-Based Time and Energy-Efficient Mobility-Aware Workflow Migration Approach,"With the emergence of the Fog paradigm, the relocation of computational capabilities to the network’s edge has become imperative to support the ever-growing requirements of latency-sensitive, data-intensive, and real-time decision-making applications. In dynamic and mobile environments, services must adapt to accommodate the mobility of users, resulting in frequent relocations across computing nodes to ensure seamless user experiences. However, these migrations incur additional costs and potentially degrade the Quality of Service (QoS) parameters. In this paper, we propose a mobility-aware workflow migration approach based on Deep Reinforcement Learning (DRL). This approach aims to minimize the system’s overall delay and energy consumption by optimizing the number of workflow task migrations, considering resource performance and network conditions in different regions. The problem is first formulated as a Markov Decision Process (MDP), and then a Double Deep Q-network (DDQN) algorithm is proposed to identify the optimal policy for workflow offloading and migration. Comprehensive experiments have been conducted and the results demonstrate that our approach outperforms significantly the existing approaches."
pub.1145612174,Design and Development of IoT based Smart Dynamic Energy Management for the Built Environment,"The effect of Internet of things (IoT) for the design of smart energy management system is largely dependent on the successful integration of IoT devices with cloud computing technique. For effective energy management system, the energy sector should be closely connected with Integrated Information and Communication Technologies (ICT). Due to lack of semantics, the existing system is unable to communicate and integrate with other home appliances from remote locations. This paper demonstrates the design of IoT based Smart energy management system including the lighting load controller that works based on the motion of occupants inside the room. This idea of occupant based lighting control reduces the energy consumption around 60%. In this context, there are two modes of operation, one is manual operation and other one is based on Cloud computing technique. The end-user can control by publishing the messages by motion sensor onto cloud server to which LED lights are subscribed. This integrated system includes 4 channel relay module and low cost 32 bit MCU board with different sensors. This proposed study develops the energy efficient Smart room while maintaining the energy usage with cost effective approach. Our results show that IoT-cloud network provide wide range of monitoring and controlling services for next generation smart environment and reduces the power consumption."
pub.1174223038,QOS Aware Self Adaptable Virtual Machines Management System for Cloud Computing,"Cloud Computing as of now is the most important distributed environment because of low level user management and system integration. But most important challenge cloud computing faces is effective resource provisioning, Solving the issue will result in effective consumption of service offered, better user satisfaction and resources for more people during peak hours, reduce operational burden to cloud service providers and less pay to clients. Current works are aimed at determining the usage, VM (Virtual Machine) establishment and setting up. The above process requires considerable time to construct and kill VMs which may be used to cater more user. So here we have provided, a Quality of Service Aware Virtual Machine management mechanism for creating new VM’s that makes use of the system resources efficiently. The existing VM for related type of requests are identified to minimize VM creation time. In our system, QOS is guaranteed by making all tasks adhere to the SLA necessities. Services are divided using need of the hour and the critical job is given higher significance. The experimental results show that a large number of users are serviced in relation to others algorithm which will fulfil clients needs during the peak traffic.  "
pub.1154244150,Lightweight Integrity Preserving Scheme for Secure Data Exchange in Cloud-Based IoT Systems,"The information obtained from external sources within the cloud and the resulting computations are not always reliable. This is attributed to the absence of tangible regulations and information management on the part of the information owners. Although numerous techniques for safeguarding and securing external information have been developed, security hazards in the cloud are still problematic. This could potentially pose a significant challenge to the effective adoption and utilization of cloud technology. In terms of performance, many of the existing solutions are affected by high computation costs, particularly in terms of auditing. In order to reduce the auditing expenses, this paper proposes a well-organised, lightweight system for safeguarding information through enhanced integrity checking. The proposed technique implements a cryptographic hash function with low-cost mathematic operations. In addition, this paper explores the role of a semi-trusted server with regard to smart device users. This facilitates the formal management of information prior to distribution through the IoT-cloud system. Essentially, this facilitates the validation of the information stored and exchanged in this environment. The results obtained show that the proposed system is lightweight and offers features such as a safeguarding capability, key management, privacy, decreased costs, sufficient security for smart device users, one-time key provision, and high degree of accuracy. In addition, the proposed method exhibits lower computation complexity and storage expenses compared with those of other techniques such as bilinear map-based systems."
pub.1152347801,First Proof of Concept for an Integrated Robot Supervision System,"Abstract
                  TotalEnergies’ use of robotics on remote or offshore sites is considered with the objective of enabling unmanned operation for long periods and as the next frontier for increased personnel safety, industry attractiveness for young talents and further OPEX and CAPEX reduction. TotalEnergies performed several IT/OT system architecture evolutions and developed a specific Robot Supervision System (RSS) equivalent to a fleet management system to help and consolidate know-how in the deployments of specific mobile ground robots.
                  As the first autonomous and explosion-proof inspection ground robots become commercial, many have difficulties seeing how these new devices can be interfaced with legacy industrial systems and how its data can be represented in a user-friendly interface to make the remote operation of robot efficient & attractive enough. Only this level of interfacing will enable the possibility for automated inspection, operation, and maintenance tasks execution with intuitive programmable interfaces. The robotic devices also need also to comply with detailed specifications in order to communicate its data in real time to the supervision system.
                  We will describe the specific modifications required to the robot associated systems to be used within a private industrial network and the specification for real time interfacing of its data. Many Robot suppliers initiated their system developments focusing on robot navigation but a lot of the interfacing complexity with operator companies remained to be achieved to exchange data with the site digital twin, the maintenance task planning tool, the cloud machine learning software resources, or the Process Data Management System (PDMS) or even the plant control system (DCS).
                  The associated functionalities of the RSS will also be presented: Acquisition, historization and user-friendly display of robot's dataIntegrate several robot types or from several vendorsInterface with all company systemsVisual 3D digital twin environment to display data acquired by the robot or useful data for its remote operationIntuitive configuration and management of robot for routine tasks or based on a specific plant event or alarmComply with specific industrial cybersecurity rules
                  Feedback will also be given on how it has been possible to adapt the IT architectures and how we intend to fit the requirements of OT systems to adopt robots with more critical interactions with the industrial process. We can present our approach for upscaling robot deployment with this on-premises supervision systems capable of historizing the data, generating immersive front-end displays for robots’ operators and managing secured interconnections to corporate systems. Figure 1An experienced operator using the RSS - View of the 2 main dashboard types (summary mashups & 3D view)"
pub.1002045584,Dynamic power management for reactive stream processing on the SCC tiled architecture,"Dynamic voltage and frequency scaling (DVFS) is a means to adjust the computing capacity and power consumption of computing systems to the application demands. DVFS is generally useful to provide a compromise between computing demands and power consumption, especially in the areas of resource-constrained computing systems. Many modern processors support some form of DVFS.In this article, we focus on the development of an execution framework that provides lightweight DVFS support for reactive stream processing systems (RSPS). RSPs are a common form of embedded control systems, operating in direct response to inputs from their environment. At the execution framework, we focus on support for many-core scheduling for parallel execution of concurrent programs. We provide a DVFS strategy for RSPs that is simple and lightweight, to be used for dynamic adaptation of the power consumption at runtime. The simplicity of the DVFS strategy became possible by the sole focus on the application domain of RSPs. The presented DVFS strategy does not require specific assumptions about the message arrival rate or the underlying scheduling method.While DVFS is a very active field, in contrast to most existing research, our approach works also for platforms like many-core processors, where the power settings typically cannot be controlled individually for each computational unit. We also support dynamic scheduling with variable workload. While many research results are provided with simulators, in our approach, we present a parallel execution framework with experiments conducted on real hardware, using the single-chip cloud computer many-core processor. The results of our experimental evaluation confirm that our simple DVFS strategy provides potential for significant energy saving on RSPs."
pub.1181155510,Research on the Construction of Evaluation System for New Building Industrialization Projects,"Although new industrialized construction technologies have the potential to improve industry performance by optimizing work processes and improving the working environment, the application of these technologies in the construction industry is still in its infancy, and its technical direction is not yet set. This study aims to put forward a theoretical framework for evaluating the level of new-type building industrialization of construction engineering projects, conduct integrated research on the existing domestic and foreign literatures on new-type building industrialization and intelligent construction, preliminarily construct the index system, and determine the evaluation index system of new-type building industrialization projects through the Delphi method. The index weight is determined by the combination of order relation analysis and entropy weight method. The evaluation model is constructed by matter-element theory and cloud model, and the evaluation grade standard is proposed. The research results will help guide the evaluation of new building industrialization projects, and provide a judgment reference for determining the development trend and direction of new building industrialization technologies in the future, and help formulate data-driven industrial policies to promote the adoption of new industrialization technologies and promote the digital transformation of the construction industry."
pub.1146814422,The Use of Cloud Firestore For Handling Real-time Data Updates: An Empirical Study of Gamified Online Quiz,"The need for technological solutions that can help improve interactivity in online learning environments is increasing during the COVID-19 pandemic. One technological solution that has been proven is the use of gamified online quizzes such as Kahoot!. One of the issues when using existing third-party platforms is related to integration with existing learning platforms. Institutions that have sufficient resources can solve this by developing solutions that are easily integrated with existing learning platforms. This paper discusses how to use database technology that can handle real-time data updates to develop a gamified online quiz system. In this experimental study, UML use cases and sequence diagrams were used to describe the functional requirements and behavior of the system. Further, entity-relationship model was also used to help the database implementation process. Two database technologies, namely Cloud Firestore and CouchDB, were compared to see how they perform in handling real-time data updates and how much they cost. The comparison results show that Cloud Firestore is better in terms of latency and more cost-effective than CouchDB."
pub.1143685860,Definition of an FHIR-based multiprotocol IoT home gateway to support the dynamic plug of new devices within instrumented environments,"Internet of Things (IoT) technologies have become a milestone advancement in the digital healthcare domain, since the number of IoT medical devices is grown exponentially, and it is now anticipated that by 2020, there will be over 161 million of them connected worldwide. Therefore, in an era of continuous growth, IoT healthcare faces various challenges, such as the collection over multiple protocols (e.g. Bluetooth, MQTT, CoAP, ZigBEE, etc.) the interpretation, as well as the harmonization of the data format that derive from the existing huge amounts of heterogeneous IoT medical devices. In this respect, this study aims at proposing an advanced Home Gateway architecture that offers a unique data collection module, supporting direct data acquisition over multiple protocols (i.e.BLE, MQTT) and indirect data retrieval from cloud health services (i.e. GoogleFit). Moreover, the solution propose a mechanism to automatically convert the original data format, carried over BLE, in HL7 FHIR by exploiting device capabilities semantic annotation implemented by means of FHIR resource as well. The adoption of such annotation enables the dynamic plug of new sensors within the instrumented environment without the need to stop and adapt the gateway. This simplifies the dynamic devices landscape customization requested by the several telemedicine applications contexts (e.g. CVD, Diabetes) and demonstrate, for the first time, a concrete example of using the FHIR standard not only (as usual) for health resources representation and storage but also as instrument to enable seamless integration of IoT devices. The proposed solution also relies on mobile phone technology which is widely adopted aiming at reducing any obstacle for a larger adoption."
pub.1129363003,A Comprehensive Study of Load Balancing Approaches in the Cloud Computing Environment and a Novel Fault Tolerance Approach,"The past few years have witnessed the emergence of a novel paradigm called cloud computing. CC aims to provide computation and resources over the internet via dynamic provisioning of services. There are several challenges and issues associated with implementation of CC. This research paper deliberates on one of CC main problems i.e. load balancing (LB). The goal of LB is equilibrating the computation on the cloud servers such that no host is under/ overloaded. Several LB algorithms have been implemented in literature to provide effective administration and satisfying customer requests for appropriate cloud nodes, to improve the overall efficiency of cloud services, and to provide the end user with more satisfaction. An efficient LB algorithm improves efficiency and asset’s usage through effectively spreading the workload across the system’s different nodes. This review research paper objective is to present critical study of existing techniques of LB, to discuss various LB parameters i.e. throughput, performance, migration time, response time, overhead, resource usage, scalability, fault tolerance, power savings, etc. The research paper also discusses the problems of LB in the CC environment and identifies the need for a novel LB algorithm that employs FT metrics. It has been found that traditional LB algorithms are not good enough and they do not consider FT efficiency metrics for their operation. Hence, the research paper identifies the need for FT efficiency metric in LB algorithms which is one of the main concerns in cloud environments. A novel algorithm that employs FT in LB is therefore proposed."
pub.1169651412,EarthCODE ,"The EarthCODE vision will provide a cloud-based, user-centric development environment which can be used to support ESA science activities and projects. Our solution is built around existing open-source solutions and building blocks, primarily the Open Science Catalogue, EOxHub and the Exploitation Platform Common Architecture (EOEPCA+). EarthCODE capability will be hosted on the Copernicus Data Space Ecosystem (CDSE). Because EarthCODE will adopt a federated approach, it will also facilitate processing on other platforms, and these may include Deep Earth System Data Lab (DeepESDL), ESA EURO Data Cube (EDC), Open EO Cloud / Open EO Platform and perhaps AIOPEN / AI4DTE. 1. GOALS EarthCODE will support FAIR Open Science. The main use cases will allow users to “Access Compute Resource” in thecloud and “Conduct Science” using their preferred approach to create reproducible workflows. “Manage Workflows” willallow users to execute workflows on a cloud platform of their choice and enable federated workflow and federated datasolutions. “Publish Results” will allow scientists to share experiments and data within the community and “Interact with Community” will enable a collaborative approach throughout this lifecycle. EarthCODE will allow scientists to concentrateon science and collaboration and hide the complexity of Managing and implementing Workflows when required. 2. OPEN SCIENCE PLATFORMSEarthCODE will provide users with a portal to the underlying platform services. Broadly speaking, this will include FAIR Open Science services that support the delivery of scientific workflows, Infrastructure Services that support the Execution ofworkflows and Community Services to support underlying collaboration and exploitation. 3. BUILDING BLOCKS EarthCODE will be created use existing building blocks. These will include EOEPCA+, EOxHub, Open Science Catalogue and other existing platform services. 4. PLATFORM FREEDOM EarthCODE will be designed using open standards to help facilitate platform freedom and platform interoperability. This means that more than one type of Development Tooling can be used to conduct science and more than one of IntegratedPlatform service can be used to Manage Workflows. 5. PLATFORM INTEGRATIONUsing the above approach, EarthCODE will facilitate the use of data and services from a wide variety of platforms as illustratedbelow. This shows how EarthCODE will promote integration with different platforms yet still provide freedom of platformimplementation. STAC will be used at the heart of the Science Catalogue to describe resources. Platform integration will also be supported by a DevOps best practice approach to facilitate integration. 6. WORKFLOW AND DATA INTEGRATION EarthCODE will prioritise the management of Workflow and Data modelling to ensure successful platform integration andsuccessful collaboration. This approach will further support the integration of platforms and data. FAIR Data principles will beapplied. ACKNOWLEDGEMENTS This wor"
pub.1100960105,Scheduling framework for distributed intrusion detection systems over heterogeneous network architectures,"The evolving trends of mobility, cloud computing and collaboration have blurred the perimeter separating corporate networks from the wider world. These new tools and business models enhance productivity and present new opportunities for competitive advantage although they also introduce new risks. Currently, security is one of the most limiting issues for technological development in fields such as Internet of Things or Cyber-physical systems. This work contributes to the cyber security research field with a design that can incorporate advanced scheduling algorithms and predictive models in a parallel and distributed way, in order to improve intrusion detection in the current scenario, where increased demand for global and wireless interconnection has weakened approaches based on protection tasks running only on specific perimeter security devices. The aim of this paper is to provide a framework to properly distribute intrusion detection system (IDS) tasks, considering security requirements and variable availability of computing resources. To accomplish this, we propose a novel approach, which promotes the integration of personal and enterprise computing resources with externally supplied cloud services, in order to handle the security requirements. For example, in a business environment, there is a set information resources that need to be specially protected, including data handled and transmitted by small mobile devices. These devices can execute part of the IDS tasks necessary for self-protection, but other tasks could be derived to other more powerful systems. This integration must be achieved in a dynamic way: cloud resources are used only when necessary, minimizing utility computing costs and security problems posed by cloud, but preserving local resources when those are required for business processes or user experience. In addition to satisfying the main objective, the strengths and benefits of the proposed framework can be explored in future research. This framework provides the integration of different security approaches, including well-known and recent advances in intrusion detection as well as supporting techniques that increase the resilience of the system. The proposed framework consists of: (1) a controller component, which among other functions, decides the source and target hosts for each data flow; and (2) a switching mechanism, allowing tasks to redirect data flows as established by the controller scheduler. The proposed approach has been validated through a number of experiments. First, an experimental DIDS is designed by selecting and combining a number of existing IDS solutions. Then, a prototype implementation of the proposed framework, working as a proof of concept, is built. Finally, singular tests showing the feasibility of our approach and providing a good insight into future work are performed."
pub.1030459135,New perspectives for the future interoperable enterprise systems,"The rapid changes in today's socio-economic and technological environment in which the enterprises operate necessitate the identification of new requirements that address both theoretical and practical aspects of the Enterprise Information Systems (EIS). Such an evolving environment contributes to both the process and the system complexity which cannot be handled by the traditional architectures. The constant pressure of requirements for more data, more collaboration and more flexibility motivates us to discuss about the concept of Next Generation EIS (NG EIS) which is federated, omnipresent, model-driven, open, reconfigurable and aware. All these properties imply that the future enterprise system is inherently interoperable. This position paper presents the discussion that spans several research challenges of future interoperable enterprise systems, specialized from the existing general research priorities and directions of IFAC Technical Committee 5.3,11IFAC Technical Committee 5.3 « Enterprise Integration and Networking », http://www.ifac-tc53.org namely: context-aware systems, semantic interoperability, cyber-physical systems, cloud-based systems and interoperability assessment."
pub.1149337777,"KVMIveggur: Flexible, secure, and efficient support for self-service virtual machine introspection","Virtual machine introspection (VMI) has evolved into a widely used technique for purposes such as digital forensics, intrusion detection, and malware analysis. The recent integration of enhanced VMI capabilities into KVM further facilitates the use of VMI. A significant obstacle, however, remains: VMI usually requires highly privileged access to the host system. Existing research prototypes that address this issue either target only the Xen hypervisor, are extremely slow, offer only a subset of the desired functionality, or are hard to deploy in real-life systems. We present our flexible KVMIveggur architecture as a novel solution to these challenges. It offers three flavors of isolation (using containers, virtual machines, and network remote access) that all enable access control for secure self-service VMI in cloud environments. It enables the full use of passive and active VMI, supports continuous monitoring also during live VM migration, and can be tailored for low overhead and minimal resource utilization on the host system. The experimental evaluation of our prototype demonstrates the feasibility and the efficiency of our approach and provides detailed insights into the differences between the three flavors."
pub.1152877638,Adaptation of powerline communications-based smart metering deployments with IoT cloud platform,"The necessity of energy management and optimization through smart devices has an essential role in sustainable energy. Smart grid features and cutting-edge technologies are progressively integrated into traditional electricity networks. One of these features is the interference between power line communications and IoT. The introduction and deployment of these grids are mainly focused on the development of the field of smart metering. A new proposed module for smart meter design within the existing infrastructure grid system using power line communication (PLC) is presented. The system will include a transmitter with a microcontroller (MCU) and numerous sensors, as well as communication channels that include PLC and an in-house powerline network, and a receiver with an MCU. The suggested system interacts with the IoT system, characterized by a free web interface showing the data directly in real-time. Based on real-world experience, this paper develops guidelines for various aspects of PLC Smart Metering network deployment via the cloud environment. The practical result of packet losses is about 0, 1, or 2 characters of received data, and the time difference between transmitter and receiver is about 5000 milliseconds for a fixed transmission line."
pub.1140815346,DLOAM: Real-time and Robust LiDAR SLAM System Based on CNN in Dynamic Urban Environments,"Dynamic object detection, state estimation, and map-building are crucial for autonomous robot systems and intelligent transportation applications in urban scenarios. Most current LiDAR Simultaneous Localization and Mapping (SLAM) systems operate on the assumption that the observed environment is static. However, the overall accuracy and robustness of a SLAM system can be compromised by dynamic objects in the environment. Aiming at the problem of inaccurate odometry estimation and wrong mapping caused by the existing LiDAR SLAM method which cannot detect the dynamic objects, we study the SLAM problem of robots and unmanned vehicles equipped with LiDAR traveling in the dynamic urban scenes. We propose a fast LiDAR-only model-free dynamic objects detection method, which uses the spatial and temporal information of point cloud through a convolutional neural network (CNN), and the detection accuracy is improved by 35 use spatial information. We further integrate it into a state-of-the-art LiDAR SLAM framework to improve the SLAM performance. Firstly, the range image constructed by LiDAR point cloud is used for ground extraction and non-ground point clustering. Then, the motion of objects in the scene is estimated by the difference between adjacent frames, and the segmented objects are further divided into dynamic objects and static objects by their motion features. After that, the stable feature points are extracted from the static objects. Finally, the pose transformation of adjacent frames is solved by matching feature point pairs. We evaluated the accuracy and robustness of our system on datasets with different challenging dynamic environments, and the results show our system has significant improvements in accuracy and robustness of odometry and mapping, while still maintain real-time performance, which is sufficient for autonomous robot systems and intelligent transportation applications in urban scenarios."
pub.1155888604,A Review on the Immediate Advancement of the Internet of Things in Wireless Telecommunications,"Internet of Things is emerging as an incredible future technology to improve the existing lifestyle from the research community, industry, and the public sector. The main intention of IoT is to create an efficient, interactive, and autonomous infrastructure for a safer and healthier world. Moreover, it grows faster day-to-day with the support of many other technologies, i.e. Cloud computing, Blockchain, Wireless Body Sensor Networks, Nanotechnology, and Artificial Intelligence for smart applications, including healthcare, environment, automotive industries, transportation, agriculture, etc. Nevertheless, managing big data is one of the challenging tasks due to the increased number of devices leading to various serious issues like security, privacy, accuracy, latency, scheduling, etc. Further, specific infrastructures with remarkable techniques are required to analyze the bulk of raw data to progress the quality of life and allow timely intervention through various capabilities, i.e., data capture, unique identification, actuation, communication, data mining, etc. In past literature, numerous reviews/surveys are presented that explore the technologies mentioned above as standalone and application specific. However, this paper aims to integrate all the mentioned technologies and deliver a clear vision to future researchers (newcomers) as a kick-start article to boost up and understand the status of the existing research through a comprehensive review of the Internet of Things and its evolution in wireless telecommunications from a general perspective. The most significant challenges and issues are highlighted to research further in these evolving domains."
pub.1134230548,Towards a Digital Process Platform for Future Construction Sites,"When it comes to digitalization, construction sites are still at the beginning of an ongoing transformation process. Two major causes are the complexity of construction site environments and the lack of interoperability between the different actors, such as machines and site suppliers that are involved in the construction project. This has direct implications on the efficiency, quality, and production time of construction projects. Thus, there is a strong need to fill this digitalization gap by considering the latest advancements in the Internet of Things (IoT), as well as edge and cloud computing. In this paper, we suggest a data-driven approach for construction sites, in which we extract data from machines, send it to a central data storage with scalable computational resources and analyze it to improve the existing construction processes. For that, we studied requirements of selected construction site processes and designed a communication architecture that scales with the complex characteristics of real-world applications. Additionally, we propose a digital process platform that supports a wide range of users with their different roles on construction sites and builds the foundation for further research, implementation, and evaluation of according construction site innovations. With the proposed platform framework, we extend the construction site towards an integrated Cyber-Physical System (CPS) and contribute to the standardization of communication infrastructures within the construction sector by merging different solutions from the area of information and communication technologies."
pub.1130869564,CREW: Cost and Reliability aware Eagle‐Whale optimiser for service placement in Fog,"Abstract Integration of Internet of Things (IoT) with industries revamps the traditional ways in which industries work. Fog computing extends Cloud services to the vicinity of end users. Fog reduces delays induced by communication with the distant clouds in IoT environments. The resource constrained nature of Fog computing nodes demands an efficient placement policy for deploying applications, or their services. The distributed and heterogeneous features of Fog environments deem it imperative to consider the reliability performance parameter in placement decisions to provide services without interruptions. Increasing reliability leads to an increase in the cost. In this article, we propose a service placement policy which addresses the conflicting criteria of service reliability and monetary cost. A multiobjective optimisation problem is formulated and a novel placement policy, Cost and Reliability‐aware Eagle‐Whale (CREW), is proposed to provide placement decisions ensuring timely service responses. Considering the exponentially large solution space, CREW adopts Eagle strategy based multi‐Whale optimisation for taking placement decisions. We have considered real time microservice applications for validating our approaches, and CREW has been experimentally shown to outperform the existing popular multiobjective meta‐heuristics such as NSGA‐II and MOWOA based placement strategies."
pub.1118196345,The origin and evolution of dust in high-redshift galaxies,"Dusty hyperluminous galaxies in the early universe provide unique
environments for studying the role of massive stars in the formation and
destruction of dust. At redshifts above 6, when the universe was less than 1
Gyr old, dust could have only condensed in the explosive ejecta of Type II
supernovae (SNe), since most of the progenitors of the AGB stars, the major
alternative source of interstellar dust, did not have time to evolve off the
main sequence. We present analytical models for the evolution of the gas, dust,
and metals in high redshift galaxies, with a special application to SDSS
J1148+5251, a hyperluminous quasar at z = 6.4. We show that an average SN must
condense at least 1 Msun of dust to account for the mass of dust in this
object, when grain destruction by supernova remnants (SNRs) is taken into
account. This required yield is in excess of ~0.05 Msun, the largest mass of
dust inferred from infrared observations of Cas A. If the yield of Cas A is
typical, then other processes, such as accretion onto preexisting grains in
molecular clouds is needed to produce the mass of dust in J1148+5251. For such
process to be effective, SNR must significantly increase, presumably by
non-evaporative grain-grain collisions during the late stages of their
evolution, the number of nucleation centers onto which refractory elements can
condense in molecular clouds."
pub.1091507833,Multi-objective optimization for rebalancing virtual machine placement," Load balancer, as a key component in cloud computing, seeks to improve the performance of a distributed system by allocating workload amongst a set of cooperating hosts. A good balancing strategy would make the distributed system efficient and enhance user satisfaction. However, the balance of Host Machines (HMs) in a real cloud environment often breaks due to frequently occurred addition and removal of Virtual Machines (VMs). Therefore, it is essential to schedule the VMs to be reBalanced (VMrB). In this paper, we first summarize and analyze the existing studies on load rebalancing. We then propose a novel solution to the VMrB problem, namely a Pareto-based Multi-Objective VM reBalance solution (MOVMrB), which aims to simultaneously minimize the disequilibrium of both inter-HM and intra-HM loads. It is one of the first solutions that leverages the inter-HM and intra-HM loads and applies a multiple objective optimization strategy to overcome the virtual machine rebalance problem. In our work, we keep migration cost in mind and propose a hybrid VM live migration algorithm that significantly reduces the I/O complexity of VMrB processing. The proposed rebalancing solution is evaluated based on two synthetic datasets and two real-world datasets under a CloudSim framework. Our experimental results show that MOVMrB outperforms other existing multi-objective solutions and also demonstrate its extensibility to support complex scenarios in cloud computing."
pub.1091886710,RETRACTED ARTICLE: Cluster and cloud computing framework for scientific metrology in flow control,Scientific metrology is one evergreen and Omni present field that has been continuously experiencing a great deal of research in developing new measurement benchmarks to cope up with the real time advancement in the current market. This has been greatly aided in recent times with the advent of intelligent computing networks and communication protocols through which there has been a great deal of migration towards cloud based services on a demand basis. The essential feature of cloud is the provision of quality service to the clients. The proposed research paper has taken the metrology of monitoring the flow rate in an industrial piping system towards a boiler as the case study and real time implementation achieved with the help of Labview and MyDAQ environment. The data from this DAQ is interpreted into the cloud network with subset reduction and regrouping based on features using a fuzzy c means clustering approach. The experimentation has been done for varying values of tuning constants in order to maintain constant flow and compared with existing research contributions. The results clearly indicate a fast computation time with low complexity overhead which is achieved with the help of cloud distribution.
pub.1176188599,MFITrack: Multi-Frame Integration Strategy for Enhanced Motion-Centric Single Object Tracking,"3D Single Object Tracking (SOT) in LiDAR point clouds is essential for applications like autonomous driving and surveillance, requiring precise tracking for safety and efficiency. Existing methods often face challenges in adapting to changes in target appearance and capturing motion details in complex environments. To overcome these challenges, in this paper, we introduce a novel three-stage tracker employing a multi-frame integration strategy, dubbed MFITrack. It comprises three stages: multi-frame probabilistic data selection, multi-frame motion prediction and bounding box optimization. MFITrack effectively integrates data from multiple frames, enhancing the extraction of motion information and understanding of the target’s trajectory and behavior. Tested on KITTI and Nuscenes datasets, MFI-Track demonstrates superior performance over existing methods, showcasing robustness and precision in varied tracking scenarios, particularly in the more complex Nuscenes dataset."
pub.1123758868,Field Trial of Cloud-Connected Wireless Completion System,"This article, written by JPT Technology Editor Judy Feder, contains highlights of paper SPE 192940, “Development and Field Trial of the World’s First Cloud-Connected Wireless Intelligent Completion System,” by Annabel Green, SPE, and Paul Lynch, SPE, Tendeka, and Bjarne Bugten, Equinor, prepared for the 2018 Abu Dhabi International Petroleum Exhibition and Conference, 12–14 November. The effectiveness of intelligent completions for production optimization and improvement of reservoir management is well established, yet the use of the technology remains limited to high-value, single-bore wells. The cost and complexity of these solutions, coupled with limitations in well types, interval quantity, and system-interface quality, have prevented broader application. This paper describes the development and field trials of a cloud-connected, wireless intelligent completion system that enables long-term monitoring and interval control to enhance production management by connecting the user wirelessly from the desktop to downhole inflow-control valves (ICVs). Introduction One reason for the high expense of intelligent completion technology is the need for a wire or tubes to control and power downhole equipment such as sliding sleeves. An intelligent completion that communicates wirelessly within the wellbore provides dual benefits in completion operations and production management, and eliminates the requirement for control lines. The all-electric technology also provides greater scope for digital well management and integration with surface systems (Fig. 1). A slimline ICV has been developed that provides infinitely variable choking capability and multiple integrated sensors. Qualification has been completed to the operator’s existing standard for interval control valves adapted and augmented to reflect the differences between the wireless solution and conventional technology. Several field installations have been performed with the system, two of which are detailed in the complete paper. In the first of these two, operations were performed with local-only access to surface data. The project demonstrated the ability to communicate effectively but highlighted some mechanical limitations in the tool itself. After design improvements, a second installation was performed, which also included a surface management system with wireless interpretation software and cloud connectivity. The system described in the paper has been proven to provide robust communication using pressure pulses within the flowing wellstream to provide operating instructions to the ICV, and for the ICV to operate as instructed and communicate effectively to the wellhead. Data from the system have been digitally managed and remotely accessed, demonstrating the ability to change tool settings remotely. Integrating a semiduplex pressure-pulse system, which operates in compressive fluid environments to provide direct communication between a downhole device and wellhead, provides a low-energy c"
pub.1111954623,Adaptive deduplication of virtual machine images using AKKA stream to accelerate live migration process in cloud environment,"Cloud Computing is a paradigm which provides resources to users from its pool based on demand to satisfy their requirements. During this process, many servers are overloaded and underloaded in the cloud environment. Thus, power consumption and load balancing are the major problems and are resolved by live virtual machine (VM) migration. Load balancing is addressed by moving virtual machines from overloaded host to under loaded host and from under loaded host to any other host which is not overloaded called VM migration. If this process is done without power off (Live) the virtual machines then it is called live VM migration. By this process, the issue of power consumption by physical hosts is also resolved. Migrating virtual machines involves virtualized components like storage disks, memory, CPU and networking, the entire state of VM is captured as a collection of data files. These data files are virtual disk files, configuration files, and log files. The virtual disk files take larger memory and take more migration time and hence the downtime. These disk files include many zero pages, similar and redundant pages. Many techniques such as compression, deduplication is used reduce the size of VM disk image file. Compression techniques are not widely used, due to the disadvantage of compression ratio and decompression time. Many researchers hence used deduplication techniques for reducing the VM disk image file in the live migration process. The significance of the research work is to design an adaptive deduplication mechanism for reducing VM disk image file size by performing fixed length and variable length block-level deduplication processes. The Rabin-Karp rolling hash algorithm is used in variable length block-level deduplication. Akka stream is used for streaming the VM disk image files as it is the bulk volume of live data transfer. To reduce the time of the deduplication process, many researchers used multithreading and multi-core technologies. We use multithreading in Akka framework to run the deduplication process concurrently without OutofMemory errors. The experiment results show that we achieved a maximum of 83% overall reduction in image storage space and 89.76% reduction in total migration time are achieved by adaptive deduplication method. 3% improvement in deduplication rate when compared with the existing image management system. The results are significant because when we apply this in the storage of data centres, there are much space savings. The reduction in size is dependent on the dataset was taken and the applications running on the VM."
pub.1181374705,Analysis on Long Term Reliability of Single-phase Immersion Solution based on Data Center Deployment,"Data centers (DCs) are mission critical infrastructures composed of thousands of Information Technology Equipment (ITE) including servers, network switches, Uninterruptible Power Supply (UPS), cooling system etc. In the past decade, skyrocketing demand from Cloud and artificial intelligence (AI) has led to explosive growth in number of DCs worldwide, which introduces tremendous needs for electricity as well as significant amounts of heat dissipation. Both ITE and cooling systems are huge-power consuming. In order to conserve energy while maintaining electronic devices under reasonable operation temperature, advanced and efficient cooling technology has become crucial and has attracted more attention from DC’s operators in pursue of optimal performance, reliability, sustainability as well as Power Usage Effectiveness (PUE). Among the novel cooling technologies, single-phase immersion cooling (1-PIC) solution has been deeply investigated by Cloud Service Provides (CSPs). Some pioneer CSPs have even adopted 1- PIC volume deployment in DC. However, existing research and literatures mostly focused on thermal design optimization. Industry is lack of long-term operation data in terms of thermal performance degradation, periodic analysis on immersion fluid content and impact on electric performance. Most DCs still hesitate to adopt this novel cooling technology for deployment. Volcano Engine, public CSP under ByteDance.COM, has investigated technical feasibility of 1-PIC with fluorochemical fluid for DC’s application for years. Based on the architecture previously published in ITherm 2023, hundreds of servers under immersion environment have been operating as pilot deployment for six months already out of planned twelve-month research. Massive amounts of first-hand operation and reliability data are being collected to support long-term reliability research, including analysis on variation of CPU Digital Temperature Sensor (DTS) temperature, fluid content, material compatibility, impact on Signal Integrity (SI) etc. This paper presents the data collected with analysis from long-term reliability tests covering multiple perspectives. The collected results and analysis should be able to support long-term stability and maturity of 1-PIC solution, which will support the following volume adoption and deployment of this novel technology in DC for achieving better PUE."
pub.1171065972,Task offloading method based on CNN-LSTM-attention for cloud–edge–end collaboration system,"Most of the existing task offloading methods in edge computing environments do not fully utilize the processing capabilities of cloud servers and have high computational complexity, making them unsuitable for real-time processing tasks. To address this problem, we propose a cloud–edge–end collaborative task offloading method based on deep learning methods, combining Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), and attention mechanisms. This method models the total cost of the cloud–edge–end collaboration system as a weighted sum function of the time delay and energy consumption of task execution. Then, with the objective of minimizing the total cost of the system, the task offloading problem is formulated as a mixed-integer joint optimization problem with offloading decision and bandwidth allocation, and two subproblems are decomposed from the optimization problem: one focuses on offloading decision and the other on bandwidth allocation. A CNN-LSTM-Attention neural network-based method is proposed to solve the optimal offloading decision efficiently. Based on this, the differential evolution algorithm is used to generate the optimal bandwidth allocation, resulting in efficient task offloading. The simulation experiment results demonstrate that our method enhances system performance and reduces the overall cost of the system, which is significantly better than existing methods."
pub.1125628255,Ensuring Flexibility and Security in SDN-Based Spacecraft Communication Networks through Risk Assessment,"Software-defined networking (SDN) has enabled elastic networking and resource distribution in cloud computing. The centralization and separation of the Control Plane also offers a high degree of network configurability and management, which can be used to mitigate and manage threats to the network. Space communication networks have historically been restricted and circuit switching in these networks has been a manual process. This study evaluates the potential role of SDN in space communication networks from a networking security standpoint. The evaluation covers the networking security needs of spacecraft missions and their associated assets. The results from the evaluation lead to a risk assessment that identifies vulnerabilities in an SDN-based communications architecture. Security challenges introduced into the network from integrating SDN are also considered. A risk register summarizes the severity of the attack outcomes, as well as occurrence likelihood. The study identifies Denial-of-Service (DoS) attacks as a new threat (presently unmitigated by existing security controls) that would be prevalent in an SDN-based space communication environment. A Mininet-based emulation testbed is built to demonstrate the susceptibility of spacecraft flight software to a flooding DoS attack when on an interconnected SDN-managed network. This type of attack would be highly consequential to mission assets, and therefore SDN-based space communications would need to be resilient to such attacks. Future work will need to be performed to fully characterize DoS attack methods that can apply to the space communication scenario, as well as to devise a comprehensive DoS-resilient solution."
pub.1168823341,Cloud Based Data Centre for Managing Logistics in the Disaster,"Considering the many man-made and natural disasters that have occurred in recent years, such as the September 2001 earthquake and the tsunami in Japan, the importance of problem solving processes cannot be ignored. The total damage caused by the disaster in Japan alone is estimated at more than $300 billion. Transportation and communications play an important role in disaster response and management to minimize loss of life, economic costs and impacts. Our research includes the development of emergency response systems for disasters of all sizes, focused on transportation systems designed to use and communicate information and technology. In this article, we discuss the use of technology for disaster management using smart transportation, including mobile, mobile and cloud. The system is intelligent because it can collect information from many places and locations, including the situation and make decisions regarding the use of the communication process, making the vehicle more efficient. Hybrid vehicle communications based on vehicle-to-vehicle and vehicle-to-infrastructure protocols are being exploited opportunistically. The effectiveness of our system has been demonstrated by simulating the impact of disasters on the real urban environment and comparing the situation with our existing disaster management systems. We declare that the adoption of our plan has led to the improvement and stabilization of traffic and evacuation."
pub.1167326731,Deep self-taught learning framework for intrusion detection in cloud computing environment,"Cloud has become a target-rich environment for malicious attacks by cyber intruders. Security is a major concern and remains an obstacle to the adoption of cloud computing. The intrusion detection system (IDS) is regarded as defense-in-depth. Unfortunately, most machine learning approaches designed for cloud intrusion detection require large amounts of labeled attack samples, but in real practice, they are limited. Therefore, the key impetus of this work is to introduce self-taught learning (STL) combining stacked sparse autoencoder (SSAE) with long short-term memory (LSTM) as a candidate solution to learn the robust feature representation and efficiently improve the performance of IDS with respect to false alarm rate (FAR) and detection rate (DR). Accordingly, the proposed approach as a first step employs SSAE to achieve dimensional reduction by learning the discriminative features from network traffic. The approach adopts LSTM to recognize the intrusion with the features encoded by SSAE. To evaluate the detective performance of our model, a comprehensive set of experiments are conducted on NSL-KDD. Also, ablation experiments are conducted to show the contribution of each component of our approach. Further, the comparative analysis shows the efficacy of our approach against the existing approaches with an accuracy of 86.31%."
pub.1160584053,Research on Blockchain-based Information System for Power business archives Co-construction,"With the continuous popularization of the emerging technology of “big cloud mobile intelligence chain”, the environment, object and content of power equipment operation and maintenance status and archive management had undergone tremendous changes. This paper discusses the existing technical solutions and shortcomings of the power business archives information system, introduces the blockchain technology, and designs the power business archives information system model on the Hyperledger Fabric alliance chain technology platform based on the natural compatibility between the power business archives information system and the blockchain technology, and constructs the integration architecture of the business system and the archives system. Build a closed loop management system for the whole process of business-archival, and realize the power business archives information system of joint construction, data management and information sharing. It improves the reliability of electronic archives and provides convenience for the management and use of Power business archives"
pub.1169240630,Modified genetic algorithm and fine-tuned long short-term memory network for intrusion detection in the internet of things networks with edge capabilities,"The emergence of smart cities is an example of how new technologies, such as the Internet of Things (IoT), have facilitated the creation of extensive interconnected and intelligent ecosystems. The widespread deployment of IoT devices has enabled the provision of constant environmental feedback, thereby facilitating the automated adaptation of associated systems. This has brought about a fundamental transformation in the way contemporary society functions. The security of emerging technologies such as IoT has become a significant challenge due to the added complexities, misconfigurations, and conflicts between modern and legacy systems. This challenge has a notable impact on the reliability and accessibility of existing infrastructure. Edge computing (EC) is a collaborative computing system that brings data processing and analysis closer to the edge of the network, where the data is generated, rather than in a centralized cloud environment. The utilization of the IoT has become more prevalent in both everyday life and the manufacturing sector, with a particular emphasis on critical infrastructure. The IoT is presently being utilized across diverse domains, including but not limited to industrial, agricultural, healthcare, and logistical sectors. The security of IoT networks has implications for the safety of individuals, the security of the nation, and economic development. Notwithstanding, conventional intrusion detection techniques that rely on centralized cloud-based systems that have been suggested in previous studies for IoT network security are insufficient to meet the requirements for data confidentiality, network capacity, and prompt responsiveness. In addition, the integration of IoT applications into smart devices has been shown to augment their functionalities. However, it is important to note that this integration also brings about potential security vulnerabilities. Furthermore, a significant number of contemporary IoT devices exhibit restricted security capabilities, rendering them vulnerable to intricate attacks and impeding the extensive integration of IoT technologies. Also, a lot of IoT network devices have been put in place that don't have hardware security measures. This means that traditional intrusion detection systems (IDS) aren't enough to protect the IoT network ecosystem. To address these issues, this research suggests the IoT-Defender framework, which combines a Modified Genetic Algorithm (MGA) model with a deep Long Short-Term Memory (LSTM) network to find cyberattacks in IoT networks. This research represents a pioneering attempt to employ the MGA for feature selection and the GA for fine-tuning the LSTM parameters within an EC framework. The parameters of the LSTM model were fine-tuned through the manipulation of the number of hidden layers, utilizing the GA fitness function. The customization of the MGA aimed to enhance its performance in selecting relevant features, optimizing the use of limited resources on IoT dev"
pub.1165183427,Optimized Provisioning Techniques for Geo-Distributed SDP-Enabled Next Generation Networks Security,"What advancements might Next Generation Networks (NGN) unleash that existing ones cannot? NGNs are envisioned to empower the connection between billions of people and zillions of heterogeneous Internet of Things (IoT) devices while consolidating intelligence and autonomy. However, several security-related challenges will be introduced and apparently, the security solutions and architectures used in previous network generations will not be sufficient. The Cloud Security Alliance's (CSA) Software Defined Perimeter (SDP) is a potential candidate to provide the much-needed security framework for next-generation networks. However, the lack of a scalable SDP controller will be a considerable drawback for the wide adoption of the SDP framework. Therefore, this paper focuses on modeling a multi-SDP controller placement problem as a VNF-FGE in a Geo-distributed NFV-based environment as a potential solution to secure next-generation networks. Due to its NP-hard nature, this type of problem can be addressed by extending the NCO approach via Reinforcement Learning (RL) to optimize the reward policy in accordance with the constraints of the problem. The agent developed can learn the placement decisions of the SDP controllers by inference (i.e., policy strategy) through the RL process. The experiment's analysis reveals the RL approach's superiority over the well-known Gecode optimization solver."
pub.1154994916,ARGUS – An Adaptive Smart Home Security Solution,"Smart Security Solutions are in high demand with the ever-increasing vulnerabilities within the IT domain. Adjusting to a Work-From-Home (WFH) culture has become mandatory by maintaining required core security principles. Therefore, implementing and maintaining a secure Smart Home System has become even more challenging. ARGUS provides an overall network security coverage for both incoming and outgoing traffic, a firewall and an adaptive bandwidth management system and a sophisticated CCTV surveillance capability. ARGUS is such a system that is implemented into an existing router incorporating cloud and Machine Learning (ML) technology to ensure seamless connectivity across multiple devices, including IoT devices at a low migration cost for the customer. The aggregation of the above features makes ARGUS an ideal solution for existing Smart Home System service providers and users where hardware and infrastructure is also allocated. ARGUS was tested on a small-scale smart home environment with a Raspberry Pi 4 Model B controller. Its intrusion detection system identified an intrusion with 96% accuracy while the physical surveillance system predicts the user with 81% accuracy."
pub.1138494325,Multi-Dependency and Time Based Resource Scheduling Algorithm for Scientific Applications in Cloud Computing,"Workflow scheduling is one of the significant issues for scientific applications among virtual machine migration, database management, security, performance, fault tolerance, server consolidation, etc. In this paper, existing time-based scheduling algorithms, such as first come first serve (FCFS), min–min, max–min, and minimum completion time (MCT), along with dependency-based scheduling algorithm MaxChild have been considered. These time-based scheduling algorithms only compare the burst time of tasks. Based on the burst time, these schedulers, schedule the sub-tasks of the application on suitable virtual machines according to the scheduling criteria. During this process, not much attention was given to the proper utilization of the resources. A novel dependency and time-based scheduling algorithm is proposed that considers the parent to child (P2C) node dependencies, child to parent node dependencies, and the time of different tasks in the workflows. The proposed P2C algorithm emphasizes proper utilization of the resources and overcomes the limitations of these time-based schedulers. The scientific applications, such as CyberShake, Montage, Epigenomics, Inspiral, and SIPHT, are represented in terms of the workflow. The tasks can be represented as the nodes, and relationships between the tasks can be represented as the dependencies in the workflows. All the results have been validated by using the simulation-based environment created with the help of the WorkflowSim simulator for the cloud environment. It has been observed that the proposed approach outperforms the mentioned time and dependency-based scheduling algorithms in terms of the total execution time by efficiently utilizing the resources."
pub.1016895796,Cryptographically Enforced Data Access Control in Personal Health Record Systems,"Personal Health Record (PHR) systems play a vital role during digital transformation of healthcare. These systems provide many value-added features like viewing one's health related information, secure transmission and tracking of that information with the health service providers. A cloud assisted PHR system maximizes the possibility for PHR systems to interoperate with other systems in health information management environments. Each patient needs to encrypt his/her PHR data before uploading it in the cloud since the patients will lose their physical access to their health data stored in cloud servers. Moreover, to achieve fine-grained data access control on encrypted PHR data in an effective and scalable manner is a challenging task. Since there are multiple owners or patients are available in a PHR system and existing data access control schemes are mostly designed for the single-authority/owner scenarios, a novel patient-centric data access control scheme called Revocable Multi Authority Attribute Set Based Encryption (R- MA- ASBE) is proposed. The proposed scheme inherits flexibility, scalability and fine-grained patient-centric data access control."
pub.1142804582,When Intelligent Transportation Systems Sensing Meets Edge Computing: Vision and Challenges,"The widespread use of mobile devices and sensors has motivated data-driven applications that can leverage the power of big data to benefit many aspects of our daily life, such as health, transportation, economy, and environment. Under the context of smart city, intelligent transportation systems (ITS), as a main building block of modern cities, and edge computing (EC), as an emerging computing service that targets addressing the limitations of cloud computing, have attracted increasing attention in the research community in recent years. It is well believed that the application of EC in ITS will have considerable benefits to transportation systems regarding efficiency, safety, and sustainability. Despite the growing trend in ITS and EC research, a big gap in the existing literature is identified: the intersection between these two promising directions has been far from well explored. In this paper, we focus on a critical part of ITS, i.e., sensing, and conducting a review on the recent advances in ITS sensing and EC applications in this field. The key challenges in ITS sensing and future directions with the integration of edge computing are discussed."
pub.1017598407,Analysis of control-theoretic predictive strategies for the adaptation of distributed parallel computations,"In adaptive distributed parallel applications the adaptation process is based on the ability to change some characteristics of parallel components, such as the parallelism form and the parallelism degree, in response to unexpected execution conditions. Although existing research work has studied this problem, it is of increasing importance to investigate adaptation strategies able to reach important properties like the stability of control decisions, i.e. to guarantee that reconfigurations are effective and durable, and control optimality, expressed by means of cooperative and non-cooperative agreements between decisions of different controllers. These properties are crucial in distributed environments like Grids and Clouds, where reconfigurations imply a cost both in terms of a performance degradation as well as a monetary charge. In this paper we briefly introduce the basic ideas of our methodology and we introduce different adaptation strategies based on alternative formulations of the Model-based Predictive Control technique. First hints about the effectiveness of our approach are discussed through experiments developed in a simulation environment."
pub.1024407176,Multiple Vehicle-like Target Tracking Based on the Velodyne LiDAR*,"This paper proposes a novel multiple vehicle-like target tracking method based on a Velodyne HDL64E light detection and ranging (LiDAR) system. The proposed method combines multiple hypothesis tracking (MHT) algorithm with dynamic point cloud registration (DPCR), which is able to solve the multiple vehicle-like target tracking in highly dynamic urban environments without any auxiliary information from GPS or IMU. Specifically, to track targets consistently, the DPCR is developed to calculate accurately the pose of the ego-vehicle for the transformation of raw measurements taken in the moving coordinate systems into a static absolute coordinate system; while in turn, MHT helps to improve the performance of DPCR by discriminating and removing the dynamic points from the scene. Furthermore, the proposed MHT method is also able to solve the occlusion problem existing in the point cloud. Experiments on sets of urban environments prove that the presented method is effective and robust, even in highly dynamic environments."
pub.1112384134,Hierarchical data fusion for Smart Healthcare,"The Internet of Things (IoT) facilitates creation of smart spaces by converting existing environments into sensor-rich data-centric cyber-physical systems with an increasing degree of automation, giving rise to Industry 4.0. When adopted in commercial/industrial contexts, this trend is revolutionising many aspects of our everyday life, including the way people access and receive healthcare services. As we move towards Healthcare Industry 4.0, the underlying IoT systems of Smart Healthcare spaces are growing in size and complexity, making it important to ensure that extreme amounts of collected data are properly processed to provide valuable insights and decisions according to requirements in place. This paper focuses on the Smart Healthcare domain and addresses the issue of data fusion in the context of IoT networks, consisting of edge devices, network and communications units, and Cloud platforms. We propose a distributed hierarchical data fusion architecture, in which different data sources are combined at each level of the IoT taxonomy to produce timely and accurate results. This way, mission-critical decisions, as demonstrated by the presented Smart Healthcare scenario, are taken with minimum time delay, as soon as necessary information is generated and collected. The proposed approach was implemented using the Complex Event Processing technology, which natively supports the hierarchical processing model and specifically focuses on handling streaming data ‘on the fly’—a key requirement for storage-limited IoT devices and time-critical application domains. Initial experiments demonstrate that the proposed approach enables fine-grained decision taking at different data fusion levels and, as a result, improves the overall performance and reaction time of public healthcare services, thus promoting the adoption of the IoT technologies in Healthcare Industry 4.0."
pub.1118876445,The VISCACHA survey - I. Overview and First Results,"The VISCACHA (VIsible Soar photometry of star Clusters in tApii and Coxi
HuguA) Survey is an ongoing project based on deep photometric observations of
Magellanic Cloud star clusters, collected using the SOuthern Astrophysical
Research (SOAR) telescope together with the SOAR Adaptive Module Imager. Since
2015 more than 200 hours of telescope time were used to observe about 130
stellar clusters, most of them with low mass (M < 10$^4$ M$_\odot$) and/or
located in the outermost regions of the Large Magellanic Cloud and the Small
Magellanic Cloud. With this high quality data set, we homogeneously determine
physical properties from statistical analysis of colour-magnitude diagrams,
radial density profiles, luminosity functions and mass functions. Ages,
metallicities, reddening, distances, present-day masses, mass function slopes
and structural parameters for these clusters are derived and used as a proxy to
investigate the interplay between the environment in the Magellanic Clouds and
the evolution of such systems. In this first paper we present the VISCACHA
Survey and its initial results, concerning the SMC clusters AM3, K37, HW20 and
NGC796 and the LMC ones KMHK228, OHSC3, SL576, SL61 and SL897, chosen to
compose a representative subset of our cluster sample. The project's long term
goals and legacy to the community are also addressed."
pub.1111952295,The VISCACHA survey – I. Overview and first results,"The VISCACHA (VIsible Soar photometry of star Clusters in tApii and Coxi HuguA) Survey is an ongoing project based on deep photometric observations of Magellanic Cloud star clusters, collected using the SOuthern Astrophysical Research (SOAR) telescope together with the SOAR Adaptive Module Imager. Since 2015 more than 200 h of telescope time were used to observe about 130 stellar clusters, most of them with low mass (M < 104 M⊙) and/or located in the outermost regions of the Large Magellanic Cloud and the Small Magellanic Cloud. With this high-quality data set, we homogeneously determine physical properties from statistical analysis of colour–magnitude diagrams, radial density profiles, luminosity functions, and mass functions. Ages, metallicities, reddening, distances, present-day masses, mass function slopes, and structural parameters for these clusters are derived and used as a proxy to investigate the interplay between the environment in the Magellanic Clouds and the evolution of such systems. In this first paper we present the VISCACHA Survey and its initial results, concerning the SMC clusters AM3, K37, HW20, and NGC 796 and the LMC ones KMHK228, OHSC3, SL576, SL61, and SL897, chosen to compose a representative subset of our cluster sample. The project’s long-term goals and legacy to the community are also addressed."
pub.1125917400,Hybrid Overlap Filter for LiDAR Point Clouds Using Free Software,"Despite the large amounts of resources destined to developing filtering algorithms of LiDAR point clouds in order to obtain a Digital Terrain Model (DTM), the task remains a challenge. As a society advancing towards the democratization of information and collaborative processes, the researchers should not only focus on improving the efficacy of filters, but should also consider the users’ needs with a view toward improving the usability and accessibility of the filters in order to develop tools that will provide solutions to the challenges facing this field of study. In this work, we describe the Hybrid Overlap Filter (HyOF), a new filtering algorithm implemented in the free R software environment. The flow diagram of HyOF differs in the following ways from that of other filters developed to date: (1) the algorithm is formed by a combination of sequentially operating functions (i.e., the output of the first function provides the input of the second), which are capable of functioning independently and thus enabling integration of these functions with other filtering algorithms; (2) the variable penetrability is defined and used, along with slope and elevation, to identify ground points; (3) prior to selection of the seed points, the original point cloud is processed with the aim of removing points corresponding to buildings; and (4) a new method based on a moving window, with longitudinal overlap between windows and transverse overlap between passes, is used to select the seed points. Our hybrid filtering method is tested using 15 reference samples acquired by the International Society of Photogrammetry and Remote Sensing (ISPRS) and is evaluated in comparison with 33 existing filtering algorithms. The results show that our hybrid filtering method produces an average total error of 3.34% and an average Kappa coefficient of 92.62%. The proposed algorithm is one of the most accurate filters that has been tested with the ISPRS reference samples."
pub.1092925537,GRACC: New generation of the OSG accounting,"Throughout the last decade the Open Science Grid (OSG) has been fielding requests from user communities, resource owners, and funding agencies to provide information about utilization of OSG resources. Requested data include traditional accounting - core-hours utilized - as well as users certificate Distinguished Name, their affiliations, and field of science. The OSG accounting service, Gratia, developed in 2006, is able to provide this information and much more. However, with the rapid expansion and transformation of the OSG resources and access to them, we are faced with several challenges in adapting and maintaining the current accounting service. The newest changes include, but are not limited to, acceptance of users from numerous university campuses, whose jobs are flocking to OSG resources, expansion into new types of resources (public and private clouds, allocation-based HPC resources, and GPU farms), migration to pilot-based systems, and migration to multicore environments. In order to have a scalable, sustainable and expandable accounting service for the next few years, we are embarking on the development of the next-generation OSG accounting service, GRACC, that will be based on open-source technology and will be compatible with the existing system. It will consist of swappable, independent components, such as Logstash, Elasticsearch, Grafana, and RabbitMQ, that communicate through a data exchange. GRACC will continue to interface EGI and XSEDE accounting services and provide information in accordance with existing agreements. We will present the current architecture and working prototype."
pub.1095056689,Data Allocation in Scalable Distributed Database Systems Based on Time Series Forecasting,"In cloud computing environments, database systems have to serve a large number of tenants instantaneously and handle applications with different load characteristics. To provide a high quality of services, scalable distributed database systems with self-provisioning are required. The number of working nodes is adjusted dynamically based on user demand. Data fragments are reallocated frequently for node number adjustment and load balancing. The problem of data allocation is different from that in traditional distributed database systems, and therefore existing algorithms may not be applicable. In this paper, we first formally define the problem of data allocation in scalable distributed database systems. Then, we propose an algorithm for the problem. The algorithm makes use of time series models to perform short-term load forecasting such that node number adjustment and fragment reallocation can be performed in advance to avoid node over loadings and performance degradation due to fragment migrations. In addition, excessive working nodes can be minimized for resource-saving."
pub.1034767578,A Cloud Service Control Approach for Distributed and Adaptive Equipment Control in Cloud Environments,"A developing trend within the manufacturing shop-floor domain is the move of manufacturing activities into cloud environments, as scalable, on-demand and pay-per-usage cloud services. This will radically change traditional manufacturing, as borderless, distributed and collaborative manufacturing missions between volatile, best suited groups of partners will impose a multitude of advantages. The evolving Cloud Manufacturing (CM) paradigm will enable this new manufacturing concept, and on-going research has described many of its anticipated core virtues and enabling technologies. However, a major key enabling technology within CM which has not yet been fully addressed is the dynamic and distributed planning, control and execution of scattered and cooperating shop-floor equipment, completing joint manufacturing tasks.In this paper, the technological perspective for a cloud service-based control approach is described, and how it could be implemented. Existing manufacturing resources, such as soft, hard and capability resources, can be packaged as cloud services, and combined to create different levels of equipment or manufacturing control, ranging from low-level control of single machines or devices (e.g. Robot Control-as-a-Service), up to the execution of high level multi-process manufacturing tasks (e.g. Manufacturing-as-a-Service). A multi-layer control approach, featuring adaptive decision-making for both global and local environmental conditions, is proposed. This is realized through the use of a network of intelligent and distributable decision modules such as event-driven Function Blocks, enabling run-time manufacturing activities to be performed according to actual manufacturing conditions. The control system's integration to the CM cloud service management functionality is also described."
pub.1160508737,Graph neural network-based spatio-temporal indoor environment prediction and optimal control for central air-conditioning systems,"Model-based optimal control has proven its effectiveness in optimizing the performance of central air-conditioning systems in terms of thermal comfort and energy efficiency. It was often assumed that temperature distribution in the entire air-conditioned space is uniform and can be represented by a single or averaged measurement in optimization. However, actual distribution in the air-conditioned space is usually uneven, which can affect thermal comfort and indoor air quality. The dynamics of air conditioning systems and their interactions with the built environment exist in both time and space domains. Conventional data-driven approaches typically concentrate on the temporal correlations only among building operation data, while neglecting the spatial correlations. This study proposed a spatio-temporal data-driven methodology for optimal control of central air conditioning systems, which aims to address the challenging issue of uneven spatial distributions of environmental parameters in air-conditioned space. The methodology consists of graph-based multi-source data integration, graph neural network-based indoor environment modeling and spatio-temporal model-based online optimal control. The proposed methodology is tested on real air handling unit-variable air volume (AHU-VAV) system in a high-rise office building through a cloud-based platform. Spatio-temporal data from building automation system (BAS), internet of things (IoT) devices and building information modeling (BIM) are integrated and organized as graph structure. Graph neural network models are developed for predicting the evolution of indoor air temperature distribution under different control settings. The developed graph neural network-recurrent neural network (GNN-RNN) model architectures show enhanced accuracy than conventional deep learning models (i.e., convolutional neural network-recurrent neural network (CNN-RNN), Dense-RNN). And the cloud-based online test demonstrates that the proposed model-based control strategy improves the percentage of time achieving Grade I thermal comfort from 36.5% (i.e., existing control strategy) to 81.3%."
pub.1174437265,A Scalable and Secure Outsourced IoT Electronic Health Records with Efficient User Revocation Using Fog-Assisted Cloud Model,"The management of Electronic Health Records (EHRs) from IoT devices has become essential with the integration of cloud computing and IoT technology in healthcare sector. Existing research works adopt cloud-based access control with the encryption solution to secure the outsourced EHRs. However, when it comes to the IoT cloud data sharing environment, where data originated from numerous devices and the authorization status of users is subject to frequent changes, there remains a gap in achieving secure and lightweight IoT data encryption and fine-grained sharing with efficient user revocation procedures. This paper proposes a cloud-based access control scheme with efficient and scalable revocation to support fine-grained, secure, and efficient revocation for IoT based EHRs based on Ciphertext policy Attribute-based encryption (CP-ABE) and fog-assisted computing. The scheme introduces IoT data encryption and secure aggregation algorithm. In this scheme, the concept of outsourced encryption and decryption is leveraged to enable secure and lightweight access control in fog-cloud computing. Additionally, a novel revocation protocol is devised along with the use of graph database to enable efficient user revocation in a highly evolvable access control system. Finally, comparative analysis is conducted to demonstrate the security and efficiency of the proposed scheme."
pub.1133613572,Online Service Migration in Mobile Edge with Incomplete System Information: A Deep Recurrent Actor-Critic Learning Approach,"Multi-access Edge Computing (MEC) is an emerging computing paradigm that
extends cloud computing to the network edge to support resource-intensive
applications on mobile devices. As a crucial problem in MEC, service migration
needs to decide how to migrate user services for maintaining the
Quality-of-Service when users roam between MEC servers with limited coverage
and capacity. However, finding an optimal migration policy is intractable due
to the dynamic MEC environment and user mobility. Many existing studies make
centralized migration decisions based on complete system-level information,
which is time-consuming and also lacks desirable scalability. To address these
challenges, we propose a novel learning-driven method, which is user-centric
and can make effective online migration decisions by utilizing incomplete
system-level information. Specifically, the service migration problem is
modeled as a Partially Observable Markov Decision Process (POMDP). To solve the
POMDP, we design a new encoder network that combines a Long Short-Term Memory
(LSTM) and an embedding matrix for effective extraction of hidden information,
and further propose a tailored off-policy actor-critic algorithm for efficient
training. The extensive experimental results based on real-world mobility
traces demonstrate that this new method consistently outperforms both the
heuristic and state-of-the-art learning-driven algorithms and can achieve
near-optimal results on various MEC scenarios."
pub.1175546581,An Efficient Observational Strategy for the Detection of the Oort Cloud,"The Oort cloud is presumably a pristine relic of the solar system formation. Detection of the Oort cloud may provide information regarding the stellar environment in which the Sun was born and on the planetesimal population during the outer planets’ formation phase. The best suggested approach for detecting Oort cloud objects in situ, is by searching for subsecond occultations of distant stars by these objects. Following Brown & Webster, we discuss the possibility of detecting Oort cloud objects by observing near the quadrature direction. Due to the Earth’s projected velocity, the occultations are longer near the quadrature direction and are therefore easier to detect, but have lower rate. We show that, for ≲1 m size telescopes, the increased exposure time will result in about one to 3 orders of magnitude increase in the number of detectable stars that have an angular size smaller than the Fresnel scale and are therefore suitable for an occultation search. We discuss the ability of this method to detect Oort cloud objects using existing survey telescopes, and we estimate the detection rate as a function of the power-law index of the size distribution of the Oort cloud objects and their distance from the Sun. We show that occultations detected using ≈1 s integration by ≲1 m telescopes at the optimal region near the quadrature points will be marginally dominated by Oort cloud objects rather than Kuiper belt objects."
pub.1044342063,A light-weight secure information transmission and device control scheme in integration of CPS and cloud computing,"Recently, cloud computing and cyber-physical system (CPS) are definitely basic elements in real industrial field. In particular, security is a mandatory factor for communications and operations in these environments. However, the existing CPS security mechanism is not suitable to the telecommunication framework provided by the standards. In addition, random number function of high entropy must be used to enhance security with encrypted communications and must support perfect secrecy. Random number functions supported by the devices instead of servers do not have sufficient entropy. Entropy injection and seed replacement are also impractical. In this paper, we propose a security scheme which provides light-weight secure CPS information transmission and device control scheme in integration of CPS and cloud computing. In this scheme, a light-weight security scheme can multicast event information to users who have heterogeneous device information access authorities based on oneM2M standards, and also be able to manage the control devices. This paper provides performance analysis of proposed scheme and confirms its security and efficiency."
pub.1150102234,Online Service Migration in Mobile Edge With Incomplete System Information: A Deep Recurrent Actor-Critic Learning Approach,"Multi-access Edge Computing (MEC) is an emerging computing paradigm that extends cloud computing to the network edge to support resource-intensive applications on mobile devices. As a crucial problem in MEC, service migration needs to decide how to migrate user services for maintaining the Quality-of-Service when users roam between MEC servers with limited coverage and capacity. However, finding an optimal migration policy is intractable due to the dynamic MEC environment and user mobility. Many existing studies make centralized migration decisions based on complete system-level information, which is time-consuming and also lacks desirable scalability. To address these challenges, we propose a novel learning-driven method, which is user-centric and can make effective online migration decisions by utilizing incomplete system-level information. Specifically, the service migration problem is modeled as a Partially Observable Markov Decision Process (POMDP). To solve the POMDP, we design a new encoder network that combines a Long Short-Term Memory (LSTM) and an embedding matrix for effective extraction of hidden information, and further propose a tailored off-policy actor-critic algorithm for efficient training. The extensive experimental results based on real-world mobility traces demonstrate that this new method consistently outperforms both the heuristic and state-of-the-art learning-driven algorithms and can achieve near-optimal results on various MEC scenarios."
pub.1170502103,Enhanced CNN-DCT Steganography: Deep Learning-Based Image Steganography Over Cloud,"Image steganography plays a pivotal role in secure data communication and confidentiality protection, particularly in cloud-based environments. In this study, we propose a novel hybrid approach, CNN-DCT Steganography, which combines the power of convolutional neural networks (CNNs) and discrete cosine transform (DCT) for efficient and secure data hiding within images over cloud storage. The proposed method capitalizes on the robust feature extraction capabilities of CNNs and the spatial frequency domain transformation of DCT to achieve imperceptible embedding and enhanced data-hiding capacity. In the proposed CNN-DCT Steganography approach, the cover image undergoes a two-step process. First, feature extraction using a deep CNN enables the selection of appropriate regions for data embedding, ensuring minimal visual distortions. Next, the selected regions are subjected to the DCT-based steganography technique, where secret data is seamlessly embedded into the image, rendering it visually indistinguishable from the original. To evaluate the effectiveness of our approach, extensive experiments are conducted using a diverse dataset comprising 500 high-resolution images. Comparative analysis with existing steganography methods demonstrates the superiority of the proposed CNN-DCT Steganography approach. The results showcase higher data hiding capacity, superior visual quality with an MSE of 112.5, steganalysis resistance with a false positive rate of 2.1%, and accurate data retrieval with a bit error rate of 0.028. Furthermore, the proposed method exhibits robustness against common image transformations, ensuring the integrity of the concealed data even under various modifications. Moreover, the computational efficiency of our approach is demonstrated by a competitive execution time of 2.3 s, making it feasible for real-world cloud-based applications. The combination of deep learning techniques and DCT-based steganography ensures a balance between security and visual quality, making our approach ideal for scenarios where data confidentiality and authenticity are paramount. In conclusion, the CNN-DCT Steganography approach represents a significant advancement in image steganography over cloud storage. Its capability to efficiently hide data, maintain visual fidelity, resist steganalysis, and withstand image transformations positions it as a promising solution for secure image communication and sharing. By continuously refining and extending this hybrid model, we pave the way for enhanced data protection and secure cloud-based information exchange in the digital era."
pub.1148322571,A Predictive Checkpoint Technique for Iterative Phase of Container Migration,"Cloud computing is a cost-effective method of delivering numerous services in Industry 4.0. The demand for dynamic cloud services is rising day by day and, because of this, data transit across the network is extensive. Virtualization is a significant component and the cloud servers might be physical or virtual. Containerized services are essential for reducing data transmission, cost, and time, among other things. Containers are lightweight virtual environments that share the host operating system’s kernel. The majority of businesses are transitioning from virtual machines to containers. The major factor affecting the performance is the amount of data transfer over the network. It has a direct impact on the migration time, downtime and cost. In this article, we propose a predictive iterative-dump approach using long short-term memory (LSTM) to anticipate which memory pages will be moved, by limiting data transmission during the iterative phase. In each loop, the pages are shortlisted to be migrated to the destination host based on predictive analysis of memory alterations. Dirty pages will be predicted and discarded using a prediction technique based on the alteration rate. The results show that the suggested technique surpasses existing alternatives in overall migration time and amount of data transmitted. There was a 49.42% decrease in migration time and a 31.0446% reduction in the amount of data transferred during the iterative phase."
pub.1175429772,An efficient observational strategy for the detection of the Oort cloud,"The Oort cloud is presumably a pristine relic of the Solar System formation.
Detection of the Oort cloud may provide information regarding the stellar
environment in which the Sun was born and on the planetesimal population during
the outer planets' formation phase. The best suggested approach for detecting
Oort cloud objects in situ, is by searching for sub-second occultations of
distant stars by these objects. Following Brown & Webster, we discuss the
possibility of detecting Oort cloud objects by observing near the quadrature
direction. Due to the Earth's projected velocity, the occultations are longer
near the quadrature direction and are therefore easier to detect, but have
lower rate. We show that, for <1-m size telescopes, the increased exposure time
will result in about one to three orders of magnitude increase in the number of
detectable stars that have an angular size smaller than the Fresnel scale and
are therefore suitable for an occultation search. We discuss the ability of
this method to detect Oort cloud objects using existing survey telescopes, and
we estimate the detection rate as a function of the power-law index of the size
distribution of the Oort cloud objects and their distance from the Sun. We show
that occultations detected using ~1-s integration by <1-m telescopes at the
optimal region near the quadrature points will be marginally dominated by Oort
cloud objects rather than Kuiper belt objects."
pub.1130016270,The Future of the Digital Enterprise,"Chapter 12 reviews the progress the ProcIndustries team has made toward operational excellence and some of the plans they have for initiatives such as cloud strategy, Internet of things (IoT) sensor integration, edge analytics, the refinery and control system of the future, and collaborative manufacturing. Many companies that have succeeded with company-wide EIDI deployments, have reduced costs, improved their processes, and are keeping their equipment in peak operating condition. These companies must now make decisions in cloud planning and deployment, using IoT devices to augment their existing information, pursuing autonomous operation of plants and equipment, and moving information beyond their firewalls and protected domains into an emerging community-based information technology (IT) environment. By embracing and integrating emerging digital technologies, a business transformation, enabled by vision, hard work, and improved workflows can make a tremendous improvement to a company’s financial health, the physical health and safety of its communities, the relationships with its customers and suppliers, its reputation as a thought leader in its industry, and the morale of its employees by being on a winning team."
pub.1061256955,Evaluation and optimization of the mixed redundancy strategy in cloud-based systems,"Mixed redundancy strategies are generally used in cloud-based systems, with different node switch mechanisms from traditional fault-tolerant strategies. Existing studies often concentrate on optimizing a single strategy in cloud computing environment and ignore the impact of mixed redundancy strategies. Therefore, a model is proposed to evaluate and optimize the reliability and performance of cloud-based degraded systems subject to a mixed active and cold standby redundancy strategy. In this strategy, node switching is triggered by a continual monitoring and detection mechanism when active nodes fail. To evaluate the transient availability and the expected job completion rate of systems with such kind of strategy, a continuous-time Markov chain model is built on the state transition process and a numerical method is used to solve the model. To choose the optimal redundancy for the mixed strategy under system constraints, a greedy search algorithm is proposed after sensitivity analysis. Illustrative examples were presented to explain the process of calculating the transient probability of each system state and in turn, the availability and performance of the whole system. It was shown that the near-optimal redundancy solution could be obtained using the optimization method. The comparison with optimization of the traditional mixed redundancy strategy proved that the system behavior was different using different kinds of mixed strategies and less redundancy was assigned for the new type of mixed strategy under the same system constraint."
pub.1119982254,A Reinforcement Learning Scheduling Strategy for Parallel Cloud-Based Workflows,"Scientific experiments can be modeled as Workflows. Such Workflows are usually computing- and data-intensive, demanding the use of High-Performance Computing environments such as clusters, grids, and clouds. This latter offers the advantage of elasticity, which allows for increasing and/or decreasing the number of Virtual Machines (VMs)on demand. Workflows are typically managed using Scientific Workflow Management Systems (SWfMS). Many existing SWfMSs offer support for cloud-based execution. Each SWfMS has its own scheduler that follows a well-defined cost function. However, such cost functions must consider the characteristics of a dynamic environment, such as live migrations and/or performance fluctuations, which are far from trivial to model. This paper proposes a novel scheduling strategy, named ReASSIgN, based on Reinforcement Learning (RL). By relying on an RL technique, one may assume that there is an optimal (or sub-optimal)solution for the scheduling problem, and aims at learning the best scheduling based on previous executions in the absence of a mathematical model of the environment. For this, an extension of a well-known workflow simulator WorkflowSim is proposed to implement an RL strategy for scheduling workflows. Once the scheduling plan is generated, the workflow is executed in the cloud using SciCumulus SWfMS. We conducted a thorough evaluation of the proposed scheduling strategy using a real astronomy workflow."
pub.1170989532,"Challenges, Attacks, and Countermeasures for Security in MANETs-IoT","The integration of Mobile Ad hoc Networks and the Internet of Things offers innovative applications but also presents significant security challenges. This paper explores these challenges, considering the decentralized nature of both MANETs and IoT. Security vulnerabilities, including unauthorized access and malicious attacks, emerge due to factors like node mobility and resource constraints. Existing security mechanisms are examined and found insufficient for this combined environment. The primary security issues are confidentially, integrity, availability, blackhole, wormhole, grayhole, rushing, flooding, attacks, authentication, intrusion detection, and trust management tailored to MANET-IoT systems. The urgent need for comprehensive security strategies to ensure safe operation of this ecosystem is emphasized."
pub.1153942403,A novel multi-objective service composition architecture for blockchain-based cloud manufacturing,"Abstract In recent years, many core technologies of Industry 4.0 have advanced significantly, particularly the integration of big data technology and cloud manufacturing (CMfg). The decentralization and traceability features of blockchain technology (BCT) provide an effective solution to provide trusted resource service in CMfg. Service composition is a core issue of CMfg to increase the value of digital assets. However, existing research on service composition based on BCT suffers from both the blockchain proof-of-work (PoW) mechanism and the service composition problem need to consume large computational overheads, as well as the blockchain fork problem affecting the system’s reliability, which reduces the usefulness of these schemes. To solve these problems, this paper proposes a novel multi-objective service composition architecture for blockchain-based CMfg (MOSC-BBCM). In MOSC-BBCM, first, a blockchain-chained storage structure is designed for the actual manufacturing cloud service constraint and scale dynamic changes, which can fully use the historical service information and accelerate the search for high-quality solutions. Second, to reduce the squandered computing resources in the PoW, a mining mechanism based on multi-objective service composition and optimal selection is proposed, where miners competitively solve a nondeterminate polynomial-hard problem to replace the mathematical puzzle. Finally, an incentive mechanism based on the environment selection method is proposed, which can avoid the fork problem while distributing on a labor basis. The effectiveness of the proposed MOSC-BBCM is verified in simulated numerical experiments of CMfg, which shows that the architecture provides a flexible and configurable scheme for blockchain-based CMfg with high availability."
pub.1113937133,Dynamic network slice resources reconfiguration in heterogeneous mobility environments,"This paper proposes a framework that optimizes network slicing provisioning in over‐the‐top (OTT) scenarios, by reducing occupied resources of slices from where the User Equipment (UE) handovers from. To achieve this, the framework leverages an existing Software Defined Networking (SDN)‐based UE virtualization scheme, which allows to instantiate in the cloud a representation of the physical device and its current connectivity context. This scheme was extended with the ability to anchor the UE's datapath and mask the underlying slices in a single end‐to‐end slice, allowing handover mechanisms between slices to become transparent to involved endpoints. This framework was implemented and evaluated in an experimental testbed where the UE moves between Wi‐Fi and long‐term evolution (LTE) slices, with results showing that upstream and downstream throughput is dynamically adapted to the UE requirements prior to the handover."
pub.1150832247,"Comparative Analysis of Quality of Service and Performance of MPLS, EoIP and SD-WAN","Software Defined – Wide Area Network (SDWAN) implementation is growing each year as one of the options for enterprise to have hybrid and redundant connection between traditional WAN and Internet. The cloud computing services, whether it is IaaS, PaaS, or SaaS has attracted most of enterprise to separate the corporate data connection from private WAN to public internet securely. This dual data traffic still can be managed by enterprise router, but it will require manual routing or at lease delay with complicated rule to mitigate any link problem. SD-WAN as the development of SDN in wide area network, have the solution to solve the manual routing data, by putting the control plane in a software environment to manage the data traffic virtually. In Indonesia, the SD-WAN technology has been introduced by several vendors and operators, some enterprise still reluctance considering the security of enterprise data through public internet service, and some of them still questioning the Quality of Services compare to legacy or traditional WAN services. Therefore, this research will perform the Quality of Service and Performance of SD-WAN, compare to traditional Multiprotocol Label Switching (MPLS) link and Ethernet over Internet Protocol (EoIP) as one of the contenders. The object of the research is an active WAN of one of Indonesian company, that having those three connections between Jakarta and Surabaya. The QoS and performance are measured using ITU- T G.1010 standard as the reference."
pub.1131000124,Efficient and Improved Navigation Application for Smart Devices in IoT Environment,"Smart cities are not merely the infusion of technology into a city’s infrastructure, but also require citizens interacting with their urban environment in a smart and informed manner. Transportation is key aspect of smart cities, where new route-planning applications provide as connected Car applications. For example, On-Board Human-Machine Interface (HMI) Apps, smartphone apps, API development, OBD-II, and SDL. Moreover, by using proprietary mobile apps, it is possible to get the GPS coordinates of a car, trace its route, open its doors, start its engine, and turn on its auxiliary devices. In addition, to study the usage of ICT to enhance quality and performance of urban services, reduce costs and resource consumption, and engage more effectively and actively with its citizens. The IoT is enabled by the latest developments in RFID, smart sensors, communication technologies, and Internet protocols. The basic premise is to have smart sensors collaborate directly without human involvement to deliver a new class of applications. The current revolution in Internet, mobile, and machine-to-machine (M2M) technologies can be seen as the first phase of the IoT. In the coming years, the IoT is expected to bridge diverse technologies to enable new applications by connecting physical objects together in support of intelligent decision making. This paper presents an overview of technical details that pertain to the IoT enabling technologies and applications. Compared to other survey papers in the field, our objective is to provide a more thorough summary of the most relevant application issues to enable researchers and application developers to get up to speed quickly on how ICT provide new route-planning application using IoT. We also provide an overview of some of the key IoT and M2M challenges presented in the recent literature and provide a summary of related research work. Moreover, we explore the relation between the IoT and M2M technologies. Also, present the need for better horizontal integration among IoT services. Finally, we present detailed service use-cases to illustrate how the different solutions presented in the paper fit together to deliver desired IoT & M2M services. Since more devices are connected to the Internet every day and for various purposes, this will generate increasing amounts of data that will need to be crunched. New and better analytics systems will have to be developed, not to mention cloud storage space to deal with all the data."
pub.1149411403,Digital Administration of the Project Based on the Concept of Smart Construction,"This study is devoted to the problem of updating the administration system of a construction project based on the integration of the concepts of process-oriented and project-oriented management in the context of the implementation of the latest technologies and digital models of smart construction, building information modeling (BIM) and the Internet of Things (IoT). In accordance with this significant transformations are subject not only to the operating system of enterprise management, but also to the course of business processes in the management environment of an investment and construction project. The result of the study is a model of a digital design company for construction lifecycle management. Such a model for managing a virtual design enterprise will optimize and reduce the costs of existing operating systems for managing a construction enterprise. The concept of introducing a life cycle management system for construction objects using BIM in the areas of reconstruction projects is proposed. With further development and implementation, this industry-specific BIM platform management model can form an information technology integration framework for reorganizing the business processes of a construction enterprise. The basic structure of the BIM platform is described, consisting of four components. There are cloud computing, big data analytics, Internet of Things and Blockchain information technologies. The need for the evolution of IoT information technologies from intelligent things to an intelligent planet is shown. In the future, it is planned to develop an integrated innovative structure of a holding type that supports the digital transformation of the operating system of enterprise management in the construction industry."
pub.1134892249,An incremental reinforcement learning scheduling strategy for data‐intensive scientific workflows in the cloud,"Summary  Most scientific experiments can be modeled as workflows. These workflows are usually computing‐ and data‐intensive, demanding the use of high‐performance computing environments such as clusters, grids, and clouds. This latter offers the advantage of the elasticity, which allows for changing the number of virtual machines (VMs) on demand. Workflows are typically managed using scientific workflow management systems (SWfMS). Many existing SWfMSs offer support for cloud‐based execution. Each SWfMS has its scheduler that follows a well‐defined cost function. However, such cost functions should consider the characteristics of a dynamic environment, such as live migrations or performance fluctuations, which are far from trivial to model. This article proposes a novel scheduling strategy, named  ReASSIgN  , based on reinforcement learning (RL). By relying on an RL technique, one may assume that there is an optimal (or suboptimal) solution for the scheduling problem, and aims at learning the best scheduling based on previous executions in the absence of a mathematical model of the environment. For this, an extension of a well‐known workflow simulator WorkflowSim is proposed to implement an RL strategy for scheduling workflows. Once the scheduling plan is generated via simulation, the workflow is executed in the cloud using SciCumulus SWfMS. We conducted a throughout evaluation of the proposed scheduling strategy using a real astronomy workflow named Montage. "
pub.1157960410,The concept of building security of the network with elements of the semiotic approach,"The object of research: First, to identify and discuss the security problems of cyber-physical systems associated with the emergence of qualitatively new technologies and qualitatively new affordable artificial intelligence software. Secondly, building the concept of the security structure of a cyber-physical system based on the Zero Trust Security approach. Creation of a new secure load transfer structure based on the semiotic approach. Investigated problem: Information system security problems continue to cause significant costs and damage to organizations. Sustainability requires comprehensive and integrated security platforms that reach customers, whether they work at headquarters, in a branch office, or individually from random touchpoints. The main scientific results: the concept of a structured protection system with the Zero Trust Security approach has been developed. The structure of the semiotic analysis of the segmentation of the transmitted load on the blocks is proposed. Blocks by signs are subjected to individual analysis. According to the features, the blocks are transformed by the selected representation into an object/groups of objects. Groups for transmission in the load are tagged, have different coding severity (depth), depending on the risk assessment. Groups are transmitted through the network in different ways (paths) – VPN (different ESP), unencrypted tunnel, open access, etc. This solution improves the throughput of malicious load analysis prior to transmission. The performance overhead for encoding/decoding the load and encapsulating/de-encapsulating during transmission is reduced. The transmission bandwidth is increased. The area of practical use of the research results: businesses requiring secure access to on-premise resources and mission-critical cloud environments. Organizations using employees in distributed networks. Specialists in the deployment and analysis of the protection of cyber-physical systems. Innovative technological product: The semiotic security concept extends the zero-trust security model, which focuses on protecting network traffic within and between organizations. This concept uses load traffic segmentation, which combines an advanced analysis and transfer load transformation framework. This concept provides for integration with other cybersecurity technologies such as endpoint discovery and response (EDR) and security information and event management (SIEM) to provide a more comprehensive security solution. This solution improves the throughput of malicious load analysis prior to transmission. Reduced performance resources for encode/decode load and encapsulate/deencapsulate in transit. Scope of the innovative technological product: this concept can be applied to enterprises that already have some elements of zero trust in their corporate infrastructure, but cannot strictly control the state of the requested assets, are limited in implementing security policies for certain classes of users. This "
pub.1009556228,Interfacing the internet of a trillion things,"Meaningful, reusable applications built on top of ubiquitous and networked devices will be slow to materialize as long as device APIs vary widely, communication protocols are not standardized, and programming support is limited and inconsistent. When even feature-identical devices present different APIs and application creators are burdened with managing the variability, the promise of the swarm of devices will go unrealized. We start addressing this issue by providing a model for devices, based on input and output ports, that allows for a set of common interfaces to represent a range of devices. Further, we provide a solution to the bootstrapping problem, providing a general means to bridge the adoption gap for a new API for the Internet of Things. We borrow both the name, accessor, and several key design concepts from a recent proposal by Latronico et. al, for our interface layer that wraps currently non-conforming devices with the standard interface. We show how a small, straightforward to write (and read) JavaScript file can convert diverse devices into common interfaces that are conducive for creating applications. We realize our system with three environments that can execute accessors, Python, Java, and Node.js, a range of accessors for both IoT and legacy devices, and a browser-based application for interacting with devices using our proposed interfaces. We show how the same accessor mechanism can form synthetic devices with higher-level interfaces and we outline how our system can be extended to support authentication, accessor control, and cloud storage support."
pub.1181696495,SaaS-Based reporting systems in higher education: A digital transition framework for operational resilience,"The rapid digital transformation in higher education necessitates innovative solutions to enhance operational resilience. Software as a Service (SaaS)-based reporting systems emerge as pivotal tools that facilitate data-driven decision-making and streamline institutional processes. This abstract presents a digital transition framework for integrating SaaS-based reporting systems in higher education institutions, emphasizing their role in improving efficiency, accessibility, and adaptability in an increasingly complex educational landscape. SaaS-based reporting systems provide scalable solutions that allow higher education institutions to collect, analyze, and report data in real-time, fostering transparency and accountability. By leveraging cloud technology, these systems enable users to access critical information from anywhere, thereby promoting collaborative decision-making across departments. The framework proposed in this study outlines essential components for a successful transition to SaaS-based reporting, including stakeholder engagement, data governance, and change management strategies. The implementation of SaaS reporting systems not only enhances operational efficiency but also supports institutional goals related to student success, faculty performance, and resource allocation. With robust data analytics capabilities, institutions can identify trends and make informed decisions, ultimately leading to improved academic outcomes and resource utilization. Furthermore, the adaptability of SaaS solutions allows institutions to respond effectively to changing regulatory requirements and emerging educational trends. Challenges associated with the transition to SaaS-based reporting systems include concerns regarding data security, integration with existing systems, and the need for training faculty and staff. The proposed framework addresses these challenges by emphasizing the importance of a comprehensive implementation strategy that includes risk assessment, user training, and continuous support. In conclusion, adopting SaaS-based reporting systems within a structured digital transition framework can significantly enhance operational resilience in higher education institutions. By embracing these technologies, institutions can not only improve their reporting capabilities but also foster a culture of innovation and responsiveness to the evolving educational environment.
 Keywords: Saas-Based Reporting Systems, Higher Education, Digital Transition Framework, Operational Resilience, Data-Driven Decision-Making, Cloud Technology, Stakeholder Engagement, Data Governance."
pub.1111772989,A lightweight machine learning-based authentication framework for smart IoT devices,"The Internet of Things (IoT) is the next generation plethora of interconnected devices that includes sensors, actuators, etc. and that can provide personalized services such as healthcare, security, and surveillance. The quality of our daily lives is improved by the IoT through pervasive computation and communication. Innumerable devices are being connected each day to IoT applications. Although the quality of our lives is enhanced by the IoT, IoT applications also cause serious challenges in securing networks and data in transit. Existing security solutions, such as password-based two-factor authentication and traditional biometric template-based authentication, can be challenged because of several threats that affect the reliability and efficiency of the entire system. Hence, there is a need for a highly secure authentication mechanism such as the Cancelable Biometric System (CBS). In essence, the CBS is a biometric template protection scheme that operates based on repeated distortions/transformations at the feature/signal level. Therefore, in this paper, we propose a framework for a cloud-based lightweight cancelable biometric authentication system. Findings from our study are used to demonstrate the potential for the proposed approach to be deployed in real-world settings (i.e., the capability to authenticate client devices with high accuracy and minimal overhead without affecting the security of the sensitive biometric templates in the cloud environment). Both theoretical and experimental analyses suggest that the proposed approach has a minimal equal error rate compared with those of the state-of-the-art techniques. Moreover, the proposed approach has been proven to consume less time, making it suitable for IoT environments."
pub.1149086211,Ten Years of Industrie 4.0,"A decade after its introduction, Industrie 4.0 has been established globally as the dominant paradigm for the digital transformation of the manufacturing industry. Amalgamating research-based results and practical experience from the German industry, this contribution reviews the progress made in implementing Industrie 4.0 and identifies future fields of action from a technological and application-oriented perspective. Putting the human in the center, Industrie 4.0 is the basis for data-based value creation, innovative business models, and agile forms of organization. Today, in the German manufacturing industry, the Internet of Things and cyber–physical production systems are a reality in newly built factories, and the connectivity of machinery has been significantly increased in existing factories. Now, the trends of industrial AI, edge computing up to the edge cloud, 5G in the factory, team robotics, autonomous intralogistics systems, and trustworthy data infrastructures must be leveraged to strengthen resilience, sovereignty, semantic interoperability, and sustainability. This enables the creation of digital innovation ecosystems that ensure long-term adaptability in a volatile economic and geopolitical environment. In sum, this review represents a comprehensive assessment of the status quo and identifies what is needed in the future to reap the rewards of the groundwork done in the first ten years of Industrie 4.0."
pub.1033890718,Desktop Search Engine Visualisation and Evaluation,"This work investigates the potential of applying a suite of visualisation for query processing in a desktop search environment. While each of these visualizations may not be totally new on its own, we have attempted to add value to each one by endowing it with useful features, and to seamless integrate them to allow easy switching of views, thereby providing the novelty in this work to create a potentially useful means to process search results and carry out query refinements and exploration. These visualisations include a List View, Tree View, Map View, Bubble View, Tile View and Cloud View. A first evaluation was undertaken by 94 M.Sc. participants to gauge the system’s potential usefulness and to detect usability issues with its interface and graphical presentations. The evaluation results were encouraging and showed that these views to be both effective and useful on the whole, and support the research premise that a combination of integrated visualisations will result in a more effective search tool."
pub.1135691274,Proactive Microservice Placement and Migration for Mobile Edge Computing,"In recent times, Mobile Edge Computing (MEC) has emerged as a new paradigm allowing low-latency access to services deployed on edge nodes offering computation, storage and communication facilities. Vendors deploy their services on MEC servers to improve performance and mitigate network latencies often encountered in accessing cloud services. A service placement policy determines which services are deployed on which MEC servers. A number of mechanisms exist in literature to determine the optimal placement of services considering different performance metrics. However, for applications designed as microservice workflow architectures, service placement schemes need to be re-examined through a different lens owing to the inherent interdependencies which exist between microservices. Indeed, the dynamic environment, with stochastic user movement and service invocations, along with a large placement configuration space makes microservice placement in MEC a challenging task. Additionally, owing to user mobility, a placement scheme may need to be recalibrated, triggering service migrations to maintain the advantages offered by MEC. Existing microservice placement and migration schemes consider on-demand strategies. In this work, we take a different route and propose a Reinforcement Learning based proactive mechanism for microservice placement and migration. We use the San Francisco Taxi dataset to validate our approach. Experimental results show the effectiveness of our approach in comparison to other state-of-the-art methods."
pub.1046947674,Towards a Rule-based Manufacturing Integration Assistant,"Recent developments and steadily declining prices in ICT enable an economic application of advanced digital tools in wide areas of manufacturing. Solutions based on concepts and technologies of the “Internet of Things” or “cyber physical systems” can be used to implement monitoring as well as self-organization of production, maintenance or logistics processes. However, integration of new digital tools in existing heterogeneous manufacturing IT systems and integration of machines and devices into manufacturing environments is an expensive and tedious task. Therefore, integration issues on IT and manufacturing level significantly prevent agile manufacturing. Especially small and medium-sized enterprises do not have the expertise or the investment possibilities to realize such an integration. To tackle this issue, we present the approach of the Manufacturing Integration Assistant - MIALinx. The objective is to develop and implement a lightweight and easy-to-use integration solution for small and medium-sized enterprises based on recent web automation technologies. MIALinx aims to simplify the integration using simple programmable, flexible and reusable “IF-THEN” rules that connect occurring situations in manufacturing, such as a machine break down, with corresponding actions, e.g., an automatic maintenance order generation. For this purpose, MIALinx connects sensors and actuators based on defined rules whereas the rule set is defined in a domain-specific, easy-to-use manner to enable rule modeling by domain experts. Through the definition of rule sets, the workers’ knowledge can be also externalized. Using manufacturing-approved cloud computing technologies, we enable robustness, security, and a low-effort, low-cost integration of MIALinx into existing manufacturing environments to provide advanced digital tools also for small and medium-sized enterprises."
pub.1104047745,Opportunities and Challenges Towards Cognitive IT Service Management in Real World,"More and more industries are experiencing digital disruption triggered by new technologies for example cloud, mobile, Internet-of-Things, Big Data, and Artificial Intelligence. Majority of applications are predicted to provide cognitive capabilities to amplify human skills and expertise in coming two years. Information Technology (IT) services industry is also shifting from people-led and technology-assisted model into a people-assisted and technology-led model. However, the ever-changing IT technologies, increasingly complicated IT environments, and ever-shortening IT delivery cycles in real world pose great challenges to existing IT Service Management (ITSM) technologies. This paper aims to discuss the trends, opportunities, and challenges in transformation of real-world ITSM in cognitive era and to trigger more practical research work in this exploited area. It firstly reviews the evolution of ITSM and discusses key technologies behind the evolution. Then, it summarizes opportunities and challenges in transforming ITSM with cognitive capabilities in real word. Further, we discuss key enabling technologies to drive the evolution of ITSM towards Cognitive one. Finally, we conclude the paper and envision realworld best practices in this area."
pub.1155070898,Automated engineering of domain-specific metamorphic testing environments,"Context: Testing is essential to improve the correctness of software systems. Metamorphic testing (MT) is an approach especially suited when the system under test lacks oracles, or they are expensive to compute. However, building an MT environment for a particular domain (e.g., cloud simulation, model transformation, machine learning) requires substantial effort. Objective: Our goal is to facilitate the construction of MT environments for specific domains. Method: We propose a model-driven engineering approach to automate the construction of MT environments. Starting from a meta-model capturing the domain concepts, and a description of the domain execution environment, our approach produces an MT environment featuring comprehensive support for the MT process. This includes the definition of domain-specific metamorphic relations, their evaluation, detailed reporting of the testing results, and the automated search-based generation of follow-up test cases. Results: Our method is supported by an extensible platform for Eclipse, called Gotten. We demonstrate its effectiveness by creating an MT environment for simulation-based testing of data centres and comparing with existing tools; its suitability to conduct MT processes by replicating previous experiments; and its generality by building another MT environment for video streaming APIs. Conclusion: Gotten is the first platform targeted at reducing the development effort of domain-specific MT environments. The environments created with Gotten facilitate the specification of metamorphic relations, their evaluation, and the generation of new test cases."
pub.1139728731,SMART-COMPLEX IN THE VOCATIONAL TRAINING OF A MODERN TEACHER,"Relevance. The transformation and development of modern information technologies (IT) have affected the educational process of all educational institutions, which makes it necessary to justify the application of numerous pedagogical innovations using IT in the vocational training of future teachers. This makes developing important for future educators to understand the SMART-complex as a component of the information education environment, since in order to develop the SMART-complex, the teacher must understand its` structure and the interaction of all components. Purpose: to highlight and characterize the main elements of the SMART-complex as an educational information environment of an educational institution, to prove the advisability of their use in the training of future teachers. Methods: direct analysis and synthesis are for substantiating the relevance of the problem, that is under consideration; deduction is for identifying the main structural elements of the SMART-complex, modelling is for the image of the structural appearance of the SMART-complex, generalization is for taking the stock of the work. Results: the emergence of the concept of the SMART-complex and some elements of its structural type were analysed; the existing SMART-complex models were described and generalized and their main structural elements were defined; the need to introduce SMART-complex as a component of the information education environment was noted. Conclusions: SMART-complex is an information dynamic system of educational and methodological direction with defined SMART-criteria (specific, measurable, attainable, relevant, time-bound), with static, dynamic and environmental components; SMART-cloud resource is a system for personalized delivery and processing of electronic content; services (e.g., cloud-based electronic storage) for handling documents, spread sheets, electronic presentations; video-web conferencing for creation of electronic questionnaires, tests; application of this resource allows to organise both general and individual work with the content, in fact SMART-cloud resource is a «core» of SMART-complex; available SMART-complexes correspond to the modern educational needs in the context of fulfilling the tasks of the New Ukrainian School and can be used in the training of future teachers."
pub.1126147931,RETRACTED ARTICLE: A novel centralized cloud information accountability integrity with ensemble neural network based attack detection approach for cloud data,"Highly scalable services are enabled by cloud computing and easily consumed over the Internet. User data is normally stored remotely in cloud services. Users do not own or operate the machines. This is a key feature of cloud services. Adoption of cloud services by the users are affected due to the fact that the users’ concerns about having to lose control on their own information and some attackers will hack them. Therefore to overcome the existing issues, this work proposed a new centralized cloud information accountability integrity with imperialist competitive key generation algorithm (CCIAI-ICKGA) is to resolve the above problems. It also provides the attack detection to monitor the practical utilization of the users’ information in the cloud environment. Second, cipher text-policy attribute-based encryption (CP-ABE) with key generation employing ICKGA and trapdoor generator is used to generate the public and private keys for every user. Third, the trapdoor generator ensures data integrity at the user level and also the cloud server level. At last, a dynamically weighted ensemble neural networks (DWENN) classifier is used for attack detection in the network. Additional guarantees of integrity and authenticity are provided by updating the structure of the log records. It is extended the framework in order to provide the security analysis which detects more possible attacks. Finally, a simulation result is carried out and renders a detailed performance analysis of the system. A result from rigorous experimental evaluation demonstrates the efficacy and resourcefulness of the novel CCIAI-ICKGA framework."
pub.1136476457,APS: A Large-Scale Multi-modal Indoor Camera Positioning System,"Navigation inside a closed area with no GPS-signal accessibility is a highly challenging task. In order to tackle this problem, recently the imaging-based methods have grabbed the attention of many researchers. These methods either extract the features (e.g. using SIFT, or SOSNet) and map the descriptive ones to the camera position and rotation information, or deploy an end-to-end system that directly estimates this information out of RGB images, similar to PoseNet. While the former methods suffer from heavy computational burden during the test process, the latter suffers from lack of accuracy and robustness against environmental changes and object movements. However, end-to-end systems are quite fast during the test and inference and are pretty qualified for real-world applications, even though their training phase could be longer than the former ones. In this paper, a novel multi-modal end-to-end system for large-scale indoor positioning has been proposed, namely APS (Alpha Positioning System), which integrates a Pix2Pix GAN network to reconstruct the point cloud pair of the input query image, with a deep CNN network in order to robustly estimate the position and rotation information of the camera. For this integration, the existing datasets have the shortcoming of paired RGB/point cloud images for indoor environments. Therefore, we created a new dataset to handle this situation. By implementing the proposed APS system, we could achieve a highly accurate camera positioning with a precision level of less than a centimeter."
pub.1174256241,Enhancing Real-Time User IP Tracking and Country Identification with APIs,"In the dynamic landscape of modern business, efficiently delivering data across diverse regions with varying demands presents a formidable chal-lenge. Current methodologies, leveraging APIs and machine learning algorithms for real-time user country identification, struggle to adequately optimize resource allocation and data delivery efficiency and calculate it. While IP-based geolocation methods and advanced programming language modules endeavor to track users’ IP addresses and identify their countries in real-time, they frequently fail to accurately predict future demand trends and integration with application. Despite efforts to analyze historical data request patterns, existing approaches lack the resilience required to scale resources effectively in specific regions, leading to suboptimal resource utilization and heightened costs for dynamic enterprises such as Over-the-Top (OTT) platforms and payment sites and cloud clients. Prevalent techniques face hurdles in data collection, cleaning, and feature engineering, resulting in inaccuracies and inconsistencies in forecasting future demand trends. As cloud providers track data but won’t share to clients in detail which affects clients to build large scale applications commonly employed machine learning algorithms have demonstrate limitations in accurately predicting demand patterns across diverse industries, including e-commerce, healthcare, and finance. The shortcomings of existing technologies in optimizing data delivery efficiency through real-time user IP tracking and country identification highlight the urgent need for an innovative system to effectively address modern data delivery challenges. Therefore, there is a pressing demand for novel approaches that can overcome these limitations and provide more robust solutions for optimizing data delivery and service efficiency in today's dynamic business environment. Such advancements have the potential to significantly enhance resource utilization, reduce costs, and improve the overall user experience, thereby driving innovation and competitiveness across various industries."
pub.1135124719,Computing Paradigms in Emerging Vehicular Environments: A Review,"Determining how to structure vehicular network environments can be done in various ways. Here, we highlight vehicle networks' evolution from vehicular ad-hoc networks (VANET) to the internet of vehicles (IoVs), listing their benefits and limitations. We also highlight the reasons in adopting wireless technologies, in particular, IEEE 802.11p and 5G vehicle-to-everything, as well as the use of paradigms able to store and analyze a vast amount of data to produce intelligence and their applications in vehicular environments. We also correlate the use of each of these paradigms with the desire to meet existing intelligent transportation systems' requirements. The presentation of each paradigm is given from a historical and logical standpoint. In particular, vehicular fog computing improves on the deficiences of vehicular cloud computing, so both are not exclusive from the application point of view. We also emphasize some security issues that are linked to the characteristics of these paradigms and vehicular networks, showing that they complement each other and share problems and limitations. As these networks still have many opportunities to grow in both concept and application, we finally discuss concepts and technologies that we believe are beneficial. Throughout this work, we emphasize the crucial role of these concepts for the well-being of humanity."
pub.1155537475,Real-Time And Robust 3D Object Detection with Roadside LiDARs,"This work aims to address the challenges in autonomous driving by focusing on the 3D perception of the environment using roadside LiDARs. We design a 3D object detection model that can detect traffic participants in roadside LiDARs in real-time. Our model uses an existing 3D detector as a baseline and improves its accuracy. To prove the effectiveness of our proposed modules, we train and evaluate the model on three different vehicle and infrastructure datasets. To show the domain adaptation ability of our detector, we train it on an infrastructure dataset from China and perform transfer learning on a different dataset recorded in Germany. We do several sets of experiments and ablation studies for each module in the detector that show that our model outperforms the baseline by a significant margin, while the inference speed is at 45 Hz (22 ms). We make a significant contribution with our LiDAR-based 3D detector that can be used for smart city applications to provide connected and automated vehicles with a far-reaching view. Vehicles that are connected to the roadside sensors can get information about other vehicles around the corner to improve their path and maneuver planning and to increase road traffic safety."
pub.1000220815,Leveraging the web of data via linked widgets,"Machine-readable datasets that have increasingly become available in open formats in recent years have great potential as a foundation for innovative applications and services. Linked Data in particular-a set of best practices for publishing and connecting structured data on the Web-has facilitated significant progress in evolving the Web of documents into a Web of Data. However, although this concept has opened up many opportunities for data sharing and collaboration, integrating data is still a challenging task that requires considerable technical expertise and a profound understanding of the underlying datasets. In this paper, we introduce a novel approach to provide knowledge workers with the necessary tools to leverage the fast growing Linked Data Cloud by creating semantic-aware dataflow processes. To this end, we introduce the “Linked Widget” concept as an enhancement of standard Web Widgets. These widgets are based on a semantic data model that facilitates powerful mechanisms for gathering, processing, integration, and visualization of data in a user-friendly Mashup environment. By allowing knowledge workers to easily create complex Linked Data applications in an adhoc manner, our approach should contribute towards reducing existing barriers of Linked Data adoption."
pub.1038375734,Information and Communication Technology (ICT) Applications for Customer Relationship Management (CRM),"Information and communication technology (ICT) being developed for the next generation is growing in many dimensions for customer relationship management (CRM) that is looking to enhance modern facilities with minimum cost and maximum security in network communications. The application of e-Health over the next-generation wireless network is considered to be a Millennium Development Goal (MDG). Future wireless systems and ICT facilities based on next-generation networked radio frequency identification (NGN-RFID) systems are very useful for the health MDG, which is approached through CRM values. In this research, we consider customer facilities, relationship and management techniques between healthcare management committees of particular healthcare businesses in profitable healthcare industries or the health MDG in nonprofit projects through a NGN-RFID system that collects and handles all data from the CRM. The purpose of this research is to analyze possible ICT applications based on an NGN-RFID system for CRM, which focuses on the satisfaction of customers who are loyal to healthcare agencies authorized by the United Nations (UN). CRM limitations of selected CRM values and their strategic approaches considered for health MDGs are also briefly discussed.The health MDG should be implemented to desperate elderly people who fight for their lives. The NGN-RFID system architecture and necessary models are investigated through the research of past and present situations in CRM and a case study of healthcare industries challenged by RFID technology. Compared with other counterparts existing in the literature, the investigated theoretical analysis shows that the proposed NGN-RFID solution can improve the effectiveness of CRM. The cost and time can be reduced by means of quick interactions and decisions within the healthcare environment. The analysis also shows that a theoretical model can be constructed for the future of CRM using NGN-RFID technology that can bring about 10 % improvements in the health MDG influenced with ICT applications. Service providers of cloud computing should expect better models and solutions for future CRM, which should then be attractive to customers as well as to all kinds of industries including the health MDG. A new version of the NGN-RFID system will be useful not only for switching over new customers, but also for increasing the number of loyal customers involved in the health MDG. With these modern RFID technologies, the legacy of CRM will be enhanced without affecting its originality."
pub.1037806515,7th International Symposium on Enabling Technologies for Life Sciences (ETP)," The seventh in the series of ETP Symposia (see Rapid Communications in Mass Spectrometry 2012, 26 , 1515–1518 for a description and report on the 6 th Symposium) was held at the Metro Toronto Convention Centre, Toronto, Canada, on 30 April to 1 May, 2013, and was chaired by Professor Daniel Figeys, Ottawa Institute for Systems Biology, University of Ottawa. The purpose of these symposia is to convene meetings of scientists and engineers from universities, government laboratories, industry and manufacturers of scientific instrumentation, to discuss novel technologies and methodologies applicable to research in molecular biology. The speakers and the Abstracts of their presentations are listed alphabetically below. The program illustrates some significant changes from earlier versions ( www.etpsymposium.org ) that were devoted almost entirely to proteomics (the 'P' in 'ETP'). In addition to new techniques and applications related to that area, this symposium featured work related to nucleic acid characterization, biomaterials discovery, water disinfection byproducts, and metabolomics, as well as more fundamental developments in mass spectrometry (ionization techniques and quadrupole theory) and sample preparation. The symposium is now more truly international, with speakers from Canada, USA, Europe and China. It is hoped that the wide variety of topics and backgrounds of participants, together with the highly informal setting, will help promote cross‐fertilization of ideas and unforeseen collaborations.   Perdita E. Barran , Department of Chemistry, The University of Edinburgh, Edinburgh, UK   Resolution of solution structures in the absence of solvent (Invited Talk)  Gentle application of nano‐electrospray to proteins buffered in solution to an appropriate pH allows us to use mass spectrometry to interrogate their solution conformations. This can be achieved directly with ion mobility mass spectrometry where we measure the rotationally averaged temperature dependent collision cross section of mass separated ions, or more indirectly with the use of dissociation methods. Data obtained from both methods provides insight to the structure and stability of the protein, and also detail on its interactions, especially when combined with atomistically resolved data from crystallography and/or computational approaches. Data were presented from two systems, the first was a peptide level model of the interaction between the transcription factor c‐MYC and its partner MAX; the second was a detailed structural evaluation of the metamorphic protein Lymphotactin.  Ronald Beavis , Beavis Informatics Ltd, Winnipeg, Canada   False negatives in proteomics (Invited Talk)  Proteomics experiments using tandem mass spectrometry to identify peptides and proteins are prone to various types of false negative identifications that can affect the utility of the information generated from the experiments, as well as their reproducibility. This talk discussed the classification "
pub.1028846179,SAGE: Semantic Annotation of Georeferenced Environments,"The emergence of portable 3D mapping systems are revolutionizing the way we generate digital 3D models of environments. These systems are human-centric and require the user to hold or carry the device while continuously walking and mapping an environment. In this paper, we adapt this unique coexistence of man and machines to propose SAGE (Semantic Annotation of Georeferenced Environments). SAGE consists of a portable 3D mobile mapping system and a smartphone that enables the user to assign semantic content to georeferenced 3D point clouds while scanning a scene. The proposed system contains several components including touchless speech acquisition, background noise adaptation, real time audio and vibrotactile feedback, automatic speech recognition, distributed clock synchronization, 3D annotation localization, user interaction, and interactive visualization. The most crucial advantage of SAGE technology is that it can be used to infer dynamic activities within an environment. Such activities are difficult to be identified with existing post-processing semantic annotation techniques. The capability of SAGE leads to many promising applications such as intelligent scene classification, place recognition and navigational aid tasks. We conduct several experiments to demonstrate the effectiveness of the proposed system."
pub.1005841179,Performance Profiling for OpenMP Tasks,"Tasking in OpenMP 3.0 allows irregular parallelism to be expressed much more easily and it is expected to be a major step towards the widespread adoption of OpenMP for multicore programming. We discuss the issues encountered in providing monitoring support for tasking in an existing OpenMP profiling tool with respect to instrumentation, measurement, and result presentation."
pub.1152098452,Autonomous DRL-based energy efficient VM consolidation for cloud data centers,"The exponential data growth and demands for high computing resources lead to excessive resource use in cloud data centers, which cause an increase in energy consumption and high carbon emissions in the environment. So, the high energy consumption, inefficient resource usage, and quality of service assurance (QoS) are major challenges for cloud data centers. The dynamic consolidation of Virtual Machines (VMs) is proven to be an efficient way to tackle these issues while reducing energy consumption and improving resource utilization in data centers. It reduces the number of active hosts for energy efficiency by switching under-utilized and idle hosts into lower power mode. So, several heuristics and Artificial Intelligence (AI) based VM consolidation approaches have been published in papers. Most existing approaches rely on the aggressive consolidation of VMs for energy efficiency, thus causing performance degradation and high SLA violation. However, an automated solution is needed to reduce energy consumption and SLA violation by ensuring efficient resource usage in the cloud data center environment. Therefore, this article proposes an energy-efficient autonomous VM consolidation (AVMC) mechanism that has Deep Reinforcement Learning (DRL) based agent for performing VM consolidation decisions. The DRL agent learns the optimal distribution of VMs in the data center, considering energy efficiency and QoS assurance. The real-time workload traces from PlanetLab have been used to validate the proposed mechanism. Experimental results reveal the superiority of the proposed AVMC system over the existing models. AVMC reduced the energy consumption and SLA violation rate significantly."
pub.1118971024,Runaway Collisions in Star Clusters,"We study the occurrence of physical collisions between stars in young and
compact star cluster. The calculations are performed on the GRAPE-4 with the
starlab software environment which include the dynamical evolution and the
nuclear evolution of all stars and binaries. The selection of the initial
conditions is based on existing and well observed star clusters, such as R136
in the 30 Doradus region in the Large Magellanic Cloud and the Arches and
Quintuplet star clusters in the vicinity of the Galactic center.
  Collisions between stars occurred rather frequently in our models. At any
time a single star dominates the collision history of the system. The collision
rate of this runaway merger scales with the initial relaxation time of the
cluster and is independent on other cluster parameters, such as the initial
mass function or the initial density profile of the cluster. Subsequent
encounters result in a steady grow in mass of the coagulating star, until it
escapes or explodes in a supernova. The collision rate in these models is about
0.00022 collisions per star per Myr for a cluster with an initial relaxation
time of 1 Myr."
pub.1127853105,Healthcare Services Monitoring in Cloud Using Secure and Robust Healthcare-Based BLOCKCHAIN(SRHB)Approach,"Health Electronic Records (HER) share the data to improve the quality and decrease the cost of Healthcare. It is challenging because of techniques complexities and privacy compatibilities issues. The existing system is more popular to use the cloud-based healthcare system. However, the healthcare system is affected by content privacy and secure data transformation during data gathering and analyzing personal health records in cloud environments. The patient’s records shared with patients, healthcare organizations, and insurance agents in a cloud environment. To offer a better solution for the above issues, a secure and Robust Healthcare-based Blockchain (SRHB) the approach proposes with Attribute-based Encryption to transmit the healthcare data securely. The proposed technique collects the data from the patient by using wearable devices in a centralized healthcare system. It observes patient health condition while in sleeping, heartbeat as well as walking distance. The patient obtained data is uploaded and stored in a cloud storage server. The doctor reviews the patient’s clinical test, genetic information, and observation report to prescribe the medicine and precaution for a speedy recovery. Meantime, an insurance agent also evaluates the patient’s clinical test, genetic information, and observation report to release the insurance amount for medical treatments. Blockchain concept implemented to maintain privacy in individual patient records. Each time it creates a separate block as a chain. Any changes in the block will be added as a new entry. Based on the experimental evaluation, SRHB reduces 2.85 AD (Average Delay) in seconds, 1.69 SET (System Execution Time) in seconds, and improves 28% SR (Success Rate) compared to conventional techniques."
pub.1153332736,CoCoTPM: Trusted Platform Modules for Virtual Machines in Confidential Computing Environments,"Cloud computing has gained popularity and is increasingly used to process sensitive and valuable data. This development necessitates the protection of data from the cloud provider and results in a trend towards confidential computing. Hardware-based technologies by AMD, Intel and Arm address this and allow the protection of virtual machines and the data processed in them. Unfortunately, these hardware-based technologies do not offer a unified interface for necessary tasks like secure key generation and usage or secure storage of integrity measurements. Moreover, these technologies are oftentimes limited in functionality especially regarding remote attestation. On the other hand, a unified interface is widely used in the area of bare-metal systems to provide these functionalities: the Trusted Platform Module (TPM). In this paper, we present a concept for an architecture providing TPM functionalities for virtual machines in confidential computing environments. We name it Confidential Computing Trusted Platform Module, short CoCoTPM. Different from common approaches for virtual machines, host and hypervisor are not trusted and excluded from the trusted computing base. Our solution is compatible with existing mechanisms and tools utilizing TPMs and thus allows the protection of virtual machines in confidential computing environments without further adaptations of these mechanisms and tools. This includes storage of integrity measurements during a measured boot and for the integrity measurement architecture, full disk encryption bound to these measurements, usage of an openssl provider for TLS connections and remote attestation. We show how our concept can be applied to different hardware-specific technologies and implemented our concept for AMD SEV and SEV-SNP."
pub.1151585959,Elastic ORB: Non-Rigid Transformation Based SLAM,"We present Elastic ORB, an improved implementation of a novel visual SLAM approach. Our system uses point cloud data to build the map of the environment and for the localization of the robot, odometry data is used. A frame-to-frame loop closure method is used at the back of the SLAM system using the concept of deformation theory. When building the reduced model of the map, the ORB keypoint extraction algorithm and the FPFH feature descriptor are used. Our system is capable of mapping a larger environment overcoming the limitations of existing SLAM systems which use the deformation theory at the backend."
pub.1148602409,Towards Dynamic and Safe Configuration Tuning for Cloud Databases,"Configuration knobs of database systems are essential to achieve high throughput and low latency. Recently, automatic tuning systems using machine learning methods (ML) have shown to find better configurations compared to experienced database administrators (DBAs). However, there are still gaps to apply the existing systems in production environments, especially in the cloud. First, they conduct tuning for a given workload within a limited time window and ignore the dynamicity of workloads and data. Second, they rely on a copied instance and do not consider the availability of the database when sampling configurations, making the tuning expensive, delayed, and unsafe. To fill these gaps, we propose OnlineTune, which tunes the online databases safely in changing cloud environments. To accommodate the dynamicity, OnlineTune embeds the environmental factors as context feature and adopts contextual Bayesian Optimization with context space partition to optimize the database adaptively and scalably. To pursue safety during tuning, we leverage the black-box and the white-box knowledge to evaluate the safety of configurations and propose a safe exploration strategy via subspace adaptation. We conduct evaluations on dynamic workloads from benchmarks and real-world workloads. Compared with the state-of-the-art methods, OnlineTune achieves 14.4% ~165.3% improvement on cumulative performance while reducing 91.0%~99.5% unsafe configuration recommendations."
pub.1106225160,A dynamic skyline technique for a context-aware selection of the best sensors in an IoT architecture," The integration of the Internet of Things (IoT) paradigm into web services and cloud computing allows us to handle thousands of sensors and their data. In this regard, sensing as a service model has recently emerged and the data generated by these sensors can be reused by different users and applications within IoT middleware solutions. With the huge number of sensors available in the IoT environment, the crucial challenge is how to efficiently search and select the best sensors depending on the users’ requirements. This paper aims to exploit the power of a dynamic skyline operator in the field of multi-criteria decision making, to reduce the search space for the purpose of improving the efficiency of context-awareness and selecting the best sensors following the users’ request. The architecture adopted in this proposition is composed by several gateways distributed in the network and connected with a server, where each gateway must respond to user requests locally. Hereafter, the server will aggregate the results of all gateways and give the final answer. The experimentation shows the efficiency of our method in comparison to existing ones."
pub.1169439975,Ensuring data integrity in deep learning-assisted IoT-Cloud environments: Blockchain-assisted data edge verification with consensus algorithms,"Ensuring the reliability and trustworthiness of massive IoT-generated data processed in cloud-based systems is paramount for data integrity in IoT-Cloud platforms. The integration of Blockchain (BC) technology, particularly through BC-assisted data Edge Verification combined with a consensus system, utilizes BC's decentralized and immutable nature to secure data at the IoT network's edge. BC has garnered attention across diverse domains like smart agriculture, intellectual property, and finance, where its security features complement technologies such as SDN, AI, and IoT. The choice of a consensus algorithm in BC plays a crucial role and significantly impacts the overall effectiveness of BC solutions, with considerations including PBFT, PoW, PoS, and Ripple in recent years. In this study, I developed a Football Game Algorithm with Deep learning-based Data Edge Verification with a Consensus Approach (FGADL-DEVCA) for BC assisted IoT-cloud platforms. The major drive of the FGADL-DEVCA algorithm was to incorporate BC technology to enable security in the IoT cloud environment, and the DL model could be applied for fault detection efficiently. In the FGADL-DEVCA technique, the IoT devices encompassed considerable decentralized decision-making abilities for reaching an agreement based on the performance of the intrablock transactions. Besides, the FGADL-DEVCA technique exploited deep autoencoder (DAE) for the recognition and classification of faults in the IoT-cloud platform. To boost the fault detection performance of the DAE approach, the FGADL-DEVCA technique applied FGA-based hyperparameter tuning. The experimental result analysis of the FGADL-DEVCA technique was performed concerning distinct metrics. The experimental values demonstrated the betterment of the FGADL-DEVCA approach with other existing methods concerning various aspects."
pub.1129563205,An efficient and privacy-preserving truth discovery scheme in crowdsensing applications,"Truth discovery is a reliable and effective technique to resolve conflicts of heterogeneous data and estimate user reliability in mobile crowdsensing systems. Despite its effectiveness, the widespread adoption of truth discovery requires solid privacy preservation against users’ sensory data and reliability information. Existing works of private truth discovery are primarily based on conventional cryptographic primitives, which introduce tremendous workloads on the system. In this work, we first propose an efficient and privacy-preserving truth discovery framework (EPTD-I) by adopting a novel data perturbation mechanism. EPTD-I not only protects users’ privacy but also introduces little overhead on the user side. Moreover, for high mobility environments, we improve the design with a user non-interactive scheme named EPTD-II to shift all encrypted truth discovery operations to cloud platforms. In EPTD-II, each user’s sensitive information is also kept private during the complete truth discovery procedure. Thorough security analysis demonstrates that our proposed schemes are secure and offer a high level of privacy preservation. Extensive experiments conducted on practical and simulated crowdsensing applications demonstrate the effectiveness and efficiency of the proposed schemes."
pub.1174738925,Enhancing IoT Security: A Novel Feature Engineering Approach for ML-Based Intrusion Detection Systems,"The integration of Internet of Things (IoT) applications in our daily lives has led to a surge in data traffic, posing significant security challenges. IoT applications using cloud and edge computing are at higher risk of cyberattacks because of the expanded attack surface from distributed edge and cloud services, the vulnerability of IoT devices, and challenges in managing security across interconnected systems leading to oversights. This led to the rise of ML-based solutions for intrusion detection systems (IDSs), which have proven effective in enhancing network security and defending against diverse threats. However, ML-based IDS in IoT systems encounters challenges, particularly from noisy, redundant, and irrelevant features in varied IoT datasets, potentially impacting its performance. Therefore, reducing such features becomes crucial to enhance system performance and minimize computational costs. This paper focuses on improving the effectiveness of ML-based IDS at the edge level by introducing a novel method to find a balanced trade-off between cost and accuracy through the creation of informative features in a two-tier edge-user IoT environment. A hybrid Binary Quantum-inspired Artificial Bee Colony and Genetic Programming algorithm is utilized for this purpose. Three IoT intrusion detection datasets, namely NSL-KDD, UNSW-NB15, and BoT-IoT, are used for the evaluation of the proposed approach. Performance analysis is conducted using various evaluation metrics such as accuracy, sensitivity, specificity, and False Positive Rate (FPR) are employed, while the cost of the IDS system is assessed based on computational time. The results are compared with existing methods in the literature, revealing that the IDS performance can be enhanced with fewer features, consequently reducing computational time, through the proposed method. This offers a better performance-cost trade-off for the IDS system."
pub.1174679230,A Modified Levy Flight Firefly-Based Approach to Optimize Turnaround Time in Fog Computing Environments,"The escalation of Internet of Things (IoT) devices has led to increased data generation at the network edge that has burdened the cloud infrastructure in terms of handling and processing of data. This has led to the rapid adoption of fog computing because of its ability to bring computation and storage closer to the edge and support for real-time applications and services by reducing latency. One of the foremost challenges in the fog computing arena is minimizing turnaround time. This research paper proposes a Modified Levy Flight Firefly Algorithm (MLFFA) to optimize task scheduling for fog computing environments. Specifically, the objective is to minimize the turnaround time of tasks. Moreover, genetic operators like crossover and mutation are also employed to achieve an optimal balance between exploration and exploitation. Experimental observations undertaken show that the proposed method improves the average turnaround time by 55%, 22%, and 13%, average waiting time by 59%, 45%, and 37%, average energy consumption by 19%, 7%, and 4%, and average failure rate by 50%, 28%, and 7% compared to the existing studies, namely Load Balancing and Optimization Strategy (LBOS), Technique for Resource Allocation and Management (TRAM), and Fuzzy Golden Eagle Load Balancing (FGELB), respectively."
pub.1118001771,A Novel Energy Efficient Resource Management System in Cloud Computing Environment.,"Primary target of cloud provider is to provide the maximum resource utilization and increase the revenue by reducing energy consumption and operative cost. In the service providers point of view, resource allocation, resource sharing, migration of resources on demand, memory management, storage management, load balancing, energy efficient resource usage, computational complexity handling in virtualization are some of the major tasks that has to be dealt with. The major issue focused in this paper is to reduce the energy consumption problem and management of computation capacity utilization.  For the same, an energy efficient resource management method is proposed to grip the resource scheduling and to minimize the energy utilized by the cloud datacenters for the computational work. Here a novel resource allocation mechanism is proposed, based on the optimization techniques. Also a novel dynamic virtual machine (VM) allocation method is suggested to help dynamic virtual machine allocation and job rescheduling to improve the consolidation of resources to execute the jobs. Experimental results indicated that proposed strategy outperforms as compared to the existing systems.  "
pub.1136420065,FSD-SLAM: a fast semi-direct SLAM algorithm,"Current visual-based simultaneous localization and mapping(SLAM) system suffers from feature loss caused by fast motion and unstructured scene in complex environments. Addressing this problem, a fast semi-direct SLAM algorithm is proposed in this paper. The main idea of this method is to combine the feature point method with the direct method in order to improve the robustness of the system in the environment of scarce visual features and low texture. First, the feature enhancement module based on subgraph is developed to extract image feature points more stably. Second, an apparent shape weighted fusion method is proposed for camera pose estimation, which can still work robustly in the absence of feature points. Third, an incremental dynamic covariance scaling algorithm is studied for optimizing the error of camera pose estimation. Finally, based on the optimized camera pose, a face element model is designed to estimate and fuse the point cloud pose, and obtain an ideal three-dimensional point cloud map. The proposed algorithm has been tested extensively on the benchmark TUM dataset and the real environment. The results show that the algorithm has better performance than existing visual based SLAM algorithms."
pub.1135459004,Evolutionary Dynamics and Multiplexity for Mobile Edge Computing in a Healthcare Scenario,"Today, on the wake of the evolution of 5G towards 6G, the complex joining of existing communication systems and the socio-technical aspects of human interactions, acquire a growing scientific interest. In particular, this can become decisive to establish innovation in healthcare. Complex networks theory and evolutionary dynamics can play a key role in designing a smart healthcare system, and enable to provide statistical estimators to understand which measures and how would be needed to dynamically manage requirements and needs. Thus, following this approach, we propose a framework for a smart healthcare scenario, to design a cognitive ambient assisted living of frail people connected. We consider the multiplex networks to represent two interdependent networks, the mobile edge computing nodes network and the social network of frail people. In the case of a multi-services environment, we evaluate the impact of evolutionary dynamics of cooperation of mobile edge computing nodes in the system. Our findings show how the evolutionary dynamics of mobile edge computing nodes allow decreasing the blocking probability, with the increasing of cooperators in the considered scenario."
pub.1101293515,Towards a Privacy Mechanism for Preventing Malicious Collusion of Multiple Service Providers (SPs) on the Cloud,"Cloud computing is cyberspace computing, where systems, packages, data and other required services (such as appliances, development platforms, servers, storage and virtual desktops) are dispensed. It has generated a very significant interest in educational, industrial and business set-ups due to its many benefits. However, cloud computing is still in its early stage of development and is faced with many difficulties. Researchers have shown that security issues are the major concerns that have prevented the wide adoption of cloud computing. One of the security issues is privacy which is about securing the personal identifiable information (PII) or attributes of users on the cloud. Although researches for addressing privacy on the cloud exist (uApprove, uApprove.JP and Template Data Dissemination (TDD)), users’ PII remains susceptible as existing researches lack efficient control of user’s attribute of sensitive data on the cloud. Similarly, users are endangered to malicious service providers (SPs) that may connive to expose a user’s identity in a cloud scenario. This paper provides a mechanism to solve the malicious SP collusion problem and control the release of user’s attribute in the cloud environment. This will require the use of policies on the SPs, where SPs are only allowed to request for attributes that are needed only to process a user’s service at any point in time. This can be achieved using a combination of Kerberos ticket concept, encryption and timestamp on the attribute to be released to SPs from the identity provider (IdP), thereby helping to control attributes given to SPs for processing the release of services to users for one-time usage by the SPs and not kept for future use by them. Thus, replay attacks and blocking other SPs from accessing them are prevented. Hence, any malicious intention of assembling users’ attributes by other SPs to harm them is defeated."
pub.1149075878,Цифрові технології рекрутингу персоналу,"The aim of the article. The aim of the article is to generalize and systematize theoretical and applied aspects for digital transformation of personnel recruitment process to determine the optimal digital recruitment technologies for the Ukrainian market. Which would enable, on the one hand, fully optimize the recruitment process and include general trends in digitalization and modern digital tools, and, on the other hand, would be adapted to the challenges of modern realities of staff involvement in the global pandemic COVID-19 and in the post-pandemic period. Analyses results. The implementation of recruitment processes during the global COVID-19 pandemic is increasingly moving to a remote format that require utilization of more and more digital technologies. Digitization and virtualization of recruitment processes is changing the current requirements for the software used. Synthesis of latest researches and global trends in the software market has allowed structuring current digital technologies and developing proposals for their implementation in Ukrainian HR practice. The analysis of the Ukrainian market and existing analytical platforms has allowed identifying the most popular HRM-systems, which are able to automate the process of staff recruiting. On this basis, the most effective software for recruiting personnel in Ukraine under pandemic conditions and post-pandemic reality is summarized, the need for digital technology is justified, taking into account all modern aspects and factors, promising ways to use software for improving the quality of staff recruiting. Conclusions and directions for further research. The results of the study on trends in digitalization of recruitment, development and usage of digital technologies for recruitment, trends in the usage of software for recruitment process in 2021 in Ukraine and in the world confirms the need for digital technologies research, taking into account all current aspects and impacting factors. Currently, there is an increase in the usage of digital methods for staff recruitment. Recruitment business processes are mainly implemented by using digital technologies. The usage of innovative digital technologies such as artificial intelligence (AI), chatbots, gamification, added (AR) and virtual (VR) reality, remote (digital, virtual) selection, increases the efficiency of the recruitment process, especially during the pandemic. After all, with the help of these technologies, organizations are able to reduce duration for staff recruiting process through fully automated resumes processing and screening process. Analytical modules modernize the recruitment process, identifying the best talents and their potential. Adaptation and training of employees with the help of AR and VR technologies is becoming commonplace. Introducing of HRM-system is a separate area for digital recruitment. Organizations that do not yet use digital technologies and HRM systems should consider rethinking the digital recru"
pub.1146895358,Real-Time Probabilistic Tropical Cyclone Forecasting in the Cloud,"Abstract Despite improvements in predicting the track and intensity of tropical cyclones (TCs), these storms with major societal and economic impacts continue to pose challenges for statically provisioned computational resources. The number of active storms varies from day to day, leading to regular bursts of irregular computational loads atop an already busy production schedule for weather prediction centers. The emergence of high-resolution ensemble TC prediction to quantify the uncertainty in track and intensity exacerbates this problem by requiring multiple forecasts run for each storm, each representing a possible outcome. With more than a decade of progress in the literature describing research and real-time numerical weather prediction in the cloud, we set out to evaluate if the commercial cloud environment could cope with the unique demands of TC ensemble forecasts. We describe a demonstration using a high-performance computing environment within the Microsoft Azure cloud to test dynamic resource provisioning to address time-varying resource challenges. We deployed existing operational models, implemented a combination of vendor-provided and open-source tools to orchestrate the cycling production workflows, and developed techniques for automatic error handling to keep production on schedule with minimal operator intervention. Despite challenges, our production pipeline from data ingest, forecast integration, graphics generation, and dissemination via social media was able to produce real-time forecasts of storm track and intensity with product latencies commensurate with existing operational forecasting systems."
pub.1135885558,Towards Smart Blockchain-Based System for Privacy and Security in a Smart City environment,"Nowadays, the digitalization of urban environments is redefining the public and private sectors. Moreover, Internet of Things (IoT) platforms, cloud computing infrastructure and smart devices are exchanging tremendous amount of data. This harmonious integration of the cyber capabilities of the corresponding devices with the physical world generates new opportunities in many areas; however it raises a lot of security and privacy challenges due to the diversity of sources and stakeholders, the centralized data management and the resulting lack of trust and governance. Hence, we introduce ""SmartPrivChain"" a Smart Blockchain Based System for preserving privacy and security in a smart city environment. The proposed scheme is different from the existing approaches on many points. The data privacy is preserved by combining data access control and data usage auditing measures based on smart contracts. In addition, the proposed solution is compliant with the main privacy laws and regulations especially the obligations of the European Union General Data Protection Regulation (GDPR). Lastly, we propose an enhanced Proof of Reputation (PoR) consensus scheme using a multidimensional Trust model."
pub.1140132034,Approaches of Shared Smart Parking Model in Fog and Roadside Cloud Environment: A Detailed Survey,"To find a parking space in crowded cities is an arduous task, over 30% of the traffic in big cities is due to cruising drivers finding the vacant parking space. In Delhi only 40% of the road space is being used for parking rather than traffic movement, it causes traffic jam, fuel consumption, CO2emission, etc. Roaming vehicles for parking in crowded and congested cities like Delhi consume over 3.9 million gallons of gasoline and emit over 60,000 tons of CO2per year, because of such issues implementation of the smart parking system was inevitable. The proposed survey has deeply investigated about the existing intelligent parking systems and solutions around the world. Further, this research work also discusses about the smart parking technologies and applications, indoor and outdoor parking categorization, and several sensors that have been used. Also, it has briefly discussed about the adoption of fog computing in smart parking system, additionally some existing solution of fog computing in smart parking scenario has also been presented. Also, this research work critically analyzes the literature work and in the end the best adaptable approach is also suggested according to our analysis."
pub.1155787984,Optimization of Execution for Machine Learning Applications in the Computing Continuum,"Today, adoption of Machine Learning (ML) techniques is widespread and is encountered in almost every aspect of our everyday lives. The plethora of IoT devices and the enormous amounts of data that are being generated has led to the evolution of existing and the development of new learning algorithms that aim in leveraging data for driving accurate inference and automated decision-making. In parallel, with the computing continuum paradigm, where heterogeneous and distributed resources are stretching all the way from Cloud to Edge and IoT devices, it is imperative that ML applications reap the performance and efficiency benefits of heterogeneity. However, ML application developers and data scientists are faced with the burden of manually deploying their applications in a manner that is frequently sub-optimal. In this paper, we design and implement an integrated MLOps framework that, initially, enables developers to decompose an ML workflow into its functional steps, which correspond to distinct stages of the development and execution of an ML model. Our developed scheduler is therefore able to efficiently schedule these individual components by considering their specific requirements for computing capacity during the training stage, and for low network latency during data ingestion and model serving. The proposed MLOps framework is evaluated with a proof-of-concept experimentation conducted in a realistic testbed environment. Results show significant benefits in performance, when compared with scheduling the whole ML workflow."
pub.1074216636,Generic E-Assessment Process Development based on Reverse Engineering,"The e-assessment, as an important part of any e-learning system, faces the same challenges and problems such as problems related to portability, reusability, adaptability, integration and interoperability. Therefore, we need an approach aiming to generate a general process of the e-assessment. The present study consists of the development of a generic e-assessment process which should be adapted to any learner profile. This e-assessment process is implemented as a composite cloud service which could be invoked by any existing LMS regardless of its environment. The authors are brought the abstraction defined by a workflow about proposing a development approach based on the Reverse Engineering and the cloud environment. To attempt these goals, they have studied the e-assessment politics of different existing LMSs to generate their e-assessment activities. These activities composing the generic e-assessment process using the Reverse Engineering and based on a set of mapping rules. Then, the authors have proposed a pedagogical scenario linking the generated, e-activities in an abstract manner using the concept of workflow. To specify this e-assessment workflow process, they use UML activity diagram language. Finally, to implement their approach, the authors have used the technology of cloud computing services"
pub.1140129790,Research on Distributed Storage Technology of Database Big Data Based on Cloud Computing,"Abstract With China entering the era of big data, the storage and management of big data has become a hot issue. The proposal and use of distributed storage technology has achieved good results. The continuous development of Internet communication technology and the popularization of computer information technology promote the evolution of traditional data model. The distributed storage system for big data has strong expansibility. It can optimize the existing storage space, realize the optimal allocation of space resources, and reduce the cost of data storage. Cloud computing as a resource sharing business computing model, it receives a huge amount of data every day, and the amount of data is still expanding. This has caused great pressure on distributed storage in cloud computing environment. This paper studies the distributed storage technology of database big data based on cloud computing, in order to strengthen the data management in the era of big data and improve the security and utilization of data."
pub.1126955316,Generic E-Assessment Process Development based on Reverse Engineering,"The e-assessment, as an important part of any e-learning system, faces the same challenges and problems such as problems related to portability, reusability, adaptability, integration and interoperability. Therefore, we need an approach aiming to generate a general process of the e-assessment. The present study consists of the development of a generic e-assessment process which should be adapted to any learner profile. This e-assessment process is implemented as a composite cloud service which could be invoked by any existing LMS regardless of its environment. The authors are brought the abstraction defined by a workflow about proposing a development approach based on the Reverse Engineering and the cloud environment. To attempt these goals, they have studied the e-assessment politics of different existing LMSs to generate their e-assessment activities. These activities composing the generic e-assessment process using the Reverse Engineering and based on a set of mapping rules. Then, the authors have proposed a pedagogical scenario linking the generated, e-activities in an abstract manner using the concept of workflow. To specify this e-assessment workflow process, they use UML activity diagram language. Finally, to implement their approach, the authors have used the technology of cloud computing services"
pub.1150649186,Features of using cloud technologies in the educational process during the COVID-19 pandemic,"The article outlines the relevance of using cloud technologies in the process of training masters in informational technologies at the current stage of the development of information and computer technologies. The main problems and features of the educational process using cloud technologies in the context of introducing distance learning technologies in higher education institutions, which provide other means, methods, other ways of interaction between teachers and students, require special training of all participants in the educational process, operating the method of its implementation. To increase the efficiency of the educational process in the online environment, the authors revealed the features of the use of cloud technologies in distance learning in higher education, on the example of distance training courses for Masters, specialty 011 Educational, Pedagogical sciences on the education and professional program Educational Measurements at Kamianets-Podilskyi Ivan Ohiienko National University. Particular attention is paid to the review of existing research on this topic, and its applications, to determine the impact of cloud computing on distance learning during the COVID-19 pandemic. This is achieved through the analysis of data that helped to analyze the trends in the study area. The authors consider the possibilities and problems of distance learning with the help of cloud services and reveal the transition from traditional lessons to online learning; highlight the impact of cloud technology on distance learning during the COVID-19 pandemic. The results of the study show key aspects of technology adaptation to this new global challenge and their level of efficiency that may help in the future. The study analyzed, systematized scientific sources on the research issues; identified the main problems that arise in the organization and implementation of cloud technologies in distance learning in higher education. The emphasis is placed on the differences between distance learning as a separate form of education and the introduction of cloud learning technologies in the educational process of higher education institutions. Currently, in the transition from distance learning to quality online education, when to implement the educational process in quarantine restrictions cloud technologies are being actively introduced, the question of how to ensure the effective organization and arrangement of the educational process needs to be studied and clarified."
pub.1172568476,LBATSM: Load Balancing Aware Task Selection and Migration Approach in Fog Computing Environment,"With the rapid advancement of Internet of Things technology, the field of fog computing has garnered significant attention and hence become a workable processing platform for upcoming applications. However, compared with vast computing capability of the cloud, the fog nodes have resource constraints, are heterogeneous in nature, and highly distributed. Due to the growing demand as well as diversity of applications, the nodes in a fog network become overloaded, which makes load balancing a prime concern. In this work, a load balancing aware task selection and migration approach is proposed comprising two algorithms to select and place tasks from multiple overloaded nodes to suitable destination nodes. The Selection algorithm determines the tasks that should be migrated from overloaded nodes. Placement algorithm focuses on finding a near optimal solution by applying modified binary particle swarm optimization. Specifically, the objective is to minimize execution time and transfer time of tasks. Simulation studies conducted on iFogSim prove that the suggested approach outperforms the existing approaches in terms of task execution time, task transfer time, and makespan."
pub.1061280645,An Information Framework for Creating a Smart City Through Internet of Things,"Increasing population density in urban centers demands adequate provision of services and infrastructure to meet the needs of city inhabitants, encompassing residents, workers, and visitors. The utilization of information and communications technologies to achieve this objective presents an opportunity for the development of smart cities, where city management and citizens are given access to a wealth of real-time information about the urban environment upon which to base decisions, actions, and future planning. This paper presents a framework for the realization of smart cities through the Internet of Things (IoT). The framework encompasses the complete urban information system, from the sensory level and networking support structure through to data management and Cloud-based integration of respective systems and services, and forms a transformational part of the existing cyber-physical system. This IoT vision for a smart city is applied to a noise mapping case study to illustrate a new method for existing operations that can be adapted for the enhancement and delivery of important city services."
pub.1049794247,A SoaML Approach for Derivation of a Process-Oriented Logical Architecture from Use Cases,"Designing logical architectures for cloud computing environments can be a complex endeavor, moreover when facing ill-defined contexts or insufficient inputs to requirements elicitation. Existing solutions are no longer enough to embrace challenges brought by complex scenarios and multi-stakeholder realities, as in Ambient Assisted Living ecosystems. As new concepts and cross-domain solutions emerge, these problems are tackled by connecting evermore the world of requirements and architectures, of business and technology, through service-oriented approaches. This due, we propose to extend the Four-Step-Rule-Set (4SRS) method, which has proven successful in generating a proper candidate logical architecture for an information system in ill-defined contexts, to a Service-Oriented Architecture approach for greater business integration, flexibility, and agility, by using the SoaML language. We present the result of a demonstration project, based in an industrial live setting where the 4SRS-SoaML reshaped method was applied, by generating the architectural participants, and respective channels of services and requests."
pub.1149604206,An Adaptive Offloading Mechanism for Mobile Cloud Computing: A Niching Genetic Algorithm Perspective,"The fast evolution of mobile applications demonstrates the growing need for more resources and processing power on mobile devices. Mobile Cloud Computing (MCC) combines cloud computing with mobile devices, enabling sophisticated and resource-intensive applications to operate on mobile devices regardless of their performance limits (e.g., battery life, memory utilization, and computation). Overcoming these constraints is accomplished using a well-known technique called computation offloading, which entails offloading heavy processing to resourceful servers and getting the results from these servers. Many studies have been done on mobile code offloading, with the goal of avoiding the stated limits and reducing execution time or battery consumption through single- or multiple-site offloading, so that people can use their phones and tablets more. However, the majority of existing techniques make offloading choices based on profile data, which implies a stable network environment and forces an object to be offloaded to the same site. As a consequence of these challenges, this research proposes a novel strategy for enhancing the multisite offloading mechanism by combining a Niching Genetic Algorithm (NGA) with a Markov Decision Process (MDP). MDP is used to determine the most optimal location for each application’s modules to be executed. GA was used to determine the optimal transition probability for components operating on several sites. To aid in initial population selection, the proposed model employs a niche model in the form of a context-based clearing (CBC) technique to improve the variability of the genes inside the chromosome in order to minimize their association. The simulation results reveal that the proposed technique consumes little power and executes quickly while determining the optimal offloading decision with lower generation numbers."
pub.1148524025,BlueSky,"Zero trust (ZT) is the term for an evolving set of cybersecurity paradigms that move defenses from static, network-based perimeters to focus on users, assets, and resources. It assumes no implicit trust is granted to assets or user accounts based solely on their physical or network location. We have billions of devices in IoT ecosystems connected to enable smart environments, and these devices are scattered around different locations, sometimes multiple cities or even multiple countries. Moreover, the deployment of resource-constrained devices motivates the integration of IoT and cloud services. This adoption of a plethora of technologies expands the attack surface and positions the IoT ecosystem as a target for many potential security threats. This complexity has outstripped legacy perimeter-based security methods as there is no single, easily identified perimeter for different use cases in IoT. Hence, we believe that the need arises to incorporate ZT guiding principles in workflows, systems design, and operations that can be used to improve the security posture of IoT applications. This paper motivates the need to implement ZT principles when developing access control models for smart IoT systems. It first provides a structured mapping between the ZT basic tenets and the PEI framework when designing and implementing a ZT authorization system. It proposes the ZT authorization requirements framework (ZT-ARF), which provides a structured approach to authorization policy models in ZT systems. Moreover, it analyzes the requirements of access control models in IoT within the proposed ZT-ARF and presents the vision and need for a ZT score-based authorization framework (ZT-SAF) that is capable of maintaining the access control requirements for ZT IoT connected systems."
pub.1182223675,СИСТЕМА СТРАТЕГІЧНОГО КОНТРОЛІНГУ В УМОВАХ ЦИФРОВІЗАЦІЇ,"The article explores the utilization of digital tools for the comprehensive integration of strategic controlling systems in the context of increasing digitalization. The research refines the concept of a strategic controlling system, clearly identifying its primary components and the interconnections between them. Furthermore, it examines the role of controlling as a means for the strategic transformation of businesses. The authors provide an extensive review of digital technologies that are now prevalent in modern business environments and have been rapidly adopted by companies worldwide. Key digital tools, such as Big Data and artificial intelligence, are emphasized for their capability to enhance the implementation of strategic controlling systems. These technologies, along with cloud platforms, are increasingly integrated into business processes, enabling more effective decision-making, monitoring, and adaptability to market changes. The study highlights the importance of aligning strategic controlling systems with the existing information resources and organizational processes to ensure a high degree of integration in the digital era. Through this approach, companies can optimize their operations and maintain a competitive edge. Moreover, the article outlines specific digital tools that facilitate this integration, such as Cascade, Notion, Jedox, and Phocas, each offering distinct advantages and disadvantages. These tools support real-time monitoring, scenario modeling, and strategic alignment, contributing to a more agile and transparent management system. The authors argue that the successful implementation of a strategic controlling system, supported by digital tools, is critical for ensuring a company's long-term transformation and development in a digitalized environment. The research concludes that implementation of strategic controlling systems is essential for creating a foundation for future company growth. In doing so, companies can better adapt to external challenges and optimize their resource allocation, ultimately driving more effective business transformations."
pub.1048468401,EI2N’10 & SeDeS’10 - PC Co-chairs Message,"After the successful fourth edition in 2009, the fifth edition of the Enterprise Integration, Interoperability and Networking workshop (EI2N’2010) has been organised as part of the OTM’2010 Federated Conferences and is supported by the IFAC Technical Committee 5.3 ”Enterprise Integration and Networking”, the IFIP TC 8 WG 8.1 ”Design and Evaluation of Information Systems”, the SIG INTEROP Grande-Région on ”Enterprise Systems Interoperability” and the French CNRS National Research Group GDR MACS. Collaboration is necessary for enterprises to prosper in the current extreme dynamic and heterogeneous business environment. Enterprise integration, interoperability and networking are the major disciplines that have studied how to do companies to collaborate and communicate in the most effective way. These disciplines are well-established and are supported by international conferences, initiatives, groups, task forces and governmental projects all over the world where different domains of knowledge have been considered from different points of views and a variety of objectives (e.g., technological or managerial). Enterprise Integration involves breaking down organizational barriers to improve synergy within the enterprise so that business goals are achieved in a more productive and efficient way. The past decade of enterprise integration research and industrial implementation has seen the emergence of important new areas, such as research into interoperability and networking, which involve breaking down organizational barriers to improve synergy within the enterprise and among enterprises. The ambition to achieve dynamic, efficient and effective cooperation of enterprises within networks of companies, or in an entire industry sector, requires the improvement of existing, or the development of new, theories and technologies. Enterprise Modelling, Architecture, and semantic techniques are the pillars supporting the achievement of Enterprise Integration and Interoperability. Internet of Things and Cloud Computing now present new opportunities to realize inter enterprise and intra enterprise integration. For these reasons, the workshop’s objective is to foster discussions among representatives of these neighbouring disciplines and to discover new research paths within the enterprise integration community. After peer reviews, 6 papers have been accepted out of 12 submissions to this workshop. Prof. Michael Sobolewski (Polish-Japanese Institute of IT, Poland) has been invited as EI2N plenary keynote on ”Exerted Enterprise Computing: from Protocol-oriented Networking to Exertion-oriented Networking”. In addition to the presentations of the accepted papers, groups have been organised into what E2IN traditionally calls ”workshop cafés”, to discuss and debate the presented topics. This year discussion enabled putting forward new research related to ”interoperability issues in collaborative information systems”. These groups reported the results of the respective di"
pub.1160722693,Load Balancing in Cloud Computing Environment,"Cloud computing has many advantages over traditional computing like scalability, elasticity, accessibility. Due to these advantages, it has gained popularity in the Information Technology industry. The popularity of cloud computing has forced all organizations (including Micro Small and Medium Enterprises) to move their businesses from traditional computing to cloud computing. This transition has increased cloud-based services worldwide. The worldwide cloud market is set to surpass $330 billion USD in the year 2020 and is expected to grow at a Compound Annual Growth Rate (CAGR) of 17.5% from 2020 to 2025. The growth in cloud adoption has resulted in a high surge of network traffic. Failure to handle this enormous traffic may result in reduced system performance, inefficient resource utilization, server outage. One of the solutions for these problems is cloud load balancers. The cloud load balancers improve system performance by distributing load among all available resources. The research work is carried out to build a new general load balancing algorithm and also extends the ability of another algorithm in the natural phenomena based category. Finally, the two new proposed algorithms are tested under the clustering environments. Popular algorithms like Throttled, Active Monitoring, etc. in the category general Load Balancing have drawbacks of longer search time and inefficient resource utilization. To decrease search time and improve resource utilization an algorithm named Capacity Based Load Balancing (CBLB) is proposed in the general category. The algorithm is implemented and analyzed using CloudSim simulator and Amazon Web Services (AWS) real cloud environments. In both environments, the performance is examined for homogeneous as well as heterogeneous setups. In both setups, the performance parameters makespan, average response time, and throughput are considered for the analysis of the results. The proposed algorithm is compared with the popular Throttled algorithm. Besides, the adaptability of the algorithm is tested for the varied number of Datacenters (DCs), a varied number of Virtual Machines (VMs), and varied capacity VMs. The CPU utilization of the algorithms is also checked by using the cloud watch of AWS to ensure effective utilization of resources. The proposed algorithm has shown better performance than the Throttled in both simulation and real cloud environments for all the parameters. The proposed CBLB algorithm is used to enhance the popular natural phenomena-based Artificial Bee Colony (ABC) algorithm. The performance of the enhanced ABC algorithm named ABC_CBLB is compared with the basic ABC algorithm. The experiments are carried out for homogeneous and heterogeneous setups in both CloudSim and AWS platforms. The performance is tested for the varied number of DCs and varied number of VM. In these experiments in addition to the parameters makespan, average response time, throughput, and CPU utilization, the average waiting time"
pub.1120991825,Resource needs prediction in virtualized systems: Generic proactive and self-adaptive solution,"Resource management of virtualized systems in cloud data centers is a critical and challenging task due to the fluctuating workloads and complex applications in such environments. Over-provisioning is a common practice to meet service level agreement requirements, but this leads to under-utilization of resources and energy waste. Thus, provisioning virtualized systems with resources according to their workload demands is essential. Existing solutions fail to provide a complete solution in this regard, as some of them lack proactivity and dynamism in estimating resources, while others are environment- or application-specific, which limits their accuracy in the case of bursty workloads. Effective resource management requires dynamic and accurate prediction. This work presents a novel prediction algorithm, which (1) is generic, and can thus be applied to any virtualized system, (2) is able to provide proactive estimation of resource requirements through machine learning techniques, and (3) is capable of real-time adaptation with padding and prediction adjustments based on prediction error probabilities in order to reduce under- and over-provisioning of resources. In several virtualized systems, and under different workload profiles, the experimental results show that our proposition is able to reduce under-estimation by an average of 86% over non-adjusted prediction, and to decrease over-estimation by an average of 67% versus threshold-based provisioning."
pub.1141145788,Importance and Uses of Microstrip Antenna in IoT,"The next step in the computer age is out of the conventional laptop or desktop world. In the paradigm of the Internet of Things (IoT), many of the objects or things that surround us will be available in one form or the other on the network. This network can be of any form, either wired or wireless or sometimes a combination of both. Wireless sensor network (WSN) technologies will rise in near future to meet this new challenging demand, where communication and information systems are invisibly embedded or hidden in the environment surrounding us. This leads to the generation of huge amounts of data that must be stored, analyzed, and represented in a seamless way, which is not only efficient but easy to interpret. For such ubiquitous computing, cloud computing capabilities can provide the best virtual infrastructure that integrates everything from sensing devices, data storage devices, data analytics tools, data visualization platforms, to delivery to clients end interface. An important feature of IoT is intelligent communication with existing wireless networks and background software-based services. The evolution of growing ubiquitous data and communication networks involved is already apparent with the increased presence of internet access availability provided by Wi-Fi, 4G, WLAN, WiMAX, and upcoming 5G wireless networks. Nevertheless, to implement the concept of the IoT successfully and effectively, the computational criteria would have to become faster than conventional mobile computing models and evolve in a way to develop a linkage between real objects or things and integrating their information into our computing environment."
pub.1182099476,An Effective Autonomous Navigation Method Using 3D LiDAR for Orchard Picking Robots,"Autonomous navigation is crucial for enabling orchard picking robots to operate independently, yet most existing approaches focus on centerline navigation, which does not cater to the specific requirements of picking tasks. To address the challenge, we propose an autonomous navigation method employing 3D LiDAR for orchard picking robots. The approach construct a global point cloud map of the orchard using the SLAM algorithm, followed by filtering ground points through the CSF algorithm. The remaining non-ground points are clustered using Euclidean clustering to compute multiple navigation target points. A point cloud alignment algorithm is then employed to determine the relative transformation between real-time scanned point clouds and the pre-constructed map, thereby providing precise localization information. Finally, path planning between multiple navigation points is executed within the Robot Operating System (ROS). Experimental validations in both simulation and real-world environments demonstrate the effectiveness of the proposed method in achieving autonomous navigation for orchard picking robots."
pub.1170552479,Innovation and Practice of “Job-Course-Competition-Certification” Professional Talents’ Cultivation Model in Tourism Major under the Background of Digitalization Era,"The rapid development of new digital economies such as artificial intelligence, cloud computing, and big data is profoundly changing various industries, especially the tourism industry. The digital upgrade has promoted the transformation and development of the tourism industry, and has given rise to a series of new employment positions closely related to digital technology, providing broader development space and transformation opportunities for the cultivation of tourism professionals. In this digitalization context, The most urgent thing is to explore the best talent cultivation model to adapt to the changes in the tourism industry environment and the demand for tourism talents. On the basis of drawing on the practical experiences from professional talent cultivation reform in existing universities, combined with the problems in the process of tourism majors` talents cultivation, this paper proposes an innovative plan for the talent cultivation mode of higher vocational tourism majors under the theoretical framework of “job-course-competition-certification” integration. It is hoped that through the vivid practice of three integration scenarios: “job- course integration”, “course-competition integration”, and “course-certification integration”, it will optimize the talent training system for tourism majors in higher vocational colleges, and achieve high-quality and skilled training for tourism majors talents."
pub.1092925511,Optimizing the resource usage in Cloud based environments: the Synergy approach,"Managing resource allocation in a cloud based data centre serving multiple virtual organizations is a challenging issue. In fact, while batch systems are able to allocate resources to different user groups according to specific shares imposed by the data centre administrator, without a static partitioning of such resources, this is not so straightforward in the most common cloud frameworks, e.g. OpenStack. In the current OpenStack implementation, it is only possible to grant fixed quotas to the different user groups and these resources cannot be exceeded by one group even if there are unused resources allocated to other groups. Moreover in the existing OpenStack implementation, when there aren't resources available, new requests are simply rejected: it is then up to the client to later re-issue the request. The recently started EU-funded INDIGO-DataCloud project is addressing this issue through ""Synergy"", a new advanced scheduling service targeted for OpenStack. Synergy adopts a fair-share model for resource provisioning which guarantees that resources are distributed among users following the fair-share policies defined by the administrator, taken also into account the past usage of such resources. We present the architecture of Synergy, the status of its implementation, some preliminary results and the foreseen evolution of the service."
pub.1138696890,Digital Twin Enhanced Optimization of Manufacturing Service Scheduling for Industrial Cloud Robotics,"The industrial cloud robotics (ICR) has the characteristics of intelligence, reliability, and scalability. In the smart manufacturing environment, ICR can be encapsulated as services through virtualization and servilization technology, enabling the rapid matching of personalized manufacturing capabilities and services for end users. However, the manufacturing resources are physically isolated and the physical workshop environment is vulnerable to dynamic disturbances, which reduces manufacturing system performance. In this context, taking the cycle time into consideration, the manufacturing service scheduling model for ICR is established and the digital twin (DT) enhanced scheduling optimization mechanism is proposed. When disturbances occur, the digital twin platform interacts with the cloud layer and physical workshop to analyze multi-source data in order to monitor the manufacturing environment in real time and optimize the production efficiency. Meanwhile, the manufacturing service scheduling based on an improved discrete differential evolution (IDDE) algorithm is proposed, in which the adaptive mutation and crossover operator and double mutation strategies are applied to converge to the optimal scheduling sequence. Finally, the case study is implemented to verify the proposed mechanism shows better performance compared with the existing optimization algorithms."
pub.1158108035,Online Task Scheduling of Big Data Applications in the Cloud Environment,"The development of big data has generated data-intensive tasks that are usually time-consuming, with a high demand on cloud data centers for hosting big data applications. It becomes necessary to consider both data and task management to find the optimal resource allocation scheme, which is a challenging research issue. In this paper, we address the problem of online task scheduling combined with data migration and replication in order to reduce the overall response time as well as ensure that the available resources are efficiently used. We introduce a new scheduling technique, named Online Task Scheduling algorithm based on Data Migration and Data Replication (OTS-DMDR). The main objective is to efficiently assign online incoming tasks to the available servers while considering the access time of the required datasets and their replicas, the execution time of the task in different machines, and the computational power of each machine. The core idea is to achieve better data locality by performing an effective data migration while handling replicas. As a result, the overall response time of the online tasks is reduced, and the throughput is improved with enhanced machine resource utilization. To validate the performance of the proposed scheduling method, we run in-depth simulations with various scenarios and the results show that our proposed strategy performs better than the other existing approaches. In fact, it reduces the response time by 78% when compared to the First Come First Served scheduler (FCFS), by 58% compared to the Delay Scheduling, and by 46% compared to the technique of Li et al. Consequently, the present OTS-DMDR method is very effective and convenient for the problem of online task scheduling."
pub.1153669528,Remote Instrumentation Science Environment for Intelligent Image Analytics,"Current scientific experiments frequently involve control of specialized instruments (e.g., scanning electron microscopes), image data collection from those instruments, and transfer of the data for processing at simulation centers. This process requires a “human-in-the-loop” to perform those tasks manually, which besides requiring a lot of effort and time, could lead to inconsistencies or errors. Thus, it is essential to have an automated system capable of performing remote instrumentation to intelligently control and collect data from the scientific instruments. In this paper, we propose a Remote Instrumentation Science Environment (RISE) for intelligent image analytics that provides the infrastructure to securely capture images, determine process parameters via machine learning, and provide experimental control actions via automation, under the premise of “human-on-the-loop”. The machine learning in RISE aids an iterative discovery process to assist researchers to tune instrument settings to improve the outcomes of experiments. Driven by two scientific use cases of image analytics pipelines, one in material science, and another in biomedical science, we show how RISE automation leverages a cutting-edge integration of cloud computing, on-premise HPC cluster, and a Python programming interface available on a microscope. Using web services, we implement RISE to perform automated image data collection/analysis guided by an intelligent agent to provide real-time feedback control of the microscope using the image analytics outputs. Our evaluation results show the benefits of RISE for researchers to obtain higher image analytics accuracy, save precious time in manually controlling the microscopes, while reducing errors in operating the instruments."
pub.1181579488,Revocable certificateless proxy re-signature with signature evolution for EHR sharing systems,"Cloud computing has revolutionized in the healthcare industry, particularly in the management and accessibility of Electronic health records (EHR). However, maintaining the integrity and authenticity of EHR in cloud environments remains a crucial concern. To tackle this challenge, certificateless proxy re-signature is a promising cryptographic primitive for developing a practical EHR sharing system in the cloud. User revocation is a necessary issue in such system, but revocation introduces a new challenge, namely the continued validity of signatures from revoked users. A conventional method to solve this problem is to make the unrevoked users re-sign those valid EHR by using their current signing keys, which brings a lot of burden to the users. Therefore, we should establish an efficient mechanism to ensure that only signatures of valid data from non-revoked users can pass verification. In this paper, we propose a notion called revocable certificateless proxy re-signature with signature evolution (RCLPRS-SE), which allows for dynamic management of users and the ability to update signatures efficiently in accordance with evolving data requirements. We present a concrete construction of RCLPRS-SE and provide formal security proofs in the standard model. Compared with the existing related works, our scheme has a significant advantage in terms of signature updating efficiency."
pub.1123319550,A Blockchain-Based Searchable Public-Key Encryption With Forward and Backward Privacy for Cloud-Assisted Vehicular Social Networks,"As the integration of the Internet of Vehicles and social networks, vehicular social networks (VSN) not only improves the efficiency and reliability of vehicular communication environment, but also provide more comprehensive social services for users. However, with the emergence of advanced communication and computing technologies, more and more data can be fast and conveniently collected from heterogeneous devices, and VSN has to meet new security challenges such as data security and privacy protection. Searchable encryption (SE) as a promising cryptographic primitive is devoted to data confidentiality without sacrificing data searchability. However, most existing schemes are vulnerable to the adaptive leakage-exploiting attacks or can not meet the efficiency requirements of practical applications, especially the searchable public-key encryption schemes (SPE). To achieve secure and efficient keyword search in VSN, we design a new blockchain-based searchable public-key encryption scheme with forward and backward privacy (BSPEFB). BSPEFB is a decentralized searchable public-key encryption scheme since the central search cloud server is replaced by the smart contract. Meanwhile, BSPEFB supports forward and backward privacy to achieve privacy protection. Finally, we implement a prototype of our basic construction and demonstrate the practicability of the proposed scheme in applications."
pub.1122048735,Sensor Fusion Based Pipeline Inspection for the Augmented Reality System,"Augmented reality (AR) systems are becoming next-generation technologies to intelligently visualize the real world in 3D. This research proposes a sensor fusion based pipeline inspection and retrofitting for the AR system, which can be used in pipeline inspection and retrofitting processes in industrial plants. The proposed methodology utilizes a prebuilt 3D point cloud data of the environment, real-time Light Detection and Ranging (LiDAR) scan and image sequence from the camera. First, we estimate the current pose of the sensors platform by matching the LiDAR scan and the prebuilt point cloud data from the current pose prebuilt point cloud data augmented on to the camera image by utilizing the LiDAR and camera calibration parameters. Next, based on the user selection in the augmented view, geometric parameters of a pipe are estimated. In addition to pipe parameter estimation, retrofitting in the existing plant using augmented scene are illustrated. Finally, step-by-step procedure of the proposed method was experimentally verified at a water treatment plant. Result shows that the integration of AR with building information modelling (BIM) greatly benefits the post-occupancy evaluation process or pre-retrofitting and renovation process for identifying, evaluating, and updating the geometric specifications of a construction environment."
pub.1156203675,A Physics-Based Modelling and Control of Greenhouse System Air Temperature Aided by IoT Technology,"The need to reduce energy consumption in greenhouse production has grown. Thermal heating demand alone accounts for 80% of conventional greenhouse energy consumption; this significantly reduces production profit. Since microclimate affects crop metabolic processes and output, it is essential to monitor and control it to achieve both quantity and quality production with minimum energy consumption for maximum profit. The Internet of Things (IoT) is an evolving technology for monitoring and controlling environments that have recently been adopted to boost greenhouse efficiency in many applications by integrating hardware and software solutions; therefore, its adoption is thus critical in enabling greenhouse energy consumption minimisation. The first objective of this study is to improve and validate a greenhouse dynamic air temperature model required to simulate or predict indoor temperature. To achieve the first objective, therefore, an existing model was enhanced and a closed loop test experimental data from the IoT cloud-based control system platform deployed in the prototype greenhouse built in Cranfield University was used to validate the model using an optimisation-based model fitting approach. The second goal is to control the greenhouse air temperature in simulation using relatively simple PI and on-off control strategies to maintain the grower’s desired setpoint irrespective of the inevitable disturbances and to verify the potential of the controllers in minimising the total energy input to the greenhouse. For the second objective, the simulation results showed that the two controllers maintained the desired setpoint; however, the on-off strategy retained a sustainable oscillation, and the tuned PI effectively maintained the desired temperature, although the average energy used by the controllers is the same."
pub.1159663702,Digital twin-enabled grasp outcomes assessment for unknown objects using visual-tactile fusion perception,"Humans can instinctively predict whether a given grasp will be successful through visual and rich haptic feedback. Towards the next generation of smart robotic manufacturing, robots must be equipped with similar capabilities to cope with grasping unknown objects in unstructured environments. However, most existing data-driven methods take global visual images and tactile readings from the real-world system as input, making them incapable of predicting the grasp outcomes for cluttered objects or generating large-scale datasets. First, this paper proposes a visual-tactile fusion method to predict the results of grasping cluttered objects, which is the most common scenario for grasping applications. Concretely, the multimodal fusion network (MMFN) uses the local point cloud within the gripper as the visual signal input, while the tactile signal input is the images provided by two high-resolution tactile sensors. Second, collecting data in the real world is high-cost and time-consuming. Therefore, this paper proposes a digital twin-enabled robotic grasping system to collect large-scale multimodal datasets and investigates how to apply domain randomization and domain adaptation to bridge the sim-to-real transfer gap. Finally, extensive validation experiments are conducted in physical and virtual environments. The experimental results demonstrate the effectiveness of the proposed method in assessing grasp stability for cluttered objects and performing zero-shot sim-to-real policy transfer on the real robot with the aid of the proposed migration strategy."
pub.1123795057,Towards DNA based data security in the cloud computing environment,"Nowadays, data size is increasing day by day from gigabytes to terabytes or even petabytes, mainly because of the evolution of a large amount of real-time data. Most of the big data is transmitted through the internet and they are stored on the cloud computing environment. As cloud computing provides internet-based services, there are many attackers and malicious users. They always try to access user’s confidential big data without having the access right. Sometimes, they replace the original data by any fake data. Therefore, big data security has become a significant concern recently. Deoxyribonucleic Acid (DNA) computing is an advanced emerged field for improving data security, which is based on the biological concept of DNA. A novel DNA based data encryption scheme has been proposed in this paper for the cloud computing environment. Here, a 1024-bit secret key is generated based on DNA computing, user’s attributes and Media Access Control (MAC) address of the user, and decimal encoding rule, American Standard Code for Information Interchange (ASCII) value, DNA bases and complementary rule are used to generate the secret key that enables the system to protect against many security attacks. Experimental results, as well as theoretical analyses, show the efficiency and effectivity of the proposed scheme over some well-known existing schemes."
pub.1123091440,Towards Efficient Integration of Blockchain for IoT Security: The Case Study of IoT Remote Access,"The booming Internet of Things (IoT) market has drawn tremendous interest
from cyber attackers. The centralized cloud-based IoT service architecture has
serious limitations in terms of security, availability, and scalability, and is
subject to single points of failure (SPOF). Recently, accommodating IoT
services on blockchains has become a trend for better security, privacy, and
reliability. However, blockchain's shortcomings of high cost, low throughput,
and long latency make it unsuitable for IoT applications. In this paper, we
take a retrospection of existing blockchain-based IoT solutions and propose a
framework for efficient blockchain and IoT integration. Following the
framework, we design a novel blockchain-assisted decentralized IoT remote
accessing system, RS-IoT, which has the advantage of defending IoT devices
against zero-day attacks without relying on any trusted third-party. By
introducing incentives and penalties enforced by smart contracts, our work
enables ""an economic approach"" to thwarting the majority of attackers who aim
to achieve monetary gains. Our work presents an example of how blockchain can
be used to ensure the fairness of service trading in a decentralized
environment and punish misbehaviors objectively. We show the security of RS-IoT
via detailed security analyses. Finally, we demonstrate its scalability,
efficiency, and usability through a proof-of-concept implementation on the
Ethereum testnet blockchain."
pub.1150134714,[Retracted] An Enhanced Dynamic Nonlinear Polynomial Integrity‐Based QHCP‐ABE Framework for Big Data Privacy and Security,"Topics such as computational sources and cloud-based transmission and security of big data have turned out to be a major new domain of exploration due to the exponential evolution of cloud-based data and grid facilities. Various categories of cloud services have been utilized more and more widely across a variety of fields like military, army systems, medical databases, and more, in order to manage data storage and resource calculations. Attribute-based encipherment (ABE) is one of the more efficient algorithms that leads to better consignment and safety of information located within such cloud-based storage amenities. Many outmoded ABE practices are useful for smaller datasets to produce fixed-size cryptograms with restricted computational properties, in which their characteristics are measured as evidence and stagnant standards used to generate the key, encipherment, and decipherment means alike. To surmount the existing problems with such limited methods, in this work, a dynamic nonlinear poly randomized quantum hash system is applied to enhance the safety of cloud-based information. In the proposed work, users’ attributes are guaranteed with the help of a dynamic nonlinear poly randomized equation to initialize the chaotic key, encipherment, and decipherment. In this standard, structured and unstructured big data from clinical datasets are utilized as inputs. Real-time simulated outcomes demonstrate that the stated standard has superior exactness, achieving over 90% accuracy with respect to bit change and over 95% accuracy with respect to dynamic key generation, encipherment time, and decipherment time compared to existing models from the field and literature. Experimental results are demonstrated that the proposed cloud security standard has a good efficiency in terms of key generation, encoding, and decoding process than the conventional methods in a cloud computing environment."
pub.1155923095,"A Review on Blockchain and IoT Integration from Energy, Security and Hardware Perspectives","Blockchain is one of the promising technologies nowadays due to its unique characteristics like security, privacy, data integrity, decentralization, immutability, and traceability. Originally used to implement cryptocurrencies, recently numerous applications have employed blockchain in their architectures including applications targeted for the internet of things (IoT) environments. It is expected that by 2025 more than 21 billion IoT devices will be used especially with use of cloud, fog and edge computing architectures. Integrating blockchain in the IoT architecture provides many advantages such as enhancing security and privacy, better speed and costs, traceability and reliability, and elimination of single point of failure. On the other hand, many issues and challenges have arisen and should be addressed. Typically, IoT system consists of lightweight devices with limited hardware resources and constraints. Hence, the energy efficiency is a fundamental challenge in such devices. The main motivation of this paper is to survey designing a secure and energy efficient blockchain-based IoT implementation using a suitable hardware design. The paper classifies, presents and analyzes existing solutions to better implement IoT environment combined with blockchain technology. Our investigation demonstrations that most of lightweight solutions handle either the energy or security issue separately. Moreover, many works are theoretical-based analysis and solutions without considering the real blockchain-based IoT validation design. Energy evaluation for IoT hardware devices is not given the adequate research bandwidth. Additionally, limited works evaluated their techniques from hardware constrained device perspective. It is recommended that the performance of any proposed solution should be validated using real designs. The hardware perspective evaluation should be in mind for efficient blockchain-based IoT hardware implementation. The proposed lightweight solutions should focus more on efficient energy implementation while considering the lightweight security mechanisms."
pub.1167047925,Game-Based Collaborative Scheduling With Fuzzy Uncertain Migration in Cloud Manufacturing,"Cloud manufacturing (CMfg) provides on-demand services offered by the cloud platform to satisfy the individual requirements of users. However, the dynamics and uncertainty of the CMfg environment pose significant challenges to implementation of efficient synchronous scheduling of processing and logistic services. This paper proposes a game theory-based collaborative scheduling approach for effective utilization of distributed manufacturing and logistic resources with fuzzy uncertain task migration in CMfg, and the Nash equilibrium in this game theory is realized by a decision tree optimization algorithm. With this model, manufacturing and logistic services can cope with unexpected events whether or not task migration occurs. Moreover, the proposed approach takes into account independent and shared logistics, as well as delayed logistics, to improve the efficiency of transportation along the same route. Simulation results demonstrate that this approach is not only effective for the relevant optimization objective but also can achieve great performance under dynamic CMfg environments. Note to Practitioners—To formulate optimal production planning, how to solve the optimization of manufacturing and logistic resources is the main focus of smart manufacturing systems. In this paper, game theory is introduced to address the collaborative service scheduling issues in cloud manufacturing. Previous dynamic scheduling methods cannot further distinguish between independent logistics and shared logistics. Also, they rarely evaluate the impact of the task migration strategy on the occurrence of unexpected events from game point of view. Therefore, in this paper, a unique two-layer scheduling method based on game theory model that is composed of a processing service scheduling sub-game and a logistic service scheduling sub-game is presented. This model is implemented by adopting a proposed decision tree optimization algorithm that settles constrained non-linear programming. Simulation experiments show that the proposed method outperforms existing scheduling methods in terms of operational efficiency and fuzzy task migration decisions. Additionally, this method can be readily implemented and incorporated into real production settings. Future work could improve the proposed method by analyzing the uncertainties in point-to-point and hub-and-spoke networks across different transportation routes."
pub.1030966239,Extratropical Transition of Western North Pacific Tropical Cyclones: An Overview and Conceptual Model of the Transformation Stage,"Extratropical transition (ET) in the western North Pacific is defined here in terms of two stages: transformation, in which the tropical cyclone evolves into a baroclinic storm; and reintensification, where the transformed storm then deepens as an extratropical cyclone. In this study, 30 ET cases occurring during 1 June–31 October 1994–98 are reviewed using Navy Operational Global Atmospheric Prediction System analyses; hourly geostationary visible, infrared, and water vapor imagery; and microwave imagery. A brief climatology based on these cases is presented for the transformation stage and the subsequent cyclone characteristics of the reintensification stage. A three-dimensional conceptual model of the transformation stage of ET in the western North Pacific Ocean is proposed that describes how virtually all 30 cases evolved into an incipient, baroclinic low. The three-step evolution of the transformation of Typhoon (TY) David (September 1997) is described as a prototypical example. Four important physical processes examined in each of the three steps include (i) environmental inflow of colder, drier (warm, moist) air in the western (eastern) quadrant of David’s outer circulation that initiates an asymmetric distribution of clouds and precipitation, and a dipole of lower-tropospheric temperature advection; (ii) the interaction between TY David and a preexisting, midlatitude baroclinic zone to produce ascent over tilted isentropic surfaces; (iii) systematic decay and tilt of the warm core aloft in response to vertical shear; and (iv) an evolution of David’s outer circulation into an asymmetric pattern that implies lower-tropospheric frontogenesis. The beginning and end of the transformation stage of ET in the western North Pacific is defined based on the interaction of the tropical cyclone circulation with a preexisting, midlatitude baroclinic zone. In particular, cases that complete the transformation stage of ET become embedded in the preexisting, midlatitude baroclinic zone, with the storm center in cold, descending air. Cases that begin transformation but do not become embedded in the baroclinic zone fail to complete transformation and simply dissipate over lower sea surface temperatures and in an environment of vertical wind shear. Use of the conceptual model, together with satellite imagery and high-resolution numerical analyses and forecasts, should assist forecasters in assessing the commencement, progress, and completion of the transformation stage of ET in the western North Pacific, and result in improved forecasts and dissemination of timely, effective advisories and warnings."
pub.1151230907,RDMA Congestion Control: It Is Only for the Compliant,"Remote direct memory access (RDMA) networks enable low latency and low central processing unit utilization, and their widespread adoption in datacenters enables improved application performance. However, there are performance isolation concerns for RDMA deployed in a shared cloud environment. In particular, congestion control enforcement and congestion control algorithms in RDMA make the network susceptible to performance hacking attacks, which give the attacker extra bandwidth and cause severe congestion in the network. These attacks can increase short flow completion times by several orders of magnitude. We surface a fundamental tradeoff in congestion control between short flow completion time and performance isolation. We discuss this tradeoff and how existing approaches do not provide a robust solution. We also advocate that researchers incorporate performance isolation concerns into the design and evaluation of congestion control."
pub.1023412426,Big Data Storage Security,"The demand for data storage and processing is increasing at a rapid speed in the big data era. The management of such tremendous volume of data is a critical challenge to the data storage systems. Firstly, since 60 % of the stored data is claimed to be redundant, data deduplication technology becomes an attractive solution to save storage space and traffic in a big data environment. Secondly, the security issues, such as confidentiality, integrity and privacy of the big data should also be considered for big data storage. To address these problems, convergent encryption is widely used to secure data deduplication for big data storage. Nonetheless, there still exist some other security issues, such as proof of ownership, key management and so on. In this chapter, we first introduce some major cyber attacks for big data storage. Then, we describe the existing fundamental security techniques, whose integration is essential for preventing data from existing and future security attacks. By discussing some interesting open problems, we finally expect to trigger more research efforts in this new research field."
pub.1175750576,Measuring the Effectiveness of Carbon-Aware AI Training Strategies in Cloud Instances: A Confirmation Study,"While the massive adoption of Artificial Intelligence (AI) is threatening the environment, new research efforts begin to be employed to measure and mitigate the carbon footprint of both training and inference phases. In this domain, two carbon-aware training strategies have been proposed in the literature: Flexible Start and Pause & Resume. Such strategies—natively Cloud-based—use the time resource to postpone or pause the training algorithm when the carbon intensity reaches a threshold. While such strategies have proved to achieve interesting results on a benchmark of modern models covering Natural Language Processing (NLP) and computer vision applications and a wide range of model sizes (up to 6.1B parameters), it is still unclear whether such results may hold also with different algorithms and in different geographical regions. In this confirmation study, we use the same methodology as the state-of-the-art strategies to recompute the saving in carbon emissions of Flexible Start and Pause & Resume in the Anomaly Detection (AD) domain. Results confirm their effectiveness in two specific conditions, but the percentage reduction behaves differently compared with what is stated in the existing literature."
pub.1171913473,Systematic evaluation of hybrid steganography approach towards secure communication in IoT,"The proliferation of real-world internet connectivity has led to a convergence of a large number of people accessing common cloud facilities. While this has facilitated increased data sharing in the cloud, it has also raised concerns regarding data privacy and security. It is crucial for both cloud service providers and users to ensure the security of data in all aspects. However, the development of weighted algorithms aimed at enhancing security often leads to increased latency and degradation of cloud performance. The system presented in this study addresses the challenges of creating a secure environment and evaluates a proposed framework using hybrid steganography. In this system, the cover image, input data, and a uniquely generated secret key are considered for hybridization. The embedded image undergoes the Sum of Absolute Difference (SAD) algorithm for steganography, followed by the Discrete Wavelet Transform (DWT) process for image transformation before being communicated over the channel. At the receiving end, the reverse process is executed to extract the original information, contingent upon the correlation with the secret key. The proposed approach achieves an MSE (Mean Squared Error) of -0.52141 and a PSNR (Peak Signal-to-Noise Ratio) of 48 dB, demonstrating improved performance compared to existing approaches."
pub.1135381786,APS: A Large-Scale Multi-Modal Indoor Camera Positioning System,"Navigation inside a closed area with no GPS-signal accessibility is a highly
challenging task. In order to tackle this problem, recently the imaging-based
methods have grabbed the attention of many researchers. These methods either
extract the features (e.g. using SIFT, or SOSNet) and map the descriptive ones
to the camera position and rotation information, or deploy an end-to-end system
that directly estimates this information out of RGB images, similar to PoseNet.
While the former methods suffer from heavy computational burden during the test
process, the latter suffers from lack of accuracy and robustness against
environmental changes and object movements. However, end-to-end systems are
quite fast during the test and inference and are pretty qualified for
real-world applications, even though their training phase could be longer than
the former ones. In this paper, a novel multi-modal end-to-end system for
large-scale indoor positioning has been proposed, namely APS (Alpha Positioning
System), which integrates a Pix2Pix GAN network to reconstruct the point cloud
pair of the input query image, with a deep CNN network in order to robustly
estimate the position and rotation information of the camera. For this
integration, the existing datasets have the shortcoming of paired RGB/point
cloud images for indoor environments. Therefore, we created a new dataset to
handle this situation. By implementing the proposed APS system, we could
achieve a highly accurate camera positioning with a precision level of less
than a centimeter."
pub.1095667046,Sensomax: An Agent-Based Middleware For Decentralized Dynamic Data-Gathering In Wireless Sensor Networks,"In this paper we describe the design and implementation of Sensomax, a novel agent-based middleware for wireless sensor networks (WSNs), which is written in Java and runs on networks of various Java-enabled embedded systems ranging from resource-constrained Sun Spot nodes to resource-rich Raspberry Pi boards. Programming WSNs tends to be a complex task for developers, as it requires detailed knowledge of underlying hardware resources as well as their firmware or operating systems. Although many solutions have been proposed up to this date, only a few of those are capable of satisfying challenging demands such as serving multiple user applications and reprogramming the network at run-time in popular high-level languages such as Java. Sensomax presents a novel combination of several best practices from existing solutions, facilitating fully distributed and decentralized bulk programming and/or updating of sensor nodes; serving multiple simultaneous applications deployed by single or multiple users; allowing dynamic run-time changes in the application requirements; and offering on-the-fly switching between time-driven, data-driven, and event-driven operational paradigms. Sensomax provides a sophisticated set of APIs, a feature-rich desktop application, a web application for cloud-based distributed networks, and a simulator. We demonstrate Sensomax in operation on a real network of 12 Sun Spots deployed as an environment-monitoring system, and 600 virtual Sun Spot nodes running continuously over periods of several weeks, using a novel statistically rigorous adaptive change-point detection algorithm to identify significant “anomalous” changes in the monitored data-streams."
pub.1153894834,Research on Digital Campus Construction of Lyuliang University Based on Luliang Big Data,"On the basis of the existing data processing and analysis capabilities of Lyuliang College, the digital campus construction scheme of Lyuliang College is reasonably designed from six aspects: information public support environment, green and energy-saving data center, cloud service platform, big data standard system, big data acquisition system, big data processing and analysis system, so as to provide information services for school teaching, scientific research and management, and promote the optimization of teaching effects. At the same time, promote the deep integration of information technology and education and teaching."
pub.1157129812,Automation of Calibration Procedure for Milk Non Automatic Weighing Instrument (NAWI) Process Using AI Methods,"Automation is the Trend of Industrial 4.0 for rational process and prevent fraudulent activities. It is the simplification of human task assigned to machine and also brings the transparency in the system so that any fraudulent that causes human health and wealth can be traced and eliminated. The present work focusing on ca liberation correction as Milk is collected at procurement centers on a weight basis, using the Non-Automatic Weighing Instrument (NAWI) of Class III - Medium Accuracy Weighing Instruments. The specification of the gravity of milk as per standards and actual have a discrepancy which is causing the losses for the milk farmers and seriously affecting their economy with general NAWI system used for the process. In corp orating the AI based techniques on cloud based collected information storage to bring the automation methods for integrating all dairy form in a place to standardise the system and eliminate the discrepancy in calculation. Daily the farmers sell lakhs of Milk to the Milk purchasing units. The densities of the Milk brought by farmers are noted, and the data is stored in the cloud system for calibration verification through AI methods. The daily transactions are recorded stored in meta file. The cloud data of transaction in purchasing of Milk (with the incorrect calibration of the Weighing Instrument put to use by the officials) can be arrived at precisely with the help of computational methods using ML and DL techniques. These methods will be useful for the farmers at large to find out the loss precisely and the gain to the purchaser. Integration of existing legacy method with proposed automation procedures of AI and ML/DL method is main focus of this work to bring fraud free environment in metro-logical system."
pub.1112164777,Mobility management on 5G Vehicular Cloud Computing systems,"Fifth generation (5G) Vehicular Cloud Computing (VCC) systems use heterogeneous network access technologies to fulfill the requirements of modern services. Multiple services with different Quality of Service (QoS) constraints could be available in each vehicle, while at the same time, user requirements and provider policies must be addressed. Therefore, the design of efficient Vertical Handover (VHO) management schemes for 5G-VCC infrastructures is needed. In this paper, a novel VHO management scheme for 5G-VCC systems is proposed. Whenever the user satisfaction grade becomes less than a predefined threshold, VHO is initiated and network selection is performed, considering the velocity of the vehicle, network characteristic criteria such as throughput, delay, jitter and packet loss, as well as provider policy criteria such as service reliability, security and price. The proposed scheme uses linguistic values for VHO criteria attributes represented by Interval Valued Pentagonal Fuzzy Numbers (IVPFNs) to express the information using membership intervals. The VHO scheme is applied to a 5G-VCC system which includes 3GPP Long Term Evolution (LTE) and IEEE 802.16 Worldwide Interoperability for Microwave Access (WiMAX) Macrocells and Femtocells, as well as IEEE 802.11p Wireless Access for Vehicular Environment (WAVE) Road Side Units (RSUs). Performance evaluation shows that the suggested method ensures the Always Best Connection (ABC) principle, while at the same time outperforms existing VHO management schemes."
pub.1173241874,Adaptive VNF Placement Considering Overall Latency and 5G Wireless Channel Reliability in Industry 4.0: A Reinforcement Learning Based Approach,"Industry 4.0 incorporates the integration of cloud computing, Industrial Internet of Things (IIoT), and modern communication technologies within the industrial automation systems. Various devices with different network requirements of high reliability and low latency, rely on connectivity. The 5G and Beyond (B5G) software-defined architecture facilitates Network Function Virtualization (NFV), which is an essential solution for fulfilling these stringent demands. NFV allows for the implementation and control of Virtual Network Functions (VNFs) in dynamic network environments. VNF placement optimization has been extensively studied in the 5G perspective outside the industry environment with a focus on minimizing delay and cost, increasing VNF reliability, and increasing resource efficiency. However, the complex dynamics of the wireless channel in industrial environments have a considerable impact on the essential delay factors that are important for optimizing the deployment of VNFs. This study focuses on modeling a Wireless Sensor Network (WSN) based Industry 4.0 factory automation scenario at mmWave band, formulating an optimization problem to minimize overall delay while considering packet loss rate in the 5G industrial wireless channel. The optimization problem is formulated as a Markov Decision Process (MDP) and two Reinforcement Learning (RL) based algorithms AVP-Q and AVP-DQN are proposed for optimizing the VNF placement. The proposed algorithms are extensively evaluated against the Value Iteration algorithm which assumes a completely known MDP model and two other algorithms from the literature. The simulated results show that AVP-DQN outperforms existing algorithms for this scenario by 39% and 22.6% and the achieved performance is only close to that of the Value Iteration algorithm."
pub.1175585911,Artificial Intelligence Based Multi-Layer Approach for Finding Unknown Attacks in Cloud Network by Using Hybrid Intrusion Detection,"Objectives : The objective of this study is to explore Intrusion Detection Systems and their various types by gathering research from previously published articles in refereed journals. The focus is on developing a proposed model capable of identifying unknown attacks in cloud networks using Signature and Anomaly-based Intrusion Detection Systems. Subsequently, the efficiency of the proposed model will be assessed, and a comparison with existing models will be conducted. The paper\'s main objective is to identify unknown attacks in a cloud network using a combination of signature and anomaly-based intrusion detection systems in an artificial intelligence-based multi-layered approach. Methods: Leveraging insights from existing literature, the proposed model combines signature-based IDS for known threat detection and anomaly-based IDS for detecting unusual behavioral patterns indicative of new or unseen attacks. Experimental evaluations using NSL-KDD and ADFA datasets demonstrate competitive accuracy and detection rates, with the proposed artificial intelligence-based Hybrid IDS achieving high performance in detecting both normal and malicious activities. Findings: This model produces above 90%, 96%, and 98% efficiency in the wired, Wireless, and Cloud networks respectively, and this model finds known attacks effectively while using parameters like event logs, file transferring time, TCP and UDP addresses, CPU Usage, Weak and synthetic data, IP and MAC address. Existing literature said that the existing model using the Hybrid Intrusion detection model can identify unknown attacks with a maximum of 80%, 92%, and 96% accuracy respectively. The findings suggest that the artificial intelligence-based multi-layered approach offers a promising solution for enhancing cloud network security, with the potential for further optimization and integration of advanced technologies in future research endeavors. Novelty: This study presents an artificial intelligence-based multi-layered approach for detecting unknown attacks in cloud networks by integrating signature-based and anomaly-based intrusion detection systems (IDS). The authors developed the model to detect the intrusion by using the Behaviour Profiling algorithm and dynamically prevent the data from intrusion by using the Statistical approach model. The authors trying to find unknown attacks, therefore the authors defined the objective of this paper as to find the unknown attacks in cloud networks by using the combination of signature and anomaly-based intrusion detection systems. The objective is to develop a model capable of effectively identifying cyber threats in cloud environments. The existing models do not concentrate on identifying unknown attacks by using Signature-based Intrusion Detection. Very few of the literature said that known attacks can be identified easily by using Signature-based Intrusion Detection but the unknown attacks identifying process is hard. Some of the Literature said that "
pub.1140755689,PROSPECTS FOR USING GEOINFORMATION TECHNOLOGIES IN UKRAIN AIRPORTS FOR ADMINISTRATIVE AND ECONOMIC MANAGEMENT,"The urgency of the research. In recent years, world's airports are actively implementing cloud technologies for collecting, processing and visualizing geospatial data: laser and lidar scanning, integration of BIM / GIS models, the use of artificial intelligence, virtual and augmented reality technologies, digital duplicates and «smart» cities. For Ukraine, which is actively following the path of digitalization and implementation of modern geographic information technologies in many areas of ac-tivity, the development of new methods and approaches for administrative and economic management of airport complexes is a relevant and promising area. Target settings.This study examines the possibilities of modern geographic information and cloud technologies and pro-spects for their use for administrative and economic management of an airport. The study is related to the implementation of the State Target Program for Airport Development until 2023 and the Aviation Transport Strategy of Ukraine until 2030, which aims to develop the aviation industry in Ukraine, bringing airport infrastructure to the requirements of the European Union. The Law of Ukraine «On the National Infrastructure of Geospatial Data» and «Consolidated Concept of VIM Implementation in Ukraine» has a great influence on the formation of geospatial data of airports.Actual scientific researches and issues analysis. The paper analyzes and summarizes publications on methods of obtain-ing geospatial data, implementation of geographic information technologies, virtual, augmented and mixed reality technolo-gies, artificial intelligence and the concept of «smart» city for administrative and economic management of airports.Uninvestigated parts of general matters defining. Analysis of recent research and publications has shown that the pro-spects for the introduction of geographic information technology for administrative and economic asset management of Ukrain-ian airports need further research, as these issues are very important and relevant, given the rapid growth of digital society, environment and infrastructure.The research objective. The purpose of this study is to analyze the possibilities and prospects for the introduction of modern technologies for processing and visualization of geospatial data for administrative and economic management of the airport and the development of a conceptual model. The task of the research is to analyze the methods of obtaining geospatial data of the airport, the use of geographic information systems in airports, artificial intelligence technologies, virtual, aug-mented and mixed reality, the Internet of Things, digital duplicates, implementation of the concept of «smart» city, etc.The statement of basic materials. Geospatial data is created digitally using modern information and cloud technologies that offer a wide range of equipment, software, methods and technologies for working with geospatial information. Every year, new technologies that are used in th"
pub.1086152817,Fog radio access network system control scheme based on the embedded game model,"As a promising paradigm for the 5G wireless communication system, a new evolution of the cloud radio access networks has been proposed, named as fog radio access networks (F-RANs). It is an advanced socially aware mobile networking architecture to provide a high spectral and energy efficiency while reducing backhaul burden. In particular, F-RANs take full advantages of social information and edge computing to efficiently alleviate the end-to-end latency. Based on the benefit of edge and cloud processing, key issues of F-RAN technique are radio resource allocation, caching, and service admission control. In this paper, we develop a novel F-RAN system control scheme based on the embedded game model. In the proposed scheme, spectrum allocation, cache placement, and service admission algorithms are jointly designed to maximize system efficiency. By developing a new embedded game methodology, our approach can capture the dynamics of F-RAN system and effectively compromises the centralized optimality with decentralized distribution intelligence for the faster and less complex decision making process. Through simulations, we compare the performance of our scheme to the existing studies and show how we can achieve a better performance under dynamic F-RAN system environments."
pub.1152623225,Real-time implementation of an implantable antenna using chicken swarm optimization for IoT-based wearable healthcare applications,"In a cognitive wireless powered communication network (CWPCN), implantable medical devices play an important role in screening patients via wireless communication. The key elements for multiple-input multiple-output (MIMO) framework appropriate for Internet of Things (IoT) applications are reliable in the behavior, consume less power, and less computational complication with reconfigurable models is the major challenge in design. This model enhances remote communications at the frontline of wireless research. Cloud model provides essential environment for software support, sharable network, and much more. Some limitations are present on the geological structure and multi hop-based network. To address these, fog computing is brought to tackle the addressing needs and enhancement of the computing service. The problem during cloud integration with IoT can be handled effectively by fog computing. For data identification and decoding, multiple antenna frameworks rely on details of channel state information at the receiver. As a consequence, an efficient and vigorous evaluation of wireless channels is critical for the consistent recovery of data. As a result, constructing an implantable MIMO antenna for IoT-based wearable healthcare applications is proposed. The main goal is to improve the antenna’s capacity parameters using effective chicken swarm optimization technique and deep learning-based convolution neural network approaches for efficient data transmission between the CWPCN in the healthcare field. A cooperative antenna selection neural network classifier is used to make the classification. The implemented technique’s behavior is linked with that of conventional techniques in terms of gain, mutual coupling, specific absorption rate, bandwidth, and efficiency. The results are analyzed and compared to existing approaches, and the proposed system is shown to be more efficient for health data transfer. This chapter proposes an implantable multiple-input multiple-output (MIMO) Internet of Things (IoT)-based antenna and employs chicken swarm optimization (CSO) method to improving the antenna capacity for its application in wearable healthcare devices. The key elements for MIMO framework appropriate for IoT applications are reliable in the behavior, consume less power, and less computational complication with reconfigurable models is the major challenge in design. The model of the implantable antenna comprises particular issues like bandwidth improvement, size limitations, biocompatibility, security of the patient, and detuning procedure. Nowadays, for healthcare utilization, an implantable antenna is employed in the absence of wired interaction. A modified CSO algorithm is used in the synthesizing linear, circular, and random antenna arrays. The antenna capacity is improved by the chicken swarm secrecy probability optimization method and classification was done using the cooperative antenna selection – neural network classifier."
pub.1182244545,Regulating Cloud Services as a Tool of International Competition: Defining the Domain,"The development of ‘new political spaces’, particularly within the information domain, is emerging as a significant trend. With the rapid expansion of the global ICT sector, cloud services have become one of the pivotal industries. This paper explores the core aspects of the cloud services sector, delineates its key components and highlights its principal feature: the ability to facilitate long-term cross-border operations through remote program execution and network access. Cloud services constitute a burgeoning global market dominated by the U.S. and China, fostering both economic and non-economic competition, along with associated threats and opportunities. In this competitive environment, various actors, especially states and large corporations, assert their political interests. The study reviews diverse scholarly approaches to cloud services within political science from 2010 to 2022. The author proposes two scientific hypotheses: the first posits that the growth of the global cloud services market, combined with their technical characteristics, contributes to the transformation of this technology into a powerful instrument of international influence and competition; the second hypothesis asserts that governmental regulation of cloud services can serve as an effective method for countries seeking to enhance their international influence and impact on other participants in international relations. The paper examines governmental regulatory goals, types and methods within legal, cultural, political and economic subsystems. Furthermore, the study outlines the existing international regulatory framework for cloud services and proposes an alternative model. This proposed model supports the coexistence of multiple regulatory regimes, enhances coordination among system participants and mitigates the likelihood of destructive actions targeting individual participants."
pub.1181405196,"Autonomous collaborative mobile robot for greenhouses: Design, development, and validation tests","This paper describes the development of a mobile agricultural robot capable of performing high-capacity transport tasks within greenhouses in presence of people or other agricultural machines. The main objective is to provide the robot with enough technology to work collaboratively with nearby human workers. In addition, the robot must also be able to transport 100 kilograms in a safe way over uneven terrain, a characteristic not usually found in existing greenhouse robots. This is important to ensure the sustainability of intensive greenhouse cultivation, as it is essential to allow more flexible use of robots when adapting. This would allow for expanding infrastructure size and operating volume to suit different greenhouse conditions, thus maximizing production. The robot is fitted with different sensors to enable autonomous navigation, perception, and to identify the environment and the operators (3D LiDAR, stereo cameras, and ultrasound). It also features the hardware necessary for cloud connection to share data in real time. All sensors have been validated to work correctly, hence the robot can move around the greenhouse. With the software currently used for collaborative robotics, the ultrasounds correctly identify the environment, and cameras and LiDAR can locate the farmer correctly. In this work, several gaps in greenhouse robotics are addressed by designing, developing, and validating a collaborative mobile robot with advanced sensors and algorithms with IoT integration. The robot lays the foundation for the implementation of autonomous navigation, collaborating with farmers in real-time and efficient operation in complex greenhouse environments, laying the groundwork for future advances in agricultural automation."
pub.1146680708,Towards Dynamic and Safe Configuration Tuning for Cloud Databases,"Configuration knobs of database systems are essential to achieve high
throughput and low latency. Recently, automatic tuning systems using machine
learning methods (ML) have shown to find better configurations compared to
experienced database administrators (DBAs). However, there are still gaps to
apply the existing systems in production environments, especially in the cloud.
First, they conduct tuning for a given workload within a limited time window
and ignore the dynamicity of workloads and data. Second, they rely on a copied
instance and do not consider the availability of the database when sampling
configurations, making the tuning expensive, delayed, and unsafe. To fill these
gaps, we propose OnlineTune, which tunes the online databases safely in
changing cloud environments. To accommodate the dynamicity, OnlineTune embeds
the environmental factors as context feature and adopts contextual Bayesian
Optimization with context space partition to optimize the database adaptively
and scalably. To pursue safety during tuning, we leverage the black-box and the
white-box knowledge to evaluate the safety of configurations and propose a safe
exploration strategy via subspace adaptation.%, greatly decreasing the risks of
applying bad configurations. We conduct evaluations on dynamic workloads from
benchmarks and real-world workloads. Compared with the state-of-the-art
methods, OnlineTune achieves 14.4%~165.3% improvement on cumulative performance
while reducing 91.0%~99.5% unsafe configuration recommendations."
pub.1006434488,Smart Cargo for Multimodal Freight Transport: When “Cloud” becomes “Fog”,"Freight transport is recognized as a complex system that is affected by globalization effects, integration of different transport modes, geographically distributed operations and extended business models. Such complexity is also amplified by the need for the real-time response on the unexpected situations detected during the transportation phase (e.g. weather conditions, strikes, accidents). In a very demanding market, request for on-time cargo delivery, transport efficiency is a critical issue, and the ability for real-time detection and resolving of all possible obstacles and exceptions becomes a core competency for logistics operators. The work presented here introduces the concept of Smart Cargo, able to autonomously react to its context, find and understand alternatives, compute adaptive behavior, optimizing its own decisions. The Smart Cargo concept, implements a paradigm shift requiring taking the control of computing applications, data, and services away from some central nodes (the “cloud”) to the other logical extreme (the “fog”) of the Internet. Fog provides an intelligent connection of people, processes, data, and things, enabling Smart Cargo to go beyond the existing Intelligent Cargo concept. Fog computing is described here, as a necessary paradigm shift on distributed freight intelligence, allowing multimodal freight transport to achieve real-time situation awareness and reaction, planning with predictions and learning from the environment."
pub.1166265579,Intelligent internet of things induced preschool education assistance system,"Abstract With the rapid development of AI technology, how to construct smart learning environment has become a hot topic in education. However, most of existing studies focus on the higher education. It is still an open issue to establish smart learning environment in kindergartens for preschool education. In order to solve this issue, this paper designs an intelligent classroom education perception system to assist preschool education. The system end‐edge‐cloud collaboration structure. The end node captures videos of children in the classroom and send the collected videos to edge node. The edge node contains a NVIDIA® Jetson™ TX2 in which an AI module is deployed. The AI module adopts end‐to‐end architecture, which contains four parts: human detector, symmetric spatial transformation network module, non‐maximum suppression module, and a spatial temporal graph convolutional network module. Compared with previous works, the proposed scheme considers both spatial information and temporal information of skeletal key points. The experimental results show that the proposed smart preschool education assistance system can help teachers to recognize most of common preschool children activities during classroom."
pub.1125554253,A fog‐based collaborative intrusion detection framework for smart grid,"Summary With the rapid development of information and communication technologies (ICTs), the conventional electrical grid is evolving towards an intelligent smart grid. Due to the complexity, how to protect the security of smart grid environments still remains a practical challenge. Currently, collaborative intrusion detection systems (CIDSs) are one important solution to help identify various security threats, through allowing various IDS nodes to exchange data and information. However, with the increasing adoption of ICT in smart grid, cloud computing is often deployed in order to reduce the storage burden locally. However, due to the distance between grid and cloud, it is critical for smart grid to ensure the timely response to any accidents. In this work, we review existing collaborative detection mechanisms and introduce a fog‐based CIDS framework to enhance the detection efficiency. The results show that our approach can improved the detection efficiency by around 21% to 45% based on the concrete attacking scenarios.
We propose a fog‐based collaborative intrusion detection system (CIDS) framework to help reduce the latency caused by the distance between smart grid (SG) and cloud. In the evaluation, we simulate a smart grid environment and investigate our framework under both internal and external attacks. The results show that our approach can improved the detection efficiency by around 21% to 45% based on the concrete attacking scenarios."
pub.1111321694,Clustering Analysis for Secondary Breaking Using a Low-Cost Time-of-Flight Camera,"The integration of robust perception in a heavy-duty manipulation control system is an enabler for autonomous mining. This paper aims to analyze performance and robustness of clustering methods for object recognition during the secondary breaking stage of mining. Secondary breaking refers to breaking over-sized rocks into smaller pieces for the purpose of grinding and extraction of valuable ores and minerals. Therefore, recognition of rock pieces is the detection of unstructured targets within a structured environment. The clustering methods are experimentally evaluated by several sets of scenes of point clouds as outputs of a Time-of-Flight camera (ToF). The challenges of rock detection from sparse 3D point cloud data are addressed. In outdoor conditions, ToFs generally provide coarse but robust output in short sample times. Therefore, some clustering methods can be prone to numerical and statistical errors. This paper highlights the weaknesses and strengths of three methods for the secondary breaking application. We propose an algorithmic method for exploiting the existing clustering and segmentation methods efficiently in the detection loop to determine a suitable contact point and approaching angle for a hydraulic jack hammer. The results verify effectiveness of the proposed approach for scattered outputs of low-cost ToFs."
pub.1107477413,Hypertracing: Tracing Through Virtualization Layers,"Cloud computing enables on-demand access to remote computing resources. It provides dynamic scalability and elasticity with a low upfront cost. As the adoption of this computing model is rapidly growing, this increases the system complexity, since virtual machines (VMs) running on multiple virtualization layers become very difficult to monitor without interfering with their performance. In this paper, we present hypertracing, a novel method for tracing VMs by using various paravirtualization techniques, enabling efficient monitoring across virtualization boundaries. Hypertracing is a monitoring infrastructure that facilitates seamless trace sharing among host and guests. Our toolchain can detect latencies and their root causes within VMs, even for boot-up and shutdown sequences, whereas existing tools fail to handle these cases. We propose a new hypervisor optimization, for handling efficient nested paravirtualization, which allows hypertracing to be enabled in any nested environment without triggering VM exit multiplication. This is a significant improvement over current monitoring tools, with their large I/O overhead associated with activating monitoring within each virtualization layer."
pub.1149050176,Destination Earth: Digital Twins of the Earth System,"<p><span>The talk describes ongoing efforts to create digital replicas of the Earth as part of the the European Commission's Destination Earth programme.</span></p><p><span>Global, coupled storm-resolving simulations are feasible and can contribute to building such information systems and are no longer a dream thanks to recent advances in Earth system modelling, supercomputing and the adaptation of weather and climate codes for novel computing architectures. Such simulations for example explicitly represent essential climate processes, such as deep convection and mesoscale ocean eddies, that today need to be parametrised even at the highest resolution used in global weather and climate information production. These simulations, combined with novel data-driven deep learning advances, thus offer a window into the future, with a promise to significantly increase the realism of Earth system information. Despite the significant compute and data challenges, there is a real prospect to better support global to local climate change mitigation and adaptation efforts, and complement the existing information derived with today's operational simulations in the range of 10-100 km.</span></p><p><span>Digital Twins of Earth thus encapsulate both the latest science as well as technology advances to provide near-real time information on Extremes and climate change in a wider digital environment. Here users can interact, modify and ultimately create their own tailored information. This is facilitated through complex workflows managed by ECMWF's digital twin engine that closely connects EuroHPC resources for the production of digital twin data, manages diverse data access patterns through cloud-based ancilliary systems such as Eumetsat's Data Lake, and provides a diverse range of tools to faciliate user interaction and data-driven applications creating new information through ESA's user service platform. The underlying system architecture and design choices will be described and justified.</span></p>"
pub.1158410084,Token Open Secure and Practical NTRU-based Updatable Encryption,"<p>Updatable encryption (UE) is suitable for key rotation and ciphertexts update in the cloud storage environment. A new security model for UE named Token Open security model is presented recently, which is closer to the real attack scenario. And the existing post-quantum secure UE schemes are all based on the LWE assumption without engineering implementation. In this paper, we improve the recently presented security model, which allows the adversary to corrupt all update tokens and keys of the past epochs. We show that our new model reflects the characteristics of UE. Then we construct the first UE scheme based on NTRU, named NTRU-UE. Our new scheme uses the key-switching technique to achieve backward-leak uni-directional key update and uni-directional ciphertext update and uses the ciphertext-masking technique to achieve rand-update. We show that the new scheme is secure under our new model. In addition, we accomplish the software implementation of NTRU-UE and the experiment results show that the scheme meets the practical requirements.</p>"
pub.1169680624,Narrow linewidth DBR and MOPA laser systems for quantum applications in the 7xx nm regime,"Lasers are a key enabling technology in the field of quantum computing, quantum sensing and quantum metrology. These applications require technically challenging properties from the lasers in use, such as stable and precisely controlled wavelength, up to watt level output power, and a narrow linewidth. Semiconductor diode lasers offer a very compact size, low power consumption, as well as scalability of cost and manufacturing volume due to their wafer scale manufacturing process. The monolithic integration of frequency selection on-chip in Distributed Bragg reflector (DBR) lasers offers advantages such as higher robustness, reduced system complexity and smaller size compared to external cavity frequency selection configurations. Thus, DBR lasers provide an optimal solution for a compact narrow linewidth laser source for selected quantum applications. Bandgap engineering of semiconductor gain media enables emission across the spectrum from UV to mid-IR. Wavelengths matching atomic transitions in the 7xx nm wavelength region include 760 nm and 770 nm for Yb, and 780 nm and 795 nm for Rb. In this work we describe the design, manufacturing, and performance of DBR lasers in the 7xx nm wavelength regime. The effects of key device design parameters are investigated to optimize the device performance. These include emitter width, gain and grating length, grating duty cycle, and residual layer thickness. To further scale the laser output power, tapered amplifiers are manufactured and characterized. The lasers are integrated into a laser system platform containing optical isolation, fiber coupling, low-noise laser drivers and temperature controllers. The system includes features such as compact footprint, controlled environment, cloud-connectivity and predictive maintenance."
pub.1156937726,Token Open Secure and Practical NTRU-based Updatable Encryption,"<p>Updatable encryption (UE) is suitable for key rotation and ciphertexts update in the cloud storage environment. A new security model for UE named Token Open security model is presented recently, which is closer to the real attack scenario. And the existing post-quantum secure UE schemes are all based on the LWE assumption without engineering implementation. In this paper, we improve the recently presented security model, which allows the adversary to corrupt all update tokens and keys of the past epochs. We show that our new model reflects the characteristics of UE. Then we construct the first UE scheme based on NTRU, named NTRU-UE. Our new scheme uses the key-switching technique to achieve backward-leak uni-directional key update and uni-directional ciphertext update and uses the ciphertext-masking technique to achieve rand-update. We show that the new scheme is secure under our new model. In addition, we accomplish the software implementation of NTRU-UE and the experiment results show that the scheme meets the practical requirements.</p>"
pub.1172788468,Secure and Fine-Grained Access Control With Optimized Revocation for Outsourced IoT EHRs With Adaptive Load-Sharing in Fog-Assisted Cloud Environment,"With the employment of IoT technology and cloud computing in healthcare, outsourcing Electronic Health Records (EHRs) generated by IoT devices becomes the critical issue. In fact, as EHRs may be collected by different units within a hospital and traverse intranet or public networks, this introduces vulnerabilities to privacy breaches. Existing research efforts employ cloud-based access control with encryption solutions to secure outsourced EHRs. Nevertheless, in the context of an IoT cloud data-sharing environment, where data originates from numerous devices and user authorization status undergoes frequent changes, a gap persists in achieving a comprehensive and systematic integration of secure IoT data transfer and aggregation, coupled with efficient user revocation procedures. In this research, we proposed a blockchain-based access control scheme for outsourced IoT-enabled EHRs in fog-assisted cloud environment. Our proposed scheme achieves secure and fine-grained access control with scalable and efficient revocation based on the pseudo-random encryption, symmetric encryption, and ciphertext-policy attribute-based encryption (CP-ABE), and graph-based modeling. In our scheme, we utilized fog computing to transfer the resource-intensive tasks of encrypting and decrypting CP-ABE. Additionally, we introduced an adaptive load sharing algorithm to facilitate effective distribution of workloads among fog nodes. Moreover, we integrate blockchain technology to perform user authentication and verify the integrity of the EHRs within the system. Finally, we conducted a security analysis, comparative computation analysis, and experiments, demonstrating that the encryption and decryption costs of our scheme are comparable to related works. Furthermore, our proposed ciphertext retrieval mechanism, which is essential for ciphertext re-encryption resulting from user revocation, is more efficient than the traditional method."
pub.1157134229,A Survey on Low-Power GNSS,"With the miniaturization of electronics, receivers are getting more and more embedded into devices with harsh energy constraints. This process has led to new signal processing challenges due to the limited processing power on battery-operated devices and to challenging wireless environments, such as deep urban canyons, tunnels and bridges, forest canopies, increased jamming and spoofing. The latter is typically tackled via new GNSS constellations and modernization of the GNSS signals. However, the increase in signal complexity leads to higher computation requirements to recover the signals; thus, the trade-off between precision and energy should be evaluated for each application. This paper dives into low-power GNSS, focusing on the energy consumption of satellite-based positioning receivers used in battery-operated consumer devices and Internet of Things (IoT) sensors. We briefly overview the GNSS basics and the differences between legacy and modernized signals. Factors dominating the energy consumption of GNSS receivers are then reviewed, with special attention given to the complexity of the processing algorithms. Onboard and offloaded (Cloud/Edge) processing strategies are explored and compared. Finally, we highlight the current challenges of today’s research in low-power GNSS."
pub.1142670071,An Energy Efficient Agent Aware Proactive Fault Tolerance for Preventing Deterioration of Virtual Machines Within Cloud Environment,"Nowadays, Cloud Computing is widely adopted in current scenario for both personal and professional usage. With advent of technology, large scale usage and increasing growth of cloud computing resources has led to an important issue which is cloud service reliability and high energy consumption. The major issue in cloud computing is data availability, backup replication, data efficiency and reliability because of the failures encountered during the execution. So, for ensuring reliability and availability and limit the use of energy in cloud a technique for fault tolerance must be developed. Presently two main types of fault tolerant techniques such as proactive and reactive fault tolerance are available. In the existing fault tolerant techniques, the focus is mainly on the way to tolerate fault but co-ordination between VMs is missing. Reactive fault tolerance techniques work only after fault is encountered i.e., the system is diagnosed after fault occurs and these techniques lower the effects of faults. To overcome this problem, we first allocate tasks to the virtual machines by using concept of Hot Queues and Cold Queues to reduce total energy consumption and allocating the load among VMs in such a manner that no task goes into starvation state. In this paper agent-based monitoring mechanism is proposed to dynamically monitor the running statistics of all jobs running on VMs. Moreover, our agent-based monitoring system is able to resolve the problem of deteriorating Virtual Machine. In case deterioration is detected, migration is initiated causing high fault tolerance rate and Performance optimization in terms of overhead and execution time, energy efficiency is observed."
pub.1016928887,A performance study of live VM migration technologies: VMotion vs XenMotion,"Due to the growing demand of flexible resource management for cloud computing services, researches on live virtual machine migration have attained more and more attention 1. Live migration of virtual machine across different hosts has been a powerful tool to facilitate system maintenance, load balancing, fault tolerance and so on. In this paper, we use a measurement-based approach to compare the performance of two major live migration technologies under certain network conditions, i.e., VMotion and XenMotion. The results show that VMotion generates much less data transferred than XenMotion when migrating identical VMs. However, in network with moderate packet loss and delay, which are typical in a VPN (virtual private network) scenario used to connect the data centers, XenMotion outperforms VMotion in total migration time. We hope that this study can be helpful in choosing suitable virtualization environments for data center administrators and optimizing existing live migration mechanisms."
pub.1169021231,Comprehensive systematic review of information fusion methods in smart cities and urban environments,"Smart cities result from integrating advanced technologies and intelligent sensors into modern urban infrastructure. The Internet of Things (IoT) and data integration are pivotal in creating interconnected and intelligent urban spaces. In this literature review, we explore the different methods of information fusion used in smart cities, along with their advantages and challenges. However, there are notable challenges in managing diverse data sources, handling large data volumes, and meeting the near-real-time demands of various smart city applications. The review aims to examine smart city applications in detail, incorporating quality evaluation and information fusion techniques and identifying critical issues while outlining promising research directions. In order to accomplish our goal, we conducted a comprehensive search of literature and applied selective criteria. We identified 59 recent studies addressing machine learning (ML) and deep learning (DL) techniques in smart city applications. These studies were obtained from various databases such as ScienceDirect (SD), Scopus, Web of Science (WoS), and IEEE Xplore. The main objective of this study is to provide more detailed insights into smart cities by supplementing existing research. The word cloud visualisation of machine learning/deep learning and information fusion in smart cities papers shows a diverse landscape, covering both technical aspects of artificial intelligence and practical applications in urban settings. Apart from technical exploration, the study also delves into the ethical and privacy implications arising in smart cities. Moreover, it thoroughly examines the challenges that must be addressed to realise this urban revolution's potential fully."
pub.1167089956,Evaluation and Challenges of IoT Simulators for Intelligent Transportation System Applications,"The Internet-of-Things (IoT) constructs a vast, intricate, and perpetually evolving ecosystem exerting profound societal implications. This labyrinthine nature often culminates in errors that directly impact human lives. A significant domain where this complexity materializes is Intelligent Transportation Systems (ITS). Present tools and methodologies inadequately accommodate the complex task of testing and validation, underscoring the urgency for comprehensive review and enhancement. This study aims to present a broad analysis of existing simulators utilized for ITS simulations. It delves into the role and effectiveness of such simulation tools, highlighting their limitations and proposing research directions. This paper scrutinizes both commercial and research-oriented IoT simulators for ITS, evaluating their features and simulation environment tools. We have detailed various ITS scenarios simulated within these frameworks, intending to gauge their readiness for real-world ITS applications and to elaborate on the challenges involved in ITS infrastructure implementation. The findings suggest that despite numerous simulators aiding the evolution of solutions for IoT challenges in recent years, their utility in actual ITS implementations remain uncertain. Consequently, we explore public cloud platforms offering IoT simulation capabilities, focusing particularly on the capabilities provided by the Amazon Web Services (AWS) IoT simulation for this study. Our research outlines the pressing challenges in this field, while proposing potential solutions and flagging opportunities for further research. This study paves the way towards improving the reliability and accuracy of IoT simulators in the context of ITS, which has immense potential to enhance the quality of human life."
pub.1132098742,A Framework for Emergency Remote Care and Monitoring Using Internet of Things,"With the growing edge of technology evolution, more or less every aspect of human life has been adopted in the format of remote observing, central data storage and analysis or prediction. In this regard, a new era of research has been introduced where environment data-sensing can be formulated and analyzed by technology and predict data output with high probability. Internet of Things (IoT) is gradually being established as the new computing paradigm, which is bound to change the manner of our everyday working and living. IoT gives an environment for developing a network/framework to establish the interconnection between stand-alone electronic devices. It offers a platform to collect, process and analyze data sets for instructing and controlling the physical world by using sensor devices (wired or non-wired). The data processing done through sensors also gives a method of being energy-efficient or using a data-processing scheme with lower power consumption, which in turn introduces the virtues of green computing. The primary purpose of this book chapter is to focus on the IoT applications for healthcare management systems and development. An IoT system applied to healthcare enhances the existing technology for diagnosis, detection and the general practice of disease analysis and medication to cure it. The infrastructure required to do these basic level analysis and usage of these technologies can be allied components, i.e. hardware platforms, cloud storage, communication technologies and wearable systems. The systems need to be developed by using various sensors which are used for collecting real-world data or data pulses and then sending them for analysis and further fragmentation or outcomes as required. It increases both the accuracy and size of medical data through diverse data collection from large sets of real-world cases. They further improve the precision of medical care delivery through more sophisticated integration of the healthcare system. The idea is to develop a remote healthcare monitoring or suggestive system that can fetch data sets like body temperature, pulse rate, movement in human body, blood flow or some emergency blood pressure changes, and simultaneously, data from human surroundings can also be retrieved by IR sensors placed in the patient’s environment, e.g. in a living room or washroom, etc. For situations where users have abnormal health conditions (or diversions from normal parameters), but not been able to visit a medical center for health advice, this technology can be utilized as a remote healthcare monitoring system. This chapter aims to focus on the Internet of Things (IoT) applications for healthcare management systems and development. It describes a framework for an emergency remote care and monitoring system using IoT, using IoT devices connected to a central server for analysis and feedback. Healthcare management is being introduced as a remote care and monitoring framework through which the distance between "
pub.1100117147,Performance Assurance Model for Applications on SPARK Platform,"The wide availability of open source big data processing frameworks, such as Spark, has increased migration of existing applications and deployment of new applications to these cost-effective platforms. One of the challenges is assuring performance of an application with increase in data size in production system. We have addressed this problem in our work for Spark platform using a performance prediction model in development environment. We have proposed a grey box approach to estimate an application execution time on Spark cluster for higher data size using measurements on low volume data in a small size cluster. The proposed model may also be used iteratively to estimate the competent cluster size for desired application performance in production environment. We have discussed both machine learning and analytic based techniques to build the model. The model is also flexible to different configurations of Spark cluster. This flexibility enables the use of the prediction model with optimization techniques to get tuned value of Spark parameters for optimal performance of deployed application on Spark cluster. Our key innovations in building Spark performance prediction model are support for different configurations of Spark platform, and simulator to estimate Spark stage execution time which includes task execution variability due to HDFS, data skew and cluster nodes heterogeneity. We have shown that our proposed approaches are able to predict within 20% error bound for Wordcount, Terasort, K-means and few TPC-H SQL workloads."
pub.1093892686,Hadoop-MapReduce Job Scheduling Algorithms Survey,"The big data computing era is coming to be a fact in all daily life. As data-intensive become a reality in many of scientific branches, finding an efficient strategy for massive data computing systems has become a multi-objective improvement. Processing these huge data on the distributed hardware clusters as Clouds needs a powerful computation model like Hadoop-MapReduce. In this paper, we studied various schedulers developed in Hadoop in Cloud Environments, features and issues. Most existing studies considered the improvement in the performance from the single point of view (scheduling, locality of data, the correctness of the data, etc) but very few literature involved multi-objectives improvements (quality requirements, scheduling entities, and dynamic environment adaptation), especially in heterogeneous parallel and distributed systems. Hadoop and MapReduce are two important aspects in big data for handling structured and unstructured data. The Creation of an algorithm for node selection is essential to improve and optimize the performance of the MapReduce. This paper introduces a survey of the previous work done in the Hadoop-MapReduce scheduling and gives some suggestion for the improvement of it."
pub.1157311891,Intelligent Analytics in Cyber-Physical Systems,"With today’s competitive business environment, companies are faced with the challenge of making timely decisions based on big data to improve productivity. The Lack of smart analytics tools makes it difficult for many manufacturing systems to manage big data. Several industries are facing the challenges of maintaining their competitiveness while taking advantage of new opportunities as a result of the rapid advancement of information and communication technologies (ICTs). In the case of digital manufacturing, computers and related technologies are used to control all aspects of the assembly process. A cyber-physical system (CPS) consists of computation, communication, and physical processes. In which the industry is being transformed into the next level. Communication and intelligence can help increase the competitiveness of manufacturing by improving efficiency, flexibility, and sustainability. Industry 4.0 relies heavily on key technologies such as Internet of Things, cloud computing, machine-to-machine communications, 3D printing, and big data. Through CPS, the systematic transformation of massive data into information is enabled, allowing degradations and inefficiencies that would otherwise be invisible to be defined and visualized for optimal decision-making. A systematic architecture for implementing CPS in manufacturing is discussed briefly in this paper, followed by a survey of existing trends in the development of industrial big data analytics and CPS. We discuss the 5C CPS architecture and how it enables the integration of CPSs into the manufacturing industry. Finally, we view a case study that illustrates smart machines being designed via the 5C CPS architecture. A cyber-physical system (CPS) consists of computation, communication, and physical processes. In which the industry is being transformed into the next level. Communication and intelligence can help increase the competitiveness of manufacturing by improving efficiency, flexibility, and sustainability. An advanced predictive analytics system integrated with communication technology and machines is known as a cyber-physical system. The 5c structure of CPS consists of smart connections, data-to-info conversion, cyber, cognition, and configuration. The main goal of this study was to design a method or approach for monitoring the health and condition of 30 industrial robots that are being used in the production process. Monitoring torque has become an increasingly popular technique for detecting faults in industrial robots because of its nature, and most research efforts in this area are focused on determining torque. The case study shows the application of the 5C architecture in a manufacturing environment for the processing and management of a fleet of CNC saw machines."
pub.1169724161,Інтеграція технологій доповненої та віртуальної реальності з адаптивними системами навчання: аналіз концептуальних моделей,"Augmented reality (AR) and virtual reality (VR) are increasingly utilized in education to provide interactive and engaging learning experiences. However, most applications do not fully exploit the potential of AR/VR technologies for adaptive and personalized learning. This paper analyzes five recent conceptual models that integrate adaptive techniques into AR/VR educational systems to identify their core components and capabilities. All reviewed models incorporate a user profile, content repository, interaction data, environment representation, and device components. Detailed user information is collected, including demographics, knowledge levels, cognitive characteristics, sensory-motor abilities, and emotional-motivational factors. This enables adapting AR/VR content to individual learners' needs, styles, and states. Two key adaptation-influencing components were identified across the models - the environment and the user adaptation mechanism based on the user model. Additional components depend on the service level and specifics of the device. For mobile applications, cloud computing enables optimal processing of objects, location, and human data. The analysis determined these models provide a strong conceptual basis for adaptive AR/VR learning systems. However, further research is needed to develop a universal framework considering domain specifics. An ontological approach should be employed to allow customization for particular educational contexts. This could significantly enhance the state of adaptive AR/VR learning systems. Existing conceptual models incorporate promising techniques but lack holistic frameworks tailored to educational domains. Developing such frameworks is essential to advance research and practice in adaptive AR/VR learning. The analysis and findings presented provide a foundation to guide future efforts in designing and evaluating adaptive AR/VR educational systems."
pub.1146962104,GELAB – The Cutting Edge of Grammatical Evolution,"The advent of cloud-based super-computing platforms has given rise to a Data Science (DS) boom. Many types of technological problems that were once considered prohibitively expensive to tackle are now candidates for exploration. Machine Learning (ML) tools that were valued only in academic environments are quickly being embraced by industrial giants and tiny startups alike. Coupled with modern-day computing power, ML tools can be looked at as hammers that can deal with even the most stubborn nails. ML tools have become so ubiquitous that the current industrial expectation is that they should not only deliver accurate and intelligent solutions but also do so rapidly. In order to keep pace with these requirements, a new enterprise, referred to as MLOps has blossomed in recent years. MLOps combines the process of ML and DS with an agile software engineering technique to develop and deliver solutions in a fast and iterative way. One of the key challenges to this is that ML and DS tools should be efficient and have better usability characteristics than were traditionally offered. In this paper, we present a novel software for Grammatical Evolution (GE) that meets both of these expectations. Our tool, GELAB, is a toolbox for GE in Matlab which has numerous features that distinguish it from existing contemporary GE software. Firstly, it is user-friendly and its development was aimed for use by non-specialists. Secondly, it is capable of hybrid optimization, in which standard numerical optimization techniques can be added to GE. We have shown experimentally that when hybridized with meta-heuristics GELAB has an overall better performance as compared with standard GE."
pub.1118985297,Genomics as a Service: a Joint Computing and Networking Perspective,"This paper provides a global picture about the deployment of networked
processing services for genomic data sets. Many current research make an
extensive use genomic data, which are massive and rapidly increasing over time.
They are typically stored in remote databases, accessible by using Internet.
For this reason, a significant issue for effectively handling genomic data
through data networks consists of the available network services. A first
contribution of this paper consists of identifying the still unexploited
features of genomic data that could allow optimizing their networked
management. The second and main contribution of this survey consists of a
methodological classification of computing and networking alternatives which
can be used to offer what we call the Genomic-as-a-Service (GaaS) paradigm. In
more detail, we analyze the main genomic processing applications, and classify
not only the main computing alternatives to run genomics workflows in either a
local machine or a distributed cloud environment, but also the main software
technologies available to develop genomic processing services. Since an
analysis encompassing only the computing aspects would provide only a partial
view of the issues for deploying GaaS system, we present also the main
networking technologies that are available to efficiently support a GaaS
solution. We first focus on existing service platforms, and analyze them in
terms of service features, such as scalability, flexibility, and efficiency.
Then, we present a taxonomy for both wide area and datacenter network
technologies that may fit the GaaS requirements. It emerges that
virtualization, both in computing and networking, is the key for a successful
large-scale exploitation of genomic data, by pushing ahead the adoption of the
GaaS paradigm. Finally, the paper illustrates a short and long-term vision on
future research challenges in the field."
pub.1090777763,Modelling and implementation of virtual radio resources management for 5G Cloud RAN,"The virtualisation of Radio Access Networks (RANs) is one of the goals in designing 5G mobile networks. This paper aims at presenting a proof of concept for the virtualisation of radio resources using Open Air Interface (OAI), a software-based Long-Term Evolution (LTE) eNodeB physical emulator. OAI was extended to support multi-tenancy, representing diverse Virtual mobile Network Operators (VNOs) with different Service Level Agreements (SLAs). A comprehensive analytical model for managing the virtual radio resources has been proposed, with two key parts: estimation of available radio resources and their allocation to different VNOs. The estimation is performed by the model, and the allocation is managed by OAI scheduling. Various scenarios and use cases are studied in this virtual RAN environment, network performance being evaluated for different situations, by varying guaranteed levels, serving weights, and used services. Results show that the proposed approach offers almost the same capacity to guaranteed VNOs regardless of other existing VNOs, experiencing at worst a degradation of 32% of its initial allocated data rate, without violation of the guaranteed data rate. The data rate allocated to best effort VNOs may decrease up to 7% of its initial value, which is acceptable, to guarantee other more demanding SLAs."
pub.1092693046,Multimodal human-machine interface devices in the cloud,"In an increasingly connected and multidisciplinary world, we propose a new paradigm of web application development. It makes the whatever modal input to use the same interface to connect to the applications. It can essentially free the web application programmers and the end users from the need of physically handling the data input devices when they are building a multimodal system. The same application can be used for a whole range of physically different peripherals, but similar from the logical point of view of data entry. This paper discusses the implementation of a pilot project, currently in a local network environment, where all the devices in the LAN are identified and described in an interface server. Users in the local network may, upon request, make use of such devices. The communication of these peripherals with the web applications will be carried out by a network of modules that run under the websocket technology. The whole process of communication and connection establishment is automatic and guided by the existing configurations in the interface server. The entire platform runs under SOA strategy and is fully scalable and configurable. Its use is not limited to games because it has much wider possibilities, interactivity in teaching, accessibility for people with special needs, adaptation of web applications to the use of uninitiated, etc."
pub.1140943989,Multi-edge collaborative offloading and energy threshold-based task migration in mobile edge computing environment,"Computation offloading and service migration are two major research hotspots in the mobile edge computing (MEC) environment. However, in the existing MEC architecture, the idle computing resources of offsite edge servers are not fully utilized, which leads to the problem of high overall system time and energy costs. In this paper, we propose a multi-edge collaborative computation offloading strategy for this problem. The strategy analyzes and calculates the energy consumption and latency cost of task execution for local terminals, edge servers and central cloud, constructs a computation offloading model with the weighted sum of latency and energy consumption as the optimization objective, and then solves the model using an improved genetic algorithm to obtain the best computation offloading decision. On the other hand, the mobility of users in the MEC environment leads to service migration, which leads to unbalanced load on the edge servers and network congestion, etc. This paper proposes an energy threshold-based task migration strategy. The strategy analyzes the time and energy consumption of service execution and data transmission, designs an edge server selection algorithm based on the energy consumption threshold, constructs a service migration model, and finally solves the optimal service migration strategy by improving the genetic algorithm. Experimental results show that the multi-edge collaborative computation offloading strategy proposed can significantly improve the performance of data transfer cost, energy consumption, and task completion time. The proposed migration strategy based on energy consumption threshold can significantly improve the performance of mobile server energy consumption, service completion time, and data transfer energy consumption."
pub.1106114146,Genomics as a service: A joint computing and networking perspective,"This paper shows a global picture of the deployment of networked processing services for genomic data sets. Many current research and medical activities make an extensive use of genomic data, which are massive and rapidly increasing over time. They are typically stored in remote databases, accessible by using Internet connections. For this reason, the quality of the available network services could be a significant issue for effectively handling genomic data through networks. A first contribution of this paper consists in identifying the still unexploited features of genomic data that could allow optimizing their networked management. The second and main contribution is a methodological classification of computing and networking alternatives, which can be used to deploy what we call the Genomics-as-a-Service (GaaS) paradigm. In more detail, we analyze the main genomic processing applications, and classify both the computing alternatives to run genomics workflows, in either a local machine or a distributed cloud environment, and the main software technologies available to develop genomic processing services. Since an analysis encompassing only the computing aspects would provide only a partial view of the issues for deploying GaaS systems, we present also the main networking technologies that are available to efficiently support a GaaS solution. We first focus on existing service platforms, and analyze them in terms of service features, such as scalability, flexibility, and efficiency. Then, we present a taxonomy for both wide area and datacenter network technologies that may fit the GaaS requirements. It emerges that virtualization, both in computing and networking, is the key for a successful large-scale exploitation of genomic data, by pushing ahead the adoption of the GaaS paradigm. Finally, the paper illustrates a short and long-term vision on future research challenges in the field."
pub.1035937898,Towards a Common Authorization Infrastructure for the Grid,"In this paper we report results of the ongoing effort to provide a seamless authorization for the UNICORE and Globus Toolkit middlewares using the UNICORE Virtual Organizations System (UVOS). The UVOS is already well integrated with the UNICORE middleware. We have designed and created a set of native Globus Toolkit 4 modules which enable a UVOS based authorization, with a similar functionality as its UNICORE counterpart. Actually the same authorization data stored on the UVOS server can serve both middlewares simultaneously. The paper provides an overview of existing approaches to the user management problem in the Grid environment with a special emphasis on those which can be used across different grid middlewares. The paper presents the UVOS system, its features and how its adoption helps to manage users of the UNICORE and Globus 4 middlewares."
pub.1151484426,Pyleoclim: Paleoclimate Timeseries Analysis and Visualization With Python,"Abstract  We present a Python package geared toward the intuitive analysis and visualization of paleoclimate timeseries, Pyleoclim . The code is open‐source, object‐oriented, and built upon the standard scientific Python stack, allowing users to take advantage of a large collection of existing and emerging techniques. We describe the code's philosophy, structure, and base functionalities and apply it to three paleoclimate problems: (a) orbital‐scale climate variability in a deep‐sea core, illustrating spectral, wavelet, and coherency analysis in the presence of age uncertainties; (b) correlating a high‐resolution speleothem to a climate field, illustrating correlation analysis in the presence of various statistical pitfalls (including age uncertainties); (c) model‐data confrontations in the frequency domain, illustrating the characterization of scaling behavior. We show how the package may be used for transparent and reproducible analysis of paleoclimate and paleoceanographic datasets, supporting Findable, Accessible, Interoperable, and Reusable software and an open science ethos. The package is supported by an extensive documentation and a growing library of tutorials shared publicly as videos and cloud‐executable Jupyter notebooks, to encourage adoption by new users. 
Plain Language Summary  This article describes a software application called Pyleoclim meant to help scientists analyze datasets of ordered observations, particularly applicable to the study of past climates, environments, and ecology. Pyleoclim is meant to be used by domain scientists as well as students interested in learning more about Earth's climate through examples provided in the documentation and online tutorials. Pyleoclim is intended to help scientists save time with their analyses, documenting the steps for better transparency, and as such, allows other scientists to reproduce results from previous studies. 
Key Points     Pyleoclim makes timeseries analysis tools accessible to practicing scientists, via a user‐friendly Python package     Three Jupyter Notebooks illustrate how Pyleoclim facilitates common and advanced tasks     Pyleoclim can enhance reproducibility and rigor of paleogeoscientific workflows involving timeseries    "
pub.1136428965,VM Consolidation based on Overload Detection and VM Selection Policy,"Even though cloud computing has been a big boon to the ICT (Information and communication technology) industry, it faces high energy consumption and substantial CO2 emission. Due to the increase in demand for computational resources, it is now necessary and of utmost significance to improve the energy consumption of the cloud system. Virtual Machine (VM) consolidation is one of the powerful tools to improve energy efficiency as it reduces the number of VM migrations by managing VMs from overloaded/underloaded hosts. Implementation of VM consolidation techniques leads to a decrease in the amount of hardware consumption, energy consumption, and data footprints which leads to an increased Quality of Service (QoS). In this paper, an energy aware VM selection algorithm is proposed along with an overload detection algorithm. The proposed algorithm runs on the CloudSim toolkit environment and analyzes it based on different parameters like energy consumption, SLA violation, server shutdown, and the number of VM migrations to analyze energy efficiency improvement. This modified approach exhibited better performance on all the parameters as compared to the existing algorithms."
pub.1100793064,LayerMover: Fast virtual machine migration over WAN with three-layer image structure," Live Virtual Machine (VM) migration across data centers is an important technology to facilitate cloud management and deepen the cooperation between cloud providers. Without the support of a shared storage system between data centers, migrating the storage data (i.e. virtual disk) of a VM becomes the bottleneck of live VM migration over Wide Area Network (WAN) due to the contradiction between the low bandwidth of the Internet and the comparatively large size of VM storage data. According to previous studies, many inter- and intra-VM duplicated blocks exist between VM disk images. Therefore, data deduplication is widely used for accelerating VM storage data migration. However, it must make a trade-off between computation cost and transmission benefit. Existing approaches are fragile as they explore only the static data feature of image files without much consideration on data semantics. They may adversely influence on migration performance when the benefit resulting from data deduplication cannot remedy its computation overhead. In this paper, we propose a new space-efficient VM image structure—three-layer structure. According to different functions and features, the data of a VM are separated into an Operating System (OS) layer, a Working Environment (WE) layer, and a User Data (UD) layer. Based on this structure, we design a novel VM migration system—LayerMover. It mainly focuses on improving migration performance through optimizing the data deduplication technique. Our experimental results show that three-layer image structure can improve data sharing between VMs, and the similarity ratio between WE images can reach 70%. The tests for LayerMover indicate that it can be significantly beneficial to VM migration across data centers, especially when multiple VMs which share base images are migrated."
pub.1135342198,"The CHIMAERA system for retrievals of cloud top, optical and microphysical properties from imaging sensors","Continuity and consistency of geophysical retrieval products obtained from different Earth-observing spaceborne or airborne atmospheric multispectral imagers can be challenging due to inherent differences in the instruments and/or the use of different retrieval algorithms. The Cross-platform HIgh resolution Multi-instrument AtmosphEric Retrieval Algorithms (CHIMAERA) system addresses the latter aspect of the inter-sensor continuity problem for cloud property retrievals by removing retrieval methodology and implementation as a source of inconsistency when applied to instruments that share common measurement capabilities. Transferring an existing retrieval algorithm to a new sensor oftentimes is a nontrivial task, as it is common for an algorithm code to be tightly coupled to the sensor for which it was developed. By creating a clear division between the science algorithm and the instrument I/O codes, CHIMAERA allows easy migration of science algorithms to different sensors. CHIMAERA is built from C and FORTRAN source code, and can operate in a variety of environments ranging from a personal laptop to a high-performance computing environment for near real-time satellite data production. It is highly adaptable, low-maintenance and allows for easy expansion such that adding new instruments into the system requires only instrument-specific I/O and provision of any external lookup tables specific to the instrument's spectral characteristics. CHIMAERA currently officially supports 13 spaceborne and airborne atmospheric imagers from a single code base, and has been in use since 2007. This paper describes the engineering aspects of CHIMAERA and a few examples from its many current applications are briefly discussed."
pub.1122286150,"The CHIMAERA system for retrievals of cloud top, optical and microphysical properties from imaging sensors","Continuity and consistency of geophysical retrieval products obtained from different Earth-observing spaceborne or airborne atmospheric multispectral imagers can be challenging due to inherent differences in the instruments and/or the use of different retrieval algorithms. The Cross-platform HIgh resolution Multi-instrument AtmosphEric Retrieval Algorithms (CHIMAERA) system addresses the latter aspect of the inter-sensor continuity problem for cloud property retrievals by removing retrieval methodology and implementation as a source of inconsistency when applied to instruments that share common measurement capabilities. Transferring an existing retrieval algorithm to a new sensor oftentimes is a nontrivial task, as it is common for an algorithm code to be tightly coupled to the sensor for which it was developed. By creating a clear division between the science algorithm and the instrument I/O codes, CHIMAERA allows easy migration of science algorithms to different sensors. CHIMAERA is built from C and FORTRAN source code, and can operate in a variety of environments ranging from a personal laptop to a high-performance computing environment for near real-time satellite data production. It is highly adaptable, low-maintenance and allows for easy expansion such that adding new instruments into the system requires only instrument-specific I/O and provision of any external lookup tables specific to the instrument's spectral characteristics. CHIMAERA currently supports 14 spaceborne and airborne atmospheric imagers from a single code base, and has been in use since 2007. In this paper we describe the engineering aspects of CHIMAERA and briefly discuss a few examples from its many current applications."
pub.1146814703,Data Fabric as an Effective Method of Data Management in Traffic and Road Systems,"The article discusses the basic principles of Data Fabric technology using the example of IBM, Microsoft and Cloudera platforms, which opened a new perspective in the field of data management, where it became possible to implement big data in the cloud, which supports the processes of organizing and preparing for use and analysis of huge amounts of information. The presented concept of Data Fabric technology can look like a system for collecting data on transport events and can be created on the principle of a blockchain network using a transport environment modeling tool. The data structures will contain data collected about accidents involving vehicles, reports of vehicle owners, roadside equipment of the transport and road complex systems. Blockchain can be used to create a secure, trusted, and decentralized autonomous Intelligent Transport Systems (ITS) ecosystem that makes better use of the legacy ITS infrastructure and resources, which is especially effective for crowdsourcing technology. While it’s not enough to just collect and store large amounts of data, businesses need to seamlessly and securely access, manage and use this data to drive digital transformation and successfully implement artificial intelligence. The article is aimed at stimulating further developments and providing useful materials for future research in this area."
pub.1162936171,EasyChain: an IoT-friendly blockchain for robust and energy-efficient authentication,"The Internet of Everything (IoE) is a bigger picture that tries to fit the Internet of Things (IoT) that is widely deployed in smart applications. IoE brings people, data, processes, and things to form a network that is more connected and increases overall system intelligence. A further investigation of the IoE can really mean creating a distributed network focusing on edge computing instead of relying on the cloud. Blockchain is one of the recently distributed network technologies which by structure and operations provide data integrity and security in trust-less P2P networks such as IoE. Blockchain can also remove the need for central entities which is the main hurdle for the wide adoption of IoT in large networks. IoT “things” are resource-constrained both in power and computation to adopt the conventional blockchain consensus algorithms that are power and compute-hungry. To solve that problem, this paper proposes EasyChain, a blockchain that is robust along with running on a lightweight authentication-based consensus protocol that is known as Proof-of-Authentication (PoAh). This blockchain based on the lightweight consensus protocol replaces the power-hungry transaction, blocks validation steps, and provides ease of usage in resource-constrained environments such as IoE. The proposed blockchain is designed using the Python language for an easy understanding of the functions and increased ease of integration into IoE applications. The designed blockchain system is also deployed on a single-board computer to analyze its feasibility and scalability. The latency observed in the simulated and experimental evaluations is 148.89 ms which is very fast compared to the existing algorithms."
pub.1176225807,Construction of an Intelligent Management Model for Ideological and Political Education in Colleges and Universities under the Internet of Things Environment,"Abstract The new generation of information technology represented by the Internet of Things, big data, and cloud computing has a great impact on human society, and the integration of the new generation of information technology into ideological and political education is of great significance. This paper focuses on the application of Internet of Things technology in the management of ideological and political education in colleges and universities and builds an intelligent management model of ideological and political education in colleges and universities relying on the campus network. The model mainly includes school registration management, student attendance management, student computer management, and educational resource management. Based on the deficiencies of the existing educational resource management, we refer to the label information, locate the target information, protect and share the extracted resource data through private cloud technology, and complete the design of the sharing system of the Civic and Political Education resources. This paper’s Civic and Political Education Resource Sharing System has a smoother operational effect and a lower operational interference rate than the current system, as demonstrated by the experimental results. Taking A university as the experimental object, the proportion of students whose ideological and political education evaluation results are excellent after adopting intelligent management is 65.42% of the total number of students, which indicates that the intelligent management model can effectively improve the effect of students’ ideological and political education."
pub.1125535444,A Web-Based Virtual Research Environment for Marine Data,"<p>Like most areas of research, the marine sciences are undergoing an increased use of observational data from a multitude of sensors. As it is cumbersome to download, combine and process the increasing volume of data on the individual researcher's desktop computer, many areas of research turn to web- and cloud-based platforms. In the scope of the SeaDataCloud project, such a platform is being developed together with the EUDAT consortium.</p><p>The SeaDataCloud Virtual Research Environment (VRE) is designed to give researchers access to popular processing and visualization tools and to commonly used marine datasets of the SeaDataNet community. Some key aspects such as user authentication, hosting input and output data, are based on EUDAT services, with the perspective of integration into EOSC at a later stage.</p><p>The technical infrastructure is provided by five large EUDAT computing centres across Europe, where operational environments are heterogeneous and spatially far apart. The processing tools (pre-existing as desktop versions) are developed by various institutions of the SeaDataNet community. While some of the services interact with users via command line and can comfortably be exposed as JupyterNotebooks, many of them are very visual (e.g. user interaction with a map) and rely heavily on graphical user interfaces.</p><p>In this presentation, we will address some of the issues we encountered while building an integrated service out of the individual applications, and present our approaches to deal with them.</p><p>Heterogeneity in operational environments and dependencies is easily overcome by using Docker containers. Leveraging processing resources all across Europe is the most challenging part as yet. Containers are easily deployed anywhere in Europe, but the heavy dependence on (potentially shared) input data, and the possibility that the same data may be used by various services at the same time or in quick succession means that data synchronization across Europe has to take place at some point of the process. Designing a synchronization mechanism that does this without conflicts or inconsistencies, or coming up with a distribution scheme that minimizes the synchronization problem is not trivial.</p><p>Further issues came up during the adaptation of existing applications for server-based operation. This includes topics such as containerization, user authentication and authorization and other security measures, but also the locking of files, permissions on shared file systems and exploitation of increased hardware resources.</p>"
pub.1159737894,Role of Blockchain Technology in ERP Systems for a Transparent Supply Chain: A Systematic Review of Literature,"This study examines blockchain technology's role in ERP systems for a transparent supply chain. Furthermore, this study investigates the application of blockchain-driven ERP systems, benefits, and challenges facing the supply chain to achieve transparency and authenticity. Over the past few decades, ERP systems have evolved with the integration of new modules with improved capabilities that include new technological improvements like blockchain and cloud-based ERP systems. Such improvements have resulted in ERP systems for firms with expanded business functions, resulting in more streamlined, efficient supply chains and transparent business processes than ever before. Due to significant advancements in the last few years, blockchain applications for operations and supply chain management (OSCM) are still in their infancy. Little is known about the function of blockchain in operational traceability in supply chain functions and other domains. Thus, the study aims to enhance the knowledge of blockchain-driven ERP applications in operation and supply chain management and how companies create and capture business value with blockchain technology. This study gives a well-articulated and in-depth discussion of the role of blockchain-driven ERP in creating value in the supply chain area. An in-depth review of the literature was done to acquire the existing knowledge. The findings of the study, combining current ERP with blockchain technology, would improve optimization, strengthen transaction security, regulate transparency, and build trust in a trustless environment."
pub.1167354642,Thermal Modeling and Thermal-Aware Energy Saving Methods for Cloud Data Centers: A Review,"Constructing energy-efficient cloud data centers (CDCs) is an essential path for the further expansion of cloud computing. As one of the core subsystems of a data center, the cooling system provides a reliable thermal environment for the safe operation of IT equipment while posing a huge energy consumption and carbon emission problem. Thus, it is evident that optimizing energy management of cooling systems with considerable energy-saving potential will be essential to realize the green and low-carbon development of CDCs. Therefore, to track the research progress of data center thermal management technologies, this review focuses on two research efforts: thermal modeling and thermal-aware energy saving methods. First, various thermal modeling approaches are reviewed for air-cooled and liquid-cooled data centers. Secondly, a comprehensive review of existing advanced thermal management approaches is conducted from three perspectives: thermal-aware IT load scheduling, cooling system control optimization, and joint optimization of the IT and cooling systems. Finally, we put forward some open issues and future research directions for thermal management that have not been completely solved. This review aims to provide reasonable suggestions to enhance cooling energy efficiency and further promote the transformation of CDCs to lower energy consumption and sustainable direction."
pub.1141977566,Managing Time-Sensitive IoT Applications via Dynamic Application Task Distribution and Adaptation,"The recent proliferation of the Internet of Things has led to the pervasion of networked IoT devices such as sensors, video cameras, mobile phones, and industrial machines. This has fueled the growth of Time-Sensitive IoT (TS-IoT) applications that must complete the tasks of (1) collecting sensor observations they need from appropriate IoT devices and (2) analyzing the data within application-specific time-bounds. If this is not achieved, the value of these applications and the results they produce depreciates. At present, TS-IoT applications are executed in a distributed IoT environment that consists of heterogeneous computing and networking resources. Due to the heterogeneous and volatile nature (e.g., unpredictable data rates and sudden disconnections) of the IoT environment, it has become a major challenge to ensure the time-bounds of TS-IoT applications. Many existing task management techniques (i.e., techniques that are used to manage the execution of IoT applications in distributed computing resources) that have been proposed to support TS-IoT applications to meet their time-bounds do not provide a sophisticated and complete solution to manage the TS-IoT applications in a manner in which their time-bounds are guaranteed. This paper proposes TIDA, a comprehensive platform for managing TS-IoT applications that includes a task management technique, called DTDA, which incorporates novel task sizing, distribution, and dynamic adaptation techniques. DTDA’s task sizing technique measures the computing resources required to complete each task of the TS-IoT application at hand in each available IoT device, edge computer (e.g., network gateways), and cloud virtual machine. DTDA’s task distribution technique distributes and executes the tasks of each TS-IoT application in a manner that their time-bound requirements are met. Finally, DTDA includes a task adaptation technique that dynamically adapts the distribution of tasks (i.e., redistributes TS-IoT application tasks) when it detects a potential application time-bound violation. The paper describes a proof-of-concept implementation of TIDA that uses Microsoft’s Orleans Actor Framework. Finally, the paper demonstrates that the DTDA task management technique of TIDA meets the time-bound requirements of TS-IoT applications by presenting an experimental evaluation involving real time-sensitive IoT applications from the smart city domain."
pub.1111614230,A Novel Fog Computing Based Architecture to Improve the Performance in Content Delivery Networks,"Along with the continuing evolution of the Internet and its applications, Content Delivery Networks (CDNs) have become a hot topic with both opportunities and challenges. CDNs were mainly proposed to solve content availability and download time issues by delivering content through edge cache servers deployed around the world. In our previous work, we presented a novel CDN architecture based on a Fog computing environment as a promising solution for real-time applications. In such architecture, we proposed to use a name-based routing protocol following the Information Centric Networking (ICN) approach, with a popularity-based caching strategy to guarantee overall delivery performance. To validate our design principle, we have implemented the proposed Fog-based CDN architecture with its major protocol components and evaluated its performance, as shown through this article. On the one hand, we have extended the Optimized Link-State Routing (OLSR) protocol to be content aware (CA-OLSR), i.e., so that it uses content names as routing labels. Then, we have integrated CA-OLSR with the popularity-based caching strategy, which caches only the most popular content (MPC). On the other hand, we have considered two similar architectures for conducting performance comparative studies. The first is pure Fog-based CDN implemented by the original OLSR (IP-based routing) protocol along with the default caching strategy. The second is a classical cloud-based CDN implemented by the original OLSR. Through extensive simulation experiments, we have shown that our Fog-based CDN architecture outperforms the other compared architectures. CA-OLSR achieves the highest packet delivery ratio (PDR) and the lowest delay for all simulated numbers of connected users. Furthermore, the MPC caching strategy shows higher cache hit rates with fewer numbers of caching operations compared to the existing default caching strategy, which caches all the pass-by content."
pub.1165953039,A checkpointing mechanism for virtual clusters using memory-bound time-multiplexed data transfers,"Transparent hypervisor-level checkpoint-restart mechanisms for virtual clusters (VCs) or clusters of virtual machines (VMs) offer an attractive fault tolerance capability for cloud data centers. However, existing mechanisms have suffered from high checkpoint downtimes and overheads. This paper introduces Mekha, a novel hypervisor-level, in-memory coordinated checkpoint-restart mechanism for VCs that leverages precopy live migration. During a VC checkpoint event, Mekha creates a shadow VM for each VM and employs a novel memory-bound timed-multiplex data (MTD) transfer mechanism to replicate the state of each VM to its corresponding shadow VM. We also propose a global ending condition that enables the checkpoint coordinator to control the termination of the MTD algorithm for every VM in a VC, thereby reducing overall checkpoint latency. Furthermore, the checkpoint protocols of Mekha are designed based on barrier synchronizations and virtual time, ensuring the global consistency of checkpoints and utilizing existing data retransmission capabilities to handle message loss. We conducted several experiments to evaluate Mekha using a message passing interface (MPI) application from the NASA advanced supercomputing (NAS) parallel benchmark. The results demonstrate that Mekha significantly reduces checkpoint downtime compared to traditional checkpoint mechanisms. Consequently, Mekha effectively decreases checkpoint overheads while offering efficiency and practicality, making it a viable solution for cloud computing environments."
pub.1120269218,Orbit Image Analysis: An open-source whole slide image analysis tool,"Abstract
                We describe the open-source whole slide image analysis tool Orbit Image Analysis. It is a generic tile-processing engine which allows the execution of various image analysis algorithms provided by either Orbit itself or other open-source solutions using a tile-based map-reduce execution framework. We show its sophisticated machine-learning approach for WSI quantification, and its flexibility by integrating a deep learning segmentation method for complex object detection. It can run locally standalone or connect to the open-source image server OMERO, and provides scale-out functionality to use the Spark framework for distributed computing. We demonstrate the application of Orbit in three real-world use-cases: Idiopathic lung fibrosis, nerve fibre density quantification, and glomeruli detection in kidney.
                
                  Author summary
                  Whole slide images (WSI) are digital scans of samples, e.g. tissue sections. It is very convenient to view samples in this digital form, and with the increasing computation power it can also be used for quantification. These images are often too large to be analysed with standard tools. To overcome this issue, we created on open-source tool called Orbit Image Analysis which divides the images into smaller parts and allows the analysis of it with either embedded algorithms or the integration of existing tools. It also provides mechanisms to process huge amounts of images in distributed computing environments such as clusters or cloud infrastructures. In this paper we describe the Orbit system and demonstrate its application based on three real-word use-cases.
                "
pub.1120375674,Mystique: A Fine-Grained and Transparent Congestion Control Enforcement Scheme,"TCP congestion control is a vital component for the latency of Web services. In practice, a single congestion control mechanism is often used to handle all TCP connections on a Web server, e.g., Cubic for Linux by default. Considering complex and ever-changing networking environment, the default congestion control may not always be the most suitable one. Adjusting congestion control to meet different networking scenarios usually requires modification of TCP stacks on a server. This is difficult, if not impossible, due to various operating system and application configurations on production servers. In this paper, we propose Mystique, a light-weight, flexible, and dynamic congestion control switching scheme that allows network or server administrators to deploy any congestion control schemes transparently without modifying existing TCP stacks on servers. We have implemented Mystique in Open vSwitch (OVS) and conducted extensive test-bed experiments in both public and private cloud environments. Experiment results have demonstrated that Mystique is able to effectively adapt to varying network conditions, and can always employ the most suitable congestion control for each TCP connection. More specifically, Mystique can significantly reduce latency by 18.13% on average when compared with individual congestion controls."
pub.1153214447,Everything You Need to Know about Intelligent Manufacturing,"With the advent of advanced technologies, smart alternatives for improving the manufacturing processes are becoming increasingly important. The manufacturing trend is shifting away from mass production to more customised production. Adopting advanced manufacturing techniques helps the industry in optimising manufacturing processes with little or no downtime. The benefits of industrial automation compel most traditional industries to transform their infrastructure in accordance with Industry 4.0 standards (I4.0). Industry 4.0 is defined as a new organisational ecosystem for controlling the entire value chain involved in the product life cycle. The main difference between I4.0 and previous generations is that it manufactures products based on customer feedback and requirements. Many industries have already shifted to Industry 4.0 due to its novel and revolutionary concepts of implementing advanced technologies like Industrial Internet of Things (IIoTs), Manufacturing 4.0 (Smart Manufacturing), Cloud and Edge Computing, etc. in the existing Industrial infrastructure. To satisfy the need of end users, I4.0 takes feedback from its customers or stakeholders and involves strict human interactions in the manufacturing processes by the time the product reaches the final production stages, it may have undergone several modifications based on customer’s feedback and continuously suggested improvements from deep machine learning algorithms. As a result, it helps the industry to efficiently manage Industrial production waste and its downtimes. This book chapter gives insights of the intelligent manufacturing I4.0 environment, and discusses its nine pillars and their applications. It further develops deep understanding of the challenges that I4.0 is susceptible to while undergoing transformation from a regular mass production industry into a Smart Manufacturing (I 4.0) factory. Many industries have already shifted to Industry 4.0 as a result of its novel and revolutionary concepts of implementing advanced technologies such as Industrial Internet of Things (IIoTs), Manufacturing 4.0 (Smart Manufacturing), Cloud and Edge Computing, and so on into existing industrial infrastructure. To meet the needs of end users, I4.0 solicits feedback from customers or stakeholders and employs strict human interactions in manufacturing processes. By the time the product reaches the final stages of production, it may have undergone several modifications based on customer feedback and continuously suggested improvements from deep machine learning algorithms. As a result, it assists the industry in effectively managing industrial production waste and downtime. This book chapter provides an overview of the intelligent manufacturing I4.0 environment and discusses its nine pillars and their applications. It also provides a thorough understanding of the challenges that I4.0 faces as it transitions from a traditional mass-production industry to a Smart Manufacturing (I 4.0) factory. Th"
pub.1136042870,Legal and Organizational Peculiarities of Digital Transformation of Management While Ensuring Environmentally Sustainable Development of Moscow,"The relevance of the article is due to the introduction of the Digital Economy program in city management, which should bring the life of Russians to a qualitatively new level by 2024. There are legal prerequisites for improving the quality of life in cities using IT technologies. Digitalization improves the system of monitoring the state of the environment, makes it possible to ensure the sustainability of the ecosystem of individual municipalities and the city as a whole. An analysis of the existing system of environmental monitoring shows that it is not effective enough because it captures a large number of scattered indicators for various natural objects, but does not recreate a holistic picture of the state of the environment in a particular municipality. Software implementation of the municipal level geoportal can be carried out on the basis of the computing power and service equipment of the municipal formation or using SaaS-applications based on cloud technologies that provide the ability to use software products of the regional or federal level. As sensor technologies develop and spread, and their cost decreases, it becomes possible not only to connect each natural component to the observing system, but also to more effectively restore, create and transform urban natural objects. However, the shortage of IT specialists adapted to the biological field, the lack of biologists who are adept at computer technology, the insufficient training of people who have to work with new equipment, create obstacles to the introduction of digital technologies in the practice of creating a comfortable urban environment. The purpose of the article is to help overcome the traditional thinking of environmental specialists who prefer the usual means of visualization to modern methods of observation and evaluation, and to develop scientifically based and verified practice of new techniques and means for training specialists combining knowledge of IT technologies and knowledge of the biological cycle sciences. The leading research method is the formal legal method, which allows us to consider this problem as a process of deliberate and conscious mastery of environmental knowledge, skills and abilities by specialists to carry out information and legal actions in digital form, subject to environmental safety and within the framework of the law. The article presents the author’s solution of environmental problems on the basis of the implementation of an automated exchange between interested subjects: territorial bodies of federal executive bodies, local governments, property administrations. Theoretical principles and proposals have been developed to improve environmental legislation and environmental education, aimed at intensifying the training of IT specialists adapted to the biological field, biologists capable of working with computer programs and applications, i.e. people who have to service new equipment to introduce digital technologies into the practice o"
pub.1132465854,3D Design of Indoor Landscape Based on GIS System and Wireless Sensors,"Visualization of 3D objects in space is a common task for almost all scientific and engineering disciplines to easily evaluate the properties of complex objects, material assembly, environmental compatibility, and many other applications. Photography has been at the forefront of software visualization and rapid hardware operation, especially since algorithmization, based on the progress of scientific visualization of profits from 3D object data providers over the past decade. They need to compare their research with the natural taxonomy communication and landscape research and planning framework. Building Information Modeling (BIM) is difficult to classify because nature is complex. Classification should be based on existing National Digital Databases (NDDB) monitoring landscape theory, but there is a theory that is clearly not on display. Some people believe that requires effective development taxonomy and taxonomy evolution and theory. GIS provides a suitable platform that facilitates this development. To overcome the issue, Proposed method using Landscape design based GIS system and wireless sensors analyzing Location-Based Services (LBS), These models are usually only new buildings; Therefore, 3D interior reconstruction from images and / or point clouds can make the task of developing automatic modes easier, faster, and cheaper. Space defines the structural elements inside a building. Doors are the most common components and their detection can be used to know the most effective or to plan ways to effectively penetrate the ecosystem or to evacuate accordingly. This method explores the problem of indoor environment and in-depth study of door candidates. The proposed method has a high gate detection rate and compatibility with potential testing of robust and effective envelope restoration in real data sets."
pub.1172534190,Enhancing Energy Efficiency in Telehealth IoT through Multi-Objective Optimization,"This paper proposes a novel approach to enhance energy efficiency in Telehealth Internet of Things (IoT) through the integration of multi-objective optimization in a hybrid fog/cloud computing platform. Extending beyond existing fog-based telehealth models, the study employs established algorithms (NSGA-II and SPEA2) to optimize diverse and occasionally conflicting metrics such as energy efficiency, response time, throughput, and resource utilization. The resulting Pareto front provides decision-makers with tailored solutions, revealing a harmonious balance between energy efficiency and performance attributes. Simulated experiments demonstrate the effectiveness of this approach, highlighting its potential to reshape optimization paradigms for Telehealth IoT deployments in fog/cloud environments. This research pioneers a comprehensive strategy for reconciling energy efficiency and performance in Telehealth IoT systems, offering insights for informed decision-making and a sustainable future for energy-saving initiatives."
pub.1084060617,Integrating building and urban semantics to empower smart water solutions,"Current urban water research involves intelligent sensing, systems integration, proactive users and data-driven management through advanced analytics. The convergence of building information modeling with the smart water field provides an opportunity to transcend existing operational barriers. Such research would pave the way for demand-side management, active consumers, and demand-optimized networks, through interoperability and a system of systems approach. This paper presents a semantic knowledge management service and domain ontology which support a novel cloud-edge solution, by unifying domestic socio-technical water systems with clean and waste networks at an urban scale, to deliver value-added services for consumers and network operators. The web service integrates state of the art sensing, data analytics and middleware components. We propose an ontology for the domain which describes smart homes, smart metering, telemetry, and geographic information systems, alongside social concepts. This integrates previously isolated systems as well as supply and demand-side interventions, to improve system performance. A use case of demand-optimized management is introduced, and smart home application interoperability is demonstrated, before the performance of the semantic web service is presented and compared to alternatives. Our findings suggest that semantic web technologies and IoT can merge to bring together large data models with dynamic data streams, to support powerful applications in the operational phase of built environment systems."
pub.1161129599,Revolutionizing Digital Healthcare: Unlocking the Power of Blockchain with an Optimized Fuzzy Logic Approach to Authentication and Key Agreement,"Digital healthcare systems play a pivotal role in providing efficient and accessible healthcare services. However, ensuring secure authentication and key agreement mechanisms is essential to protect sensitive patient data and maintain the integrity of the system. The existing methods face limitations in terms of vulnerability to cyber attacks, scalability, and resource utilization. Furthermore, the integration of blockchain technology introduces new complexities that need to be addressed. This research proposes an optimized fuzzy logic approach combined with blockchain technology to address the authentication and key agreement challenges in digital healthcare systems. The proposed solution leverages the flexibility and adaptability of fuzzy logic algorithms to handle uncertainty and imprecision in authentication decisions. By employing fuzzy logic, the system can effectively minimize false positives and false negatives, enhancing the robustness against adversarial attacks. Moreover, the integration of blockchain technology provides a decentralized and tamper-proof infrastructure for securely storing and managing authentication and key agreement data. This ensures transparency and trust in the system, mitigating the risks of unauthorized access and data manipulation. The blockchain-based architecture also enables efficient resource utilization and scalability, allowing the system to handle authentication requests in a timely manner, even in large-scale digital healthcare environments.The proposed method is evaluated by using the NIST Special Database 302 and it shows superior performance compared to existing methods, with minimum False Rejection Rate (FRR), False Acceptance Rate (FAR), and response time. Moreover, the proposed method minimizes communication overhead during the authentication process and resists different cyber attacks including a Replay attack, Man-in-the-middle attack, Denial of Service (DoS) attack, and Impersonation attack. The proposed method achieves excellent performance in terms of security, efficiency, and resistance to various cyber-attacks, making it a promising approach for secure data sharing in P2P cloud environments."
pub.1124433104,Blockchain for IoT-Based Digital Supply Chain: A Survey,"This exploratory investigation aims to discuss current network environment of digital supply chain system and security issues, especially from the Internet world, of digital supply chain management system with applying some advanced information technologies, such as Internet of Things and blockchain, for improving various system performance and properties. This paper introduces the general histories and backgrounds, in terms of information science, of the supply chain and relevant technologies which have been applied or are potential to be applied on supply chain with purpose of lowering cost, facilitating its security and convenience. It provides a comprehensive review of current relative research work and industrial cases from several famous companies. It also illustrates the IoT enablement and security issues of current digital supply chain system, and existing blockchain’s role in this kind of digital system. Finally, this paper concludes several potential or existing security issues and challenges which supply chain management is facing."
pub.1113194572,Intelligent Oilfield - Cloud Based Big Data Service in Upstream Oil and Gas,"Abstract The Oil and Gas (O&G) industry is embracing modern and intelligent digital technologies such as big data analytics, cloud services, machine learning etc. to increase productivity, enhance operations safety, reduce operation cost and mitigate adverse environmental impact. Challenges that come with such an oil field digital transformation include, but are certainly not limited to: information explosion; isolated and incompatible data repositories; logistics for data exchange and communication; obsoleted processes; cost of support; and the lack of data security. In this paper, we introduce an elastically scalable cloud-based platform to provide big data service for the upstream oil and gas industry, with high reliability and high performance on real-time or near real-time services based on industry standards. First, we review the nature of big data within O&G, paying special attention to distributed fiber optic sensing technologies. We highlight the challenges and necessary system requirements to build effective and scalable downhole big data management and analytics. Secondly, we propose a cloud-based platform architecture for data management and analytics services. Finally, we will present multiple case studies and examples with our system as it is applied in the field. We demonstrate that a standardized data communication and security model enables high efficiency for data transmission, storage, management, sharing and processing in a highly secure environment. Using a standard big data framework and tools (e.g., Apache Hadoop, Spark and Kafka) together with machine learning techniques towards autonomous analysis of such data sources, we are able to process extremely large and complex datasets in an efficient way to provide real-time or near real-time data analytical service, including prescriptive and predictive analytics. The proposed integrated service comprises multiple main systems, such as a downhole data acquisition system; data exchange and management system; data processing and analytics system; as well as data visualization, event alerting and reporting system. With emerging fiber optic technologies, this system not only provides services using legacy O&G data such as static reservoir information, fluid characteristics, well log, well completion information, downhole sensing and surface monitoring data, but also incorporates distributed sensing data (DxS) such as distributed temperature sensing (DTS), distributed strain sensing (DSS) and distributed acoustic sensing (DAS) for continuous downhole measurements along the wellbore with very high spatial resolution. It is the addition of the fiber optic distributed sensing technology that has increased exponentially the volume of downhole data needed to be transmitted and securely managed."
pub.1128318987,"The Internet of Things and Big Data Analytics, Integrated Platforms and Industry Use Cases","SHELVING GUIDE: Big Data This book presents academic and practical research in IoT and big data. With contributions from both practitioners and academic researchers, this book examines new technology and compares it to the existing technology. Experimental case studies are related to real-time scenarios. This book: Analyzes current research and development in IoT and big data analytics. Gives an overview of emerging advanced trends and technologies in IoT-based data analytics Studies existing IoT and big-data-related standards and technologies Provides an in-depth analysis of big data analytics implementation, challenges, and issues for the society. Analyzes current research and development in IoT and big data analytics. Gives an overview of emerging advanced trends and technologies in IoT-based data analytics Studies existing IoT and big-data-related standards and technologies Provides an in-depth analysis of big data analytics implementation, challenges, and issues for the society. The Internet of Things and Big Data Analytics: Integrated Platforms and Industry Use Cases examines how multiple challenges in the integration of IoT and big data can be met. The device ecosystem is growing steadily. It is forecast that there will be billions of connected devices in the years to come. Then there is another set of edge and fog devices. When these IoT devices, resource-constrained as well as resource-intensive, interact with one another locally and remotely, the amount of multi-structured data generated, collected, and stored is bound to grow exponentially. Another prominent trend is the integration of IoT devices with cloud-based applications, services, infrastructures, middleware solutions, and databases. This book examines pioneering technologies and tools emerging and evolving in order to capture, cleanse, and crunch data heaps in order to extricate actionable insights. A variety of industry verticals are benefiting from the power of data to produce useful and usable intelligence. This book shows how these industries are embracing IoT-based data analytics platforms and processes to be extremely competitive. This book also examines the application of machine and deep learning algorithms in intelligent systems, services, and environments."
pub.1093362633,A Robust Fusion Method for RGB-D SLAM,"RGB-D cameras are becoming more and more popular in the areas of Simultaneous Localisation and Mapping (SLAM). Visual Feature matching and dense point cloud ICP are the main methods to estimate camera pose in the existing RGB-D SLAM system. Only using visual information, feature matching method can hardly obtain accurate enough result as the appearance features are sparse. Making use of depth information alone, ICP method cannot avoid the result converging to an incorrect local minimum when the environment has poor 3D geometry. In this paper, we propose a robust fusion method, which acquires accurate and robust pose estimation via combining visual information with depth information. The method can obtain relative transformation by minimizing a robust error function, which integrates matching errors from both visual features and dense point clouds. In addition, a loop closure detection mechanism and a pose graph optimizing method are utilized to estimate the globally consistent pose. Therefore the 3D environment can be well reconstructed. Extensive experiments demonstrate that the method can robustly and accurately estimate the camera trajectory for the RGB-D SLAM system, even in the environment with poor 3D geometry or the area with sparse texture."
pub.1118001603,Cloud Based Medical Image Data Analytics in Healthcare Management,"In today’s world, the images form a huge amount of unstructured data from the public and the corporate sector. As a result of the growth of these types of data, modern analytical systems need to interpret and assimilate images. This brings in the need of image processing which involves the transformation from images to analytically organized and structured data. It performs required operation on the given input image and returns the related outputs based on the query. Digital image processing has pushed the envelope for the appraisal in various domains such as healthcare, defense and security, remote sensing, robotic visions, pattern recognitions and satellites. The complication involved in the healthcare domain makes it suitable to explore and induce the concept of image processing and increases the potential for prescriptive analytics. The cloud combined with the Image Processing provides the best environment for analyzing these images using the proposed technique. The proposed work involves processing the input images using the cloud data analytics that provides a user-friendly environment and retrieves the relevant images as well as text for the given user query which produces the outputs that are more efficient in terms of parameters like time, size, security and speed comparatively with the existing data mining process.  "
pub.1174557380,A Real Network Performance Analysis Testbed for Encrypted MQTT in DMS,"The widespread adoption of IoT devices has led to the development of Distributed Measurement Systems (DMS). However, cyber attacks aimed at destroying critical infrastructure and retrieving sensitive data are rising. We proposed a security-oriented VLAN testbed deployed with opensource firmware and constrained hardware to address this issue. The approach uses local MQTT brokers, TLS tunnels for local sensor data, and an SSL tunnel to transmit encrypted data to a cloud-based central broker. The proposal evaluates critical metrics such as Total Ratio, Total Runtime, Average Runtime, Message time, Average Bandwidth, and Total Bandwidth to predict the minimum network throughput for the selected QoS and security. From a measurement science perspective, lower productivity may result in phenomena being observed less frequently, potentially leading to misinterpretations. This paper does not introduce a new method for safeguarding measurement data, but rather evaluates the network performance of commonly used methods applicable to captive and commercial hardware. This scenario is typical for IoT-based DMS. The proposal considers a security-focused VLAN approach, along with targeted solutions for hardware constraints. Given the nature of the worst case, this allows for broad application of the results obtained. Our study marks the initial phase of a larger and more extensive research effort. Specifically, we used commercial hardware such as the Raspberry Pi 4. Our goal is to extend the scope of the research and delve into more complex and detailed questions in our next work. The primary objective is to identify algorithms that ensure optimal data transmission and encryption ratios and explore algorithms that ensure maximum compatibility with existing infrastructures supporting MQTT technology and will facilitate secure connections for geographically dispersed DMS IoT networks, particularly in challenging environments."
pub.1106137439,Enterprise WLAN Security Flaws,"The Increasing number of mobiles and handheld devices that allow wireless access to enterprise data and services is considered a major concern for network designers, implementers and analysts. Enhancements of wireless technologies also accelerate the adoptions of enterprise wireless networks that are widely deployed solely or as an extension to existing wired networks. Bring Your Own Device is an example of the new challenging wireless trends. BYOD environments allow the use of personal mobile computing devices like smart phones, tablets, and laptops for business activities. BYOD has become popular in work places since they keep users in their comfort zone leading to higher productivity and cost reduction for businesses. Nevertheless, business data and services are consequentially subject to several wireless attacks, whether they are hosted on a cloud or on premises, especially when travelling through the open air. Corporates usually apply network-access-control systems for securing enterprise wireless LANs. However, the security systems may be compromised due to detected flaws posing the enterprise critical information to leakage or the entire network to interruption or complete failure. In this paper, we study the impact of wireless attacks on enterprise wireless LANs. The study helps in evaluating the real risks that threaten wireless technologies. In additions, recommended mitigations and practices to overcome the detected vulnerabilities and security flaws are proposed."
pub.1135492702,Smart Grids and Smart Buildings,"The evolutionary transition to smart grids and smart buildings holds a great promise for a cleaner, more efficient power; healthier air; and lower greenhouse gas emissions. This chapter opens with an introduction that provides an overview of conventional power grid and its evolution to smart grid. In section “Smart Grid Technologies”, the smart grid technologies are presented. Section “Renewable Energy Resources in the Grid: Challenges and Opportunities” discusses a number of potential challenges and opportunities in integrating renewable energy (RE) with the existing grid. The next section presents the demand response (DR) concept, which is a way to decrease the electrical load. In section “Geographical Information System (GIS) and Smart Grid”, the importance of geographical information system (GIS) to provide a virtual representation of the smart grid is highlighted. In section “Smart Grid Networking and Communication Technologies”, the role of communication networks in smart grid operation is described. Section “Security and Privacy” presents a discussion of network security and privacy concerns. Section “Cloud Computing for the Smart Grid” explores the use of cloud computing to facilitate the operation of smart grid. Section “Advances and Challenges in the Development of Smart Grids” deals with the advances and challenges in the development of smart grid. The economics of smart grid are discussed in section “Smart Grid Economics”. In section “Road Map to “Mature” Smart Grids”, a model is discussed that can provide guidance and a road map to establish a fully functioning “mature” smart grid. Section “Smart Buildings” introduces smart buildings, their integration within the smart grid infrastructure, the energy management approaches between building clusters and the microgrid environment, and finally provides real case studies of smart buildings."
pub.1138176107,Identity-based outsider anonymous cloud data outsourcing with simultaneous individual transmission for IoT environment,"The integration of the Internet of Things (IoT) and cloud computing has become an attractive cloud-oriented big data processing paradigm, which is playing an important role in efficiency and productivity for digitalization of numerous IoT enabled industries. However, cloud-assisted IoT is also becoming an increasingly attractive target for various cyber-attacks, including the authenticity of outsourcing data, untrustworthiness of third parties, and data security and privacy. As a potential and promising solution to securely outsource data, we present the first construction for an efficient cloud data outsourcing system with simultaneous individual transmission preserving outsider anonymity of the subscribed consumer set. In our system, a data owner generates personalized data for each of the consumers in a group and transmits a common encrypted data for the group in such a way that the subscribed consumer set is completely hidden from the outsiders. Personalized data can be recovered only by an authorized consumer, while the common data can be decrypted by all the authorized consumer in that group. The communication bandwidth is compact in our construction, and the decryption algorithm requires significantly less computation cost. We design our scheme using asymmetric bilinear map over the prime order group to prevent fault attacks on symmetric bilinear map. Our construction is built in identity-based setting without any non-standard q -type security assumption and does not use random oracles. Our scheme enjoys adaptive security against an indistinguishable chosen-plaintext attack under the hardness of the standard decisional bilinear Diffie–Hellman exponent problem. Furthermore, our design supports an exponential number of consumers as the size of the valid identity set grows exponentially with the security parameter, whereas it is only polynomial in the security parameter for the existing cloud data outsourcing systems. In particular, the implementation and performance analysis explicates the advantages of our design for resource-constrained IoT enabled frameworks."
pub.1174649275,An Efficient LiDAR SLAM With Angle-Based Feature Extraction and Voxel-Based Fixed-Lag Smoothing,"Light detection and ranging (LiDAR) simultaneous localization and mapping (SLAM), which provides accurate pose estimation and map construction, has a broad range of applications in autonomous driving. However, in some complex scenarios, such as degraded environments, SLAM performance is not up to the requirements of an autonomous driving system. To aim at the abovementioned problems, we propose an accurate and robust LiDAR SLAM method. First, in order to avoid the uncertainty of LiDAR viewpoint variation, an angle-based feature extraction method is proposed based on the equiangular distribution property of the point cloud. Second, in view of the fact that odometry errors accumulate, we constructed a fixed-lag smoothing (FLS) to jointly optimize the poses of multiple keyframes. In addition, to improve the environmental representation of point cloud maps within a sliding window, we maintain two types of abstract voxel maps. Finally, a voxel-based feature matching method based on voxel geometry constraints is proposed for the refinement of the pose transformation within a sliding window. The performance and efficiency of the proposed method are evaluated on the public KITTI, Mulran, and The Newer College dataset benchmarks and the dataset collected by our sensor system. The experimental results show that accurate feature extraction, efficient voxel feature matching, and consistent FLS help our LiDAR SLAM method achieve better performance in multiple spatially and temporally large-scale scenarios compared to other existing state-of-the-art methods."
pub.1176175697,Marking-Based Perpendicular Parking Slot Detection Algorithm Using LiDAR Sensors,"The emergence of automotive-grade LiDARs has given rise to new potential methods to develop novel advanced driver assistance systems (ADAS). However, accurate and reliable parking slot detection (PSD) remains a challenge, especially in the low-light conditions typical of indoor car parks. Existing camera-based approaches struggle with these conditions and require sensor fusion to determine parking slot occupancy. This paper proposes a parking slot detection (PSD) algorithm which utilizes the intensity of a LiDAR point cloud to detect the markings of perpendicular parking slots. LiDAR-based approaches offer robustness in low-light environments and can directly determine occupancy status using 3D information. The proposed PSD algorithm first segments the ground plane from the LiDAR point cloud and detects the main axis along the driving direction using a random sample consensus algorithm (RANSAC). The remaining ground point cloud is filtered by a dynamic Otsu’s threshold, and the markings of parking slots are detected in multiple windows along the driving direction separately. Hypotheses of parking slots are generated between the markings, which are cross-checked with a non-ground point cloud to determine the occupancy status. Test results showed that the proposed algorithm is robust in detecting perpendicular parking slots in well-marked car parks with high precision, low width error, and low variance. The proposed algorithm is designed in such a way that future adoption for parallel parking slots and combination with free-space-based detection approaches is possible. This solution addresses the limitations of camera-based systems and enhances PSD accuracy and reliability in challenging lighting conditions."
pub.1172993572,Novel Framework for Multi-Scale Occupancy Sensing for Distributed Monitoring in Internet-of-Things,"Occupancy sensing is one of the integral parts of modern evolving security surveillance and monitoring systems used over different types of infrastructure. With multiple forms of occupancy sensors, the prime idea of occupancy sensing is to identify the presence or absence of occupants in a precisely monitored area, followed by transmitting the sensing information for storage or prompting a set of commands from the connected control units. A review of existing schemes exhibits the presence of the adoption of multiple methodologies over different variants of use cases; however, they are quite case-specific, use expensive deployment processes, and perform highly sophisticated operations. Currently, no studies specifically reported using multi-scale occupancy sensing suitable for large and distributed Internet-of-Things (IoT) environments. Therefore, the proposed study introduces a mechanism of novel multi-scale occupancy sensing considering a smart university campus use case. However, it can be implemented over different infrastructures connected to an IoT environment. The proposed scheme is implemented considering varied forms of cost-effective sensors, handheld devices and access points for determining the state of occupancy in a large number of rooms on campus. The sensed data from distributed connected campuses are aggregated over a cloud server subjected to suitable preprocessing to increase the data quality suitable for reliable prediction. Multiple potential learning-based schemes are integrated with a proposed model to explore the best-fit model. This assessment scenario is not reported in the existing scheme to classify states of occupancy. The study outcome shows Convolution Neural Network and Long Short-Term Memory to accomplish higher accuracy than other learning approaches."
pub.1181780763,Design and build an Android-based Occupational Health and Safety Information System application for Universitas Brawijaya,"Occupational Health and Safety (OHS) plays a critical role in safeguarding the well-being of staff, students, and visitors within academic institutions. Universitas Brawijaya recognizes the importance of a comprehensive OHS Information System to efficiently manage and disseminate vital safety information across its campus. This paper presents the design and development of an Android-based Occupational Health and Safety Information System (OHSIS) application tailored specifically for Universitas Brawijaya. The proposed OHSIS application aims to enhance the university’s OHS practices by providing a user-friendly mobile platform that empowers staff, students, and visitors with essential safety-related information. Leveraging the popularity and accessibility of Android smartphones, the application will facilitate real-time access to safety guidelines, emergency protocols, hazard notifications, and safety training materials. The development process involved a systematic approach that encompassed several stages, including requirements gathering, system analysis, design, implementation, and testing. In the requirements gathering phase, input was solicited from key stakeholders, such as university administrators, faculty members, students, and OHS experts, to ensure that the application’s features align with their needs and preferences. The OHSIS application boasts an intuitive user interface, which incorporates efficient navigation and streamlined access to critical safety information. Users can readily browse through a wealth of safety resources, such as safety manuals, instructional videos, and educational modules. Furthermore, the application features an emergency alert system, enabling users to report incidents promptly and receive real-time assistance. Incorporating cloud-based storage and integration with the university’s existing OHS infrastructure, the OHSIS application ensures seamless synchronization and data sharing. Additionally, the app will employ robust security measures to safeguard sensitive user information and protect against unauthorized access. The application was rigorously tested through user acceptance testing and simulated scenarios to evaluate its usability, functionality, and performance. Initial feedback from beta testers exhibited positive responses, highlighting the app’s effectiveness in promoting safety awareness and incident reporting. In conclusion, the Android-based Occupational Health and Safety Information System (OHSIS) application developed for Universitas Brawijaya serves as a powerful tool in strengthening the university’s safety culture. With its comprehensive features and user-friendly design, the OHSIS application is poised to make a significant impact on ensuring the well-being of all members of the university community and fostering a safer academic environment. Future enhancements may include incorporating data analytics to identify patterns and trends in incidents and accidents, enabling proactive safety m"
pub.1148381544,Smart Grids and Smart Buildings,"The evolutionary transition to smart grids and smart buildings holds a great promise for a cleaner, more efficient power; healthier air; and lower greenhouse gas emissions. This chapter opens with an introduction that provides an overview of conventional power grid and its evolution to smart grid. In section “Smart Grid Technologies,” the smart grid technologies are presented. Section “Renewable Energy Resources in the Grid: Challenges and Opportunities” discusses a number of potential challenges and opportunities in integrating renewable energy (RE) with the existing grid. The next section presents the demand response (DR) concept, which is a way to decrease the electrical load. In section “Geographical Information System (GIS) and Smart Grid,” the importance of geographical information system (GIS) to provide a virtual representation of the smart grid is highlighted. In section “Smart Grid Networking and Communication Technologies,” the role of communication networks in smart grid operation is described. Section “Security and Privacy” presents a discussion of network security and privacy concerns. Section “Cloud Computing for the Smart Grid” explores the use of cloud computing to facilitate the operation of smart grid. Section “Advances and Challenges in the Development of Smart Grids” deals with the advances and challenges in the development of smart grid. The economics of smart grid are discussed in section “Smart Grid Economics.” In section “Road Map to ‘Mature’ Smart Grids,” a model is discussed that can provide guidance and a road map to establish a fully functioning “mature” smart grid. Section “Smart Buildings” introduces smart buildings, their integration within the smart grid infrastructure, the energy management approaches between building clusters and the microgrid environment, and finally provides real case studies of smart buildings."
pub.1143990309,Information and communication technologies for intelligent control of ground unmanned multipur-pose vehicles,"At present, there is a situation when the practice of creating modern devices, units and systems of ground unmanned vehicles (GUV) has preceded the theory of information analysis and synthesis of complex systems. Some existing solutions for the information support of the GUV need to be generalized, standardized and unified, the definition of new special requirements for the creation of computer systems and networks in transport should be made. Therefore, it is necessary and actual to develop information and communication technology for intelligent control of multi-purpose GUV. Goal. The purpose of the article is to develop information and communication technology for intelligent control of multi-purpose GUV, which work in conditions of intensive loads, difficult operating conditions, increased responsibility of mechanisms, resulting in reduced human losses and energy consumption, increased machine reliability and control accuracy. Methodology. The methodology of tasks is based on the use of a synergetic approach to the creation of information and communication technology for intelligent control of multi-purpose GUV, as the development of a systematic approach taking into account the adaptation and integration of the vehicle to transport infrastructure and/or rough terrain. The principles of synergetics underlie the construction of mechatronic systems – a combination in one unit of components of different technical nature (mechanical, electrical, computer), which adaptively interact with the external environment as a single functional and structural organism. The synergetic approach deals with phenomena and processes, as a result of which the system – as a whole – may have properties that none of the parts has. Results. The concept of intelligent control of multi-purpose GUV on the basis of artificial hybrid neuro-phase regulators with the use of cloud computing services and deep learning technology is proposed, substantiated and implemented, which allows to qualitatively increase the efficiency of both one vehicle and the transport system on the base combining a synergetic approach and evolutionary methods of learning multilayer artificial neural networks by objectively forming the architecture of these networks on the basis of learning functionalities and appropriate control objectives. Originality. Use of cloud computing services based on deep learning for intelligent control technology of GUV. Practical value. The proposed information and communication technology of intelligent control of multi-purpose GUV will significantly reduce energy consumption, psychophysical stress on crew members, as well as increase the accuracy of location."
pub.1105430515,OpenFlow-based Virtual TAP using Open vSwitch and DPDK,"Currently, server (host) virtualization technology that brings effective use of server resources to a data center is promising as cloud services are being prevalent with increasing traffic volumes and requirements for higher service quality. Proposed network TAP, named vTAP (Virtual Test Access Port), overcomes the problem that existing hardware TAP devices cannot be utilized for virtual network links to monitor traffic among virtual machines (VMs) at a packet level. vTAP can be implemented by a virtual switch that gives network connectivity to VMs by switching packets over the virtual network links. The port mirroring feature of a virtual switch can be a naive solution to provide packet level monitoring among VMs. However, using the feature in an environment that needs to treat large volume of network traffic with low delay such as NFV (Net-work Function Virtualization) incurs performance degradation in packet switching capability of the switch and error-prone manual configurations. This paper provides design and implementation approaches to vTAP using Open vSwitch with DPDK (Data Plane Development Kit) and an OpenFlow SDN (Software-Defined Networking) controller to overcome the problems. DPDK can accelerate overall packet processing operations needed in vTAP, and OpenFlow controller can provide a centralized and flexible way to apply and manage TAP policies in an SDN network. This paper also provides performance comparisons of the proposed vTAP and the naive method, port mlrrorina."
pub.1170705302,Modified bi-directional long short-term memory and hyperparameter tuning of supervised machine learning models for cardiovascular heart disease prediction in mobile cloud environment,"In recent decades, cardiovascular heart disease (CHD) has emerged as the predominant cause of mortality on a global scale. Numerous risk factors for cardiovascular disease necessitate prompt access to accurate, dependable, and efficient early diagnosis and disease management strategies. The timely and precise identification of cardiac disease holds significant importance in the prevention and treatment of heart failure. The reliability of diagnosing heart disease by standard medical history has been widely questioned. Medical imaging, including angiography, is the most widely used method for diagnosing CAD. Nevertheless, angiography is renowned for its high cost and its correlation with many adverse consequences. Noninvasive techniques, such as machine learning, demonstrate reliability and efficiency in the classification of individuals as either healthy or afflicted with cardiac disease. In this research, a new efficient real-time diagnostic method for CHD was designed via hyperparameter tuning of supervised machine learning (SML). This study consists of five significant components: the data preprocessing stage, classification with and without hyperparameter tuning, model integration and clinical decision, intervention and patient engagement, and lastly, model deployment and API integration on Amazon Cloud SageMaker. The data was preprocessed, which included finding outliers using the interquartile range (IQR), normalizing the data, and using the synthetic minority oversampling method (SMOTE) to fix the imbalance in the data. Following this, we propose a new adaptation of the Bi-Directional Long-Short Term Memory (BiLSTM) model, incorporating an attention mechanism (AM) to facilitate feature selection. The feature relevance was determined by calculating the coefficient scores for the top ten (10) features in each of the SMLs, namely the Catboost, extreme gradient boosting (XgBoost), random forest (RF), and linear discriminant analysis (LDA). Both with and without the inclusion of hyperparameter tuning, the grid search process further improved the models. We deployed our proposed models at scale by outsourcing them to the SageMaker cloud environment. The detailed experimental analysis showed that the BiLSTM + AM-XgBoost model worked best when tuned for hyperparameters via the grid search method. Specifically, this model achieved an accuracy of 99.99%, recall of 99.65%, precision of 99.68%, and AUC of 88.16%. The BiLSTM + AM-XgBoost model, without any hyperparameter adjustment, achieved an accuracy of 97.58%, recall of 96.32%, precision of 97.32%, and AUC of 87.32%. The tuning of hyperparameters affects the performance of SML in terms of accuracy, recall, precision, and AUC. In addition, we conducted a comparative analysis of our proposed models against the current state-of-the-art approaches. The findings of our evaluation indicate that our models exhibit a level of performance that outperforms the existing methods. The proposed model can functio"
pub.1145669824,Creating a Reliable IIoT Framework to Prioritize Workplace Safety in Industries Involving Hazardous Processes,"One of the major concerns that plagues the industry even in the 21st century is the loss of life and property that occurs as a consequence of preventable industrial disasters. It may be the Bhopal gas tragedy of 1984 or the Vishakhapatnam gas leak accident of 2020; the substance may vary and the place may vary, but the net result is always a tragedy. The Industrial Internet of Things (IIoT) paradigm that comprises sensors and actuators for the industrial domain may be able to address this grave industrial concern with a reasonably high degree of reliability. The paradigm of real-time systems finds a complementary application in the arena of industrial disaster prevention. Various sensors may monitor the state of the desired and relevant critical variables relating to difficult industrial substances. Data received from these sensors must be analyzed under hard deadline constraints. The actuators may be employed to take automated preventive actions wherever possible. Notifications of imminent disaster (based on the analysis of the sensed data) dispatched to the administrative authorities that give a reasonable headstart are also a welcome step. This chapter reviews the stated issues in greater depth. Best practices for ensuring a reliable IIoT monitoring framework for industrial disaster prevention and management are also presented and discussed. This chapter discusses the existing literature for architectures related to IIoT and propose an architectural view that emphasizes the computational needs of a safety-critical system. General Electric coined the term industrial internet from Evans as the third wave of innovation and change in the evolution of the industrial sector. The concept of ambient intelligence is such that given the information gathered in real time by using sensors and a knowledge base with historical context, a system with such intelligence may be able to take decisions to help the actors of a particular environment. The three major computing paradigms for IoT are cloud, fog, and edge. In the cloud paradigm, the data is stored on multiple servers, and although cloud has the property that the data may be accessed by the end users from anywhere, it still has issues of latency. The communication layer provides the paradigms necessary to transfer the preprocessed good data to the upper decision-making layer."
pub.1139079754,Toward Proactive and Efficient DDoS Mitigation in IIoT Systems: A Moving Target Defense Approach,"Nowadays, a large number of intelligent devices involved in the industrial Internet of Things (IIoT) environment lead to unprecedented challenges in security. Due to limited resources with weak security protection, the IIoT devices can be easily compromised to launch distributed denial-of-service (DDoS) attacks, resulting in catastrophic results. Although there are many DDoS mitigations of traditional static schemes, the proactive defense method to resist attacks has not been well studied. Furthermore, existing proactive schemes ignored the delay-sensitive characteristic of applications under the IIoT environments. To address these issues, we first adopt two kinds of moving target defense (MTD) techniques that dynamically control the admission of devices and migrate service replicas to isolate attackers on limited edge clouds and mitigate DDoS attacks early near its source. Then, we formulate a multistage optimization problem of MTD mechanisms deployment and model it as constrained Markov decision processes in order to maximize the available resources of the system under the limitations of the IIoT environments. Besides, we present an MTD optimal strategy algorithm to solve decision problems in a cost-effective manner. In this article, the proposed algorithm can achieve an optimal admission allocation by means of attackers gathering within the same service where the service migration decisions are assisted by means of value iteration. The experimental results verify that the proposed algorithm, compared with existing strategies, can effectively mitigate DDoS attacks with acceptable degradation of the quality of service."
pub.1153292628,Data Fabric Digital Array Processing in Road Transport Systems,"The article discusses the technology of processing data arrays in Data Fabric using the QML language, which is a powerful open source data management tool. Data is an integral element of the digital transformation of enterprises. But as organizations seek to leverage their data, they face the challenge of handling data from multiple environments and platforms. This predicament with multi-dimensional data becomes even more difficult when hybrid and multi-cloud environments and architectures are implemented in the organization. Today, for many businesses, operational data has largely remained siled and hidden, resulting in massive amounts of big unstructured data. While it’s not enough to simply collect and store large amounts of data, companies need to seamlessly and securely access, manage, and use that data to drive digital transformation and successfully implement artificial intelligence. Data must not only be collected and stored, but also structured and analyzed in order to further use it to make business management decisions. Such decisions can be made not only by shareholders and top managers, but also by middle managers, as well as all employees who need it. In other words, data and the results of their analysis in a modern enterprise should become a service. Such a service should be as flexible and scalable as possible, easily adapting to the needs and size of the organization. Data Fabric solves these serious and complex tasks. It is important to understand that this is not a ""boxed"" solution, which is enough to buy, install, configure integration with existing systems, and then successfully operate. We are talking about methodologies, so you need to understand how they work, how to apply them correctly. The article is aimed at stimulating further developments and providing useful materials for future research in this area."
pub.1113065960,Resource-Aware Detection and Defense System against Multi-Type Attacks in the Cloud: Repeated Bayesian Stackelberg Game,"Cloud-based systems are subject to various attack types launched by Virtual Machines (VMs) manipulated by attackers having different goals and skills. The existing detection and defense mechanisms might be suitable for simple attack environments but become ineffective when the system faces advanced attack scenarios wherein simultaneous attacks of different types are involved. This is because these mechanisms overlook the attackers’ strategies in the detection system’s design, ignore the system’s resource constraints, and lack sufficient knowledge about the attackers’ types and abilities. To address these shortcomings, we propose a repeated Bayesian Stackelberg game consisting of the following phases: risk assessment framework that identifies the VMs’ risk levels, live-migration-based defense mechanism that protects services from being successful targets for attackers, machine-learning-based technique that collects malicious data from VMs using honeypots and employs one-class Support Vector Machine to learn the attackers’ types distributions, and resource-aware Bayesian Stackelberg game that provides the hypervisor with the detection load’s optimal distribution over VMs that maximizes the detection of multi-type attacks. Experiments conducted using Amazon’s datacenter and Amazon Web Services honeypot data reveal that our solution maximizes the detection, minimizes the number of attacked services, and runs efficiently compared to the state-of-the-art detection and defense strategies, namely Collabra, probabilistic migration, Stackelberg, maxmin, and fair allocation."
pub.1152548789,Towards healthcare system integrity using fault-tolerant-based scheduling in edge data center,"Effective healthcare services are one of the vital issues nowadays that required ICT for better management. It is because the volume of data in healthcare services is growing rapidly where the physical computing resources are not anymore enough to handle it. Cloud computing by its adaptive infrastructure is helping healthcare organizations to effectively manage large volumes of data, also supporting better clinical responses. Such computing employed a confederation data center for handling massive users’ requests through edge computing where network devices can improve data processing, analysis speed, and reduce network costs. But the issue arises when it comes to unpredictable deficiencies in network performance such as increases in delay and overhead. It needs a better provisioning mechanism for isolating the deficiency. Due to the healthcare data is comprised of sensitive and non-sensitive data hence the fault isolation process needs to thoroughly manage. It means to sustain dataset privacy and trustworthiness. In this work, the fault-tolerant scheduling approach is proposed for realizing consistency in the data processing. It aims to ensure any network performance issues will not be affected by users’ requests. Our edge computing environment is designed by using the multi-tenant model where it represents diversity in healthcare organizations. The integration between Cloud and edge computing in our environment represents global service (friend) and local service (family). Our fault-tolerant family-and-friend (FT-FnF) scheduling approach is used fuzzy logic for realizing rescheduling and re-allocating decisions. The square matrix multiplication is employed to develop the logic structure and related migration threshold. This strategy intends to identify the suitable nodes prior that to be used as alternative edge processing or storage. By incorporating such strategies, fault isolation and tolerance able to be done effectively while enduring data integrity. Our experimental results show better execution time and computing overhead compared to the existing algorithm. Implicitly, our fault-tolerant approach improves system performance and able to deal with large-scale healthcare communities."
pub.1170195422,Enhancing the resilience of error-prone computing environments using a hybrid multi-objective optimization algorithm for edge-centric cloud computing systems,"The integration of advanced technologies like cloud and edge computing has facilitated efficient resource coordination, resulting in improved overall management and widespread applicability. Simultaneously, addressing energy consumption, reliability, and server and communication link failure rates has become a pressing research concern. To address this challenge, an advanced combined-decomposition whale optimization algorithm has been developed. This algorithm utilizes a unique combined-decomposition operator to identify nearly optimal server coalitions, enhancing the quality of service performance. Through a training process at various utilization levels, servers determine their optimal utilization for achieving the maximum energy-to-reliability trade-off ratio. Then, a head server with the highest optimal utilization leads each of these clusters of servers. Unlike other population-based clustering methods, this algorithm incorporates the whale optimization algorithm, extending its exploration and exploitation capabilities beyond other leading scheduling algorithms. The integration of these techniques successfully achieves the dual objective of balancing energy and reliability, addressing existing challenges, and ensuring optimal energy-reliability trade-offs. Simulation experiments using various evaluation metrics demonstrate that the proposed approach enhances energy efficiency by approximately 17 to 35% and reliability by 25 to 55%, all while meeting quality service standards."
pub.1170471644,Real-time indoor tracking for augmented reality using computer vision technique,"In recent times, there has been an increase in the stability and integration of augmented reality (AR) technology in everyday applications. AR relies on tracking techniques to capture the characteristics of the surrounding environment. Tracking falls into two categories: outdoor and indoor. While outdoor tracking predominantly relies on the global positioning system (GPS), it is performance indoors is hindered by imprecise GPS signals. Indoor tracking offers a solution for navigating complex indoor environments. This paper introduces an indoor tracking system that combines smartphone sensor data and computer vision using the oriented features from accelerated and segments test and rotated binary robust independent elementary features (ORB) algorithm for feature extraction, along with brute force match (BFM) and k-nearest neighbor (KNN) for matching. This approach outperforms previous systems, offering efficient navigation without relying on pre-existing maps. The system uses the A* algorithm to find the shortest path and cloud computing for data storage. Experimental results demonstrate an impressive 99% average accuracy within a 7-10 cm error range, even in scenarios with varying distances. Moreover, all users successfully reached their destinations during the experiments. This innovative model presents a promising advancement in indoor tracking, enhancing the accuracy and effectiveness of navigation in complex indoor spaces"
pub.1158256116,Validating User-Centric Business Process based Service Composition,"Now-a-days, service composition is an interesting mechanism in various fields, such as the Internet of Things (IoT), Cloud Computing, and Software-defined networks. Service composition aims to allow interactions among user requirements and their components. Several research studies have been performed on the formal verification to validate the service composition. However, less attention is paid to the service composition validation (completeness and correctness) in the IoT environment using business processes (BPs). Therefore, this paper provides a validated service composition (VSC) approach in the IoT environment. This approach considers the look-up table and Triple Graph Grammar (TGG) rule for the service composition. Look-up table enables to establish a one-to-one relationship between business process (BP) and service composition. This table has limitations in-terms of representation. So, the TGG rule is used to establish the m-to-n relationships. The BP elements are mapped with the service composition, but the backward transformation is not possible. An algorithm is designed for validation purposes. The case study related to healthcare, i.e., clinical decision support system (CDSS), is used to evaluate the proposed approach. Various dynamic features are considered to compare the VSC with the existing approaches. The novelty of this paper is to prove the VSC is complete but not correct. The effectiveness of the VSC is shown in terms of validation and execution time with respect to BPs."
pub.1141807841,IoT-Cloud Empowered Aerial Scene Classification for Unmanned Aerial Vehicles,"Recent trends in communication technologies and unmanned aerial vehicles (UAVs) find its application in several areas such as healthcare, surveillance, transportation, etc. Besides, the integration of Internet of things (IoT) with cloud computing environment offers several benefits for the UAV communication. At the same time, aerial scene classification is one of the major research areas in UAV-enabled MEC systems. In UAV aerial imagery, efficient image representation is crucial for the purpose of scene classification. The existing scene classification techniques generate mid-level image features with limited representation capabilities that often end up in producing average results. Therefore, the current research work introduces a new DL-enabled aerial scene classification model for UAV-enabled MEC systems. The presented model enables the UAVs to capture aerial images which are then transmitted to MEC for further processing. Next, Capsule Network (CapsNet)-based feature extraction technique is applied to derive a set of useful feature vectors from the aerial image. It is important to have an appropriate hyperparameter tuning strategy, since manual parameter tuning of DL model tend to produce several configuration errors. In order to achieve this and to determine the hyperparameters of CapsNet model, Shuffled Shepherd Optimization (SSO) algorithm is implemented. Finally, Backpropagation Neural Network (BPNN) classification model is applied to determine the appropriate class labels of aerial images. The performance of SSO-CapsNet model was validated against two openly-accessible datasets namely, UC Merced (UCM) Land Use dataset and WHU-RS dataset. The proposed SSO-CapsNet model outperformed the existing state-of-the-art methods and achieved maximum accuracy of 0.983, precision of 0.985, recall of 0.982, and F-score of 0.983."
pub.1168233166,"Integration of Autonomous Robotics, Indoor Localization Technologies, and IoT Sensing for Real-Time Cloud-Based Indoor Air Quality Monitoring and Visualization","Real-time monitoring of indoor air quality (IAQ) is significant for ensuring occupants health and comfort. While smart sensing technologies were used for IAQ monitoring, there is still a gap of quantitatively assessing IAQ and visualizing its interactions with the surrounding physical world. Therefore, this study proposes a novel real-time IAQ monitoring and visualization system. This system specifically integrates an unmanned ground vehicle (UGV), an indoor localization system, and the Internet of Things (IoT). First, an IoT sensing unit was developed to dynamically measure the concentration of different indoor air pollutants. Second, a UGV was configured with the capability of full autonomy for providing mobility to the sensing unit. Third, an indoor localization system was developed using ultra-wideband technology to track the sensing unit. Fourth, a cloud-based web server was created to establish communication for data transmission. Fifth, a 2D visualization interface was generated to visualize the indoor air condition and its interactions with the physical world. The proposed system was validated in a real-world application at the authors institution to test its applicability and performance. The proposed system achieved promising performance regarding (1) full navigation autonomy to sense the indoor environment, (2) reliably monitoring and quantifying the indoor air condition, (3) accurately localizing the sensing unit, and (4) intuitively visualizing the IAQ with consideration of both temporal and spatial characteristics of indoor air condition. This study contributes to the body of knowledge by enhancing existing practices of IAQ monitoring using a cost-effective and mobile robotic system and indoor localization technology."
pub.1123693526,An Efficient Resource Monitoring Service for Fog Computing Environments,"With the increasing number of Internet of Things (IoT) devices, the volume and variety of data being generated by these devices are increasing rapidly. Cloud computing cannot process this data due to its high latency and scalability. In order to process this data in less time, fog computing has evolved as an extension to Cloud computing. In a fog computing environment, a resource monitoring service plays a vital role in providing advanced services, such as scheduling, scaling and migration. Most of the research in fog computing has assumed that a resource monitoring service is already available. Conventional methods proposed for other distributed systems may not be suitable due to the unique features of a fog environment. To improve the overall performance of fog computing and to optimise resource usage, effective resource monitoring techniques are required. Hence, we propose a support and confidence based (SCB) technique which optimises the resource usage in the resource monitoring service. The performance of our proposed system is evaluated by examining a real-time traffic use case in a fog emulator with synthetic data. The experimental results obtained from the fog emulator show that the proposed technique consumes 19 percent lesser resources compared with the existing technique."
pub.1092437005,A green policy to schedule tasks in a distributed cloud,"In the last years, demand and availability of computational capabilities experienced radical changes. Desktops and laptops increased their processing resources, exceeding users’ demand for large part of the day. On the other hand, computational methods are more and more frequently adopted by scientific communities, which often experience difficulties in obtaining access to the required resources. Consequently, data centers for outsourcing use, relying on the cloud computing paradigm, are proliferating. Notwithstanding the effort to build energy-efficient data centers, their energy footprint is still considerable, since cooling a large number of machines situated in the same room or container requires a significant amount of power. The volunteer cloud, exploiting the users’ willingness to share a quote of their underused machine resources, can constitute an effective solution to have the required computational resources when needed. In this paper, we foster the adoption of the volunteer cloud computing as a green (i.e., energy efficient) solution even able to outperform existing data centers in specific tasks. To manage the complexity of such a large scale heterogeneous system, we propose a distributed optimization policy to task scheduling with the aim of reducing the overall energy consumption executing a given workload. To this end, we consider an integer programming problem relying on the Alternating Direction Method of Multipliers (ADMM) for its solution. Our approach is compared with a centralized one and other non-green targeting solutions. Results show that the distributed solution found by the ADMM constitutes a good suboptimal solution, worth to be applied in a real environment."
pub.1104171820,Lessons Learned with Laser Scanning Point Cloud Management in Hadoop HBase,"While big data technologies are growing rapidly and benefit a wide range of science and engineering domains, many barriers remain for the remote sensing community to fully exploit the benefits provided by these powerful and rapidly developing technologies. To overcome existing barriers, this paper presents the in-depth experience gained when adopting a distributed computing framework – Hadoop HBase – for storage, indexing, and integration of large scale, high resolution laser scanning point cloud data. Four data models were conceptualized, implemented, and rigorously investigated to explore the advantageous features of distributed, key-value database systems. In addition, the comparison of the four models facilitated the reassessment of several well-known point cloud management techniques founded in traditional computing environments in the new context of a distributed, key-value database. The four models were derived from two row-key designs and two columns structures, thereby demonstrating various considerations during the development of a data solution for high-resolution, city-scale aerial laser scan for a portion of Dublin, Ireland. This paper presents lessons learned from the data model design and its implementation for spatial data management in a distributed computing framework. The study is a step towards full exploitation of powerful emerging computing assets for dense spatio-temporal data."
pub.1142418341,Green Computing and Blockchain Fundamentals,"The evolution of technology has proven to be a worthwhile revolution for humankind. Technology has touched all aspects of this world from computers to our health; no places are left where we can’t use technology. But everything comes at some cost; technology bringing ease to our lives also brings threats to our lives by degrading the environment. Now, researchers are going on to use technology in a less waste-generating manner and moreover using it to promote greenness, and broadly this concept is known as green computing. So, this chapter discusses the concept of green computing, including the evolution of green computing like how pollution created by computers and peripherals has come into the awareness of people and what measures have been chosen to reduce or deduct the polluting elements. In the latter part, the role of industry and government is mentioned, as are the different approaches to green computing, such as virtualization, power management, and cloud computing. The benefits as well as disadvantages are also discussed. In the next part, information related to blockchain is discussed, consisting of the basic terminology, helping to understand its meaning. The history along with the development of blockchain is discussed and also the application of blockchain like cryptocurrency is discussed; various kinds of blockchain are mentioned as are the challenges which should be overcome to attain greater heights in blockchain technology. In the last part the interrelation of blockchain and green computing is mentioned – how blockchain can contribute in a greater percentage to promote green computing. This chapter discusses the concept of green computing, including the evolution of green computing like how pollution created by computers and peripherals has come into the awareness of people and what measures have been chosen to reduce or deduct the polluting elements. Green computing consists of two words: ‘GREEN’ and ‘COMPUTING.’ GREEN concerns the environment, and COMPUTING means processing. So, it’s computing in a way that doesn’t harm the environment. In the 1990s computers were huge and so was their energy consumption, and they were not switched off when they were idle, so the waste generated from them was tremendous. Virtualization is the creation of a virtual version of something, such as a server, a desktop, a storage device, an operating system, etc. It’s a process that allows sharing a single example of a source or application between multiple clients or organizations. The virtualization of the machine over existing software and hardware is known as Hardware Virtualization."
pub.1169566614,Lightweight privacy-preserving authentication mechanism in 5G-enabled industrial cyber physical systems,"With the deep integration of informatization and industrialization, cyber-physical system (CPS), which integrates computing, communication and control technologies, comes into being and has been widely used in industrial applications. A large number of information physical system devices and control systems are based on open internet connections, and security and privacy protection issues are gradually emerging, especially in the process of dynamic and difficult to control information physical system stream data transmission and storage, which will face new security threats. To alleviate these issues, user authentication mechanisms can prevent unauthorized access by adversaries in the CPS environment. For this purpose, we put forth a lightweight privacy-preserving authentication scheme for 5G-enabled ecosystems. This method utilizes edge devices to save online transaction data in network and securely transmits IoT devices data from the sensors to the edge gateway. Then, the edge gateway delivers and stores these data in the cloud. A detailed comparative study based on experimental results indicates that our proposal achieves a better balance between security and functionality features, communication and capitulation costs compared to other existing competitive solutions."
pub.1118037994,UFPR CampusMap: a laboratory for a Smart City developments,"Abstract. A Smart City is based on intelligent exchanges of information that flow between its many different subsystems. This flow of information is analyzed and translated into citizen and commercial services. The city will act on this information flow to make its wider ecosystem more resource-efficient and sustainable. The information exchange is based on a smart governance operating framework designed to make cities sustainable.The public administration needs updated and reliable geospatial data which depicts the urban environment. These data can be obtained through smart devices (smartphones, e.g.), human agents (collaborative mapping) and remote sensing technologies, such as UAV (Unnamed Aerial Vehicles). According to some authors, there are four dimensions in a Smart City. The first dimension concerns the application of a wide range of electronic and digital technologies to create a cyber, digital, wired, informational or knowledge-based city; the second is the use of information technology to transform life and work; the third is to embed ICT (Information and Communication Technology) in the city infrastructure; the fourth is to bring ICT and people together to enhance innovation, learning, and knowledge. Analyzing these dimensions, it is possible to say that in all of them the geospatial information is crucial, otherwise, none of them are possible. Considering these aspects, this research intends to use the Smart City concept as a methodological approach using the UFPR (Federal University of Parana) as a target to develop a case study.The UFPR has 26 campus in different cities of the Paraná State, south of Brazil. Its structure has 14 institutes. It comprises 11 million square meters of area, 500,000 square meters of constructed area and 316 buildings. There are more than 6,300 employees (staff and administration), 50,000 undergraduate students and 10,000 graduate students. Besides these figures, there are external people who need access to the UFPR facilities, such as deliveries, service providers and the community in general.The lack of knowledge about the space and its characteristics has a direct impact on issues such as resources management (human and material), campi infrastructure (outside and inside of the buildings), security and other activities which can be supported using an updated geospatial database. In 2014, the UFPR CampusMap project was started with the indoor mapping as the main goal. However, the base map of the campus was needed in order to support the indoor mapping, the available one was produced in 2000. Thereafter, the campus Centro Politécnico (located in the city of Curitiba) is being used as a case study to develop methodologies to create a geospatial database which will allows to different users the knowledge and management of the space.According to Gruen (2013), a Smart City must have spatial intelligence. Moreover, it is necessary the establishment of a database, in particular, a geospatial database. The kno"
pub.1105570134,Hardware Accelerated Mappers for Hadoop MapReduce Streaming,"Heterogeneous architectures have emerged as an effective solution to address the energy-efficiency challenges. This is particularly happening in data centers where the integration of FPGA hardware accelerators with general purpose processors such as big Xeon or little Atom cores introduces enormous opportunities to address the power, scalability, and energy-efficiency challenges of processing emerging applications, in particular in domain of big data. Therefore, the rise of hardware accelerators in data centers, raises several important research questions: What is the potential for hardware acceleration in MapReduce, a defacto standard for big data analytics? What is the role of processor after acceleration; whether big or little core is most suited to run big data applications post hardware acceleration? This paper answers these questions through methodical real-system experiments on state-of-the-art hardware acceleration platforms. We first present the implementation of four highly used big data applications in a heterogeneous CPU+FPGA architecture. We develop the MapReduce implementation of K-means, K nearest neighbor, support vector machine, and naive Bayes in a Hadoop Streaming environment that allows developing mapper functions in a non-Java based language suited for interfacing with FPGA based hardware accelerating environment. We present a full implementation of the HW+SW mappers on existing FPGA+core platform and evaluate how a cluster of CPUs equipped with FPGAs uses the accelerated mapper to enhance the overall performance of MapReduce. Moreover, we study how various parameters at the application, system, and architecture levels affect the performance and power-efficiency benefits of Hadoop streaming hardware acceleration. This analysis helps to better understand how presence of HW accelerators for Hadoop MapReduce, changes the choice of CPU, tuning optimization parameters, and scheduling decisions for performance and energy-efficiency improvement. The results show a promising speedup as well as energy-efficiency gains of upto 5.7 and 16 is achieved, respectively, in an end-to-end Hadoop implementation using a semi-automated HLS framework. Results suggest that HW+SW acceleration yields significantly higher speedup on little cores, reducing the performance gap between little and big cores after the acceleration. On the other hand, the energy-efficiency benefit of HW+SW acceleration is higher on the big cores, which reduces the energy-efficiency gap between little and big cores. Overall, the experimental results show that a low cost embedded FPGA platform, programmed using a semi-automated HW+SW co-design methodology, brings significant performance and energy-efficiency gains for Hadoop MapReduce computing in cloud-based architectures and significantly reduces the reliance on large number of big high-performance cores."
pub.1093364589,Performance Evaluation of Energy-Aware Best Fit Decreasing Algorithms for Cloud Environments,"Cloud computing is emerging computational paradigm that provide resources to perform complex tasks. Large datacenters are used to facilitate the incoming tasks by providing resources, such as CPU, memory, storage, and network bandwidth. Datacenters offers hosting and processing of complex tasks and services, where servers and cooling systems consume huge amount of energy. Excessive amount of energy consumption results in large power bills and Green House gases (GHG) emissions. Substantial amount of energy can be saved by powering down servers that are idle. Various authors have come up with energy efficient solutions that try to minimize overall energy consumption. One set of energy-efficient solutions is based on best fit decreasing (BFD) algorithm. In this paper, we evaluate the performance of existing energy efficient BFD algorithms based on various workloads and migration techniques. Moreover, considering the significance of Service Level Agreement (SLA), we introduce SLA-awareness in traditional BFD algorithm to minimize the SLA violation. We present the analysis and observations for each of the considered techniques based on total energy consumption, average SLA violations, and SLA performance degradation due to migration."
pub.1063252167,Abstract 5141: GenomeSpace: an environment for frictionless bioinformatics.,"Abstract Over the past several years, cancer genome characterization initiatives such as The Cancer Genome Atlas, International Cancer Genome Consortium, and the Tumor Sequencing Project have produced an explosion of genomic data. The pace of data production has increased with the adoption of next-generation sequencing technologies and large-scale data production efforts to discover the breadth of genomic variation in humans. Comprehensive analysis of these datasets requires the coordinated use of Web-based data repositories and applications, desktop analysis tools and visualizers, and single-purpose algorithms. However, the effort required to transfer data between tools, convert between data formats, and manage results often prevents researchers from utilizing the wealth of methods available to them. Many integrative genomics and translational “bench to bedside” discoveries are possible with combinations of existing tools, but the necessary transitions between them puts them out of the reach of most researchers. Cloud technologies have produced a new wave of applications that transfer next-generation sequence data directly from sequencers to a compute cloud for storage and analysis, but these systems are at an early stage of maturity and many are tied to commercial sequencing vendors. GenomeSpace, http://www.genomespace.org, is an environment that brings together diverse computational tools, enabling scientists without programming skills to easily combine their capabilities. It aims to offer a common space to create, manipulate and share an ever-growing range of genomic analysis tools. GenomeSpace features support for cloud-based data storage and analysis, multi-tool analytic workflows, automatic conversion of data formats, and ease of connecting new tools to the environment. A set of six “GenomeSpace-enabled” seed tools developed by collaborating organizations provides a comprehensive platform for the analysis of cancer data: Cytoscape (UCSD), Galaxy (Penn State University), GenePattern (Broad Institute), Genomica (Weitzmann Institute), Integrative Genomics Viewer (Broad Institute), and the UCSC Genome Browser (UCSC). The extensible format of the system has empowered a wider range of cancer analyses through the addition of ArrayExpress (European Bioinformatics Institute), InSilico DB (University of Brussels), geWorkbench (Columbia University), and Cistrome (Dana-Farber Cancer Institute). We show how researchers can use GenomeSpace to effortlessly combine the capabilities of all of these tools in several cancer research scenarios. Citation Format: Michael Reich, John Liefeld, Helga Thorvaldsdottir, Marco Ocana, Thorin Tabor, DK Jang, Jill P. Mesirov. GenomeSpace: an environment for frictionless bioinformatics. [abstract]. In: Proceedings of the 104th Annual Meeting of the American Association for Cancer Research; 2013 Apr 6-10; Washington, DC. Philadelphia (PA): AACR; Cancer Res 2013;73(8 Suppl):Abstract nr 5141. doi:10.1158/1538-7445.AM2013-514"
pub.1091004126,Abstract 2593: Accelerating pediatric brain tumor research through team science solutions,"Abstract Introduction: The Children’s Brain Tumor Tissue Consortium (CBTTC), an international repository of genomic and phenotypic data, has partnered with Blackfynn, Inc., to create a cloud-based data management platform to facilitate team-science across disciplines. Background: The CBTTC through the CHOP Department of Biomedical and Health Informatics (DBHi) has developed a network of informatics and data applications for researchers across the globe to work together and perform real-time analyses on existing clinical, phenotypic, and genomic data. Historically, rare disease datasets are siloed, locked in proprietary formats, segregated by data types, and hidden from the view of experts in the field. This has been a significant barrier to finding effective therapeutics for children with pediatric brain tumors. Blackfynn was founded by a group of multidisciplinary experts in neuroscience, neurology, medicine, software development, engineering, computer science and business with the goal to empower researchers to cure neurologic disease and provide solutions to these challenges. Description of Methods: The CBTTC and Blackfynn teamed up to provide a cloud-based, team-focused data management and analytics platform. The platform provides a commercial grade, scalable approach to upload, view, and integrate digital pathology images with relevant subject data such as MRIs, pathology reports and genomic information. Stakeholders can search integrated data without requiring users to change their current workflow or conform to imposed data standards. This platform is a simple, intuitive, end-to-end software platform for teams of scientists and pathologists to review, annotate and discuss cases, enabling rapid diagnostic consensus, quality control, and empowered discovery. Summary of Unpublished Results: The CBTTC/Blackfynn data platform enabled CBTTC members to engage in a cross-institutional collaboration to reach consensus on digital pathology data in ways that were previously not possible. We demonstrated that this solution removes existing barriers to collaborative efforts and provides a rich analytic and discovery platform bridging imaging with genomics and other data formats. The platform provides a new model for the scientific community to facilitate translation towards improved treatments for children diagnosed with brain tumors. Discussion and Future Direction: This pilot project will be scaled to other CBTTC sites for centralized review of pathology images to enable the research community to collaborative on specific projects. The next phase of platform development will include further integration CBTTC platforms fully integrating genomics data, and side-by-side viewing and analyses of MRI, pathology and clincal data to facilitate specific project work around large and complex research data types in a cloud environment. Citation Format: Amanda Christini, Angela J. Waanders, Joost B. Wagenaar, Alex S. Felmeister, Mariarita Santi, Nitin R. Wadhwan"
pub.1125892122,Additional Resources,"Key concepts, definitions, examples, and historical contexts for understanding smart cities, along with discussions of both drawbacks and benefits of this approach to urban problems. Over the past ten years, urban planners, technology companies, and governments have promoted smart cities with a somewhat utopian vision of urban life made knowable and manageable through data collection and analysis. Emerging smart cities have become both crucibles and showrooms for the practical application of the Internet of Things, cloud computing, and the integration of big data into everyday life. Are smart cities optimized, sustainable, digitally networked solutions to urban problems? Or are they neoliberal, corporate-controlled, undemocratic non-places? This volume in the MIT Press Essential Knowledge series offers a concise introduction to smart cities, presenting key concepts, definitions, examples, and historical contexts, along with discussions of both the drawbacks and the benefits of this approach to urban life. After reviewing current terminology and justifications employed by technology designers, journalists, and researchers, the book describes three models for smart city development—smart-from-the-start cities, retrofitted cities, and social cities—and offers examples of each. It covers technologies and methods, including sensors, public wi-fi, big data, and smartphone apps, and discusses how developers conceive of interactions among the built environment, technological and urban infrastructures, citizens, and citizen engagement. Throughout, the author—who has studied smart cities around the world—argues that smart city developers should work more closely with local communities, recognizing their preexisting relationship to urban place and realizing the limits of technological fixes. Smartness is a means to an end: improving the quality of urban life."
pub.1125892112,Smart Cities,"Key concepts, definitions, examples, and historical contexts for understanding smart cities, along with discussions of both drawbacks and benefits of this approach to urban problems. Over the past ten years, urban planners, technology companies, and governments have promoted smart cities with a somewhat utopian vision of urban life made knowable and manageable through data collection and analysis. Emerging smart cities have become both crucibles and showrooms for the practical application of the Internet of Things, cloud computing, and the integration of big data into everyday life. Are smart cities optimized, sustainable, digitally networked solutions to urban problems? Or are they neoliberal, corporate-controlled, undemocratic non-places? This volume in the MIT Press Essential Knowledge series offers a concise introduction to smart cities, presenting key concepts, definitions, examples, and historical contexts, along with discussions of both the drawbacks and the benefits of this approach to urban life. After reviewing current terminology and justifications employed by technology designers, journalists, and researchers, the book describes three models for smart city development—smart-from-the-start cities, retrofitted cities, and social cities—and offers examples of each. It covers technologies and methods, including sensors, public wi-fi, big data, and smartphone apps, and discusses how developers conceive of interactions among the built environment, technological and urban infrastructures, citizens, and citizen engagement. Throughout, the author—who has studied smart cities around the world—argues that smart city developers should work more closely with local communities, recognizing their preexisting relationship to urban place and realizing the limits of technological fixes. Smartness is a means to an end: improving the quality of urban life."
pub.1156577066,Resource Management in Mobile Edge Computing: A Comprehensive Survey," With the evolution of 5G and Internet of Things technologies, Mobile Edge Computing (MEC) has emerged as a major computing paradigm. Compared to cloud computing, MEC integrates network control, computing, and storage to customizable, fast, reliable, and secure distributed services that are closer to the user and data site. Although a popular research topic, MEC resource management comes in many forms due to its emerging nature and there exists little consensus in the community. In this survey, we present a comprehensive review of existing research problems and relevant solutions within MEC resource management. We first describe the major problems in MEC resource allocation when the user applications have diverse performance requirements. We discuss the unique challenges caused by the dynamic nature of the environments and use cases where MEC is adopted. We also explore and categorize existing solutions that address such challenges. We particularly explore traditional optimization-based methods and deep learning-based approaches. In addition, we take a deeper dive into the most popular applications and use cases that adopt MEC paradigm and how MEC provides customized solutions for each use cases, in particular, video analytics applications. Finally, we outline the open research challenges and future directions.  1  "
pub.1017064232,The James Clerk Maxwell Telescope Spectral Legacy Survey,"Stars form in the densest, coldest, most quiescent regions of molecular clouds. Molecules provide the only probes that can reveal the dynamics, physics, chemistry, and evolution of these regions, but our understanding of the molecular inventory of sources and how this is related to their physical state and evolution is rudimentary and incomplete. The Spectral Legacy Survey (SLS) is one of seven surveys recently approved by the James Clerk Maxwell Telescope (JCMT) Board of Directors. Beginning in 2007, the SLS will produce a spectral imaging survey of the content and distribution of all the molecules detected in the 345 GHz atmospheric window (between 332 and 373 GHz) toward a sample of five sources. Our intended targets are a low‐mass core (NGC 1333 IRAS 4), three high‐mass cores spanning a range of star‐forming environments and evolutionary states (W49, AFGL 2591, and IRAS 20126), and a photodissociation region (the Orion Bar). The SLS will use the unique spectral imaging capabilities of HARP‐B/ACSIS (Heterodyne Array Receiver Programme B/Auto‐Correlation Spectrometer and Imaging System) to study the molecular inventory and the physical structure of these objects, which span different evolutionary stages and physical environments and to probe their evolution during the star formation process. As its name suggests, the SLS will provide a lasting data legacy from the JCMT that is intended to benefit the entire astronomical community. As such, the entire data set (including calibrated spectral data cubes, maps of molecular emission, line identifications, and calculations of the gas temperature and column density) will be publicly available."
pub.1095452834,"Big data for cyber physical systems an analysis of challenges, solutions and opportunities","Cyber-Physical Systems (CPS) covers from M2M and Internet of Things (IoT) communications, heterogeneous data integration from multiple sources, security/privacy and its integration into the cloud computing and Big Data platforms. The integration of Big Data into CPS solutions presents several challenges and opportunities. Big Data for CPS is not suitable with conventional solutions based on offline or batch processing. The interconnection with the real-world, in industrial and critical environments, requires reaction in real-time. Therefore, real-time will be a vertical requirement from communication to Big Data analytics. Big Data for CPS requires on the one hand, real-time streams processing for real-time control, and on the other hand, batch processing for modeling and behaviors learning. This paper describes the existing solutions and the pending challenges, providing some guidelines to address the challenges."
pub.1125892113,Series Foreword,"Key concepts, definitions, examples, and historical contexts for understanding smart cities, along with discussions of both drawbacks and benefits of this approach to urban problems. Over the past ten years, urban planners, technology companies, and governments have promoted smart cities with a somewhat utopian vision of urban life made knowable and manageable through data collection and analysis. Emerging smart cities have become both crucibles and showrooms for the practical application of the Internet of Things, cloud computing, and the integration of big data into everyday life. Are smart cities optimized, sustainable, digitally networked solutions to urban problems? Or are they neoliberal, corporate-controlled, undemocratic non-places? This volume in the MIT Press Essential Knowledge series offers a concise introduction to smart cities, presenting key concepts, definitions, examples, and historical contexts, along with discussions of both the drawbacks and the benefits of this approach to urban life. After reviewing current terminology and justifications employed by technology designers, journalists, and researchers, the book describes three models for smart city development—smart-from-the-start cities, retrofitted cities, and social cities—and offers examples of each. It covers technologies and methods, including sensors, public wi-fi, big data, and smartphone apps, and discusses how developers conceive of interactions among the built environment, technological and urban infrastructures, citizens, and citizen engagement. Throughout, the author—who has studied smart cities around the world—argues that smart city developers should work more closely with local communities, recognizing their preexisting relationship to urban place and realizing the limits of technological fixes. Smartness is a means to an end: improving the quality of urban life."
pub.1168359712,[ Front Matter ],"Key concepts, definitions, examples, and historical contexts for understanding smart cities, along with discussions of both drawbacks and benefits of this approach to urban problems. Over the past ten years, urban planners, technology companies, and governments have promoted smart cities with a somewhat utopian vision of urban life made knowable and manageable through data collection and analysis. Emerging smart cities have become both crucibles and showrooms for the practical application of the Internet of Things, cloud computing, and the integration of big data into everyday life. Are smart cities optimized, sustainable, digitally networked solutions to urban problems? Or are they neoliberal, corporate-controlled, undemocratic non-places? This volume in the MIT Press Essential Knowledge series offers a concise introduction to smart cities, presenting key concepts, definitions, examples, and historical contexts, along with discussions of both the drawbacks and the benefits of this approach to urban life. After reviewing current terminology and justifications employed by technology designers, journalists, and researchers, the book describes three models for smart city development—smart-from-the-start cities, retrofitted cities, and social cities—and offers examples of each. It covers technologies and methods, including sensors, public wi-fi, big data, and smartphone apps, and discusses how developers conceive of interactions among the built environment, technological and urban infrastructures, citizens, and citizen engagement. Throughout, the author—who has studied smart cities around the world—argues that smart city developers should work more closely with local communities, recognizing their preexisting relationship to urban place and realizing the limits of technological fixes. Smartness is a means to an end: improving the quality of urban life."
pub.1125892114,Introduction,"Key concepts, definitions, examples, and historical contexts for understanding smart cities, along with discussions of both drawbacks and benefits of this approach to urban problems. Over the past ten years, urban planners, technology companies, and governments have promoted smart cities with a somewhat utopian vision of urban life made knowable and manageable through data collection and analysis. Emerging smart cities have become both crucibles and showrooms for the practical application of the Internet of Things, cloud computing, and the integration of big data into everyday life. Are smart cities optimized, sustainable, digitally networked solutions to urban problems? Or are they neoliberal, corporate-controlled, undemocratic non-places? This volume in the MIT Press Essential Knowledge series offers a concise introduction to smart cities, presenting key concepts, definitions, examples, and historical contexts, along with discussions of both the drawbacks and the benefits of this approach to urban life. After reviewing current terminology and justifications employed by technology designers, journalists, and researchers, the book describes three models for smart city development—smart-from-the-start cities, retrofitted cities, and social cities—and offers examples of each. It covers technologies and methods, including sensors, public wi-fi, big data, and smartphone apps, and discusses how developers conceive of interactions among the built environment, technological and urban infrastructures, citizens, and citizen engagement. Throughout, the author—who has studied smart cities around the world—argues that smart city developers should work more closely with local communities, recognizing their preexisting relationship to urban place and realizing the limits of technological fixes. Smartness is a means to an end: improving the quality of urban life."
pub.1181541929,Investigating Accountability in Business-intensive Systems-of-Systems,"CONTEXT: The evolution of business processes has driven the integration of systems-of-systems (SoS) across various domains, leveraging technologies such as cloud computing, e-commerce platforms, and smart environments. In this context, the integration of various heterogeneous and independent constituents systems include information systems that collaborate to achieve business goals. Hence, the accountability of these systems must be a concern, but traditional accountability approaches can obscure the responsibility and ownership of data, processes, and outcomes. PROBLEM: This complexity often results in studies offering specific solutions, then highlighting the ongoing need for a shared understanding of accountability. Furthermore, establishing accountability as a quality requirement poses a significant challenge due to limited research and an undefined agenda for underlying challenges. SOLUTION: This paper presents an overview of accountability from reporting on the current landscape to proposing a research agenda to address existing challenges. METHOD: The study adopts a prescriptive approach based on a systematic mapping study. RESULTS: The study yields insights into accountability, and a research agenda when identifying seven topics for further investigation. CONCLUSION: By consolidating knowledge on accountability, this study facilitates the expansion of the body of knowledge on the field and brings new inquiry to inspire innovative solutions."
pub.1125892117,Smart City Technologies,"Key concepts, definitions, examples, and historical contexts for understanding smart cities, along with discussions of both drawbacks and benefits of this approach to urban problems. Over the past ten years, urban planners, technology companies, and governments have promoted smart cities with a somewhat utopian vision of urban life made knowable and manageable through data collection and analysis. Emerging smart cities have become both crucibles and showrooms for the practical application of the Internet of Things, cloud computing, and the integration of big data into everyday life. Are smart cities optimized, sustainable, digitally networked solutions to urban problems? Or are they neoliberal, corporate-controlled, undemocratic non-places? This volume in the MIT Press Essential Knowledge series offers a concise introduction to smart cities, presenting key concepts, definitions, examples, and historical contexts, along with discussions of both the drawbacks and the benefits of this approach to urban life. After reviewing current terminology and justifications employed by technology designers, journalists, and researchers, the book describes three models for smart city development—smart-from-the-start cities, retrofitted cities, and social cities—and offers examples of each. It covers technologies and methods, including sensors, public wi-fi, big data, and smartphone apps, and discusses how developers conceive of interactions among the built environment, technological and urban infrastructures, citizens, and citizen engagement. Throughout, the author—who has studied smart cities around the world—argues that smart city developers should work more closely with local communities, recognizing their preexisting relationship to urban place and realizing the limits of technological fixes. Smartness is a means to an end: improving the quality of urban life."
pub.1113670334,Automated Schema Quality Measurement in Large-Scale Information Systems,"Assessing the quality of information system schemas is crucial, because an unoptimized or erroneous schema design has a strong impact on the quality of the stored data, e.g., it may lead to inconsistencies and anomalies at the data-level. Even if the initial schema had an ideal design, changes during the life cycle can negatively affect the schema quality and have to be tackled. Especially in Big Data environments there are two major challenges: large schemas, where manual verification of schema and data quality is very arduous, and the integration of heterogeneous schemas from different data models, whose quality cannot be compared directly. Thus, we present a domain-independent approach for automatically measuring the quality of large and heterogeneous (logical) schemas. In contrast to existing approaches, we provide a fully automatable workflow that also enables regular reassessment. Our implementation allows to measure the quality dimensions correctness, completeness, pertinence, minimality, readability, and normalization."
pub.1151000659,Towards gestured-based technologies for human-centred Smart Factories,"Despite the increasing degree of automation in industry, manual or semi-automated are commonly and inevitable for complex assembly tasks. The transformation to smart processes in manufacturing leads to a higher deployment of data-driven approaches to support the worker. Upcoming technologies in this context are oftentimes based on the gesture-recognition, − monitoring or – control. This contribution systematically reviews gesture or motion capturing technologies and the utilization of gesture data in the ergonomic assessment, gesture-based robot control strategies as well as the identification of COVID-19 symptoms. Subsequently, two applications are presented in detail. First, a holistic human-centric optimization method for line-balancing using a novel indicator – ErgoTakt – derived by motion capturing. ErgoTakt improves the legacy takt-time and helps to find an optimum between the ergonomic evaluation of an assembly station and the takt-time balancing. An optimization algorithm is developed to find the best-fitting solution by minimizing a function of the ergonomic RULA-score and the cycle time of each assembly workstation with respect to the workers’ ability. The second application is gesture-based robot-control. A cloud-based approach utilizing a generally accessible hand-tracking model embedded in a low-code IoT programming environment is shown."
pub.1151235781,Research Directions for Merging Geospatial Technologies with Smart Manufacturing Systems,"Abstract As industrial Internet of Things concepts and technologies continue to be retrofitted onto existing manufacturing infrastructure, geospatial considerations, such as asset localization, registration, and tracking, become more critical to ensure better flexibility, capability understanding, and agility. In response, there have been efforts to merge state-of-the-art Geographical Information Systems and Smart Manufacturing Systems in production environments. However, these solutions are often product- or platform-centric and proprietary, such as (i) computer vision technologies embedded on an automatic guided vehicle and (ii) point cloud translation after 3-D scan within a Product Lifecycle Management solution. Standards exist for various steps and functions within these computer-supported pipelines, but little work exists that tests their scalability and robustness. This paper aims to critically evaluate the current state of the integration of Smart Manufacturing Systems and Geographic Information Science and Technology and identifies the potential overlap between the two fields and lists opportunities for further collaboration. The methodological approach of this paper is two-fold: we utilize (a) a survey with experts in both fields and (b) an algorithmic literature meta-analysis. The results reveal that both fields have concepts that could mutually support each other and that smart manufacturing could benefit from Geographic Information technologies—especially from a standardized representation of indoor environments. The results show a great number of potential overlaps and thus present a preliminary roadmap to foster the integration."
pub.1181653902,"A systematic review on security aspects of fog computing environment: Challenges, solutions and future directions","The dynamic and decentralized architecture of fog computing, which extends cloud computing closer to the edge of the network, offers benefits such as reduced latency and enhanced bandwidth. However, the existing fog architecture introduces unique security challenges due to the large number of distributed fog nodes, often deployed in diverse and resource-constrained environments. Further, the proximity of fog computing nodes to end-users and the open, distributed nature of the architecture make fog environments particularly vulnerable to unauthorized access and various types of cyberattacks. Therefore, in order to address these challenges, the study presented a detailed systematic review that aims to analyze existing security technologies in fog computing environments, identify current security gaps, and propose future research directions. The comprehensive literature review uses quality databases, focusing on articles published within the last four years, i.e. from 2020 to 2024. Further, the review followed a systematic methodology with clear inclusion and exclusion criteria to ensure relevance and quality with respect to security in fog computing. Consequently, key research questions are also formulated and answered for addressing various security concerns, such as architectural security, IoT integration vulnerabilities, and dynamic security management. Finally, the detailed review summarizes the key findings through MTGIR analysis to give valuable insights on the existing security framework of fog computing systems. The result analysis further revealed that 16% of the research is focusing on blockchain and elliptic curve cryptography, alongside the utilization of artificial intelligence and machine learning, which is around 13.2%, specifically for dynamic threat detection. Furthermore, there are few technologies which require attention are federated learning, secure key management, and secure communication mechanisms, as these technologies are less considered in literature, i.e. around 3% only. Finally, the analysis underscored the necessity for real-time security monitoring and adaptive threat response to manage the dynamic nature of fog computing environments effectively."
pub.1144268539,Design of an Embedded Machine Learning Based System for an Environmental-friendly Crop Prediction Using a Sustainable Soil Fertility Management,"Most of the existing precision agriculture solutions recommend the use of fertilizers as a remedy to poor soil fertility and to boost yields. Such solutions cause environmental degradation in the long run mainly due to the overuse of fertilizers. There is therefore, a need for a system to ensure that farmers can practice precision farming in terms of a sustainable soil management approach so as to attain high yields while at the same time conserving the environment. In this research, a design and simulation of an embedded machine learning based system to predict the best crop to grow with minimal use of fertilizers with an aim of conserving the environment is presented. The system senses different real time soil parameters on a daily basis, integrates them with forecast weather information and uses embedded machine learning technique to determine which crop would grow best under the existing soil conditions so as to minimize fertilizer use. In addition to crop prediction, the system helps farmers to monitor the soil nutrients evolution so that action can be done on real time. The results are either displayed on the device or sent to the farmer’s mobile phone. This is a move from the existing solutions that depend on cloud analytics and do not consider the change of soil conditions on time in making the predictions and decisions since this is expensive when done at the cloud. The implementation of the proposed solution is expected to not only lead to high productivity and reduced costs but also conserve the environment."
pub.1174543991,"Enabling Spatial Digital Twins: Technologies, Challenges, and Future Research Directions","A Digital Twin (DT) is a virtual replica of a physical object or system, created to monitor, analyze, and optimize its behavior and characteristics. A Spatial Digital Twin (SDT) is a specific type of digital twin that emphasizes the geospatial aspects of the physical entity, incorporating precise location and dimensional attributes for a comprehensive understanding of its spatial environment. With the recent advancement in spatial technologies and breakthroughs in other computing technologies such as Artificial Intelligence (AI) and Machine Learning (ML), the SDTs market is expected to rise to 25 billion, covering a wide range of applications. The majority of existing research focuses on DTs and often fails to address the necessary spatial technologies essential for constructing SDTs. The current body of research on SDTs primarily concentrates on analyzing their potential impact and opportunities within various application domains. As building an SDT is a complex process and requires a variety of spatial computing technologies, it is not straightforward for practitioners and researchers of this multi-disciplinary domain to grasp the underlying details of enabling technologies of the SDT. In this paper, we are the first to systematically analyze different spatial technologies relevant to building an SDT in a layered approach (starting from data acquisition to visualization). More specifically, we present the tech stack of SDTs into five distinct layers of technologies: (i) data acquisition and processing; (ii) data integration, cataloging, and metadata management; (iii) data modeling, database management & big data analytics systems; (iv) Geographic Information System (GIS) software, maps, & APIs; and (v) key functional components such as visualizing, querying, mining, simulation, and prediction. Moreover, we discuss how modern technologies such as AI/ML, blockchains, and cloud computing can be effectively utilized in enabling and enhancing SDTs. Finally, we identify a number of research challenges and opportunities in SDTs. This work serves as an important resource for SDT researchers and practitioners as it explicitly distinguishes SDTs from traditional DTs, identifies unique applications, outlines the essential technological components of SDTs, and presents a vision for their future development along with the challenges that lie ahead."
pub.1144271555,The development of the system for arc nordugrid based grid-computing organization using virtual environments of the docker platform,"The study of modern frameworks and means of using virtualization in a grid environment confirmed the relevance of the task of automated configuration of the environment for performing tasks in a grid environment. Setting up a task execution environment using virtualization requires the implementation of appropriate algorithms for scheduling tasks and distributed storage of images of virtual environments in a grid environment. Existing cloud infrastructure solutions to optimize the process of deploying virtual machines on computing resources do not have integration with the Arc Nordugrid middleware, which is widely used in grid infrastructures. An urgent task is to develop tools for scheduling tasks and placing images of virtual machines on the resources of the grid environment, taking into account the use of virtualization tools. The results of the implementation of services of the framework are presented that allow to design and perform computational tasks in a grid environment based on ARC Nordugrid using the virtual environment of the Docker platform. The presented results of the implementation of services for scheduling tasks in a grid environment using a virtual computing environment are based on the use of a scheduling algorithm based on the dynamic programming method. Evaluations of the effectiveness of the solutions developed on the basis of a complex of simulation models showed that the use of the proposed algorithm for scheduling and replicating virtual images in a grid environment can reduce the execution time of a computational task by 88 %. Such estimates need further refinement; it is predicted that planning efficiency will increase over time with an increase in the number of running tasks due to the redistribution of the storage of virtual images"
pub.1174305287,IoT-Based Liquefied gas Tank Level Monitoring System for Industrial Welding Process,"The capabilities of industrial processes have witnessed exponential growth, evolving from the development of steam engines in the 1st generation to the integration of the Internet of Things (IoT) and cyber-physical systems (CPS) during the 4th Industrial Revolution. Among the rapid innovations in diverse industrial areas, one trending research field is the welding process, which has significantly transitioned from the forging method used during ancient civilizations to the utilization of gas welding with advanced robotic machinery in the contemporary world. However, current industrial operations still rely on conventional approaches to refilling gas tanks for industrial operations. This project aims to address this gap by proposing an Industrial Internet of Things (IIoT)-based system to monitor the percentage of liquefied gas in a welding tank through edge and cloud, emphasizing the seamless integration of the existing industrial environment. The first section of this paper thoroughly reviews the related work and industrial solutions before presenting the proposed system architecture. Then, we discuss each component in the project's architectural layers and present the implementation results, including an intuitive user interface and an alarm feature based on threshold via email service for a non-interruptive welding process. Finally, we present the conclusions, including the limitations and possible future work."
pub.1001071990,Increasing performance in KVM virtualization within a Tier-1 environment,"This work shows the optimizations we have been investigating and implementing at the KVM (Kernel-based Virtual Machine) virtualization layer in the INFN Tier-1 at CNAF, based on more than a year of experience in running thousands of virtual machines in a production environment used by several international collaborations. These optimizations increase the adaptability of virtualization solutions to demanding applications like those run in our institute (High-Energy Physics). We will show performance differences among different filesystems (like ext3 vs ext4) when used as KVM host local storage. We will provide guidelines for solid state disks (SSD) adoption, for deployment of SR-IOV (Single Root I/O Virtualization) enabled hardware and what is the best solution to distribute and instantiate read-only virtual machine images. This work has been driven by the project called Worker Nodes on Demand Service (WNoDeS), a framework designed to offer local, grid or cloud-based access to computing and storage resources, preserving maximum compatibility with existing computing center policies and workflows."
pub.1094776411,WACO: Workload Aware Column Order for Scan Operator in Wide Table,"Entering the big data era, wide table, which contains thousands of columns, is being widely adopted in cloud systems for its facilitation to critical fields such as warehouse/log analysis system, scientific applications and RDF storage. Although taking the advantage of avoiding expensive join in distributed environment, wide table puts an urgent demand for fast scan during data processing. However, represented by horizontal row-store, vertical column store and hybrid columnar store, existing promising data placement structures are witnessing a massive waste of computing resources on disk seeks, due to their ignorance of column order in physical data placement. To make the matter worse, some superior placement methods require additional adjustment to query processing engine or violate the data replication strategy and thus changed error-prone and incur additional disk seeks. In this paper, we explore a Workload Aware Column Order solution, WACO, to boost scan operator in wide table. The new WACO solution maximizes the sequential disk access and turns out to be transparent to underlying cloud systems, which therefore does not exhibit any above-mentioned shortcomings. In particular, acquire query workload, we exploit the recent access patterns on wide table and their frequencies via query logs. Given access patterns and their workload, we investigate the column placement strategy and proof that it belongs to NP-hard to figure out the optimal column layout. Furthermore, we propose a linear programming based solution to efficiently obtain an effective column order in internal physical placement. To make our solution robust and practical, we implement such scan-optimized data placement strategy as a library and thus it is a seamless integration with the underlying system and does not require any adjustment to existing system. We conduct extensive experiments on real-world TPC-H benchmark and SDSS dataset for simulate wide table to demonstrate the superiority of our solution. The experiment results show that our approach is 2x faster than the state-of-the-art."
pub.1163132467,Enabling Secure and Efficient Data Analytics Pipeline Evolution with Trusted Execution Environment,"Modern data analytics pipelines are highly dynamic, as they are constantly monitored and fine-tuned by both data engineers and scientists. Recent systems managing pipelines ease creating, deploying, and tracking their evolution. However, privacy concerns emerge as many of them are deployed on the public cloud with less or no trust. Unfortunately, the unique nature of pipelines prevents the adoption of existing confidential computing techniques with different computational patterns and large performance overhead. Being a potential approach, trusted execution environments (TEEs) are efficient in protecting the confidentiality and integrity of data and computation. However, fast-changing pipelines with latency requirements bring the challenge of reducing the cold start overhead --- the main bottleneck in the latest TEE. To support end-to-end private pipeline evolution, we present SecCask, a TEE-based data analytics pipeline management system. SecCask overcomes the problems of a naive design that isolates complete pipeline execution in one enclave by administering enclaves and runtimes. To reduce cold start overheads, our approach consists of reusing trusted runtimes for different pipeline components and caching them to avoid the cost of initialization. We leverage the latest Intel SGX to conduct experiments on representative workloads. The results demonstrate that SecCask reduces the total execution time by 68.4% compared to not reusing, is faster than running all components in one enclave, and incurs a modest average performance overhead of 29.9% over insecure baselines."
pub.1104219414,A VHO Scheme for Supporting Healthcare Services in 5G Vehicular Cloud Computing Systems,"Fifth Generation Vehicular Cloud Computing (5G-VCC) systems use heterogeneous network access technologies in order to fulfill the requirements of modern services, including medical services with strict constraints. Therefore, the need for efficient Vertical Handover (VHO) management schemes must be addressed. In this paper, a VHO management scheme for supporting medical services in 5G-VCC systems, is described. It consists of the VHO initiation and the network selection processes, while at the same time, the vehicle's velocity, its current connection type, as well as the status of the onboard patient's health, are considered. Specifically, during the VHO initiation process the necessity to perform handover is evaluated. Subsequently, the network selection process selects the appropriate network alternative considering both medical service requirements and patients' health status. The proposed scheme is applied to a 5G-VCC system which includes Long Term Evolution (LTE) and Worldwide Interoperability Microwave Access (WiMAX) Macrocells and Femtocells, as well as Wireless Access for Vehicular Environment Road Side Units (WAVE RSUs). Performance evaluation shows that the proposed algorithm outperforms existing VHO management schemes."
pub.1158284604,"Artificial Cognitive Computing for Smart Communications, 5G and Beyond","Artificial Intelligence (AI) is computer intelligence that manifests itself in “cognitive” capabilities that people identify with other brains. AI employs various technologies including Deep Learning, Machine Learning and Natural Language Processing. The self-learning systems are utilizing pattern recognition, natural language processing and data mining to replicate the person's brain functions are called cognitive computing. Cloud-based communication has bolstered this by delivering vital communication services. However, due to restricted capacities and a need for low latency, high reliability, and a good user experience, providing a cloud-based environment and intensive data processing algorithms are insufficient. Cognitive computing is considered a branch of computer science that simulates human cognitive processes. As a result, when cognitive science skills are combined with communications and existing systems may be improved, resulting in higher accuracy and lower latency. We have gone through cognition-based communications in depth in this study, which blends smart communication technologies and intelligent computing based on AI. Following is an overview of the cognitive computing and its evolution. Then, combining networking, analytics, and cloud computing, a systematic and comprehensive framework for using cognition in communication is provided."
pub.1141409777,A Digital Information Model Framework for UAS-Enabled Bridge Inspection,"Unmanned aerial systems (UAS) provide two main functions with regards to bridge inspections: (1) high-quality digital imaging to detect element defects; (2) spatial point cloud data for the reconstruction of 3D asset models. With UAS being a relatively new inspection method, there is little in the way of existing framework for storing, processing and managing the resulting inspection data. This study has proposed a novel methodology for a digital information model covering data acquisition through to a 3D GIS visualisation environment, also capable of integrating within a bridge management system (BMS). Previous efforts focusing on visualisation functionality have focused on BIM and GIS as separate entities, which has a number of problems associated with it. This methodology has a core focus on the integration of BIM and GIS, providing an effective and efficient information model, which provides vital visual context to inspectors and users of the BMS. Three-dimensional GIS visualisation allows the user to navigate through a fully interactive environment, where element level inspection information can be obtained through point-and-click operations on the 3D structural model. Two visualisation environments were created: a web-based GIS application and a desktop solution. Both environments develop a fully interactive, user-friendly model which have fulfilled the aims of coordinating and streamlining the BMS process."
pub.1020915460,Multiscale Interactions in the Life Cycle of a Tropical Cyclone Simulated in a Global Cloud-System-Resolving Model. Part II: System-Scale and Mesoscale Processes*,"Abstract
                  The life cycle of Tropical Storm Isobel was simulated reasonably well in the Nonhydrostatic Icosahedral Atmospheric Model (NICAM), a global cloud-system-resolving model. The evolution of the large-scale circulation and the storm-scale structure change was discussed in Part I. Both the mesoscale and system-scale processes in the life cycle of the simulated Isobel are documented in this paper. In the preconditioned favorable environment over the Java Sea, mesoscale convective vortices (model MCVs) developed in the mesoscale convective systems (MCSs) and convective towers with cyclonic potential vorticity (PV) anomalies throughout the troposphere [model vortical hot towers (VHTs)] appeared in the model MCVs. Multiple model VHTs strengthened cyclonic PV in the interior of the model MCV and led to the formation of an upright monolithic PV core at the center of the concentric MCV (primary vortex enhancement). As the monolithic PV core with a warm core developed near the circulation center, the intensification and the increase in horizontal size of the cyclonic PV were enhanced through the system-scale intensification (SSI) process (the secondary vortex enhancement), leading to the genesis of Isobel over the Timor Sea. The SSI process can be well explained by the balanced dynamics.
                  After its genesis, the subsequent evolution of the simulated Isobel was controlled by both the external influence and the internal dynamics. Under the unfavorable environmental conditions, the development of asymmetric structure reduced the axisymmetric diabatic heating in the inner core and the SSI process became ineffective and the storm weakened. Later on, as the eyewall reformed as a result of the axisymmetrization of an inward-propagating outer spiral rainband, the SSI process became effective again, leading to the reintensification of Isobel. Therefore, the large-scale environmental flow provided the precondition for the genesis of Isobel and the triggering mechanism for subsequent storm-scale structure change as discussed in Part I. The system-scale and mesoscale processes, such as the evolution of MCVs and merging VHTs, were responsible for the genesis, while the eyewall processes were critical to the storm intensity change through the SSI process."
pub.1035935922,Fostering Innovation Through Coopetition: The E015 Digital Ecosystem,"Expo Milano 2015 is expecting to welcome millions of people from around the globe. They will visit the Universal Exposition and use local services provided by public and private organizations. Thus, this event is a major opportunity to introduce innovation in all aspects of urban life. The E015 digital ecosystem is a multi-stakeholder service-based environment enabling the realization and integration of advanced digital services made available before, during and also after the Expo by different stakeholders. E015 operates since 2013 and aims to become one of the legacies that the Expo will leave to Milan and to the European public and private system after the event."
pub.1101629068,Dynamic Adaptation to Environmental Changes of Optical Virtual Networking and Cloud Computing Systems for Tightly Coupling Big Data and Peripheral Computer Resources,"Recently, the use of big data has attracted attention as a profitable business strategy, and is expected to keep increasing in the future. In contrast to existing ways of big data analysis based on centralized computing environment such on a few huge data centers, we advocate a distributed and parallel computation environment aiming at fine-grained cloud computing, which includes so-called edge computing. In the advocated environment, it is assumed that many users, which own big data to be analyzed, dynamically participate in the network to request computer resources and leave after finishing their analyses. In such a dynamic and realistic environment, this paper improves the proximity of computer resources to big data by applying virtualized network in which nodes with each big data and the corresponding computer resources are mutually connected by proper optical paths. Optical path arrangement is periodically updated for new users, without affecting other users currently using computer resources. Moreover, a resource assignment algorithm suitable for such dynamic changes is also proposed to achieve fairness in terms of the network distance between big data and computer resources, and effective load balancing among resource suppliers. We evaluate its effectiveness by computer simulation."
pub.1123201306,Arduino and NodeMcu based Ingenious Household Objects Monitoring and Control Environment,"With the advent of innovation and automation, convenience and simplicity has permeated all walks of life. Home automation is one such emerging technology which empowers the residents to have wireless, ubiquitous and computerized control over the household gadgets. Some popular ways to implement wireless connectivity amongst the connected devices are cellular networks, IR sensors, Bluetooth, ZigBee frameworks and Wi-Fi networks with each type having its intrinsic strengths and setbacks. There are a plethora of IoT setups available but most of them have restricted compatibility and are tailor-made for manufacturer supported devices. In order to overcome these difficulties and provide a cost efficient solution, a generic, all product supporting Wi-Fi based remote home automation scheme using an Arduino UNO (microcontroller), an 8 channel Relay module and a NodeMcu (Wi-Fi module) is proposed in this paper. Naive users are familiar with Wi-Fi as it is already used in consumer electronics sector. They can utilise their existing hotspots with minimal additional infrastructure for new IoT applications. Therefore, a Wi-Fi based system brings down the cost by eliminating the need to buy expensive auxiliaries. Being compatible with the Internet Protocol (IP), a significantly higher number of devices can be connected to the internet when Wi-Fi is used. Firebase functions as the cloud hosted real-time database assisting data exchange and synchronisation. The connected devices are monitored and controlled through a mobile application from anywhere across the globe. Additionally, voice based control can be provided with the integration of Google Assistant. The device statistics are visually presented as charts to provide the users an overview of their usage."
pub.1003468660,Optimizing virtual machine live storage migration in heterogeneous storage environment,"Virtual machine (VM) live storage migration techniques significantly increase the mobility and manageability of virtual machines in the era of cloud computing. On the other hand, as solid state drives (SSDs) become increasingly popular in data centers, VM live storage migration will inevitably encounter heterogeneous storage environments. Nevertheless, conventional migration mechanisms do not consider the speed discrepancy and SSD's wear-out issue, which not only causes significant performance degradation but also shortens SSD's lifetime. This paper, for the first time, addresses the efficiency of VM live storage migration in heterogeneous storage environments from a multi-dimensional perspective, i.e., user experience, device wearing, and manageability. We derive a flexible metric (migration cost), which captures various design preference. Based on that, we propose and prototype three new storage migration strategies, namely: 1) Low Redundancy (LR), which generates the least amount of redundant writes; 2) Source-based Low Redundancy (SLR), which keeps the balance between IO performance and write redundancy; and 3) Asynchronous IO Mirroring, which seeks the highest IO performance. The evaluation of our prototyped system shows that our techniques outperform existing live storage migration by a significant margin. Furthermore, by adaptively mixing our proposed schemes, the cost of massive VM live storage migration can be even lower than that of only using the best of individual mechanism."
pub.1063162956,Optimizing virtual machine live storage migration in heterogeneous storage environment,"Virtual machine (VM) live storage migration techniques significantly increase the mobility and manageability of virtual machines in the era of cloud computing. On the other hand, as solid state drives (SSDs) become increasingly popular in data centers, VM live storage migration will inevitably encounter heterogeneous storage environments. Nevertheless, conventional migration mechanisms do not consider the speed discrepancy and SSD's wear-out issue, which not only causes significant performance degradation but also shortens SSD's lifetime. This paper, for the first time, addresses the efficiency of VM live storage migration in heterogeneous storage environments from a multi-dimensional perspective, i.e., user experience, device wearing, and manageability. We derive a flexible metric (migration cost), which captures various design preference. Based on that, we propose and prototype three new storage migration strategies, namely: 1) Low Redundancy (LR), which generates the least amount of redundant writes; 2) Source-based Low Redundancy (SLR), which keeps the balance between IO performance and write redundancy; and 3) Asynchronous IO Mirroring, which seeks the highest IO performance. The evaluation of our prototyped system shows that our techniques outperform existing live storage migration by a significant margin. Furthermore, by adaptively mixing our proposed schemes, the cost of massive VM live storage migration can be even lower than that of only using the best of individual mechanism."
pub.1125145808,"The Robotic Process Automation Handbook, A Guide to Implementing RPA Systems","While Robotic Process Automation (RPA) has been around for about 20 years, it has hit an inflection point because of the convergence of cloud computing, big data and AI. This book shows you how to leverage RPA effectively in your company to automate repetitive and rules-based processes, such as scheduling, inputting/transferring data, cut and paste, filling out forms, and search. Using practical aspects of implementing the technology (based on case studies and industry best practices), you’ll see how companies have been able to realize substantial ROI (Return On Investment) with their implementations, such as by lessening the need for hiring or outsourcing. By understanding the core concepts of RPA, you’ll also see that the technology significantly increases compliance – leading to fewer issues with regulations – and minimizes costly errors. RPA software revenues have recently soared by over 60 percent, which is the fastest ramp in the tech industry, and they are expected to exceed $1 billion by the end of 2019. It is generally seamless with legacy IT environments, making it easier for companies to pursue a strategy of digital transformation and can even be a gateway to AI. The Robotic Process Automation Handbook puts everything you need to know into one place to be a part of this wave. You will: Develop the right strategy and plan Deal with resistance and fears from employees Take an in-depth look at the leading RPA systems, including where they are most effective, the risks and the costs Evaluate an RPA system"
pub.1031391718,Energy-efficiency enhanced virtual machine scheduling policy for mixed workloads in cloud environments,"Virtualization technology is an effective approach to improving the energy-efficiency in cloud platforms; however, it also introduces many energy-efficiency losses especially when I/O virtualization is involved. In this paper, we present an energy-efficiency enhanced virtual machine (VM) scheduling policy, namely Share-Reclaiming with Collective I/O (SRC-I/O), with aiming at reducing the energy-efficiency losses caused by I/O virtualization. The proposed SRC-I/O scheduler allows VMs to reclaim extra CPU shares in certain conditions so as to increase CPU utilization. Meanwhile, it separates I/O-intensive VMs from CPU-intensive ones and schedules them in a collective manner, so as to reduce the context-switching cost when scheduling mixed workloads. Extensive experiments are conducted on various platforms to investigate the performance of the proposed scheduler. The results indicate that when the system is in presence of mixed workloads, SRC-I/O scheduler outperforms many existing VM schedulers in terms of energy-efficiency and I/O responsiveness."
pub.1174002390,MOHBA: Multi-objective Honey Badger Algorithm for workflow scheduling in heterogeneous Cloud–Fog-IoT networks,"Nowadays, the Internet of Things (IoT) is used for the purpose of gathering data through sensors as well as for storing and processing data. Due to the inherent limitations in the processing and computing capabilities of IoT devices, the inclination towards the integration of cloud computing (CC) with IoT systems is growing rapidly. The CC is capable of efficiently handling substantial volumes of data at remarkable speeds. However, the data owners are required to upload their data to the cloud, and the transmission of such extensive data requires a significant amount of bandwidth. Moreover, the existence of latency and jitter arises as a consequence of the significant geographical separation between IoT devices and the cloud. Hence, fog computing (FC) is utilized in proximity to IoT devices so that the extent of transmission delay is diminished. Both CC and FC are employed in collaboration to improve the performance of the IoT system. A wide range of workflow scheduling optimization techniques were proposed for both CC and FC environments. However, in heterogeneous remote computing systems, major challenges encountered are related to execution time, energy efficiency, latency, cost, and load balancing. In this study, the Multi-objective Honey Badger Algorithm (MOHBA) is proposed for optimizing workflow schedules aimed at optimizing makespan, energy consumption, and overall cost, which hold significant importance for real-time systems. FogWorkflowSim is used for simulating where HBA outperforms other existing algorithms like Particle Swarm Optimization (PSO), Grey Wolf Optimizer (GWO), and Artificial Algae Algorithm (AAA)."
pub.1173642442,FewVV: Few-Shot Adaptive Bitrate Volumetric Video Streaming With Prompted Online Adaptation,"In recent years, volumetric videos have brought immersive experiences to users. Existing viewport-based volumetric video streaming (VVS) systems prune the point cloud according to visibility to reduce bandwidth consumption, leading to a better responsiveness. They also predict bandwidth and allocate bitrate to different parts of the video to enhance Quality-of-Experience (QoE). However, such designs sometimes result in drastic quality fluctuations in real-world deployment, due to limited generalization performance. Our measurement notes that these systems tend to have a significant accuracy loss under an unseen Out-of-Distribution (OoD) environments. On the other hand, open world prediction/adaptation problem have been addressed in the recent reinforcement learning advances, particularly through prompt-based few-shot and zero-shot learning. Inspired by this development, in this work, we first reformulate the volumetric bitrate adaptation (volumetric ABR) into a sequence prediction problem, then we design a volumetric causal transformer algorithm to solve it. We train our model on a large action trajectory data set, then evaluate it on various OoD scenarios. The result show that FewVV consistently outperforms the existing systems on both performance and generalization."
pub.1008477044,Utilizing Optical Circuits in Hybrid Packet/Circuit Data-Center Networks,"Existing Data Center Networks (DCNs) continue to evolve to keep up with application requirements in terms of bandwidth, latency, agility, etc. According to the updated release of the Cisco Global Cloud Index [1], by 2019, more than 86% of traffic workloads will be processed by cloud DCs. Traditional DCNs, which are based on electrical packet switching (EPS) with hierarchical, tree-like topologies can no longer support future cloud traffic requirements in terms of dynamicity, bandwidth and latency. Hence, existing DCNs can be enhanced with OCS (Optical Circuit Switching), which provides high bandwidth, low latency and low power consumption [2], giving rise to hybrid OCS-EPS topologies. In this research, we assess a virtualized, hybrid, flat DCN topology consisting of a single layer of high radix ToR (Top of Rack) switches, interconnected with each other and through an OCS plane. The benefit of such flat topology is twofold: 1) In terms of bandwidth, over-subscription is reduced, and bisection bandwidth is increased; and 2) In terms of latency, the diameter (longest path) of topology is reduced. Moreover, we present new algorithms and orchestration functionality to detect and offload suitable flows (e.g. elephant flows) from the EPS to the OCS plane. Our DC architecture consists of hybrid EPS-OCS DCN, an Openflow(OF) based control plane, and an orchestration layer. Our orchestration layer decouples the elephant flows detection from the rerouting decision logic in the DCN. Specifically, the elephant flows detection is done by flow tagging in the hypervisor, while the flow rerouting is executed at the EPSs, which are connected directly to the OCS. Hence, it provides a more efficient, scalable, and easy to configure architecture as compared to existing hybrid solutions. The orchestrator monitors the ToR switches by sFlow and detects high volume traffic between two ToRs, exceeding a given bandwidth threshold. Such traffic may consist of either few elephant flows or many mice flows. To further increase the optical circuit utilization, we introduce two types of optical circuits: 1) private circuit, presented in existing solutions, is utilized only by flows that originate and end at the ToR switches connected to the circuit endpoints. 2) shared circuit, is part of our novel approach. It can be used also by flows that are transmitted through ToR switches connected to the circuit endpoints, but originate and/or end at other ToRs. Moreover, the orchestrator may dynamically decide to configure private or shared optical circuits, according to various criteria including current network utilization, traffic flows nature, tenants SLAs, etc. Configuring or changing the optical circuit type requires installing a single OpenFlow rule for each ToR connected to the circuit endpoints; hence, enabling low overhead and fast network configuration. To assess the benefit of such optical circuit configurations, we implement the proposed algorithms and test them over an emula"
pub.1008599872,A Design of DBaaS-Based Collaboration System for Big Data Processing,"With the recent growth in cloud computing, big data processing and collaboration between businesses are emerging as new paradigms in the IT industry. In an environment where a large amount of data is generated in real time, such as SNS, big data processing techniques are useful in extracting the valid data. MapReduce is a good example of such a programming model used in big data extraction. With the growing collaboration between companies, problems of duplication and heterogeneity among data due to the integration of old and new information storage systems have arisen. These problems arise because of the differences in existing databases across the various companies. However, these problems can be negated by implementing the MapReduce technique. This paper proposes a collaboration system based on Database as a Service, or DBaaS, to solve problems in data integration for collaboration between companies. The proposed system can reduce the overhead in data integration, while being applied to structured and unstructured data."
pub.1172309682,"Deep Reinforcement Learning (DRL)-Based Methods for Serverless Stream Processing Engines: A Vision, Architectural Elements, and Future Directions","Streaming applications are becoming widespread across an extensive range of business domains as an increasing number of sources continuously produce data that need to be processed and analysed in real time. Modern businesses are aggressively using streaming data to generate valuable knowledge that can be used to automate processes, help decision-making, optimize resource usage, and ultimately generate revenue for the organization. Despite their increased adoption and tangible benefits, support for the automated deployment and management of streaming applications is yet to emerge. Although a plethora of stream management systems have flooded the open-source community in recent years, all of the existing frameworks demand a considerably challenging and lengthy effort from human operators to manually and continuously tune their configuration and deployment environment in order to reach and maintain the desired performance goals. To address these challenges, this article proposes a vision for creating Deep Reinforcement Learning (DRL)-based methods for transforming stream processing engines into self-managed serverless solutions. This will lead to an increase in productivity as engineers can focus on the actual development process, an increase in application performance potentially leading to reduced response times and more accurate and meaningful results, and a considerable decrease in operational costs for organizations."
pub.1174646231,A Development of Integration of Edge Cloud Computing with 5G N/W,"Multi-access computing on the edge (MEC) represents a developing environment that tries to combine communications and IT services, creating a cloud computing platform located at the edge of the radio control network. The main goal of MEC is to offer storage and computing powers at the edge that was thereby lowering delay for global end users and improving the use of handheld transport and core networks. This review paper offers an in-depth analysis of MEC, with a particular emphasis on the important supporting technologies. It dives into MEC accompaniment, addressing both individual functions and linked MEC systems that support mobility, shining light on various distribution choices for orchestration. Furthermore, the paper studies the MEC reference design and key implementation situations, stressing the provision of multitenancy aid to app builders, suppliers of content, and external organizations. Finally, the paper offers insights into current standards of efforts and looks into existing research issues in the MEC area."
pub.1147800491,Trust dimensions of e-records in an African context,"This chapter presents the trust dimension of e-records within the African context beyond legislative provisions. It is based on case studies that examined the digital records management in enterprise-wide systems in the public service in four countries: Botswana, Kenya, South Africa, and Zimbabwe. The case studies draw on literature review and surveys to examine the state of enterprise-wide systems and enterprise content management (ECM) applications in the public sector. It also determines their relationship with existing archives and records management (ARM) practices and contextualizes these enterprise-wide systems and ECM applications concerning identified ARM challenges in these countries and Africa as a whole. The studies identified a large number of enterprise-wide systems and showed that the emerging cloud-based computing environment was nascent and evolving. Furthermore, despite all four countries’ stated policy ambitions for digital government initiatives, their connection to ARM practice was not always clear. Across the board, the studies emphasize the importance of capitalizing on their respective governments’ apparent willingness to undertake public sector structural reforms that highlight the importance of establishing information and records management systems that ensure digital records provide evidence and authenticity that support the open and e-government drive. The studies provide evidence, lay the groundwork for understanding enterprise architecture and its impact on digital record management, as well as the integration of ARM practice in digital and open government, and provide lessons to other African countries. This chapter presents the trust dimension of e-records within the African context beyond legislative provisions. It is based on case studies that examined the digital records management in enterprise-wide systems in the public service in four countries: Botswana, Kenya, South Africa, and Zimbabwe. The case studies draw on literature review and surveys to examine the state of enterprise-wide systems and enterprise content management (ECM) applications in the public sector. It also determines their relationship with existing archives and records management (ARM) practices and contextualizes these enterprise-wide systems and ECM applications concerning identified ARM challenges in these countries and Africa as a whole. For any form of record to be considered authoritative evidence of business transactions, they should be considered trustworthy. Most African countries have seen changes from manual record-keeping practices to digital ones as part of the e-government drive and transition in public sector, where records may be supported by information and communication technologies (ICTs) or generated within ICT systems."
pub.1020166895,Realisation of a Geodetic Datum Using a Gridded Absolute Deformation Model (ADM),"This paper describes a schema for a gridded absolute deformation model (ADM) and non-linear deformation patch model that can be used to transform point positions captured in the International Terrestrial Reference Frame (ITRF), or other closely aligned reference frame, to a reference epoch consistently over time for practical applications. The schema described utilises existing models of rigid plate motion, plate boundary deformation and non-linear deformation (e.g. coseismic and postseismic effects or subsidence). Application of an ADM and patch model can enable consistent Precise Point Positioning (PPP) over time and seamless integration of Continuously Operating Reference Station (CORS) networks within deforming zones. The strategy described can also ensure consistency of time-tagged spatial datasets (e.g. laser scanned point clouds and digital cadastral databases) and GIS within a kinematic environment. An ADM can also be used as the basis for static epoch projections of a national or regional kinematic datum. A case study from New Zealand is described."
pub.1182010177,A Proactive Approach to Fault Tolerance Using Predictive Machine Learning Models in Distributed Systems,"In the era of cloud computing and large-scale distributed systems, ensuring uninterrupted service and operational reliability is crucial. Conventional fault tolerance techniques usually take a reactive approach, addressing problems only after they arise. This can result in performance deterioration and downtime. With predictive machine learning models, this research offers a proactive approach to fault tolerance for distributed systems, preventing significant failures before they arise. Our research focuses on combining cutting-edge machine learning algorithms with real-time analysis of massive streams of operational data to predict abnormalities in the system and possible breakdowns. We employ supervised learning algorithms such as Random Forests and Gradient Boosting to predict faults with high accuracy. The predictive models are trained on historical data, capturing intricate patterns and correlations that precede system faults. Early defect detection made possible by this proactive approach enables preventative remedial measures to be taken, reducing downtime and preserving system integrity. To validate our approach, we designed and implemented a fault prediction framework within a simulated distributed system environment that mirrors contemporary cloud architectures. Our experiments demonstrate that the predictive models can successfully forecast a wide range of faults, from hardware failures to network disruptions, with significant lead time, providing a critical window for implementing preventive measures. Additionally, we assessed the impact of these pre-emptive actions on overall system performance, highlighting improved reliability and a reduction in mean time to recovery (MTTR). We also analyse the scalability and adaptability of our proposed solution within diverse and dynamic distributed environments. Through seamless integration with existing monitoring and management tools, our framework significantly enhances fault tolerance capabilities without requiring extensive restructuring of current systems. This work introduces a proactive approach to fault tolerance in distributed systems using predictive machine learning models. Unlike traditional reactive methods that respond to failures after they occur, this work focuses on anticipating faults before they happen."
pub.1119903673,Research Framework for the Future Development of IoT Information Service Technology,"The Internet of Things information service system is a natural evolution of existing information systems. The source of information is multi-source. The storage and processing of information in a cloud environment may be distributed in entities in different regions, or may be concentrated in an information service center. The functional services provided by the information service system can be centralized or distributed. This study proposes a research framework for IoT information services, and gives a general description of the system structure and information services. The structure of the entire IoT system consists of three parts: information service interface, information service interface engine and information flow engine. The Internet of Things information service has the following development trends: (1) The environment of big data processing has expanded to the entire Internet of Things; (2) The development of services that meet the requirements of clear users to meet the needs of fuzzy users; (3) The calculation of cloud computing The organization further deepens and expands; (4) The post-evaluation of service quality develops to advance prediction; (5) The intelligentialize and automation of decision-making. This research framework can be used as a reference for future research on IoT information services."
pub.1175940193,Cloud-Edge-End Collaboration for Intelligent Train Regulation Optimization in TACS,"Advancing from large-scale complex railway network construction to refined network operation management is a significant trend in promoting the high-quality development of modern rail transportation services. With the emergence of the next-generation train control system—Train Autonomous Circumambulation System (TACS), the transportation environment manifests obvious intricate correlations with strong couplings, multiple constraints, and rapid evolution. Most of the existing works focus on low-dimensional passenger flow prediction and independent train adjustment optimization, while the potential of network-level situation assessment and generalized experience across multi-tasks are neglected. In this paper, we propose a novel cloud-edge-end collaboration empowered TACS intelligent train regulation optimization scheme with situation awareness at end layer, arithmetic provision at edge layer, and intelligent fusion at cloud layer. Specifically, a Graph Convolutional Network (GCN)- based passenger flow prediction model is introduced to enable accurate assessment of the urban rail transit operation situation at the network level. Moreover, the Deep Reinforcement Learning (DRL)-based train dynamic adjustment algorithm is proposed to ensure efficient matching of passenger and traffic flows. In addition, a Actor-Mimic based multi-task and transfer reinforcement learning method is implemented in TACS to facilitate generalizing the trained experience across multiple tasks and accelerate the ability to adapt to new environments. Extensive simulation results illustrate that the proposed scheme can effectively improve the transportation capacity matching of TACS and enhance the generalization of train dynamic adjustment strategies"
pub.1165802682,An energy-saving joint resource allocation approach for mobile edge computing based on NOMA,"Mobile edge computing can use the wireless access network to provide the services required by telecom users and cloud computing functions nearby, thereby creating a service environment with high performance, low latency and high bandwidth. Computational task offloading is a core issue in mobile edge computing. In order to improve the security when computing tasks are partially offloaded in non-orthogonal multiple access-based mobile edge computing systems, this paper studies the physical layer security of MEC networks considering the presence of eavesdroppers. We employ the secrecy outage probability to measure the secrecy performance of computation offloading. Under the premise of comprehensively considering the joint constraints of transmit power, local task calculation and confidentiality outage probability, we introduce the energy consumption weight factor to balance the transmission energy consumption and calculation energy consumption, and finally achieve the minimum weighted sum of system energy consumption. On the premise of satisfying two user priorities, in order to reduce system overhead, we design a joint task offloading and resource allocation mechanism. The mechanism uses binary search-based iterative optimization algorithm to seek the optimal solution after problem transformation, and obtain optimal task offloading and power allocation. A large number of simulation results show that the algorithm proposed in this paper can effectively reduce system energy consumption."
pub.1091309638,An Extension of the MiSCi Middleware for Smart Cities Based on Fog Computing,"In a Smart City is required computational platforms, which allow environments with multiple interconnected and embedded systems, where the technology is integrated with the people, and can respond to unpredictable situations. One of the biggest challenges in developing Smart City is how to describe and dispose of enormous and multiple sources of information, and how to share and merge it into a single infrastructure. In previous works, we have proposed an Autonomic Reflective Middleware with emerging and ubiquitous capabilities, which is based on intelligent agents that can be adapted to the existing dynamism in a city for, ubiquitously, respond to the requirements of citizens, using emerging ontologies that allow the adaptation to the context. In this work, we extend this middleware using the fog computing paradigm, to solve this problem. The fog extends the cloud to be closer to the things that produce and act on the smart city. In this paper, we present the extension to the middleware, and examples of utilization in different situations in a smart city."
pub.1173718613,Application of Big Data and Quantum Computing in the Secure Federated Internet of Things,"Federated Internet of Things (IoT) presents both unprecedented opportunities and challenges in security and data management. This study explores the integration of big data analytics and Quantum Computing as potential solutions to address security concerns within the Federated IoT ecosystem. The study examines the implications of leveraging big data analytics to process and analyze the massive volume of data generated by IoT devices. Advanced analytics techniques, including machine learning and anomaly detection algorithms, are employed to enhance the detection and mitigation of security threats such as unauthorized access, data breaches and malicious attacks. Furthermore, the study investigates the role of Quantum Computing infrastructure in providing scalable and reliable resources for securely storing, processing and transmitting IoT data. By offloading computational tasks to quantum-based platforms, the aim is to alleviate the burden on edge devices while ensuring robust security measures are in place to safeguard sensitive information. A comprehensive review of existing literature and case studies identifies key challenges and opportunities in implementing big data and Quantum Computing solutions within the Federated IoT environment. The study also proposes potential frameworks and methodologies for integrating these technologies effectively, considering factors such as data privacy, scalability and interoperability. Overall, this research aims to advance secure IoT systems by leveraging big data analytics and cloud computing. By addressing security concerns proactively and adopting innovative approaches, the goal is to create a more resilient and trustworthy Federated IoT ecosystem, benefiting society at large."
pub.1167169674,"Cloud Computing in Smart Cities: Privacy, Ethical and Social Issues","The rapid development of cloud computing technologies has revolutionized various sectors, and smart cities are no exception. Smart cities leverage cloud computing to optimize urban services, enhance resource management, and improve the overall quality of life for their inhabitants. However, as these technological advancements proliferate, concerns about privacy, ethical considerations, and social implications have emerged. This research paper critically examines the multifaceted challenges associated with the integration of cloud computing in smart cities, shedding light on the potential risks and highlighting the need for comprehensive solutions. The research employs a mixed-methods approach, combining quantitative data analysis and qualitative case studies to offer a comprehensive perspective on the identified issues. The primary focus lies in identifying privacy risks, ethical dilemmas, and social disparities that arise due to the extensive use of cloud-based systems and data in smart cities. Furthermore, the study investigates the role of key stakeholders, including governments, technology providers, and citizens, in mitigating or exacerbating these challenges. Key findings reveal that while cloud computing empowers smart cities with unparalleled capabilities, it also exposes residents' personal information to potential breaches and misuse. Ethical concerns arise from the handling of sensitive data, data ownership, and algorithmic biases that could perpetuate discrimination. Moreover, social issues like the digital divide and access disparities may further exacerbate existing inequalities in smart city implementation. This research paper concludes by proposing a comprehensive framework of guidelines and best practices to address the identified issues effectively. These recommendations encompass enhanced data privacy measures, transparent and accountable data governance, the promotion of ethical data usage, and inclusive strategies to bridge social disparities. By adopting these measures, smart cities can harness the full potential of cloud computing while safeguarding individual rights and fostering a more equitable and inclusive urban environment. Overall, this study underscores the critical importance of addressing privacy, ethical, and social challenges in the context of cloud computing in smart cities. By adopting a holistic and proactive approach, city planners, policymakers, and technology providers can build sustainable and responsible smart cities that ensure the well-being and dignity of their residents in the digital era."
pub.1149918715,Discrete GWO Optimized Data Aggregation for Reducing Transmission Rate in IoT,"The conventional hospital environment is transformed into digital transformation that focuses on patient centric remote approach through advanced technologies. Early diagnosis of many diseases will improve the patient life. The cost of health care systems is reduced due to the use of advanced technologies such as Internet of Things (IoT), Wireless Sensor Networks (WSN), Embedded systems, Deep learning approaches and Optimization and aggregation methods. The data generated through these technologies will demand the bandwidth, data rate, latency of the network. In this proposed work, efficient discrete grey wolf optimization (DGWO) based data aggregation scheme using Elliptic curve Elgamal with Message Authentication code (ECEMAC) has been used to aggregate the parameters generated from the wearable sensor devices of the patient. The nodes that are far away from edge node will forward the data to its neighbor cluster head using DGWO. Aggregation scheme will reduce the number of transmissions over the network. The aggregated data are preprocessed at edge node to remove the noise for better diagnosis. Edge node will reduce the overhead of cloud server. The aggregated data are forward to cloud server for central storage and diagnosis. This proposed smart diagnosis will reduce the transmission cost through aggregation scheme which will reduce the energy of the system. Energy cost for proposed system for 300 nodes is 0.34μJ. Various energy cost of existing approaches such as secure privacy preserving data aggregation scheme (SPPDA), concealed data aggregation scheme for multiple application (CDAMA) and secure aggregation scheme (ASAS) are 1.3 μJ, 0.81 μJ and 0.51 μJ respectively. The optimization approaches and encryption method will ensure the data privacy."
pub.1175859627,Evaluation of time-based virtual machine migration as moving target defense against host-based attacks,"Moving Target Defense (MTD) consists of applying dynamic reconfiguration in the defensive side of the attack-defense cybersecurity game. Virtual Machine (VM) migration could be used as MTD against specific host-based attacks in the cloud computing environment by remapping the distribution of VMs in the existing physical hosts. This way, when the attacker’s VM is moved to a different machine, the attack has to be restarted. However, one significant gap here is how to select a proper VM migration-based MTD schedule to reach the desired levels of system protection. This paper develops a Stochastic Petri Net (SPN) model to address this issue. The model leverages empirical knowledge about the dynamics of the attack defense in a VM migration-enabled setup. First, we present the results of an experimental campaign to acquire knowledge about the system’s behavior. The experiments provide insights for the model design. Then, based on the model, we propose a tool named PyMTDEvaluator, which provides a graphical interface that serves as a wrapper for the simulation environment of the model. Finally, we exercise the tool using Multi-Criteria Decision-Making methods to aid the MTD policy selection. Hopefully, our results and methods will be helpful for system managers and cybersecurity professionals."
pub.1109822501,Overcoming Virtualization Overheads for Large- vCPU Virtual Machines,"Virtual Machines (VM) frequently run parallel applications in cloud environments, and high performance computing platforms. It is well known that configuring a VM with too many virtual processors (vCPUs) worsens application performance due to scheduling cross-talk between the hypervisor and the guest OS. Specifically, when the number of vCPUs assigned to a VM exceeds available physical CPUs then parallel applications in the VM experience worse performance, even when number of application threads remains fixed. In this paper, we first track the root cause of this performance loss to inefficient hypervisor-Ievel emulation of inter-vCPU synchronization events. We then present three techniques to minimize hypervisor-induced overheads on parallel workloads in large- VCpuVMs. The first technique pins application threads to dedicated vCPUs to eliminate inter-vCPU thread migrations, reducing the overhead of emulating inter-processor interrupts (IPIs). The second technique para-virtualizes inter-vCPU TLB flush operations. The third technique enables faster reactivation of idle vCPUs by prioritizing the delivery of rescheduling IPIs. Unlike existing solutions which rely on heavyweight and slow vCPU hotplug mechanisms, our techniques are lightweight and provide more flexibility in migrating large-vCPU VMs. Using several parallel benchmarks, we demonstrate the effectiveness of our prototype implementation in the Linux KVM/QEMU virtualization platform. Specifically, we demonstrate that with our techniques, parallel applications can maintain their performance even when 255 vCPUs are assigned to a VM running on only 6 physical cores."
pub.1152580390,Internet of Things (IoT) for Controlled Environment in Greenhouses,"The book entry focused on IoT for a controlled environment in agricultural greenhouses. The following considerations inform the focus on IoT and greenhouses. First, regulating the greenhouse microclimate, particularly humidity, light intensity, soil nutritional content, water and temperature, and plant physiology, is key to higher crop yields. Second, IoT systems encompass cloud systems for intelligent agricultural engineering, sensors, soft robots, and wireless communication systems for crop production. The interoperability of the different systems eliminated the need for expensive human labor and improved data-driven decision-making. However, there are practical constraints to widespread adoption, including cost, risk of cyber-physical attacks, resource inequalities in advanced and emerging economies, and consumer attitudes. IoT holds great promise in revolutionizing agriculture and future global food security despite the challenges. The existing challenges will be resolved through future research and development and investment in smart farming by smallholders and large commercial farms."
pub.1172438672,Poster: Fast On-Device Adaptation with Approximate Forward Training,"Enabling real-time machine learning (ML) model adaptation to previously unseen, but highly specific contexts and environments can vastly extend the capability of mobile and ubiquitous AI systems. Cloud-aided approaches often fall short of meeting the time constraints without assuming pre-acquisition of data. Other existing approaches targeting efficient training on mobile devices focus on the generally complex context-agnostic tasks where achieving the performance of DNNs without proper backpropagation-based training is unlikely. In this work, we introduce a novel approximate forward training scheme to leverage the relationship that the updates to the parameters of a specific linear (and convolutional) layer in each training step are the linear combinations of outputs from the previous layer. Our preliminary results demonstrate the feasibility of this approach on mobile platforms."
pub.1182068597,Outsourcing Attribute-Based Encryption to Enhance IoT Security and Performance,"As the adoption of Internet of Things (IoT) systems, particularly those integrated with cloud technology, continues to expand, ensuring data security and privacy while maintaining optimal performance becomes increasingly challenging. Complex encryption algorithms, when run on IoT devices with limited resources, can significantly hinder processing speed and resource efficiency. This paper introduces an innovative Attribute-Based Encryption (ABE) framework that offloads computationally intensive cryptographic operations to a proxy server. This approach alleviates the computational strain on resource-constrained IoT devices, allowing them to efficiently handle encryption and decryption tasks despite their limited processing power, memory, and battery life. Additionally, we present a robust security model that ensures the privacy and integrity of data in IoT environments, in line with the requirements of ABE. We conduct an extensive performance analysis, evaluating key metrics such as execution time, ciphertext size, and memory usage, demonstrating that our proposed scheme surpasses existing state-of-the-art methods in efficiency. The primary contributions of this work include the development of a lightweight ABE offloading framework, the creation of a strong security model, and a thorough performance assessment that highlights the scheme’s efficiency and practicality for real-world IoT applications."
pub.1134293634,Public key encryption with equality test for Industrial Internet of Things system in cloud computing,"Abstract Present day world have evolved from traditional environment to smart industries using IoT scheme which in turn forms Industrial Internet of Things (IIoT), which significantly elaborated by providing enhance integration using smart communication through IoT based sensors. IIoT has been providing cost reduction and enhancement in technology by bringing availability, flexibility and data sharing through real time scenario. Despite being unsecure environment of cloud, the privacy of data transfer and information confidentiality is guaranteed. In this context, this work presents a Public Key Encryption with Equality Test based on DLP with double decomposition problems over near‐ring. Computation Diffie‐Hellman is utilized in algebraic structure which involves DLP with Double Decomposition problem for proposing a Public Key Encryption with Equality Test which provides more security to the scheme. The proposed method is highly secure and it solves the problem of quantum algorithm attacks in IIoT systems. Further, the suggested system is significantly secure and it prevents the chosen‐ciphertext attack in type‐I rival and it is indistinguishable against the random oracle model for the type‐II rival. The recommended scheme is highly secure and the security analysis measures are comparatively stronger than existing techniques. Search time of the proposed scheme is 150 milliseconds for which the number of attributes is 50 and when comparing to the decryption time of the proposed model which is lower when compared to other existing scheme for 50 attributes."
pub.1120038556,Live maintenance robot for high-voltage transmission lines," Purpose The purpose of this paper is to improve the operation and maintenance intelligence of power systems, and summarize the transmission line robots and their key technologies. High-voltage power cables are important channels for power transmission systems. Their special geographical environment and harsh natural environment can lead to many different faults. At present, such special operations in dangerous and harsh environments are performed manually, which have not only high labor intensity and low work efficiency but also great personal safety risks.   Design/methodology/approach For maintenance works that are far away from the tower, power outages are required. With the increasing evaluation of transmission quality and operational safety, and the urgent need for automation and operation of modern power systems, the contradiction between this manual operation and modern high-quality power transmission has become increasingly prominent. An effective method to replace the manual maintenance work is to use the mobile robot to carry the operation manipulator and its end tool, that is, the live maintenance robot.   Findings Some achievements have been made in the key technologies of live maintenance robots, the work to be done to meet the basic requirements of complex and changeable line environment and practical application. Based on the existing research results of live overhaul robot, the follow-up research will focus on the practical application needs and the frontier of scientific and technological development, and truly realize the human–machine integration between live overhaul robot–human working environment. Only in this way can the robot better serve the operation and maintenance of the power system.   Originality/value This paper reviews the system platform, operation function, structural characteristics and key technologies involved in the power cable robot, and the combination of live maintenance robots and modern high-tech such as big data and cloud computing is also given, and finally, the future development direction of the special operation robot is pointed out. "
pub.1120742251,P2P-based open health cloud for medicine management,"Much attention has recently been given to changes in medical services, such as remote medical services and healthcare services customized for users, where cloud technology is utilized in the health and medical industries. It is possible to utilize health information needed by users in real time if medical records and health information are saved through this, so that the level of medical practice increases, and medical information in a new form can be predicted and provided based on big data analysis and processing through the accumulated medical data. Thus, studies of mobile services utilizing this are being conducted. In particular, there is an increasing demand for the development of a cloud-based service in pharmaceutical management. With the development of modern medicine, by simply taking medicine it has become possible to treat diseases that in the past might have threatened lives. When patients get prescriptions for various medicines, it is necessary to know what role and effect they have, and in order to prevent misuse and abuse, it is necessary to provide correct and accurate information about these medicines. Numerous drugs enhance quality of life, but those aimed at treatment may be lethal unless patients know how to take them. In this study, we propose a peer-to-peer (P2P)-based open health cloud for medicine management. The proposed system is designed to smoothly provide a virtual cloud service by building up various cloud environments and communicating with cloud servers in a pre-reserved, on-demand method. The aim is to resolve the problems of data processing and reduce delays with wireless body area networks (WBANs), which occur in existing cloud health services, and to enhance stability and QoS for things like response time. In addition, to integrate personal health record (PHR) data stored in an internal medical database built up by each institution, and for the integration of various medical systems, the proposed system is designed to allow access to file managers through open database connectors, and it includes source connectors that link to external systems. Based on this, an access interface was designed to provide a mobile chat-bot service for user convenience. The proposed system is a mobile health service in the form of a chat-bot to quickly deal with changes through incidents that may occur because users take the wrong medicine by mistake in their everyday lives. To build up and analyze big data based on information collected in various ways, and to provide the data to users conveniently, the service is broadly divided into five classes, and in each class and in each service class is a customized user interface/user experience."
pub.1106322442,Evaluating a Fit-For-Purpose Integrated Service-Oriented Land and Climate Change Information System for Mountain Community Adaptation,"Climate change challenges mountain communities to prepare themselves via Community-Based Adaptation (CBA) plans that reduce vulnerability. This paper outlines the evaluation of a developed web-based information system to support CBA, referred to as a Mountain Community Adaptive System (MCAS). The web-based user interface visualizes collated data from data providers, integrating it with near real-time climate and weather datasets. The interface provides more up-to-date information than was previously available on the environment, particularly on land and climate. MCAS, a cloud-based Land Information System (LIS), was developed using an Agile-inspired approach offering system creation based on bare minimum system requirements and iterative development. The system was tested against Fit-For-Purpose Land Administration (FFP LA) criteria to assess the effectiveness in a case from Nepal. The results illustrate that an MCAS-style system can provide useful information such as land use status, adaptation options, near real-time rainfall and temperature details, amongst others, to enable services that can enhance CBA activities. The information can facilitate improved CBA planning and implementation at the mountain community level. Despite the mentioned benefits of MCAS, ensuring system access was identified as a key limitation: smartphones and mobile technologies still remain prohibitively expensive for members of mountain communities, and underlying information communication technology (ICT) infrastructures remain under-developed in the assessed mountain communities. The results of the evaluation further suggest that the land-related aspects of climate change should be added to CBA initiatives. Similarly, existing LIS could have functionalities extended to include climate-related variables that impact on land use, tenure, and development."
pub.1165778560,Federated Digital Twin,"Digital Twin (DT) is a virtual replica of a physical system that is constantly receiving information from different data sources, enhancing its operations and processes through data analytics, predictions and simulations. The development of DTs relies on advancements in cutting-edge technologies namely IoT, Big data, Cloud computing and Artificial Intelligence; and although it was initially conceived in manufacturing, it is currently contributing to the digital transformation of several fields including aeronautics, healthcare, urban planning and agriculture. The existing body of research suggests that it will be expanded in the next few years with the implementation of sophisticated applications, therefore different proposals to achieve collective work between DTs have been investigated. Nevertheless, much research is needed to develop and validate appropriate mechanisms to ensure its successful deployment in complex real-world cases that require collaboration among individual systems. A Federated Digital Twin (FDT) has been identified as a promising solution for this approach, since it allows the interconnection among autonomous DTs in the virtual space, leveraging their advantages and enabling interaction, collaboration and shared learning. Additionally, since a FDT is envisaged as a network of cooperative DTs, cognitive principles can be applied to assist the overall operations through knowledge acquisition and reasoning, leading to an informed and intelligent decision making. This study aims to expand the FDT concept, develop mechanisms for coordination and synchronization based on well-defined FDT goals and connectionism theory. Furthermore, four architectural styles are provided to enable the integration of collaborative DTs within a federated environment, aiming to improve the operations in complex real-world systems."
pub.1142414682,Amalgamation of Advanced Technologies for Sustainable Development of Smart City Environment: A Review,"The concept of smart city evolved with the integration of information and communication technology (ICT) in various sub-systems and processes in urban environment. The development of the smart cities is the best possible solution to major urban issues. It contributes towards economic and social development of the residents. It aims to provide the cordial environment in the domains of healthcare, education, transportation, power generation and dissipation, security, living, industry, etc., to the inhabitants to make their lives comfortable. Sustainability of these services is another major objective in a smart city framework. Along with the true realization of the idea of a smart city, advanced computational and communication technologies are contributing hugely towards its sustainable development. Communication technologies act as backbone to ensure connectivity at the various levels in a smart city framework. Novel smart city solutions for different application domains are designed and deployed by the industry using advanced computational technologies like IoT, Artificial Intelligence, Blockchain, Big Data and Cloud Computing. In this work, authors discuss the concept of smart city, its architecture and sustainability. Different operational domains in a smart city ecosystem are elaborated. The cyber physical aspect of the smart cities is discussed in brief. The role of various computational and communication technologies in the sustainable development of smart cities is presented. Limiting factors in the deployment of various advanced technologies in different smart city domains are highlighted. Security issues associated with the technological sustainable development of different smart city services along with existing solutions are discussed. The article is concluded by highlighting the future research directions."
pub.1126803195,SPARC’s Open Online Simulation Platform for Computational Modeling of the ANS’s Physiological Role and its Modulation by Electroceutical Devices: o2S2PARC,"   The NIH SPARC initiative aims to “transform the understanding of nerve‐organ interactions” and “advance the neuromodulation field towards precise disease treatment.” To this end, a freely‐accessible online platform (o 2 S 2 PARC) has been designed to complement and leverage in vitro and in vivo physiological experiments with computational modeling. Computational models validated on existing data, can be used to investigate manifold subjects including organ electrophysiology, neural dynamics and electroceutical device interactions.   At the core of o 2 S 2 PARC are detailed human and animal models, which include functionalized and dynamic peripheral nerves. These models serve as integration centers for simulations and measurement data and as contexts for physical modeling. The o 2 S 2 PARC platform also allows users to create, host, connect, share and execute simulations without requiring any software other than a web browser. A full‐stack system production version of the platform has been implemented and deployed in the cloud. It includes a scalable network of computational services deployed inside Dockers, essential solvers (e.g., for coupled electromagnetic‐neuronal dynamics modeling) and a flexible, user‐friendly graphical user interface. In addition, users can select from feature‐rich post processing services including 3D visualisations, plotters, and the possibility to include the user’s own data analytics tools.   Selected models from SPARC teams (cardiac physiology & regulation, compound action potential prediction, enteric nervous system) can now be freely accessed and executed through the SPARC data portal website. In addition, solvers implemented on the platform have already successfully applied to study a range of therapeutic applications, such as vagus nerve stimulation. In short, o 2 S 2 PARC has established a freely accessible, extendible, and intuitive online simulation platform for neuromodulation that facilitates the integration of organ models and simulation of nerve‐organ interactions within the complex environment of the body. The platform is constantly progressing and amassing collaborators, becoming an important contribution for translational research.    Support or Funding Information National Institutes of Health SPARC award OT3OD025348 "
pub.1000249033,Towards building information modelling for existing structures,"
                    Purpose
                    The transformation of cities from the industrial age (unsustainable) to the knowledge age (sustainable) is essentially a “whole life cycle” process consisting of planning, development, operation, reuse and renewal. During this transformation, a multi‐disciplinary knowledge base, created from studies and research about the built environment aspects is fundamental: historical, architectural, archeologically, environmental, social, economic, etc., and critical. Although there are a growing number of applications of 3D VR modelling applications, some built environment applications such as disaster management, environmental simulations, computer‐aided architectural design and planning require more sophisticated models beyond 3D graphical visualization such as multifunctional, interoperable, intelligent, and multi‐representational. Advanced digital mapping technologies such as 3D laser scanner technologies can be enablers for effective e‐planning, consultation and communication of users' views during the planning, design, construction and lifecycle process of the built environment. These technologies can be used to drive the productivity gains by promoting a free‐flow of information between departments, divisions, offices, and sites; and between themselves, their contractors and partners when the data captured via those technologies are processed and modelled into building information modelling (BIM). The use of these technologies is a key enabler to the creation of new approaches to the “Whole Life Cycle” process within the built and human environment for the twenty‐first century. This paper aims to look at this subject.
                  
                  
                    Design/methodology/approach
                    The paper describes the research towards BIM for existing structures via the point cloud data captured by the 3D laser scanner technology. A case study building is used to demonstrate how to produce 3D CAD models and BIM models of existing structures based on designated techniques.
                  
                  
                    Findings
                    The paper finds that BIM can be achieved for existing structures by modelling the data captured with 3D laser scanner from the existing world. This can be accomplished by adapting appropriate automated data processing and pattern recognition techniques through applied science research.
                  
                  
                    Practical implications
                    BMI will enable automated and fast data capture and modelling for not only in design and planning, building refurbishment, effective heritage documentation and VR modelling but also disaster management, environmental analysis, assessment and monitoring, GIS implementation, sophisticated simulation environments for different purposes such as climate change, regeneration simulation for complexity and uncertainty and so on. As a result, it wi"
pub.1181407787,V2IViewer: Towards Efficient Collaborative Perception via Point Cloud Data Fusion and Vehicle-to-Infrastructure Communications,"Collaborative perception (CP) with vehicle-toinfrastructure (V2I) communications is a critical scenario in high-level autonomous driving. This paper presents a novel framework called V2IViewer to facilitate collaborative perception, which consists of three modules: object detection and tracking, data transmission, and object alignment. On this basis, we design a heterogeneous multi-agent middle layer (HMML) as the backbone to extract feature representations, and utilize a Kalman filter (KF) with the Hungarian algorithm for object tracking. For transmitting object information from infrastructure to egovehicle, Protobuf is utilized for data serialization using binary encoding, which reduces communication overheads. For object alignment from multiple agents, a Spatiotemporal Asynchronous Fusion (SAF) method is proposed, which uses a Multilayer Perceptron (MLP) for generating post-synchronization object sequences. These sequences are then utilized for fusion to enhance the accuracy of the integration. Experimental validation on DAIR-V2X-C, V2X-Seq, and V2XSet datasets shows that V2IViewer enhances long-range object detection accuracy by an average of 12.9% over state-of-the-art collaborative methods. Moreover, V2IViewer demonstrates an average improvement in accuracy of 3.3% across various noise conditions compared to existing models. Finally, the system prototype is implemented and the performance has been validated in realistic environments."
pub.1171078291,IoV-6G+: A secure blockchain-based data collection and sharing framework for Internet of vehicles in 6G-assisted environment,"The growing need for wide and ubiquitous accessibility to advanced Intelligent Transportation Systems (ITS) has led to the evolution of conventional Vehicle to Everything (V2X) paradigms into the Internet of Vehicles (IoVs). Next-generation IoVs establish seamless connections among humans, vehicles, Internet of Things (IoT) devices, and service platforms to enhance transit efficiency, road safety, and environmental conservation, with notable advancements in IoV technologies, specifically in secure data exchange and user privacy protection. Additionally, Unmanned Aerial Vehicles (UAVs) are envisioned as scalable and adaptable solutions for comprehensive IoV service coverage. The current approaches in IoV mainly concentrate on local infrastructure setups, overlooking the potential of infrastructure-less IoVs that lack extensive edge facilities. Also, the transmission of data over public channels by vehicles equipped with servers is susceptible to interception and tampering by malicious attackers. Moreover, the substantial amount of real-time data generated by vehicles, IoT devices, travelers, and social interactions pose a significant strain on servers as well as latency issues. To address these security concerns, this research suggests a secure-lightweight data collection and sharing framework for 6G-assisted smart transportation using blockchain and UAV, called IoV-6G+. The process entails drones collecting information and transmitting it to dedicated edge servers that securely aggregate the information and generate transactions on a cloud server. A cloud server securely collects data from edge servers, creates transactions, combines them into blocks, and then confirms and adds these blocks to the blockchain through a voting-based consensus method in a peer-to-peer network of cloud servers. Additionally, the security analysis of our proposed IoV-6G+ is assessed using the “Informal and formal (i.e., Scyther Tool, and Real-or-Random (RoR) Model) methods”, showcasing its efficacy in delivering a secure and efficient authentication mechanism for IoV. Moreover, the proposed IoV-6G+ exhibits lower computational and communication costs as well as energy consumption, along with enhanced security features, when compared to existing authentication frameworks."
pub.1022796639,Russia's new personal data localization regulations: A step forward or a self-imposed sanction?,"The paper represents one of the first comprehensive analyses of Russian personal data localization regulations, which became effective at September 1, 2015. This work describes in detail the main components of the data localization mechanism: triggers of its application, scope, exemptions and enforcement. It also takes into account the official and non-official interpretations of the law by Russian regulators, some of which were developed with the participation of the author. Special consideration is given to the jurisdictional aspects of the Russian data protection legislation and the criteria of its application to foreign data controllers. The author also reveals the rationale behind the adoption of data localization provisions and analyzes their possible impact on foreign companies operating in Russia and implementation of innovative IT-technologies (Cloud computing, Big Data and Internet of Things). The paper concludes that most of the potential benefits of data localization provisions, i.e. in the area of public law, law enforcement activities and taxation. Nevertheless, data localization provisions may still have medium-term positive impact on privacy, since they force all stakeholders to revisit the basic concepts of existing personal data legislation (the notion of personal data, data controller, processing, etc.), thus serving as a driver for re-shaping existing outdated data privacy regulations and crafting something more suitable for the modern IT-environment."
pub.1112863789,Impact of IoT to Accomplish a Vision of Digital Transformation of Cities,"In this chapter, the researchers have reviewed and assessed the IoT issues and challenges in the implementation of a smart city in the Indian scenario. They extensively acknowledge that the IoT advancements and applications are still in their emerging stages. Moreover, there are many issues and challenges associated with IoT such as legal, regulatory, and economic queries, and infrastructural development, security, and privacy issues. In addition, the study also revisits and explains the network communication models used in establishing connection with electronic devices and the Internet through different layers. The applications of the IoT and blockchain concepts in building future cities and information systems and hypothesized models on EVM and UID cards have also been discussed. Further, the study presents a hypothetical model explaining the device as it relates to cloud communication for monitoring water usage and pumping. The study adds to the existing literature on the IoT and creation of the smart city. The chapter states that smart cities and IoT innovation solutions will positively affect the growth of the country, although its reception rate among countries is moderately low because of the absence of trust, inadequate innovation, and limited access to technological infrastructure. To ensure that this new innovation works, all governing bodies should make effective policies. For the administration of advanced technology, first support the development of progressive networks by giving the required motivation and freedom to the communication industry. The chapter concludes by discussing the futuristic approach of IoT for creating smart cities in India and the challenges in the implementation of technologies. Communication through the Internet is done between humans, but when considering future internet objects that can communicate through themselves and back, that interaction becomes the Internet of Things (IoT). The IoT is an interweaving of networks, from the user end to virtual reality, and within devices. The IoT communication process in a device-to-device or device-to-cloud occurs over various layers. A device-to-device communication model can be designed as a system that is with or without human intervention. The chapter describes the tuturistic approach to IoT for smart cities and Blockchain approach and challenges in India. The ideology behind blockchain creation is to create a trustworthy environment for financial transactions, reducing costs, improvisations to securing databases, and making devices and systems impermeable to external threats. The literature on smart cities displays several innovative ideas that have segmented the nature of the advance of digital properties."
pub.1014492901,The Unusual Tidal Dwarf Candidate in the Merger System NGC 3227/3226: Star Formation in a Tidal Shock?,"We report the discovery of active star formation in the H I cloud associated with the interacting Seyfert system NGC 3227/3226 that was originally identified as a candidate tidal dwarf galaxy (TDG) by Mundell et al. and that we name J1023+1952. We present broadband optical B, R, I (from the Isaac Newton Telescope), and ultraviolet images (from XMM-Newton) that show that the H I cloud is associated with massive ongoing star formation seen as a cluster of blue knots (MB ≲ -15.5 mag) surrounded by a diffuse ultraviolet halo and cospatial with a ridge of high neutral hydrogen column density (NH ~ 3.7 × 1021 cm-2) in the southern half of the cloud. We also detect Hα emission from the knots with a flux density of FHα ~ 2.55 × 10-14 ergs s-1 cm-2 corresponding to a star formation rate of SFR(Hα) ~ 10.6 × 10-3 M☉ yr-1. J1023+1952 lies at the base of the northern tidal tail, and, although it spatially overlaps the edge of the disk of NGC 3227, Mundell et al. showed that the H I cloud is kinematically distinct with an H I mean velocity 150 km s-1 higher than that of NGC 3227. Comparison of ionized (Hα) and neutral (H I) gas kinematics of the cloud shows closely matched recessional velocities, providing strong evidence that the star-forming knots are embedded in J1023+1952 and are not merely optical knots in the background disk of NGC 3227, thus confirming J1023+1952 as a gas-rich (MH/LB > 1.5) dwarf galaxy. No star formation is detected in the northern half of the cloud, despite similar H I column densities; instead, our new high-resolution H I image shows a ridge of high column density coincident with the reddest structures evident in our B - I image. We suggest that these structures are caused by the background stellar continuum from the disk of NGC 3227 being absorbed by dust intrinsic to J1023+1952, thus placing J1023+1952 in front of NGC 3227 along the line of sight. We discuss two scenarios for the origin of J1023+1952: as a third, preexisting dwarf galaxy involved in the interaction with NGC 3227 and NGC 3226, or as a newly forming dwarf galaxy condensing out of the tidal debris removed from the gaseous disk of NGC 3227. The first scenario is feasible given that NGC 3227 is the brightest member of a galaxy group, an environment in which preexisting dwarf galaxies are expected to be common. However, the lack of a detectable old stellar population in J1023+1952 makes a tidal origin more likely. If J1023+1952 is a bound object forming from returning gaseous tidal tail material, its unusual location at the base of the northern tail implies a dynamically young age similar to its star formation age, and suggests it is in the earliest stages of TDG evolution. Whatever the origin of J1023+1952, we suggest that its star formation is shock-triggered by collapsing tidal debris."
pub.1135124179,Lightweight and Scalable DAG based distributed ledger for verifying IoT data integrity,"Verifying the integrity of IoT data in cloud-based IoT architectures is crucial for building reliable IoT applications. Traditional data integrity verification methods rely on a Trusted Third Party (TTP) that has issues of risk and operational cost by centralization. Distributed Ledger Technology (DLT) has a high potential to verify IoT data integrity and overcome the problems with TTPs. However, the existing DLTs have low transaction throughput, high computational and storage overhead, and are unsuitable for IoT environments, where a massive scale of data is generated. Recently, Directed Acyclic Graph (DAG) based DLTs have been proposed to address the low transaction throughput of linear DLTs. However, the integration of IoT Gateways (GWs) into the peer to peer (P2P) DLT network is challenging because of their low storage and computational capacity. This paper proposes Lightweight and Scalable DAG based distributed ledger for IoT (LSDI) that can work with resource-constrained IoT GWs to provide fast and scalable IoT data integrity verification. LSDI uses two key techniques: Pruning and Clustering, to reduce 1) storage overhead in IoT GWs by removing sufficiently old transactions, and 2) computational overhead of IoT GWs by partitioning a large P2P network into smaller P2P networks. The evaluation results of the proof of concept implementation showed that the proposed LSDI system achieves high transaction throughput and scalability while efficiently managing storage and computation overhead of the IoT GWs."
pub.1125892119,Future Directions for Smart Cities,"Key concepts, definitions, examples, and historical contexts for understanding smart cities, along with discussions of both drawbacks and benefits of this approach to urban problems. Over the past ten years, urban planners, technology companies, and governments have promoted smart cities with a somewhat utopian vision of urban life made knowable and manageable through data collection and analysis. Emerging smart cities have become both crucibles and showrooms for the practical application of the Internet of Things, cloud computing, and the integration of big data into everyday life. Are smart cities optimized, sustainable, digitally networked solutions to urban problems? Or are they neoliberal, corporate-controlled, undemocratic non-places? This volume in the MIT Press Essential Knowledge series offers a concise introduction to smart cities, presenting key concepts, definitions, examples, and historical contexts, along with discussions of both the drawbacks and the benefits of this approach to urban life. After reviewing current terminology and justifications employed by technology designers, journalists, and researchers, the book describes three models for smart city development—smart-from-the-start cities, retrofitted cities, and social cities—and offers examples of each. It covers technologies and methods, including sensors, public wi-fi, big data, and smartphone apps, and discusses how developers conceive of interactions among the built environment, technological and urban infrastructures, citizens, and citizen engagement. Throughout, the author—who has studied smart cities around the world—argues that smart city developers should work more closely with local communities, recognizing their preexisting relationship to urban place and realizing the limits of technological fixes. Smartness is a means to an end: improving the quality of urban life."
pub.1125892118,Citizen Input and Engagement,"Key concepts, definitions, examples, and historical contexts for understanding smart cities, along with discussions of both drawbacks and benefits of this approach to urban problems. Over the past ten years, urban planners, technology companies, and governments have promoted smart cities with a somewhat utopian vision of urban life made knowable and manageable through data collection and analysis. Emerging smart cities have become both crucibles and showrooms for the practical application of the Internet of Things, cloud computing, and the integration of big data into everyday life. Are smart cities optimized, sustainable, digitally networked solutions to urban problems? Or are they neoliberal, corporate-controlled, undemocratic non-places? This volume in the MIT Press Essential Knowledge series offers a concise introduction to smart cities, presenting key concepts, definitions, examples, and historical contexts, along with discussions of both the drawbacks and the benefits of this approach to urban life. After reviewing current terminology and justifications employed by technology designers, journalists, and researchers, the book describes three models for smart city development—smart-from-the-start cities, retrofitted cities, and social cities—and offers examples of each. It covers technologies and methods, including sensors, public wi-fi, big data, and smartphone apps, and discusses how developers conceive of interactions among the built environment, technological and urban infrastructures, citizens, and citizen engagement. Throughout, the author—who has studied smart cities around the world—argues that smart city developers should work more closely with local communities, recognizing their preexisting relationship to urban place and realizing the limits of technological fixes. Smartness is a means to an end: improving the quality of urban life."
pub.1125892115,An Introduction to Smart Cities,"Key concepts, definitions, examples, and historical contexts for understanding smart cities, along with discussions of both drawbacks and benefits of this approach to urban problems. Over the past ten years, urban planners, technology companies, and governments have promoted smart cities with a somewhat utopian vision of urban life made knowable and manageable through data collection and analysis. Emerging smart cities have become both crucibles and showrooms for the practical application of the Internet of Things, cloud computing, and the integration of big data into everyday life. Are smart cities optimized, sustainable, digitally networked solutions to urban problems? Or are they neoliberal, corporate-controlled, undemocratic non-places? This volume in the MIT Press Essential Knowledge series offers a concise introduction to smart cities, presenting key concepts, definitions, examples, and historical contexts, along with discussions of both the drawbacks and the benefits of this approach to urban life. After reviewing current terminology and justifications employed by technology designers, journalists, and researchers, the book describes three models for smart city development—smart-from-the-start cities, retrofitted cities, and social cities—and offers examples of each. It covers technologies and methods, including sensors, public wi-fi, big data, and smartphone apps, and discusses how developers conceive of interactions among the built environment, technological and urban infrastructures, citizens, and citizen engagement. Throughout, the author—who has studied smart cities around the world—argues that smart city developers should work more closely with local communities, recognizing their preexisting relationship to urban place and realizing the limits of technological fixes. Smartness is a means to an end: improving the quality of urban life."
pub.1037846533,Profiling and evaluating hardware choices for MapReduce environments: An application-aware approach,"The core business of many companies depends on the timely analysis of large quantities of new data. MapReduce clusters that routinely process petabytes of data represent a new entity in the evolving landscape of clouds and data centers. During the lifetime of a data center, old hardware needs to be eventually replaced by new hardware. The hardware selection process needs to be driven by performance objectives of the existing production workloads. In this work, we present a general framework, called Ariel, that automates system administrators’ efforts for evaluating different hardware choices and predicting completion times of MapReduce applications for their migration to a Hadoop cluster based on the new hardware. The proposed framework consists of two key components: (i) a set of microbenchmarks to profile the MapReduce processing pipeline on a given platform, and (ii) a regression-based model that establishes a performance relationship between the source and target platforms. Benchmarking and model derivation can be done using a small test cluster based on new hardware. However, the designed model can be used for predicting the jobs’ completion time on a large Hadoop cluster and be applied for its sizing to achieve desirable service level objectives (SLOs). We validate the effectiveness of the proposed approach using a set of twelve realistic MapReduce applications and three different hardware platforms. The evaluation study justifies our design choices and shows that the derived model accurately predicts performance of the test applications. The predicted completion times of eleven applications (out of twelve) are within 10% of the measured completion times on the target platforms."
pub.1145600571,Enhancing coding skills with CloudStor SWAN,"CloudStor SWAN (AARNet, 2022) is a research-focused web service for running analyses that is available to staff and students at many research institutes and Universities across Australia and New Zealand. In 2021, we used SWAN as a teaching tool in the master-level subject, Computational Genomics (COMP90016) at The University of Melbourne. This subject aims to teach students how to analyse large genomic datasets using best practices software tools, pipelines and student-written, custom code.    Although CloudStor SWAN was not conceived as a teaching tool, we worked with their technical staff to tailor the service to our use case. This innovative use of existing research infrastructure allowed us to effectively transition the subject to remote learning. Students and staff could log in to the service using their existing University credentials, from anywhere in the world, without the use of a VPN. The ability to access the platform from a web browser allowed for a consistent computing environment for all students regardless of operating system, and without having to worry about software installations on local machines. This presented a significantly improved experience from the custom servers that had been used in the past.   We used SWAN for weekly workshops during semester and for assessment in the form of assignments and an exam. It allowed us to format subject material in Jupyter notebooks where we could seamlessly integrate text, graphics and code. Additionally, assessed code questions can incorporate automatic marking and written submissions can be checked for plagiarism. SWAN also allowed us to introduce students to the UNIX command line, an important skillset that was not previously taught in the University of Melbourne Master of Science (Bioinformatics) program.   From a student perspective, SWAN allowed for a practical skillset to be developed alongside theoretical knowledge from other aspects of the course. The platform was simple to learn and allowed students to focus on the subject content and the tasks asked of them, rather than on the interface. From a teacher’s perspective, having a unified platform allowed for a single set of clear instructions, improved troubleshooting and clearer management of tool versions and software dependencies. The use of Jupyter notebooks simplified lesson plans and assessments by integrating multiple elements into single documents. This element also made the lessons more easily sharable between colleagues and collaborators.   Our integration of this technology into our tertiary teaching has served as a model for a similar use at a different Australian university. We hope to share the lessons learned from this subject, the advantages of using CloudStor SWAN in a teaching environment for both staff and students and provide some advice for others who may want to adapt it to fit their own teaching needs. Presentation link: https://youtu.be/8tutCO1hd9c References   AARNet. (2022). CloudStor: Access, store, shar"
pub.1150765763,An adaptive anti-disturbance navigation method for polarized skylight-based autonomous integrated navigation system,"In satellite-denied confrontation or unstructured environments, the navigation performance of the GPS will be severely restricted or even deceived. The SINS/polarization navigation system (PNS)/odometer (OD) integration is not only free from electromagnetic interference, but also has complementary navigation performance, which can meet the needs of autonomous navigation without GPS. However, PNS is limited to the disturbances from the environment, such as shelter, cloud, or large maneuvers. The effect of multiple scattering from real weather conditions and computation methods on the navigation model is considered in this article. It is modeled as an angle of polarization error. The error is estimated in real time to improve the accuracy of PNS navigation model. Aiming at the outliers in polarization measurement caused by shelter, cloud or large maneuvers, an adaptive anti-disturbance navigation method is designed. In particular, this method introduces the degree of polarization to design an adaptive factor to adjust the filter gain. Then, the innovation is utilized to estimate the measurement noise matrix in the presence of outliers. Several simulation experiments and outdoor tests were performed to verify the effectiveness of the proposed method. The experimental results show that the proposed method has better heading estimation than existing methods under large maneuvers."
pub.1125892116,Models for Smart City Development,"Key concepts, definitions, examples, and historical contexts for understanding smart cities, along with discussions of both drawbacks and benefits of this approach to urban problems. Over the past ten years, urban planners, technology companies, and governments have promoted smart cities with a somewhat utopian vision of urban life made knowable and manageable through data collection and analysis. Emerging smart cities have become both crucibles and showrooms for the practical application of the Internet of Things, cloud computing, and the integration of big data into everyday life. Are smart cities optimized, sustainable, digitally networked solutions to urban problems? Or are they neoliberal, corporate-controlled, undemocratic non-places? This volume in the MIT Press Essential Knowledge series offers a concise introduction to smart cities, presenting key concepts, definitions, examples, and historical contexts, along with discussions of both the drawbacks and the benefits of this approach to urban life. After reviewing current terminology and justifications employed by technology designers, journalists, and researchers, the book describes three models for smart city development—smart-from-the-start cities, retrofitted cities, and social cities—and offers examples of each. It covers technologies and methods, including sensors, public wi-fi, big data, and smartphone apps, and discusses how developers conceive of interactions among the built environment, technological and urban infrastructures, citizens, and citizen engagement. Throughout, the author—who has studied smart cities around the world—argues that smart city developers should work more closely with local communities, recognizing their preexisting relationship to urban place and realizing the limits of technological fixes. Smartness is a means to an end: improving the quality of urban life."
pub.1140186395,EnTruVe: ENergy and TRUst-aware Virtual Machine allocation in VEhicle fog computing for catering applications in 5G,"It is undoubted that fog computing contributes in catering the latency-stringent applications of 5G, and one of the enabling technologies that fundamentally ensures the success of fog computing is virtualization as it offers isolation and platform independence. Although the emergence of vehicle-based fog (referred to as v-fog) facilities can certainly benefit from these desirable features of virtualization, there are several challenges that need to be addressed in order to realize the full potential that v-fogs can offer. One of the challenges of virtualization in v-fog is Virtual Machine (VM) migration. There are several factors that trigger a VM migration in a v-fog such as vehicle resource depletion. VM migrations would not only lead to nonessential usage of valuable resources (e.g. energy, bandwidth, memory) in the v-fogs, but also incur various overheads and performance degradation throughout the whole network. Thus, minimizing VM migrations is necessary. Furthermore, to ensure the seamless VM migrations between v-fogs, trust of v-fogs is required. While there exists studies of trust in the virtualization of cloud, they are irrelevant to v-fogs as v-fogs are different in nature (i.e. heterogeneous, mobile) from the cloud. Additionally, trust is not included in the decision making mechanisms of VM allocation for vehicular environments in the existing works. Moreover, as vehicle resources are constrained, their energy has to be utilized efficiently. In this paper, we propose EnTruVe, an ENergy and TRUst-aware VM allocation in VEhicle fog computing solution that aims to minimize the number of VM migrations while reducing VM processing associated energy consumption as much as possible. The VM allocation algorithm in EnTruVe provides a larger selection pool of v-fogs that meets the VMs requirements (e.g. trust, latency), thereby ensuring higher chances of success of VM allocation. Using Analytic Hierarchy Process (AHP), the proposed EnTruVe solution evaluates the v-fogs based on a set of metrics (e.g. energy consumption and end-to-end latency) to select the optimal v-fog for a VM allocation. Results obtained demonstrate that EnTruVe has the least number of VM migrations and it is the most energy efficient solution. Additionally, it shows that EnTruVe provides the highest utilization of v-fogs of up to 57.6% in comparison to other solutions as the number of incoming requests increases."
pub.1166695716,Analysis of Smart Parking System Using IOT Environment,"The typical parking experience has been transformed by smart parking systems that use the Internet of Things (IoT) environment to integrate technology to improve efficiency, convenience, and sustainability. In order to monitor and manage parking spaces in real-time, this unique technique makes use of IoT devices, such as sensors, cameras, and networking technologies. As a result of the system's reliable information on parking availability, drivers may find and book parking spaces in advance, which eases traffic and reduces aggravation. Additionally, parking systems with IoT capabilities optimize resource use, lowering carbon emissions and fostering sustainability. The adoption of IoT in parking systems is a crucial step towards building smarter, more connected cities that will enhance both drivers' and parking operators' experiences with parking. There are numerous crucial elements in the process for developing a smart parking system in an IoT context. First, sensors are placed in parking places to gather up-to-the-minute occupancy information. Then, using wireless communication protocols, this data is sent to a central server or cloud computing platform. After that, a data processing and analysis module interprets the gathered data using algorithms and machine learning techniques and presents parking availability information to users via a mobile application or other user interfaces. For effective management and monitoring of parking spaces, the system also includes automated payment methods and interacts with existing infrastructure. Taken as Alternative parameters is Park Smart, Street line, Park Whiz, ParkMobile, Spot Hero. Taken as evaluation parameters is Light Sensor, CCTV coins, SMS, Cost-effectiveness, Timestamp. This demonstrates the rank of the data set Park Smart is on 1st Rank, ParkMobile is on 2nd Rank, Park Whiz is on 3rd Rank, Street line is on 4th Rank and Spot Hero is on 5th Rank. To sum up, implementing a smart parking system employing IoT technology has shown to be a potential way to deal with the problems associated with urban parking. The system increases parking efficiency, lessens traffic congestion, and enhances user experience by utilising IoT sensors, data analytics, and real-time communication. The parking scene in smart cities has the potential to change dramatically, enhancing ease and sustainability."
pub.1117728181,Cyber Industry Networks as an environment of the Industry 4.0 implementation,"The fourth industrial (r)evolution called Industry 4.0 transfers production to a different dimension of productivity, flexibility and also mobility. Enterprises using advanced technologies such as Mobile Technologies, Big Data Analytics, Cloud Computing, Internet of Things, etc. and using intelligent resources (e.g. mobile devices, mechatronic machines, means of transport) communicating with each other via the Internet in real time can produce customized products quickly and at low cost. On the one hand the Industry 4.0 concept creates great benefits for the client, and on the other hand raises challenges for industrial enterprises. There is a need to improve existing and create new business models that will combine the potential of many enterprises and together create innovative, very modern production networks based on mobile technologies and the Internet of Things. The article presents the results of research about the creation of innovative production networks operating in the environment of advanced technologies within the Industry 4.0. The aim of the research is to develop the concept of creating temporary Cyber Industry Networks based on e-business platforms that integrate dispersed enterprise resources through mobile devices and mobile software, creating a temporary network in the situation of emerging business opportunities. The main benefit of such networks is the ability to manufacture products through geographically dispersed corporate resources. The Cyber Industry Network allows business processes through the integration of mobile technologies in real time to be managed, and enables to reduce costs, improve flexibility, and as a result increase the competitiveness of manufacturing enterprises."
pub.1045919584,Implementing a resource list management system in an academic library,"
                    Purpose
                    – The purpose of this paper is to review the key components of the introduction of a new resource list management system (RLMS) at Nottingham Trent University (NTU) using the Aspire application from Talis Education. It explains the key service goals; the implementation milestones; the main technical challenges which needed to be addressed; and the dynamic relationship between the rollout of the RLMS and existing selection, acquisition and resource delivery processes.
                  
                  
                    Design/methodology/approach
                    – This evidence in this paper is drawn from the experiences of the NTU RLMS project group, which involved colleagues from Libraries and Learning Resources, Information Systems and the Virtual Learning Environment (VLE) teams at the university. It draws on both qualitative evaluations and quantitative assessments of adoption and use by academics, students and library staff; and the internal mechanisms of project review.
                  
                  
                    Findings
                    – This paper concludes that the successful technical implementation of a cloud-based mission-critical service for academics and students depends on a successful collaboration between library, VLE and technical teams; and reaffirms that a hosted RLMS service still requires the deployment of local technical expertise. It is essential (although not always straightforward) to try to anticipate the impact that the introduction of a new RLMS will have on existing processes (inside the library and without). Ultimately, however, the successful implementation of an RLMS is dependent on securing its adoption by both academics and students; not least by ensuring that the application meets their needs. Although it is not a technical prerequisite, the prospect of a successful implementation of an RLMS is greatly improved when working with the grain of a supportive institutional policy environment.
                  
                  
                    Originality/value
                    – Interest in “next-generation” resource list systems which can address the needs of students, academics and library services is likely to increase sharply in the next few years, as library services seek to align both resource spend and resource discovery more closely than ever with the student experience around “directed reading”. The experiences of an “early adopter” implementer of an RLMS highlight some of the key prerequisites and significant operational decisions, and provide a number of insights for those about to embark on a similar implementation process.
                  "
pub.1094854208,Decision Support in Data Centers for Sustainability,"In this paper, we propose a decision support system (DSS) for the greening of data centers to help the environment, hence promoting sustainability. As society continues the relentless shift towards electronic communications there is a growing demand for greater storage and processing on data centers. A potential area of improvement is to gain greater server utilization rates since traditionally the phenomenon of “server sprawl” occurs where more servers are added to the data center without seeking greater utilization rates on existing servers first. This implies maintaining more servers than actually needed that translates to greater carbon dioxide emissions causing potential environmental problems. Presently, average server utilization rates in most data centers are rather low, and we make the claim that utilization rates should be increased so that we can lower the number of servers for enhanced sustainability. A shift to the cloud could potentially be useful here. Some servers can be phased out with their operations being hosted on the cloud instead. We propose an approach based on data mining using CBR and decision trees to build a DSS that would help make decisions pertaining to issues such as server sprawl and migration to the cloud in order to promote data center sustainability. We provide recommendations based on our DSS that would be useful to data center operators in academia and industry."
pub.1119016133,From Molecular Cores to Planet-forming Disks: A SIRTF Legacy Program,"Crucial steps in the formation of stars and planets can be studied only at
mid-infrared to far-infrared wavelengths, where SIRTF provides an unprecedented
improvement in sensitivity. We will use all three SIRTF instruments (IRAC,
MIPS, and IRS) to observe sources that span the evolutionary sequence from
molecular cores to protoplanetary disks, encompassing a wide range of cloud
masses, stellar masses, and star-forming environments. In addition to targeting
about 150 known compact cores, we will survey with IRAC and MIPS (3.6 to 70
micron) the entire areas of five of the nearest large molecular clouds for new
candidate protostars and substellar objects as faint as 0.001 solar
luminosities. We will also observe with IRAC and MIPS about 190 systems likely
to be in the early stages of planetary system formation(ages up to about 10
Myr), probing the evolution of the circumstellar dust, the raw material for
planetary cores. Candidate planet-forming disks as small as 0.1 lunar masses
will be detectable. Spectroscopy with IRS of new objects found in the surveys
and of a select group of known objects will add vital information on the
changing chemical and physical conditions in the disks and envelopes. The
resulting data products will include catalogs of thousands of previously
unknown sources, multiwavelength maps of about 20 square degrees of molecular
clouds, photometry of about 190 known young stars, spectra of at least 170
sources, ancillary data from ground-based telescopes, and new tools for
analysis and modeling. These products will constitute the foundations for many
follow-up studies with ground-based telescopes, as well as with SIRTF itself
and other space missions such as SIM, JWST, Herschel, and TPF."
pub.1045327723,From Molecular Cores to Planet‐forming Disks: An SIRTF Legacy Program,"Crucial steps in the formation of stars and planets can be studied only at mid‐ to far‐infrared wavelengths, where the Space Infrared Telescope (SIRTF) provides an unprecedented improvement in sensitivity. We will use all three SIRTF instruments (Infrared Array Camera [IRAC], Multiband Imaging Photometer for SIRTF [MIPS], and Infrared Spectrograph [IRS]) to observe sources that span the evolutionary sequence from molecular cores to protoplanetary disks, encompassing a wide range of cloud masses, stellar masses, and star‐forming environments. In addition to targeting about 150 known compact cores, we will survey with IRAC and MIPS (3.6–70 μm) the entire areas of five of the nearest large molecular clouds for new candidate protostars and substellar objects as faint as 0.001 solar luminosities. We will also observe with IRAC and MIPS about 190 systems likely to be in the early stages of planetary system formation (ages up to about 10 Myr), probing the evolution of the circumstellar dust, the raw material for planetary cores. Candidate planet‐forming disks as small as 0.1 lunar masses will be detectable. Spectroscopy with IRS of new objects found in the surveys and of a select group of known objects will add vital information on the changing chemical and physical conditions in the disks and envelopes. The resulting data products will include catalogs of thousands of previously unknown sources, multiwavelength maps of about 20 deg2 of molecular clouds, photometry of about 190 known young stars, spectra of at least 170 sources, ancillary data from ground‐based telescopes, and new tools for analysis and modeling. These products will constitute the foundations for many follow‐up studies with ground‐based telescopes, as well as with SIRTF itself and other space missions such as SIM, JWST, Herschel, and TPF/Darwin."
pub.1132442109,"A Serious Gaming Approach to Integrate BIM, IoT, and Lean Construction in Construction Education","Whereas building information modeling (BIM) and lean construction (LC) can be considered state-of-the-art practices in construction project design, planning, and control, internet of things (IoT) has yet a chance to connect processes in construction site monitoring applications and beyond. An opportunity might be that all of the aforementioned methods are hardly well enough integrated in a systematic way for purposes in knowledge-based decision making. A prominent example is that LC and associated simulation games in academic or professional education still rely much on manual data input and analysis. Proposed is a learning platform (a serious gaming environment) that teaches the concept of permanent availability of up-to-date actual performance data sets using an integrated BIM-IoT-LC approach. While the platform builds upon existing cloud-based BIM tools and well-known takt planning and takt control principles, a novel IoT platform was developed that rapidly elevates the gathering and transformation from data to information to knowledge while the participants play the game. The platform was tested in several academic learning environments around the world. Findings demonstrate the participants understand the important role of technology in practical applications. By extending their experiences from traditional LC simulation games with the BIM and IoT components, specifically, they were able to interact and collaborate more closely. Access to just-in-real-time project status information on smart wearable devices and large display screens helped improving the expected project execution timeline while reducing certain waste factors. One limitation that was discovered is the tight integration of the developed technology into new learning curricula that aim at digitalizing construction."
pub.1175810444,Edge Computing Management With Collaborative Lazy Pulling for Accelerated Container Startup,"With the growing demand for latency-sensitive applications in 5G networks, edge computing has emerged as a promising solution. It enables instant response and dynamic resource allocation based on real-time network information by moving resources from the cloud to the network edge. Containers, known for their lightweight nature and ease of deployment, have been recognized as a valuable virtualization technology for service deployment. However, the prolonged startup time of containers can lead to long response time, particularly in edge computing scenarios characterized by long propagation time, frequent deployment, and migration. In this paper, we comprehensively consider image caching, container assignment, and registry selection problem in an edge system. To our best effort, there is no existing work that has taken all the above aspects into account. To address the problem, we propose a novel image caching strategy that employs partial caching, allowing local registries to cache either the least functional or complete version of application images. In addition, a container assignment and registry selection problem is solved by using an edge-based collaborative lazy pulling algorithm. To evaluate the performance of our proposed algorithms, we conduct experiments with real-world app usage data and popular images in a testbed environment. The experimental results demonstrate that our algorithms outperform traditional greedy algorithms in terms of average user response time and cache hit rate."
pub.1105107792,Integration of Lessons Learned Knowledge in Building Information Modeling,"Lessons learned systems are vital means for integrating construction knowledge into the various phases of the construction project life cycle. Many such systems are tailored toward the owner-organization’s specific needs and workflows to overcome challenges with information collection, documentation, and retrieval. Previous works have relied on the development of conventional local and network/cloud-based database management systems to store and retrieve lessons gathered on projects. These lessons learned systems operate independently and have not been developed to take full advantage of the benefits of integration with emerging building information modeling (BIM) technology. As such, construction professionals are faced with a lack of efficient and speedy retrieval of context-focused information on lessons learned for appropriate utilization in projects. To tackle this challenge, we propose the integration of lessons learned knowledge management in BIM in addition to existing two-dimensional to eight-dimensional modeling of project information. The integration was implemented through the embedding of nonstructured query system, NoSQL (MongoDB), in a BIM-enabled environment to host lessons learned information linked to model items and four-dimensional modeling project tasks of the digitized model. This is beyond existing conventional text-based queries and is novel. The system is implemented in .NET Frameworks and interfaced with a project management BIM tool, Navisworks Manage. The demonstration with a test case of a federated model from a predesign school project suggests that lessons learned systems can become an integral part of BIM environments and contribute to enhancing knowledge reuse in projects."
pub.1155848377,Enabling Technologies for Effective Planning and Management in Sustainable Smart Cities,"With the rapid penetration of technology in varied application domains, the existing cities are getting connected more seamlessly. Cities becomes smart by inducing ICT in the classical city infrastructure for its management. According to McKenzie Report, about 68% of the world population will migrate towards urban settlements in near future. This migration is largely because of the improved Quality of Life (QoL) and livelihood in urban settlements. In the light of urbanization, climate change, democratic flaws, and rising urban welfare expenditures, smart cities have emerged as an important approach for society’s future development. Smart cities have achieved enhanced QoL by giving smart information to people regarding healthcare, transportation, smart parking, smart traffic structure, smart home, smart agronomy, community security etc. Typically, in smart cities data is sensed by the sensor devices and provided to end users for further use. The sensitive data is transferredwith the help of internet creating higher chances for the adversaries to breach the data. Considering the privacy and security as the area of prime focus, this book covers the most prominent security vulnerabilities associated with varied application areas like healthcare, manufacturing, transportation, education and agriculture etc. Furthermore, the massive amount of data being generated through ubiquitous sensors placed across the smart cities needs to be handled in an effective, efficient, secured and privacy preserved manner. Since a typical smart city ecosystem is data driven, it is imperative to manage this data in an optimal manner. Enabling technologies like Internet of Things (IoT), Natural Language Processing (NLP), Blockchain Technology, Deep Learning, Machine Learning, Computer vision, Big Data Analytics, Next Generation Networks and Software Defined Networks (SDN) provide exemplary benefits if they are integrated in the classical city ecosystem in an effective manner. The application of Artificial Intelligence (AI) is expanding across many domains in the smart city, such as infrastructure, transportation, environmental protection, power and energy, privacy and security, governance, data management, healthcare, and more. AI has the potential to improve human health, prosperity, and happiness by reducing our reliance on manual labor and accelerating our progress in the sciences and technologies. NLP is an extensive domain of AI and is used in collaboration with machine learning and deep learning algorithms for clinical informatics and data processing. In modern smart cities, blockchain provides a complete framework that controls the city operations and ensures that they are managed as effectively as possible. Besides having an impact on our daily lives, it also facilitates many areas of city management."
pub.1093194406,A Noval Algorithm for Distributed Data Mining in HDFS,"Evolution of Cloud computing technology over the Internet and drastic increase in data size and intensity (Big Data) persuade Map Reduce and distributed file systems like HDFS (Hadoop Distributed File System) as the paradigm of choice for distributed data mining applications. With size and complexity of data growing every day, distributed data mining algorithms has to be designed to handle Big Data in compatible with the latest technology available on distributed computing. Earlier research activities in data mining comprises, focus on increasing the performance for single task computing algorithms rather than distributed computing which would provide more fast and scalable environment for processing large datasets. Existing algorithms in the field of distributed frequent pattern data mining includes, TPFP-tree, BTP tree, and CARM. But these algorithms suffer from unbalanced workload management among its clusters. In this paper, a novel algorithm, named Association rule mining based on Hadoop (ARMH) has been proposed to utilize the clusters effectively and mining frequent pattern from large databases. Hadoop distributed framework helps in managing the workload among the clusters. The ARMH was implemented in hadoop using Map Reduce programming paradigm."
pub.1041057666,Closed-loop design evolution of engineering system using condition monitoring through internet of things and cloud computing,"Flexibility of a manufacturing system is quite important and advantageous in modern industry, which function in a competitive environment where market diversity and the need for customized product are growing. Key machinery in a manufacturing system should be reliable, flexible, intelligent, less complex, and cost effective. To achieve these goals, the design methodologies for engineering systems should be revisited and improved. In particular, continuous or on-demand design improvements have to be incorporated rapidly and effectively in order to address new design requirements or resolve potential weaknesses of the original design. Design of an engineering system, which is typically a multi-domain system, can become complicated due to its complex structure and possible dynamic coupling between domains. An integrated and concurrent approach should be considered in the design process, in particular in the conceptual and detailed design phases. In the context of multi-domain design, attention has been given recently to such subjects as multi-criteria decision making, multi-domain modeling, evolutionary computing, and genetic programing. More recently, machine condition monitoring has been considered for integration into a scheme of design evolution even though many challenges exist for this to become a reality such as lack of systematic approaches and the existence of technical barriers in massive condition data acquisition, transmission, storage and mining. Recently, the internet of things (IoT) and cloud computing (CC) are being developed quickly and they offer new opportunities for evolutionary design for such tasks as data acquisition, storage and processing. In this paper, a framework for the closed-loop design evolution of engineering systems is proposed in order to achieve continuous design improvement for an engineering system through the use of a machine condition monitoring system assisted by IoT and CC. New design requirements or the detection of design weaknesses of an existing engineering system can be addressed through the proposed framework. A design knowledge base that is constructed by integrating design expertise from domain experts, on-line process information from condition monitoring and other design information from various sources is proposed to realize and supervise the design process so as to achieve increased efficiency, design speed, and effectiveness. The framework developed in this paper is illustrated by using a case study of design evolution of an industrial manufacturing system."
pub.1101883107,A Distributed Management Method Based on the Artificial Fish-Swarm Model in Cloud Computing Environment,"Recently, there are some problems in the centralized management, such as centralized management, heavy burden, excessive number of virtual machine migration, lack of mutual cooperation mechanism between nodes, can’t adapt to the cluster of change. The existing distributed management methods exist between the nodes have less cooperation mechanism and only simple communication, quality of service is no obvious improvement, system can save energy consumption is not obvious. According to the behavior characteristics of the fish, this paper presents an artificial fish to achieve mutual cooperation of nodes in the distributed management method."
pub.1095407357,Distributed FP-ARMH Algorithm in Hadoop Map Reduce Framework,"Evolution of Cloud computing technology over the Internet and drastic increase in data size and intensity (Big Data) persuade Map Reduce and distributed file systems like HDFS (Hadoop Distributed File System) as the paradigm of choice for distributed data mining applications. With size and complexity of data growing every day, distributed data mining algorithms has to be designed to handle Big Data in compatible with the latest technology available on distributed computing. Earlier research activities in data mining comprises, focus on increasing the performance for single task computing algorithms rather than distributed computing which would provide more fast and scalable environment for processing large datasets. Existing algorithms in the field of distributed frequent pattern data mining includes, TPFP-tree, BTP tree, and CARM. But these algorithms suffer from unbalanced workload management among its clusters. In this paper, a novel algorithm, named Association rule mining based on Hadoop (ARMH) has been proposed to utilize the clusters effectively and mining frequent pattern from large databases. Hadoop distributed framework helps in managing the workload among the clusters. The ARMH was implemented in hadoop using Map Reduce programming paradigm."
pub.1163712079,Blend of IoT and Extended Reality for Safety in Construction Trend of Technology,"The unprecedented challenges of fast growing cities, due to exponential increase in urban population, demands for massive housing and infrastructure projects. It is a big challenge to cater to the huge demand meeting stringent timeline and optimized economics without compromise in the high quality products. Shorter deadlines are resulting in increased fatalities at jobsite and needs immediate attention worldwide. Latest IT tools like Extended Reality (XR) applications are the solutions which will be an integral part of construction Industry. The present paper provides an insight into the extended reality (Virtual Reality, Augmented Reality and Mixed Reality) application to prevent the Operational Safety and Health (OSH) related hazards in constructions, XR technology, working of XR, present status of research, site limitations while implementing the latest tools etc. It’s an exciting time to explore new solutions for enhanced safety on construction sites, improvising the technological applications with an objective to make zero accidents (OSH) a reality at site. The digital and physical views are combined through Augmented Reality, for achieving accuracy and higher efficiency, in order to build the overall confidence in any projects. Latest IT tools advocate the use of an integrated human workforce and technology to optimise the resource consumption, contributing to the vision of the company. The technology will continue to mature and become economically feasible, while seeing increased investment and adoption. With foresightedness of identifying critical points of that field / structure and setting up sensors and camaras to capture the data, analysing the big data with references to physical environment and field conditions, makes IoT an integral part of technology even in construction field.The interaction with digital world through real and virtual world, is leading to many versions of ER and IoT is to be used integrating these two worlds with sensed data and automation of Industry. Technology enabled response system to alert the user while using hazardous materials. Similarly, creating a virtual city with existing scenario, connecting it to the real city through cloud based sensors and cameras to gather the data, analytics of big data to develop IoT applications will streamline the city planning and improves 2-D blue prints. All data points gathered from IoT helps in envision the historical repository of data which includes the infrastructures and facilities like pedestrian traffic, street traffic, lighting and other services, weather conditions so on. Industrial safety is one such area."
pub.1118682735,"IAU Commission 37 ""Star Clusters and Associations"" Legacy report","It is widely accepted that stars do not form in isolation but result from the
fragmentation of molecular clouds, which in turn leads to star cluster
formation. Over time, clusters dissolve or are destroyed by interactions with
molecular clouds or tidal stripping, and their members become part of the
general field population. Star clusters are thus among the basic building
blocks of galaxies. In turn, star cluster populations, from young associations
and open clusters to old globulars, are powerful tracers of the formation,
assembly, and evolutionary history of their parent galaxies. Although their
importance had been recognised for decades, major progress in this area has
only become possible in recent years, both for Galactic and extragalactic
cluster populations. Star clusters are the observational foundation for stellar
astrophysics and evolution, provide essential tracers of galactic structure,
and are unique stellar dynamical environments. Star formation, stellar
structure, stellar evolution, and stellar nucleosynthesis continue to benefit
and improve tremendously from the study of these systems. Additionally,
fundamental quantities such as the initial mass function can be successfully
derived from modelling either the H-R diagrams or the integrated velocity
structures of, respectively, resolved and unresolved clusters and cluster
populations. Star cluster studies thus span the fields of Galactic and
extragalactic astrophysics, while heavily affecting our detailed understanding
of the process of star formation in dense environments.This report highlights
science results of the last decade in the major fields covered by IAU
Commission 37: Star clusters and associations."
pub.1139851038,Virtual European Solar & Planetary Access (VESPA) 2021: consolidation,"VESPA (Virtual European Solar and Planetary Access) has been focusing for nearly 10 years on adapting Virtual Observatory (VO) techniques to handle Planetary Science data [1] [2]. The objective of this activity is to build a contributive data distribution system where data services are located and maintained in research institutes, as well as in space agencies and observatories. This system is responsive to the new paradigm of Open Science and FAIR access to the data.During the previous Europlanet-2020 program, VESPA has defined an architecture adapted from the astronomy VO, incorporating concepts and standards from other areas (Earth observation, Heliophysics, etc). The basic system uses the VO infrastructure: data services are installed in any location but are declared in a system of harvested registries with identifiers, end-point (URL), mention of supported access protocols, and rough description of content. Such services are interoperable via clients and tools, which also provide visualization and analysis functions.The activity in Europlanet-2024 focuses on expanding this environment, enforcing sustainability, and opening new possibilities to improve data processing – such as workflows, cloud-based computation, and readiness for exploitation through Machine Learning techniques.Data access. VESPA has defined a specific access protocol called EPN-TAP which at the time of writing is a Working Draft of the Internal Virtual Observatory Alliance (IVOA), and expected to become a Recommendation in the coming months [3]. The EPN-TAP metadata system provides uniform description of datasets not only to access data in a VO context, but also for research projects. EPN-TAP is compliant with the general TAP protocol, allowing usage of existing VO tools and communication protocols with data services pertaining to Solar System studies. Some VO tools (TOPCAT, Aladin, CASSIS) were also adapted to improve the handling of such data.The VESPA portal, intended as a discovery tool to browse the EPN-TAP services, is under study to improve the user experience. ElasticSearch capacities are being implemented, and all interface mechanisms are being evaluated. Other, more specific access modes (via script, web services, VO tools, etc) are also being reviewed.Data services. There are currently 55 EPN-TAP data services published in the IVOA registry, and about 20 in development phase. Most of them are implemented on DaCHS, a VO data server provided by Heidelberg University. A major upgrade of DaCHS published last year implements recent evolutions of IVOA standards. Existing data services are currently reviewed for compliance, and upgraded to benefit from the latest developments. In many cases, this is also an occasion to extend their content with new data. This upgrade also addresses low-level technical aspects, e.g. related to declaration in the IVOA registry.Larger data infrastructures with EPN-TAP interface (AMDA, SSHADE, PVOL) also continue to develop their content an"
pub.1181778178,Optimized Dual Access Control for Cloud-based Data Storage and Distribution using Global-Context Residual Recurrent Neural Network,"Cloud computing has become a prevalent platform for data storage and evaluation, but its open access through the internet makes stored data vulnerable to a range of security threats. The existing security mechanisms, though effective, often fall short in addressing advanced and evolving intrusion techniques. This motivates the need for a more robust, efficient, and intelligent access control system that can not only detect intrusions but also prevent unauthorized access in real-time. This study proposes an Optimized Dual Access Control mechanism for secure cloud-based data storage and distribution using Global-Context Residual Recurrent Neural Network (DAC-GCRRNN-FOFMOA-CDS). This novel approach aims to enhance both data protection and access management in cloud environments. The proposed methodology contains three distinct phases: (i) registration phase, (ii) intrusion detection phase, (iii) intrusion prevention phase. During the registration phase, user credentials are created for secure data uploads. The data undergoes pre-processing in the intrusion detection phase, where redundant and noisy information is filtered out using Poisson Multi-Bernoulli Mixture Filtering (PMBMF). This ensures data normalization and sanitization, preparing it for the classification process. The core of this process involves the use of the Global-Context Residual Recurrent Neural Network (GCRRNN), which automatically classifies users as legitimate or unauthorized. To further enhance classification accuracy, the GCRRNN is optimized using the Fractional-Order Fish Migration Optimization Algorithm (FOFMOA). The intrusion prevention phase involves verifying user identity through login credentials, ensuring robust protection against unauthorized access. The effectiveness of the proposed DAC-GCRRNN-FOFMOA-CDS model is validated using the Enron Email Database, and its performance is measured through key metrics like sensitivity, precision, F-score, specificity, error rate, accuracy, and ROC curve analysis. The proposed method attains 23.98%, 21%, 29% higher accuracy, 27.94%, 21.09%, 30% higher precision, 10.87%, 17.98%, 22% lower error rate compared with the existing techniques."
pub.1125164839,Weaver: A Novel Configuration Designer for IT/NW Services in Heterogeneous Environments,"A configuration of an information technology/network (IT/NW) service is composed of components (e.g., applications, servers, and switches), relationships among them, and their attributes. To run a service appropriately, its configuration should be carefully organized to satisfy customer requirements, as well as dependencies and constraints derived from the components. Designing such a service configuration is daunting when the service is deployed in a heterogeneous environment including multiple clouds, edge devices, and different types of existing servers and network nodes. Thus, the time and cost for the designing task are serious problems in providing new services. In this paper, we present Weaver, an automated service configuration designer that generates a concrete service configuration on the basis of abstract customer requirements and the environment in which the service is deployed. Weaver accepts information about such requirements and environment as input and converts it into a fully concretized service configuration that can be deployed in the designated environment. In this paper, we take an example of a video surveillance Internet of Things (IoT) service, where multiple components including cameras, video analyzers and terminals, are distributed over multiple locations. In the evaluation, we show that Weaver produces configurations for designated heterogeneous environments. We also show that design time is dramatically shorter than in traditional system integration procedures. For example, the design time is shorter than a minute when the number of components is less than 150."
pub.1141886754,Cybersecurity capabilities for critical infrastructure resilience," Purpose For many innovative organisations, Industry 4.0 paves the way for significant operational efficiencies, quality of goods and services and cost reductions. One of the ways to realise these benefits is to embark on digital transformation initiatives that may be summed up as the intelligent interconnectivity of people, processes, data and cyber-connected things. Sadly, this interconnectivity between the enterprise information technology (IT) and industrial control systems (ICS) environment introduces new attack surfaces for critical infrastructure (CI) operators. As a result of the ICS cybersecurity risk introduced by the interconnectivity between the enterprise IT and ICS networks, the purpose of this study is to identify the cybersecurity capabilities that CI operators must have to attain good cybersecurity resilience.   Design/methodology/approach A scoping literature review of best practice international CI protection frameworks, standards and guidelines were conducted. Similar cybersecurity practices from these frameworks, standards and guidelines were grouped together under a corresponding National Institute of Standards and Technology (NIST) cybersecurity framework (CF) practice. Practices that could not be categorised under any of the existing NIST CF practices were considered new insights, and therefore, additions.   Findings A CI cybersecurity capability framework comprising 29 capability domains (cybersecurity focus areas) was developed as an adaptation of the NIST CF with an added dimension. This added dimension emphasises cloud computing and internet of things (IoT) security. Each of the 29 cybersecurity capability domains is executed through various capabilities (cybersecurity processes and procedures). The study found that each cybersecurity capability can further be operationalised by a set of cybersecurity controls derived from various frameworks, standards and guidelines, such as COBIT®, CIS®, ISA/IEC 62443, ISO/IEC 27002 and NIST Special Publication 800-53.   Practical implications CI sectors are immediately able to adopt the CI cybersecurity capability framework to evaluate their levels of resilience against cyber-attacks, given new attack surfaces introduced by the interconnectivity of cyber-connected things between the enterprise and ICS levels.   Originality/value The authors present an added dimension to the NIST framework for CI cyber protection. In addition to emphasising cryptography, IoT and cloud computing security aspects, this added dimension highlights the need for an integrated approach to CI cybersecurity resilience instead of a piecemeal approach. "
pub.1171453928,Assessing the Potential of Onboard LiDAR-Based Application to Detect the Quality of Tree Stems in Cut-to-Length (CTL) Harvesting Operations,"This paper investigated the integration of LiDAR technology in cut-to-length (CTL) harvesting machines to enhance tree selection accuracy and efficiency. In the evolution of CTL forest machines towards improving operational efficiency and operator conditions, challenges persist in manual tree selection during thinning operations, especially under unmarked conditions and complex environments. These can be improved due to advances in technology. We studied the potential of LiDAR systems in assisting harvester operators, aiming to mitigate workload, reduce decision errors, and optimize the harvesting workflow. We used both synthetic and real-world 3D point cloud data sets for tree stem defect analysis. The former was crafted using a 3D modelling engine, while the latter originated from forest observations using 3D LiDAR on a CTL harvester. Both data sets contained instances of tree stem defects that should be detected. We demonstrated the potential of LiDAR technology: The analysis of synthetic data yielded a Root Mean Square Error (RMSE) of 0.00229 meters (m) and an RMSE percentage of 0.77%, demonstrating high detection accuracy. The real-world data also showed high accuracy, with an RMSE of 0.000767 m and an RMSE percentage of 1.39%. Given these results, we recommend using on-board LiDAR sensor technologies for collecting and analyzing data on tree/forest quality in real-time. This will help overcome existing barriers and drive forest operations toward enhanced efficiency and sustainability."
pub.1139169700,“MumCare”: An Artificial Intelligence Based Assistant,"Bringing a new life to the world is a wonder to every mother. Experience faced by pregnant mothers vary from one pregnancy to another pregnancy. There is a vast amount of information available on the Internet and printed materials. Yet, this knowledge is too complex or lengthy and very few applications provide customized information to pregnant mothers. In a time where smart phones have become a necessity in our life, a mobile app is one of the easiest ways to obtain prenatal information. Hence, we have developed a mobile application to help pregnant mothers. This application includes an artificial intelligence (AI) based chatbot. AI chat bot communicates and guides the mother in a way that creates the illusion as if they are talking to their unborn child. The Spiral Model was used as the development methodology and the application was developed in an environment of continuous integration and deployment with GIT personal repository. This application was implemented using React Native and the Node.js. Chat bot was created with Dialogflow agent and integrated with the firebase through the Google cloud functions. Some existing applications were studied to identify the features and limitations of current pregnancy care mobile applications. This solution is realistic and successful and it has an upgradable model of growth. The rapid development of the Internet and mobile devices in the world has changed people's lifestyles. This mobile app will be helpful for pregnant mothers living in rural as well as in metropolitan areas alike and can enhance education and health."
pub.1173430966,A Lightweight Blockchain Architecture with Smart Collaborative and Progressive Evolution for Privacy-Preserving 6G IoT,"As 6G wireless perception and interconnection capabilities become increasingly pervasive, the interconnected metadata of the world introduces significant challenges in preserving privacy. Blockchain technology has emerged as a potentially effective means of preserving privacy in open 6G IoT networks. However, the complexity of existing methods often inhibits their widespread deployment and the maintenance of secure ledgers for societal entities. This article introduces a novel private blockchain architecture known as smart collaborative blockchain with progressive evolution (SCOPE), designed to tackle the complexity-security trade-off in 6G IoT. In the context of the 6G IoT, we propose a swarm intelligence model called hierarchical raft (HRAFT), which gathers valuable workload information from all nodes to facilitate the election of reliable leaders. SCOPE employs these leaders to accomplish high-performance decentralization, scaling terminal, edge, and cloud resources through intelligent collaboration and offloading policies. Furthermore, we present reinforcement proof of workload (RPoW), an independent subnetting mechanism that identifies abnormal blocks via random-verifiable tasks. Evaluated in fifteen real industrial 6G IoT scenarios, RPoW has been shown to optimize energy utilization by incrementally adjusting workload levels in accordance with defined risks. Lastly, we construct a prototype system and an experimental testbed, providing scalable storage and privacy-sharing within resource-constrained 6G AIoT environments."
pub.1136071123,Virtual Tours and Augmented Reality for Direct Data Integration,"<p>The symptoms of overdue maintenance and underinvestment in historic infrastructure are ever-present in our society (rated with D+ by American Society of Civil Engineers, ASCE). To ensure the safety of historic and existing structures, on-site inspections are required, and structural health monitoring systems are frequently adopted. While these methods capture large amounts of information about a structure, there is not a comprehensive method for integrating these diverse datasets into a single, intuitive environment for both on- and off-site usage. The objective of this work is to explore two different methodologies for integrating datasets derived from the built environment. The first method, Virtual tours and informational modeling (VTIM), uses spherical panoramas to capture the interior and exterior of the built environment as well as enables a user to access data through on-click conditionals attached to the panoramas. The second method, Image-based documentation and augmented reality (IBDAR), uses a sparse point cloud to capture the built environment for on-site viewing and additionally stores an image at every annotation location to facilitate off-site viewing. Both methods are applied to case studies including a masonry lighthouse in Charleston, SC and a pedestrian bridge in Princeton, NJ. The advantages and disadvantages of each approach are discussed as well as steps for future work are briefly outlined.</p>"
pub.1118395210,Ices in starless and starforming cores,"Icy grain mantles are commonly observed through infrared spectroscopy toward
dense clouds, cloud cores, protostellar envelopes and protoplanetary disks. Up
to 80% of the available oxygen, carbon and nitrogen are found in such ices; the
most common ice constituents - H2O, CO2 and CO - are second in abundance only
to H2 in many star forming regions. In addition to being a molecular reservoir,
ice chemistry is responsible for much of the chemical evolution from H2O to
complex, prebiotic molecules. Combining the existing ISO, Spitzer, VLT and Keck
ice data results in a large sample of ice sources (\sime80) that span all
stages of star formation and a large range of protostellar luminosities
(<0.1-105 L\odot). Here we summarize the different techniques that have been
applied to mine this ice data set on information on typical ice compositions in
different environments and what this implies about how ices form and evolve
during star and planet formation. The focus is on how to maximize the use of
empirical constraints from ice observations, followed by the application of
information from experiments and models. This strategy is used to identify ice
bands and to constrain which ices form early during cloud formation, which form
later in the prestellar core and which require protostellar heat and/or UV
radiation to form. The utility of statistical tests, survival analysis and ice
maps is highlighted; the latter directly reveals that the prestellar ice
formation takes place in two phases, associated with H2O and CO ice formation,
respectively, and that most protostellar ice variation can be explained by
differences in the prestellar CO ice formation stage. Finally, special
attention is paid to the difficulty of observing complex ices directly and how
gas observations, experiments and models help in constraining this ice
chemistry stage."
pub.1111934166,Evaluating Indoor Location Triangulation Using Wi-Fi Signals,"Abstract
The advancement in Global Positioning System (GPS), has led to a huge number of location-based applications. Such applications can also be very useful for indoor environment; however, GPS technology struggles with indoor location mapping. Currently, there are various techniques, which are used for indoor localization namely: wireless fidelity-based, Bluetooth, radio frequency identification (RFID), infrared beam, and Sensors. The Wi-Fi access points (APs) are installed at various indoor locations to cover most of the areas, and the smart phones and tablets, are equipped with wireless transceiver modules, which can receive Wi-Fi signals. Therefore, it becomes more practical to use Wi-Fi signal for such application in comparison to infrared beam, Bluetooth and other wireless technologies, as Wi-Fi has significant advantages, including wider range, higher stability, and there are no requirements for additional hardware devices. Literature review confirms that the non-line of sight (NLOS) factors and the multipath effect easily affects most of the existing indoor localization algorithms based on Wi-Fi access points (APs). There also exist many other problems, such as positioning stability and blind spots, which can cause a decline in positioning accuracy at certain positions or even failure of positioning. In this research, we propose to use triangulation of location based on Wi-Fi signals from multiple APs. This method utilizes the received signal strength indications (RSSI) from multiple static APs to determine the location. Based on this, evaluation is done using experiments to measure the accuracy and effectiveness of the new proposed algorithm. The results are promising and can be improved with the use of Artificial intelligence, which is the future work of this project. The proposed method will overcome most of the problems caused by NLOS factors and the multipath effect."
pub.1123765345,Power Management as a Service (Pmaas) – An Energy Saving Service by “Auto-Fit VM Placement” Algorithm in Cloud Computing for Maximum Resource Utilization at Minimum Energy Consumption Without Live-Migrating the Virtual Machines by using the Concept of Virtual-Servers,"Now a day Energy Consumption is one of the most promising fields amongst several computing services of cloud computing. A maximum amount of Power resources are absorbed by the data centre because of huge amount of data processing which is increased abnormally. So it’s the time to think about the energy consumption in cloud environment. Existing Energy Consumption systems are limited in terms of virtualization because improper virtualization leads to loads imbalance and excessive power consumption and inefficiency in terms of computational power. Billing[1,2 ] is another exciting feature that is closely related to energy consumption, because higher or lesser billing depends on energy consumption somehow-as we know that cloud providers allow cloud users to access resources as pay-per-use, so these resources need to be optimally selected to process the user request to maximize user satisfaction in the distributed virtualized environment. There may be an inequity between the actual power consumption by the users and the provided billing records by the providers, So any false accusation that may claimed by each other to get illegal compensations. To avoid such accusation, we propose a work to consolidate the VMs using the Power Management as a Service (PMaaS) model in such a way, to reduce power consumption by maximum resource utilization without live-migration of the virtual machines by using the concept of Virtual Servers. The proposed PMaaS model uses a new “Auto-fit VM placement algorithm”, which computes tasks resource demands, models a Virtual Machine that fits those demands, and places the Virtual Machines on a Virtual server made by the collective resources (CPU, Memory, Storage and Bandwidth) from the respective schedulers directly connected to the actual physical servers and that has the minimum remaining resources which is large enough to accommodate such a Virtual Machine."
pub.1120301220,VM Selection and Allocation Policy to Optimize VM Migration in Cloud Environment,"Cloud computing, a metered based technology provides the services using virtualized technology over the internet. In the cloud environment, to improve the performance (such as utilization of the resources, energy minimization) extreme number of virtual machines (VMs) can be installed on the servers as per their resource capacity. In this way, servers can be overloaded. Overloaded servers consume more energythan normal status servers. VM migration (VMM) is an efficient technique to become a server in a normal state. VMM technique is used to consolidate the resources to increase resource utilization (RU) and reduceenergy usage. In the VMM technique, selection of VM such as which VM is migrated from one server to another server and allocation of VM on servers is an important aspect. Appropriate VM selection declines the numeral of VMMs and increasesenergy efficiency. Appropriate VM allocation declines the server to become overloaded. In this paper, the VM selection and allocation strategy is presented. CloudSim toolkit is used to verify the strength of proposed VM selection and allocation algorithm. Proposed VM Selection algorithm (MaMT) performs better than existing MiMT algorithm in terms of total energy consumption, number of hosts shut down, number of VMM, and average Service Level Agreement (SLA) violation rate. MaMT algorithm with resource aware provisioning (RAP) and MiMT+RAP algorithm combines both VM selection and allocation policies. RAP algorithm used both energy and RU parameters while allocating VM to the server.MaMTreduces the energy consumption up to 7.25% and reduces the SLA violation rate up-to 2.6% in comparison to MiMT algorithm. When VM selection and allocation policies combines together than more system performance is improved. MaMT+RAPreduces the energy consumption up to6.76% and reduces the SLA violation rate up-to 0.22% in comparison to MaMT algorithm.MiMT+RAPreduces the energy consumption up to15.23% and reduces the SLA violation rate up-to 0.95% in comparison to MiMT algorithm."
pub.1133635892,Securing Multimedia by Using DNA-Based Encryption in the Cloud Computing Environment,"Today, the size of a multimedia file is increasing day by day from gigabytes to terabytes or even petabytes, mainly because of the evolution of a large amount of real-time data. As most of the multimedia files are transmitted through the internet, hackers and attackers try to access the users’ personal and confidential data without any authorization. Thus, maintaining a strong security technique has become a significant concerned to protect the personal information. Deoxyribonucleic Acid (DNA) computing is an advanced field for improving security, which is based on the biological concept of DNA. A novel DNA-based encryption scheme is proposed in this article for protecting multimedia files in the cloud computing environment. Here, a 1024-bit secret key is generated based on DNA computing and the user's attributes and password to encrypt any multimedia file. To generate the secret key, the decimal encoding rule, American Standard Code for Information Interchange value, DNA reference key, and complementary rule are used, which enable the system to protect the multimedia file against many security attacks. Experimental results, as well as theoretical analyses, show the efficiency of the proposed scheme over some well-known existing schemes."
pub.1151410378,Accuracy determination using deep learning technique in cloud-based IoT sensor environment,"The Internet of Things (IoT) offers users a wide variety of facilities because it interconnects billions of smart devices. However, when connected to wireless connections, unlimited access to IoT gadgets poses potential risks. As it eases cost constraints on sensor nodes, the cloud service with IoT networks has received greater attention. In addition, the high complexity of the distribution and networking of IoT makes them vulnerable to attacks. Intrusion detection systems (IDSs) are selected to ensure the security of reliable information and operations. IDS successfully detects anomalies in complex network situations and guarantees network security. Deep Convolution Network (DCN) IDS have a slow learning curve and poor categorization precision. Deep Learning (DL) methods are often used in a wide range of safety data processing, imaging, and signal processing like Poor transfer learning ability, reusability of modules, and integration. To overcome the constraints of Machine Learning (ML) IDS is intended to provide a comprehensive mechanism to learn the detection mechanism for multicloud IoT environments. The proposed IDS approach increases training efficiencies while increasing detection accuracy. Experimental investigations of the proposed system using the considered database confirms that the performance of the proposed system is capable and in the range of acceptance with relative to existing methods. Further, achieving detection capability, reliability, and accuracy of 97.51, 96.28, and 94.41% respectively are achieved."
pub.1146262233,Research and Construction of University Data Governance Platform Based on Smart Campus Environment,"Campus data is a subset of education big data. It is a variety of data generated by teachers and students in the life, teaching, scientific research, management and service process, as well as various school affairs management status data. It has the characteristics of a wide variety of data. Contains great information value, and giving full play to its role is an indispensable part of achieving the school's strategic goals. Taking the opportunity of building a smart campus, using advanced technologies such as cloud computing, big data, Internet of Things, and artificial intelligence, through a big data management platform, the full collection of existing business data inside and outside the school is provided, and a normal data governance model is provided to eliminate data islands. Realize normal data sharing services. The article conducts research on the data governance platform, and builds a data governance platform system with functions such as a normalized data quality monitoring system, information resource catalog system and full-link data monitoring to realize university data integration, business collaboration, service upgrades and assistance Decision-making provides a solid foundation for the application and expansion of smart campuses in universities and provides a reference for smart campus builders in universities."
pub.1154573648,A Privacy-Preserving Scheme for Smart Grid Using Trusted Execution Environment,"The increasing transformation from the legacy power grid to the smart grid brings new opportunities and challenges to power system operations. Bidirectional communications between home-area devices and the distribution system empower smart grid functionalities. More granular energy consumption data flows through the grid and enables better smart grid applications. This may also lead to privacy violations since the data can be used to infer the consumer’s residential behavior, so-called power signature. Energy utilities mostly aggregate the data, especially if the data is shared with stakeholders for the management of market operations. Although this is a privacy-friendly approach, recent works show that this does not fully protect privacy. On the other hand, some applications, like nonintrusive load monitoring, require disaggregated data. Hence, the challenging problem is to find an efficient way to facilitate smart grid operations without sacrificing privacy. In this paper, we propose a privacy-preserving scheme that leverages consumer privacy without reducing accuracy for smart grid applications like load monitoring. In the proposed scheme, we use a trusted execution environment (TEE) to protect the privacy of the data collected from smart appliances (SAs). The scheme allows customer-oriented smart grid applications as the scheme does not use regular aggregation methods but instead uses customer-oriented aggregation to provide privacy. Hence the accuracy loss stemming from disaggregation is prevented. Our scheme protects the transferred consumption data all the way from SAs to Utility so that possible false data injection attacks on the smart meter that aims to deceive the energy request from the grid are also prevented. We conduct security and game-based privacy analysis under the threat model and provide performance analysis of our implementation. Our results demonstrate that the proposed method overperforms other privacy methods in terms of communication and computation cost. The execution time of aggregation for 10,000 customers, each has 20 SAs is approximately 1 second. The decryption operations performed on the TEE have a linear complexity e.g., 172800 operations take around 1 second while 1728000 operations take around 10 seconds. These results can scale up using cloud or hyper-scalers for real-world applications as our scheme performs offline aggregation."
pub.1125851398,Access Control Role Evolution Mechanism for Open Computing Environment,"Data resources in open computing environments (including big data, internet of things and cloud computing) are characterized by large scale, wide source, and strong dynamics. Therefore, the user-permission relationship of open computing environments has a huge scale and will be dynamically adjusted over time, which enables effective permission management in the role based access control (RBAC) model to become a challenging problem. In this paper, we design an evolution mechanism of access control roles for open computing environments. The mechanism utilizes the existing user-permission relationship in the current system to mine the access control role and generate the user-role and role-permission relationship. When the user-permission relationship changes, the roles are constantly tuned and evolved to provide role support for access control of open computing environments. We propose a novel genetic-based role evolution algorithm that can effectively mine and optimize roles while preserving the core permissions of the system. In addition, a role relationship aggregation algorithm is proposed to realize the clustering of roles, which provides a supplementary reference for the security administrator to give the role real semantic information. Experimental evaluations in real-world data sets show that the proposed mechanism is effective and reliable."
pub.1117136941,Analysis and monitoring of IoT-assisted human physiological galvanic skin responsefactor for smart e-healthcare,"
                    Purpose
                    Background: Every so often, one experiences different physically unstable situations which may lead to possibilities of suffering through vicious physiological risks and extents. Dynamic physiological activities are such a key metric that they are perceived by means of measuring galvanic skin response (GSR). GSR represents impedance of human skin that frequently changes based on different human respiratory and physical instability. Existing solutions, paved in literature and market, focus on the direct measurement of GSR by two sensor-attached leads, which are then parameterized against the standard printed circuit board mechanism. This process is sometimes cumbersome to use, resulting in lower user experience provisioning and adaptability in livelihood activities. The purpose of this study is to validate the novel development of the cost-effective GSR sensing system for affective usage for smart e-healthcare.
                  
                  
                    Design/methodology/approach
                    This paper proposes to design and develop a flexible circuit strip, populated with essential circuitry assemblies, to assess and monitor the level of GSR. Ordinarily, this flexible system would be worn on the back palm of the hand where two leads would contact two sensor strips worn on the first finger.
                  
                  
                    Findings
                    The system was developed on top of Pyralux. Initial goals of this work are to design and validate a flexible film-based GSR system to detect an individual’s level of human physiological activities by acquiring, amplifying and processing GSR data. The measured GSR value is visualized “24 × 7” on a Bluetooth-enabled smartphone via a pre-incorporated application. Conclusion: The proposed sensor-system is capable of raising the qualities such as adaptability, user experience, portability and ubiquity for possible application of monitoring of human psychodynamics in a more cost-effective way, i.e. less than US$50.
                  
                  
                    Practical implications
                    Several novel attributes are envisaged in the development process of the GSR system that made it different from and unique as compared to the existing alternatives. The attributes are as follows: (i) use of reproductive sensor-system fabrication process, (ii) use of flexible-substrate for hosting the system as proof of concept, (iii) use of miniaturized microcontroller, i.e. ATTiny85, (iv) deployment of energy-efficient passive electrical circuitry for noise filtering, (v) possible use case scenario of using CR2032 coin battery for provisioning powering up the system, (vi) provision of incorporation of internet of things (IoT)-cloud integration in existing version while fixing related APIs and (vii) incorporation of heterogeneous software-based solutions to validate and monitor the GSR output such as"
pub.1146903481,A SLAM System with Direct Velocity Estimation for Mechanical and Solid-State LiDARs,"Simultaneous localization and mapping (SLAM) is essential for intelligent robots operating in unknown environments. However, existing algorithms are typically developed for specific types of solid-state LiDARs, leading to weak feature representation abilities for new sensors. Moreover, LiDAR-based SLAM methods are limited by distortions caused by LiDAR ego motion. To address the above issues, this paper presents a versatile and velocity-aware LiDAR-based odometry and mapping (VLOM) system. A spherical projection-based feature extraction module is utilized to process the raw point cloud generated by various LiDARs, hence avoiding the time-consuming adaptation of various irregular scan patterns. The extracted features are grouped into higher-level clusters to filter out smaller objects and reduce false matching during feature association. Furthermore, bundle adjustment is adopted to jointly estimate the poses and velocities for multiple scans, effectively improving the velocity estimation accuracy and compensating for point cloud distortions. Experiments on publicly available datasets demonstrate the superiority of VLOM over other state-of-the-art LiDAR-based SLAM systems in terms of accuracy and robustness. Additionally, the satisfactory performance of VLOM on RS-LiDAR-M1, a newly released solid-state LiDAR, shows its applicability to a wide range of LiDARs."
pub.1154431165,Partition Placement and Resource Allocation for Multiple DNN-Based Applications in Heterogeneous IoT Environments,"The evolution of the Internet of Things (IoT) has been driving the explosive growth of deep neural network (DNN)-based applications and processing demands. Hence, edge computing has emerged as a potential solution to meet these processing requirements. However, emerging IoT applications have increasingly demanded to run multiple DNNs to extract multifaceted knowledge, requiring more computational resources and increasing response time. Consequently, edge nodes cannot act as a complete substitute for the previous cloud paradigm, owing to their relatively limited resources. To address this problem, we propose to incorporate nearby IoT devices when allocating resources to multiple DNN models. Furthermore, the optimization of resource allocation can be hindered by the heterogeneity of IoT devices, which affects the delay performance of DNN-based computing. In this context, we propose a DNN partition placement and resource allocation strategy that considers different processing powers, memory, and battery levels for heterogeneous IoT devices. We evaluate the performance of the proposed strategy through extensive simulations. Simulation results reveal that the proposed strategy outperforms other existing solutions in terms of end-to-end delay, service probability, and energy consumption. The proposed solution was further simulated in a Kubernetes testbed consisting of actual devices to assess its feasibility."
pub.1139045365,A literature review on cloud based smart transport system,"since the inception of human civilization, the invention of fire and wheel has played a vital role in its gradual evolution through several decades. Precisely, after human beings have learned to cook tasty food using fire, they also faced the issue of immediate paucity of food in their vicinity. As a result, they had to travel from one location to another location in search of food, shelter and for a better lifestyle. This eternal search for a better lifestyle is still carried forward in this present generation. With gradual scientific developments, its applications have made this search more easier and convenient for us. Mainly due to the attraction of urban and comfortable lifestyles, this flow of masses directs from rural areas to urban areas, which has created huge pressure over the lifestyle and particularly the transportation system of urban areas. The economic development of any city is dependent on its business friendly environment. Researchers have applied several advanced technologies to implement the smart transportation system within and out a city to roll out urban lifestyle in full gear. In this paper authors have studied those existing research works to find scope for further contribution in the area of smart transportation system in a more integrated manner."
pub.1041395693,HIGHLIGHTS OF COMMISSION 37 SCIENCE RESULTS,"Abstract  It is widely accepted that stars do not form in isolation but result from the fragmentation of molecular clouds, which in turn leads to star cluster formation. Over time, clusters dissolve or are destroyed by interactions with molecular clouds or tidal stripping, and their members become part of the general field population. Star clusters are thus among the basic building blocks of galaxies. In turn, star cluster populations, from young associations and open clusters to old globulars, are powerful tracers of the formation, assembly, and evolutionary history of their parent galaxies. Although their importance (e.g., in mapping out the Milky Way) had been recognised for decades, major progress in this area has only become possible in recent years, both for Galactic and extragalactic cluster populations. Star clusters are the observational foundation for stellar astrophysics and evolution, provide essential tracers of galactic structure, and are unique stellar dynamical environments. Star formation, stellar structure, stellar evolution, and stellar nucleosynthesis continue to benefit and improve tremendously from the study of these systems. Additionally, fundamental quantities such as the initial mass function can be successfully derived from modelling either the Hertzsprung-Russell diagrams or the integrated velocity structures of, respectively, resolved and unresolved clusters and cluster populations. Star cluster studies thus span the fields of Galactic and extragalactic astrophysics, while heavily affecting our detailed understanding of the process of star formation in dense environments. This report highlights science results of the last decade in the major fields covered by IAU Commission 37: Star clusters and associations. Instead of focusing on the business meeting - the out-going president presentation can be found here: http://www.sc.eso.org/gcarraro/splinter2015.pdf - this legacy report contains highlights of the most important scientific achievements in the Commission science area, compiled by 5 well expert members. "
pub.1153695701,Proposed Methodology for Accuracy Improvement of LOD1 3D Building Models Created Based on Stereo Pléiades Satellite Imagery,"Three-dimensional city models play an important role for a large number of applications in urban environments, and thus it is of high interest to create them automatically, accurately and in a cost-effective manner. This paper presents a new methodology for point cloud accuracy improvement to generate terrain topographic models and 3D building modeling with the Open Geospatial Consortium (OGC) CityGML standard, level of detail 1 (LOD1), using very high-resolution (VHR) satellite images. In that context, a number of steps are given attention (which are often (in the literature) not considered in detail), including the local geoid and the role of the digital terrain model (DTM) in the dense image matching process. The quality of the resulting models is analyzed thoroughly. For this objective, two stereo Pléiades 1 satellite images over Iasi city were acquired in September 2016, and 142 points were measured in situ by global navigation satellite system real-time kinematic positioning (GNSS-RTK) technology. First, the quasigeoid surface resulting from EGG2008 regional gravimetric model was corrected based on data from GNSS and leveling measurements using a four-parameter transformation, and the ellipsoidal heights of the 142 GNSS-RTK points were corrected based on the local quasigeoid surface. The DTM of the study area was created based on low-resolution airborne laser scanner (LR ALS) point clouds that have been filtered using the robust filter algorithm and a mask for buildings, and the ellipsoidal heights were also corrected with the local quasigeoid surface, resulting in a standard deviation of 37.3 cm for 50 levelling points and 28.1 cm for the 142 GNSS-RTK points. For the point cloud generation, two scenarios were considered: (1) no DTM and ground control points (GCPs) with uncorrected ellipsoidal heights resulting in an RMS difference (Z) for the 64 GCPs and 78 ChPs of 69.8 cm and (2) with LR ALS-DTM and GCPs with corrected ellipsoidal height values resulting in an RMS difference (Z) of 60.9 cm. The LOD1 models of 1550 buildings from the Iasi city center were created based on Pléiades-DSM point clouds (corrected and not corrected) and existing building sub-footprints, with four methods for the derivation of the building roof elevations, resulting in a standard deviation of 1.6 m against high-resolution (HR) ALS point cloud in the case of the best scenario. The proposed method for height extraction and reconstruction of the city structure performed the best compared with other studies on multiple satellite stereo imagery."
pub.1145817538,"Pro ASP.NET Core 6, Develop Cloud-Ready Web Applications Using MVC, Blazor, and Razor Pages","Professional developers will produce leaner applications for the ASP.NET Core platform using the guidance in this best-selling book, now in its 9th edition and updated for ASP.NET Core for .NET 6. It contains detailed explanations of the ASP.NET Core platform and the application frameworks it supports. This cornerstone guide puts ASP.NET Core for .NET 6 into context and dives deep into the tools and techniques required to build modern, extensible web applications. New features and capabilities such as MVC, Razor Pages, Blazor Server, and Blazor WebAssembly are covered, along with demonstrations of how they are applied. ASP.NET Core for .NET 6 is the latest evolution of Microsoft’s ASP.NET web platform and provides a ""host-agnostic"" framework and a high-productivity programming model that promotes cleaner code architecture, test-driven development, and powerful extensibility. Author Adam Freeman has thoroughly revised this market-leading book and explains how toget the most from ASP.NET Core for .NET 6. He starts with the nuts-and-bolts topics, teaching you about middleware components, built-in services, request model binding, and more. As you gain knowledge and confidence, he introduces increasingly more complex topics and advanced features, including endpoint routing and dependency injection. He goes in depth to give you the knowledge you need. This book follows the same format and style as the popular previous editions but brings everything up to date for the new ASP.NET Core for .NET 6 release and broadens the focus to include all of the ASP.NET Core platform. You will appreciate the fully worked case study of a functioning ASP.NET Core application that you can use as a template for your own projects. What You Will Learn Explore the entire ASP.NET Core platform Apply the new ASP.NET Core for .NET 6 features in your developer environment See how to create RESTful web services, web applications, and client-side applications Build on your existing knowledge to get up and running with new programming models quickly and effectively This book is for web developers with a basic knowledge of web development and C# who want to incorporate the latest improvements and functionality in ASP.NET Core for .NET 6 into their own projects. Adam Freeman is an experienced IT professional who has held senior positions in a range of companies, most recently serving as chief technology officer and chief operating officer of a global bank. Now retired, he spends his time writing and long-distance running."
pub.1155005577,Approach Evaluates Geothermal Potential in Existing Oil and Gas Wells,"
                    _
                    This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 207801, “Geothermal Production From Existing Oil and Gas Wells: A Sustainable Repurposing Model,” by Oscar M. Molina, SPE, and Camilo Mejia, Enovate Upstream, and Mayank Tyagi, SPE, Louisiana State University, et al. The paper has not been peer reviewed.
                  
                  
                    _
                    In the complete paper, the authors discuss an integrated cloud-based work flow aimed at evaluating the cost-effectiveness of adopting geothermal production in low- to medium-enthalpy systems either by repurposing existing oil and gas wells or by coproducing thermal and fossil energy. The work flow introduces an automated and intrinsically secure decision-making process to convert mature oil and gas wells into geothermal wells, enabling both operational and financial assessment of the conversion process, whether partial or complete.
                  
                  
                    Background
                    The strong projection of geothermal-market growth allows oil and gas companies to consider geothermal energy coproduction from existing wells. This process considers, first, full conversion of wells in depleted hydrocarbon-bearing formations and, second, partial conversion to enable coproduction of hydrocarbons and economical volumes of water (wells with high water cut are suitable candidates for the partial-conversion approach).
                    Aside from the financial gain resulting from repurposing candidate wells for geothermal production, the conversion process yields operational cost optimization by reducing the amount of wells in need of plugging and abandonment, extending the productive life of an otherwise depleted asset, and significantly reducing carbon dioxide emissions on location.
                  
                  
                    Methodology
                    The premise of the work flow is that candidate wells for conversion are producing hydrocarbons from low- to medium-enthalpy systems. The conversion from hydrocarbon to geothermal energy producer can be full or partial, meaning that a candidate well can produce water either by itself (full conversion) or alongside oil and gas (partial conversion). The authors discuss the fundamentals of the geothermal-conversion work flow, which is based on a well-structured and physics-informed decision-making process. Operational and financial metrics pertaining to the conversion process, whether partial or complete, are assessed by the work flow.
                    The proposed work flow focuses on the reliability of the surface-to-subsurface model and the production system to ensure the financial success of the conversion project in terms of the realizable heat production and associated cost of development. The decision-making part of the work flow considers technical-, social-, and environmental"
pub.1092546928,Integrating mobile Building Information Modelling and Augmented Reality systems: An experimental study,"The benefits of Building Information Modelling (BIM) have typically been tied to its capability to support information structuring and exchange through the centralization of information. Its increasing adoption and the associated ease of data acquisition has created information intensive work environments, which can result in information overload and thus negatively impact workers task efficiency during construction. Augmented Reality (AR) has been proposed as a mechanism to enhance the process of information extraction from building information models to improve the efficiency and effectiveness of workers' tasks. Yet, there is limited research that has evaluated the effectiveness and usability of AR in this domain. This research aims to address this gap and evaluate the effectiveness of BIM and AR system integration to enhance task efficiency through improving the information retrieval process during construction. To achieve this, a design science research approach was adopted that enabled the development and performance of a mobile BIM AR system (artefact) with cloud-based storage capabilities to be tested and evaluated using a portable desktop experiment. A total of 20 participants compared existing manual information retrieval methods (control group), with information retrieval through the artefact (non-control group). The results revealed that the participants using the artefact were approximately 50% faster in completing their experiment tasks, and committed less errors, when compared to the control group. This research demonstrates that a minor modification to existing information formats (2D plans) with the inclusion of Quick Response markers can significantly improve the information retrieval process and that BIM and AR integration has the potential to enhance task efficiency."
pub.1171088649,Federated Learning for Cybersecurity in Edge and Cloud Computing,"Purpose: The article explores the integration of federated learning within edge and cloud computing frameworks to address complex cybersecurity challenges. It aims to illustrate how federated learning, by enabling collaborative model training across decentralized devices without data exchange, can serve as an effective mechanism for enhancing cybersecurity defenses. This study investigates the potential of federated learning to improve privacy-preserving data analysis and augment real-time threat detection capabilities in the context of the growing Internet of Things (IoT) ecosystem.
 Methodology: The research delves into the conceptual framework of federated learning, examining its application in cybersecurity contexts through a detailed literature review and theoretical analysis. It evaluates the benefits and limitations of federated learning in enhancing data privacy and reducing latency in threat detection. Furthermore, the article assesses the technical and security challenges of implementing federated learning, including communication overhead, model aggregation complexities, and vulnerability to model poisoning, through qualitative analysis.
 Findings:  The study finds that federated learning significantly improves privacy-preserving data analysis and enhances real-time threat detection capabilities by keeping data localized while enabling collaborative learning. However, it also identifies key challenges in deploying federated learning strategies, such as the risk of model poisoning and the complexities involved in model aggregation and communication overhead. The research highlights the need for robust mechanisms to address these challenges to fully leverage federated learning in cybersecurity.
 Unique Contribution to Theory, Policy, and Practice: This article contributes uniquely to the theoretical understanding of federated learning as a cybersecurity measure, offering a comprehensive analysis of its applications, benefits, and limitations within edge and cloud computing environments. Practically, it provides insights for cybersecurity professionals and researchers on integrating federated learning into existing cybersecurity frameworks to enhance data privacy and threat detection. The article recommends further exploration into combining federated learning with other cutting-edge technologies to develop resilient cybersecurity measures. Additionally, it suggests that policymakers should consider the implications of federated learning on data privacy regulations and cybersecurity standards. Through its thorough examination of federated learning's potential and challenges, the article offers valuable recommendations for fortifying cybersecurity frameworks in an increasingly interconnected world."
pub.1147095329,Versatile and Scalable Platform Streamlines Data Collection for Patient-Centered Studies (Preprint),"<sec>
                  BACKGROUND
                  <p>The Food and Drug Administration (FDA) Center for Biologics Evaluation and Research (CBER) established the Biologics Effectiveness and Safety (BEST) Initiative with several objectives, including to expand and enhance CBER’s access to fit-for-purpose data sources, analytics, tools, and infrastructure to improve the understanding of patient experiences with conditions related to CBER-regulated products. Due to existing challenges in data collection, especially for rare disease research, CBER recognized the need for a comprehensive platform where study coordinators can engage with study participants and design and deploy studies, while patients and/or caregivers could also enroll, consent, and securely participate in those studies. This would support data capture for clinical trials, pragmatic trials, observational studies, disease registries, and more.</p>
                </sec>
                <sec>
                  OBJECTIVE
                  <p>To describe the design and development of the Survey of Health and Patient Experience (SHAPE) Platform, and its data collection support for health research and patient experience studies.</p>
                </sec>
                <sec>
                  METHODS
                  <p>We used an Agile-based process that engaged multiple stakeholders in SHAPE’s design and development.
SHAPE is hosted in a Google Cloud environment and consists of three parts: the administrator application, participant application, and application programming interface. The administrator can build a study consisting of a set of questionnaires and self-report entries through the application. 

Once the study is deployed, the participant can access their app, consent to the study, and complete its components. 
To build SHAPE to be scalable and flexible, we leveraged the open-source software development kit, Ionic Framework. This enabled building and deploying applications across platforms, including iOS, Android, and Progressive Web Apps from a single codebase, using standardized web technologies.

SHAPE has integrated with a leading Fast Healthcare Interoperability Resources (FHIR®) API platform, 1upHealth, which allows participants to consent to a one-time data pull of their electronic health records (EHRs).</p>
                </sec>
                <sec>
                  RESULTS
                  <p>SHAPE allows study coordinators to plan, develop, and deploy their questionnaires to obtain important endpoints directly from patients or caregivers. The EHR integration enables access to the patient’s health records, which can validate and enhance the accuracy of data capture methods. The administrator can then download the study data in FHIR®-formatted JSON files.  

In this paper, we illustrate how study coordinators can use SHAPE to design patient-centered studies. We demonstrate its broad applicability through a hypothetical type 1 diabetes cohort study and an ongoing"
pub.1023145280,Industrial and Business Systems for Smart Cities,"To truly develop Smart Cities a combination of multi-media, human factors, and user-centered systems methodology and design principles will have to be applied. Large capital projects and development of Smart Cities could turn to the use of cloud, analytics, mobile, social and security solutions, which could change the outcomes of economic investments and employment opportunities. In addition, the 'Internet of Things', the interconnection of sensors, devices, and everyday objects, requires a standard platform and 'battle-tested' framework for the next generation of Smart Cities. Improved productivity, asset health, profitability, quality, employee safety, and environmental impact are the desired outcomes. Capitalizing on technology to deliver positive results and preventing 'black swan' events or accidents is a complex puzzle. Legacy infrastructure adopting new technologies, gaps in the workforce, regulatory guidelines, safety performance criteria, unexpected risks, and political challenges can add to the complexity and difficulty. We are finding ourselves in a dilemma where detailed specifications, changes and relationships among key elements in the market are needed but still are ambiguous, changing, and untraceable. In order to be successful, critical best practices in process, requirements, engineering, and risk modeling using interdisciplinary engineering practices could enable successful and rapid transformation. In response to these increasing challenges; governments, academics and industry are increasingly leveraging the systems and software engineering best practices developed in fail-safe industries such as nuclear power, aerospace, defense and capital intensive heavy industries, to aid in optimally balancing competing interests and dealing with increased complexity to deliver results. The presentation will introduce ""Systems Thinking"", ""Continuous Engineering"" and ""Internet of Things"" concepts and technologies to describe how they can be successfully leveraged in the transformation to Smart Cities. This presentation shows the need and importance of combining different points of view coming from different disciplines. This way of thinking is crucial to many areas, going beyond the Web and will in time lead to a new genre of computational social sciences that transcend specific applications. Systems Thinking or Systems Engineering differs from downstream engineering disciplines in that the outcomes for downstream engineering are implementations, while the outcomes for systems engineering are specification and governance. Systems engineering is a hybrid engineering discipline focused on the characterization of system properties, such as requirements, design, analysis, and process governance. The primary activities of systems engineering include: Identification of customer needs, Promoting engineering collaboration, Continuous validation and verification, Strategic knowledge reuse, and Systems governance throughout the life cycle. The Syste"
pub.1125540496,"Advance in a meteorological station, developed in educational environment, for agricultural and urban purposes","<p>Internet of Things (IoT) has revolutionized many fields in every-day life. It addresses many aspects related to data management, storage and connectivity.</p><p>The main objective of this project focuses on the application of IoT to a low-cost system to be used on land for monitoring plant life parameters (humidity, temperature, rain, solar radiation, etc.) in crop growing control, viticulture, pest prevention for olive groves, greenhouse automation and other applications in agriculture.</p><p>Additional applications are in urban environment (where major problems of extreme weather phenomena occur) and in the integration with existing trust networks for better characterization of weather phenomena on very limited space and time scales. Adaptation strategies must start from the knowledge and the availability of additional information.</p><p>In a previous project (EGU2018), an ArduinoUno-based control system board was utilized. The fully automatic equipment allowed transmission of real-time data using external esp8266 Wi-Fi.</p><p>In the new version, a LoLiN board, an Arduino board-compatible with integrated ESP8266 and RTC with a few Lua script lines, is used. The board allows a simplification of the design-and-development phase, and an overall reduction of costs.</p><p>The proposed system uses wireless sensors placed in open space and collects information stored on cloud server. The diffusion of a large number of sensors is possible through the use of low-cost sensors and technologies. The new target for this project is to develop a microcontroller system on Wi-Fi protocol based on ESP8266 connected in station mode for data collection, and on LoRa protocol for interconnection among multiple systems that cannot be connected with Wi-Fi.</p><p>The system has been fully developed in the University of Florence, and a high school under the supervision of teachers, involving potential stakeholders interested in the use of low-cost sensors in agriculture. Some traditional sensors, tipping bucket raingauges, magnetic reed devices anemometers, capacitive/resistive thermos-hygrometers, and an innovative impact piezo-element raingauge have been adapted in order to develop the meteorological station.</p><p>During the current year 2020, the LoRa protocol will be developed on the new system to interconnect multiple systems in the absence of Wi-Fi coverage.</p><p>Despite the low nominal cost of data collection, the current use for application in precision and smart agriculture, as well as in climate change monitoring and adaptation, could be possible only through a massive work of sensor calibration in order to reach the standards of the WMO. In any case, also in absence of absolute calibration the quantification of measurement uncertainties is mandatory to give value to the amateur network observations.</p><p>All these aspects are included in the presented project, an attempt to develop a low-cost weather monitoring system for educational purposes, but with "
pub.1105736898,Robots in Home Automation and Assistive Environments,"Elderly population in Europe is increasing, while the size of the healthcare workforce remains the same. Robotics, home automation, assistive technology and other telecare support independence of older adults and chronically ill populations can provide their daily lives with health monitoring at home and provide them with the potential to moderate the burden from healthcare institutions. Existing stand-alone technologies are now covering these needs but multiple stand-alone systems in one dwelling can be complicated for users and their caregivers. Ideally, integrated solutions providing a complete set of services would cover more needs of the users under one solution. This section explores the external environment for integrating robots in the home automation and assistive environments looking into the technological challenges and proposes a roadmap of integration drawing on the RADIO project experience."
pub.1094651539,Adding Virtual Machine Abstractions Into SimGrid,"As real systems become larger and more complex, the use of simulator frameworks grows in our research community. By leveraging them, users can focus on the major aspects of their algorithm, run in-siclo experiments (i.e., simulations), and thoroughly analyze results, even for a large-scale environment without facing the complexity of conducting in-vivo studies (i.e., on real testbeds). Since nowadays the virtual machine (VM) technology has become a fundamental building block of distributed computing environments, in particular in cloud infrastructures, our community needs a full-fledged simulation framework that enables us to investigate large-scale virtualized environments through accurate simulations. To be adopted, such a framework should provides easy-to-use APIs, close to the real ones and preferably fully compatible with those of an existing popular simulation framework. In this paper, we present the current implementation status of a highly-scalable and versatile simulation framework supporting VM environments, extending a widely-used, open-source frame-work, SimGrid. Our simulation framework allows users to launch hundreds of thousands of VMs on their simulation programs and control VMs in the same manner as in the real world (e.g., suspend/resume and migrate). Users can execute computation and communication tasks on physical machines (PMs) and VMs through the same SimGrid API, which will provide a seamless migration path to IaaS simulations for thousands of SimGrid users. Preliminary validations showed that the resource sharing mechanism of the VM support worked correctly."
pub.1157188028,A comprehensive study on cybersecurity challenges and opportunities in the IoT world,"Abstract It has become possible to link anything and everything to the Internet in recent decades due to the expanding Internet of Things (IoT). As a result, our usage of technology has changed a lot, causing digital disruption in the real world. IoT allows drones, sensors, digital set‐top boxes, surveillance cameras, wearable technology, and medical equipment to be connected to the internet. Healthcare, manufacturing, utilities, transportation, and housing are among the various sectors that has become intelligent. Recently, we have seen a surge in cybersecurity challenges and opportunities for the improvement of various IoT applications. Although cybersecurity and the IoT are extensively researched, there is a dearth of studies that exclusively focus on the evolution of cybersecurity challenges in the area of AI and machine learning, blockchain and zero trust, lightweight security, integration of IoT with 5G networks, and many more in the IoT world. The availability of environment‐capturing sensors and internet‐connected tracking devices allows for private life surveillance and cloud data transmission. Therefore, a significant problem for researchers and developers is to ensure the CIA (Confidentiality, Integrity, and Availability) security triangle for people. This paper presents a comprehensive study of cybersecurity applications, challenges, and opportunities in the IoT world. The IoT architectural layer, attacks against the IoT layer, and related issues are highlighted. Furthermore, cybersecurity issues and challenges in IoT along with the strength and weaknesses of existing techniques are discussed in detail. Our study will provide insight into various current cybersecurity research trends in the IoT world."
pub.1008046717,Futuristic Smart Architecture for a Rapid Disaster Response,"The ability to control and contain an unexpected disaster event such as a bushfire or flooding in real-time is fraught with logistic and planning challenges. Information is difficult to assimilate both from structured and unstructured data that may be collected in real-time. Unreliability of mostly unstructured data from social media and mobile devices, though extremely helpful, can make it difficult to deploy needed help/assistance in time. Ad hoc planning specifically targeted at saving lives may be severely hampered when part of the infrastructure is destroyed that is normally relied on for data collection. Also when part of the infrastructure is destroyed that normally is relied on for collecting structured data, the situation can even make it harder for ad hoc planning specifically targeted at saving lives first. In such situations, a combination of unstructured data that carries uncertainties and limited structured data from infrastructure that might still be working after/during a disaster event can be used to the best of advantages and still enhance the control process to better achieve desired outcomes: i.e. real-time event monitoring through real-time limited and high uncertainty data; filtering unstructured data through crowd sourcing, not only for reliability but sometimes for language translation as well; short-term predictions of anticipated changes from already existing interoperable simulation models, using the limited structured data from infrastructure that might be still standing following a disaster; and all this with the aim of appropriate, timely responses to saving lives in a rapidly evolving environment. A generic management framework designed to be used during a “phase transition” between pre- and post events, and characterised by the interoperability of distributed simulation models, and the collection and sharing of structured and unstructured data via cloud services and “connected devices”, is essential for the consistent provision of highly effective responses. We explore this framework from a science and innovations perspective, advocating “antifragility” for emergency response system designs. For antifragility systems, failures do not stand for a breakdown or malfunctioning of normal system functions, but rather represent the adaptations necessary to cope with the real world complexity through the management of “robustness trade-offs” as it occurs in dynamic and real-world contexts."
pub.1112034925,Towards Secure Data Flow Oriented Multi-Vendor ICT Governance Model,"Today, still, ICT Governance is being regarded as a departmental concern, not an overall organizational concern. History has shown us that implementation strategies, which are based on departments, results in fractional implementations leading to ad hoc solutions with no central control and stagnation for the in-house ICT strategy. Further, this recently has created an opinion trend; many are talking about the ICT department as being redundant, a dying out breed, which should be replaced by on-demand specialized external services. Clearly, the evermore changing surroundings do force organizations to accelerate the pace of new adaptations within their ICT plans, more vivacious than most organizations currently is able to. This leads to that ICT departments tend to be reactive rather than acting proactively and take the lead in the increased transformation pace in which organizations find themselves. Simultaneously, the monolithic systems of the 1980ies/1990ies is often very dominating in an organization, consume too much of the yearly IT budget, leaving healthy system development behind. These systems were designed before data became an organizational all-encompassing resource; the systems were designed more or less in isolation in regards to the surrounding environment. These solutions make data sharing costly and not at all optimal. Additionally, in strives to adapt to the organization’s evolution, the initial architecture has become disrupted and built up in shreds. Adding to this, on May 25, 2018, an upgraded EU Privacy Regulation on General Data Protection Regulation (GDPR) will be activated. This upgraded privacy regulation includes a substantial strengthening of 1994’s data privacy regulation, which will profoundly affect EU organizations. This regulation will, among other things, limit the right to collect and process personal data and will give the data subject all rights to his/her data sets, independentof where this data is/has been collected and by whom. Such regulation force data collecting and processingorganizations to have total control over any personal data collected and processed. This includes detailedunderstanding of data flows, including who did what and when and under who’s authorization, and how data istransported and stored. Concerning data/information flows, maps are a mandatory part of the system documentation. This encompasses all systems, including outsourced such as cloud services. Hence, individual departments cannot any longer claim they “own” data. Further, since mid-2000, we have seen aglobal inter-organizational data integration, independent of organizations, public or private. If this integration ceasesto exist, the result will be a threat to the survival of the organization. Additionally, if the organization fails to providea transparent documentation according to the GDPR, substantial economic risk is at stake. So, the discussion aboutthe ICT departments’ demise is inapt. Any organizational change will require"
pub.1095793658,A Distributed Service Framework for Integrating Robots with Internet Services,"With the rapid advance of the integration of internet and robot areas, various service platforms are proposed that assume cloud environments. For robot services, as the numbers of devices and their types increase, a mechanism that enables the entry of developers in various areas is required. However, existing platforms are insufficient to the problem. In this paper, we developed a distributed service framework that realizes the coordination of various devices, robots, and service functions based on RSNP (Robot Service Network Protocol), a protocol specification for robot services. Furthermore, we implement a pet-sitting service as a prototype system that uses a proposed framework and verify the effectiveness of the framework."
pub.1168307851,Accident reduction through a privacy-preserving method on top of a novel ontology for autonomous vehicles with the support of modular arithmetic,"Cloud of Things (CoT) emerges as a pivotal paradigm, connecting Internet of Things (IoT) devices to the Cloud Computing space, facilitating the efficient management of smart cities. In navigating the intricate landscape of smart city environments, this paper confronts two paramount challenges— heterogeneity and privacy preservation. Heterogeneity, rooted in the diverse origins of CoT devices from various vendors, intro- duces compatibility gaps and data format variations, impeding seamless communication among devices. Simultaneously, privacy preservation concerns itself with averting the inadvertent disclo- sure of sensitive data generated by CoT devices. Existing solutions often exhibit limitations in effectively addressing both challenges concurrently. To bridge this gap, our proposed solution employs a novel ontology-based approach, commencing with the introduc- tion of a groundbreaking ”Ontology” using the Protege software. This foundational tool serves a dual purpose—standardizing and unifying general and privacy-related information among diverse CoT devices. The ontology addresses the heterogeneity challenge by fostering a shared understanding and vocabulary, promoting interoperability for smoother communication among disparate devices. Complementing the ontology, a privacy-preservation method, implemented with ”MININET-WIFI” and grounded in Modular Arithmetic, dynamically adjusts the privacy-preserving rules of each CoT device. This adaptive mechanism signifi- cantly enhances security, mitigating the risk of unintentional data disclosure—a critical aspect evaluated extensively within the context of a widely used CoT application, specifically, the Autonomous Vehicle (AV) environment. The computational cost is meticulously evaluated, showcasing that our solution introduces a modest overhead, notably below 1.8 s, compared to alternative models. Furthermore, the penetration rate analysis reveals the solution's resilience against honest but curious Remote Service Units (RSUs). Communication overhead is quantified for various privacy-preserving methods, providing a comprehensive view of the solution's performance. Through rigorous simula- tions, encompassing assessments of communication overhead, computational costs, and penetration rates, our solution exhibits not only affordability for a diverse array of CoT devices in smart cities but also heightened resilience against malicious activities and adversaries, surpassing current studies. This paper, therefore, not only presents a novel ontology-based solution but also delves into the nuanced intricacies of heterogeneity and privacy preservation within CoT-based smart cities. The proposed approach, characterized by its dual focus on standardization and dynamic privacy adaptation, signifies a significant stride towards fostering secure, interoperable, and privacy-aware CoT ecosystems amid the dynamic landscape of smart cities."
pub.1118855147,Open storm: a complete framework for sensing and control of urban watersheds,"Leveraging recent advances in technologies surrounding the Internet of
Things, ""smart"" water systems are poised to transform water resources
management by enabling ubiquitous real-time sensing and control. Recent
applications have demonstrated the potential to improve flood forecasting,
enhance rainwater harvesting, and prevent combined sewer overflows. However,
adoption of smart water systems has been hindered by a limited number of proven
case studies, along with a lack of guidance on how smart water systems should
be built. To this end, we review existing solutions, and introduce open
storm---an open-source, end-to-end platform for real-time monitoring and
control of watersheds. Open storm includes (i) a robust hardware stack for
distributed sensing and control in harsh environments (ii) a cloud services
platform that enables system-level supervision and coordination of water
assets, and (iii) a comprehensive, web-based ""how-to"" guide, available on
open-storm.org, that empowers newcomers to develop and deploy their own smart
water networks. We illustrate the capabilities of the open storm platform
through two ongoing deployments: (i) a high-resolution flash-flood monitoring
network that detects and communicates flood hazards at the level of individual
roadways and (ii) a real-time stormwater control network that actively
modulates discharges from stormwater facilities to improve water quality and
reduce stream erosion. Through these case studies, we demonstrate the
real-world potential for smart water systems to enable sustainable management
of water resources."
pub.1123725992,Survey and Scan to BIM Model for the Knowledge of Built Heritage and the Management of Conservation Activities,"Surveying a historic building means to measure, to detect and to analyse its geometries, its structural elements, the connections still existing between the different parts, in order to define its state of conservation, to make structural analysis and finally to plan a proper project of conservation, consolidation and reuse. The survey represents the first necessary moment for building’s knowledge investigation. Nowadays, the wide use of tools and accurate surveying techniques makes it possible to achieve an adequate level of accuracy of information related to the buildings; BIM tools offer a great potential, in terms of both planning and evaluation of the entire knowledge and conservation process of an historical building, and in terms of its management and future maintenance. In particular, the BIM technologies allow the communication between data coming from different software, allowing a greater exchange of information between many actors. In recent years, the generative process of Building Information Modelling (BIM) oriented to the digitization of built heritage has been supported by the development of new commands modelling able to integrate the output data produced by laser scanner surveys (point clouds) in major modelling applications. Structural elements, such as vaulted historical systems, arches, decorations, architectural ornaments and wall partitions with variable cross sections, require higher levels of detail (LOD) and information (LOI) compared to the digitalization process of new buildings. Therefore, the structure of a BIM model aimed at representing existing and historical artefacts (HBIM) requires the definition of a new digital process capable of converting traditional techniques used for the management of new buildings to those suitable for creation of digital versions of historical buildings that are unique of their kind. The aim of this paper is to present the results of the ongoing researches and activities carried out on survey and HBIM model of historical buildings."
pub.1166773521,Memento: Architectural Support for Ephemeral Memory Management in Serverless Environments,"Serverless computing is an increasingly attractive paradigm in the cloud due to its ease of use and fine-grained pay-for-what-you-use billing. However, serverless computing poses new challenges to system design due to its short-lived function execution model. Our detailed analysis reveals that memory management is responsible for a major amount of function execution cycles. This is because functions pay the full critical-path costs of memory management in both userspace and the operating system without the opportunity to amortize these costs over their short lifetimes. To address this problem, we propose Memento, a new hardware-centric memory management design based upon our insights that memory allocations in serverless functions are typically small, and either quickly freed after allocation or freed when the function exits. Memento alleviates the overheads of serverless memory management by introducing two key mechanisms: (i) a hardware object allocator that performs in-cache memory allocation and free operations based on arenas, and (ii) a hardware page allocator that manages a small pool of physical pages used to replenish arenas of the object allocator. Together these mechanisms alleviate memory management overheads and bypass costly userspace and kernel operations. Memento naturally integrates with existing software stacks through a set of ISA extensions that enable seamless integration with multiple languages runtimes. Finally, Memento leverages the newly exposed memory allocation semantics in hardware to introduce a main memory bypass mechanism and avoid unnecessary DRAM accesses for newly allocated objects. We evaluate Memento with full-system simulations across a diverse set of containerized serverless workloads and language runtimes. The results show that Memento achieves function execution speedups ranging between 8–28% and 16% on average. Furthermore, Memento hardware allocators and main memory bypass mechanisms drastically reduce main memory traffic by 30% on average. The combined effects of Memento reduce the pricing cost of function execution by 29%. Finally, we demonstrate the applicability of Memento beyond functions, to major serverless platform operations and long-running data processing applications."
pub.1131402245,A high performance machine learning algorithm TspINA; scheduling multifariousness destined tasks by better efficiency,"The ability to successfully execute task scheduling for any computing system is a performance measurement in any multi-processing environment, such as working of cloud or continuous scheduling and much more. Over the years, for many cloud deployments, cost-effective resource scheduling (CERS) algorithms and dynamic and integrated resource scheduling (DAIRS) algorithms were commonly used. But these algorithms have certain drawbacks in terms of retention and efficient task response time and setting threshold values along with causing lot of overhead. Through this paper we try to put forward an algorithm based on machine learning (ML) paradigm, Task scheduling process Improved Novel Algorithm (TspINA). Our proposed algorithm explores the methods used by CERS and DAIRS algorithms and provides a reasonable enhancement by integrating machine learning features techniques for prior-learning and persistent adaptation to minimize the completion time for a defined series of objectives and tasks. Our suggested methodology would be further compared to the existing CERS and DAIRS algorithms by using generic datasets and the outcomes will be correlated with the amount of machine effort, delay and responsiveness."
pub.1092337983,An efficient scheduling multimedia transcoding method for DASH streaming in cloud environment,"As a result of technological evolution, streaming service providers have been dealing with the problem of delivery multimedia content to the diversity of devices with different resolutions. This issue can be solved by using dynamic adaptive streaming over hypertext (DASH) transfer protocol. However, a transcoding job in DASH requires a lot of computation resource which could lead to delaying the starting of multimedia streaming. Recently, new studies have addressed novel scheduling methods on video transcoding, but those research did not solve the problem entirely, such as the solution did not concern server performance or speed connection between a server and its requested users. Moreover, the load and speed connection status of the data servers is often unstable, leading to increasing the starting delay. So in this article, we solve such problem by modeling transcoding jobs in the form of an optimization problem and propose an algorithm to find an optimal schedule to transcode video source files. In which, we use moving average method to find average points for a short period to deal with server state changes. In the experiment, we implement our proposed method with DASH to demonstrate the effectiveness of the optimization scheduling method. In the system, we create several servers running on the Docker platform to simulate a cloud environment. Experimental results show that our methodology reduces the time of the transcoding process up to 30% compared to existing research."
pub.1141151193,Integrating Sensing and Communication in Cellular Networks via NR Sidelink,"RF-sensing, the analysis and interpretation of movement or
environment-induced patterns in received electromagnetic signals, has been
actively investigated for more than a decade. Since electromagnetic signals,
through cellular communication systems, are omnipresent, RF sensing has the
potential to become a universal sensing mechanism with applications in smart
home, retail, localization, gesture recognition, intrusion detection, etc.
Specifically, existing cellular network installations might be dual-used for
both communication and sensing. Such communications and sensing convergence is
envisioned for future communication networks. We propose the use of NR-sidelink
direct device-to-device communication to achieve device-initiated,flexible
sensing capabilities in beyond 5G cellular communication systems. In this
article, we specifically investigate a common issue related to sidelink-based
RF-sensing, which is its angle and rotation dependence. In particular, we
discuss transformations of mmWave point-cloud data which achieve rotational
invariance, as well as distributed processing based on such rotational
invariant inputs, at angle and distance diverse devices. To process the
distributed data, we propose a graph based encoder to capture spatio-temporal
features of the data and propose four approaches for multi-angle learning. The
approaches are compared on a newly recorded and openly available dataset
comprising 15 subjects, performing 21 gestures which are recorded from 8
angles."
pub.1164079709,Mobility-aware multi-user service placement and resource allocation in edge computing,"The rise of Mobile Edge Computing (MEC) has brought us its enormous potential and value in mobile service applications. By pushing computing and storage resources from the cloud to the network edge, it reduces transmission latency, and supports applications that require low latency, such as virtual reality and video analytics IoT services. However, many existing works only consider the problem of service placement in MEC, and the problem of computing resource allocation has received less attention. This paper focuses on the joint optimization of Service Placement and computational Resource Allocation (SPRA) in the MEC environment, with the goal of minimizing the total cost of service latency, communication latency, and service migration. For offline cases, we propose an optimal algorithm, D-SPA , based on dynamic programming and an improved algorithm, S-SPA, based on state sampling, which effectively alleviates the state explosion problem in D-SPA. In the online case, due to the unpredictability of future environmental information, we propose the online greedy algorithm OGA, and theoretically show the approximate ratio of OGA. Extensive experiments show that our algorithm is more efficient than other baseline algorithms, and it reduces the total cost by 28.6% on average."
pub.1144231988,Impact of internet of things paradigm towards energy consumption prediction: A systematic literature review,"The contribution of buildings to energy consumption (both residential and commercial) is expected to gradually increase by 2040 in developed countries globally. Energy demand is rising around the world because of population growth and increased access to power through rapid urbanisation. These increases significantly impact the environment due to the processing of electricity from fossil fuels in heavy-duty power generation plants to cater for demand. A large amount of research has been undertaken in the recent past to mitigate energy usage via the internet of things and energy consumption prediction (IoT-ECP). However, systematic reviews of IoT-ECP applications remain scarce. Therefore, this study aims to systematically review the existing literature to identify the latest trends and their technological advances, including the integration concept and solutions to problems encountered in IoT-ECP. It also highlights the advantages between cloud and edge computing in IoT-ECP integration for real-time data streaming. Additionally, IoT-ECP smart integration promotes close interaction and monitoring of energy usage. Battery storage is a major challenge to tackle energy losses along network bandwidth and live streaming data traffic. Finally, the studies indicates that many existing research in IoT-ECP are using short-term load predictions. These are domains where future research could further expand to cover medium- to long-term time frames, forecasting to better balance demand fluctuations, and provide operational reserves with renewable energies."
pub.1170821571,ETPSR: Employee Turnover Prediction with Security Recommendations,"Abstract In today’s dynamic business environment, employee turnover poses a significantchallenge for organizations due to the competitive job market. Departing employ-ees may pose cybersecurity risks by retaining access to sensitive data and criticalsystems. This necessitates a focus on data security during the offboarding process.Traditional approaches lack proactive measures, relying on manual interventionsand potentially leaving gaps in data protection. To address this, the research pro-poses integrating a Cloud Services Security Recommendation (CSSR) model intothe prediction model. Leveraging machine learning and cybersecurity principles,the CSSR model identifies likely switchers and provides personalized security rec-ommendations. The research comprises of four components: a predictive model foridentifying potential switchers, an Explainable AI approach for behavior profilingand risk assessment, and integration with existing cybersecurity infrastructure toprovide tailored security recommendations. The CSSR model offers a proactivesolution to employee turnover and data security issues, enabling organizations tofortify their cybersecurity posture and protect sensitive information. The system employed in this study, the Employee Transition Prediction and Security Recom-mendations (ETPSR), integrates advanced analytics and cybersecurity principlesto offer tailored security measures during employee offboarding, ensuring dataintegrity and confidentiality throughout the transition process."
pub.1141849528,A Machine Learning Inspired Task Scheduling Process Improved Novel Algorithm (TspINA) for Reduction in Overall System Requirements to Perform Multi-functional Tasks by Boosting Efficiency,"The ability to successfully execute task scheduling for any computing system is a performance measurement in any multi-processing environment, such as working of cloud or continuous scheduling and much more. Over the years, for many cloud deployments, cost-effective resource scheduling (CERS) algorithms and dynamic and integrated resource scheduling (DAIRS) algorithms were commonly used. But these algorithms have certain drawbacks in terms of retention and efficient task response time and setting threshold values along with causing lot of overhead. Through this paper, we try to put forward an algorithm based on machine learning (ML) paradigm, task scheduling process improved novel algorithm (TspINA). Our proposed algorithm explores the methods used by CERS and DAIRS algorithms and provides a reasonable enhancement by integrating machine learning features techniques for prior-learning and persistent adaptation to minimize the completion time for a defined series of objectives and tasks. Our suggested methodology would be further compared to the existing CERS and DAIRS algorithms by using generic datasets and the outcomes will be correlated with the amount of machine effort, delay and responsiveness."
pub.1154100919,Geospatial Data Acquisition Using Unmanned Aerial Systems (Uas): A Paradigm for Mapping the Built Environment of the Niger Delta Region of Nigeria,"The Rivers State University campus in Portharcourt is one of the university campuses in the city of Portharcourt, Nigeria covering over 21 square kilometers and housing a variety of academic, residential, administrative and other support buildings. The University Campus has seen significant transformation in recent years, including the rehabilitation of old facilities, the construction of new academic facilities and the most recent update on the creation of new collages, faculties and departments. The current view of the transformations done within the University Campus is missing from several available maps of the university. Numerous facilities have been constructed on the University Campus that are not represented on these maps as well as the qualities associated with these facilities. Existing information on the various landscapes on the map is outdated and it needs to be streamlined in light of recent changes to the University's facilities and departments. This research article aims to demonstrate the effectiveness of unmanned aerial systems (UAS) in geospatial data collection for physical planning and mapping of infrastructures at the Rivers State University Port Harcourt campus by developing a UAS-based digital map and tour guide for RSU's main campus covering all collages, faculties and departments and this offers visitors, staff and students with location and attribute information within the campus.Methodologically, Unmanned Aerial Vehicles were deployed to obtain current visible images of the campus following the growth and increasing infrastructural development. At a flying height of 76.2m (250 ft), a DJI Phantom 4 Pro UAS equipped with a 20-megapixel visible camera was flown around the campus, generating imagery with 1.69cm spatial resolution per pixel. To obtain 3D modeling capabilities, visible imagery was acquired using the flight-planning software DroneDeploy with a near nadir angle and 75 percent front and side overlap.Vertical positions were linked to the World Geodetic System 1984 and horizontal positions to the 1984 World Geodetic Datum universal transverse Mercator (UTM) (WGS 84). To match the UAS data, GCPs were transformed to UTM zone 32 north.Finally, dense point clouds, DSM, and an orthomosaic which is a geometrically corrected aerial image that provides an accurate representation of an area and can be used to determine true distances, were among the UAS-derived deliverables.Keywords; UAS, Geospatial, Acquisition, Orthophoto, Mosaic, Flying –Height."
pub.1136559664,Artificial Intelligence/Machine Learning Challenges and Evolution,"For 2018, one of the big challenges is the construction of security systems based on AI. However, it should take time and considerable resources to verify the effect of technologies involving machine learning and driving patterns. We can say that these structures are, conceptually, a computerized replica of their developers. While the trend is interesting, it does not provide any real guarantee as long as the same steps can be performed by criminals. The two armies faced in the virtual environment are in a continuous arms race, as faithful copies of their creators. Even if efforts seem to be hampered by the dynamics of the criminal spectrum, they are necessary precisely because of their mobility. Malware and ransomware attacks have targeted disparate and seemingly unrelated targets, globally. Condensing huge amounts of data at certain points or in mega-cloud spaces offers management advantages, but it can also be the premise of future offensive, not at all desirable. The confrontation engages the brightest minds on the planet, from both camps."
pub.1150459183,Telemedicine: The Computer Transformation of Healthcare,"This book provides an overview of the innovative concepts, methodologies and frameworks that will increase the feasibility of the existing telemedicine system. With the arrival of advanced technologies, telehealth has become a new subject, requiring a different understanding of IT devices and of their use, to fulfill health needs. Different topics are discussed - from the basics of TeleMedicine, to help readers understand the technology from ground up, to details about the infrastructure and communication technologies to offer deeper insights into the technology. The use of IoT and cloud services along with the use of blockchain technology in TeleMedicine are also discussed. Detailed information about the use of machine learning and computer vision techniques for the proper transmission of medical data - keeping in mind the bandwidth of the network - are provided. The book will be a readily accessible source of information for professionals working in the area of information technology as well as for the all those involved in the healthcare environment."
pub.1095007194,Virtual Machine Migration Method between Different Hypervisor Implementations and its Evaluation,"Virtualization technologies are an important building block for cloud services. Each service will run on virtual machines (VMs) deployed over different hypervisors in the future. Therefore, a VM migration method between different hypervisor implementations will be required. Existing methods, such as VM image conversion, generate dead copies of VM images during migration. This causes an operating system running on a VM to fail to boot up after migration and to identify virtual devices provided by destination hypervisors because the dead copied image does not contain requisite files for booting up the copied VM environment. To solve these problems, we propose a VM migration method that generates a destination-dependent VM image before migration. The destination dependency is a set of requisite files enabling a guest OS to boot up from the image, and is determined from the device configurations of destination physical machines and virtual ones. The proposed method extracts the dependency from the file structure that enables the guest OS to run on the destination hypervisor. Our evaluation confirms that the proposed method succeeds in VM migration between hypervisors such as VMWare, VirtualBox, and VirtualPC. As for performance efficiency, our method can reduce the size of a transferred VM image to approximately 20% that of a source VM image."
pub.1114005006,Artificial Intelligence Governed by Laws and Regulations,"While promoting human progress,
 AI has also triggered the transformation of social structure, which exerts profound influence on the existing legal system and legal concepts. While actively responding to technological progress,
 legislation also needs to get ready for potential risks brought by technical innovation. Having broken through the traditional boundary between space and time, AI has brought to the world a convenient interactive environment for development, with its transformation to nature penetrating into every respect in human life. Actually, AI has evoked a number of issues, including status definition for intelligent robots, the controversy over the copyright of the works by AI, data information security, and protection of privacy, etc. Considering that law has its stability and can’t be frequently altered while AI industry is still on rapid development, it is advisable to take the policies as major guidance for AI industry regulation and legal norms as a supplement. In addition, legislation should be put on agenda as early as possible before crisis can happen, sphere of which should include protection of intellectual property right, information application and privacy protection, as well as formulation of technical standards."
pub.1172421733,Augmenting the FedProx Algorithm by Minimizing Convergence,"The Internet of Things has experienced significant growth and has become an
integral part of various industries. This expansion has given rise to the
Industrial IoT initiative where industries are utilizing IoT technology to
enhance communication and connectivity through innovative solutions such as
data analytics and cloud computing. However this widespread adoption of IoT is
demanding of algorithms that provide better efficiency for the same training
environment without speed being a factor. In this paper we present a novel
approach called G Federated Proximity. Building upon the existing FedProx
technique our implementation introduces slight modifications to enhance its
efficiency and effectiveness. By leveraging FTL our proposed system aims to
improve the accuracy of model obtained after the training dataset with the help
of normalization techniques such that it performs better on real time devices
and heterogeneous networks Our results indicate a significant increase in the
throughput of approximately 90% better convergence compared to existing model
performance."
pub.1150812826,A web-based tourism forecasting system,"This chapter discusses the development and potential of an existing web-based tourism demand forecasting system (TDFS). The TDFS consists of a two-stage rigorous forecasting process that combines advanced statistical forecasting techniques with judgemental interventions from a panel of experts and industry decision-makers. This system’s ability to promote university–industry collaboration leads to more accurate and reliable forecasting results. When launched in 2008, the TDFS was written in Active Server Pages (ASP) and hosted on a Microsoft Internet Information Server (IIS). Subsequently, it was transferred to an Apache Tomcat Server to allow communication between the web platform and the R environment. Recent developments in information and communication technologies have increased the amount of tourism-related data available, with databases reaching the petabyte (PB) or even zettabyte (ZB) size. In the future, to exploit the opportunities created by this big data phenomenon, the system will be moved from a web-based to a cloud-based platform to support the adoption of more advanced forecasting approaches."
pub.1170011029,Revolutionizing Data Center Networks: Dynamic Load Balancing via Floodlight in SDN Environment,"In the digital era, the evolution of big data, cloud computing, Internet of Things (IoT), blockchain, and quantum computing demands a preferable networking infrastructure to handle network expansion and network usage optimally. In traditional Data Center Networks (DCNs), bundling of control and data plane in the same networking device limits its functionality for dynamic computation and storage access. The load balancing developed in traditional network infrastructure is not precise, as it is based on the local information of the network. Due to existing static routing mechanisms in traditional networks, most of the network resources are still underutilized. This dissipation of network assets is becoming common in today’s traditional typical networks. SDN emerges as a new platform that promises to control, change, and manage the inherent services of networking nodes by extracting statistics from lower layers of the network topology, facilitating network engineers and administrators. Load balancing in SDN offers a fair load share between network nodes, optimizing the best path along with bandwidth and reducing latency. SDN offers a global view of the whole network in one place, a centralized controller while helping in making satisfactory and upright decisions. In this paper, an SDN-based controller, Floodlight, is chosen for the implementation of dynamic load balancing. The Dijkstra’s algorithm is exercised in our application running on the controller. A data center network, FatTree topology of open flow switches, is deployed to depict the real-life traffic complexity in a data center network. To create a virtual topology of nodes, a Mininet emulation platform is utilized. Different load-balancing verification mechanisms validate that our load- balancing technique is doing a splendid piece of work."
pub.1173709517,Robotic Repair: In-Place 3D Printing for Repair of Building Components Using a Mobile Robot,"Through the deployment of a mobile construction robot capable of conducting high-resolution object scanning and precise in situ Additive Manufacturing (AM), we present a novel design-to-fabrication workflow for repairing existing building structures. The integration of AM techniques into context-aware mobile robotic systems enables high-precision in-place fabrication for new construction and for repair of existing structures. The benefits of transferring AM processes on-site extend in relation to tolerance handling, direct manipulation of existing structures, and removing constraints on shape stability compared to pre-fabricated elements by utilizing the context. By incorporating geometrical data obtained through 3D capture methods into the design and planning environment of architects and engineers, a direct interface between the existing building site and the planned digital geometry is created, facilitating accurate design of in-place repair or additions to existing building components. To evaluate this approach, we conducted an experiment in which a mobile robot equipped with a clay extrusion 3D printing system conceptually repaired a set of damaged brick wall segments. This workflow involved capturing the existing context with two levels of resolution: low-resolution 3D scene capture with a depth camera to generate a trajectory for high-resolution scanning, from which a dense point cloud is recorded using a 2D laser profile sensor by following the designated trajectories. This dense point cloud enables the operator to identify both the geometry of the existing brick wall, generate the missing volume, and a print path trajectory that fits the bounds of the volume while considering functional and architectural parameters. The accurate completion of the missing volume was successfully demonstrated by in-place 3D printing using clay extrusion with the mobile robotic system, showing the conceptual effectiveness of the proposed approach."
pub.1086000927,Constructing synthetic biology workflows in the cloud,"The synthetic biology design process has traditionally been heavily dependent upon manual searching, acquisition and integration of existing biological data. A large amount of such data is already available from Internet‐based resources, but data exchange between these resources is often undertaken manually. Automating the communication between different resources can be done by the generation of computational workflows to achieve complex tasks that cannot be carried out easily or efficiently by a single resource. Computational workflows involve the passage of data from one resource, or process, to another in a distributed computing environment. In a typical bioinformatics workflow, the predefined order in which processes are invoked in a synchronous fashion and are described in a workflow definition document. However, in synthetic biology the diversity of resources and manufacturing tasks required favour a more flexible model for process execution. Here, the authors present the Protocol for Linking External Nodes (POLEN), a Cloud‐based system that facilitates synthetic biology design workflows that operate asynchronously. Messages are used to notify POLEN resources of events in real time, and to log historical events such as the availability of new data, enabling networks of cooperation. POLEN can be used to coordinate the integration of different synthetic biology resources, to ensure consistency of information across distributed repositories through added support for data standards, and ultimately to facilitate the synthetic biology life cycle for designing and implementing biological systems."
pub.1141607908,Digital Field Development Planning: A Collaboration Between Technology & Process to Enable Fast and Efficient Field Development Planning,"Abstract O&G industry is facing difficult business climate with many uncertainties and challenges. Companies including National Oil Companies, NOCs have to be more efficient particularly in developing fields. The challenge is to create an environment to allow E&P companies to efficiently optimize their Field Development Plan, FDP processes and align with technology that enables integration & collaboration between different E&P domains. The environment should be agile to allow changing of circumstances while providing in-depth understanding of the risks and uncertainties involved. PETRONAS has a large portfolio of domestic and international oil & gas assets and is one of the leading NOCs in the world. With the ongoing potential of uncertainty of oil price, it is even more important to fast track field development planning while understanding the risk across domains and recognizing value from investments. PETRONAS has embarked on a digital field development pilot project called Live FDP that enriches internal existing FDP processes & tools to provide integration and generate efficiencies across multi-discipline in E&P workflows and systems that leverage on capabilities enabled by a Digital Cloud based solution. The Digital Planning Application methodology starts with Project Orchestration: Building FDPs using multi-disciplinary inputs and sensitivities followed by managing and framing via capturing an opportunity framework and concept decision. The process will then lead to generating multiple scenarios and evaluations for development options via seamless connectivity and integration with other systems in an Open Platform. At this point, process automation via connectivity of technical domain inputs to Value Based Decision Making will take place alongside Data Discovery & Benchmarks, underpinned by insights, Optimization & Advisory. The Data Analytics will then enable powerful business intelligence & analytics reporting capabilities translated into a Digital Dashboard: alignment with the UPMS process and management systems. Such systems allow project maturation to be completed fast and thus future scalability with expansion apart from Development phase to other phases such as Exploration, Drilling, Facility & Business Planning Workflows can be implemented. Based on recent internal evaluation on a pilot project in Peninsular Malaysia, by conducting Live FDP, the process efficiency in FDP evaluation scenarios was improved by up to 50% while simulation runs were shortened from 2 hrs to 20mins. On top of that, Integration & Collaboration involving benchmarking capability and via Data Ecosystem that allow cross domain collaboration between departments. This provides business continuity through data log for auditing purpose, single source of truth that leads to the increase of confidence and less uncertainties with breadth of multiple scenarios that allow techno-commercial evaluations and benchmarking with internal and external data. This paper will open"
pub.1120288874,"Solar accessibility in developing cities: A case study in Kowloon East, Hong Kong","Solar accessibility, defined as the solar irradiation received in a spatial and temporal domain, is increasingly becoming a practical demand in a variety of applications, especially in dense and high-rise urban areas where people prefer natural daylighting accommodations and offices and enterprises desire rooftops with high exposure to sun for photovoltaic cells. However, new buildings may substantial alter spatio-temporal solar distribution and obstruct exposure to solar power significantly. Thus, providing an accurate quantification of how solar accessibility is impacted by a developing urban environment is a key step in the development of sustainable cities. Motivated by this observation, a solar irradiation estimation model has been designed, which allows solar radiation from a particular elevation and azimuth to pass through urban surfaces modelled as 3D polygons, resulting in the creation of 3D shadow surfaces. As such, the urban surfaces can also be represented as 3D point clouds of irradiations determined by both solar radiation and shadow. By applying the model to existing and planned urban environment, it is possible to estimate the transformation of solar accessibility at the district scale. As a case study, a master plan for the Kowloon East district proposed by the Hong Kong government has been considered, and an application of the propose methodology found that new buildings to be built in the district can obtain considerable solar energy, while having a marginal impact on existing buildings. This case study suggests that the model can be used in many other cities for a variety proposes."
pub.1145703135,3D-RPP: a novel 3D vision-based Pose Perception Approach for Industrial Robots,"Enhanced by artificial intelligence, industrial robots are becoming more powerful, gaining a large variety of applications in intelligent factories. Pose perception, the aim of which is to obtain the joint coordinates of the robot in the camera coordinate system, has been proposed as a promising technology for multi-robot intelligent interaction. A large number of efforts have been made on studying robot pose perception. However, most of existing studies estimate the position of robot joints in 3D space by just using 2D color images, and perform pose perception by estimating the position of joint key points in the image, which could become infeasible in scenarios with various camera views and background environments. To address the issue, considering the information about the robot itself as a prior knowledge, we propose a novel approach for robot pose perception, named 3D-RPP. We adopt a 3D visual point cloud to estimate the rigid transformation of the camera coordinate system with respect to the robot base coordinate system, which effectively improves the accuracy of the obtained robot joint position in the camera coordinate system. We conduct extensive experiments on ROKAE xMate3 robot to investigate the performance of 3D-RPP, and the experimental results show that 3D-RPP could solve the pose perception problem well."
pub.1132364794,Cyber Physical Systems for Disaster Response NetworksConceptual Proposal using Never Ceasing Network,"In the face of disasters, communication between the entities becomes more diverse, and the intricacy of the disaster response also turns out to be greater. The increased demand for communication, oscillating environmental aspects, abandoned communication infrastructure, etc., leads to the development of disaster response networks, which become accustomed to altering situations during disasters. This system was developed by the connection of cyber physical systems (CPS) to networks, with the consolidation of physical and computational elements. CPS includes an always-open network and cloud to provide a constant service to users, based on their requests in disaster situations. The dynamic requests and continuously variable environment encouraged the evolution of a new technology to make an efficient and precise decision in the minimum time. At present, cloud computing has become the most broadly used system for providing virtual resources to the user. The most important process of such a system is scheduling and monitoring of resources. In the present work, task-based resource scheduling, using the Precedence Scheduling algorithm (PSA), has been proposed. Here, the resources are provisioned in accordance with precedence, which is calculated, based on the level of user interest and affinity towards a specific resource. Furthermore, the proposed algorithm is compared with existing algorithms on the basis of aspects such as time, power, throughput, scalability and reliability, resource usage, and so forth. This chapter focuses on the development of a prototype of the Never Ceasing Network (NCN) in disaster-response networks. An amalgamation of network, computation and physical processes was depicted as a cyber physical system. Disasters cause interruption to the information and communication systems which, in turn, make the system inaccessible when there is a peak demand for communication. The intention of the present work is to provide continuous services to users, according to their requests, through the Never-Ceasing Network system. NCN is built using a Cognitive Radio Network to offer communication under any disaster situations. Water level, temperature and humidity sensors were deployed to measure the flow speed and water level. Star topology is utilized for communication among sensors. The mitigation and preparation phases are organized prior to the occurrence of the disaster. In the course of the mitigation phase, information drives were presented, regarding public awareness, education, guarding and enhancing the prevailing infrastructure."
pub.1174248294,Experimental Design of Intranet Penetration for Anti Anti-Virus Traffic Characterization,"Due to the continuous evolution of modern network threats, comprehensive internal network penetration testing experiments can assist penetration testers in simulating external attacks and identifying potential threats. This, in turn, aids in the development and deployment of effective defense measures. However, the traffic characteristics generated by Frp intranet penetration are currently recognized and blocked by existing network security devices, resulting in the termination of established TCP connections. Therefore, this paper presents a complete internal network penetration testing experiment designed to address the issues of evading Frp traffic characteristics and implementing multi-layer encoding evasion for reverse shell files. The experiment involves setting up an Frp server on Alibaba Cloud and configuring a client on the Kali attack machine. Various evasion techniques are applied to Frp tools, including source code modifications, traffic encryption, and feature obfuscation. Additionally, Cobalt Strike penetration testing tools are used to disassemble and pack reverse shell files, thereby gaining control of the target machine's shell. This successful implementation of internal network penetration testing evading Frp features across different local area network environments is crucial for penetration testers to address internal network penetration challenges, allowing them to circumvent antivirus software and security protection effectively and conduct penetration testing activities seamlessly."
pub.1124266695,Smart City IoT Platform Respecting GDPR Privacy and Security Aspects,"The Internet of Things (IoT) paradigm enables computation and communication among tools that everyone uses daily. The vastness and heterogeneity of devices and their composition offer innovative services and scenarios that require a new challenging vision in interoperability, security and data management. Many IoT frameworks and platforms claimed to have solved these issues, aggregating different sources of information, combining their data flows in new innovative services, providing security robustness with respect to vulnerability and respecting the GDPR (General Data Protection Regulation) of the European Commission. Due to the potentially very sensible nature of some of these data, privacy and security aspects have to be taken into account by design and by default. In addition, an end-to-end secure solution has to guarantee a secure environment at the final users for their personal data, in transit and storage, which have to remain under their full control. In this paper, the Snap4City architecture and its security solutions that also respect the GDPR are presented. The Snap4City solution addresses the full stack security, ranging from IoT Devices, IoT Edge on premises, IoT Applications on the cloud and on premises, Data Analytics, and Dashboarding, presenting a number of integrated security solutions that go beyond the state of the art, as shown in the platform comparison. The stress test also included the adoption of penetrations tests verifying the robustness of the solution with respect to a large number of potential vulnerability aspects. The stress security assessments have been performed in a piloting period with more than 1200 registered users, thousands of processes per day, and more than 1.8 million of complex data ingested per day, in large cities such as Antwerp, Helsinki and the entire Tuscany region. Snap4City is a solution produced in response to a research challenge launched by the Select4Cities H2020 research and development project of the European Commission. Select4Cities identified a large number of requirements for modern Smart Cities that support IoT/IoE (Internet of Things/Everything) in the hands of public administrations and Living Labs, and selected a number of solutions. Consequently, at the end of the process after 3 years of work, Snap4City has been identified as the winning solution."
pub.1166428746,DEEPFAKER: A Unified Evaluation Platform for Facial Deepfake and Detection Models,"Deepfake data contains realistically manipulated faces—its abuses pose a huge threat to the security and privacy-critical applications. Intensive research from academia and industry has produced many deepfake/detection models, leading to a constant race of attack and defense. However, due to the lack of a unified evaluation platform, many critical questions on this subject remain largely unexplored. How is the anti-detection ability of the existing deepfake models? How generalizable are existing detection models against different deepfake samples? How effective are the detection APIs provided by the cloud-based vendors? How evasive and transferable are adversarial deepfakes in the lab and real-world environment? How do various factors impact the performance of deepfake and detection models?  To bridge the gap, we design and implement DEEPFAKER  1  a unified and comprehensive deepfake detection evaluation platform. Specifically, DEEPFAKER has integrated 10 state-of-the-art deepfake methods and 9 representative detection methods, while providing a user-friendly interface and modular design that allows for easy integration of new methods. Leveraging DEEPFAKER , we conduct a large-scale empirical study of facial deepfake/detection models and draw a set of key findings: (i) the detection methods have poor generalization on samples generated by different deepfake methods; (ii) there is no significant correlation between anti-detection ability and visual quality of deepfake samples; (iii) the current detection APIs have poor detection performance and adversarial deepfakes can achieve about 70% attack success rate on all cloud-based vendors, calling for an urgent need to deploy effective and robust detection APIs; (iv) the detection methods in the lab are more robust against transfer attacks than the detection APIs in the real-world environment; and (v) deepfake videos may not always be more difficult to detect after video compression. We envision that DEEPFAKER will benefit future research on facial deepfake and detection. "
pub.1143223878,Scalable edge-based hyperdimensional learning system with brain-like neural adaptation,"In the Internet of Things (IoT) domain, many applications are running machine learning algorithms to assimilate the data collected in the swarm of devices. Sending all data to the powerful computing environment, e.g., cloud, poses significant efficiency and scalability issues. A promising way is to distribute the learning tasks onto the IoT hierarchy, often referred to edge computing; however, the existing sophisticated algorithms such as deep learning are often overcomplex to run on less-powerful and unreliable embedded IoT devices. Hyperdimensional Computing (HDC) is a brain-inspired learning approach for efficient and robust learning on today's embedded devices. Encoding, or transforming the input data into high-dimensional representation, is the key first step of HDC before performing a learning task. All existing HDC approaches use a static encoder; thus, they still require very high dimensionality, resulting in significant efficiency loss for the edge devices with limited resources. In this paper, we have developed NeuralHD, a new HDC approach with a dynamic encoder for adaptive learning. Inspired by human neural regeneration study in neuroscience, NeuralHD identifies insignificant dimensions and regenerates those dimensions to enhance the learning capability and robustness. We also present a scalable learning framework to distribute NeuralHD computation over edge devices in IoT systems. Our solution enables edge devices capable of real-time learning from both labeled and unlabeled data. Our evaluation on a wide range of practical classification tasks shows that NeuralHD provides 5.7X and 6.1X (12.3X and 14.1X) faster and more energy-efficient training compared to the HD-based algorithms (DNNs) running on the same platform. NeuralHD also provides 4.2X and 11.6X higher robustness to noise in the unreliable network and hardware of IoT environments as compared to DNNs."
pub.1165316854,Novel Framework for Multi-Scale Occupancy Sensing for distributed monitoring in Internet-of-Things,"<p>Occupancy sensing is one of the integral parts of modern evolving security surveillance and monitoring system used over different types of infrastructure. With an aid of multiple form of occupancy sensors, the prime idea of occupancy sensing is to identify the presence or absence of occupants in specifically monitored area followed by transmitting back the sensing information either for storage or for prompting a set of commands from the connected control units. Review of existing schemes exhibits the presence of adoption of multiple methodologies over different variants of use-cases; however, they are quite case specific, uses expensive deployment process, and performs highly sophisticated operation. At present, there are no studies specifically reported of using multi-scale occupancy sensing suitable for large and distributed environment of Internet-of-Things (IoT). Therefore, the proposed study introduces a mechanism of novel multi-scale occupancy sensing considering a use case of smart university campus, although, it can be implemented over any form of different infrastructures too connected over IoT environment. The proposed scheme is implemented considering different types of cost-effective sensors, handheld devices and access points in order to identify the state of occupancy in large number of rooms present in the campus. The sensed data from distributed connected campus are aggregated over cloud server where they are subjected to suitable preprocessing to increase the data quality suitable for reliable prediction. Multiple set of potential learning-based schemes are integrated with proposed model to explore best fit model. This assessment scenario is not found reported in existing scheme to perform classification of states of occupancy. The study outcome shows Convolution Neural Network and Long Short-Term Memory to accomplish higher accuracy compared to other learning approach.</p>"
pub.1137670880,Optimal web service composition using hybrid optimization algorithm in cloud environment,"In recent years, service-based applications are deemed to be one of the new solutions to build an enterpriseapplication system. In order to answer the most demandingneeds or adaptations to the needs of changed servicesquickly, service composition is currently used to exploitthe multi-service capabilities in the Information Technologyorganizations. While web services, which have been independently developed, may not always be compatible with each other, the selection of optimal services and compositionof these services are seen as a challenging issue. In this paper we propose optimal web service composition (OWSC) model, we introduce an improved Dragonfly Algorithm (IDA) to compute services whose network positions are closer to each other and to the users, which ensures the QoS of web service and network. The rule search swarm searcher (RSSA) is used to obtain the desirable characteristics from composite web services for further optimal selection of service composition set in composite planner. Then, we propose optimal tree based replanning mechanism to adapt the execution plan to the actual behavior of already executed services by a dynamic service selection at runtime.Finally, the proposed OWSC model is going to develop by cloudsim toolkit and the performance is compared with the existing models similar to our contributions in terms of QoS parameters."
pub.1157652216,The Efficiency of All Crime Prevention Through Environmental Design Generations in Malaysia Housings,"Crime Prevention Through Environmental Design (CPTED) is an effective approach in reducing the crime rate in residential area which recognized by experts. This approach was introduced in 1960s where the built environment of housing was emphasized initially in first generation CPTED while social dimension was utilized in second generation CPTED. Nowadays, CPTED has improved significantly from the first generation which only emphasize on the physical features of the artificial environment to second generation which consider the social aspects such as mutual trust and sense of belongings among the residents. Since the implementation of CPTED has been long executed, this theory should evolve along with the application of internet technology today as social cohesion has weak physically. People prefer to communicate online during their convenience time. Thus, third generation CPTED which emphasizes the adoption of internet and cloud system should be considered. This study has gone through all generations of CPTED by reviewing relevant literature and intended to develop a third generation CPTED. As a result, the concept of sustainability that consists of environmental, economic, and social dimensions; and connectivity via internet should be added into CPTED on top of the existing tangible and intangible criteria arise from first and second generation CPTED. The outcome of this article can be made as the foundation of the creating of third generation CPTED which can help to reduce crime without huge initial, implementation and maintenance cost to the residents."
pub.1103989788,Security of smart manufacturing systems," A revolution in manufacturing systems is underway: substantial recent investment has been directed towards the development of smart manufacturing systems that are able to respond in real time to changes in customer demands, as well as the conditions in the supply chain and in the factory itself. Smart manufacturing is a key component of the broader thrust towards Industry 4.0, and relies on the creation of a bridge between digital and physical environments through Internet of Things (IoT) technologies, coupled with enhancements to those digital environments through greater use of cloud systems, data analytics and machine learning. Whilst these individual technologies have been in development for some time, their integration with industrial systems leads to new challenges as well as potential benefits. In this paper, we explore the challenges faced by those wishing to secure smart manufacturing systems. Lessons from history suggest that where an attempt has been made to retrofit security on systems for which the primary driver was the development of functionality, there are inevitable and costly breaches. Indeed, today's manufacturing systems have started to experience this over the past few years; however, the integration of complex smart manufacturing technologies massively increases the scope for attack from adversaries aiming at industrial espionage and sabotage. The potential outcome of these attacks ranges from economic damage and lost production, through injury and loss of life, to catastrophic nation-wide effects. In this paper, we discuss the security of existing industrial and manufacturing systems, existing vulnerabilities, potential future cyber-attacks, the weaknesses of existing measures, the levels of awareness and preparedness for future security challenges, and why security must play a key role underpinning the development of future smart manufacturing systems."
pub.1174642514,A Development of Edge Computing Method in Integration with IOT System for Optimizing and To Produce Energy Efficiency System,"The number of the Internet of Things (IoT) devices is growing, leading to increased generation of data at a level that has never been witnessed from the network edge. These pervading are through different industries: in the likes of healthcare, transportation, manufacturing, and smart cities, explicit needs call for effective data processing and analysis at the edge. This paper discusses the idea of edge computing in an IoT context, focusing on how this can best be optimized in terms of minimizing the two aforementioned areas to improve overall system performance. The traditional approach to data processing in IoT systems, which is cloud-centric,json, often experiences latency problems because data has to travel over distances to servers that are centralized for analysis. More so, the continuous transmission of voluminous raw data into the cloud is very consuming of the energy and weighs down the network bandwidth. That is where the edge computing comes into place, filling this gap through the relocation of computational tasks near the source, thereby reducing latency that translates to relieving the network from congestion. This paper tries to reflect key principles and benefits of edge computing in an IoT environment based on a comprehensive review of existing literature and relevant case studies. Near IoT devices, edge nodes enable computational capabilities."
pub.1182032946,TL-GILNS: A Trajectory-Layer-Enhanced GNSS/INS/LiDAR Integrated Approach Towards Reliable Positioning and Accurate Accuracy Quantification,"Multi-sensor integrated navigation, combining Global Navigation Satellite System (GNSS), Inertial Navigation System (INS), and Light Detection and Ranging (LiDAR), is at the forefront of high-precision positioning technology. However, existing integration methods rely on raw point cloud data, which cannot be obtained in some projects due to geospatial data privacy concerns. For this problem, this work introduces the Trajectory-Layer-Enhanced GNSS/INS/LiDAR Integrated Navigation System (TL-GILNS), which operates without raw point cloud data. This novel approach faces two primary challenges: 1) the difficulty in resolving the divergence of LiDAR positioning results due to cumulative errors, and 2) the challenge of accurately assigning weights to LiDAR data in the integrated system. To overcome these obstacles, we propose a trajectory-layer LiDAR positioning enhancement method to reduce cumulative errors and a trajectory-layer LiDAR positioning accuracy quantification method to determine the weight of LiDAR. Finally, the performance of TL-GILNS is verified through experiments conducted in semi-open and large-scale complex scenes. In these scenes, TL-GILNS achieves horizontal accuracy better than 0.3 m, vertical accuracy better than 0.7 m, and yaw angle accuracy better than 1∘. These results demonstrate the potential of TL-GILNS as a leading approach for high-precision navigation and positioning in complex environments."
pub.1166824201,Towards the decentralized coordination of multiple self-adaptive systems,"When multiple self-adaptive systems share an environment and goals, they may coordinate their adaptations to avoid conflicts and satisfy their goals. There are two approaches to coordination. (1) Logically centralized, where a supervisor has complete control over the self-adaptive systems. Such an approach is infeasible when the systems have different owners or administrative domains. (2) Logically decentralized, where coordination is achieved through direct interactions. Because the individual systems have control over the information they share, decentralized coordination accommodates multiple administrative domains. However, existing techniques do not account simultaneously for local concerns, e.g., preferences, and shared concerns, e.g., conflicts, which may lead to goals not being achieved as expected. We address this shortcoming by expressing both types of concerns within one constraint optimization problem. Our technique, CoADAPT, introduces two types of constraints: preference constraints, expressing local concerns, and consistency constraints, expressing shared concerns. At runtime, the problem is solved in a decentralized way using distributed constraint optimization algorithms. As a first step in realizing CoADAPT, we focus on the coordination of adaptation planning strategies, traditionally addressed only with centralized techniques. We show the feasibility of CoADAPT in an exemplar from cloud computing and analyze experimentally its scalability."
pub.1171853393,Navigating Challenges in Chinese Enterprise Management: A Focus on Budgeting and Performance Evaluation,"In the ever-evolving landscape of global economic integration, comprehensive budget management and performance evaluation have emerged as critical tools for enhancing enterprise efficiency and fostering development. However, Chinese enterprises face challenges in breaking through existing management bottlenecks, hindering innovation and leading to phenomena of rapid growth followed by swift decline. This paper aims to address these challenges by exploring the relationship between budget management and performance appraisal, highlighting their vital role in enterprise total value management. By aligning budgeting with strategy and operations and establishing fair performance evaluation systems, organizations can cultivate environments conducive to internal competition, fostering a corporate culture replete with entrepreneurial and innovative spirit. The study analyzes existing issues in current budget management and performance appraisal, such as imperfect systems, rigid practices, and talent shortages, proposing strategies to strengthen enterprise budget and performance management. These strategies include enhancing organizational structures, integrating resources with strategic focus, strengthening PDCA cycles, and establishing effective assessment mechanisms. By leveraging cloud platforms and big data, and actively pursuing legislative reforms, enterprises can optimize their budget management and performance evaluation systems, ultimately driving sustained growth and competitiveness in dynamic market environments."
pub.1154574881,GRAFCET Virtual Machine Enables Digital Twin and Implements PLCopen Systems,"Industrial Internet of Things (IIoT) systems challenges implementing programming logic controller (PLC) real-time technology. Firstly, the design of IIoT is fragmented and distributed. Secondly, existing PLC tools are hard to meet and dramatically complicated in emergent industries. Most of all, the deployment of IIoT is state-of-the-art engineering in complexity. A scheme to integrate the GRAFCET virtual machine (GVM) with a digital twin proposed enables IIoT systems in rapidly changing environments. They are compiling digital logic programs that support GVM at remote locations, deploying distributed manufacturing systems through IIoT communication networks, and building operational models of digital virtual and physical systems through digital twin modeling. Its behavior in dynamic operating environments is predictable. The results show that GRAFCET is not only IEC 60848 standard but also IEC 61131–3 for the PLCopen programming tool. Furthermore, a gateway of GVM performs rapid deployment of edge-cloud PLC programming. Significantly a cyber-physical system (CPS) through the digital twin improves the practicality of the digital transformation of PLC's value."
pub.1166980738,MATCHING FILTER-BASED VSLAM OPTIMIZATION IN INDOOR ENVIRONMENTS,"Abstract. An important factor that reduced the accuracy of motion trajectories in existing VSLAM (Visual Simultaneous Localization and Mapping) systems is the poor estimation of the position pose of the vision odometer. The existing methods generate many incorrect matches during the feature matching process, resulting in low computational accuracy of rotations and translations between cameras, which further leads to a reduction in the robustness of the overall system. In addition, the sparse feature point maps do not provide a detailed description of the surrounding environment, which makes it difficult for the devices equipped with VSLAM systems to perform advanced tasks such as navigation, path planning and human-computer interaction. To address the accuracy problem, we select the set of matches from existing feature matching algorithms based on the motion consistency constraint and use a random sampling consistency algorithm to obtain the best quality matches from the selected samples for computing the geometric transformation model and estimating the current pose. To address the problem of sparse map points, we use the depth information from the RGB-D or Stereo camera to build a dense map module to ensure that information about the surrounding environment is recorded as a point cloud, which provides data support for the implementation of advanced tasks of the device."
pub.1139838890,Digital project driven supply chains: a new paradigm," Purpose This paper aims to propose an integrated framework for digital project-driven supply chains (PDSC) to address multiple objectives in Architecture, Engineering, Construction and, Operations and Maintenance (AECO) value chain. Additionally, the following sub-objectives were also to be addressed: to assess emerging themes of Fourth Industrial Revolution (4IR) technologies in AECO and to identify lacunae in existing project supply chains.   Design/methodology/approach The research relies on qualitative approaches and mixed methodologies, for building theories based on domain expert interviews and questionnaire surveys administered on industry professionals. Hypothesis testing has been used to analyze data and identify significant 4IR technology applications and evolve a PDSC framework to address multiple objectives in the AECO context.   Findings 4IR technologies can completely revolutionize AECO supply chains and catapult the discipline into a completely new paradigm. The immense computing power unleashed can contribute to enhancing effectiveness in delivery. Technologies such as the Internet of Things, Internet of Services, Cloud Computing, Big Data, Smart Factory, 3 D-Printing, Cyber-Physical Systems or Embedded Systems, Augmented Reality, Virtual Reality and Robotics hold immense future potential. The study proposes an integrated framework to address the multiple objectives of improved project delivery, increased productivity and cost savings, activity monitoring, reporting and agility, better workflow processes and reduction of wastage.   Research limitations/implications The study offers ideas for complete integration of the AECO supply chain to deliver value to end customers. It, however, relies on opinions, perspectives and recollections of respondents, which is its limitation. Their opinion is expected to be influenced by their domain and project expertise.   Practical implications In today’s global environment, information and data management is a meaningful intermediary in 4IR. It can be delivered with the aid of the cloud to collect, appraise and evaluate data efficiently; faster machine operations to manufacture quality goods at a lower cost; boost productivity; and competitiveness in AECO companies. Appropriate exchange of information and knowledge transfer will lead to innovation, effective communication in terms of frequency and quality of information; willingness to share information to improve overall performance; commitment to a common goal and mutual support; and continuous innovative effort.   Originality/value This paper suggests fresh perspectives to integrated digital project-driven supply chains propelled by 4IR technologies, with a purpose to deliver multiple project objectives and end-customer value addition. "
pub.1053117074,Modelling topography with SAR interferometry: illustrations of a favourable and less favourable environment,"Radar remote sensing has faced an increasing interest in a wide variety of Earth observation studies during this last decade. This interest is notably related to the necessity to map areas where the cloud cover does not allow the use of classical optical sensors, or the increasing interest for ocean studies. Amongst the large number of radar applications different techniques exist for mapping the topography. Digital elevation models (DEMs) are required in a growing field of applications and especially in earth sciences where their use in structural geology, environmental geology or engineer geology has become essential. In this paper, we discuss the results obtained with SAR interferometry (InSAR) from two different environments and for two specific applications. The first is the study of the recent and present day tectonic evolution of the Rukwa rift (SW Tanzania) and the other, the study of the topography from the Bulusan volcano (Philippines). In the first situation, interferometry gave encouraging results. The computed InSAR DEM helped to identify unsuspected lineaments closely related to tectonic. In the Philippines, SAR interferometry failed to provide DEM because of the temporal decorrelation due to the dense vegetation and the humid climate. Radargrammetry or radar stereoscopy was tested with few different geometries, but the obtained results were evidencing the need in accurate ground control points. Discrepancies with regard to the reference computed from existing 1/50,000 topographic maps are reported to be larger where no or too few GCPs were identified."
pub.1158506529,DA-LSTM: A dynamic drift-adaptive learning framework for interval load forecasting with LSTM networks,"Load forecasting is a crucial topic in energy management systems (EMS) due to its vital role in optimizing energy scheduling and enabling more flexible and intelligent power grid systems. As a result, these systems allow power utility companies to respond promptly to demands in the electricity market. Deep learning (DL) models have been commonly employed in load forecasting problems supported by adaptation mechanisms to cope with the changing pattern of consumption by customers, known as concept drift. A drift magnitude threshold should be defined to design change detection methods to identify drifts. While the drift magnitude in load forecasting problems can vary significantly over time, existing literature often assumes a fixed drift magnitude threshold, which should be dynamically adjusted rather than fixed during system evolution. To address this gap, in this paper, we propose a dynamic drift-adaptive Long Short-Term Memory (DA-LSTM) framework that can improve the performance of load forecasting models without requiring a drift threshold setting. We integrate several strategies into the framework based on active and passive adaptation approaches. To evaluate DA-LSTM in real-life settings, we thoroughly analyze the proposed framework and deploy it in a real-world problem through a cloud-based environment. Efficiency is evaluated in terms of the prediction performance of each approach and computational cost. The experiments show performance improvements on multiple evaluation metrics achieved by our framework compared to baseline methods from the literature. Finally, we present a trade-off analysis between prediction performance and computational costs."
pub.1128046048,On Security Policy Migrations,"There has been over the past decade a rapid change towards computational environments that are comprised of large and diverse sets of devices, many of them mobile, which can connect in flexible and context-dependent ways. Examples range from networks where we can have communications between powerful cloud centers, to the myriad of simple sensor devices on the IoT. As the management of these dynamic environments becomes ever more complex, we want to propose policy migrations as a methodology to simplify the management of security policies by re-utilizing and re-deploying existing policies as the systems change. We are interested in understanding the challenges raised answering the following question: given a security policy that is being enforced in a particular source computational device, what does it entail to migrate this policy to be enforced in a different target device? Because of the differences between devices and because these devices cannot be seen in isolation but in the context where they are deployed, the meaning of the policy enforced in the source device needs to be re-interpreted and implemented in the context of the target device. The aim of the paper is to present a formal framework to evaluate the appropriateness of the migration."
pub.1158079551,DA-LSTM: A Dynamic Drift-Adaptive Learning Framework for Interval Load Forecasting with LSTM Networks,"Load forecasting is a crucial topic in energy management systems (EMS) due to
its vital role in optimizing energy scheduling and enabling more flexible and
intelligent power grid systems. As a result, these systems allow power utility
companies to respond promptly to demands in the electricity market. Deep
learning (DL) models have been commonly employed in load forecasting problems
supported by adaptation mechanisms to cope with the changing pattern of
consumption by customers, known as concept drift. A drift magnitude threshold
should be defined to design change detection methods to identify drifts. While
the drift magnitude in load forecasting problems can vary significantly over
time, existing literature often assumes a fixed drift magnitude threshold,
which should be dynamically adjusted rather than fixed during system evolution.
To address this gap, in this paper, we propose a dynamic drift-adaptive Long
Short-Term Memory (DA-LSTM) framework that can improve the performance of load
forecasting models without requiring a drift threshold setting. We integrate
several strategies into the framework based on active and passive adaptation
approaches. To evaluate DA-LSTM in real-life settings, we thoroughly analyze
the proposed framework and deploy it in a real-world problem through a
cloud-based environment. Efficiency is evaluated in terms of the prediction
performance of each approach and computational cost. The experiments show
performance improvements on multiple evaluation metrics achieved by our
framework compared to baseline methods from the literature. Finally, we present
a trade-off analysis between prediction performance and computational costs."
pub.1174529782,Automated Drilling Program through Digital Well Construction Planning Framework: A Case Study from Malaysia,"Abstract Effective well design constitutes a collaborative effort involving well engineers and diverse subject-matter experts, encompassing the entire well construction process. Traditionally, this process has relied heavily on manual data consolidation and iterative revisions at various project maturity stages. This paper elucidates the transformative impact of digitalization initiatives within well planning stages, achieved through a digital decision gate system. This system ultimately facilitated the automation of generating the Drilling Program and regulatory-required Notice of Operations (NOOP). The process of digitizing operations began with a thorough assessment that explored the evolving digital landscape in well planning. This assessment aimed to digitize the existing manual workflows, identify data analytics requirements, foster collaboration among previously isolated teams using different software systems, and facilitate data exchange. The ultimate effort was to construct a holistic digital project framework encompassing the scoping, planning, design, operation, and eventual close-out stages. A user-friendly cloud-based solution was implemented to facilitate this transition, granting stakeholders access to a web-based platform. This platform streamlined planning through digitized standardization and task automation in well engineering. Furthermore, comprehensive data management within well engineering and its open environment contributed to orchestrating multiple planning workflows and expediting the delivery of well programs. The Malaysia case study demonstrated the successful application of digital strategies in well design. The digital platform fostered an open environment that enhanced design optimization and encouraged the exchange of expertise. It seamlessly incorporated calculations and results from the company's standardized well engineering suite, preserving workflow continuity without interruptions. Additionally, this platform provided service partners with the capability to document their specialized knowledge and innovations within a unified framework. The platform's project management capabilities ensured transparent progress monitoring, aligning stakeholders, and maintaining accountability. Rigorous digital technical assurance validated and ensured well design data's accuracy, compliance, and completeness with the company's predefined standards and criteria. The automated process eliminated the need for manual intervention when updating the drilling program, maintaining data integrity and consistency throughout the entire drilling planning process. As a result, the platform effectively streamlined the well design for 14 wells over a year. It amplified the synergy across more than 12 multidisciplinary teams, ultimately yielding a 40% (70 days) reduction in design cycle time and a 60% (28 days) decrease in time required for manual report generation. The paper offers key insights for operators embarking on the digitalization"
pub.1181426186,N Optimizing Multi-Tenant DAG Execution Systems for High-Throughput Inference,"In large-scale data processing and machine learning systems, Directed Acyclic Graphs (DAGs) serve as the backbone for orchestrating complex workflows that involve multiple dependent stages. Multi-tenant DAG execution systems are increasingly being used to handle concurrent workloads from multiple users and applications. However, these systems face significant challenges when it comes to achieving high-throughput inference, particularly in shared environments where resource contention, scheduling efficiency, and tenant isolation become critical concerns. High-throughput inference is a necessity in use cases such as real-time recommendation engines, large-scale data processing pipelines, and cloud-based AI services, where latency and throughput are vital to maintaining system performance.
 This research paper aims to address the primary challenges associated with optimizing multi-tenant DAG execution systems for high-throughput inference. We begin by analyzing the limitations of existing frameworks such as Apache Airflow, Luigi, and Prefect in multi-tenant environments, focusing on issues like resource contention, inefficient scheduling, and lack of dynamic scalability. To tackle these issues, we propose a set of optimization strategies that include adaptive resource allocation, tenant-aware scheduling, and hybrid execution models that balance between real-time and batch inference.
 Our first strategy involves dynamic partitioning of resources to prevent contention and ensure fair allocation among tenants based on workload priority and expected resource utilization. This approach is supplemented by intelligent scheduling techniques that leverage cost-based heuristics and priority queues, reducing overall latency and improving system throughput. Additionally, we introduce a hybrid execution model that supports both real-time and batch processing pipelines, enabling flexible execution of diverse workload types in the same shared environment. This allows the system to dynamically switch between real-time and batch modes based on workload characteristics, thereby optimizing resource utilization.
 To further enhance performance, we propose incorporating memory-aware caching mechanisms that prioritize data locality and reduce redundant data movements between nodes in the DAG. This not only decreases execution time for individual DAG stages but also minimizes I/O overhead, a critical factor in high-throughput systems. These strategies are integrated into a multi-tenant DAG execution framework designed to support various machine learning and data analytics workloads in a cloud-native environment.
 The effectiveness of our optimizations is evaluated through comprehensive experiments using real-world datasets and synthetic benchmarks, comparing our approach against baseline systems. Our results demonstrate significant improvements in throughput, latency, and scalability, validating the proposed techniques for real-world adoption in multi-tenant DAG execution"
pub.1174648054,MPLS: A Way to Achieve Optimized Framework for Secured Transmission of Data in Various Cloud Environments,"Traditional MPLS-based recovery methods, such as link-based and one-to-one dedicated routing methods, typically use multiple resources. To address this issue, we offer a new approach called the Common Protection Path (CPP) that provides better resource management. Methods/Accounts. Unlike existing methods, CPP considers factors such as route failure probability and first return position when selecting a shared safety channel for traffic flows with identical intake and egress routers We real-world COST with link capabilities used ns-2 simulations to evaluate the efficiency of the CPP in a 239 network. Dedicated single channels, network-based systems, and modified UNIFR (RFNS) with UNIFR-like names all stood out significantly. Our findings show that CPP outperforms conventional methods in providing more efficient bandwidth protection and handling higher levels of traffic congestion. In MPLS-based recovery systems, the CPP scheme offers a significant improvement in resource utilization, with interesting implications for switching and network management."
pub.1175621457,Towards Efficient Learning on the Computing Continuum: Advancing Dynamic Adaptation of Federated Learning,"Federated Learning (FL) has emerged as a paradigm shift enabling heterogeneous clients and devices to collaborate on training a shared global model while preserving the privacy of their local data. However, a common yet impractical assumption in existing FL approaches is that the deployment environment is static, which is rarely true in heterogeneous and highly-volatile environments like the Edge-Cloud Continuum, where FL is typically executed. While most of the current FL approaches process data in an online fashion, and are therefore adaptive by nature, they only support adaptation at the ML/DL level (e.g., through continual learning to tackle data and concept drift), putting aside the effects of system variance. Moreover, the study and validation of FL approaches strongly rely on simulations, which, although informative, tends to overlook the real-world complexities and dynamics of actual deployments, in particular with respect to changing network conditions, varying client resources, and security threats. In this paper we make a first step to address these challenges. We investigate the shortcomings of traditional, static FL models and identify areas of adaptation to tackle real-life deployment challenges. We devise a set of design principles for FL systems that can smartly adjust their strategies for aggregation, communication, privacy, and security in response to changing system conditions. To illustrate the benefits envisioned by these strategies, we present the results of a set of initial experiments on a 25-node testbed. The experiments, which vary both the number of participating clients and the network conditions, show how existing FL systems are strongly affected by changes in their operational environment. Based on these insights, we propose a set of take-aways for the FL community, towards further research into FL systems that are not only accurate and scalable but also able to dynamically adapt to the real-world deployment unpredictability."
pub.1151795293,Multi-Dimensional Resource Allocation in Distributed Data Centers Using Deep Reinforcement Learning,"With the development of edge-cloud computing technologies, distributed data centers (DCs) have been extensively deployed across the global Internet. Since different users/applications have heterogeneous requirements on specific types of ICT resources in distributed DCs, how to optimize such heterogeneous resources under dynamic and even uncertain environments becomes a challenging issue. Traditional approaches are not able to provide effective solutions for multi-dimensional resource allocation that involves the balanced utilization across different resource types in distributed DC environments. This paper presents a reinforcement learning based approach for multi-dimensional resource allocation (termed as NESRL-MRM) that is able to achieve balanced utilization and availability of resources in dynamic environments. To train NESRL-MRM’s agent with sufficiently quick wall-clock time but without the loss of exploration diversity in the search space, a natural evolution strategy (NES) is employed to approximate the gradient of the reward function. To realistically evaluate the performance of NESRL-MRM, our simulation evaluations are based on real-world workload traces from Amazon EC2 and Google datacenters. Our results show that NESRL-MRM is able to achieve significant improvement over the existing approaches in balancing the utilization of multi-dimensional DC resources, which leads to substantially reduced blocking probability of future incoming workload demands."
pub.1170132257,Energy Aware Scheduling and Resource Allocation for Virtual Machine,"With the growing demand for high-performance computing on virtual machines and cloud servers, which consume more power to deliver optimal performance, the need for energy-aware scheduling and resource allocation of virtual machines has become crucial. This research investigates the impact and effectiveness of energy-aware scheduling in optimizing energy consumption and resource utilization. To improve energy efficiency and optimize the consumption of virtual machines, several strategies have been employed. These include allocating clients using different scheduling algorithms based on the workload status of the VMs, as well as implementing Live Migration along with algorithms to enhance network efficiency. These algorithms are established based on process scheduling algorithms used in operating systems, ensuring users experience seamless performance on virtual machines and cloud services. Perfor-mance metrics, including response time, throughput, and average turnaround time are measured in this research to evaluate the different approaches. The experimental results demonstrate the efficacy of the proposed techniques, which achieved significant energy savings compared to existing algorithms. We observed a 0.09% improvement in overall turnaround time compared to the static scheduling algorithm and a 9.96% improvement in overall throughput through our dynamic scheduling algorithm. These findings highlight the potential of energy-aware algorithms and techniques in optimizing energy consumption and improving resource utilization in virtual machine environments."
pub.1129429277,Adaptive Energy-Aware Algorithms to Minimize Power Consumption and SLA Violation in Cloud Computing," Objective: With the establishment of virtualized datacenters on a large scale, cuttingedge technology requires more energy to deliver the services 24*7 hours. With this expansion and accumulation of information on a massive scale on data centers, the consumption of an excessive amount of power results in high operational costs. Therefore, there is an urgent need to make the environment more adaptive and dynamic, where the overutilization and underutilization of hosts are well known to the system and active measures can be taken accordingly. To serve this purpose, an energy-efficient method, for the detection of overloaded and under-loaded hosts, has been proposed in this paper. For implementing VM migration, VM placement decision has also been taken to save energy and reduce SLA (Service Level Agreement) rate over the cloud.   Methods: In the paper, a novel adaptive heuristics approach has been presented that concerns with the utilization of resources for dynamic consolidation of VMs based on the mustered data from the usage of resources by VMs, while ensuring the high level of relevancy to the SLA. After identification of under-load and overload hosts, VM placement decision has been taken in the way that takes minimum energy consumption. A minimum migration policy has been adopted in the proposed methodology to minimize execution time. The validation of the effectiveness and efficiency of the suggested approach has been performed by using real-world workload traces in the CloudSim simulator.   Results: The results shows that the proposed methodology is ideal for SLA, but costs more for VM migration.   Conclusion: A deep analysis must be done in existing energy efficient approaches and a new platform should be suggested to save energy in real life. "
pub.1148614225,Change of Education and Training Business in the Age of 5G,"Regions are facing a huge competition, to attract companies, businesses, inhabitants, students etc. and this way to improve living and business environment, which is rapidly changing due to the impact of digitalization.On the other hand, for the point of view of the industry, the availability of skillful labor force and innovation environment are crucial factors. In this context, qualified staff has been seen to be able to utilize the opportunities of digitalization and response the needs of future skills. World Manufacturing Forum has stated on year 2019- report, that in next five years 40% of workers have to change their core competences. Through digital transformation, the use of new technologies like cloud, mobile, big data, 5G- infrastructure, platform- technology, data- analysis, and social networks with increasing intelligence and automation, enterprises can capitalize on new opportunities and optimize existing operations to achieve significant business improvement.Digitalization is going to be an important part of everyday life of citizens, and present in the working day of the average citizen and employee in the future. For that reason, also education system and education programs on all levels of education from diaper age to doctorate have been directed to fulfill this ecosystem strategy. The Fourth Industrial Revolution will bring unprecedented change to societies, education organizations and business environments. The goal of this article is to identify how education, education content, the way education is proceeded and overall whole the education business is changing. Most important is how we should respond to this inevitable co- evolution.The purpose of the study is to verify how the learning process is boosted by new digital content, new learning software and tools and customer- oriented learning environments. The change of education programs and individual education modules can be supported by applied research projects. You can use them in making proof- of- concept of new technology, new way to teach and train and through the experiences gathered change education content, way to educate and finally education business as whole.Applied research projects can be used to make proof of concept- phases on real environment field labs to test technology opportunities and new tools for training purposes.  Customer oriented applied research projects are also excellent environments for students to make assignments and use new knowledge and content and teachers to test new tools and create new ways to educate. New content and problem-based learning is used on future education modules.This research has used qualitative and conceptual analytic methodology.  The data analyzed has been collected from case study environments and transdisciplinary digital transformation projects.  Project settings have also been testing environments for new education tools and contents. In this article are introduced some case study experiences on customer oriented"
pub.1181943784,eBPF-sec: A Defensive Framework Against eBPF Attacks on Containers,"The eBPF technology allows users to efficiently extend kernel functionalities and is widely used in cloud-native environments for security control, network monitoring, and system debugging, among other purposes. However, the advent of eBPF introduces new attack surfaces for containers. Attackers can exploit eBPF features to breach container isolation mechanisms and attack the host. Existing container protection mechanisms fail to ensure resistance against potential eBPF attacks while allowing normal use of eBPF functions within containers. In response to this situation, we propose the first defense framework for eBPF attacks in containers that can automatically generate defense strategies. This framework has the following characteristics: 1) Automatic generation of defense strategies: It can generate defense strategies based on the data produced during the normal operation of containers. 2) Nonintrusive: It does not require any modifications to the kernel or container runtime. 3) Low overhead: The application of numerous defense rules only introduces negligible overhead to the container. 4) Dynamic integration: The framework can be dynamically integrated into running containers without the need for stopping or restarting them. Finally, our experiments show that our method can effectively defend against potential eBPF attacks in containers while meeting users’ requirements for eBPF functionalities, with only negligible performance loss."
pub.1181664727,Experimental Evaluation of an Internet of Things enabled Blood Pressure Prediction System using Enhanced Learning Methodology,"There are several needs and immense challenges in the existing health care conditions for which necessitates the development of a new model for blood pressure prediction in an IoT enabled environment. Many traditional models and systems do not have the accuracy or processing speed characteristics that allow real-time analysis of health information. The paper illustrates IoT-based Blood pressure prediction system with the help of Enhanced Learning Methodology (ELM) using Hybrid GoogleNet-SVM model. The system reads smart blood pressure monitors and wearable health monitors to stream in live data on key vital stats like systolic and diastolic BP, resting heart rate, and more. Using a middleware platform, the data is transmitted in real-time to a centralized server via CoAP (Constrained Application Protocol) and saved in the cloud platform Azure. Preprocessing steps like data aggregation, timestamp alignment, data cleaning, and data transformation to have an appropriate dataset from all relevant features. The hybrid GoogleNet-SVM model uses both deep learning with GoogleNet to extract complex patterns from the data and for precise blood pressure predictions leveraging the classification power of SVM. Able to deliver highly reliable real-time health advice with an accuracy of $97.56 \%$, the system allows to manage blood pressure proactively and safely. It is a powerful, scalable platform for continuous health monitoring, representing an important leap forward in the field of predictive healthcare technology."
pub.1135924280,Improved Encryption Towards Data Security in Serverless Computing,"Serverless computing is growing rapidly due to its rapid adoption by the cloud providers and tenants in terms of its scalability, elasticity, flexibility and ease of deployment. Such increase in deployment of serverless computing makes the research to rethink on its security aspects. Since, the serverless security computing may undergo problems due to malicious users or hackers. In this paper, a secure and an efficient access control system is designed for serverless security computing for both knowledge and resource sharing using attributed based encryption. Initially, the data is encrypted using user attributes; further the data is split into cipher text. It is finally decrypted using a decryption algorithm and then the shares of the cipher text are distributed in the network and the encapsulated texts are stored in the serverless system. The performance on security analysis shows that the proposed method achieves improved data security in serverless environment than the existing methods."
pub.1120401557,ANTIBIOTIC 2.0: A Fog-based Anti-Malware for Internet of Things,"The Internet of Things (IoT) has been one of the key disruptive technologies over the last few years, with its promise of optimizing and automating current manual tasks and evolving existing services. However, the increasing adoption of IoT devices both in industries and personal environments has exposed businesses and consumers to a number of security threats, such as Distributed Denial of Service (DDoS) attacks. Along the way, Fog computing was born. A novel paradigm that aims at bridging the gap between IoT and Cloud computing, providing a number of benefits, including security. In this paper, we present ANTIBIOTIC 2.0, an anti-malware that relies upon Fog computing to secure IoT devices and to overcome the main issues of its predecessor (ANTIBIOTIC 1.0). In particular, we discuss the design and implementation of the system, including possible models for deployment, security assumptions, interaction among system components, and possible modes of operation."
pub.1142540010,Digital transformation of community health and social services for ageing cohorts,"Municipalities in the European Union are ageing fast. However, achieving societal and environmental change with a focus on the development of an age-friendly environment, ambient assisted living, eHealth, public spaces that facilitate active ageing and wellbeing for all generations and social support networks for older adults with declining functional capacities represent a major challenge for European municipalities. Additionally, developing and financing smart social infrastructure to support a growing number of older adults with declining functional capacities so that they can postpone moving to a nursing home and live longer in the community is a major challenge for European municipalities. Social innovations for the digital transformation of health care and social care delivery systems can help older adults live autonomously and independently in their own communities and postpone or even avoid entering a nursing home. The innovations will lead to a more efficient combination of the existing societal resources in the communities for fulfilling the health care and social needs of the ageing members of the society who are dependent on the help of others due to illness or functional decline. On the supply side, new scientific (optimisation of the supply networks), organisational (self-managed communities) and technological innovations, such as robotics, domotics, CPS based on the Internet of Things and cloud computing, offer new utilities and create new businesses for the supply of goods and services to older people and also provide new job opportunities for young people. The aim of this paper is to consider the development and financing of community and smart social infrastructure and present the case for Slovenia."
pub.1150459528,Sant’Aniceto Castle from the Survey to the Enhancement,"The Scan to BIM method applied to the field of cultural heritage, today represents an opportunity for public administrations in order to enhance and disseminate the cultural heritage present in their territories, building models able to connect different databases and, consequently, allow a multidisciplinary approach in such a way as to guarantee the transition and the transformation from an unsustainable production system from the point of view of the use of resources, to a model that instead has its own strength in sustainability, environmental, social and economic, especially in existing buildings.The proposed method was applied to the case study of the Castle of Sant’Aniceto (often referred to as S. Niceto) located in Motta San Giovanni (RC). To obtain the 3D model of the structure under examination, it was necessary to carry out, as a first step, an investigation with drone photogrammetry that allowed the construction of a dense cloud of points and high-resolution orthophotos useful for the study of the architectural and historical analysis of the elements present within the rocky site. Subsequently, the individual objects identified using Edificius were modeled allowing to reconstruct the objects in a virtual environment that can be fully visited through virtual reality applications."
pub.1150161253,Smart Airport: Mobile Asset Information Modeling Management based on Gamificative VR Environment --- A Case Study of Ningbo Lishe International Airport Staff Restaurant,"With the popularization of Virtual Reality (VR) technology, the application of the complementary advantages of BIM+VR collaboration in facility management (FM) and asset management (AM) is an emerging research field. This theory holds that BIM information generated and acquired in the life cycle of the facility can improve the management of the facility and asset based on the visual expression of VR technology. With this proposition as a starting point, based on the Ningbo Lishe International Airport’s staff restaurant project, the purpose of this paper is to investigate the value of BIM and VR and the challenges they pose in influencing their adoption in AM applications. By establishing a BIM model and a quantitative asset information model, and using the 720 cloud VR platform and related smart devices to complete the visual expression of asset information. The results demonstrated that that the value of BIM+VR in AM comes from the improvement of the existing manual information transfer process, which reduces the data delay to a large extent. Secondly, the accuracy and accessibility of AM data are improved, and the efficiency of the management is improved. The originality of this paper is to prove the great potential of BIM and VR in AM optimization and improvement through real case study, and to provide empirical evidence for the value and challenges of BIM+VR in AM application."
pub.1165220239,GNSS/IMU/LiDAR fusion for vehicle localization in urban driving environments within a consensus framework,"The continuous, reliable, and accurate acquisition of position information is a fundamental requirement for autonomous vehicles. It underpins perception, decision-making, and planning modules in automated driving systems. However, the performance of multi-modal sensors, including the Global Navigation Satellite System (GNSS), Inertial Measurement Unit (IMU), and LiDAR, typically found in autonomous vehicles, is easily influenced by complex environmental factors encountered in urban driving conditions. This creates challenges for existing localization algorithms. More specifically, while GNSS performs accurately in open-sky areas, it is susceptible to the multi-path effect. Although the IMU-based Inertial Navigation System (INS) provides continuous position data, it suffers from cumulative errors. LiDAR-based map-matching is highly effective when sufficient point cloud features are present but fall short in plain environments. To address these individual sensor deficiencies, we propose the application of a consensus framework for GNSS/IMU/LiDAR fusion, and it is able to be expanded for multi-modal sensor fusion. Our approach seeks to take advantage of the redundancy inherent in these sensor systems. Our method begins with the derivation of the INS mechanization and its error dynamics. Subsequently, based on the Consensus Kalman Filter (CKF), we establish a GNSS measurement node and a Normal Distribution Transformation (NDT) map-matching measurement node. These are designed to estimate position errors in the INS. To counter sensor measurement failure or degradation, we introduce an adaptation mechanism for the CKF. This ensures position accuracy and continuity by normalizing measurement quality across different measurement nodes. Finally, to validate our consensus-based localization framework, we conducted extensive real-world vehicle tests in urban driving conditions. The results have confirmed the superior accuracy and robustness of our approach, even in the face of individual sensor failure. In our experiments, the proposed method reduces the localization mean absolute error by around 17% compared to INS/map-matching fusion, and around 59% compared to INS/GNSS fusion while map-matching fails. While the GNSS and map-matching measurements contain large noises, our framework achieves at least approximately a 30% improvement compared to traditional methods."
pub.1170157389,Blockchain-enabled authentication framework for Maritime Transportation System empowered by 6G-IoT,"In recent years, Maritime Transportation Systems (MTS) have experienced significant advancements through the adoption of Internet of Things (IoT) technology. The introduction of 6G mobile networks has further expanded possibilities by offering enhanced communication services and additional functionalities such as processing, caching, sensing, and control for a wide array of IoT devices. Despite these technological advancements, the integration of IoT and 6G has introduced new risks and challenges in terms of safety and reliability. The involvement of various maritime stakeholders in scheduling and managing marine transportation exacerbates these challenges. Moreover, the MTS has encountered escalating security and privacy concerns with unauthorized data access and message tampering pose significant vulnerabilities in the 6G-IoT-enabled MTS environment. Therefore, a shared and controlled access mechanism that cannot be manipulated or tampered with by unauthorized parties is also an essential requirement in MTS. To address these issues, this paper presents an authentication framework leveraging blockchain technology, specifically designed for 6G-IoT-enabled MTS. The objective is to handle real-time data using decentralized peer-to-peer cloud servers with minimal latency, all while addressing security and privacy concerns specific to MTS. This integration also serves to mitigate various security threats. The protocol’s security and privacy are verified through rigorous evaluations, including the ROR model, Scyther tool, and informal security analysis. A simulation of the proposed protocol using MIRACL is conducted, offering a comprehensive assessment of its computational costs and security features when compared to existing protocols, demonstrating its superior security and efficiency."
pub.1181327391,"Data Ingestions as a Service (DIaaS): A Unified Interface for Heterogeneous Data Ingestion, Transformation, and Metadata Management for Data Lake","Data ingestion tools are critical component of Data Lake. Existing data ingestion tools face challenges of handling large variety, formats, sources of data. There exists void for unified data ingestion interface to handle the above research problems. This study proposes an innovative and integrated framework for data ingestion in a data lake, addressing the challenges posed by heterogeneous data sources, formats, and metadata management. The framework comprises three novel modules: First Unified Data Integration Connectors (UDIC), which provide seamless connectivity and data retrieval capabilities from diverse sources including databases, data warehouses, file systems, cloud storage, and APIs; Second, Adaptive Data Variety Transformation (ADVT), a module that intelligently handles the transformation and processing of structured, semi-structured, and unstructured data types, ensuring efficient ingestion into the data lake; and third, Intelligent Metadata Management (IMM), a module that captures, stores, and manages metadata associated with the ingested data, offering advanced search, discovery, and enrichment functionalities. Comparative study corroborates features offered by the service with existing data ingestion tools to evaluate the novelty and significance of the study. Performance validation shows varying ingestion latencies across different data types: approximately 148.1 microseconds per record for structured data, 234.2 microseconds per record for semi-structured data, 65.6 microseconds per kilobyte (KB) for video data, and 42.7 microseconds per KB for image data. These results underscore the importance of considering data structure and size in optimizing ingestion processes. Overall, this research aims to revolutionize data ingestion in data lake environments by providing a unified solution for handling diverse data sources, formats, and metadata management."
pub.1151491668,Improved Temperature Monitoring and Control of Production Lines in Casting through BaSyx Framework and Edge Intelligence,"Metal production is still permeated by brown-field environments, which are continuously becoming increasingly digitized. Therefore, smart and superordinate approaches facilitating autonomous decision-support systems and plant control, while accompanying the running production, are fundamental for the modernization progress. Cyber Physical Production Systems generating a digital envelope for modular communication and optimization offer these necessary prerequisites. Still, current platforms concentrate on mostly singular aspects ranging from cloud-based access, end-to-end security, connectivity, and interoperability. This paper is dedicated to shifting analytical capabilities to edge components, avoiding high-volume data transfers, comprising the main information to smart data, and reducing latency, allowing a real-time control under improved conditions. Here, a new consistent design is applied and tested within a brown-field environment of a centrifugal casting plant. Improving the highly energy-intensive setup process before casting is tackled by utilizing existing edge-controller hardware and designing an integrated approach based on the BaSyx-framework for deploying developed process models at plant level for online control. Results show how white-box models, including density of the coating and temperature behaviour of the mould, are integrated to generate and display suitable setup strategies for the casting process."
pub.1094616567,Line-based Extrinsic Calibration of Range and Image Sensors,"Creating rich representations of environments requires integration of multiple sensing modalities with complementary characteristics such as range and imaging sensors. To precisely combine multisensory information, the rigid transformation between different sensor coordinate systems (i.e., extrinsic parameters) must be estimated. The majority of existing extrinsic calibration techniques require one or multiple planar calibration patterns (such as checkerboards) to be observed simultaneously from the range and imaging sensors. The main limitation of these approaches is that they require modifying the scene with artificial targets. In this paper, we present a novel algorithm for extrinsically calibrating a range sensor with respect to an image sensor with no requirement of external artificial targets. The proposed method exploits natural linear features in the scene to precisely determine the rigid transformation between the coordinate frames. First, a set of 3D lines (plane intersection and boundary line segments) are extracted from the point cloud, and a set of 2D line segments are extracted from the image. Correspondences between the 3D and 2D line segments are used as inputs to an optimization problem which requires jointly estimating the relative translation and rotation between the coordinate frames. The proposed method is not limited to any particular types or configurations of sensors. To demonstrate robustness, efficiency and generality of the presented algorithm, we include results using various sensor configurations."
pub.1141009930,PR Wallet-Based Blockchain Access Protocol to Secure EHRs,"With the increase in the amount of data generated, especially in the healthcare sectors, one demands a revolution in the way data and records are being handled in this sector. The patients demand to gain instant access to their health data whenever the need calls for. Many applications have promised their clients for such instant access. But the centralized systems driving the healthcare industry, which comes handy to these instant access data, pose yet another concern for privacy and security of these patients' health records. These extremely sensitive health data risks are being compromised by malicious intents and hence further attract cybercrimes. Legacy systems concerned with all the record-keeping and tracking of the patient's data are centralized and often need some third-party access to the data. These systems are also confined to an institution or an authority governing them, and hence, it becomes difficult to share this sensitive information with some external concerned legal authorities, even in the case of any emergency. Blockchain, simply defined as a distributed ledger, has the characteristics to provide a decentralized, privacy-preserving environment in which records can be searched and shared easily with the concerned authorities. Although many institutions are contributing to developing blockchain as a means to secure electronic health records (EHRs), there have been discussions on its usability, especially like how elderly people or people with certain illnesses will be able to authorize institutions. Hence, in this chapter, to address this issue, we introduce a patient record (PR) wallet and a new protocol to access the hybrid blockchain. We shall further introduce a similar application that addresses some of the common issues in this modal. This chapter discusses the patient record wallet and a new protocol to access the hybrid blockchain. One such work is MedRec, a 2016 MIT Media Lab initiative to create a blockchain-based record management system of patient's electronic health records (EHRs). Developed on the Ethereum blockchain, the system uses that only the nodes with permissions shall access the personal blockchain. Hardware wallets have been a key asset in managing private keys in blockchain, but its integration in securing EHRs can add a new layer of security to it. The authorization system proposed is built over the Ethereum public blockchain. The application is developed using truffle and Ganache that are an easy alternative to develop local Ethereum blockchains. Blockchain has the characteristics to deal with common issues such as single node failure, data integrity, intercommunication in a distributed environment, and various system vulnerabilities, pertaining to many central servers and cloud-based applications."
pub.1181645262,Implementation of IoT Based Hybrid Uninterrupted Power Supply,"The main aim of this research paper is to develop hardware along with the software system that will provide the data along with the control monitoring used in the Uninterrupted power supply. Our proposed system aims to improve the existing system by introducing the concept of hybrid integration technology based on IoT. This research work mainly focuses on solving the issues of energy availability stemming from the increasing growth of organizations and the population of individuals. The extraction of energy from the sun results in solar energy generation, energy from the flow of air in the form of wind. Nowadays, along with the power generation techniques, adding energy from different energy sources will be very difficult due to more challenges when we implement the integration process. Here our proposed system aims to develop an IoT-based Small Hybrid Uninterrupted Power Supply (HUPS). It is a small-scale power system that manages and optimizes energy output and consumption by combining energy sources and storage units. Appropriate real-time monitoring of HUPS is critical in providing reliable data that allows the users to get to know about the efficiency of the proposed system and energy loss data along with abnormalities. To improve the efficiency of solar cells, this proposal proposes using energy management of thermal dissipation. Along with the energy supplied by the Peltier Plate and Wind Turbine, the sun's radiated energy is stored as an alternate energy source. Peltier plates are designed to catch heat dissipation from the environment and transform it into electrical energy. Bismuth Telluride Semiconductor is the material used in Peltier plates. This hybrid technology improves the solar power system's efficiency by 30% when compared to the conventional UPS, the power utilization from the grid will be also reduced by 20% thereby allowing us to utilize green energy in both the presence and absence of sunlight. IoT is utilized to send data into the cloud, which makes data storage easier and allows for access at any moment during the energy generation process."
pub.1143477820,Technology Focus: Data Analytics (October 2021),"With a moderate- to low-oil-price environment being the new normal, improving process efficiency, thereby leading to hydrocarbon recovery at reduced costs, is becoming the need of the hour. The oil and gas industry generates vast amounts of data that, if properly leveraged, can generate insights that lead to recovering hydrocarbons with reduced costs, better safety records, lower costs associated with equipment downtime, and reduced environmental footprint. Data analytics and machine-learning techniques offer tremendous potential in leveraging the data.
                  An analysis of papers in OnePetro from 2014 to 2020 illustrates the steep increase in the number of machine-learning-related papers year after year. The analysis also reveals reservoir characterization, formation evaluation, and drilling as domains that have seen the highest number of papers on the application of machine-learning techniques. Reservoir characterization in particular is a field that has seen an explosion of papers on machine learning, with the use of convolutional neural networks for fault detection, seismic imaging and inversion, and the use of classical machine-learning algorithms such as random forests for lithofacies classification.
                  Formation evaluation is another area that has gained a lot of traction with applications such as the use of classical machine-learning techniques such as support vector regression to predict rock mechanical properties and the use of deep-learning techniques such as long short-term memory to predict synthetic logs in unconventional reservoirs.
                  Drilling is another domain where a tremendous amount of work has been done with papers on optimizing drilling parameters using techniques such as genetic algorithms, using automated machine-learning frameworks for bit dull grade prediction, and application of natural language processing for stuck-pipe prevention and reduction of nonproductive time.
                  As the application of machine learning toward solving various problems in the upstream oil and gas industry proliferates, explainable artificial intelligence or machine-learning interpretability becomes critical for data scientists and business decision-makers alike. Data scientists need the ability to explain machine-learning models to executives and stakeholders to verify hypotheses and build trust in the models. One of the three highlighted papers used Shapley additive explanations, which is a game-theory-based approach to explain machine-learning outputs, to provide a layer of interpretability to their machine-learning model for identification of identification of geomechanical facies along horizontal wells.
                  A cautionary note: While there is significant promise in applying these techniques, there remain many challenges in capitalizing on the data—lack of common data models in the industry, data silos, data stored in on-premises resources, slow migration of data to the cloud, "
pub.1131143667,Smart Data Collection in Mobile Edge Computing Environment,"With the digital transformation, businesses and public administrations must change the place of data in the value chain to serve all areas of the business and open up information systems. The value of the knowledge extracted from this data is directly linked to the quality of data collection. Mobile devices are particularly suitable for reporting data. They are very widespread, very suitable and can be used at any time. These characteristics mean that the use of mobile support for data collection corresponds to a paradigm shift more than a simple new additional technology compared to the panoply of existing tools. The explosion of information sharing and data, which stems from our daily by these devices is stored mostly in the cloud servers. Thus, to reduce the number of data transferred and generated by mobile devices to the cloud servers, the edge computing allows to process data at the network edge where they are generated directly reducing certain characteristics of Big Data. Big data involves the collection of complex data on the “V” dimensions which describe the quantity and type of data collected, as well as their importance and relevance to the challenges of the requester. However, the smart data goes a step further and consist to extract from the data collected only the most relevant information for the client in order to make predictions. Our results show that using an intelligent data collection process in mobile computing could generate savings in terms of data storage and analysis at the cloud level."
pub.1164649017,A Perspicacious Multi-level Defense System Against DDoS Attacks in Cloud Using Information Metric & Game Theoretical Approach,"Distributed Denial of Service (DDoS) attack poses a significant threat to the cloud environment that can impoverish the resource availability, engage the server busy, and damage the entire system within a short period. The recent DDoS attacks use clever strategies such as low-rate attacks and attacking as an authenticated user. Apparently, the current research lacks auxiliary defense components to mitigate these attacks and instead mostly relies on a single component to perform attack detection. In this work, we propose a Multi-level defense system, a timely and active lightweight mechanism to handle the above constraints. This defense system introduces a novel filtering component called filter, sniffer, and analyzer (FSA) to discard malicious packets from authenticated users. The malicious packets are further inspected using Game-theory based attack prevention algorithm and sent to the blacklist database for future reference. Here the FSA filtering and game-theory are employed as they are more efficient in the detection of low-rate attacks in specific. Along with this, the defense system also employs a decision tree classifier followed by a ϕ-entropy component to detect malicious packets. Another novelty of this work is this integration of detection, filtering, and prevention and our experiments show the efficacy of this approach. The results evaluated using the CAIDA dataset shows that our proposed multi-level defense system attains 97% detection accuracy which outperforms the existing ϕ-entropy detection system by 10% absolute gain. In addition to this, the system achieves precision of 93% and detection rate of 82% along with 0.06 FAR. This shows that our multi-level defense system is faster and efficient in detecting and preventing the low-rate and high-rate DDoS attacks as compared to existing methods such as ϕ-Entropy, Generalized Entropy (GE) and Generalized Information Distance (GID) metrics. The future direction of this research work can be further enhanced by exploring different deep learning algorithms for network traffic classification. The deployment of the defense mechanism can be extended into multiple locations using a hybrid deployment model to improve detection accuracy."
pub.1158195162,Improved Harris Hawks Optimizer with chaotic maps and opposition-based learning for task scheduling in cloud environment,"Scheduling tasks in the cloud system is the main issue that needs to be addressed in order to improve customer satisfaction and system performance. This paper proposes DCOHHOTS, a novel multi-objective task scheduling algorithm based on a modified Harris hawks optimizer. In overall, this paper has two main stages. As the first step, DCOHHO is introduced as a new version of Harris Hawks Optimizer. Using the Differential Evolution algorithm, an optimal configuration is selected from the chaotic map, the opposition-based learning, and the ratio of the population. In order to improve the performance of the Harris Hawks Optimizer, this optimal configuration is applied to initialize the hawk’s position. In the second stage, DCOHHOTS, a DCOHHO-based Task Scheduling algorithm, is proposed. Multi-objective behavior in the proposed task scheduling algorithm optimizes resource utilization to decrease the makespan, energy consumption, and execution cost. Moreover, prioritizing tasks before submitting them to the scheduler is done using the hierarchical process in the DCOHHOTS algorithm. For the purpose of investigating the performance of the proposed DCOHHO algorithm, a number of experiments are conducted using 20 standard functions and twelve algorithms. The experimental results demonstrate that the DCOHHO algorithm is superior at determining the optimal test function solutions. Additionally, makespan, execution cost, resource utilization, and energy efficiency of DCOHHOTS task scheduling algorithms are analyzed. Compared to existing algorithms, the proposed algorithm saves up to 16% energy in heavy loads. Additionally, resource utilization has increased by 17%. Compared to the conventional algorithm, the proposed algorithm reduced makepan and execution cost by 26% and 8%, respectively."
pub.1181215191,Cherry Tomato Detection for Harvesting Using Multimodal Perception and an Improved YOLOv7-Tiny Neural Network,"Robotic fruit harvesting has great potential to revolutionize agriculture, but detecting cherry tomatoes in farming environments still faces challenges in accuracy and efficiency. To overcome the shortcomings of existing cherry tomato detection methods for harvesting, this study introduces a deep-learning-based cherry tomato detection scheme for robotic harvesting in greenhouses using multimodal RGB-D perception and an improved YOLOv7-tiny Cherry Tomato Detection (YOLOv7-tiny-CTD) network, which has been modified from the original YOLOv7-tiny by eliminating the “Objectness” output layer, introducing a new “Classness” method for the prediction box, and incorporating a new hybrid non-maximum suppression. Acquired RGB-D images undergo preprocessing such as color space transformation, point cloud normal vector angle computation, and multimodal regions of interest segmentation before being fed into the YOLOv7-tiny-CTD. The proposed method was tested using an AGV-based robot in a greenhouse cherry tomato farming facility. The results indicate that the multimodal perception and deep learning method improves detection precision and accuracy over existing methods while running in real time, and the robot achieved over 80% successful picking rates in two-trial mode in the greenhouse farm, showing promising potential for practical harvesting applications."
pub.1111053643,An Efficient ABE Scheme With Verifiable Outsourced Encryption and Decryption,"Attribute-based encryption (ABE) is a promising cryptographic tool for data owner (DO) to realize fine-grained date sharing in the cloud computing. In the encryption of most existing ABE schemes, a substantial number of modular exponentiations are often required; the computational cost of it is growing linearly with the complexity of the access policy. Besides, in the most existing ABE with outsourced decryption, the computation cost of generating transformation key is growing linearly with the number of attributes associated with user private key; these computations are prohibitively high for mobile device users, which becomes a bottleneck limiting its application. To address the above issues, we propose a secure outsourcing algorithm for modular exponentiation in one single untrusted server model and a new method to generate the transformation key. Based on these techniques and Brent Waters’s ciphertext-policy ABE scheme, we propose an ABE scheme with verifiable outsourced both encryption and decryption, which can securely outsource encryption and decryption to untrusted encryption service provider (ESP) and decryption service provider (DSP), respectively, leaving only a constant number of simple operations for the DO and eligible users to perform locally. In addition, both DO and the eligible users can check the correctness of results returned from the ESP and the DSP with a probability, respectively. Finally, we provide the experimental evaluation and security analysis of our scheme, which indicates that our construction is suitable for the mobile environment."
pub.1092606329,QoS-Aware Energy and Jitter-Efficient Downlink Predictive Scheduler for Heterogeneous Traffic LTE Networks,"Energy-efficient communications have become one fundamental aspect for today's cutting-edge wireless technologies due to its valuable impact on the environment. In this paper, we augment our earlier study for the user equipment's (UE) energy efficiency (EE) in the long-term evolution (LTE) downlink by looking at real-time heterogeneous traffic QoS requirements. In particular, we utilize the previously proposed cloud radio access network (C-RAN) and ray tracing (RT)-based scheduling model to optimize both of the EE and the packet delay jitter for real-time applications with fixed packet delay budget subject to other traffic types requirements. Using the utility-based scheduling approach, we formulate the resource allocation problem as a weighted sum binary integer programming (BIP) problem. Due to the inherent complexity of the problem formulation which hinders finding its solution directly, four heuristic algorithms are proposed to solve the optimization problem. Numerical simulations are conducted on three different traffic types each belonging to one of the popular QoS classes; best-effort class, rate, and delay-constrained classes. The obtained results demonstrate a substantial improvement in the system's performance achieved by our proposed schemes compared to other existing schemes."
pub.1170010737,Cloud RAN Based Privacy Preserving Federated Cross Domain Anomaly Detection in IoT Devices Logs,"Existing deep learning models for log anomaly detection assume user logs are collected from the central server system, exposing the data collection process to the risk of leaking sensitive information. Additionally uploading enormous amounts of raw log data requires a lot of bandwidth. We propose a federated learning framework for multi-domain environment in which various participating nodes hold datasets obtained from different log domains. An embedding transformation method is utilized on the server side to learn the cross-domain embedding transformation model in order to distill the relationship of user embedding between domains. In this paper, we propose a Privacy-Preserving Federated Cross Domain Anomaly Detection (CD-FAD) technique that uses a relatively information-rich source domain to boost the detection performance of the data-sparse target domain and comprehensively analyzes all aspects of log messages including health logs, to effectively identify abnormalities arising from unusual parameter patterns. Extensive tests on real-world logs show that our suggested solution adequately preserves user privacy while achieving performance comparable to that of detection systems already in use."
pub.1169760271,3D-VLA: A 3D Vision-Language-Action Generative World Model,"Recent vision-language-action (VLA) models rely on 2D inputs, lacking
integration with the broader realm of the 3D physical world. Furthermore, they
perform action prediction by learning a direct mapping from perception to
action, neglecting the vast dynamics of the world and the relations between
actions and dynamics. In contrast, human beings are endowed with world models
that depict imagination about future scenarios to plan actions accordingly. To
this end, we propose 3D-VLA by introducing a new family of embodied foundation
models that seamlessly link 3D perception, reasoning, and action through a
generative world model. Specifically, 3D-VLA is built on top of a 3D-based
large language model (LLM), and a set of interaction tokens is introduced to
engage with the embodied environment. Furthermore, to inject generation
abilities into the model, we train a series of embodied diffusion models and
align them into the LLM for predicting the goal images and point clouds. To
train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by
extracting vast 3D-related information from existing robotics datasets. Our
experiments on held-in datasets demonstrate that 3D-VLA significantly improves
the reasoning, multimodal generation, and planning capabilities in embodied
environments, showcasing its potential in real-world applications."
pub.1068441226,ICT and farmers: lessons learned and future developments,"Information and Communication Technologies (ICT) evolution is well advancing Moore?s Law prediction of geometric progression of computer performance indexes. Indeed, these technologies are not only fast developed but, in addition, are giving birth to newer ones nicely branching existing “old fashion” ICT systems and tools. These innovations of ICT are not only regenerating traditional sciences, like Agriculture, and practices, like farming, but also, awake well neglected human sensitiveness and indifference for poverty, environmental protection, climatic deterioration issues and the future of our planet as a whole. To refer to a few examples of these innovations affecting Agriculture and Environmental Sciences: Cloud Computing provides equality in resources management and exploitability to small budget farms against the big ones. Web2 browser allows, as a platform, effective runtime environment and considerably easy access to applications by farmers lacking proper education and training. Parallel Computing brings exponentially increased core processing to low-end computers facilitating the use of huge computer power by small agricultural research units. Never the less agricultural and farming communities, in their majority, do not adopt new ICT tools and systems to the degree required for substantial agricultural development. In this paper, experience gained over the years is used to evaluate and reason poor performance in the area of applicability of ICT innovations and tools by the vast majority of farmers throughout the world."
pub.1106879480,Distributed MASON: A scalable distributed multi-agent simulation environment,"Computational Social Science (CSS) involves interdisciplinary fields and exploits computational methods, such as social network analysis as well as computer simulation with the goal of better understanding social phenomena. Agent-Based Models (ABMs) represent an effective research tool for CSS and consist of a class of models, which, aim to emulate or predict complex phenomena through a set of simple rules (i.e., independent actions, interactions and adaptation), performed by multiple agents. The efficiency and scalability of ABMs systems are typically obtained distributing the overall computation on several machines, which interact with each other in order to simulate a specific model. Unfortunately, the design of a distributed simulation model is particularly challenging, especially for domain experts who sporadically are computer scientists and are not used to developing parallel code. D-MASON framework is a distributed version of the MASON library for designing and executing ABMs in a distributed environment ensuring scalability and easiness. D-MASON enable the developer to exploit the computing power of distributed environment in a transparent manner; the developer has to do simple incremental modifications to existing MASON models, without re-designing them. This paper presents several novel features and architectural improvements introduced in the D-MASON framework: an improved space partitioning strategy, a distributed 3D field, a distributed network field, a decentralized communication layer, a novel memory consistency mechanism and the integration to cloud environments. Full documentation, additional tutorials, and other material can be found at https://github.com/isislab-unisa/dmason where the framework can be downloaded."
pub.1130619435,3RD BIM/GIS INTEGRATION WORKSHOP AND 15TH 3DGEOINFO CONFERENCE 2020 – PREFACE,"Abstract. The interest in and use of 3D models in built environments is rapidly increasing, and they are now a key component of decision-making in areas including climate change mitigation (e.g., calculating solar panel potential, flood modelling, modelling housing age for retrofitting of thermal insulation), urban planning and cadastral systems (modelling rights, restrictions and responsibilities in complex buildings, streamlining the process to issue planning permits, design of existing or new developments) and infrastructure (construction, transport, utility management and modelling, asset management). 3D models are also an integrator for the data underpinning smart cities – knowing where a sensor is in 3D space allows the data to be integrated with the surrounding context – for example, noise data could be integrated with traffic information. Reflecting this interest, national mapping and cadastral agencies (NMCA) including Ordnance Survey (GB) are now increasingly generating 3D mapping at national scale, and there is extensive research as to how this data can be integrated with another emerging source of 3D models such as building information modelling (BIM).These trends were evident during the 3rd BIM/GIS Integration Workshop and 15th 3DGeoInfo 2020 events, which were co-hosted by University College London and Ordnance Survey (GB) in September 2020. The workshop and conference brought together international researchers from academia, industry, government and national mapping and cadastral agencies in the field of 3D geoinformation, in an interdisciplinary gathering of researchers in the fields of data collection, data management, data quality, data analysis, advanced modelling approaches, applications, users, visualisation, augmented reality, artificial intelligence and many more.This year’s theme was Users and Use Cases. The workshop and conference covered a wide range of topics including 3D data acquisition and processing, 3D city modelling and related standards, visualisation and dissemination of 3D data, augmented and virtual reality, 3D and Artificial Intelligence/Machine Learning. Three sessions of the BIM/GIS Integration Workshop were dedicated to Applications of BIM/GIS Integration, and an entire day of 3DGeoInfo 2020 to Users and Use Cases within 3DGeoInfo. Additionally, two sessions were specifically aimed at NMCA participants.Although initially intended to be a face-to-face event in London, the team rapidly adjusted to the emerging COVID-19 situation, identifying an online solution that facilitated and encouraged participant interaction. This meant that the events could still provide a platform for learning, discussion, and exchange of ideas that they have been able to in previous years, as well as providing opportunities to promote international collaboration in these topics. This special issue of the ISPRS International Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences contains 22 papers selected by doub"
pub.1151295749,EXPLORE - Innovative Scientific Data Exploration and Exploitation Applications for (Planetary) Space Sciences ,"<p><strong>Introduction</strong></p>
<p>In this contribution we present the EXPLORE Horizon 2020 Research & Innovation project and the new tools and services it is providing to promote scientific exploitation of planetary and astrophysical data. EXPLORE's main objective is to develop and deploy a suite of scientific data applications (SDAs, aka Apps) for Lunar exploration and Gaia. EXPLORE is placed in a broader landscape of open science platforms and initiatives such as the European Open Science Cloud [1]. Our ambition is to support and accelerate space science exploration and exploitation with open data and open science.</p>
<p><strong>EXPLORE platform</strong></p>
<p>To support the development, testing, and demonstration of the SDAs we set-up a virtual platform, https://explore-platform.eu which, in addition to running SDAs for users, also has a space browser to easily find data from selected planetary missions (see Figures below). The final goal is to deploy the SDAs, under open-source licences, on different cloud science platforms where and when possible (e.g. ESA Datalabs [2] and ESCAPE Science Analysis Platform [3]) to stimulate uptake and sustainability of both SDAs and these platforms. The EXPLORE platform could also be used, as a bespoke (application agnostic) service, by research groups / institutions.</p>
<p><img src="""" alt="""" width=""850"" height=""476"" /></p>
<p><img src="""" alt="""" width=""851"" height=""519"" /></p>
<p><strong>Scientific Data Applications</strong></p>
<p>Within EXPLORE six scientific data applications (SDAs) produce new scientific data products and offer new scientific services. Two applications integrate data from a range of lunar missions to focus on characterisation of the Moon’s surface and potential human landing sites. The other four applications leverage data primarily from Gaia, developing tools to study the evolution of our galaxy (stellar archaeology), interstellar dust clouds, and support the discovery, classification, and characterisation of stars. The SDAs utilise state-of-the-art machine learning and visual analytics to further enhance scientific return and discovery potential of space science missions.</p>
<p><strong>Our SDAs (Apps)</strong></p>
<p>The six (initial) SDAs, from Lunar, stellar to Galactic science are summarised as follows (proto-types on https://explore-platform.eu):</p>
<ul>
<li>L-Explo is a service that aims to be complementary to existing initiatives such as Moon Trek [4], QuickMap [5], and Orbital Data Explorer [6]. We focused it on supporting large scale lunar mapping in a 2/2.5D environment. The backend access and queries the ingested higher-level lunar data through WMS standards and custom processing APIs.</li>
<li>L-Hex focusses on the smaller scale Lunar features that are of interest to local, (human and robotic) exploration, with visualisations extending to 3D. Backend is common to L-Explo. See also [9].</li>
<li>S-Phot reconstructs stellar spectral energy distributions (SEDs) by col"
pub.1099907302,Open storm: a complete framework for sensing and control of urban watersheds,"<p> Leveraging recent advances in technologies surrounding the Internet of Things , “smart” water systems are poised to transform water resources management by enabling ubiquitous real-time sensing and control. </p>
<p> Leveraging recent advances in technologies surrounding the Internet of Things , “smart” water systems are poised to transform water resources management by enabling ubiquitous real-time sensing and control. Recent applications have demonstrated the potential to improve flood forecasting, enhance rainwater harvesting, and prevent combined sewer overflows. However, adoption of smart water systems has been hindered by a limited number of proven case studies, along with a lack of guidance on how smart water systems should be built. To this end, we review existing solutions, and introduce open storm —an open-source, end-to-end platform for real-time monitoring and control of watersheds. Open storm includes (i) a robust hardware stack for distributed sensing and control in harsh environments (ii) a cloud services platform that enables system-level supervision and coordination of water assets, and (iii) a comprehensive, web-based “how-to” guide, available on open-storm.org, that empowers newcomers to develop and deploy their own smart water networks. We illustrate the capabilities of the open storm platform through two ongoing deployments: (i) a high-resolution flash-flood monitoring network that detects and communicates flood hazards at the level of individual roadways and (ii) a real-time stormwater control network that actively modulates discharges from stormwater facilities to improve water quality and reduce stream erosion. Through these case studies, we demonstrate the real-world potential for smart water systems to enable sustainable management of water resources. </p>"
pub.1000252906,AmbiSense: Identifying and Locating Objects with Ambient Sensors,"In order to simplify processes in logistics, warehousing, and surveillance, our interdisciplinary joint project AmbiSense has combined solutions for efficient acquisition and mapping of environments. These environments are equipped with diverse ambient technology such as WLAN, Bluetooth, and RFID. The research is based on techniques stemming from the fields of robotics, embedded systems, augmented reality (AR) and Enterprise Resource Planning (ERP).More precisely, we present a novel complete system for machine-aided inventory. Our system covers automatic product identification using RFID, localization based on ambient sensors, the enrichment of raw RFID data with product information from ERP backend systems and real-time augmented reality visualization.One key component of our project is the continuous integration of all developed algorithms and techniques into a real-world demonstrator to illustrate their practicability and usefulness. We have chosen warehousing and retail as our current application scenario: Robot-assisted inventory is applied in a supermarket as we expect goods to be labeled individually with RFID tags in the near future. This enables products to be tracked from production to sale consistently and to be localized permanently.In order to provide a working demonstrator, we set up an application scenario resembling a supermarket at the AmbiSense lab at the University of Tübingen. It consists of individually tagged products placed in typical shop shelves. Our robot, equipped with an RFID reader, traverses the supermarket environment while constantly detecting products within its range. The data are transmitted using WLAN to a central computer which holds a model of the current state of the system. We augment these data by additional product-specific information provided by the ERP system. The detected objects as well as additional product data are visualized using AR techniques.This scenario aims at synchronizing the product stock of supermarkets or stores automatically. Other sample tasks could be the identification of products that are past their sell-by dates or located in the wrong places.In addition the robot localizes itself using the existing infrastructure of different, cost-efficient ambient wireless sensors. To achieve the location we develop and combine novel positioning techniques using passive UHF RFID, Bluetooth, and WLAN. We thereby employ three orthogonal measuring techniques: detection rates, signal strength, and round trip time. The orthogonality of the methods is designed to achieve robustness to noise and unforeseen changes in the surroundings. Moreover, due to their different read ranges, the technologies can complement each other at different scales of the environment. An effective and cost-efficient indoor location solution can only be achieved with multiple and heterogeneous ambient sensors combined together."
pub.1072776693,The HAIL platform for big health data,"Big data analytics in health is an emerging area due to the urgent need to derive actionable intelligence from the large volumes of healthcare data to efficiently manage the healthcare system and to improve health outcomes. In this paper we present a ‘big’ healthcare data analytics platform—termed as Healthcare Analytics for Intelligence and Learning (HAIL)—that is an end-to-end healthcare data analytics solution to derive data-driven actionable intelligence and situational awareness to inform and transform health decision-making, systems management and policy development. The innovative aspects of HAIL are: (a) the integration of data-driven and knowledge-driven analytics approaches, (b) a sand-box environment for healthcare analysts to develop and test health policy/process models by exploiting a range of data preparation, analytical and visualization methods, (c) the incorporation of specialized healthcare data standards, terminologies and concept maps to support data analytics, and (d) text analytics to analyze unstructured healthcare data. The architecture of HAIL comprises the following four main modules (fig 1): (A) Health Data Integration module that entails a semantics-based metadata manager to synthesize health data originating from a range of healthcare institutions to formulate a rich contextualized data resource. The data integration is achieved through ETL workflows designed by health analysts and researchers, (B) Health Analytics Module provides a range of healthcare analytics capabilities including, (i) Exploratory Analytics using data mining to perform data clustering, classification and association tasks, (ii) Predictive Analytics to predict future trends/outcomes derived from past observations of the healthcare processes, (iii) Text Analytics to analyze unstructured texts (such as clinical notes, discharge summaries, referral notes, clinical guidelines, etc.), (iv) Simulation-based Analytics to simulate what-if questions based on simulation models, (v) Workflow analytics to interact with modeled clinical workflows to understand the affects of various confounding factors, (vi) Semantic Analytics to infer contextualized relationships, anomalies and deviations through reasoning over a semantic health data model, and (vii) Informational Analytics to present summaries, aggregations, charts and reports, (C) Data Visualization Module offers a range of interactive data visualizations, such as geospatial visualizations, causal networks, 2D and 3D graphs, pattern clusters and interactive visualizations to explore high dimensional data, (D) Data Analytics Workbench is an interactive workspace to enable data health analysts to specify and set-up their analytics process in terms of data preparation, selection and set-up of analytical methods and the selection of visualization methods. Using the workbench analysts can design sophisticated data analytics workflows/models using a range of data integration, analytical and visualization methods."
pub.1134046128,Cooperative Raw Sensor Data Fusion for Ground Truth Generation in Autonomous Driving,"Ground truth data plays an important role in validating perception algorithms and in developing data-driven models. Yet, generating ground truth data is a challenging process, often requiring tedious manual work. Thus, we present a post-processing approach to automatically generate ground truth data from environment sensors. In contrast to existing approaches, we incorporate raw sensor data from multiple vehicles. As a result, our cooperative fusion approach overcomes drawbacks of occlusions and decreasing sensor resolution with distance. To improve the alignment precision for raw sensor data fusion, we include mutual detections and match the jointly-observed static environment to support differential global positioning system localization. We further provide a new registration algorithm, where all point clouds are moved simultaneously, while restricting the transformation parameters to increase the robustness against misalignments. The benefits of our raw sensor data fusion approach are demonstrated with real lidar data from two test vehicles in different scenarios."
pub.1095618195,Interface Design in Cyber-Physical Systems-of-Systems,"Ahstract-A Cyber-Physical System-of-Systems (CPSoS) is a possibly huge information processing and energy transforming system. Its autonomous Constituent Systems (CSs), i.e., computer systems, and optionally physical systems and humans, interact in their common environment to realize often mutually beneficial emergent CPSoS services. Many key challenges in understanding, designing, and engineering such CPSoSs are related to the interactions of CSs. This paper conceptualizes these interactions of CSs, i.e., their interfaces, at three abstraction levels: cyber-physical, Itom, and service. The cyber-physical level concernes message-based interactions in cyber space, and stigmergic interactions in the physical environment. The Itom level involves the description of direct and indirect information transfers. The service level helps to tackle challenges related to the dynamicity and evolution of CPSoSs. The second part of the paper discusses design aspects of Relied Upon Interfaces (RUIs) which are the interfaces the overall emergent CPSoS service relies upon. The conceptualization of interfaces in CPSoSs and the proposed RUI design aspects are intended to simplify the modeling of CPSoSs and to facilitate their deployment with existing Internet-of-Things and cloud technologies."
pub.1175151808,Integrating AI and Blockchain for Enhanced Data Security in IoT-Driven Smart Cities,"Blockchain is recognized for its robust security features, and its integration with Internet of Things (IoT) systems presents scalability and operational challenges. Deploying Artificial Intelligence (AI) within blockchain environments raises concerns about balancing rigorous security requirements with computational efficiency. The prime motivation resides in integrating AI with blockchain to strengthen IoT security and withstand multiple variants of lethal threats. With the increasing number of IoT devices, there has also been a spontaneous increase in security vulnerabilities. While conventional security methods are inadequate for the diversification of IoT devices, adopting AI can assist in identifying and mitigating such threats in real time, whereas integrating AI with blockchain can offer more intelligent decentralized security measures. The paper contributes to a three-layered architecture encompassing the device/sensory, edge, and cloud layers. This structure supports a novel method for assessing legitimacy scores and serves as an initial security measure. The proposed scheme also enhances the architecture by introducing an Ethereum-based data repositioning framework as a potential trapdoor function, ensuring maximal secrecy. To complement this, a simplified consensus module generates a conclusive evidence matrix, bolstering accountability. The model also incorporates an innovative AI-based security optimization utilizing an unconventional neural network model that operates faster and is enhanced with metaheuristic algorithms. Comparative benchmarks demonstrate that our approach results in a 48.5% improvement in threat detection accuracy and a 23.5% reduction in processing time relative to existing systems, marking significant advancements in IoT security for smart cities."
pub.1135603816,IoT-Based Smart Agriculture in India,"Proliferation of technologies can strengthen agriculture field for assessment of agriculture-related information such as water level, productivity of crops, soil quality, and proper fertilizers as per soil type. In fact, farmers can remotely access, receive updates of weather forecasts, and monitor their land through mobiles and computers. Furthermore, there is also a need to enhance skills and knowledge of farmers in harvesting so that excessive use of pesticides and fertilizers should not affect natural ecosystems and quality of food products as well. Thus, proper awareness of important information in agriculture, such as soil forecasting and weather forecasting, is one of few common problems for nationwide Indian farmers. For transformation of traditional agriculture to smart agriculture, the Internet of things (IoT) plays a crucial role in providing information to farmers about their agriculture fields. Monitoring of environmental factors can be done with the help of IoT-based devices and related environment. This chapter initially critically analyzes, assesses, and addresses the existing problems related to agriculture, and then, integration of sensor technology and its integration with the IoT have been studied and reviewed based on real-life existing problems. Furthermore, this attempt presents a solution for sustaining soil quality in varying weather conditions so that issues related to sufficient knowledge about the soil can be optimized. This chapter discusses the comparative analysis between the developed system and the existing systems. Smart solutions to agriculture issues have been deployed in farming at various levels, and the IoT has already brought revolutionary changes in agriculture. Precise soil prediction is an essential part of precision farming and smart agriculture. Smart agriculture deploys various sensors across the farmer's field to collect data from the environment and stores it over a cloud. Smart farming solutions such as drones, automated tractors, and soil moisture monitoring can ease the problems faced by farmers in the current manual setup. Soil erosion on hill slopes, shifting cultivation, sheet erosion, ravine lands and floods, shifting sand dunes, wind erosion, and improper land management are major challenges in various agroclimatic areas of India. A major reason for failure to produce a crop good in quality and quantity is soil erosion."
pub.1173036984,LiVeR: Lightweight Vehicle Detection and Classification in Real-Time," Detection of vehicles and their classification is a significant component of wide-area monitoring and surveillance, as well as intelligent- transportation . Existing solutions tend to employ heavy-weight infrastructure and costly equipment, as well as largely depend on constant support from the cloud through round-the-clock internet connectivity and uninterrupted power supply. Moreover, existing works mainly concentrate on localized measurement and do not discuss their efficient integration to address the problem over a wide area. For practical use in an outdoor environment, apart from being technically sound and accurate, a solution also needs to be cost-effective , lightweight , easy to install , flexible , low overhead , and easily maintainable , as well as self-sufficient as much as possible. However, fulfilling all these goals together is a challenging task. In this work, we propose an IoT-assisted strategy, LiVeR , to accomplish it. For self-sufficient on-the-fly classification in resource-constrained low-power IoT devices, LiVeR minimizes not only the computational requirements but also the energy consumption, which enables sustained operation in a hostile outdoor environment for a considerably long time solely based on battery power. Through extensive studies based on outdoor measurement and trace-based simulation on empirical data, we demonstrate that LiVeR classifies vehicles of small, medium, and large size with an accuracy of 91.3% up to 98.8%, 92.3% up to 98.5%, and 93.8% up to 98.8%, respectively, for single-lane traffic. We also demonstrate that LiVeR spends only about one-third of the number of RF packets to achieve vehicle detection and classification compared to the state-of-the-art RF-based solution, considerably extending the lifetime of the system. "
pub.1108068715,"Pro Microsoft Hyper-V 2019, Practical Guidance and Hands-On Labs","Successfully create and manage your Hyper-V environment without any of the marketing fluff. This book's lab-driven, hands-on approach will get you up and running as quickly and efficiently as possible. Virtualization is the cornerstone of today’s data center. As a modern-day IT pro, you are required to manage environments that are in a regular state of flux and increasing in both size and complexity. To keep up, you need practical information in a format that is succinct, yet comprehensive and highly applicable. Pro Hyper-V 2019 breaks down critical and time-saving topics into a series of easy-to-digest chapters, showing you how to perform Hyper-V management tasks using both GUI and PowerShell-based tools. Building on your existing knowledge of Windows Server management, Active Directory, networking, and storage, experts and Microsoft MVPs Syrewicze and Siddaway begin with a foundation of why computing workloads are virtualized. This is followed by chapters covering the range of management tasks associated with virtualized environments, including: managing hosts and guest machines; networking, storage, and high availability (host and guest); disaster recovery and virtual machine migration; and monitoring. What You'll Learn: Apply practical information to administer your Hyper-V environments Understand multiple administration styles (GUI, command line, and automation) Written by IT pros for IT pros – just the information you really need without the padding Administer and use containers Utilize hands-on labs to learn about storage, networking, and high availability This book is for IT administrators tasked with implementing Hyper-V environments or migrating from VMware. IT pros joining a team that is responsible for managing Hyper-V and “lone administrators” covering the gamut in smaller organizations will also find this book indispensable. Andy Syrewicze is Technical Evangelist for Altaro Software, makers of Altaro VM Backup. In that role he does podcasts, webinars, blogging, public speaking, and serves as chief editor for all Altaro blog platforms. Prior to that, Andy spent 15 years providing technology solutions across several industry verticals. Andy is a 4-time Microsoft MVP focused on virtualization, cloud services, and the Microsoft Server Stack. Richard Siddaway has worked with Microsoft technologies for more than 25 years, predominantly in the IT sector. He has designed, built, and managed numerous enterprise-level systems, including implementation of virtualized environments for organizations. Richard is a Microsoft MVP (11 years and counting), an author, and a regular speaker on the North America and Europe speaker circuit."
pub.1137543353,The impact of binaries on the evolution of star clusters from turbulent molecular clouds,"Most of massive stars form in binary or higher-order systems in clumpy,
sub-structured clusters. In the very first phases of their life, these stars
are expected to interact with the surrounding environment, before being
released to the field when the cluster is tidally disrupted by the host galaxy.
We present a set of N-body simulations to describe the evolution of young
stellar clusters and their binary content in the first phases of their life. To
do this, we have developed a method that generates realistic initial conditions
for binary stars in star clusters from hydrodynamical simulations. We
considered different evolutionary cases to quantify the impact of binary and
stellar evolution. Also, we compared their evolution to that of King and
fractal models with different length scales. Our results indicate that the
global expansion of the cluster from hydrodynamical simulations is initially
balanced by the sub-clump motion and accelerates when a monolithic shape is
reached, as in a post-core collapse evolution. Compared to the spherical
initial conditions, the ratio of the 50% to 10% Lagrangian radius shows a very
distinctive trend, explained by the formation of a hot core of massive stars
triggered by the high initial degree of mass segregation. As for its binary
population, each cluster shows a self-regulating behaviour by creating
interacting binaries with binding energies of the order of its energy scales.
Also, in absence of original binaries, the dynamically formed binaries present
a mass dependent binary fraction, that mimics the trend of the observed one."
pub.1043407185,Use of Consolidation Technology for Meteorological Data Processing,"This paper is concerned with the technology of meteorological data consolidation. The technology is used to create one of the components of the virtual environment for forecasting dangerous convective phenomena - thunderstorms, squalls, hail and heavy rainfall. Nowadays, progress in the field of such phenomena simulation is significantly associated with the verification of the already existing models rather than with the development of the new ones [1]. Verification and adjustment of the models are very difficult due to the lack of freely available integrated data on the location and time of the observed phenomena in conjunction with meteorological data used as initial and boundary conditions for numerical models of convective clouds. In the paper we apply consolidation technology to develop the system for heterogeneous data extraction, transformation and loading to the relational database that contains the whole set of meteorological information about the state of the atmosphere at the place and at the time when a dangerous convective phenomenon is recorded. Data sources which are freely available via the Internet are used. The format of the information stored in the database does not require further decoding and can be directly used for the numerical simulation."
pub.1145951298,Application of Key Technologies of Distributed Storage Based on the Internet of Things in Urban Fire Protection,"Due to the rapid development of science and technology in the current era, fires occur more frequently, and the relationship between various economic activities and things is becoming more and more frequent. The need for real-time monitoring and remote monitoring of various firefighting facilities in buildings has become very urgent; this is a task that must be put on the agenda. The existing urban fire remote monitoring system has fewer intelligent networks, so it has high requirements for the firefighters on duty in the fire control room, which is no longer sufficient for the firefighting needs of the modern society. This paper proposes a wireless city fire remote monitoring system based on Internet of Things technology, NB-IoT technology, and cloud computing technology and studies core technologies such as designing wireless monitoring nodes at the perception layer. In the urban fire protection environment of distributed storage, several important theories, key technologies, and related algorithms are being studied in detail. After a variety of experimental verification results, the spatial data engine and adaptive spatial data model of the metadata database are developed. This provides a distributed storage virtual city geographic environment, multilevel, multiregional design and development of a simulated prototype platform and storage, management, sharing, and visualization of urban geospatial data. After technical analysis, a visualization framework was installed, which has the characteristics of global vector grid integration and distributed spatial data. Through the research on the key technology of distributed storage of the Internet of Things, this paper applies it to urban fire protection and promotes the intelligent development of urban fire protection. The application of IoT storage technology in urban fire protection can solve the shortcomings of the current urban fire remote monitoring system and automatic fire alarm system. Wireless city fire remote monitoring system, real-time collection, and transmission and storage of working status information of various fire facilities in the building are provided. The functions of real-time monitoring, real-time alarm, real-time search query, record query, maintenance management, route guidance, and user management are also realized."
pub.1136547792,MODERNIZATION OF THE TRANSPORT SYSTEM CONTROL OF THE PRODUCTION SYSTEM,"Urgency of the research. Nowadays, it is crucial to keep up with modern technologies. Therefore, this work aims to modernize the production system Festo MPS 500. Thanks to this, it will be possible to apply to the system technologies meeting the latest trends in Industry 4.0. The MPS 500 system prepared in this way can be used to research new trends in accordance with Industry 4.0. The modernized MPS 500 system will also find use in the education of students in the field of automation and mechatronics so that they are sufficiently prepared for practice. Target setting. The goal of the research was to modernize the transport system of the modular production system Festo MPS 500 according to Industry 4.0 platform. Actual scientific researches and issues analysis. When upgrading the system MPS 500 and preparing this paper, we took into account both current sources – publications and papers dealing with the current state of Industry 4.0 and modular production systems as well as existing modular production systems based on Industry 4.0 platform. Uninvestigated parts of general matters defining. At this stage of the research, data acquisition from the system MPS 500 and interconnection with the cloud was not realized. The research objective. The purpose of this article is to modernize the MPS 500, which will allow focusing on Industry 4.0 research specifically for the deployment of Cyber-physical systems, Internet of Things, Big Data, Cloud Computing. The statement of basic materials. Effective research of the new technologies in the industry requires to use modern systems which meet the criteria of Industry 4.0 platform. So the original system Festo MTS 500 was upgraded by systems from Siemens. Conclusions. The main aim of this work was to modernize the transport system of the production system MPS 500. Elements of the system management were changed, and a new control program was created in the TIA Portal environment. The functionality of the MPS 500 was subsequently verified, where the full functionality of the system was confirmed. It makes the MPS 500 ready for further expansion in accordance with Industry 4.0."
pub.1127609338,"The Response of Higher Education Institutions to Global, Regional, and National Challenges","Higher education—as many other sectors—is challenged by the dynamics of the global and local socio-economic influencing our lives, economy, environment, and lifestyle. For instance, currently existing jobs and skills are expected to be replaced by super-fast artificial intelligent cloud-based employees. Traditional higher education system and institutions eagerly pursuing transformation to cope with the current and future demands in skills, teaching and learning, research, technology, funds amongst other demanding factors of our world. This paper showcase the University of Bahrain and present how the University is transforming to address the global, regional, and national challenges it is facing today. This Paper describes the key pillars and the key performance indicators of the Transformation Plan 2016–2021 of the University of Bahrain to respond to those challenges. The Transformation Plan is inspired by the Bahrain Economic Vision 2030. Furthermore, the Plan is aligned to the National Higher Education Strategy 2014–2024, and the National Research Strategy 2014–2024. The Transformation Plan also is in alignment with the national endeavors to achieve the United Nations Sustainable Development Goals 2030."
pub.1138405091,Safeguarding Cluster Heads in UAV Swarm Using Edge Intelligence: Linear Discriminant Analysis-Based Cross-Layer Authentication,"While the unmanned aerial vehicles (UAVs) swarm travels under a dynamic environment, the cluster head (CH) switching is unavoidable due to the mitigation of mobility, quality of service, and energy consumption. If an attacker becomes the new CH, the entire swarm will be controlled and the sensitive data will be leaked. Unlike the other mobile networks with constant network connectivity, the authentication in the UAV swarm suffers from intermittent connection with the ground station under a hostile environment or spectrum constraint condition. Hence, this paper proposes a novel CH safeguarding mechanism enabled by edge intelligence utilizing a situational-aware authentication scheme. This low-latency mechanism provides extra security at the CH selection and switching without cloud server support. By adopting the unique cross-layer attributes, the system security is significantly improved based on the extracted multi-dimensional information. The Linear Discriminant Analysis (LDA) algorithm fuses the authentication decision accurately by projecting the high dimensional estimations into a low dimensional space for maximum separability by only keeping the necessary attributes. A situation-aware cross-layer attribute selection algorithm is developed to select a minimum number of attributes so that the time required for attribute estimation and computation overhead of authentication can be reduced. The simulation results demonstrate that our scheme performs better under a dynamic environment compared with the physical layer authentication scheme and some existing state-of-the-art authentication techniques."
pub.1009994736,The weather report from IRC+10216: evolving irregular clouds envelop carbon star,"High angular resolution images of IRC+10216 are presented in several near-infrared wavelengths spanning more than 8 years. These maps have been reconstructed from interferometric observations obtained at both Keck and the VLT, and also from stellar occultations by the rings of Saturn observed with the Cassini spacecraft. The dynamic inner regions of the circumstellar environment are monitored over eight epochs ranging between 2000 January and 2008 July. The system is shown to experience substantial evolution within this period including the fading of many previously reported persistent features, some of which had been identified as the stellar photosphere. These changes are discussed in the context of existing models for the nature of the underlying star and the circumstellar environment. With access to these new images, we are able to report that none of the previously identified bright spots in fact contains the star, which is buried in its own dust and not directly visible in the near-infrared."
pub.1139953424,FEATURES OF IMPLEMENTATION OF MANAGEMENT INNOVATIONS BY IT ENTREPRENEURS,"The article forms the theoretical basis for the development and implementation of managerial innovations. Sectoral features of the IT sphere in terms of innovative changes in management processes are identified: staff dependence, high level of unpredictability of consequences, the need for appropriate research environment, creative nature of decisions, high complexity, the presence of external barriers from the state and internal resistance, within labor collectives, active use of outsourcing, the predominance of project and team forms of activity. It is proved that every managerial innovation has a technological basis, while almost every technological innovation is based on a managerial decision. The components necessary for the implementation of managerial innovations are formulated: a problem that does not have an existing solution and needs new ideas; new principles and views that should provide new approaches; revision of traditions and dogmas that limit creative thinking; examples and analogues that will help to rethink the possibilities. Management innovations in the IT sphere are structured and the following are distinguished: innovations in management methodology (synergetic management, tender management, network management, creative management, team management, communicative management, etc.), innovations in internal structural subtypes of general management, financial innovation, marketing), innovations in the basics of science and practice of classical management (innovations in the principles, methods, management functions and elements of regulatory information and staffing). The main management innovations used by IT companies in Ukraine are analyzed: quality management, controlling, reengineering, system intervention strategy, neural network technologies, information-associative modeling, structural-functional modeling, etc. It is proved that the successful implementation of these management innovations requires the following conditions: a systematic vision of managerial change by management, formed innovation infrastructure, high level of management professionalism, timeliness of innovations and their compliance with overall objectives, staff loyalty to innovation change and development, approval or decisions by external consultants. The application of cloud technologies for the implementation of management innovations in the IT field and the introduction of service consulting, which is a comprehensive solution of business problems of the client based on an individual approach. It is determined that as management innovations spread, they improve, acquire new qualities and properties, adapt to new requirements and ultimately modernize themselves and become more efficient and effective. It is proved that innovation requires a certain management culture that requires managers of IT companies to be prone to risk and experimentation, as well as constant monitoring of the environment, requires an appropriate system to assess the current"
pub.1118645213,The weather report from IRC+10216: Evolving irregular clouds envelop carbon star,"High angular resolution images of IRC+10216 are presented in several near
infrared wavelengths spanning more than 8 years. These maps have been
reconstructed from interferometric observations obtained at both Keck and the
VLT, and also from stellar occultations by the rings of Saturn observed with
the Cassini spacecraft. The dynamic inner regions of the circumstellar
environment are monitored over eight epochs ranging between January 2000 and
July 2008. The system is shown to experience substantial evolution within this
period including the fading of many previously reported persistent features,
some of which had been identified as the stellar photosphere. These changes are
discussed in context of existing models for the nature of the underlying star
and the circumstellar environment. With access to these new images, we are able
to report that none of the previously identified bright spots in fact contain
the star, which is buried in its own dust and not directly visible in the near
infrared."
pub.1122197529,Star formation and environment: a step in understanding the formation and evolution of local dwarf galaxies,"Abstract
                  We present here the results obtained from studying the resolved stellar populations of two dwarf irregular galaxies in the nearby Universe. These galaxies, DDO 68 and NGC 4449, were studied within the Legacy ExtraGalactic UV Survey, an HST program aimed to uncover the many ways in which the star formation (SF) process occurs at different scales. Thanks to the deep photometry obtained in different bands (from λ2704 Å to λ8057 Å), we were able to connect the location and timescales of the star forming regions within the galaxies to merging and interaction with gas clouds and satellites, a crucial aspect of galaxy evolution, even in such small systems. From the color-magnitude diagrams of the analyzed galaxies we were able to recover their star formation history (up to ∼ 2 − 3 Gyr ago since we do not observe the oldest main sequence turn-off or horizontal branch, due to the systems’ distance), finding that the SF never really stopped, but proceeded continuously even with the succession of high and low activity. The time intervals where we find higher SF rates in the two galaxies well agree with the dynamical timescales of previous interactions events, which might represent a major channel for triggering the SF in relatively isolated galaxies."
pub.1152985465,A new hyper-heuristic based on ant lion optimizer and Tabu search algorithm for replica management in cloud environment,"Information can be shared across the Internet using cloud computing, a powerful paradigm for meeting the needs of individuals and organizations. To minimize access time and maximize load balancing for data nodes (DNs), a dynamic data replication algorithm is necessary. Even so, few of the existing algorithms consider each objective holistically during replication. An improved ant lion optimizer (ALO) algorithm and a fuzzy system are used in this paper to determine dynamically the number of replicas and the DNs for replication. Further, it balances the trade-offs among different objectives (e.g., service time, system availability, load, and monetary cost). The ALO algorithm has been widely applied to solve complex optimization problems due to its simplicity in implementation. However, ALO has premature convergence and can thus easily get trapped into the local optimum solution. In this paper, to overcome the shortcomings of ALO by balancing exploration and exploitation, a hybrid ant lion optimizer with Tabu search algorithm (ALO-Tabu) is proposed. There are several improvements of the ALO, in which the appropriate solutions are selected for the initial population based on chaotic maps (CMs) and opposition-based learning (OBL) strategies. On the other hand, there are many CMs, OBLs, and random walk strategies that make it difficult to select the best one for optimization. Generally, they are selected manually, which is time-consuming. As a result, this paper presents a hyper-heuristic ALO (HH-ALO-Tabu) that automatically chooses CMs, OBLs, and random walk strategies depending on the differential evolution (DE) algorithm. Based on 20 well-known test functions, the experiment results and statistical tests show that HH-ALO-Tabu can solve optimization problems effectively."
pub.1111609880,An Examination of CAPTCHA for Tolerance of Relay Attacks and Automated Attacks,"CAPTCHA is a type of challenge response test used to distinguish human users from malicious computer programs such as bots, and is used to protect email, blogs, and other web services from bot attacks. So far, research on enhance of CAPTCHA’s resistance to bot attacks has been proceeded to counter advanced automated attacks method. However, an attack technique known as a relay attack has been devised to circumvent CAPTCHA. In this attack, since human solves CAPTCHA, the existing measures assuming bots have no effect on this attack. We designed a new CAPTCHA scheme for relay attacks tolerance and automated attacks tolerance. In this paper, we tested the robustness of the proposed method against several types of automated attacks. We constructed an experimental environment in which a relay attack can be simulated, and designed a series of experiments to evaluate the performance of the proposed method. As a result, we found that the proposed CAPTCHA scheme offers some of level of resistance to automated attacks and relay attacks."
pub.1110481800,Marker-free coregistration of UAV and backpack LiDAR point clouds in forested areas," Unmanned aerial vehicle Laser Scanning (ULS) and Backpack Laser Scanning (BLS) are two emerging mobile mapping technologies applicable for monitoring forested environments in unprecedented detail from complementary perspectives. Although ground-based backpack techniques provide detailed information about the forest understory and terrain, the measured point clouds based on SLAM techniques are stitched together gradually and normally expressed in a less-accurate arbitrary coordinate system. Conversely, ULS point clouds are acquired from above and usually georeferenced, yet the point density and penetrability near the ground may still suffer from dense overstory despite the low attitude operation. Coregistering the ground and aerial point clouds in the ULS coordinate system therefore provides a method for fusing understory and overstory information at single tree level without the time consuming procedure of applying ground control points. Since the ULS and BLS acquisition viewpoints differ greatly, standard coregistration methods requiring 3D point-level correspondences are likely to fail. This paper presents an object-level coregistration approach which instead operates on two sets of tree positions, with the goal of finding the optimal 3D transformation (consisting of rotation, translation and scaling) between the respective coordinate systems. The entire task is decomposed into separate problems of computing the common Z axis, estimating the scale, and 2D coregistration. In contrast to existing methods, our approach does not require additional information such as tree diameters or heights. We evaluated our method on real test plots involving diverse stem densities and tree species situated in forest farm of the eastern coastal region of Jiangsu, China. The tree positions for ground and aerial data were obtained respectively by cylinder fitting and tree segmentation. On 3 coniferous (dawn redwood) plots, 46–81% trees were matched with a distance below 50 cm, and mean position deviation of 27–36 cm. For 4 broadleaf (poplar) plots, no more than 50% trees were matched below a 1 m threshold and mean error of 54–67 cm, which can be attributed to the broadleaf trees’ more irregular shape and lack of a well defined tree top. Moreover, we show that the introduction of scaling into the transform can increase the matched tree count by up to 20 percentage points and decrease the mean matched distance by up to 13% compared to a strictly rigid transform."
pub.1165249858,Towards the decentralized coordination of multiple self-adaptive systems,"When multiple self-adaptive systems share the same environment and have
common goals, they may coordinate their adaptations at runtime to avoid
conflicts and to satisfy their goals. There are two approaches to coordination.
(1) Logically centralized, where a supervisor has complete control over the
individual self-adaptive systems. Such approach is infeasible when the systems
have different owners or administrative domains. (2) Logically decentralized,
where coordination is achieved through direct interactions. Because the
individual systems have control over the information they share, decentralized
coordination accommodates multiple administrative domains. However, existing
techniques do not account simultaneously for both local concerns, e.g.,
preferences, and shared concerns, e.g., conflicts, which may lead to goals not
being achieved as expected. Our idea to address this shortcoming is to express
both types of concerns within the same constraint optimization problem. We
propose CoADAPT, a decentralized coordination technique introducing two types
of constraints: preference constraints, expressing local concerns, and
consistency constraints, expressing shared concerns. At runtime, the problem is
solved in a decentralized way using distributed constraint optimization
algorithms implemented by each self-adaptive system. As a first step in
realizing CoADAPT, we focus in this work on the coordination of adaptation
planning strategies, traditionally addressed only with centralized techniques.
We show the feasibility of CoADAPT in an exemplar from cloud computing and
analyze experimentally its scalability."
pub.1152369057,Deep Learning Enabled Intelligent Healthcare Management System in Smart Cities Environment,"In recent times, cities are getting smart and can be managed effectively through diverse architectures and services. Smart cities have the ability to support smart medical systems that can infiltrate distinct events (i.e., smart hospitals, smart homes, and community health centres) and scenarios (e.g., rehabilitation, abnormal behavior monitoring, clinical decision-making, disease prevention and diagnosis postmarking surveillance and prescription recommendation). The integration of Artificial Intelligence (AI) with recent technologies, for instance medical screening gadgets, are significant enough to deliver maximum performance and improved management services to handle chronic diseases. With latest developments in digital data collection, AI techniques can be employed for clinical decision making process. On the other hand, Cardiovascular Disease (CVD) is one of the major illnesses that increase the mortality rate across the globe. Generally, wearables can be employed in healthcare systems that instigate the development of CVD detection and classification. With this motivation, the current study develops an Artificial Intelligence Enabled Decision Support System for CVD Disease Detection and Classification in e-healthcare environment, abbreviated as AIDSS-CDDC technique. The proposed AIDSS-CDDC model enables the Internet of Things (IoT) devices for healthcare data collection. Then, the collected data is saved in cloud server for examination. Followed by, training and testing processes are executed to determine the patient’s health condition. To accomplish this, the presented AIDSS-CDDC model employs data pre-processing and Improved Sine Cosine Optimization based Feature Selection (ISCO-FS) technique. In addition, Adam optimizer with Autoencoder Gated Recurrent Unit (AE-GRU) model is employed for detection and classification of CVD. The experimental results highlight that the proposed AIDSS-CDDC model is a promising performer compared to other existing models."
pub.1176009748,МЕТОДИЧНІ АСПЕКТИ ФОРМУВАННЯ УПРАВЛІНСЬКОЇ ЗВІТНОСТІ В СУЧАСНИХ УМОВАХ,"This article delves deeply into the methodological aspects of management reporting within the context of contemporary challenges, with a particular focus on the unique circumstances presented by wartime conditions. The ongoing economic instability, widespread infrastructure damage, and a range of other significant challenges have created an urgent need for timely and accurate management information. Such information is essential for informed and effective decision-making, which is more critical than ever in such volatile environments. The automation of accounting processes, facilitated by the adoption of advanced digital technologies, presents an opportunity to significantly reduce the time required for report preparation. However, this transition is not without its challenges, as it demands substantial financial investments, as well as the availability of highly skilled personnel capable of managing and maintaining these complex systems. It is important to integrate new methods with existing standards and requirements of regulatory authorities, which complicates the work of accounting departments. The author identifies several approaches to management reporting: informational, functional, process, system, and strategic. As a result, it has been established that the application of an integrative approach allows for a comprehensive assessment and analysis of the enterprise. Based on this approach, the study formulates the author’s own vision of the essence of management reporting. The traditional components of the reporting methodology are considered, as well as modern challenges and the need to adapt to rapid changes. The author defines the objectives of management reporting in modern conditions as a component of the relevant methodology. The list of components of integrated management reporting, which can become a sufficient information basis for business management in conditions of uncertainty, is defined and substantiated. The article also describes the prospects of using the latest technologies, such as artificial intelligence, cloud technologies, the Internet of Things, and blockchain, to improve the accuracy, security, and efficiency of management processes. The conclusions emphasize the importance of analytical tools and the need to increase attention to cybersecurity in the context of business digitalization."
pub.1056961011,Surveys of the Milky Way and Magellanic System in the λ21-cm line of atomic hydrogen,"In the next three years, surveys of the Northern and Southern skies using focal plane arrays on aperture synthesis radio telescopes will lead to a breakthrough in our knowledge of the warm and cool atomic phases of the interstellar medium and their relationship with the diffuse molecular gas. The sensitivity and resolution of these surveys will give an order of magnitude or more improvement over existing interstellar medium data. The GASKAP (South) and GAMES (North) projects together constitute a complete survey of the Milky Way plane and the Magellanic Clouds and Stream in both emission and absorption in the H I 21-cm line and the OH 18-cm lines. The overall goal of this project is to understand the mechanism of galaxy evolution, through a detailed tracing of the astrophysical processes that drive the cycle of star formation in very different environments. Comparison of 21-cm emission and absorption highlights the transition from the warm, diffuse medium to cool clouds. Tracing turbulence in the Magellanic Stream shows how extra-galactic gas makes the difficult passage through the halo to replenish the disk. Finally, high resolution images of OH masers trace outflows from evolved stars that enrich the medium with heavy elements. To understand how the Milky Way was assembled and how it has evolved since, the speed and efficiency of these processes must be measured, as functions of Galactic radius and height above the plane. Observations of similar processes in the Magellanic Clouds show how differently they might have worked in conditions typical of the early universe."
pub.1157076885,A Cybersecurity Evaluation Model (CSEM) for Indian SMEs Working in a Virtual Team Environment,"During COVID-19 pandemic, there has been unprecedented increase in the number of employees working outside an organisations IT infrastructure due to the use of personal devices. The scale and sophistication of cyberattacks also continue to increase post-COVID-19 and it has become critical for SMEs (Small and Medium Sized Enterprises) to safeguard their information and IT assets. COVID19 proved to be a major catalyst for the adoption of digital approaches to remote working that many organisations did not previously believe to be feasible. The systems are becoming increasingly exposed to cyber-attacks as a result of remote access technology and cloud networks. The literature points to a gap in the existing knowledge to address the cybersecurity requirements for SMEs in India working in a virtual setup. The purpose of this paper is to develop a cybersecurity evaluation model (CSEM) that can be leveraged by SMEs which will eventually help them assess their cyber-risk portfolio. Based on the research project and the methodology used in the past for similar research, a quantitative approach will be chosen for this research. This research requires the researcher to roll out an online survey, which will enable the participants to evaluate cybersecurity risks by responding to the survey questionnaire. Analysing and implementing a CSEM will not only assist SMEs in identifying their strengths and weaknesses but will also include simple best practice guidelines for effectively plugging their cybersecurity flaws while working remotely."
pub.1168134350,Fog-Assisted Dynamic IoT Device Access Management Using Attribute-Based Encryption,"The management and control of heterogeneous IoT devices in cyber-physical systems (CPS) involves ensuring authorized access to cloud-stored data, including instructions, commands, and configuration settings, and issuing them securely to IoT devices for remote execution. Existing access management techniques present various security challenges in ensuring fine-grained access control to sensitive data present on untrusted cloud servers. These challenges are further complicated by the need to dynamically evaluate contextual parameters linked to IoT devices before issuing instructions. This work proposes a secure and context-aware encryption technique for remote access control of IoT devices. Leveraging ciphertext-policy attribute-based encryption (CP-ABE), the scheme encrypts instructions, requiring the user’s decryption key to satisfy embedded access policies for access. The integration of access policies considers both user attributes and dynamic parameters associated with IoT devices, ensuring a comprehensive evaluation before access is granted. To verify the dynamic parameters, fog-based servers are employed, positioned in proximity to IoT devices for efficient and real-time assessment. The scheme introduces a two-phase decryption process, involving fog servers in generating key components based on the verified dynamic parameters (of IoT devices) that are combined with the user’s existing key to ensure secure partial decryption. Final decryption is performed by the user who securely sends instructions for execution on the IoT devices. Our proposed cryptosystem security and computational complexity analysis demonstrate the scheme’s effectiveness in achieving secure and context-aware IoT device access in dynamic CPS environments, ensuring efficient control, monitoring, and automation while preserving data privacy."
pub.1157424700,Cloud-IoT Technologies in Society 5.0,"This book provides in-depth knowledge in the areas of convergence of cloud-IoT technologies and industry 4.0 with society 5.0, machine-to-machine communication, machine-to-person communication, techno-psychological perspective of society 5.0, sentiment analysis of smart digital societies, multi-access edge computing for 5G networks, discovery & location reporting of multi-access edge enabled clients/servers, m-health systems, enhancing the concert of M-health technologies in smart societies, supervising communication services in smart societies, life quality enhancement in smart city societies, multiple disease infection predictions, and societal opinion mining algorithms for smart cities societies using cloud-IoT integrated intelligent machine / deep learning technologies to the readers in the distributive environment. In this book, the authors have mandatorily discussed the implementation of cloud-IoT based machine learning technologies like clustering technique, Naïve Bayes classifier, artificial neural network (ANN), Firefly algorithm, Rough set classifiers, support vector machine classifier, decision tree classifier, ensemble classifier, random forest, and deep learning algorithms to analyze the behavior of intelligent machines and human habits using automated data scheduling and smart digital networks.At present, we live in a self-motivated and dynamic global society where technologies and challenges are unexpectedly changing overnight. These rapid changes in globalization and technological advances are creating new market forces every day. Therefore, day-to-day innovation is essential for any business or institution to survive and flourish in such an atmosphere. Though, innovation is no longer just to create value to do good to individuals, societies, or organizations. The utmost purpose of innovation is to create a smart futuristic society where people can enjoy the best quality of life using natural resources and manmade technologies including cloud-IoT technologies, and industry4.0. Hence, the innovators and their innovations must search for intelligent solutions to tackle major socio-technical problems and remove barriers of rural, urban and smart city societies. The smart digitization and intelligent implementation of manufacturing development processes are the necessities for today’s rural, urban, and smart city industries. All types of industries including development, manufacturing, and research are presently shifting from bunch production to customized production. The fast advancements in manufacturing technologies have an in-depth impact on all types of societies including societies of rural areas, urban areas, and smart cities. Industry 4.0 includes the Internet of Things (IoT), Industrial Internet, Smart Manufacturing, Cloud-based computing, and Manufacturing Technologies. The objective of this book is to establish linkage between the Industry 4.0 components and various rural, urban & smart city societies (including society 5.0"
pub.1144444121,Logistics 4.0 and Smart Supply Chain Management,"With the holistic approaches of Industry 4.0, products, services, standards, and application techniques have been improved. This digitalization era has not only impacted the production and service dynamics, but also added advanced dimensions to logistics and supply chain management. According to the current world standards, consumer behavior makes the logistics and supply chain processes more challenging. Especially during the COVID-19 outbreak, logistics and supply chain operations became more crucial for the firms, as most consumers have tended toward online shopping while they are in lockdown. Therefore, the competitive environment today enables firms to adapt the technologies and approaches of Logistics 4.0 and smart/digital supply chain, as they must respond to consumers' demands quickly. Moreover, firms need to have strong relations with their supply chain partners via these technologies. The technologies such as the Internet of Things (IOT), cyber-physical system, Big Data, and cloud computing help to change the fundamentals of logistics and supply chain and improve processes for all industries. This study aims to analyze the transformation of traditional logistics and supply chain activities into Logistics 4.0 and smart/digital supply chain. Primarily, we hope to analyze the existing studies by investigating the concept of Logistics 4.0 within Industry 4.0 dynamics. As firms develop their logistics operations, their supply chain processes will be shaped by the technologies and applications, and this situation also leads us to find out the importance of smart or digital supply chain operations. Discussing the potentials of smart or digital supply chain also lets us to reveal how companies handle their logistics operations during the COVID-19 period."
pub.1167013440,FAPP: Fast and Adaptive Perception and Planning for UAVs in Dynamic Cluttered Environments,"Obstacle avoidance for Unmanned Aerial Vehicles (UAVs) in cluttered
environments is significantly challenging. Existing obstacle avoidance for UAVs
either focuses on fully static environments or static environments with only a
few dynamic objects. In this paper, we take the initiative to consider the
obstacle avoidance of UAVs in dynamic cluttered environments in which dynamic
objects are the dominant objects. This type of environment poses significant
challenges to both perception and planning. Multiple dynamic objects possess
various motions, making it extremely difficult to estimate and predict their
motions using one motion model. The planning must be highly efficient to avoid
cluttered dynamic objects. This paper proposes Fast and Adaptive Perception and
Planning (FAPP) for UAVs flying in complex dynamic cluttered environments. A
novel and efficient point cloud segmentation strategy is proposed to
distinguish static and dynamic objects. To address multiple dynamic objects
with different motions, an adaptive estimation method with covariance
adaptation is proposed to quickly and accurately predict their motions. Our
proposed trajectory optimization algorithm is highly efficient, enabling it to
avoid fast objects. Furthermore, an adaptive re-planning method is proposed to
address the case when the trajectory optimization cannot find a feasible
solution, which is common for dynamic cluttered environments. Extensive
validations in both simulation and real-world experiments demonstrate the
effectiveness of our proposed system for highly dynamic and cluttered
environments."
pub.1091846493,Query Refinement for Correlation-Based Time Series Exploration,"In this paper, we focus on the problem of exploring sequential data to discover time sub-intervals that satisfy certain pairwise correlation constraints. Differently than most existing works, we use the deviation from targeted pairwise correlation constraints as an objective to minimize in our problem. Moreover, we include users preferences as an objective in the form of maximizing similarity to users’ initial sub-intervals. The combination of these two objectives are prevalent in applications where users explore time series data to locate time sub-intervals in which targeted patterns exist. Discovering these sub-intervals among time series data is extremely useful in various application areas such as network and environment monitoring.Towards finding the optimal sub-interval (i.e., optimal query) satisfying these objectives, we propose applying query refinement techniques to enable efficient processing of candidate queries. Specifically, we propose QFind, an efficient algorithm which refines a user’s initial query to discover the optimal query by applying novel pruning techniques. QFind applies two-level pruning techniques to safely skip processing unqualified candidate queries, and early abandon the computations of correlation for some pairs based on a monotonic property. We experimentally validate the efficiency of our proposed algorithm against state-of-the-art algorithm under different settings using real and synthetic data."
pub.1120645380,Intelligent Embedded Vision for Summarization of Multiview Videos in IIoT,"Nowadays, video sensors are used on a large scale for various applications, including security monitoring and smart transportation. However, the limited communication bandwidth and storage constraints make it challenging to process such heterogeneous nature of Big Data in real time. Multiview video summarization (MVS) enables us to suppress redundant data in distributed video sensors settings. The existing MVS approaches process video data in offline manner by transmitting them to the local or cloud server for analysis, which requires extra streaming to conduct summarization, huge bandwidth, and are not applicable for integration with industrial Internet of Things (IIoT). This article presents a light-weight convolutional neural network (CNN) and IIoT-based computationally intelligent (CI) MVS framework. Our method uses an IIoT network containing smart devices, Raspberry Pi (RPi) (clients and master) with embedded cameras to capture multiview video data. Each client RPi detects target in frames via light-weight CNN model, analyzes these targets for traffic and crowd density, and searches for suspicious objects to generate alert in the IIoT network. The frames of each client RPi are encoded and transmitted with approximately 17.02 smaller size of each frame to master RPi for final MVS. Empirical analysis shows that our proposed framework can be used in industrial environments for various applications such as security and smart transportation and can be proved beneficial for saving resources.11[Online]. Available: https:github.comtanveer-hussainEmbedded-Vision-for-MVS. [Online]. Available: https:github.comtanveer-hussainEmbedded-Vision-for-MVS."
pub.1165240902,Drilling Parameters Multi-Objective Optimization Method Based on PSO-Bi-LSTM,"The increasing exploration and development of complex oil and gas fields pose challenges to drilling efficiency and safety due to the presence of formations with varying hardness, abrasiveness, and rigidity. Consequently, there is a growing demand for drilling parameter optimization and speed-up technologies. However, existing models based on expert experience can only achieve single-objective optimization with limited accuracy, making real-time adaptation to changing drilling conditions and formation environments challenging. The emergence of artificial intelligence provides a new approach for optimizing drilling parameters. In this study, we introduce the Bi-directional Long Short-Term Memory (Bi-LSTM) deep learning algorithm with the attention mechanism to predict the rate of penetration (ROP). This algorithm improves the ROP prediction accuracy to 98.33%, ensuring reliable subsequent optimization results. Additionally, we propose a coupling optimization algorithm that combines Bi-LSTM with the particle swarm optimization algorithm (PSO) to enhance drilling efficiency through parameter optimization. Our approach aims to maximize drilling footage while maintaining the highest ROP. The optimal solutions obtained are verified through multi-parameter cloud image analysis, yielding consistent results. The application of our approach demonstrates an 81% increase in drilling speed and a 28% reduction in drill bit energy losses. Moreover, the real-time optimization results effectively guide field operations."
pub.1105220892,Experiences in the Development of a Data Management System for Genomics,"GMQL is a high-level query language for genomics, which operates on datasets described through GDM, a unifying data model for processed data formats. They are ingredients for the integration of processed genomic datasets, i.e. of signals produced by the genome after sequencing and long data extraction pipelines. While most of the processing load of today’s genomic platforms is due to data extraction pipelines, we anticipate soon a shift of attention towards processed datasets, as such data are being collected by large consortia and are becoming increasingly available.In our view, biology and personalized medicine will increasingly rely on data extraction and analysis methods for inferring new knowledge from existing heterogeneous repositories of processed datasets, typically augmented with the results of experimental data targeting individuals or small populations. While today’s big data are raw reads of the sequencing machines, tomorrow’s big data will also include billions or trillions of genomic regions, each featuring specific values depending on the processing conditions.Coherently, GMQL is a high-level, declarative language inspired by big data management, and its execution engines include classic cloud-based systems, from Pig to Flink to SciDB to Spark. In this paper, we discuss how the GMQL execution environment has been developed, by going through a major version change that marked a complete system redesign; we also discuss our experiences in comparatively evaluating the four platforms."
pub.1160122524,WebAssembly as an Enabler for Next Generation Serverless Computing,"WebAssembly is a new binary instruction format and runtime environment capable of executing both client side and server side workloads. With its numerous advantages, including drastically reduced cold start times, efficiency, easy portability, and compatibility with the most popular programming languages today, it has the potential to revolutionize serverless computing. We evaluate the impact of WebAssembly in terms of serverless computing, building on top of existing research related to WebAssembmly in cloud and edge environments. To this end, we introduce a novel benchmarking suite comprised of 13 different functions, compatible with WebAssembly, and focusing on both microbenchmarking and real-world workloads. We also discuss possibilities of integrating WebAssembly runtimes with the application programming interfaces and command line interfaces of popular container runtimes, representing an initial step towards potential reuse of existing orchestration engines in the future, thus solving the open issue of WebAssembly workload scheduling. We evaluate the performance of such an integration by comparing the cold start delays and total execution times of three WebAssembly runtimes: WasmEdge, Wasmer, and Wasmtime to the performance of the containerd container runtime, using distroless and distro-oriented container images. Results show that WebAssembly runtimes show better results in 10 out of 13 tests, with Wasmtime being the fastest WebAssembly runtime among those evaluated. Container runtimes still offer better compute performance for complex workloads requiring larger execution times, in cases where cold start times are negligible compared to the total execution time."
pub.1173897848,Energy-aware virtual machine placement based on a holistic thermal model for cloud data centers,"As energy-intensive infrastructures, data centers (DCs) have become a pressing challenge for managers due to their significant energy consumption and carbon emissions. Information technology (IT) and cooling systems contribute the most to energy consumption. Energy-aware virtual machine (VM) scheduling methods have been widely demonstrated to reduce energy consumption and operating costs in DCs. However, as realistic DCs exhibit complex power and thermodynamic behaviors, existing works cannot provide efficient measures to optimize computing and cooling power consumption simultaneously. To overcome this challenge, we construct a holistic thermal model (including CPU and server inlet thermal models) to accurately represent the non-uniform, dynamic thermal environment. Subsequently, this work proposes a thermal model-based energy-aware VM placement method (TEVP) to minimize the holistic energy consumption of the DCs, considering resource and thermal constraints. We develop a novel hybrid swarm intelligence algorithm (DE-ERPSO) combining differential evolution (DE) and particle swarm optimization with an elite re-selection mechanism (ERPSO) to explore more energy-efficient VM placement schemes. Extensive experiments are conducted on an extended CloudSim to validate the performance of the proposed TEVP using real-world workload traces (PlanetLab and Azure). Results show that TEVP saves over 5.6% of the total energy consumption over the advanced baselines while maintaining low thermal violations."
pub.1170999488,Decentralized Federated GAN for Hyperspectral Change Detection in Edge Computing,"Change detection on hyperspectral images (HSIs) is an essential task for Earth observation. Due to the vast amounts of remote sensing (RS) Big Data resulting from the ongoing advancements in RS hardware, employing centralized learning through cloud computing emerges as a logical and convenient solution. Nevertheless, this approach overlooks the influence of the isolation and heterogeneity of RS data on the reliability of change detection outcomes. In contrast, federated learning (FL) enables collaborative change detection on nonindependent identical distributed RS data without the need to transfer the original data. Simultaneously, it is important to acknowledge that the dependency of FL on the central node may pose potential data security risks. To address this issue, this article proposes a decentralized FL generative adversarial network (GAN) network. This ensures that raw data remains stationary during participation in unsupervised learning. In addition, the network employs edge devices to implement FL, allowing adaptation to diverse devices in practical scenarios. Lastly, blockchain is integrated into a decentralized architecture for the dynamic selection of leader nodes, effectively enhancing the robustness and security of the framework. Decentralized federated generative adversarial network (DFGAN) is a novel approach for cooperative privacy preservation introduced into hyperspectral change detection systems. The method outperforms most of existing methods on three datasets, achieving more than 90 detection accuracy on all of them. In addition, by simulating in a real on-orbit environment using a satellite constellation simulator on a Jeston TX2, FedGAN demonstrates superior accuracy in HSI change compared to existing algorithms while significantly reducing training time by 17.3."
pub.1140218250,The impact of binaries on the evolution of star clusters from turbulent molecular clouds,"ABSTRACT
                  Most of massive stars form in binary or higher order systems in clumpy, substructured clusters. In the very first phases of their life, these stars are expected to interact with the surrounding environment, before being released to the field when the cluster is tidally disrupted by the host galaxy. We present a set of N-body simulations to describe the evolution of young stellar clusters and their binary content in the first phases of their life. To do this, we have developed a method that generates realistic initial conditions for binary stars in star clusters from hydrodynamical simulations. We considered different evolutionary cases to quantify the impact of binary and stellar evolution. Also, we compared their evolution to that of King and fractal models with different length-scales. Our results indicate that the global expansion of the cluster from hydrodynamical simulations is initially balanced by the subclump motion and accelerates when a monolithic shape is reached, as in a post-core collapse evolution. Compared to the spherical initial conditions, the ratio of the 50 per cent to 10 per cent Lagrangian radius shows a very distinctive trend, explained by the formation of a hot core of massive stars triggered by the high initial degree of mass segregation. As for its binary population, each cluster shows a self-regulating behaviour by creating interacting binaries with binding energies of the order of its energy scales. Also, in the absence of original binaries, the dynamically formed binaries display a mass-dependent binary fraction, spontaneously reproducing the trend of the observed binary fraction."
pub.1138404855,Does Technostress Trigger Insider Threat? A Conceptual Model and Mitigation Solutions,"Since the emergence of the Internet in the twentieth century and the rapid growth of different types of information technologies (IT), our lives, either personal or professional, have become digitised. Adoption and diffusion of IT enhance individuals and organisational performance, yet scholars discovered a dual nature of IT in which IT usage may have negative aspects too. First, the inability to cope with IT in a healthy manner creates stress in users, termed technostress. Second, digitisation and adoption of new technologies (e.g. IoT and multi-cloud environments) have increased vulnerabilities to information security (InfoSec) threats. Although organisations utilise counteraction strategies (e.g., security systems, security policies), end-users remain the top source of security incidents. Existing behavioural research has approached technostress and InfoSec independently. However, it is not clear how technology-stressors influence employees’ security-related behaviours. This chapter reviews the interaction effect of these concepts in detail by proposing a conceptual model that explains that technostress is the main reason for employees’ non-compliance with security policies in which users with high-level perceptions of technostress are more likely to violate InfoSec policies. Counteraction strategies to mitigate technostress and security threats are also discussed."
pub.1175536938,How is the Adoption of E-invoicing System Affecting the Outsourcing of Accounting Services?,"Digital technologies have significantly transformed our society by changing the way people communicate, work, and trade. Due to digitalisation, transactions occur faster and they can be easily tracked. Electronic data transfer enables automatic data processing and electronic archiving of documents. Worldwide, there is a growing interest from the tax authorities to monitor and control the business transactions. Considering that in many countries, taxpayers have to exchange e-invoices through the servers administered by the tax authorities. Over the years, the outsourcing of accounting services has become a very widespread practice among small companies for cost reduction reasons. This paper investigates the effects of e-invoicing system adoption on the outsourcing of accounting services. We had in view Technology Organisation Environment (TOE) framework as theory and Romanian business context. We collected data using the content shared online by big four companies on their own websites and accountants on social networks. Following our theoretical framework, the results indicated that environmental context, especially the government regulations, put pressure on organisations to digitalise their business processes or to update the existing technologies in order to accommodate e-invoicing. Our findings revealed that the mandatory adoption of an e-invoicing system creates new opportunities for those companies being larger in size to fully or partially outsource their accounting services. Most accountants believe that an e-invoicing system can remove information delays and it can ensure immediate access to data which is crucial for decision making in case of larger companies. Furthermore, e-invoicing system enables invoice tracking, continuous invoice processing in accounting, and near real-time reporting. Our study has implications for clients and accounting firms, and we emphasised that mandatory adoption of e-invoicing system has a strong impact on accounting outsourcing since it facilitates remote delivery of services and the use of cloud accounting, automatic data collection, and processing due to electronic data transfer."
pub.1154335988,From the abacus to enterprise resource planning: is blockchain the next big accounting tool?,"
                    Purpose
                    This study conducts a systematic review using 452 academic and industry articles from an initial set of 60,899 records obtained by 3 databases from 2012 to 2020. The authors compare and contrast blockchains with existing legacy systems. The authors identify existing regulation, accounting standards, guidelines and potential amendments in under-explored areas such as taxation, accounting treatment of crypto-assets/liabilities and detailed auditing procedures. The study aims to highlight the trends, differences and gaps between academic and industry literature. The authors provide a behavioral, social, cultural, organizational, regulatory, ethical, accountability and managerial perspectives of blockchain adoption in accounting. Finally, the study develops two adoption frameworks.
                  
                  
                    Design/methodology/approach
                    
                      The authors' study follows (Moher
                      et al
                      ., 2009) and (Briner and Denyer, 2012) methodology to conduct the systematic review and the steps are mentioned below. The authors construct a final sample of 452 from a preliminary search of three multi-disciplinary databases from 2012 to 2020. First, the authors motivate the review and formulate the research questions. Second, the authors aggregate relevant literature from both industry and academia and implement quality assessments. Third, the authors analyze the literature and construct the final sample of articles. Fourth, the authors conducted textual analysis, keyword frequencies and identify gaps, trends and similarities between academic and industry literature and develop the authors' frameworks
                    
                  
                  
                    Findings
                    The authors identify 3 (ABDC, B and A* ranked) journals as publishing top article numbers with the highest article count for 2017 with 96 articles in academia and 2019 for the industry with 21 articles. Second-highest publications for academia occur in 2018 with 77 followed by, whereas in the industry, publications occur in the year 2016 with 16 articles. Two co-authors appear most popular with 103 articles. Word clouds, a mind map and article theme counts are used to identify nine key research clusters: data management, financial applications, sustainability, accounting and auditing, business and industrial, education, governance, privacy/security and disruptive technology.
                  
                  
                    Research limitations/implications
                    Systematic reviews can have selection biases mainly due to search and selection criteria distortions when constructing the final sample of articles. The authors address selection bias by refining our search keyword combinations by using different permutations and using keywords from articles already collected. The authors employ "
pub.1168903208,Recent advances in geostationary satellites for inland and coastal aquatic systems: scientific research and applications,"Inland and coastal environments are complex ecosystems composed of suspended and dissolved materials, affecting light propagation within the water column. Satellite-based water quality research relies on water optical properties provided by optical sensors on board of polar orbit satellites since the 1980’s. Specifically, Geostationary (GEO) ocean colour satellites offer high temporal resolution (e.g. every 15-minute observations), moderate spatial resolution (0.5–1 km) at regional scale, making them a promising alternative to polar orbiting satellites for near-continuous monitoring of highly dynamic aquatic ecosystems. This literature review examines the evolution of geostationary satellite technology and its applications in monitoring inland and coastal waters. A summary of the most relevant studies using geostationary sensors is provided for key water quality indicators such as chlorophyll-a and algal organisms, total suspended solids, and turbidity. Also, geostationary missions were well-detailed, with their available sensors and characteristics. Although this research topic is still incipient, recent studies have demonstrated the potential of GEO multi-spectral observations in understanding sub-daily water quality patterns. Notably, most research studies have focused on Asia, suggesting unexplored opportunities globally. Advanced Himawari Imager (AHI) and Geostationary Ocean Colour Imager (GOCI) have been used to improve water quality estimates, and inherent challenges were documented, such as algorithm validation, limited spatial resolution, and high volume of images and auxiliary files to be managed. The opportunities for new studies range from algorithm development for atmospheric correction, cloud masking, and bidirectional reflectance corrections to inter-comparison with existing sun-synchronous satellites. Geostationary satellites are promising avenues for future research on near-continuous monitoring of inland and coastal water resources."
pub.1140077889,"Efficient, Dynamic Multi-Task Execution on FPGA-Based Computing Systems","With growing Field Programmable Gate Array (FPGA) device sizes and their integration in environments enabling sharing of computing resources such as cloud and edge computing, there is a requirement to share the FPGA area between multiple tasks. The resource sharing typically involves partitioning the FPGA space into fix-sized slots. This results in suboptimal resource utilisation and relatively poor performance, particularly as the number of tasks increase. Using OpenCL’s exploration capabilities, we employ clever clustering and custom, task-specific partitioning and mapping to create a novel, area sharing methodology where task resource requirements are more effectively managed. Using models with varying resource/throughput profiles, we select the most appropriate distribution based on the runtime, workload needs to enhance temporal compute density. The approach is enabled in the system stack by a corresponding task-based virtualisation model. Using 11 high performance tasks from graph analysis, linear algebra and media streaming, we demonstrate an average $2.8\times$2.8× higher system throughput at $2.3\times$2.3× better energy efficiency over existing approaches."
pub.1170812218,Energy consumption data collection: case study on data center in a Thai University,"ObjectiveEnergy usage in has been increased due to the rising demand of cloud infrastructure. The government policy has been focused on building the green IT data center. The energy data need to be collected in order to monitor the energy usage. However, in an old typical data center, the building has been built with no support of such data collection. In this research, we aim to design the energy data collection system for our existing data center, a case study of data center in Thailand at the university. Based on the collected data, an energy usage monitoring system and prediction can be developed.MethodsIn the case study of Kasetsart University data center, the building and electric layouts were predetermined. The building layout and existing IT hardware were investigated. We designed the meter types and the number of meters to be installed for the building the energy data collection system. The corresponding database system was also designed for data logging, data visualization and analysis purpose.ResultsAs a result, 25 installed meters along with the add-on network system were installed for logging data. A data usage example was demonstrated by building the data visualization and analysis. The presented 1 year dataset collected showed the changes of energy usages which can be used to compare with real activities happening in the campus. This encourages the integration of other related environment data such as outside temperature which may affect the electric billing cost. The dataset can be used for prediction of the electric usage; thus, the policy for reducing the electric billing cost could be established. In this paper, as a data note, we focus on the methodology of data collection required for data center."
pub.1121080364,"Bioscience Data Literacy At The Interface Of The Environment, Human And Wildlife: One Health-centred education, research and practice perspectives in Rwanda","Advances in information technology have led to the availability of state-of-the-art technologies which in turn have been enabling the generation of unprecedented amounts of complex, structured or unstructured data sets that are sometimes difficult to process using conventional techniques. In particular, handling these large scale data in terms of collection, and aggregation, synthesis and analysis, interpretation, reporting, sharing and archiving processes, and interpreting them into descriptive models and enable effective interpretation requires continued development of robust computational models, algorithms and interoperable analytical frameworks (Hampton et al. 2017). This also involves the vital availability of data management expertise and reflects an imperative need for data science professionals, especially in the context of generating the most informative data for use and drive evidence-based decisions. Considering this, Rwanda has been fueling its economic transformation agenda, and, while this solely depends on natural resources exploitation, the scenario has led to critically concerning anthropogenic threats and unprecedented environmental vulnerability. Acknowledging the urgency to achieve its development needs while at the same time safeguarding the environmental sustainability, Rwanda has been promoting technology-enabled systems and approaches for sustainable management of environment and natural resources. Learning from global initiatives, Rwanda’s journey targets the effective use of technology-supported systems and data science expertise to effectively drive management and decision making needs in environmental management, health research systems and biodiversity conservation planning (Karame et al. 2017). Rwanda champions the adoption and effective use of technology towards delivering its vision of knowledge-based economy. A particular emphasis relates to streamlining the education, research and application of technology-supported systems and platforms and strengthening their effective use. From a practical One Health perspective, Rwanda has been bridging inter-sectoral gaps related to joint planning and resource sharing for informed decision processes. This One Health concept emphasizes the interconnection of the health of human, animals and ecosystems and involves the applications of multidisciplinary, coordinated, cross-sectoral collaborative efforts to attain optimal health for people, animals and the environment (Buttke et al. 2015). One Health constitutes a promising approach in the advancement of biosciences. For example, big data and ecological and digital epidemiology analysis has led to promising progress beyond the traditional transdisciplinary conservation medicine approach, and One Health is now driving solutions to major conservation and health challenges. This paper aims to explore the perspectives of solving challenges in handling heterogeneous data and sources of uncertainty, the progress and feasibility of ad"
pub.1182015848,3D dataset generation using virtual reality for forest biodiversity,"While forest biodiversity faces a concerning decline, modern technology presents promising avenues for mitigation. However, a critical gap persists in reconciling ecological knowledge with the technical expertise required to use state-of-the-art technologies in 3D data classification. Currently, one main issue is the scarcity of 3D datasets for biodiversity, particularly within the context of machine learning applications. Unlike the straightforward classification of human-made structures, forest environments are uniquely intricate and nuanced due to its inherently complex nature. This study addresses this challenge by introducing a fully automated pipeline for tree stem 3D point cloud segmentation, focussing on a biodiversity indicator: tree-related microhabitats (TreMs). Furthermore, our research advances the field by demonstrating that machine learning models trained with labels generated by our proposed virtual reality (VR) method, Labelling Flora, yield predictions statistically similar to the traditional desktop-based labelling methods. This implies that existing 3D datasets could be augmented using the more rapid approach of VR labelling. Additionally, the findings of this paper demonstrate the potential integration of VR and immersive technology into the 3D labelling workflow, facilitating a quicker and more intuitive labelling process. This could empower users, who are non-familiar with 3D modelling, to contribute their expertise to the segmentation process."
pub.1181226331,Continual Learning in the Frequency Domain,"Continual learning (CL) is designed to learn new tasks while preserving
existing knowledge. Replaying samples from earlier tasks has proven to be an
effective method to mitigate the forgetting of previously acquired knowledge.
However, the current research on the training efficiency of rehearsal-based
methods is insufficient, which limits the practical application of CL systems
in resource-limited scenarios. The human visual system (HVS) exhibits varying
sensitivities to different frequency components, enabling the efficient
elimination of visually redundant information. Inspired by HVS, we propose a
novel framework called Continual Learning in the Frequency Domain (CLFD). To
our knowledge, this is the first study to utilize frequency domain features to
enhance the performance and efficiency of CL training on edge devices. For the
input features of the feature extractor, CLFD employs wavelet transform to map
the original input image into the frequency domain, thereby effectively
reducing the size of input feature maps. Regarding the output features of the
feature extractor, CLFD selectively utilizes output features for distinct
classes for classification, thereby balancing the reusability and interference
of output features based on the frequency domain similarity of the classes
across various tasks. Optimizing only the input and output features of the
feature extractor allows for seamless integration of CLFD with various
rehearsal-based methods. Extensive experiments conducted in both cloud and edge
environments demonstrate that CLFD consistently improves the performance of
state-of-the-art (SOTA) methods in both precision and training efficiency.
Specifically, CLFD can increase the accuracy of the SOTA CL method by up to
6.83% and reduce the training time by 2.6$\times$."
pub.1149072574,Eiffel: Efficient and Fair Scheduling in Adaptive Federated Learning,"Emerging machine learning (ML) technologies, in combination with the increasing computational power of mobile devices, lead to the extensive adoption of ML-based applications. Different from conventional model training that needs to collect all the user data in centralized cloud servers, federated learning (FL) has recently drawn increasing research attention as it enables privacy-preserving model training. With FL, decentralized edge devices in participation, train their model copies locally over their siloed datasets, and periodically synchronize the model parameters. However, model training is computationally extensive which easily drains the battery of mobile devices. In addition, due to the uneven distribution of siloed datasets, the shared model may become biased. To address the efficiency and fairness concerns in a resource-constrained federated learning setting, in this paper, we propose Eiffel to judiciously select mobile devices to participate in the global model aggregation, and adaptively adjust the frequency of local and global model updates. Eiffel aims to make scheduling and coordination for the federated learning towards both resource efficiency and model fairness. We have conducted theoretical analysis of Eiffel from the perspectives of fairness and convergence. Extensive experiments with a wide variety of real-world datasets and models, both on a networked prototype system and in a larger-scale simulated environment, have demonstrated that while maintaining similar accuracy performance, Eiffel outperforms existing baselines with respect to reducing communication overhead by up to 6× for higher efficiency and improving the fairness metric by up to 57% compared to the state-of-the-art algorithms."
pub.1182031786,AI Based Attendance System using Machine Learning,"In our modern-digital age, monitoring attendance system in education institutions play a pivotal role for insuring accountability, optimizing resource utilization and enhancing efficiency. Methods that are being followed traditionally such as taking attendance Manually are prone to errors, very time-taking, causes lack of scalability. To over come this challenges, our research proposes an innovative AI based attendance monitoring system using Ml is designed to bring a revolutionary change in the way attendance is managed in Education System. This proposed system importance's ""State of the art"" computer vision techniques, Machine Learning Algorithms and modules of Deep Learning to automate the attendance tracking process. On Integrating cameras installed in universities with advance facial technology, which accurately identifies, record and stores the students attendance without requiring any type of manual intervention. Besides, the system incorporates intelligent features to deal with various scenarios such as occlusions, varying lighting conditions, and facial expressions, ensuring robust performance in real-world environments Through seamless integration with existing student data set and cloud-based platforms, this proposed solution facilitates centralized data management, empowering administrators with actionable insights for strategic decision-making and real time monitoring."
pub.1173294133,A Novel intelligent SAV oriented QL-based task offloading in mobile edge environments,"Edge computing is a novel and potential computing model which moves storage and computing capabilities to the network edge, substantially decreasing service latency and network traffic. The existing Internet of Things (IoT) network offloading algorithm faces limitations such as a fixed number of applications, high edge-to-edge delay, and reliance on a single Mobile Edge Computing (MEC) server, posing security and privacy concerns. Moreover, resource-constrained mobile devices need more effective data integration and compression strategies. Addressing these challenges, this study suggests an approach based on deep reinforcement learning (DRL), specifically the SAV (State Action Value) Oriented QL (Q-Learning) based Task Offloading method, to optimise task offloading and resource allocation in edge-cloud computing. The model aims to empower Mobile Devices (MDs) to develop optimal offloading decisions for long-term Quality Perception, utilising a neural network to establish the relationship between MD state and action value. The paper introduces a Recurrent Extended Memory Network (REMN) to capture dynamic workload behaviour at Edge nodes (ENs). It incorporates Quality Mapping, Quality Estimation, and a Quality-Aware DRL Task Offloading Algorithm to improve the accuracy and efficiency of the offloading procedure in MEC systems. This systematic approach improves overall system performance and enables MDs to leverage ENs for neural network training, reducing computational burdens. As a result, it can accomplish a more significant number of tasks, reducing latency from 0.74 ms to 7.168 ms and decreasing energy consumption from 270 J to 1820.39 J for tasks ranging from 10 to 50, respectively."
pub.1157380602,Boiling Down Aviation Data: Development of the Aviation Data Distillery,"Army Aviation is one of the heaviest data consumers of all service branches, and is heavily reliant upon consistent data streams. Currently, a complex web of information systems contains the various pieces of data, which hinders not only the use of the existing data systems, but also the development of cutting-edge data consumers. The AVX Aircraft Company, under the direction of the U.S. Army Combat Capabilities Development Command Aviation and Missile Center, assembled a team from industry and academia with uniquely complimentary skills to prototype and showcase a centralized data analysis space and toolset. Team members from the Applied Research Laboratory at Penn State University developed a cloud-hosted integration environment, granting secure and compliant access to a variety of developers and other interested parties. PeopleTec, Inc. curated a comprehensive dataset from six different H-60 Blackhawk helicopters, including health and usage monitoring data, maintenance records, and flight recorder data. Researchers from the Institute for Materials, Manufacturing, and Sustainment at Texas Tech University set to work developing statistical data combinations and reductions, as well as interactive and novel visualizations of the above data and results. The final cloud-hosted toolset demonstrated the overwhelming value of the additional analysis capabilities a centralized data access portal could present to Army Aviation. The ability to rapidly draw conclusions from the alignment, refinement, and display of what are currently disparate data sources represents a massive opportunity to enhance the capability and reliability of units across U.S. Army Aviation."
pub.1129231616,"Manufacturing in Digital Industries, Prospects for Industry 4.0","Industry 4.0 (I4.0) brings with it a series of changes in the companies; among them we find those that affect the business models to get the concept of smart factory. This change in business models implies a complete communication network between different companies, factories, suppliers, resources and others optimized in real time, so that maximum efficiency is achieved for all parties involved. The objective is in identifying a methodology that allows to define the supply chain (SC) that improves the performance and sustainability of the shipbuilding industry. Therefore, this chapter aims at connecting each of the key I4.0 technologies with the most significant SC paradigms: lean, agile, resilience and green to define what the shipbuilding SC should be. This study shows how each of the enabling technologies affects the SC, what paradigms to achieve and what steps to follow through the simulation of discrete events to end up implementing the shipbuilding supply chain in a 4.0 environment In the past decades, new technologies that relate to the Internet have been able to reshape the economy completely. Nowadays, blockchain technology is recognized as one of the new technologies with potential for completely transforming sectors such as banking. Nevertheless, the philosophy and basic principles in which the technology is developed may be applicable in other sectors such as manufacturing. Despite the technology is still immature, the technology can offer solutions to improving management and traceability in supply chains, fostering collaborative and open innovation, enhancing the use of certification, optimizing human resources management, developing improved R&D funding strategies and creating trust by means of online accounting. In this chapter, a multipurpose blockchain network is proposed to be used by all manufacturing actors involved in activities such as the aforementioned. Smart factories, being an essence of the Industry 4.0 concept, are expected to be vertically integrated (within an enterprise) and horizontally integrated (along the industrial value chain). This chapter aims at addressing elements of the vertical integration, considering primarily integration of computer-aided inspection (CAI) with other computer-aided technologies (CAx), such as computer-aided design (CAD) and computer-aided manufacturing (CAM). In a digitized world, virtual engineering presents a powerful means for designing and optimizing the factory’s entities (parts, processes, etc.) in a software environment based on their digital twins, prior to physical mock-ups. Therefore, this chapter is focused on the virtual optimization of CAI-CAx loop for designing, inspecting and machining of parts with freeform surfaces, which are challenging tasks due to specifics and geometric complexity of freeform surfaces. Three case studies were used to demonstrate the CAICAx optimization in a virtual environment. The first demonstrator presents designed experimentations performed v"
pub.1130615321,3RD BIM/GIS INTEGRATION WORKSHOP AND 15TH 3DGEOINFO CONFERENCE 2020 – PREFACE,"Abstract. The interest in and use of 3D models in built environments is rapidly increasing, and they are now a key component of decision-making in areas including climate change mitigation (e.g., calculating solar panel potential, flood modelling, modelling housing age for retrofitting of thermal insulation), urban planning and cadastral systems (modelling rights, restrictions and responsibilities in complex buildings, streamlining the process to issue planning permits, design of existing or new developments) and infrastructure (construction, transport, utility management and modelling, asset management). 3D models are also an integrator for the data underpinning smart cities – knowing where a sensor is in 3D space allows the data to be integrated with the surrounding context – for example, noise data could be integrated with traffic information. Reflecting this interest, national mapping and cadastral agencies (NMCA) including Ordnance Survey (GB) are now increasingly generating 3D mapping at national scale, and there is extensive research as to how this data can be integrated with another emerging source of 3D models such as building information modelling (BIM).These trends were evident during the 3rd BIM/GIS Integration Workshop and 15th 3DGeoInfo 2020 events, which were co-hosted by University College London and Ordnance Survey (GB) in September 2020. The workshop and conference brought together international researchers from academia, industry, government and national mapping and cadastral agencies in the field of 3D geoinformation, in an interdisciplinary gathering of researchers in the fields of data collection, data management, data quality, data analysis, advanced modelling approaches, applications, users, visualisation, augmented reality, artificial intelligence and many more.This year’s theme was Users and Use Cases. The workshop and conference covered a wide range of topics including 3D data acquisition and processing, 3D city modelling and related standards, visualisation and dissemination of 3D data, augmented and virtual reality, 3D and Artificial Intelligence/Machine Learning. Three sessions of the BIM/GIS Integration Workshop were dedicated to Applications of BIM/GIS Integration, and an entire day of 3DGeoInfo 2020 to Users and Use Cases within 3DGeoInfo. Additionally, two sessions were specifically aimed at NMCA participants.Although initially intended to be a face-to-face event in London, the team rapidly adjusted to the emerging COVID-19 situation, identifying an online solution that facilitated and encouraged participant interaction. This meant that the events could still provide a platform for learning, discussion, and exchange of ideas that they have been able to in previous years, as well as providing opportunities to promote international collaboration in these topics. This special issue of the ISPRS International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences contains 23 papers selected by a "
pub.1122492410,Isopod,"Kubernetes is an open-source cluster orchestration system for containerized workloads to reduce idiosyncrasy across cloud vendors [2]. Using Kubernetes, Cruise has built a multi-tenant platform with thousands of cores and tens of terabytes of memory. Such a scale is possible in part thanks to the declarative abstraction of Kubernetes, where desired states are described in YAML manifests [5]. However, YAML as a data serialization format is unfit for workload specification. Structured data in YAML are untyped and prone to wrong indents and missing fields. Due to poor meta-programming support, composing YAML with control logic---loops and branches---suffers from YAML fragmentation and indentation tracking (example at bit.ly/yml-hell). Moreover, YAML manifests are often generated by filling a shared template with cluster-specific parameters---the image tag and the replica count might differ in development and production environments. Existing templating tools---Helm [11], Kustomize [9], Kapitan [7] and the likes---assume these parameters are statically known and use CLIs to query dynamic ones, such as secrets stored in HashiCorp Vault [10]. Such scheme is hard to test, since side effects escape through CLIs, and highly depends on the execution environment, since CLI versions vary across machines or might not exist. Not least, YAML manifests describe the eventual state but not how existing workloads will be affected. Blindly applying the manifest---for example, from a stale version of code---can be disastrous and cause unexpected outages. Isopod presents an alternative configuration paradigm by treating Kubernetes objects as first-class citizens. Without intermediate YAML artifacts, Isopod renders Kubernetes objects directly in Protocol Buffers [8], so they are strongly typed and consumed directly by the Kubernetes API. With Isopod, configurations are scripted in Starlark [3], a Python dialect by Google also used by Bazel [1] and Buck [4] build systems. To replace CLI dependencies, Isopod extends Starlark with runtime built-ins to access services and utilities such as Vault, Kubernetes apiserver, Base64 encoder, and UUID generator, etc. Isopod uses a separate runtime for unit tests to mock all built-ins, providing test coverage that was not possible before. Isopod is also hermetic and secure. The common reliance on the kubeconfig file for cluster authentication leaks secrets to disk, a security risk if working from a shared host, such as a cluster node or CICD worker. Instead, Isopod builds Oauth2 tokens [6] to the target cluster using the Identity & Access Management (IAM) service of the cloud vendor. Application secrets are stored in Vault and queried at runtime. Hence, no secrets escape to the disk. In fact, Isopod prohibits disk IO except for loading Starlark modules from other scripts. No external libraries can be loaded unless explicitly implemented as an Isopod built-in. Distributed as a single binary, Isopod is self-contained with all dependen"
pub.1124571981,Energy-Efficient Provisioning for Service Function Chains to Support Delay-Sensitive Applications in Network Function Virtualization,"The efficient deployment of virtual network functions (VNFs) for network service provisioning is key for achieving network function virtualization (NFV); however, most existing studies address only offline or one-off deployments of service function chains (SFCs) while neglecting the dynamic (i.e., online) deployment and expansion requirements. In particular, many methods of energy/resource cost reduction are achieved by merging VNFs. However, the energy waste and device wear for large-scale collections of servers (e.g., cloud networks and data centers) caused by sporadic request updating are ignored. To solve these problems, we propose an energy-aware routing and adaptive delayed shutdown (EAR-ADS) algorithm for dynamic SFC deployment, which includes the following features: 1) energy-aware routing (EAR): by considering a practical deployment environment, a flexible solution is developed based on reusing open servers and selecting paths with the aims of balancing energy and resources and minimizing the total cost and 2) adaptive delayed shutdown (ADS): the delayed shutdown time of the servers can be flexibly adjusted in accordance with the usage of each device in each time slot, thus eliminating the no-load wait time of the servers and frequent on/off switching. Therefore, the EAR-ADS can achieve dual-energy savings by both decreasing the number of open servers and reducing the idle/switching energy consumption of these servers. The simulation results show that EAR-ADS not only minimizes the cost of energy and resources but also achieves an excellent success rate and stability. Moreover, EAR-ADS is efficient compared with an improved Markov algorithm (SAMA), reducing the average deployment time by more than a factor of 40."
pub.1085538938,A Global Closed-Form Refinement for Consistent TLS Data Registration,"Existing global registration methods are prominently iterative. They require iterations and can be sensitive to point densities and noise. In contrast, closed-form solutions provide a more robust estimation model and do not involve iterations. In this letter, we present a global closed-form refinement for the terrestrial laser scanner (TLS) data registration problem. Our proposed method segments the task in three key steps. First, the method exploits a plane-based approach to compute the transformation parameters, resulting in the pairwise registration between point clouds. Second, we place all rotation parameters into a common coordinate system exploring one operation of the quaternions' properties. Third, we constrain the refined rotation parameters to globally refine the translation. The effectiveness of the proposed method is demonstrated with a TLS data set. Experiments have demonstrated that the proposed method can properly create a consistent 3-D map of outdoor environments with accuracy at the decimeter level."
pub.1096993977,Automated Modeling by Recognition,"Abstract Facility 3D Computer Aided Design (CAD) models are not updated regularly, or at all, once a facility goes into operations. In many brownfield assets, there is no 3D facility model to begin with. As-built models are required as a foundational technology that, through the virtual and visual experience of dynamic 3D models, will enable several business functions such as training and simulation, operations monitoring and surveillance, design modifications and project execution planning support, and hazard analysis and system reviews. In addition, the regulatory environment is changing and could result in the use of immersive 3D environments for operator competency assurance and various other compliance requirements, similar to the airline industry. While there are existing model update processes, such as automated pipe recognition, the creation of the as-built model is a very time-consuming and labor-intensive activity that prohibits broader adoption and use, especially on large offshore Major Capital Projects. The current objective of our R&D effort is to build an innovative modeling system, which combines data-driven and primitive-based modeling techniques, to automate the creation of 3D geometry models and texture maps from LiDAR (Light Detection and Ranging) point cloud data. In essence, this system acts as a highly automated bridge between ""dumb"" laser scan data and ""intelligent"" 3D models, enabling pervasive use of as-built models throughout an asset’s lifecycle."
pub.1173038631,Collaborative SLAM with Convolutional Neural Network-based Descriptor for Inter-Map Loop Closure Detection,"This paper introduces a novel Collaborative Si-multaneous Localization and Mapping (CSLAM) framework, enhanced with a Histogram of Oriented Gradients (HOG) de-scriptor, to improve Inter-Map Loop Closure Detection. Our framework stands out by integrating a convolutional neural network-based loop closure detection, employing the HOG de-scriptor for enhanced illumination robustness, and utilizing collaborative mapping from multiple robotic agents for refined pose estimations and mapping precision. Tested in diverse real-world fields, particularly for landmine detection, the framework demonstrates superior robustness and accuracy, outperforming the existing CCM-SLAM model. Additionally, it incorporates a transformation matrix from visual SLAM for LiDAR Point Clouds correction, showcasing its efficacy in 3D mapping and localization in GNSS-denied settings. Our results indicate that incorporating the CALC descriptor within a CSLAM system significantly enhances loop closure detection and mapping precision, marking a significant step forward in autonomous cooperative navigation and mapping in challenging environments"
pub.1152721550,Intelligent Digital Transformation Strategy Management: Development of a Measurement Framework,"Intelligent Digital transformation (DX) targets the implementation of interconnecting, smart, and self-controlled business processes by utilizing various technologies such as the Internet of Things (IoT), cloud computing, and data analytics. Organizations have been trying to reshape their business processes and transform them into a smart environment to have competitive advantages in the market. The literature review reveals a fundamental need for a measurement framework for intelligent DX strategy management to assist companies in measuring their current capabilities and guiding them to improve their existing situation in a standardized, objective, and more intelligent way. To address this research gap, this chapter proposes a measurement framework that aims to enable organizations to evaluate readiness and their current DX strategy capabilities. The framework consists of dimensions, corresponding sub-dimensions, and metrics to guide the organizations toward intelligent DX strategy management. The main contributions of the study are as follows: establishing a common base for performing an assessment for intelligent DX strategy management, benchmarking their capabilities with other organizations, and providing a roadmap towards achieving a higher capability level to maximize the economic benefits of DX. The measurement process is shown in order to demonstrate the applicability of the proposed measurement framework."
pub.1175900102,Reliable LiDAR-based ship detection and tracking for Autonomous Surface Vehicles in busy maritime environments,"Environmental perception is a crucial requirement of Autonomous Surface Vehicles (ASVs) if required to perform tasks safely in a dynamically complex operational environment. Most existing methods for ship detection rely on camera-based methods, which are sensitive to environmental conditions and cannot directly provide spatial location information related to detected targets. To overcome this limitation, we propose a LiDAR-based ship detection and tracking framework that can be applied to busy maritime environments. The proposed framework consists of two functional modules: a ship detection and multi-object tracking. For ship detection, a modularised network structure was adapted, allowing for ease of switching between different types of detection network to prioritise either detection accuracy, detection speed or a compromise of both, depending on the task requirements. A Kalman Filter-based multi-object tracking method is also implemented to compensate for any detections that may have been missed as a result of ship motions or occlusions, relying solely on the detection results. We also collected the first-ever real-world LiDAR dataset for maritime applications across the River Thames and marinas, including a range of ship types, with lengths ranging from 5 m up to 40 m, and different hull types. The datasets are organised in a similar manner to the KITTI datasets, which can be easily applied to the well-developed point cloud detection networks. Remarkably, our methods achieve an overall detection accuracy of 74.1% in the collected datasets. The proposed framework and dataset make LiDAR-based environmental perception feasible for implementation in ASVs and support development in the autonomous maritime navigation field."
pub.1031138848,GANDOLF: a system for generating automated nowcasts of convective precipitation,"Abstract During the past decade hydrologists have become increasingly aware of the problems of fluvial flood prediction during periods of intense convection, particularly in urbanised catchments whose rainfall‐runoff responses tend to be rapid. Existing approaches to deterministic, short‐range rainfall prediction are often deficient in their treatment of convective precipitation because they cannot resolve individual convective clouds or effectively model their evolution. In 1994 the UK Met. Office established a joint R&D programme with the Environment Agency (responsible for flood prediction in England and Wales) to explore the benefits of an Object‐Oriented conceptual Model (OOM) of convection in the nowcasting of fluvial floods. This involved the development of an automated nowcasting system (GANDOLF) designed to run the OOM during episodes of air mass convection. This paper describes the structure and function of the GANDOLF system and compares the performance of the OOM with that of two other precipitation models routinely used by Thames Region of the Agency. Copyright © 2000 Royal Meteorological Society"
pub.1158247320,Predictable inventory management within dairy supply chain operations," Purpose With the current wave of modernization in the dairy industry, the global dairy market has seen significant shifts. Making the most of inventory planning, machine learning (ML) maximizes the movement of commodities from one site to another. By facilitating waste reduction and quality improvement across numerous components, it reduces operational expenses. The focus of this study was to analyze existing dairy supply chain (DSC) optimization strategies and to look for ways in which DSC could be further improved. This study tends to enhance the operational excellence and continuous improvements of optimization strategies for DSC management   Design/methodology/approach Preferred reporting items for systematic reviews and meta-analyses (PRISMA) standards for systematic reviews are served as inspiration for the study's methodology. The accepted protocol for reporting evidence in systematic reviews and meta-analyses is PRISMA. Health sciences associations and publications support the standards. For this study, the authors relied on descriptive statistics.   Findings As a result of this modernization initiative, dairy sector has been able to boost operational efficiency by using cutting-edge optimization strategies. Historically, DSC researchers have relied on mathematical modeling tools, but recently authors have started using artificial intelligence (AI) and ML-based approaches. While mathematical modeling-based methods are still most often used, AI/ML-based methods are quickly becoming the preferred method. During the transit phase, cloud computing, shared databases and software actually transmit data to distributors, logistics companies and retailers. The company has developed comprehensive deployment, distribution and storage space selection methods as well as a supply chain road map.   Practical implications Many sorts of environmental degradation, including large emissions of greenhouse gases that fuel climate change, are caused by the dairy industry. The industry not only harms the environment, but it also causes a great deal of animal suffering. Smaller farms struggle to make milk at the low prices that large farms, which are frequently supported by subsidies and other financial incentives, set.   Originality/value This paper addresses a need in the dairy business by giving a primer on optimization methods and outlining how farmers and distributors may increase the efficiency of dairy processing facilities. The majority of the studies just briefly mentioned supply chain optimization. "
pub.1121058252,Video Verification in the Newsroom,"This chapter describes the integration of a video verification process into newsrooms of TV broadcasters or news agencies, which enables journalists to analyze and assess user-generated videos User-generated video (UGV)(UGV) from platforms such as YouTube, Facebook, or Twitter. We regard the organizational integration concerning the workflow, responsibility, and preparations as well as the inclusion of innovative verification tools and services into an existing IT environment. This includes the technical prerequisites required to connect the newsroom to video verification services in the cloud with the combined employment of third-party Web services for retrieval, analysis, or geolocation. We describe the different features to verify source, time, place, content, and rights of the video offered for journalists by the InVID Verification ApplicationInVID Verification ApplicationVerification App for short, which can serve as a blueprint for realizing a video verification process for professional newsroom systems. In the outlook, we discuss further potential to improve the current verification process through additional services, such as speech-to-text, OCR, translation, or deep fake detection."
pub.1166725287,MCFilter: feature filter based on motion-correlation for LiDAR SLAM," Purpose This study aims to introduce a novel noise filter module designed for LiDAR simultaneous localization and mapping (SLAM) systems. The primary objective is to enhance pose estimation accuracy and improve the overall system performance in outdoor environments.   Design/methodology/approach Distinct from traditional approaches, MCFilter emphasizes enhancing point cloud data quality at the pixel level. This framework hinges on two primary elements. First, the D-Tracker, a tracking algorithm, is grounded on multiresolution three-dimensional (3D) descriptors and adeptly maintains a balance between precision and efficiency. Second, the R-Filter introduces a pixel-level attribute named motion-correlation, which effectively identifies and removes dynamic points. Furthermore, designed as a modular component, MCFilter ensures seamless integration into existing LiDAR SLAM systems.   Findings Based on rigorous testing with public data sets and real-world conditions, the MCFilter reported an increase in average accuracy of 12.39% and reduced processing time by 24.18%. These outcomes emphasize the method’s effectiveness in refining the performance of current LiDAR SLAM systems.   Originality/value In this study, the authors present a novel 3D descriptor tracker designed for consistent feature point matching across successive frames. The authors also propose an innovative attribute to detect and eliminate noise points. Experimental results demonstrate that integrating this method into existing LiDAR SLAM systems yields state-of-the-art performance. "
pub.1156511984,Secure Authentication in IoT Based Healthcare Management Environment Using Integrated Fog Computing Enabled Blockchain System,"With internet of things (IoT) tools, human health management is becoming more closely integrated with security today. Through these IoT enabled devices (such as embedded and wearable devices), the patient’s physical health status can be monitored at different time intervals. These devices are able to collect and store patient health data on local and cloud networks, which also assist health providers in real-time analysis and decision-making processes. Although health centers have been using IoT devices for many years now, they have also included these devices to take care of the patient’s health through the patient’s local area and body network. However, medical agencies, hospitals and companies are not much concerned about the security risk of these healthcare IoT devices, which are connected to cloud networks to store data. The easy hacking of these healthcare devices is a major concern, and poor authentication and encryption practices can lead to a number of potentially life-threatening risks. Existing traditional solutions, including machine learning algorithms, symmetric encryption, and access control for transmission of data for healthcare, lack real-word implementations for secure data transmission and even a single point of failure. Therefore, blockchain has been introduced as a decentralized framework to establish secure and reliable transactions in healthcare IoT. Whereas, fog computing has been introduced to extend cloud network services at the edge of the network. The integration of fog computing with blockchain has been able to provide an innovative way to address this issue of healthcare IoT device identification, authentication and verification for scalable persistent data transmission in a decentralized analytical model, a mathematical framework, and an advanced signature-based encryption algorithm to collect patient healthcare data through healthcare enabled IoT devices including authentication, identification and verification process."
pub.1166850136,GRU-based digital twin framework for data allocation and storage in IoT-enabled smart home networks,"In recent years, the Internet of Things (IoT) devices utilization with Information Communication Technology (ICT) has grown exponentially in various Smart City applications, including Smart Homes, Smart Enterprises, and others. The fusion of IoT, ICT, and Smart Home delivers interactive solutions to reduce costs and resource consumption, enhance performance, and engage people's needs more virtually and proactively. A smart home has numerous advantages with the integration of emerging advanced technologies. Big data, centralization, data and resource allocation, security, and privacy issues persist as challenges in IoT-enabled Smart Home Networks. To address these challenges, in this paper, we propose a GRU-based Digital Twin Framework for Data Allocation in IoT-enabled Smart Home Networks. Data and resource allocation of smart home applications are completed at the virtual twin layer using Gated Recurrent Unit (GRU)-based Digital Twin Networks. Low-priority data is stored and processed at the Macro-based Stations (MBSs), and high-priority data is transferred to the upper (Security) layer for authentication and validation. Blockchain-based distributed networks are utilized for Smart Home Data authentication at the security layer with a Proof of Authentication (PoAh) Consensus Algorithm; Data is stored at the cloud layer after validation. The validation results of the proposed framework demonstrate superior performance as the quantitative analysis with accuracy 0.9412, Root Mean Square Error (RMSE) 0.0588 for IoT-enable Smart Home compared to existing works as LSTM-based Digital Twin network and provide a secure environment in IoT-enabled Smart Home."
pub.1014827071,Recent changes in wetlands on the Tibetan Plateau: A review,"About 80% of global wetland resources are degrading or disappearing; thus the wetland ecosystem has become one of the most seriously threatened ecosystems in the world. As an area sensitive to global changes and acting as a security barrier for the Asian ecosystem, the Tibetan Plateau has about 13.19×104 km2 of wetlands of special significance within China. With the increasing application of remote sensing technology to wetland research, Tibetan Plateau wetland research has entered a period of rapid development. This paper summarizes the remote sensing research literature of the Tibetan Plateau wetlands from 1992 to 2014, and is intended to provide references for future research into the wetlands of the Tibetan Plateau. We have reviewed monitoring methods, research topics, and existing problems. Our review has revealed the following characteristics: (1) Over the past 40 years, the research paradigm of the Tibetan Plateau wetlands has undergone dynamic changes in the monitoring of wetland areas, landscape patterns and the eco-environment based on remote sensing technology. Attention has also been focused on constructing models with an ecological system perspective and analyzing three patterns of change trends within the Tibetan Plateau wetlands. (2) The results of Tibetan Plateau wetland research based on remote sensing were as follows: (a) between 1970 and 2006, the Tibetan Plateau wetland area decreased overall at a rate of 0.23%/a, and the landscape diversity declined at a rate of 0.17%/a; (b) by contrast, between 1976 and 2009, the lake area of the inland river basins in the Tibetan Plateau increased at a rate of 0.83%/a; and (c) the change trend in the Tibetan Plateau wetlands was controlled by climate change. Current problems relating to remote sensing (RS)-based research in the Tibetan Plateau wetlands are computer interpretation accuracy and the processing precision of cloud removal, and the lack of a comprehensive overview of the Tibetan Plateau wetland system. Finally, based on the review, some key activities for future study have been proposed, as follows: (1) Strengthening the integration of the Tibetan Plateau wetland research with remote sensing research; (2) discussing the response and adaptation mechanisms of the Tibetan Plateau wetland ecosystem within the context of global change; (3) strengthening the integration of remote sensing (RS), geographic information system (GIS), and global positioning system (GPS), and promoting the construction of a Tibetan Plateau wetland information platform."
pub.1176237391,Leveraging Procore for Improved Collaboration and Communication in Multi-Stakeholder Construction Projects,"The multifaceted roles of the contractor, architect, engineer, and project owner are fundamental in effective successful accomplishment of a construction project. Procore is a cloud-based construction management solution that provides the perfect amalgamation of these critical attributes of project management. As outlined in this report, Procore enables improved communication, document sharing, and task streamlining for improved project outcomes. These include Real-time Communication Capability, Centralized Document Management, and Mobile Access. Procore helps work more effectively-not to delay anything-and not to make errors. More importantly, it will discuss how the collection feature of the Procore data can be utilized by project managers for tracking the way work gets done, tasks accomplished, and clear reports that give insight to the performance of the projects. While much benefit exists in Procore, several issues related to implementing existing issues such as training and integration into existing systems add an extra challenge. However, despite these complexities, successful implementation of the product takes place in several projects that make completion faster and communications better. The final report concludes with a summary summarizing that Procore is of significant influence to the use thereof in enhancing collaborative communication in complex construction environments. Consideration has also been given towards future enhancements including AI and VR integration that could further enhance the ability of Procore to optimize project management within the construction industry."
pub.1007808383,Biomedical Ontology Quality Assurance Using a Big Data Approach,"This article presents recent progresses made in using scalable cloud computing environment, Hadoop and MapReduce, to perform ontology quality assurance (OQA), and points to areas of future opportunity. The standard sequential approach used for implementing OQA methods can take weeks if not months for exhaustive analyses for large biomedical ontological systems. With OQA methods newly implemented using massively parallel algorithms in the MapReduce framework, several orders of magnitude in speed-up can be achieved (e.g., from three months to three hours). Such dramatically reduced time makes it feasible not only to perform exhaustive structural analysis of large ontological hierarchies, but also to systematically track structural changes between versions for evolutional analysis. As an exemplar, progress is reported in using MapReduce to perform evolutional analysis and visualization on the Systemized Nomenclature of Medicine—Clinical Terms (SNOMED CT), a prominent clinical terminology system. Future opportunities in three areas are described: one is to extend the scope of MapReduce-based approach to existing OQA methods, especially for automated exhaustive structural analysis. The second is to apply our proposed MapReduce Pipeline for Lattice-based Evaluation (MaPLE) approach, demonstrated as an exemplar method for SNOMED CT, to other biomedical ontologies. The third area is to develop interfaces for reviewing results obtained by OQA methods and for visualizing ontological alignment and evolution, which can also take advantage of cloud computing technology to systematically pre-compute computationally intensive jobs in order to increase performance during user interactions with the visualization interface. Advances in these directions are expected to better support the ontological engineering lifecycle."
pub.1141770708,[Retracted] Conceptual Implementation of Artificial Intelligent based E‐Mobility Controller in smart city Environment,"Testing and implementation of integrated and intelligent transport systems (IITS) of an electrical vehicle need many high‐performance and high‐precision subsystems. The existing systems confine themselves with limited features and have driving range anxiety, charging and discharging time issues, and inter‐ and intravehicle communication problems. The above issues are the critical barriers to the penetration of EVs with a smart grid. This paper proposes the concepts which consist of connected vehicles that exploit vehicular ad hoc network (VANET) communication, embedded system integrated with sensors which acquire the static and dynamic parameter of the electrical vehicle, and cloud integration and dig data analytics tools. Vehicle control information is generated based on machine learning‐based control systems. This paper also focuses on improving the overall performance (discharge time and cycle life) of a lithium ion battery, increasing the range of the electric vehicle, enhancing the safety of the battery that acquires the static and dynamic parameter and driving pattern of the electrical vehicle, establishing vehicular ad hoc network (VANET) communication, and handling and analyzing the acquired data with the help of various artificial big data analytics techniques."
pub.1125799010,Physical Design for 3D Chiplets and System Integration,"The convergence of 5G and Artificial Intelligence (AI) that covers the gamut from cloud data centers through network routers to edge applications is poised to open possibilities beyond our imagination and transform how we will go about our daily lives. As the foundational technology supporting 5G and AI innovation, semiconductors strive for greater system performance and broader bandwidth, while increasing functionality and lowering cost. In response, device innovation is transitioning from SoCs to 3D chiplets that combine advanced wafer-level system integration (WLSI) technologies such as CoWoS® (Chip on Wafer on Substrate), Integrated Fan-Out (InFO), Wafer-on-Wafer (WoW) and System-on-Integrated-Chips (SoIC), to enable system integration that meets these demands. Designing 3D chiplets and housing various chips on wafer-level for system integration creates a whole new set of challenges. These start with design partitioning and include handling interfaces between or passing through chips, design for testing (DFT), thermal dissipation, databases and tools integration for chip and packaging design, new IO/ESD (electrostatic discharge), simulation run time and tool capacity, among others. Considering current capabilities and constraints, divide-and-conquer remains the most feasible approach for 3D chiplet design and packaging. Chiplet design needs to integrate data bases and tools with packaging environments for both verification and optimization. Leveraging existing 2D physical design solutions and chip-level abstraction can help meet 3D verification and optimization requirements. The IC industry also needs more DFT and thermal dissipation innovation, especially the latter one. Thermal optimization is critical to 3D chiplets and system integration. The current thermal solution only covers thermal analysis + system-level thermal dissipation. It should start at the IPs and across chip design process, i.e., thermal-aware 3D IC design, to cover IP, macros, and transistors. This speech will address these and other challenges, then propose physical design solutions for 3D chiplets and system integration. CCS CONCEPTS - VLSI design, 3D integrated circuits, VLSI system specification and constraints, and VLSI packaging KEYWORDS Physical design, 3D chiplets and system integration, thermal optimization BIOGRAPHY Dr. Cliff Hou was appointed Vice President of Research and Development at Taiwan Semiconductor Manufacturing Co. Ltd. (TSMC) in 2011. Since 1999, he has worked to establish node-specific reference flows from 0.13μm to today's leading-edge 3nm at TSMC. Dr. Hou also led TSMC's in-house IP development teams from 2008 to 2010. He is now spearheading TSMC's efforts to build total platform solutions for the industry's high growth markets in Mobile, IoT, Automotive, and High-Performance Computing. Dr. Hou holds 44 U.S. Patents and serves as a member of Board of Directors in Global Unichip Corp. He received B.S. degree in Control Engineering from Taiwan's Nat"
pub.1165654235,Real-time stored product insect detection and identification using deep learning: System integration and extensibility to mobile platforms,"Existing stored product insect monitoring methods are time-consuming, costly, and often require specialized equipment or training. This study proposed an integrated insect monitoring system that employs a simple RGB camera and data-driven deep-learning models to detect and identify stored product insect species in warehouses, food facilities, and retail environments. Top-down images of six common insect species were acquired with a setup simulating a conceptualized probe-type monitor under varying lighting and background conditions. These images were preprocessed and manually annotated, resulting in an insect dataset of 2630 images with 14,509 labeled insects. A state-of-the-art computer vision model from YOLO family was selected, and six YOLO variants (YOLOv5s/m/l and YOLOv5s/m/l) were trained and evaluated on the insect dataset. All trained YOLO models delivered an impressive performance in terms of high detection accuracy (above 76% for mAP@[0.50:0.95]) and fast inference time (12–36 ms range). Subsequently, the best-performing YOLOv8l model was integrated and deployed on a mobile device, achieving a good detection performance with average detection speeds of 16 and 29 fps on a desktop computer and smartphone, respectively. The study provided an end-to-end framework for automatic and real-time insect detection and identification in a stored product environment. The lightweight variant of YOLO can be deployed on low-cost edge hardware, mobile devices, or cloud computing. The proposed system is relatively fast, accurate, and inexpensive for insect monitoring and may prove an alternate solution to existing methods. The system would serve as a decision-making tool for stored product facilities managers and can be easily scaled and adapted in a variety of stored product environments."
pub.1092476254,S 3 ORAM,"Oblivious Random Access Machine (ORAM) enables a client to access her data without leaking her access patterns. Existing client-efficient ORAMs either achieve O(log N) client-server communication blowup without heavy computation, or O(1) blowup but with expensive homomorphic encryptions. It has been shown that O(log N) bandwidth blowup might not be practical for certain applications, while schemes with O(1) communication blowup incur even more delay due to costly homomorphic operations. In this paper, we propose a new distributed ORAM scheme referred to as Shamir Secret Sharing ORAM (S3ORAM), which achieves O(1) client-server bandwidth blowup and O(1) blocks of client storage without relying on costly partial homomorphic encryptions. S3ORAM harnesses Shamir Secret Sharing, tree-based ORAM structure and a secure multi-party multiplication protocol to eliminate costly homomorphic operations and, therefore, achieves O(1) client-server bandwidth blowup with a high computational efficiency. We conducted comprehensive experiments to assess the performance of S3ORAM and its counterparts on actual cloud environments, and showed that S3ORAM achieves three orders of magnitude lower end-to-end delay compared to alternatives with O(1) client communication blowup (Onion-ORAM), while it is one order of magnitude faster than Path-ORAM for a network with a moderate bandwidth quality. We have released the implementation of S3ORAM for further improvement and adaptation."
pub.1147832454,An Ameliorated Multiattack Network Anomaly Detection in Distributed Big Data System-Based Enhanced Stacking Multiple Binary Classifiers,"The growth of the Internet of Things (IoT) generates new processing, networking infrastructure, data storage, and management capabilities. This massive volume of data may be used to provide high-value information for decision support and data-intensive science research, etc. However, owing to the nature of IoT in distribution, virtualisation, cloud integration, and internet connectivity, the IoT environment is prone to various cyber-attacks and security issues. Hence, the increasing frequency and potency of recent attacks and constantly evolving attack vectors necessitate the development of improved detection methods. Therefore, this study proposes a distributed computing-based security model to safeguard big data systems. The proposed ensemble multi binary attack model (EMBAM) is an intrusion detection system (IDS) that offers a unique anomaly based IDS to detect normal behaviour and abnormal attack(s), for example, threats in a network. EMBAM ensembles multiple binary classifiers into a single model through stacking. The core binary model is a decision tree classifier with hyperparameters optimised using the grid search method. The use of multiple binary classifiers allows each binary classifier to adopt the limitations of the others. Empirical analysis of the experimental profile of the EMBAM has been discussed with eight-plus state-of-the-art methods using performance metrics, such as accuracy, detection rate, precision, specificity, false alarm rate, and F1-score. EMBAM can recognise multiple attack types as a star plug and play advantageous in a highly dynamic scheme. The proposed approach outperforms existing approaches on the UNSW-NB15 dataset and yields competitive results on the CICIDS2017 dataset."
pub.1050181080,Hypervisor Memory Forensics,"Memory forensics is the branch of computer forensics that aims at extracting artifacts from memory snapshots taken from a running system. Even though it is a relatively recent field, it is rapidly growing and it is attracting considerable attention from both industrial and academic researchers.In this paper, we present a set of techniques to extend the field of memory forensics toward the analysis of hypervisors and virtual machines. With the increasing adoption of virtualization techniques (both as part of the cloud and in normal desktop environments), we believe that memory forensics will soon play a very important role in many investigations that involve virtual environments.Our approach, implemented in an open source tool as an extension of the Volatility framework, is designed to detect both the existence and the characteristics of any hypervisor that uses the Intel VT-x technology. It also supports the analysis of nested virtualization and it is able to infer the hierarchy of multiple hypervisors and virtual machines. Finally, by exploiting the techniques presented in this paper, our tool can reconstruct the address space of a virtual machine in order to transparently support any existing Volatility plugin - allowing analysts to reuse their code for the analysis of virtual environments."
pub.1131862316,"SQL Server Data Automation Through Frameworks, Building Metadata-Driven Frameworks with T-SQL, SSIS, and Azure Data Factory","Learn to automate SQL Server operations using frameworks built from metadata-driven stored procedures and SQL Server Integration Services (SSIS). Bring all the power of Transact-SQL (T-SQL) and Microsoft .NET to bear on your repetitive data, data integration, and ETL processes. Do this for no added cost over what you’ve already spent on licensing SQL Server. The tools and methods from this book may be applied to on-premises and Azure SQL Server instances. The SSIS framework from this book works in Azure Data Factory (ADF) and provides DevOps personnel the ability to execute child packages outside a project—functionality not natively available in SSIS. Frameworks not only reduce the time required to deliver enterprise functionality, but can also accelerate troubleshooting and problem resolution. You'll learn in this book how frameworks also improve code quality by using metadata to drive processes. Much of the work performed by data professionals can be classified as “drudge work”—tasks that are repetitive and template-based. The frameworks-based approach shown in this book helps you to avoid that drudgery by turning repetitive tasks into ""one and done"" operations. Frameworks as described in this book also support enterprise DevOps with built-in logging functionality. You will: Create a stored procedure framework to automate SQL process execution Base your framework on a working system of stored procedures and execution logging Create an SSIS framework to reduce the complexity of executing multiple SSIS packages Deploy stored procedure and SSIS frameworks to Azure Data Factory environments in the cloud"
pub.1121664553,Mobile Cancer Prophecy System to Assist Patients: Big Data Analysis and Design,"The growth of cancer in India is growing hastily in recent years. Efficient monitoring and medication procedures are needed in high demand. Recent research states diagnose of cancer during its early break through will prevent mortality. The evolution of smart mobile devices paves its
 mutual focus in healthcare sectors. In this paper, an Intellectual model of disease diagnosis using the advantage of smart mobile devices has been proposed. This mobile based cancer diagnosis model uses a cloud environment for disease prediction and analysis. The Principal Component Analysis
 (PCA) technique is utilized to confiscate the superfluous features and choose the most appropriate features. Using the optimized features, cancer disease classification is accomplished using Support Vector Machines with sigmoid kernel function. SVM classifies the patients as normal and abnormal
 and the evaluated results are conveyed to the patients as well as the respective medical practitioners. The accuracy achieved through proposed model is satisfiable in comparison with other existing methods. Proposed Model incorporates with big data technologies to address the current issues
 of cancer system."
pub.1175163520,A Secure Framework for Continuous Compliance across Heterogeneous Policy Validation Points,"Regulated compliance has become a business liabil-ity given the constant changes to the IT environments driven by the new Cloud normal enabling daily upgrades, and by the new programs related to cybersecurity. To keep pace with the changes, enterprises and auditor agencies shifted from annual audit to continuous compliance. Critical enablers of continuous compliance are standardization and automation. To achieve this, we need to treat everything as code, from the compliance artifacts to the policies and their results. Currently, Compliance as Code (CaC) is utilized to represent compliance artifacts and processes, while Policy as Code (PaC) is employed to express the logic of validating the actual state of the systems against the desired state. Although there is emerging technology for codifying compliance and policy, there are challenges that hinder the compliance digitization transformation towards continuous compliance. The first challenge is how to migrate from document-based operations to the program-based operations. The second challenge arises from the disconnect between CaC and the varied native interfaces of existing PaC solutions. Lastly, there is a concern regarding the reliability of results from end-to-end automation when replacing human-in-the-loop overseeing and guaranteeing the integrity with fully automated process. To address these challenges, we have developed a GitOps-based secure pipeline framework, seamlessly integrating compliance authoring with CaC while ensuring the secure integration of Compliance as Code and Policy as Code, protecting data integrity and preserving data traceability. In this paper, we present the details of the framework which enables end-to-end automation of compliance processes while supporting integration with various Policy Validation Points (PVPs), and showcase the evaluation of our implemented solution's reliability by testing risks such as unintentional modification of policies."
pub.1164192815,Camera-Lidar Extrinsic Calibration via Traffic Signs,"To address the problem of how to carry out camera and 2D Lidar calibration in general natural environment, this paper proposes a method that can be calibrated based on various traffic signs on the road. Firstly, this method utilizes the features of the traffic signs extracted from the image plane. In order to obtain accurate traffic signs edge features, the guided filtering of images and Otsu algorithm are used to extract the traffic signs area. Then this method utilizes Sobel operator and Hough transform to extract the straight features, utilizes Canny operator and arc-support line segments to extract the curve features. Secondly, aiming at the problem of how to find the corresponding relationship between laser points and the points on the camera image when the scanning laser of 2D Lidar are not visible, the angle information collected by 2D Lidar and the linear fitting of the point cloud collected by laser is used to find the intersection point between the laser scanning plane and the edge of traffic signs. Finally, the projective matrix of the camera and Lidar is solved by using the Point-to-Line geometric constraints for the intersection points and the edge features of traffic signs, and this method utilizes the internal reference matrix of the camera and the projective matrix to obtain the rigid transformation matrix from the Lidar coordinate system to the camera coordinate system. The experimental results are analyzed and it is concluded that this method is more convenient to realize the external parameter calibration of Lidar and camera under the premise of maintaining a certain accuracy."
pub.1149798156,A Secret-Free Hypervisor: Rethinking Isolation in the Age of Speculative Vulnerabilities,"In recent years, the epidemic of speculative side channels significantly increases the difficulty in enforcing domain isolation boundaries in a virtualized cloud environment. Although mitigations exist, the approach taken by the industry is neither a long-term nor a scalable solution, as we target each vulnerability with specific mitigations that add up to substantial performance penalties. We propose a different approach to secret isolation: guaranteeing that the hypervisor is Secret-Free (SF). A Secret-Free design partitions memory into secrets and non-secrets and reconstructs hypervisor isolation. It enforces that all domains have a minimal and secret-free view of the address space. In contrast to state-of-the-art, a Secret-Free hypervisor does not identify secrets to be hidden, but instead identifies non-secrets that can be shared, and only grants access necessary for the current operation, an allow-list approach. SF designs function with existing hardware and do not exhibit noticeable performance penalties in production workloads versus the unmitigated baseline, and outperform state-of-the-art techniques by allowing speculative execution where secrets are invisible. We implement SF in Xen (a Type-I hypervisor) to demonstrate that the design applies well to a commercial hypervisor. Evaluation shows performance comparable to baseline and up to 37% improvement in certain hypervisor paths compared with Xen default mitigations. Further, we demonstrate Secret-Free is a generic kernel isolation infrastructure for a variety of systems, not limited to Type-I hypervisors. We apply the same model in Hyper-V (Type-I), bhyve (Type-II) and FreeBSD (UNIX kernel) to evaluate its applicability and effectiveness. The successful implementations on these systems prove the generality of SF, and reveal the specific adaptations and optimizations required for each type of kernel."
pub.1172277445,Industry Automation: The Contributions of Artificial Intelligence,"In the last decade, there has been a tremendous technological breakthrough in almost any industry, like healthcare, the military, education, and manufacturing industries, which includes new gadgets, discoveries, and refinements in the existing system. The current implementation of 5G technology tells us that Industry 5.0 has stormed into the market, where people are working with smart machines and robots. This also addresses the automated information exchange mechanism and other various technologies. So, in order to create smart factories and industries, it connects the ideas of the Internet of Things, cloud technology, and cognitive computing. Industry 5.0 ties together physical and digital advancements to build such intelligent businesses. Furthermore, it simplifies and maintains the actual needs as well as the supply chain in businesses. Companies and organizations profit from it because of its efficiency and capacity for quick judgments. Artificial intelligence is the core of this technology and a crucial factor in Industry 5.0. AI manages every technological item we use nowadays. The environment significantly impacts AI since every piece of data is linked to the Internet. A range of enterprises, such as those associated with manufacturing, e-commerce, sports, security, and defense, have shown the efficacy of AI due to its malleability. Thanks to 58the combination of Industry 5.0 and AI, almost all applications can access various options and services. Even though Industry 5.0 has demonstrated great success across all domains, it is still in the early stages of its extension, adoption, and hiring. We researched AI and Industry 5.0 in this work and explored their difficulties, significance, methods, and results. The relevance of innovation in the context of industry and everyday life has also been examined, along with its impacts and advantages."
pub.1166252761,Progression and Challenges of IoT in Healthcare: A Short Review,"Smart healthcare, an integral element of connected living, plays a pivotal
role in fulfilling a fundamental human need. The burgeoning field of smart
healthcare is poised to generate substantial revenue in the foreseeable future.
Its multifaceted framework encompasses vital components such as the Internet of
Things (IoT), medical sensors, artificial intelligence (AI), edge and cloud
computing, as well as next-generation wireless communication technologies. Many
research papers discuss smart healthcare and healthcare more broadly. Numerous
nations have strategically deployed the Internet of Medical Things (IoMT)
alongside other measures to combat the propagation of COVID-19. This combined
effort has not only enhanced the safety of frontline healthcare workers but has
also augmented the overall efficacy in managing the pandemic, subsequently
reducing its impact on human lives and mortality rates. Remarkable strides have
been made in both applications and technology within the IoMT domain. However,
it is imperative to acknowledge that this technological advancement has
introduced certain challenges, particularly in the realm of security. The rapid
and extensive adoption of IoMT worldwide has magnified issues related to
security and privacy. These encompass a spectrum of concerns, ranging from
replay attacks, man-in-the-middle attacks, impersonation, privileged insider
threats, remote hijacking, password guessing, and denial of service (DoS)
attacks, to malware incursions. In this comprehensive review, we undertake a
comparative analysis of existing strategies designed for the detection and
prevention of malware in IoT environments."
pub.1176176531,CREATE SOLUTIONS FOR VERSIONING AND MANAGING DATASETS USED IN AI AND ML.,"It is also essential to correctly version and manage datasets to make them easily recognizable, traceable, and sharable throughout the various stages of AI & ML model development. Notably, there are many solutions to dataset versioning and management, with the best one touching on existing machine learning pipelines, highlighted by tools like DVC and MLflow, in this paper. To achieve this, the study provides simulation reports on using these tools in the current dynamic data environments, including healthcare, finance, and e-commerce, requiring robust version control mechanisms to counter quickly evolving data. Potential issues such as scale, data accuracy, and compatibility with present system adoptions are discerned with suggested solutions such as cloud-based management, checks and balances on data integrity, and ease of integration. The use of visuals shows how data lineage visualization helps in understanding the data flow for better implementation of measures and how different versioning tools compare in performance. The conclusions drawn from the study pertain to the fact that the implementation of structured data versioning strategies contributes to the enhancement of model quality and efficiency in addition to enhancing interaction between data scientists and engineers. This research finds that proper methods of developing and applying data versioning and data management practices are critical for effectively implementing AI and ML models in complex ecosystems that make decisions based on the most contemporary data. Future work will investigate the applicability of these tools as the number of data points to process increases, as well as the variability of those data points."
pub.1019480773,Development and Nondevelopment of Binary Mesoscale Vortices into Tropical Cyclones in Idealized Numerical Experiments,"Abstract
                  The evolution of two symmetric midlevel mesoscale vortices situated above a warm ocean is examined with a basic cloud-resolving model. Idealized numerical experiments provide insight into how the evolution may vary with the initial vortex separation distance D and other parameters that influence the time scale for an isolated vortex to begin rapid intensification. The latter parameters include the ambient middle-tropospheric relative humidity (RH) and the initial midlevel wind speed of each vortex. At relatively low RH, there exists an interval of D where binary midlevel vortex interaction prevents tropical cyclone formation. While tropical cyclones generally develop at high RH, similar values of D can delay the process if the vortices are initially weak. Prevention or inhibition of tropical cyclone formation occurs in association with the outward expulsion of lower-tropospheric potential vorticity anomalies as the two vortices merge in the middle troposphere. It is proposed that the primary mechanism for midlevel merger and low-level potential vorticity expulsion involves the excitation of rotating misalignments in each vortex. An analog model based on this premise provides a good approximation for the range of D in which the merger–expulsion scenario occurs. Relatively strong vortices in high-RH environments promptly develop vigorous convection and begin rapid intensification. Differences between the interaction of such diabatic vortices and their adiabatic counterparts are briefly illustrated. In systems that generate tropical cyclones, the mature vortex properties (size and strength) are found to vary significantly with D."
pub.1176104684,CREATE SOLUTIONS FOR VERSIONING AND MANAGING DATASETS USED IN AI AND ML.,"It is also essential to correctly version and manage datasets to make them easily recognizable, traceable, and sharable throughout the various stages of AI & ML model development. Notably, there are many solutions to dataset versioning and management, with the best one touching on existing machine learning pipelines, highlighted by tools like DVC and MLflow, in this paper. To achieve this, the study provides simulation reports on using these tools in the current dynamic data environments, including healthcare, finance, and e-commerce, requiring robust version control mechanisms to counter quickly evolving data. Potential issues such as scale, data accuracy, and compatibility with present system adoptions are discerned with suggested solutions such as cloud-based management, checks and balances on data integrity, and ease of integration. The use of visuals shows how data lineage visualization helps in understanding the data flow for better implementation of measures and how different versioning tools compare in performance. The conclusions drawn from the study pertain to the fact that the implementation of structured data versioning strategies contributes to the enhancement of model quality and efficiency in addition to enhancing interaction between data scientists and engineers. This research finds that proper methods of developing and applying data versioning and data management practices are critical for effectively implementing AI and ML models in complex ecosystems that make decisions based on the most contemporary data. Future work will investigate the applicability of these tools as the number of data points to process increases, as well as the variability of those data points."
pub.1172580868,Contextualizing Polarimetric Retrievals of Boundary Layer Height Using State-of-the-Art Boundary Layer Profiling,"Abstract  Knowledge about the depth of the planetary boundary layer (PBL) is crucial for a variety of applications, but direct observations of PBL depth are spatiotemporally sparse. Recent studies have proposed using operational dual-polarization weather radars to observe the evolution of PBL depth by capitalizing on unique differential reflectivity ( Z DR ) signatures of Bragg scatter at the top of the PBL. While this approach appears promising and cost-effective, uncertainties remain about the representativeness of these estimates and how its efficacy may vary by geography and climatology. To address these outstanding uncertainties, this study compares collocated observations collected from two WSR-88D radars and two state-of-the-art mobile boundary layer profiling systems and evaluates the proposed methodology over the full diurnal cycle. Results indicate good overall correspondence between the profiling- and radar-based PBL depth estimates, with an abrupt divergence during the early evening transition and large discrepancies overnight. Relatively large root-mean-square-deviations (RMSDs) coupled with small biases match expectations when comparing spatially averaged data with point observations during PBL growth, which capture frequent fluctuations. A qualitative examination of the radar data reveals signatures of elevated residual layers, clouds, and ground clutter, all of which can obfuscate the desired surface-based PBL signal but which may have their own utility. The prominence of the Bragg scatter signal is found to be correlated with the observed moisture gradient at the top of the PBL, reflecting climatological variability that should be considered. These findings motivate further work to improve the automated detection of Bragg scatter layers from polarimetric radar data.   Significance Statement Knowledge of the height of the planetary boundary layer matters for weather forecasting, air quality, and renewable energy production. Currently, boundary layer height measurements are taken at select locations twice a day. However, a method to use the existing national network of polarimetric weather radars for this purpose has been proposed. This work evaluates this method against specialized boundary layer measurements. The results show that the method is generally reliable during the daytime and could be used for a variety of applications including climatologies and model evaluation. There remain a number of situational caveats, including residual turbulence, clouds/precipitation, ground clutter, and certain meteorological environments, that may require modification of the approach and need to be considered in future work. "
pub.1131272286,Modeling cloud-to-ground lightning probability in Alaskan tundra through the integration of Weather Research and Forecast (WRF) model and machine learning method,"Wildland fires exert substantial impacts on tundra ecosystems of the high northern latitudes (HNL), ranging from biogeochemical impact on climate system to habitat suitability for various species. Cloud-to-ground (CG) lightning is the primary ignition source of wildfires. It is critical to understand mechanisms and factors driving lightning strikes in this cold, treeless environment to support operational modeling and forecasting of fire activity. Existing studies on lightning strikes primarily focus on Alaskan and Canadian boreal forests where land-atmospheric interactions are different and, thus, not likely to represent tundra conditions. In this study, we designed an empirical-dynamical method integrating Weather Research and Forecast (WRF) simulation and machine learning algorithm to model the probability of lightning strikes across Alaskan tundra between 2001 and 2017. We recommended using Thompson 2-moment and MellorYamadaJanjic schemes as microphysics and planetary boundary layer parameterizations for WRF simulations in the tundra. Our modeling and forecasting test results have shown a strong capability to predict CG lightning probability in Alaskan tundra, with the values of area under the receiver operator characteristics curves above 0.9. We found that parcel lifted index and vertical profiles of atmospheric variables, including geopotential height, dew point temperature, relative humidity, and velocity speed, important in predicting lightning occurrence, suggesting the key role of convection in lightning formation in the tundra. Our method can be applied to data-scarce regions and support future studies of fire potential in the HNL."
pub.1151295907,Virtual European Solar & Planetary Access (VESPA) 2022: Sustainability,"<p>VESPA (Virtual European Solar and Planetary Access) has focused for 10 years on adapting Virtual Observatory (VO) techniques to handle Planetary Science data [1] [2]. The objective of this activity is to build a contributory data distribution system both to access and publish data with minimum fuss. This system is responsive to the new paradigm of Open Science and FAIR access to the data, and is optimized to publish data from public-funded programmes with limited resources.</p>
<p>VESPA’s architecture was defined during the previous Europlanet-2020-RI program, incorporating concepts and standards from various areas: astronomy, Earth observation, space physics, heliophysics, etc. It relies on the VO infrastructure: data services are installed in any location but are declared in a system of harvested registries with identifiers, end-point (URL), mention of supported access protocols, and a rough description of content. Such services are interoperable via clients and tools, which also provide visualization and analysis functions.</p>
<p>The activity in Europlanet-2024-RI focuses on expanding this environment, enforcing sustainability, and opening new possibilities to improve data handling – such as workflows, cloud-based computation, and readiness for exploitation through Machine Learning techniques.</p>
<p><strong>Data access</strong>. VESPA uses a specific access protocol called EPN-TAP, associated with a metadata vocabulary providing uniform description of datasets in the field. At the time of writing EPN-TAP is in the final stage of becoming a Recommendation of the International Virtual Observatory Alliance (IVOA) [3].</p>
<p>EPN-TAP is compliant with the general TAP protocol, allowing usage of existing VO tools and communication protocols with data services pertaining to Solar System studies. Some VO tools (TOPCAT, Aladin, CASSIS) are also adapted to improve handling of such data, e.g. visualisation of footprints (spatial or temporal), reflected light, or spectral cubes on planetary surfaces. In parallel, OGC-compliant definitions of planetary coordinate reference systems will facilitate the use of GIS tools in Planetary Science.</p>
<p>The VESPA portal, intended as a discovery tool to browse the EPN-TAP services, is being redesigned to improve the user experience (new version expected to be released for the conference). Other, more specific access modes (via script, web services, Jupyter notebook, VO tools, etc) are also available.</p>
<p><strong>Data services</strong>. 67 EPN-TAP data services are currently searchable from the VESPA portal, and about 20 are in development phase. Contributions from space agencies have increased significantly this year, with now 25+ million files in ESA’s PSA, and 60 datasets from the NASA PDS PPI node (declared in the IVOA registry but not yet reviewed for the portal). New services include atmospheric modelling from GCM (Venus and Mars), surface and asteroid spectra, radio observations, solar databases, and"
pub.1173847681,Low Latency Instance Segmentation by Continuous Clustering for LiDAR Sensors,"Low-latency instance segmentation of LiDAR point clouds is crucial in real-world applications because it serves as an initial and frequently-used building block in a robot’s perception pipeline, where every task adds further delay. Particularly in dynamic environments, this total delay can result in significant positional offsets of dynamic objects, as seen in highway scenarios. To address this issue, we employ a new technique, which we call continuous clustering. Unlike most existing clustering approaches, which use a full revolution of the LiDAR sensor, we process the data stream in a continuous and seamless fashion. Our approach does not rely on the concept of complete or partial sensor rotations with multiple discrete range images; instead, it views the range image as a single and infinitely horizontally growing entity. Each new column of this continuous range image is processed as soon it is available. Obstacle points are clustered to existing instances in real-time and it is checked at a high-frequency which instances are completed in order to publish them without waiting for the completion of the revolution or some other integration period. In the case of rotating sensors, no problematic discontinuities between the points of the end and the start of a scan are observed. In this work we describe the two-layered data structure and the corresponding algorithm for continuous clustering. It is able to achieve an average latency of just 5 ms with respect to the latest timestamp of all points in the cluster. We are publishing the source code at https://github.com/UniBwTAS/continuous_clustering."
pub.1132897291,Benefits of short-term photovoltaic power production forecasting to the power system,"The impact of intermittent power production by Photovoltaic (PV) systems to the overall power system operation is constantly increasing and so is the need for advanced forecasting tools that enable understanding, prediction, and managing of such a power production. Solar power production forecasting is one of the enabling technologies, which can accelerate the transition to sustainable energy environment. Short-term forecast information on the expected power production can assist existing forecasting techniques and enable efficient integration of renewable energy sources through the efficient energy trading, power system control and management of energy storage units. The paper presents an approach to predict local PV power output based on short-term solar forecasting using ground-based camera and analyzes the benefits of such forecast to the power system operation. PV power plant production data collected over 216 days is used to analyze the magnitude and energy contained in transients caused by changes in sky cover. Cost-effectiveness was calculated with different scales of a power plant. An overview of the benefits for the transmission system operator is given. This overview considers the ways in which short-term forecasting can improve the efficiency of power management in an electric grid. A system cost-effectiveness analysis was carried out for electricity producers that can use this system to generate more precise forecasts and thus reduce penalties for non-compliance with the anticipated production."
pub.1166348986,Low Latency Instance Segmentation by Continuous Clustering for LiDAR Sensors,"Low-latency instance segmentation of LiDAR point clouds is crucial in
real-world applications because it serves as an initial and frequently-used
building block in a robot's perception pipeline, where every task adds further
delay. Particularly in dynamic environments, this total delay can result in
significant positional offsets of dynamic objects, as seen in highway
scenarios. To address this issue, we employ a new technique, which we call
continuous clustering. Unlike most existing clustering approaches, which use a
full revolution of the LiDAR sensor, we process the data stream in a continuous
and seamless fashion. Our approach does not rely on the concept of complete or
partial sensor rotations with multiple discrete range images; instead, it views
the range image as a single and infinitely horizontally growing entity. Each
new column of this continuous range image is processed as soon it is available.
Obstacle points are clustered to existing instances in real-time and it is
checked at a high-frequency which instances are completed in order to publish
them without waiting for the completion of the revolution or some other
integration period. In the case of rotating sensors, no problematic
discontinuities between the points of the end and the start of a scan are
observed. In this work we describe the two-layered data structure and the
corresponding algorithm for continuous clustering. It is able to achieve an
average latency of just 5 ms with respect to the latest timestamp of all points
in the cluster. We are publishing the source code at
https://github.com/UniBwTAS/continuous_clustering."
pub.1103997900,Enhancing Performance and Energy Efficiency for Hybrid Workloads in Virtualized Cloud Environment,"Virtualization has attained mainstream status in enterprise IT industry. Despite its widespread adoption, it is known that virtualization also introduces non-trivial overhead when tasks are executed on a virtual machine (VM). In particular, a combined effect from device virtualization overhead and CPU scheduling latency can cause performance degradation when computation intensive tasks and I/O intensive tasks are co-located on a VM. Such an interference also causes extra energy consumption. In this paper, we present Hylics, a novel solution that enables efficient data traverse paths for both I/O and computation intensive workloads. This is achieved with the provision of in-memory file system and network service at the hypervisor level. Several important design issues are pinpointed and addressed during our prototype implementation, including efficient intermediate data sharing, network service offloading, and QoS-aware memory usage management. Based on our real-world deployment on KVM, we show that Hylics can significantly improve computation and I/O performance for hybrid workloads. Moreover, this design also alleviates the existing virtualization overhead and naturally optimizes the overall energy efficiency."
pub.1162778728,Active Navigation System for a Rubber-Tapping Robot Based on Trunk Detection,"To address the practical navigation issues of rubber-tapping robots, this paper proposes an active navigation system guided by trunk detection for a rubber-tapping robot. A tightly coupled sliding-window-based factor graph method is proposed for pose tracking, which introduces normal distribution transform (NDT) measurement factors, inertial measurement unit (IMU) pre-integration factors, and prior factors generated by sliding window marginalization. To actively pursue goals in navigation, a distance-adaptive Euclidean clustering method is utilized in conjunction with cylinder fitting and composite criteria screening to identify tree trunks. Additionally, a hybrid map navigation approach involving 3D point cloud map localization and 2D grid map planning is proposed to apply these methods to the robot. Experiments show that our pose-tracking approach obtains generally better performance in accuracy and robustness compared to existing methods. The precision of our trunk detection method is 93% and the recall is 87%. A practical validation is completed in robot rubber-tapping tasks of a real rubber plantation. The proposed method can guide the rubber-tapping robot in complex forest environments and improve efficiency."
pub.1171326712,MASTER: Machine Learning-Based Cold Start Latency Prediction Framework in Serverless Edge Computing Environments for Industry 4.0,"The integration of serverless edge computing and the Industrial Internet of Things (IIoT) has the potential to optimize industrial production. However, cold start latency is one of the main challenges in this area, resulting in resource waste. To address this issue, we propose a new machine learning-based resource management framework called MASTER which utilizes an extreme gradient boosting (XGBoost) model to predict the cold start latency for Industry 4.0 applications for performance optimization. Furthermore, we created a new cold start dataset using an IIoT scenario (i.e. predictive maintenance) to validate the proposed MASTER framework in serverless edge computing environments. We have evaluated the performance of the MASTER framework using a real-world serverless platform, Google Cloud Platform for single-step prediction (SSP) and multiple-step prediction (MSP) operations and compared it with existing frameworks that used deep deterministic policy gradient (DDPG) and long short-term memory (LSTM) models. The experimental results show that the XGBoost-based resource management framework is the most successful model in predicting cold start with mean absolute percentage error (MAPE) values of 0.23 in SSP and 0.12 in MSP. It has been also identified that the Linear Regression model (utilized in the MASTER framework) has the least computational time (0.03 seconds) as compared to other deep learning and machine learning models considered in this work. Finally, we compare the energy consumption and $\text{CO}_{2}$ emissions of all models to emphasize resource awareness."
pub.1139840100,"DataStorm: Coupled, Continuous Simulations for Complex Urban Environments","
                    Urban systems are characterized by complexity and dynamicity. Data-driven simulations represent a promising approach in understanding and predicting complex dynamic processes in the presence of shifting demands of urban systems. Yet, today’s silo-based, de-coupled simulation engines fail to provide an end-to-end view of the complex urban system, preventing informed decision-making. In this article, we present
                    DataStorm
                    to support integration of existing simulation, analysis and visualization components into integrated workflows.
                    DataStorm
                    provides a flow engine,
                    DataStorm-FE
                    , for coordinating data and decision flows among multiple actors (each representing a model, analytic operation, or a decision criterion) and enables ensemble planning and optimization across cloud resources.
                    DataStorm
                    provides native support for simulation ensemble creation through parameter space sampling to decide which simulations to run, as well as distributed instantiation and parallel execution of simulation instances on cluster resources. Recognizing that simulation ensembles are inherently sparse relative to the potential parameter space, we also present a density-boosting partition-stitch sampling scheme to increase the effective density of the simulation ensemble through a sub-space partitioning scheme, complemented with an efficient stitching mechanism that leverages partial and imperfect knowledge from partial dynamical systems to effectively obtain a global view of the complex urban process being simulated.
                  "
pub.1127511637,Cradle to Cradle Building Components Via the Cloud: A Case Study,"This paper expands upon ‘cradle to cradle carpets and cities’ presented by Ness and Field (Cradle to cradle carpets and cities. In Proceedings of SASBE 03, Brisbane, 2003) at SASBE 03, where the notion of providing modular carpets as a service was introduced, and a paper at SASBE 06, where the theme of providing C2C products as a service was further developed by Ness and Pullen (Decoupling resource consumption from growth: new business model towards a sustainable built environment in China. In: Proceedings of SASBE 06, Shanghai, 2006). It reports on the outcomes of an ARUP Global Research Challenge Project 2017, undertaken by University of South Australia, ARUP, Prismatic Architectural Research and other partners, under the theme of adapting the circular economy to the built environment. The project addresses the challenge of reusing building components, so they deliver more value over their extended life-cycle, with consequent reductions in resource consumption, greenhouse gas emissions, pollution and waste, coupled with creation of new enterprises and jobs. A universally accessible ‘Cloud-based building information management platform’ is being developed, which enables components to be identified, reclaimed reused and exchanged multiple times over their lifecycle, within the same or different facilities. A cyber-physical information exchange system was established between physical building components and their virtual counterparts, known as Building Information Models, so that their life cycle information including history of ownership, condition, maintenance history, technical specifications and physical performance could be tracked, monitored and managed. In addition, designers could identify reused components via the cloud platform, and assess their suitability for incorporation in building projects when compared with new products. This research was complemented by an innovative business model, whereby components and products can be provided as a service, with producers retaining responsibility for their repair, remanufacturing and/or reuse over their life cycle. The methodology involved establishing a Cyber-Physical System by connecting a series of existing technologies, including Radio Frequency Identification (RFID) and Building Information Modelling (BIM). Using a case study of a section of a major new hospital for ‘proof of concept’, information on the history, location, properties and performance of physical components could be exchanged in real-time from RFID tags to a local BIM system and thence to the cloud platform. Complemented by interviews within Australia and Europe with key stakeholders including designers, project managers, manufacturers, owners, investors and facility managers, the research led to the development of a ‘products as service’ business model and associated business case for the new paradigm. In short, a self-populating relational database that can execute predefined multiple/ conditional ownership exchange via a"
pub.1113718294,Challenges in implementing industry revolution 4.0 in INDIAN manufacturing SMES: insights from five case studies,"Purpose-This research examines Small and Medium manufacturing Enterprises (SME’s) awareness, current capability, willingness and ability to identify the challenges involved in implementing Industry 4.0(I 4.0) at their premises.Design/methodology/approach-A set of questionnaire was framed to collect qualitative and quantitative data from five manufacturing SME’s and they were analyzed to gain insight.Findings –3 out of 5 manufacturing SME’s are aware, capable, willing and have ability to identify the challenges for implementing Industry 4.0 at their premises. The study also found that implementation of I 4.0 depends on size of the firm. Medium size manufacturing firms had started investing in Information Technology but small scale industries is still struggling to figure out their long term benefit.Practical implications – The advancement and the integration of the technologies such as Cyber Physical system, Internet Of Things, Artificial intelligence, Big data, Cloud computing and 3D printing provides greater flexibility to the manufacturing firms. In today’s global competition with a huge demand for personalized products at low price with best quality, innovation and capability to full filling batch size of one is becoming important. Hence, to meet the market demand many large-scale industries started investing in advanced technology where SME has yet to pay attention.Originality/value –Paper indicates the Indian manufacturing SME’s preparedness for Industry 4.0. It contains five cases capturing the current manufacturing practices followed in the SME’s and their capability towards implementing Industry I 4.0 in Indian environment.  "
pub.1170969641,Intelligent Health Monitoring in Smart Homes,"Intelligent Health Monitoring in Smart Homes is a burgeoning field that integrates advanced technologies to revolutionize healthcare delivery within residential environments. This paper explores the concept of smart homes as proactive health monitoring ecosystems, leveraging interconnected devices, sensors, and data analytics to monitor residents' health parameters and behaviors in real-time. Through a comprehensive review of existing literature and case studies, this study elucidates the potential benefits, challenges, and emerging trends in intelligent health monitoring within smart home environments.Key components of intelligent health monitoring systems, including wearable devices, ambient sensors, and machine learning algorithms, are examined in detail, highlighting their roles in detecting early signs of health deterioration, predicting health-related events, and facilitating timely interventions. Furthermore, the integration of telemedicine platforms and communication technologies enhances remote patient monitoring and enables seamless interaction between residents and healthcare providers.The paper also addresses critical considerations such as privacy, data security, and interoperability standards to ensure the ethical and responsible implementation of smart health monitoring solutions. By fostering collaboration among healthcare professionals, technology developers, and policymakers, intelligent health monitoring in smart homes has the potential to improve health outcomes, enhance quality of life, and reduce healthcare costs. The integration of artificial intelligence, Internet of Things (IoT) devices, and cloud computing enables seamless communication and coordination among various components of the smart home ecosystem. Through real-time monitoring and analysis, potential health risks can be identified early, enabling timely interventions and proactive healthcare management. Additionally, the ability to remotely access and share health data facilitates collaborative care and empowers individuals to take control of their own health and well-being. However, the widespread adoption of intelligent health monitoring in smart homes also raises important ethical, privacy, and regulatory considerations. Safeguarding the security and privacy of sensitive health data, ensuring transparency and accountability in algorithmic decision-making, and promoting equitable access to technology are essential aspects that require careful attention and proactive measures. In conclusion, Intelligent Health Monitoring in Smart Homes holds tremendous potential to revolutionize healthcare delivery, promoting preventive care, empowering individuals, and ultimately improving health outcomes. By addressing technical, ethical, and regulatory challenges, this transformative approach can pave the way for a more connected, proactive, and personalized healthcare experience.This paper studies recent state-of-the-art research on the field of IoT for health monitoring and"
pub.1156396898,Building Resilient Smart Cities: Sustainability and Inclusiveness,"Growth of cities, increasing urbanisation and associated need for sustainable need for sustainable urban development are globally recognised as one of the most challenges of 21st century. Rapid urbanization and globalisation across the world entails of a resource deficient planet, aspirations of people and need of the sustainable environment have fuelled the mind of the mankind to develop & innovate technologies to help societies and government and organizations manage cities professionally and efficiently. The concept that has emerged from this initiative is termed as “Smart City” which essentially is the interconnected and integrated management of information dissemination by application of advanced technologies to search, access, transfer and process required information by using information and communication technology (ICT) tools coupled with deploying censors and other intelligent devices. Besides managing a city efficiently, it is also crucial in emergency governance such as disasters and riots etc. It is the need of the time for transformation of our society into citizen-friendly society. This transformation shall be possible by developing our cities as “Smart City”. Smart city as a concept initially came in public discourse in the years 1990s. Truly,, there is no definite definition of a smart city as such so far since definition and features of smart cities vary with topography, ecology, geography, size, density of population and more importantly resources of a country besides, and with climate and culture as well . In absence of adoption of a universally accepted definition of ‘smart cities’, the trend is evolving across the world in recent years and likely to accelerate rapidly in near future. The term smart city can be described as an urban community where emergent information technologies could be employed in order to collect, analyse, and apply data essential infrastructure and services, with the objective of improving the quality of life for its residents (Moser, 2001). Over a period of time, there has been large scale innovations globally in digital technologies which has also combined low- cost censors and usage of smart phones, massive deployment of ICT towers made of optical fibres and infusion of censors for information sharing, information kiosks along with Machine learning and Artificial Intelligence besides enhanced data storage capacity of cloud computing. DIGITAL cities FEATURES the integration of digital technology into the city ‘core infrastructure rely on the digital city infrastructure to build intelligent buildings, Intelligent transportation systems, smart schools, enterprises, public spaces and essential public services. Increasing urbanisation because of social migration from smaller rural areas and sub-urban cities thinkers across the globe have to think about the developing smart cities development in existing and brown field development facilities for the poor urban labour class, Rag pickers and marginalised s"
pub.1149551371,A Multi-Sensory Guidance System for the Visually Impaired Using YOLO and ORB-SLAM †,"Guidance systems for visually impaired persons have become a popular topic in recent years. Existing guidance systems on the market typically utilize auxiliary tools and methods such as GPS, UWB, or a simple white cane that exploits the user’s single tactile or auditory sense. These guidance methodologies can be inadequate in a complex indoor environment. This paper proposes a multi-sensory guidance system for the visually impaired that can provide tactile and auditory advice using ORB-SLAM and YOLO techniques. Based on an RGB-D camera, the local obstacle avoidance system is realized at the tactile level through point cloud filtering that can inform the user via a vibrating motor. Our proposed method can generate a dense navigation map to implement global obstacle avoidance and path planning for the user through the coordinate transformation. Real-time target detection and a voice-prompt system based on YOLO are also incorporated at the auditory level. We implemented the proposed system as a smart cane. Experiments are performed using four different test scenarios. Experimental results demonstrate that the impediments in the walking path can be reliably located and classified in real-time. Our proposed system can function as a capable auxiliary to help visually impaired people navigate securely by integrating YOLO with ORB-SLAM."
pub.1173744584,BMUNPLC: Design of a Bioinspired Model for Improving Usability of New PLC Deployments for IoT Applications,"Internet of Things (IoT) have paved the way into households and industries due to their ease-of-use and fine-tuned control characteristics. Every IoT network requires multiple wireless devices that can be connected via a common routing and control protocol. These devices utilize wireless transmitters and receivers to communicate with a central control router, which is connected to the cloud. This router is capable of aggregating data from multiple IoT devices and send it to the cloud for logging and control purposes. Control signals from the cloud are fetched by router and then wirelessly transmitted to IoT devices for seamless connectivity and control. But a major drawback of such scenarios is presence of wireless components at each IoT device, which increases cost and energy consumption of the deployed network under practical use cases. Due to incorporation of wireless devices, delay needed to communicate between router and the device is higher when compared with its wired counterparts. To overcome these limitations, a novel bioinspired device placement model for new electrical sites, is proposed in this text. The proposed model uses power line communications (PLC) in broadband mode to efficiently transfer data and control signals between different electrical appliances. This communication is controlled via use of a hybrid genetic algorithm with teacher learner based optimization (GA TLbO) that assists in efficient placement of IoT router and control of communication data rates on phase lines. The proposed model also discusses design of new age electrical appliances that can directly connect with the proposed GA TLbO based PLC interfaces. These devices are capable of sensing current device status, and communicating that information on the power line via a power line control module (PLCM) that is built using low-power and low-cost components. Integration the GA TLbO model with PLCM is capable of reducing power consumption, reduce deployment costs, and improve data and control signal communication speed, across multiple real-time scenarios. The proposed model was tested on small-scale, medium-scale, and large-scale household & industrial environments, and it was observed that due to integration of PLC with IoT cost of deployment was reduced by 39.5%, energy consumption was reduced by 43.8%, and communication speed was improved by 8.3% when compared with IoT deployments, and averaged over different simulations. This performance was also compared with existing PLC based IoT models, and an energy reduction of 8.5%, cost reduction of 1.9%, and speed improvement of 3.4% was observed via the proposed GA TLbO model on similar electrical loads. Due to this performance enhancement, the proposed model is capable of being deployed at large-scale IoT sites with high-speed control, energy efficiency, and low-cost operations."
pub.1104587073,Working Set Size Estimation Techniques in Virtualized Environments,"Energy consumption is a primary concern for datacenters' management. Numerous datacenters are relying on virtualization, as it provides flexible resource management means such as virtual machine (VM) checkpoint/restart, migration and consolidation. However, one of the main hindrances to server consolidation is physical memory. In nowadays cloud, memory is generally statically allocated to VMs and wasted if not used. Techniques (such as ballooning) were introduced for dynamically reclaiming memory from VMs, such that only the needed memory is provisioned to each VM. However, the challenge is to precisely monitor the needed memory, i.e., the working set of each VM. In this paper, we thoroughly review the main techniques that were proposed for monitoring the working set of VMs. Additionally, we have implemented the main techniques in the Xen hypervisor and we have defined different metrics in order to evaluate their efficiency. Based on the evaluation results, we propose Badis, a system which combines several of the existing solutions, using the right solution at the right time. We also propose a consolidation extension which leverages Badis in order to pack the VMs based on the working set size and not the booked memory."
pub.1169562225,3D CHANGE DETECTION FOR SEMI-AUTOMATIC UPDATE OF BUILDINGS IN 3D CITY MODELS,"Abstract. Automatic update of 3D city models has become a crucial operation in the context of urban digital twins. It commonly relies on a reconstruction from ALS point clouds. However, frequent reconstructions due to a dynamic urban environment are resource-intensive and lack change information about the scene. In this paper, we present a novel framework which aims to combine an instance change detection approach based on a distance-computing algorithm, geometric features and thresholding with a building reconstruction algorithm to ensure an efficient geometric, semantic and thematic (change labeling) update of an existing 3D city model. This approach comprises three stages spanning from data preparation to the integration of change results in the updated model. First, we prepare our input data. Next, we assess the changes between the two epochs. This process involves two stages. New and lost buildings are extracted in the first, and the changed and unchanged in the other. The results of the change detection are evaluated using standard evaluation metrics. The evaluation results are encouraging considering the various sources of errors. Finally, unchanged buildings are kept in the model, while the changed and new ones are reconstructed using Geoflow3D. The final model is semantically augmented using a change attribute. Since the 3D city model undergoes an update rather than complete reconstruction, the tracking of both geometric and semantic changes of some buildings can be made possible through a versioning system. The change information can be leveraged in multiple applications like 3D cadastre, urban inventory, urban planning…"
pub.1111523238,Working Set Size Estimation Techniques in Virtualized Environments,"Energy consumption is a primary concern for datacenters' management. Numerous datacenters are relying on virtualization, as it provides flexible resource management means such as virtual machine (VM) checkpoint/restart, migration and consolidation. However, one of the main hindrances to server consolidation is physical memory. In nowadays cloud, memory is generally statically allocated to VMs and wasted if not used. Techniques (such as ballooning) were introduced for dynamically reclaiming memory from VMs, such that only the needed memory is provisioned to each VM. However, the challenge is to precisely monitor the needed memory, i.e., the working set of each VM. In this paper, we thoroughly review the main techniques that were proposed for monitoring the working set of VMs. Additionally, we have implemented the main techniques in the Xen hypervisor and we have defined different metrics in order to evaluate their efficiency. Based on the evaluation results, we propose Badis, a system which combines several of the existing solutions, using the right solution at the right time. We also propose a consolidation extension which leverages Badis in order to pack the VMs based on the working set size and not the booked memory."
pub.1171927900,LMChain: An Efficient Load-Migratable Beacon-Based Sharding Blockchain System,"Sharding is an important technology that utilizes group parallelism to enhance the scalability and performance of blockchain. However, the existing solutions use a historical transaction-based approach to reallocate shards, which cannot handle temporary overload and incurs additional overhead during the reallocation process. To this end, this paper proposes LMChain, an efficient load-migratable beacon-based sharding blockchain system. The primary goal of LMChain is to eliminate reliance on historical transactions and achieve the high performance. Specifically, we redesign the state maintenance data structure in Beacon Shard to effectively manage all account states at the shard level. Then, we innovatively propose a load-migratable transaction processing protocol built upon the new data structure. To mitigate read-write conflicts during the selection of migration transactions, we adopt a novel graph partitioning scheme. We also adopt a relay-based method to handle cross-shard transactions and resolve inter-shard state read-write conflicts. We implement the LMChain prototype and conduct experiments in a real network environment comprising 17 cloud servers. Experimental results show that, compared with state-of-the-art solutions, LMChain effectively reduces the average transaction waiting latency of overloaded transactions by 30% to 48% in different cases within 16 transaction shards, while improving throughput by 3% to 10%."
pub.1171060844,The impact and adoption of emerging technologies on accounting: perceptions of Canadian companies," Purpose This study aims to focus on the five most relevant and discursive emerging technologies in accounting (cloud computing, big data and data analytics, blockchain, artificial intelligence (AI) and robotics process automation [RPA]). It investigates the adoption and use of these technologies based on data collected from accounting professionals in a technology-developed country – Canada, through a survey.   Design/methodology/approach The study investigates the adoption and use of emerging technologies based on data collected from accounting professionals in a technology-developed country – Canada, through a survey. This study considers the said nature and characteristics of emerging technologies and proposes a model using the factors that have been found to be significant and most commonly investigated by existing prior technology-organization-environment (TOE)-related technology adoption studies. This survey applies the TOE framework and examines the influence of significant and most commonly known factors on Canadian firms’ intention to adopt the said emerging technologies.   Findings Study results indicate that Canadian accounting professionals’ self-assessed knowledge (about these emerging technologies) is more theoretical than operational. Cloud computing is highly used by Canadian firms, while the use of other technologies, particularly blockchain and RPA, is reportedly low. However, firms’ intention about the future adoption of these technologies seems positive. Study results reveal that only the relative advantage and top management commitment are found to be significant considerations influencing the adoption intention.   Research limitations/implications Study findings confirm some results presented in earlier studies but provide additional insights from a new perspective, that of accounting professionals in Canada. The first limitation relates to the respondents. Although accounting professionals provided valuable insights, their responses are personal views and do not necessarily represent the views of other professionals within the same firm or the official position of their accounting departments or firms. Therefore, the exclusion of diverse viewpoints from the same firm might have negatively impacted the results of this study. Second, this study sample is limited to Canada-based firms, which means that the study reflects only the situation in that country. Third, considering the research method and the limit on the number of questions the authors could ask, respondents were only asked to rate the impact of these five technologies on the accounting field and to clarify which technologies are used.   Practical implications This study’s findings confirm that the organizational intention to adopt new technology is not primarily based on the characteristics of the technology. In the case of emerging technology adoption, the decision also depends upon other factors related to the internal organization. Furthermore, although this st"
pub.1132782748,Neuron – Digital console innovative by Arup,"At the age of digital era, supervised / unsupervised machine learning, sharp data analysis and neuron network are some of many ways to formulate optimization problems with millions or billions of variables. Classical building system and environmental optimization algorithms are not designed to scale to instances of this size; new approaches are needed. The Digital Platform developed by Arup – Neuron is an innovative console which embodies 5G, IoT, Big Data, Cloud Computing and AI technology for smart building with advanced data analytic capabilities. It can fully integrate with the existing Building Management System as well as various data sources generated from the IoT sensors, enabling prompt and adaptive response to dynamic environment, serving as a foundation for buildings to consolidate and connect data from disparate equipment and devices to provide customised insight and machine learning models for built environmental management and improvement, new building design, operation and asset management, energy monitoring and optimisation. One significant success achieved with our integrated digital platform is that it has been adopted by multiple clients in the East Asia region in managing their building. With full implementation and other appropriate energy saving and/or retro-commissioning practices, Neuron is expected to save up to 30% total HVAC energy consumption for a typical existing commercial building. Apart from energy saving, Neuron also focuses on creating a better indoor environment, which could significantly improve employees’ productivity. Neuron, as a smart building console, is set to have a momentous impact on Hong Kong, not only for a better built environment, but also a game changer for the East Asia region to excel in the era of digital economy. This paper elaborates how Neuron could help in digital transformation of new and existing buildings in the new era. The challenges and experiences of projects will also be discussed.Start your abstract here…"
pub.1155808211,NSANet: Noise Seeking Attention Network,"LiDAR (Light Detection and Ranging) technology has remained popular in
capturing natural and built environments for numerous applications. The recent
technological advancements in electro-optical engineering have aided in
obtaining laser returns at a higher pulse repetition frequency (PRF), which
considerably increased the density of the 3D point cloud. Conventional
techniques with lower PRF had a single pulse-in-air (SPIA) zone, large enough
to avoid a mismatch among pulse pairs at the receiver. New multiple
pulses-in-air (MPIA) technology guarantees various windows of operational
ranges for a single flight line and no blind zones. The disadvantage of the
technology is the projection of atmospheric returns closer to the same
pulse-in-air zone of adjacent terrain points likely to intersect with objects
of interest. These noise properties compromise the perceived quality of the
scene and encourage the development of new noise-filtering neural networks, as
existing filters are significantly ineffective. We propose a novel
dual-attention noise-filtering neural network called Noise Seeking Attention
Network (NSANet) that uses physical priors and local spatial attention to
filter noise. Our research is motivated by two psychology theories of feature
integration and attention engagement to prove the role of attention in computer
vision at the encoding and decoding phase. The presented results of NSANet show
the inclination towards attention engagement theory and a performance boost
compared to the state-of-the-art noise-filtering deep convolutional neural
networks."
pub.1170173223,NSANet: Noise-Seeking Attention Network,"Light detection and ranging technology has remained popular in capturing natural and built environments for numerous applications. The recent technological advancements in electrooptical engineering have aided in obtaining laser returns at a higher pulse-repetition frequency (PRF), which considerably increased the density of the 3-D point cloud. Conventional techniques with lower PRF had a single pulse-in-air zone, large enough to avoid a mismatch among pulse pairs at the receiver. New multiple-pulse-in-air technology guarantees various windows of operational ranges for a single flight line and no blind zones. The disadvantage of the technology is the projection of atmospheric returns closer to the same pulse-in-air zone of adjacent terrain points likely to intersect with objects of interest. These noise properties compromise the perceived quality of the scene and encourage the development of new noise-filtering neural networks, as the existing filters are significantly ineffective. We propose a novel dual-attention noise-filtering neural network called noise-seeking attention network (NSANet) that fuses physical priors and local spatial attention to filter noise. Our research's fusion module is motivated by two psychological theories of feature integration and attention engagement to prove the role of attention in computer vision at the encoding and decoding phase. The presented results of NSANet show the benefit of attentional engagement theory and a performance boost of 7.30 on recall and 4.10 on F1-score compared to the state-of-the-art noise-filtering deep convolutional neural networks."
pub.1150012735,A Comparative Analysis of the Acquisition Transaction of Management Information Systems through Virtual and Face-to-Face Negotiations—The Perspective of Green IT Industry in Poland,"The COVID-19 pandemic that began in 2020 has significantly impacted businesses, regardless of size or industry. The hybrid and remote working models have moved all meetings with potential and existing suppliers to an online environment. This also applies to small- and medium-sized enterprises (SMEs), which have had to adapt themselves to the new situation and implement the solutions necessary to survive on the market. On the other hand, clients have become more aware of the environment and its changes. Customers are trying to be more eco-friendly, by choosing and moving towards Green IT. Thus, this needs to be considered. The acquisition of management information systems (MIS) in the pandemic era is based only on virtual meetings. The main goals of this paper were the identification of the changes in the negotiations caused by the COVID-19 pandemic, the transformation of this process into virtual environment, discussion of the possibility of using Green IT in addition to Management Information Systems, and the changes caused by the pandemic. The article was prepared based on the results of qualitative research using the case study method. The comparative analysis includes purposely selected cloud-based Enterprise Resource Planning (ERP) and Customer Relationship Management (CRM) systems’ acquisition processes, presented from the clients’ perspective. The research was conducted in 2021, based on the authors’ practical experience, and presents four cases. This research illustrates the negotiations concerning an acquisition transaction pre-pandemic and during the pandemic. Finally, the conclusions and main differences caused by the pandemic in the acquisition transaction process of management information systems (MIS) are presented."
pub.1143486379,A Python Library for the Jupyteo IDE Earth Observation Processing Tool Enabling Interoperability with the QGIS System for Use in Data Science,"This paper describes JupyQgis – a new Python library for Jupyteo IDE enabling interoperability with the QGIS system. Jupyteo is an online integrated development environment for earth observation data processing and is available on a cloud platform. It is targeted at remote sensing experts, scientists and users who can develop the Jupyter notebook by reusing embedded open-source tools, WPS interfaces and existing notebooks. In recent years, there has been an increasing popularity of data science methods that have become the focus of many organizations. Many scientific disciplines are facing a significant transformation due to data-driven solutions. This is especially true of geodesy, environmental sciences, and Earth sciences, where large data sets, such as Earth observation satellite data (EO data) and GIS data are used. The previous experience in using Jupyteo, both among the users of this platform and its creators, indicates the need to supplement its functionality with GIS analytical tools. This study analyzed the most efficient way to combine the functionality of the QGIS system with the functionality of the Jupyteo platform in one tool. It was found that the most suitable solution is to create a custom library providing an API for collaboration between both environments. The resulting library makes the work much easier and simplifies the source code of the created Python scripts. The functionality of the developed solution was illustrated with a test use case."
pub.1043123267,SIGCHI Lifetime Research Award Talk,"We think with things, with our bodies, with marks on paper, and with other people. Thinking is a distributed, situated, and social activity that exploits the extraordinary facilities of language, representational media, and embodied interaction with the world. Today we increasingly think with computers. This includes desktop and laptop computers but more and more involves massive computer networks that deliver search results and provide access to a vast array of cloud-based services. At the same time, the monolithic computer of the recent past is coming apart and being reassembled in myriad new forms, including cell phones, tablets, digital pens, cameras that allow focusing after a picture is taken, glasses that can take a picture of what we see and tell us about it, sensors that capture our activity and support interaction by movement of hands and bodies as well as voice and eyes, and even cars that drive themselves. We are on the threshold of an era that is being described as the Internet of Things or, perhaps more accurately, as the Internet of Everything. The historical moment when people primarily worked in front of a single computer has passed. Computers are ubiquitous. They are being embedded in virtually every new device and the Internet and wireless communication enable their connection to each other and to ever-expanding information resources with previously unimaginable computational power. This is being accelerated by a radically changing cost structure in which the price is the same or less to use a thousand computers for a minute or a day than to use one computer for a thousand minutes or days. These and related developments are fundamentally changing the world in which we live and the ways we think, communicate, and interact within it. Yet for all their capacity and speed, using computers too often remains difficult, awkward, and frustrating. Even after six decades of design evolution there is little of the naturalness, spontaneity, and contextual sensitivity that characterize other activities nor is there the conviviality and flexibility of working with tangible media. Most time using computers is spent within applications, each designed to support specific functions and allowing interaction with information in only predetermined ways that are rarely sensitive to overall activity context or history of use and far too often we can look, but not touch, annotate, or personalize the information involved. Access to information needed to conduct daily life is spread across multiple applications and resources, resulting in fragmentation of activities and increased complexity. The rapidly expanding world of mobile apps, which are often walled off from each other and from other information sources, and the increasing use of multiple devices further fragments the ways we work and interact. Although we commonly use paper and digital work materials together, they remain elements of two distinct worlds. Disconnects between digital and physical"
pub.1175713121,Digital technologies in crisis management,"Crisis management has become a critical aspect of modern business and public administration, especially in the face of global crises such as economic recessions, pandemics, and natural disasters. In this context, digital technologies are playing an increasingly important role, providing new tools and approaches for effective crisis management. The definition of crisis management includes a set of measures aimed at identifying, assessing and neutralizing crisis situations, as well as minimizing their negative consequences. It is a management discipline that covers strategic, operational and tactical actions that allow organizations to respond quickly to changes in the external and internal environment. The role of digital technologies in modern management cannot be overestimated. They provide tools for the rapid collection, analysis and processing of information, which is critical in crisis situations. For example, Big Data management systems allow analyzing huge amounts of information in real time, which contributes to a more accurate assessment of the situation and informed decision-making. Cloud technologies provide access to resources and data from anywhere in the world, which is especially important in a crisis when it is necessary to ensure the continuity of business processes and the work of teams on a remote basis. Big data analytics is one of the key components of digital technologies in crisis management. It allows collecting and analyzing data from various sources, including social media, news, internal company systems, etc., to identify potential crises at early stages and predict their development. This enables organizations to respond quickly to threats and minimize negative consequences. Cloud technologies provide flexibility and scalability of the IT infrastructure, allowing organizations to quickly adapt to changes in the external environment and ensure business continuity. They also help reduce IT infrastructure costs and increase resource efficiency. Artificial intelligence and machine learning are powerful tools for automating crisis management processes. They can be used to analyze large amounts of data, detect anomalies, predict the development of crisis situations, and support decision-making. Machine learning algorithms can analyze historical data to identify patterns that precede crises and recommend appropriate actions to prevent them. Digital platforms and tools for communication and collaboration, such as Microsoft Teams, Slack, Zoom, ensure continuous interaction between employees and teams, which is critical in crisis situations. They allow for quick information exchange, virtual meetings, and coordination of actions, which contributes to more effective crisis management. Practical cases of successful use of digital technologies in crisis management include the experience of large corporations, government organizations, and international organizations. For example, Microsoft uses Azure cloud technologies to ensure bus"
pub.1153540871,Education 4.0: Revisiting Contemporary Paradigms in Social Work Education,"Education is the cornerstone of social development. It determines the quality of an individual’s life. Throughout history, education has been strongly influenced by economic paradigms and industrial revolutions. Earlier industrial revolutions have had a significant impact on education, but it is pretty obvious that Industry 4.0 will affect dynamic transformation of higher education. In the process of aligning itself to industry demands, higher education needs to revisit existing paradigms and enhance the application of technological advancements. With the tectonic forces reshaping life of earth, including economies, industries and jobs across the globe, it is inevitable for education to prepare students to face the unprecedented challenges in the future. Higher education institutions are therefore expected to prepare a competent workforce that suits the evolving job requirements with new skill sets to take advantage of the Industry 4.0 era. The teaching models and curriculum of yesterday are not enough to prepare the students for tomorrow. This calls the institutions to revamp the present curriculum such that the process of learning will transform completely in the future. As countries across the globe are experiencing industrial transformation at an unprecedented rate with the pillars of Industry 4.0 such as artificial intelligence, Big Data and data analytics, Internet of Things and cloud computing, this chapter seeks to analyze how social workers can harness technology accelerating the digital revolution to address the accentuating problems in the society and how social work education should be amended to face the emerging challenges. Education is the cornerstone of social development. It determines the quality of an individual’s life. The Fourth Industrial Revolution has transformed industries from traditional labor-intensive operations to completely digitalized ecosystems. Social work is a multi-branched profession that seeks to deal with the myriad and most complicated interactions between persons and their environments. From production and consumption, information and communication, and networking through social media to organizing digital campaigns, social technologies have reshaped our milieu and created both opportunities and obstacles to individual, social and community growth and wellbeing. The world today is undergoing rapid transformation, particularly in the industrial sector as an outcome of Industry 4.0. Internet of Things refers to a set of systems, principles, applications and technologies related to Internet-connected objects. The term was primarily used to describe a world of unified connected devices that might reduce costs while saving time as well."
pub.1121932358,Convergence and digital fusion lead to competitive differentiation," Purpose Organizations are consistently seeking innovative strategies and novel pathways to enhance business processes and create differentiation. The global business ecosystem is changing and there is growing demand for multi-modal digital technologies, big data consolidation and data analytics to harness a cost-competitive agile system. Technological convergence and integration of digital systems is one of the preferred methodologies that facilitates new and effective workflows and revives business processes. The progressive interlinking of digital technologies with business operations leads to the convergence and blending of management disciplines, devices and applications. The growing inconsistencies in managerial understanding regarding the benefits of convergence prompts a comprehensive examination of digital convergence pathways, identifying the impacts on converging entities and business objectives. The State bank of India (SBI) mega-merger case study was selected to investigate the pragmatic framework of digital convergence and to understand the impacts on interlinked entities such as: business operations, strategic management, project team that support value creation and competitive differentiation. The purpose of this paper is to focus on the phenomena of techno-fusion of emerging technologies creating new opportunities, business models and unique strategies for global banking and financial service organizations.   Design/methodology/approach This study applies the qualitative, inductive research method using critical reflection of before and after the implementation of convergence and digital integration strategies. The SBI case study employs this research strategy based on the premise that banks must stay agile and highly responsive to the changing environment to enhance its value proposition and competitive differentiation objectives. The study methodology incorporates cooperative inquiry and multiple levels of analysis using data collection techniques of exhaustive review of archives, informal interviews, questionnaires and observations to identify the synergistic process improvement pathway. The study is grounded on the concept that the convergence of diverse business pathways involves innovative and interlinked project, strategic and information technology (IT) workflows that results in open innovative systems.   Findings The studies identify that organizational innovation and creative solutions are a result of ecosystem turbulence, environmental force diversity, competitive pressure and the need for differentiation. Organizations that harness the power of digital fusion and convergence of management, systems and data generate a competitive advantage. The technological convergence strategy pulls multiple business and technology processes (project, strategic, IT, Cloud, AI and business process management) at the organizational, divisional or functional level generating new opportunities and threats, new business models and unique "
pub.1173328814,The Role of Supply Chain Flexibility in Adapting Marketing Strategies to Changing Consumer Preferences,"Supply chain flexibility plays a pivotal role in enabling organizations to adapt their marketing strategies to evolving consumer preferences in dynamic market environments. This qualitative study explores how supply chain flexibility dimensions—responsiveness, agility, resilience, and sustainability—impact marketing strategy adaptation. Through semi-structured interviews and document analysis, insights were gathered from industry practitioners across diverse sectors. Key findings highlight that responsive supply chains facilitate quick adjustments in production, distribution, and sourcing to meet changing consumer demands. Agility enables rapid reconfiguration of operations to capitalize on market opportunities and respond to disruptions effectively. Resilient supply chains mitigate risks and maintain continuity during crises, safeguarding customer satisfaction and brand reputation. Integrating sustainability practices not only meets regulatory standards but also aligns with consumer preferences for eco-friendly products, enhancing corporate social responsibility. Technological advancements such as AI, IoT, blockchain, and cloud computing enhance supply chain visibility, optimize decision-making, and support real-time responsiveness. Despite benefits, challenges like legacy systems, organizational silos, resistance to change, and resource constraints hinder effective implementation. Overcoming these barriers requires strategic leadership, cross-functional collaboration, and continuous investment in technology and talent. Embracing supply chain flexibility empowers organizations to navigate complexities, drive innovation, and sustain competitive advantage. By aligning supply chain capabilities with marketing strategies, companies can enhance market responsiveness, customer satisfaction, and long-term growth in today's dynamic business landscape"
pub.1164515371,Modelling of marketing management algorithms of related economic systems,"In recent decades, the world has been experiencing the modernization of traditional manufacturing and service indus-tries in the global penetration of new information technologies into all spheres of social life. Digitization (or the economy dependent on digital technologies) gives a new impetus to creating new markets, new digital skills, and opportunities in society, business, and the state. Digitization of the economy is a particular stage of the modern development of scientific and technical progress, which is associated with the comprehensive implementation of Internet networks, computer and infor-mation technologies, electronic trade and commerce, cloud services in industrial, social, public, and other spheres of activity, as well as affect all sectors of the economy. The development of information technologies and systems today facilitates the work and management of the company. It improves the enterprise's activity in general, creating new opportunities and in-creasing the efficiency of the creation of each employee and staff in general. The impact of information technologies is at all stages of the company's activity – from business planning to changing the direction of action. Currently, completing indus-trialization and transitioning to Industry 4.0, the country is digitizing all economic activity, creating and developing new innovative products and technologies at an accelerated pace, dominated by digital platforms, artificial intelligence, in-formatization, and automation. The main goal of the economic transformation processes of digitization is the transformation of production into a flexible one adapted to existing realities, which increases the country's competitiveness in the ""digital space"". Digitization is a means of obtaining the necessary results that meet modern society's and businesses' needs for addi-tional profits. The digital economy requires knowledge about advanced goods and services and increasing the importance of innovation in the sustainable development of the world economy. The amount of information inherent in the digital economy fundamentally changes the functioning of markets, creating new development opportunities. Management processes are also changing, which in the digital economy are based on the transparency of management and decision-making processes and on the completeness and accuracy of source information. Digital technologies make it possible to process large amounts of in-formation to make optimal economic decisions, as well as improve the quality of the processed data. Keywords: digital economy, digital technologies, digitalization, economic processes, competitive environment."
pub.1135469426,On the potential of demand-contsrolled ventilation system to enhance indoor air quality and thermal condition in Australian school classrooms,"Indoor thermal comfort and air quality in school classrooms are of interest worldwide, primarily because of their potential impacts on students’ health, learning performance and productivity. Further, increasing concerns with changing climate and building energy efficiency highlight the importance of ventilation and comfort in educational settings. The existing literature on indoor air quality (IAQ), ventilation, and thermal comfort in classrooms in subtropical regions of Australia is sparse. Here, we present the results of a field study conducted in secondary school classrooms in Sydney during the school year in 2018 and 2019. We collected data with subjective surveys through questionnaires and with field measurements related to IAQ the thermal comfort in two adjacent similar classrooms. The infiltration and ventilation rates were measured during the non-occupied period using the concentration decay method. We analysed the performance of a cloud-connected demand-controlled mechanical extract ventilation system (DCV), which was installed in one of the two surveyed classrooms during mid-season. Before application of the DCV, CO2 levels were similar in both classrooms with a maximum concentration of approximately 2418 ppm during cold season. The DCV reduced the peak CO2 concentration to 1335 ppm, while CO2 raised to 2981 ppm in the classrooms without DCV during mid-season. Further, Volatile Organic Compounds (VOCs) analysed from air samples show improved air quality in the classroom with DCV. Our results highlight the impact of both indoor temperature and CO2 concentration on students’ feeling of fatigue. Students showed adaptability to indoor temperature change. A period of one week is needed for students’ adaptation to a step change in the mean outdoor temperature. Understanding classroom indoor air quality and thermal environment, as well as students’ perceived comfort, is vital to develop child-based design guidelines for schools."
pub.1117412979,Design and simulation platform for evaluation of grid distribution system and transactive energy,"With the advent of remarkable development of solar power panel and inverter technology and focus on reducing greenhouse emissions, there is increased migration from fossil fuels to carbon-free energy sources (e.g., solar, wind, and geothermal). A new paradigm called Transactive Energy (TE) [3] has emerged that utilizes economic and control techniques to effectively manage Distributed Energy Resources (DERs). Another goal of TE is to improve grid reliability and efficiency. However, to evaluate various TE approaches, a comprehensive simulation tool is needed that is easy to use and capable of simulating the power-grid along with various grid operational scenarios that occur in the transactive energy paradigm. In this research, we present a web-based design and simulation platform (called a design studio) targeted toward evaluation of power-grid distribution system and transactive energy approaches [1]. The design studio allows to edit and visualize existing power-grid models graphically, create new power-grid network models, simulate those networks, and inject various scenario-specific perturbations to evaluate specific configurations of transactive energy simulations. The design studio provides (i) a novel Domain-Specific Modeling Language (DSML) using the Web-based Generic Modeling Environment (WebGME [4]) for the graphical modeling of power-grid, cyber-physical attacks, and TE scenarios, and (ii) a reusable cloud-hosted simulation backend using the Gridlab-D power-grid distribution system simulation tool [2]."
pub.1174179560,The transformation of CRE technology with the digital workplace,"In summer 2014 (Vol. 3, No. 4) this Journal published a paper by the same author on the importance of the convergence of the technologies of big data/analytics, mobile and cloud computing, social media (the nexus of forces) and the internet of things to the changing work of the CRE profession where new roles and skill sets were emerging. Now, two years later, the industry is much deeper into the disruption of business models in the digital era with more new technologies and structures emerging that are blurring the virtual and the real, and blending art with science at the same time. Nowhere is this more evident than in the work of the world-renowned architect Frank Gehry, who has transformed the art of the architecture field into a new digital business model by applying concepts in computer science, borrowed from another industry, into a more efficient and effective way to bring new forms of architecture into the world. At the same time, these buildings are enlivening the streets and neighbourhoods of cities (think of the Guggenheim in Bilbao, for example) that many new buildings fail to do. There is the ability. Now the concepts of ‘build to last’ have to be changed into ‘build to change’, allowing structures, from external walls to internal configurations, the ability to adapt to future unknown requirements. It is time for CRE to borrow from the architecture field in the art of design thinking to create a new digital workplace (DW) to bring the latest technologies to organisations to transform the work of planning, designing, constructing, operating, managing and optimising these more agile built environments. These new technologies available for the DW include innovative platforms, smart machines, robots, 3D printers, sensors and even ‘digital twins’, as well as existing systems such as IWMS, CAFM, BIM, GIS and CAD. This paper proposes to cover what DW is and how CRE professionals can join their partners in the new workplace, HR and IT, who are already designing DWs for knowledge workers. With this cross-functional team, an extension of the DW can be created for facilities management and real estate experts both within their organisations and for their suppliers and vendors. The result will be a more engaged workforce, better designed spaces and more efficient and effective work in the built environment."
pub.1172887062,Soft computing approaches for dynamic multi-objective evaluation of computational offloading: a literature review,"Optimizing computational offloading in Mobile Edge Computing (MEC) environments presents a multifaceted challenge requiring innovative solutions. Soft computing, recognized for its ability to manage uncertainty and complexity, emerges as a promising approach for addressing the dynamic multi-objective evaluation inherent in computational offloading scenarios. This paper conducts a comprehensive review and analysis of soft computing approaches for Dynamic Multi-Objective Evaluation of Computational Offloading (DMOECO), aiming to identify trends, analyze existing literature, and offer insights for future research directions. Employing a systematic literature review (SLR) methodology, we meticulously scrutinize 50 research articles and scholarly publications spanning from 2016 to November 2023. Our review synthesizes advancements in soft computing techniques, including fuzzy logic, neural networks, evolutionary algorithms, and probabilistic reasoning, as applied to computational offloading optimization within MEC environments. Within this comprehensive review, existing approaches are categorized and analyzed into distinct research lines based on methodologies, objectives, evaluation metrics, and application domains. The evolution of soft computing-based DMOECO strategies is emphasized, showcasing their effectiveness in dynamically balancing various computational objectives, including energy consumption, latency, throughput, user experience, and other pertinent factors in computational offloading scenarios. Key challenges, including scalability issues, lack of real-world deployment validation, and the need for standardized evaluation benchmarks, are identified. Insights and recommendations are provided to enhance computational offloading optimization. Furthermore, collaborative efforts between academia and industry are advocated to bridge the theoretical developments with practical implementations. This study pioneers the use of SLR methodology, offering valuable perspectives on soft computing in DMOECO and synthesizing state-of-the-art approaches. It serves as a crucial resource for researchers, practitioners, and stakeholders in the MEC domain, illuminating trends and fostering continued innovation in computational offloading strategies."
pub.1104910845,"Update, Conclusions, and Recommendations for the “Unconventional Water Resources and Agriculture in Egypt”","This chapter encapsulates the essential sustainability challenges (in terms of conclusions and recommendations) of the existing main agri-food system and presents insights derived from the cases in the volume. In addition, some (update) findings from a few recently published research work related to the sustainability covered themes. This chapter focuses on unconventional water resources and sustainability of agricultural environment in Egypt that were documented in this volume. To this end, we identify six main contribution areas, which includes toward integration of irrigation and drainage water, assessment of drainage systems, optimization of agricultural waste and reuse, drainage water quality assessment and improvement technologies, toward a sustainable reuse of water resources in Egypt, and securing water resources in Egypt. Therefore, conclusions will be built on researcher visions gained concerning study findings and limitations. In addition, this chapter encompasses evidence on a set of recommendations to direct future research toward sustainability of the agriculture, which is a main strategic theme of the Egyptian government. The set of recommendations is presented for specialists involved in following additional research to exceed the scope and findings of this volume."
pub.1101245365,"The North West Shelf (NWS), a Digital Petroleum Ecosystem (PDE) in a Big Data Scale","The North West Shelf (NWS) and its associated petroleum systems have varied geographies, geomorphologies and complex geological environments. In spite of the ongoing exploration activities in many sedimentary basins, the appraisal and field development campaigns are challenging. Besides, interpreting the connectivity between petroleum systems is challenging. The heterogeneity and multidimensionality of multi-stacked reservoirs associated with multiple oil and gas fields complicate the data integration process. Volumes and varieties of data existing in these basins are in different scales, sizes and formats, demanding new storage and retrieval methods, emphasizing both data integration and data structuring. Since the data are in terabyte size; the multiple dimensions and domains need to be brought in a single repository, we take advantage of Big Data tools and technologies. In this context, we aim at articulating the digital petroleum ecosystems and petroleum database management systems, with new data modelling, data warehousing and mining, visualization and interpretation artefacts. This approach facilitates the data management not only for individual basins but groups of basins of the NWS. Warehoused cuboid metadata can explore the connections providing new insights in the data interpretation and knowledge of new prospective areas. The multidimensional warehousing repository that supported by cloud computing, data analytics and virtualization features, provide new opportunities for delivering quality and just-in-time online ecosystem services. Other goals are deducing an integrated unified metadata model and characterizing the connectivity among the basins of the NWS and associated oil & gas fields. The study supports the features of PDE and its knowledge management."
pub.1140203722,Casual Rerouting of AERONET Sun/Sky Photometers: Toward a New Network of Ground Measurements Dedicated to the Monitoring of Surface Properties?,"This paper presents an innovative method for observing vegetation health at a very high spatial resolution (~5 × 5 cm) and low cost by upgrading an existing Aerosol RObotic NETwork (AERONET) ground station dedicated to the observation of aerosols in the atmosphere. This study evaluates the capability of a sun/sky photometer to perform additional surface reflectance observations. The ground station of Toulouse, France, which belongs to the AERONET sun/sky photometer network, is used for this feasibility study. The experiment was conducted for a 5-year period (between 2016 and 2020). The sun/sky photometer was mounted on a metallic structure at a height of 2.5 m, and the acquisition software was adapted to add a periodical (every hour) ground-observation scenario with the sun/sky photometer observing the surface instead of being inactive. Evaluation is performed by using a classical metric characterizing the vegetation health: the normalized difference vegetation index (NDVI), using as reference the satellite NDVI derived from a Sentinel-2 (S2) sensor at 10 × 10 m resolution. Comparison for the 5-year period showed good agreement between the S2 and sun/sky photometer NDVIs (i.e., bias = 0.004, RMSD = 0.082, and R = 0.882 for a mean value of S2A NDVI around 0.6). Discrepancies could have been due to spatial-representativeness issues (of the ground measurement compared to S2), the differences between spectral bands, and the quality of the atmospheric correction applied on S2 data (accuracy of the sun/sky photometer instrument was better than 0.1%). However, the accuracy of the atmospheric correction applied on S2 data in this station appeared to be of good quality, and no dependence on the presence of aerosols was observed. This first analysis of the potential of the CIMEL CE318 sun/sky photometer to monitor the surface is encouraging. Further analyses need to be carried out to estimate the potential in different AERONET stations. The occasional rerouting of AERONET stations could lead to a complementary network of surface reflectance observations. This would require an update of the software, and eventual adaptations of the measurement platforms to the station environments. The additional cost, based on the existing AERONET network, would be quite limited. These new surface measurements would be interesting for measurements of vegetation health (monitoring of NDVI, and also of other vegetation indices such as the leaf area and chlorophyll indices), for validation and calibration exercise purposes, and possibly to refine various scientific algorithms (i.e., algorithms dedicated to cloud detection or the AERONET aerosol retrieval algorithm itself). CIMEL is ready to include the ground scenario used in this study in all new sun/sky photometers."
pub.1130819101,Role of Interactive Multimedia to support MOOC for Enhanced E-learning in the Higher Education Sector in Oman,"E-learning is the best way of getting knowledge at present especially when it comes to distant education. E-learning is a computer-based educational system that allows you to learn in any place at any time. Earlier e-learning solutions were delivered on CDROMs but nowadays, an online platform is making easier ways for such implementation in terms of scalability as well as usability. Online courses such a MOOC (Massive Open Online Course), which is a way of delivering the content of learning online for any person with easy access from any part of the World is becoming much more popular for certification courses. This research paper focuses on the requirements of such implementation as a part of teaching and learning strategy in one of the higher education institutions in Oman. The existing system in these days is Moodle which is one of the Virtual Learning Environments. Through Moodle, teachers are sharing many eLearning based tools with students. It is based on PowerPoint in Moodle learning and different activities on papers. The methodology used in this project is a mixed methodology – quantitative and qualitative, which includes a questionnaire to get more opinions from different people about this research and there also interviews. The study is performed in context to one of the leading private higher education institutions (HEI) in Oman. The proposed research will suggest a new E-Learning application, especially for some practical modules. Based on the above study, researchers have a plan to propose a new framework and solution for practical modules in HEI especially with the integration of MOOC. The large scale implementation of the proposed research and its solution will be on a cloud, in order to provide easy access along with scalability."
pub.1110577510,Konkurencyjność polskich przedsiębiorstw wobec potencjału cloud computingu,"Enterprises seek for possibilities to limit the costs and for areas that stimulate the level of innovation. Both of these aspects can be effectively supported by application of cloud computing, without a simultaneous need to make a choice of a trade-off type. The aim of the article is to prove that cloud computing provides entrepreneurs with the possibility to limit the costs and at the same time to support their activities related to the selected direction of innovation development. It has both a direct influence on the level and structure of costs in the enterprise, as well as an indirect influence, e.g. related to shortening the time for introduction of new solutions to the market, making decisions or limiting the costs of projects. Introduction Polish enterprises struggle with a growing number of barriers in their closer or further environment, determining the potential and the rate of their development. The possibility to compete – locally (at the national level), regionally (within the EU), as well as internationally or globally – due to the number of entities from the SME sector, in the cumulative approach, becomes a problem of a macroeconomic nature. While the external barriers conditioning the scope of development of enterprises, and the world trends in the directions of development are similar or even the same to all entities, those that skilfully construct their own business models have the best chances not only to survive in the difficult times, but also, or even primarily, to effectively compete on the international market. Because, according to the approach that not the strongest but those who are able to easily adapt to changing conditions will survive, the selection of the resources allowing for flexibility and adaptability of the enterprise determines the potential for the development of its competitiveness, also in the context of increasing of the level of innovation. The solution that allows for achieving such a result is the application of the cloud computing model, which is still a relatively new concept of organisation management among the Polish business entities. The purpose of this elaboration is to indicate the potential of the role of cloud computing in supporting the achievement of competitiveness of Polish enterprises. This will be accomplished by reviewing literature and current market studies, which in the first part of this elaboration are associated only with the essence of the concept of competitiveness, and the issues of competitiveness of Polish enterprises. The second part is focused on the analysis of the role of cloud computing, and determination of the scope of influence of its properties on the possibility to balance the barriers which determine the level of competitiveness of Polish enterprises. The work is conceptual, showing the possible areas of influence of the two phenomena of initially independent meaning and coexistence. Innovativeness of Poland in relation to other EU countries Essential definitio"
pub.1174625610,From Satellite to Ground: Satellite Assisted Visual Localization with Cross-view Semantic Matching,"One of the key challenges of visual Simultaneous Localization and Mapping (SLAM) in large-scale environments is how to effectively use global localization to correct the cumulative errors from long-term tracking. This challenge presents itself in two main aspects: first, the difficulty for robots in revisiting previous locations to perform loop closure, and second, the considerable memory resources required to maintain point-cloud-based global maps. Recent solutions have resorted into neural networks, using satellite images as the references for ground-level localization. However, most of these methods merely provide cross-view patch-matching results, which leads to unfeasible in integration with the SLAM system. To address these issues, we present a semantic-based cross-view localization method. This approach combines semantic information with a reward and penalty mechanism, enabling us to obtain a global probability map and achieve precise 3-degree-of-freedom (3-DoF) localization. Based on that, we develop a SLAM system that capitalizes on satellite imagery for global localization. This strategy effectively bridges the gap between SLAM and real-world coordinates while also substantially reducing accumulated errors. Our experimental results demonstrate that our global localization method significantly outperforms existing satellite-based systems. Moreover, in scenarios where the robot struggles to find loop closures, employing our localization method improves the SLAM accuracy."
pub.1103155416,Working Set Size Estimation Techniques in Virtualized Environments,"Energy consumption is a primary concern for datacenters? management. Numerous datacenters are relying on virtualization, as it provides flexible resource management means such as virtual machine (VM) checkpoint/restart, migration and consolidation. However, one of the main hindrances to server consolidation is physical memory. In nowadays cloud, memory is generally statically allocated to VMs and wasted if not used. Techniques (such as ballooning) were introduced for dynamically reclaiming memory from VMs, such that only the needed memory is provisioned to each VM. However, the challenge is to precisely monitor the needed memory, i.e., the working set of each VM. In this paper, we thoroughly review the main techniques that were proposed for monitoring the working set of VMs. Additionally, we have implemented the main techniques in the Xen hypervisor and we have defined different metrics in order to evaluate their efficiency. Based on the evaluation results, we propose Badis, a system which combines several of the existing solutions, using the right solution at the right time. We also propose a consolidation extension which leverages Badis in order to pack the VMs based on the working set size and not the booked memory. The implementation of all techniques, our proposed system, and the benchmarks we have used are publicly available in order to support further research in this domain."
pub.1110737522,Smart Aagricultural Eenvironment Monitoring System using IOT,"Agriculture is the chief support of an economy and the pivotal sector for ensuring food security. The agriculture sector recorded satisfactory growth due to improved technology, irrigation, inputs and pricing policies.But now due to migration of farmers from rural to urban there is hindrance in agriculture. To overcome this problem we go for smart agricultural environment monitoring techniques using IOT. The Internet of things (IOT) is remodeling the agriculture enabling the farmers with the wide range of techniques such as precision and sustainable agriculture to face challenges in the field. IOT interconnects human to thing, thing to thing and human to human. IOT enables the objects to be sensed and controlled remotely across existing network model. The paper comprises of sensors that sense the field parameters such as phlevel,temperature, humidity,moisture and fertility in the farm. The sensed values are validated and later sent to the WI-FI module and from WI-FI module the validated data are sent to the farmer’s mobile or laptop using cloud. The farmers are also notified by SMS if the field needs a care. An algorithm is developed with threshold values of temperature, humidity, moisture and fertility that are programmed into a node MCU to control water quantity. Now the farmer can automate the motor from anywhere in the world."
pub.1168273041,AscDAMs: Advanced SLAM-based channel detection and mapping system,"Obtaining high-resolution, accurate channel topography and deposit conditions
is the prior challenge for the study of channelized debris flow. Currently,
wide-used mapping technologies including satellite imaging and drone
photogrammetry struggle to precisely observe channel interior conditions of
mountainous long-deep gullies, particularly those in the Wenchuan Earthquake
region. SLAM is an emerging tech for 3D mapping; however, extremely rugged
environment in long-deep gullies poses two major challenges even for the
state-of-art SLAM: (1) Atypical features; (2) Violent swaying and oscillation
of sensors. These issues result in large deviation and lots of noise for SLAM
results. To improve SLAM mapping in such environments, we propose an advanced
SLAM-based channel detection and mapping system, namely AscDAMs. It features
three main enhancements to post-process SLAM results: (1) The digital
orthophoto map aided deviation correction algorithm greatly eliminates the
systematic error; (2) The point cloud smoothing algorithm substantially
diminishes noises; (3) The cross section extraction algorithm enables the
quantitative assessment of channel deposits and their changes. Two field
experiments were conducted in Chutou Gully, Wenchuan County in China in
February and November 2023, representing observations before and after the
rainy season. We demonstrate the capability of AscDAMs to greatly improve SLAM
results, promoting SLAM for mapping the specially challenging environment. The
proposed method compensates for the insufficiencies of existing technologies in
detecting debris flow channel interiors including detailed channel morphology,
erosion patterns, deposit distinction, volume estimation and change detection.
It serves to enhance the study of full-scale debris flow mechanisms, long-term
post-seismic evolution, and hazard assessment."
pub.1181657324,ЦИФРОВА ТРАНСФОРМАЦІЯ ІНФОРМАЦІЙНО-АНАЛІТИЧНОГО ЗАБЕЗПЕЧЕННЯ УПРАВЛІНСЬКИХ ПРОЦЕСІВ У СУЧАСНИХ ОРГАНІЗАЦІЯХ В УМОВАХ ГЛОБАЛЬНОЇ ЦИФРОВІЗАЦІЇ,"The topic of transforming the information and analytical support of organization management in the context of global digitalization is of great importance for the effective management of an organization. It contributes to the creation of efficient, transparent and adaptive management processes that provide competitive advantages in a dynamic digital environment. The study focuses on the introduction of new information systems for collecting, processing and analyzing data in real time. The study is based on the transformation of information and analytical support for the organization's management in the context of global digitalization, which includes: the use of advanced technologies such as artificial intelligence, blockchain, machine learning to automate data collection and analysis processes; integration of various data sources, which allows to obtain a complete picture of the organization's activities and external environment; the use of analytical tools for processing large amounts of data (Big Data), which allows for more accurate forecasts and identifying hidden trends; cloud technologies, which allow for increased mobility and efficiency of decision-making; automation of data collection, processing and visualization processes frees up human resources to perform strategic tasks. The analysis has shown that the data mining approach to business intelligence is becoming increasingly important in the modern world, where data is the basis for decision-making. This approach involves the use of various methods and technologies to collect, process, analyze and visualize data, allowing organizations to obtain valuable information and make informed decisions. It is concluded that the approach to data analysis as a method of business intelligence can help identify existing gaps in research and practice, and point to new areas for further study. The practical significance of the study is that digital technologies in the information and analytical support of organizations provide opportunities for more efficient management of organizations, active promotion of the principles of sustainable development in the context of global digitalization."
pub.1136779658,A Feature Adaptive Learning Method for High-Density sEMG-Based Gesture Recognition,"Surface electromyography (sEMG) array based gesture recognition, which is widely-used, could provide natural surfaces for human-computer interaction. Currently, most existing gesture recognition methods with sEMG array only work with the fixed and pre-defined electrodes configuration. However, changes in the number of electrodes (i.e., increment or decrement) is common in real scenarios due to the variability of physiological electrodes. In this paper, we study this challenging problem and propose a random forest based ensemble learning method, namely feature incremental and decremental ensemble learning (FIDE). FIDE is able to support continuous changes in the number of electrodes by dynamically maintaining the matrix sketches of every sEMG electrode and spatial structure of sEMG array. To evaluate the performance of FIDE, we conduct extensive experiments on three benchmark datasets, including NinaPro, CSL-hdemg, and CapgMyo. Experimental results demonstrate that FIDE outperforms other state-of-the-art methods and has the potential to adapt to the evolution of electrodes in the changing environments. Moreover, based on FIDE, we implement a multi clients/server collaboration system, namely McS, to support feature adaption in real-world environment. By collecting sEMG using two clients (smartphone and personal computer) and adaptively recognizing gestures in the cloud server, FIDE significantly improves the gesture recognition accuracy in electrode increment and decrement circumstances."
pub.1168297619,A Parallel Sequential SBAS Processing Framework Based on Hadoop Distributed Computing,"With the rapid development of microwave remote sensing and SAR satellite systems, the use of InSAR techniques has been greatly encouraged due to the abundance of SAR data with unprecedented temporal and spatial coverage. Small Baseline Subset (SBAS) is a promising time-series InSAR method for applications involving deformation monitoring of the Earth’s crust, and the sequential SBAS method is an extension of SBAS that allows long-term and large-scale surface displacements to be obtained with continuously auto-updating measurement results. As the Chinese LuTan-1 SAR system has begun acquiring massive SAR image data, the need for an efficient and lightweight InSAR processing platform has become urgent in various research fields. However, traditional sequential algorithms are incapable of meeting the huge challenges of low efficiency and frequent human interaction in large-scale InSAR data processing. Therefore, this study proposes a distributed parallel sequential SBAS (P2SBAS) processing chain based on Hadoop by effectively parallelizing and improving the current sequential SBAS method. P2SBAS mainly consists of two components: (1) a distributed SAR data storage platform based on HDFS, which supports efficient inter-node data transfer and continuous online data acquisition, and (2) several parallel InSAR processing algorithms based on the MapReduce model, including image registration, filtering, phase unwrapping, sequential SBAS processing, and so on. By leveraging the capabilities associated with the distributed nature of the Hadoop platform, these algorithms are able to efficiently utilize the segmentation strategy and perform careful boundary processing. These parallelized InSAR algorithm modules can achieve their goals on different nodes in the Hadoop distributed environment, thereby maximizing computing resources and improving the overall performance while comprehensively considering performance and precision. In addition, P2SBAS provides better computing and storage capabilities for small- and medium-sized teams compared to popular InSAR processing approaches based on cloud computing or supercomputing platforms, and it can be easily deployed on clusters thanks to the integration of various existing computing components. Finally, to demonstrate and evaluate the efficiency and accuracy of P2SBAS, we conducted comparative experiments on a set of 32 TerraSAR images of Beijing, China. The results demonstrate that P2SBAS can fully utilize various computing nodes to improve InSAR processing and can be applied well in large-scale LuTan-1 InSAR applications in the future."
pub.1176170319,Distributed Data Analysis in Cloud Services for Insurance Companies,"This article embarks on an insightful journey through the realm of advanced data analysis techniques which can be used in the insurance area, with a keen focus on the applications and capabilities of Graph Neural Networks (GNN) in the following sector. The article is structured into several chapters, which include the overview of existing and commonly used approaches of the data representation, the possible ways of data analysis of the data in such a representation, deep dive into the concept of GNN for the graph data analysis and the applicability of each approach in the insurance industry. The initial chapter introduces the two main concepts of the data representation, which are the commonly used relational database and the more modern approach of dimensional data design. Then the focus is moved to the graph data representation, which also can be used for data analysis in the cloud environment. To achieve the best applicability in the insurance industry, particularly in underwriting and claims management, the article analyzes the advantages of each approach to the data representation as well as its drawbacks. To conclude the chapter, the comparison table of the three approaches is presented. Based on the comparison table, the decision to use the graph representation is made as it enables the industry to unravel complex relationships and dependencies amid various data points—such as policyholder history, incident particulars, and third-party information—resulting in more accurate risk assessments and efficient claim resolutions. Then the article presents the concept of Graph Neural Networks, a rather new concept which can be used to analyze the data, represented in a graph form using machine learning algorithms. The potential of using this approach for the data analysis in the insurance area and some possible use cases are described. The advantages of using this approach include ability to effectively capture and leverage the complex relationships inherent in graph- structured data and a powerful framework for analyzing and processing graph-structured data. However, the potential drawbacks of the approach such as complexity to design and difficulties in scaling are also considered. Further along, the article probes the strategic integration of Graph Neural Networks with real-time and dynamic data environments, examining their adaptability to evolving network patterns and temporal dependencies. We discuss how this adaptability is paramount in contexts like real-time decision-making and predictive analysis, which are crucial for staying agile in a rapidly changing market landscape. Then the exact use cases of the GNN applicability in the insurance area are provided, including the claim assignment and underwriting process are described in detail. Furthermore, the simplified mathematical formulation of the underwriting process is provided, which elaborates the role GNNs play in propelling actuarial science with their capability to incorporate node a"
pub.1156897796,RETRACTED ARTICLE: System simulation of land use spatial planning method and environment management strategy analysis by using machine vision,"In March 2018, the State Council issued a reform plan for institutions. Through the establishment of the Ministry of natural resources, the planning of the national space system was carried out, and the historical division phenomenon within the administrative institutions was solved by using the transformation of fragmentation. Subsequently, relevant functional departments began to design the land spatial planning system and coordinate the mechanism. From the policy guidance, it can be seen that land use spatial planning has risen to the central position in the national management system. From the current situation of planning control, planning control becomes more and more important in the management process. At the same time, the construction of ecological civilization requires us to examine the regulatory mechanism from a new perspective. Therefore, based on machine vision technology, this paper develops a land use spatial planning method system. The sub-pixel edge detection algorithm used in the system design can obtain the point coordinates on the edge of the gauge block with high accuracy. After testing, we can see that in terms of storage space, the octree map used in this paper can save more space than the point cloud map under the same resolution. The data results obtained from the test are also better than the existing system, indicating that the machine vision spatial planning system studied in this paper can better serve the field of land and resources planning."
pub.1160046699,Artificial Intelligence and Machine Learning for Smart Farming Using Cloud Computing,"In spite of the interpretation individuals may have regarding the farming business, the actuality is that today’s farming industry is data oriented, detailed, accurate, and smarter. The fast exposure of Internet of Things (IoT) based technology has reconfigured nearly every industry, including agriculture, by converting it into smart agriculture. Such comprehensive advancements are sweeping away existing farming techniques and generating novel options with a collection of obstacles. This paper emphasize the prospects and capabilities of wireless sensors implemented with IoT with the usage of artificial intelligence (AI) and machine learning (ML) in agriculture, as well as the challenges which the farming industry can face when combining this approach with the conventional. IoT devices and ML techniques associated with various types of sensors that came across in the farming applications are examined. ML techniques that can be helpful in the implementation of smart agriculture are elaborated. Which type of sensors can be used in various agricultural applications like evaluation of crops, identification of variety of the crop, detection of plant disease etc., are defined. How AI and IoT is benefitting agriculture is also explained in detail. Furthermore, how the use of unmanned aerial vehicles like drones can be used to keep an eye on the crop is also mentioned. At the end, the challenges associated with the adoption of IoT/AI in farming are also highlighted. This chapter emphasizes the prospects and capabilities of wireless sensors implemented with IoT with the usage of Artificial Intelligence (AI) and Machine Learning (ML) in agriculture, as well as the challenges which the farming industry can face when combining this approach with the conventional. It examines IoT devices and ML techniques associated with various types of sensors that came across in the farming applications and elaborates ML techniques that can be helpful in the implementation of smart agriculture. With the usage of AI and ML, farmers can use knowledgeable data and various tools that helps in better decision making, better outcomes, reduction of wastage of food and hence minimize any negative impacts upon the environment. A huge amount of data is gathered by sensors using a smart farming approach and hence this data can be used in the improvement of business strategies, performance of employees, and increasing the efficiency of various devices involved."
pub.1171427069,AGRICULTURAL MARKETING IN THE CONDITIONS OF DIGITALIZATION,"In the work, the management of agricultural resources is considered from the standpoint of the economic agrosystem, which should be built on the principles of optimization and predictability, which can be achieved due to the digitization of the industry. This, in turn, leads to an increase in the role of marketing in the process of collecting, analyzing and processing information. The purpose of the article is the theoretical and methodological substantiation of the applied aspects of agricultural marketing in the conditions of digital transformation. The problems of the implementation of digital technologies in rural areas are identified: the reduction of the share of the rural population, which reduces opportunities in the field of education and employment, the limitation of the basic IT infrastructure and the need for investment costs for the implementation of specific digital solutions taking into account existing opportunities. Agromarketing is proposed to be considered as an integrative management system of an enterprise for the production and sale of agricultural products, which ensures the formation of sustainable relationships between agricultural enterprises and consumers to meet the needs of the population in agricultural products and services. This goal should be achieved by providing not only monitoring of the agricultural market, but also forecasting of long-term conditions with a high degree of reliability; minimization of the impact on the environment and production of quality products; formation of the appropriate material and technical base for the storage of agricultural products; minimization of intermediary structures. The main task of agricultural marketing is to identify and research the needs of potential consumers of agro-industrial complex; selection of products for production, establishment of its properties; coordination of terms and volumes of production; preservation and expansion of the existing market share in accordance with the set goals; increasing sales volumes and obtaining the desired results. The specificity of agro-industrial production and sale of agricultural products determines the variety of specific marketing schemes, including the study of the state and dynamics of consumer demand and the use of the obtained data in the process of developing and making economic decisions; maximum adaptation of production to market requirements in order to improve the efficiency of the enterprise's functioning; influencing market and consumer demand through means such as advertising, sales promotion and shaping in a direction that is meaningful to the enterprise. The work defines the main areas of implementation of digital technologies in marketing activities of agribusiness: digitalization of the agricultural market, digitalization in the field of enterprise management, precision agriculture with climate control, use of cloud platforms and big data analytics. At the same time, the implementation of digital technologie"
pub.1063244654,Abstract 3966: GenomeSpace: An environment for frictionless bioinformatics,"Abstract Over the past several years, cancer genome characterization initiatives such as The Cancer Genome Atlas and the Tumor Sequencing Project have produced an explosion of genomic data. The pace of data production has recently increased with the adoption of next-generation sequencing technologies and large-scale data production efforts to discover the breadth of genomic variation in humans. Comprehensive analysis of these datasets requires the coordinated use of Web-based applications and data repositories, desktop analysis tools and visualizers, and single-purpose algorithms. However, the effort required to transfer data between tools, convert between data formats, and manage results often prevents researchers from utilizing the wealth of methods available to them. Many integrative genomics and translational “bench to bedside” discoveries are possible with combinations of existing tools, but the necessary transitions between them puts them out of the reach of most researchers. GenomeSpace, http://www.genomespace.org, is an environment that brings together diverse computational tools, enabling scientists without programming skills to easily combine their capabilities. It aims to offer a common space to create, manipulate and share an ever-growing range of genomic analysis tools. GenomeSpace features support for cloud-based data storage and analysis, multi-tool analytic workflows, automatic conversion of data formats, and ease of connecting new tools to the environment. A set of six “GenomeSpace-enabled” seed tools developed by collaborating organizations provides a comprehensive platform for the analysis of cancer data: Cytoscape (UCSD), Galaxy (Penn State University), GenePattern (Broad Institute), Genomica (Weitzmann Institute), Integrative Genomics Viewer (Broad Institute), and the UCSC Genome Browser (UCSC). We show how researchers can use GenomeSpace to effortlessly combine the capabilities of all of these tools in several cancer research scenarios. Citation Format: {Authors}. {Abstract title} [abstract]. In: Proceedings of the 103rd Annual Meeting of the American Association for Cancer Research; 2012 Mar 31-Apr 4; Chicago, IL. Philadelphia (PA): AACR; Cancer Res 2012;72(8 Suppl):Abstract nr 3966. doi:1538-7445.AM2012-3966"
pub.1029559580,"Beyond the Pipelines: Cloud Computing Facilitates Management, Distribution, Security, and Analysis of High‐Speed Sequencer Data","With the increasing quantity and complexities of molecular data, bioinformatics has gained increasing prominence and importance. Substantial resources have been invested in bioinformatic research and substantial gains have been realized, particularly in three areas: data analysis algorithms, data repositories, and data visualization. Tools to analyze various types of biological data have been developed, described, and distributed; giant public repositories have been set up; and graphical visualization tools can be used either directly from public servers or downloaded locally to the user's computer. Until now, these have proved to be sufficient. However, as new technologies, in particular high‐speed sequencers (HSSs), greatly increase quantities of molecular data, existing approaches to handle these three areas, while vital, are proving insufficient. In this chapter, we will describe additional tools and techniques needed to effectively handle the giant datasets that are already being generated, how these datasets should be managed in the modern distributed research environment, and, perhaps most importantly, how the complexity of handling these datasets could be reduced through effective integration of new and existing tools, techniques, and algorithms. We expect that this chapter will be most useful to individuals tasked with establishing computational environments needed to support research utilizing HSSs and other high‐throughput devices capable of generating large quantities of molecular data. This chapter is intended to provide sufficient detail to enable the development of appropriate requests for proposals as well as to assist in evaluating competing proposals. For organizations looking to develop an HSS support system internally, this chapter should be very useful in helping them develop appropriate requirements and high‐level design specifications."
pub.1146607789,Recent achievements of the “PROBE” COST Action: Towards profiling of the atmospheric boundary layer at European scale,"<p>Meteorological and air quality surface sensor networks sample atmospheric variables close to the ground while satellite observations provide global spatial coverage of the upper atmosphere. There is however, an observation gap on the temporal variability and vertical structure of atmospheric parameters in the atmospheric boundary layer (ABL). The ABL is the lowest 2 – 3 km of atmosphere above ground where the vertical structure is driven by surface-atmosphere exchanges, ABL-to-free-troposphere exchanges, in addition to larger-scale processes. Most human activities take place in the ABL, it is hence very important to improve our ability to characterize those processes that affect weather conditions, air quality, transport and energy provision systems, and longer-term issues such as climate change adaptation and mitigation, of particular importance in urban settings.</p><p>Motivated by the overarching objective to support the efficient exploitation of ABL data and to maximize their societal impact, the PROBE COST action is creating a cooperation hub where a wide range of stakeholders from Academia, Research structures, Industry, Operational agencies, and general end-users can share advances and expertise on ABL profiling.</p><p>In the first two years of the action, the PROBE partners were able to attract a diverse community of more than 200 users that share information through webinars (on instruments, networks, and high-quality observations) and working group meetings (on ABL profiling in complex terrain and urban environments), and engage the community in a wide range of activities through efficient multi-media communication (http://www.probe-cost.eu/, newsletters, videos, social channels). No less than 5 working groups on thermodynamics, clouds, ABL height, wind and turbulence, and aerosol profiling reported on key ABL parameters, their applications and end-user requirements. A comprehensive document is being compiled that gives insights on “overview, access and benefits” of existing ABL profiling networks (e.g. E-PROFILE, ACTRIS, ICOS, …). Also less known (“hidden”) networks were identified. 5 specific instrument task groups (on microwave radiometers, cloud radars, doppler lidars, automatic lidars and ceilometers, and drones) are developing recommendations for configuration, operation, calibration, and quality control procedures.</p><p>Over the remaining period of the PROBE COST action (until fall 2023), the partners will continue to develop a solid literature (technical reports and scientific publications) on the topic of ABL profiling, improving content through short term scientific visits (either in person or virtual) and focused working groups (mostly virtual). Some partners will participate in a large international effort to better characterize the ABL in urban environments through an intensive measurement campaign to be held in the Paris region (France) in summer 2022 while others are involved in the TEAMx collaboration initiative obse"
pub.1014056157,Security evaluation of the OAuth 2.0 framework,"
                    Purpose
                    
                      – The interoperability of cloud data between web applications and mobile devices has vastly improved over recent years. The popularity of social media, smartphones and cloud-based web services have contributed to the level of integration that can be achieved between applications. This paper investigates the potential security issues of OAuth, an authorisation framework for granting third-party applications revocable access to user data. OAuth has rapidly become an interim
                      de facto
                      standard for protecting access to web API data. Vendors have implemented OAuth before the open standard was officially published. To evaluate whether the OAuth 2.0 specification is truly ready for industry application, an entire OAuth client server environment was developed and validated against the speciation threat model. The research also included the analysis of the security features of several popular OAuth integrated websites and comparing those to the threat model. High-impacting exploits leading to account hijacking were identified with a number of major online publications. It is hypothesised that the OAuth 2.0 specification can be a secure authorisation mechanism when implemented correctly.
                    
                  
                  
                    Design/methodology/approach
                    – To analyse the security of OAuth implementations in industry a list of the 50 most popular websites in Ireland was retrieved from the statistical website Alexa (Noureddine and Bashroush, 2011). Each site was analysed to identify if it utilised OAuth. Out of the 50 sites, 21 were identified with OAuth support. Each vulnerability in the threat model was then tested against each OAuth-enabled site. To test the robustness of the OAuth framework, an entire OAuth environment was required. The proposed solution would compose of three parts: a client application, an authorisation server and a resource server. The client application needed to consume OAuth-enabled services. The authorisation server had to manage access to the resource server. The resource server had to expose data from the database based on the authorisation the user would be given from the authorisation server. It was decided that the client application would consume emails from Google’s Gmail API. The authorisation and resource server were modelled around a basic task-tracking web application. The client application would also consume task data from the developed resource server. The client application would also support Single Sign On for Google and Facebook, as well as a developed identity provider “MyTasks”. The authorisation server delegated authorisation to the client application and stored cryptography information for each access grant. The resource server validated the supplied access token via public cryptography and returned the requested data.
                  
 "
pub.1143765112,Geothermal Production from Existing Oil and Gas Wells: A Sustainable Repurposing Model,"Abstract
                  The geothermal energy industry has never quite realized its true potential despite the seemingly magical promise of nonstop, 24/7 renewable energy sitting just below the surface of the Earth. In this paper, we discuss an integrated cloud-based workflow aimed at evaluating the cost-effectiveness of adopting geothermal production in low to medium enthalpy systems by either repurposing existing oil and gas wells or by co-producing thermal and fossil energy. The workflow introduces an automated and intrinsically secure decision-making process to convert mature oil and gas wells into geothermal wells, enabling both operational and financial assessment of the conversion process, whether partial or complete.
                  The proposed workflow focuses on the reliability and transparency of fully automated technical processes for the geological, hydrodynamic, and mechanical configuration of the production system to ensure the financial success of the conversion project, in terms of heat production potential and cost of development. The decision-making portion of the workflow comprises the technical, social, environmental factors driving the return on investment for the total or partial conversion of wells to geothermal production. These components are evaluated using artificial intelligence (AI) algorithms that reduce bias in the decision-making process. The automated workflow involves assessment of the following: Heat Potential: A data-driven model to determine the geothermal heat potential using geological conditions from basin modeling and data from offset wells.Flow Modeling: An ultra-fast, physics-based modeling approach to determine pressure and temperature changes along wellbores to model fluid flow potential, thermal flux, and injection operations.Mechanical Integrity: Casing and completions integrity and configuration are embedded in the process for flow rates modeling.Environmental, Social, and Governance (ESG): A decision modeling framework is setup to ensure the transparent validation of the technical components and ESG factors, including potential for water pollution, carbon emissions, and social factors such as induced seismicity and ambient noise levels
                  The assurance of key ESG metrics will ensure a viable and sustainable transition into a globally available low-carbon source of energy such as geothermal. Our novel cloud- based automated decision-making environment incorporates a blockchain framework to ensure transparency of technical-related processes and tasks, driving the financial success of the conversion project. Ultimately, our automated workflow is designed to encourage and support the widespread adoption of low-carbon energy in the oil and gas industry."
pub.1145547796,"HEAD Access Control Metamodel: Distinct Design, Advanced Features, and New Opportunities","Access control (AC) policies are a set of rules administering decisions in systems and they are increasingly used for implementing flexible and adaptive systems to control access in today’s internet services, networks, security systems, and others. The emergence of the current generation of networking environments, with digital transformation, such as the internet of things (IoT), fog computing, cloud computing, etc., with their different applications, bring out new trends, concepts, and challenges to integrate more advanced and intelligent systems in critical and heterogeneous structures. This fact, in addition to the COVID-19 pandemic, has prompted a greater need than ever for AC due to widespread telework and the need to access resources and data related to critical domains such as government, healthcare, industry, and others, and any successful cyber or physical attack can disrupt operations or even decline critical services to society. Moreover, various declarations have announced that the world of AC is changing fast, and the pandemic made AC feel more essential than in the past. To minimize security risks of any unauthorized access to physical and logical systems, before and during the pandemic, several AC approaches are proposed to find a common specification for security policy where AC is implemented in various dynamic and heterogeneous computing environments. Unfortunately, the proposed AC models and metamodels have limited features and are insufficient to meet the current access control requirements. In this context, we have developed a Hierarchical, Extensible, Advanced, and Dynamic (HEAD) AC metamodel with substantial features that is able to encompass the heterogeneity of AC models, overcome the existing limitations of the proposed AC metamodels, and follow the various technology progressions. In this paper, we explain the distinct design of the HEAD metamodel, starting from the metamodel development phase and reaching to the policy enforcement phase. We describe the remaining steps and how they can be employed to develop more advanced features in order to open new opportunities and answer the various challenges of technology progressions and the impact of the pandemic in the domain. As a result, we present a novel approach in five main phases: metamodel development, deriving models, generating policies, policy analysis and assessment, and policy enforcement. This approach can be employed to assist security experts and system administrators to design secure systems that comply with the organizational security policies that are related to access control."
pub.1149411904,Digital Transformation of Enterprises and Post-Pandemic Sustainable Developmental Goals,"Digitalization and sustainability are central issues in today’s world. Innovation in this area represents catalysts for the development of the circular economy. Digital technologies have the potential to streamline the operation of existing circular business models and thus significantly facilitate the emergence of new ones. The area that has been the subject of research includes tasks such as: product or resource awareness. Along with resources and products, another task is to harmonize the demand side and the supply side. The use of secondary materials or resources must be improved, also increase the longevity of products, and simplify the sharing of product portfolios and, of course, services. All this is taken to some extent as assistance in the process of closing the material cycle or cycles. Among the essential digital technologies that belong to the area of circular economy are Big Data, AI, which is considered one of the most important and best-used systems, for example, the use of so-called chatbots, detailed analysis of the customer and his feedback and is also characterized by to offer of personalization services, Blockchain, Cloud computing, especially in the fields of banks, these are cases of using a network of servers at a greater distance at lower, so optimal costs, 3D printing, but also online platforms. Digitalization can contribute in principle to the functioning of the circular economy and the improvement of resource use with the lowest possible impact on nature and the environment. This paper aims to conceptually define the enterprise digital transformation and sustainable goals after the end of the pandemic. It is also necessary to focus on the consumer perception of the research issue."
pub.1175729046,AscDAMs: advanced SLAM-based channel detection and mapping system,"Abstract. Obtaining high-resolution, accurate channel topography and deposit conditions has been a challenge for the study of channelized debris flow. Currently, widely used mapping technologies including satellite imaging and drone photogrammetry struggle to precisely observe channel interior conditions of long and deep mountainous gullies, particularly those in the Wenchuan earthquake region. SLAM is an emerging tech for 3D mapping; however, extremely rugged environment in long and deep gullies poses two major challenges even for the state-of-the-art SLAM: (1) atypical features and (2) violent swaying and oscillation of sensors. These issues result in large deviation and lots of noise for SLAM results. To improve SLAM mapping in such environments, we propose an advanced SLAM-based channel detection and mapping system, namely AscDAMs. It features three main enhancements to post-process SLAM results: (1) the digital orthophoto map-aided deviation correction algorithm greatly eliminates the systematic error; (2) the point cloud smoothing algorithm substantially diminishes noise; (3) the cross-section extraction algorithm enables the quantitative assessment of channel deposits and their changes. Two field experiments were conducted in Chutou gully, Wenchuan County in China in February and November 2023, representing observations before and after the rainy season. We demonstrate the capability of AscDAMs to greatly improve SLAM results, promoting SLAM for mapping the specially challenging environment. The proposed method compensates for the insufficiencies of existing technologies in detecting debris flow channel interiors including detailed channel morphology, erosion patterns, deposit distinction, volume estimation and change detection. It serves to enhance the study of full-scale debris flow mechanisms, long-term post-seismic evolution, and hazard assessment."
pub.1157079481,"Digital Twin, Servitization, Circular Economy, and Lean Manufacturing","The sustainable production represents important sustainable development goals. The sustainable production can be achieved by adopting sustainability compatible production management systems. The literature refers to some of the important production management systems which can promise sustainability. Among various important servitization, circular economy and lean manufacturing are important ones. Designing and implementing servitization, circular economy, and lean manufacturing have been a problem for firms as this production system requires investment, knowledge, and infrastructure. However, with emergence of technology ecosystem of Industry 4.0, designing and implementing servitization, circular economy, and lean manufacturing have become a goal to be realized easily. The digital twin which is an important technology of Industry 4.0 facilitates designing and implementation of these production management systems. This chapter’s research shows that using digital twin technology, which creates a digital copy of physical things like products, value chains, and the actions and behavior of people involved, can help businesses design products and create value chain systems for things like improving customer service, using resources efficiently, and streamlining production. The digital twin through twinning of physical entity of various activities such as product and value chain activities and effective information management and analytics can help companies to easily implement servitization, circular economy, and lean manufacturing as production systems. This chapter shows that using digital twin technology, which creates a digital copy of physical things like products, value chains, and the actions and behavior of people involved, can help businesses design products and create value chain systems for things like improving customer service, using resources efficiently, and streamlining production. However, digital twin is an innovative integration of some of Industry 4.0 technologies such as cyber-physical system, simulation, big data and analytics, cloud computing, and virtual and augmented reality, etc. In the recent years and particularly mainstreaming of Industry 4.0 concept, interest in the digital twin gas increased in both academic and industry communities. The servitization is becoming highly important in today’s ultracompetitive industrial environment. Servitization enables value offering differentiation, deterring competitors from imitating firm’s value proposition. The circular economy from production management perspective enables manufacturing and remanufacturing of existing products. Lean manufacturing espoused to address the sustainability problem through efficient use of resources, energy conservation, and reduce waste."
pub.1124959037,A Scalable Method for Scheduling Distributed Energy Resources using Parallelized Population-based Metaheuristics,"Recent years have seen an increasing integration of distributed renewable
energy resources into existing electric power grids. Due to the uncertain
nature of renewable energy resources, network operators are faced with new
challenges in balancing load and generation. In order to meet the new
requirements, intelligent distributed energy resource plants can be used which
provide as virtual power plants e.g. demand side management or flexible
generation. However, the calculation of an adequate schedule for the unit
commitment of such distributed energy resources is a complex optimization
problem which is typically too complex for standard optimization algorithms if
large numbers of distributed energy resources are considered. For solving such
complex optimization tasks, population-based metaheuristics -- as e.g.
evolutionary algorithms -- represent powerful alternatives. Admittedly,
evolutionary algorithms do require lots of computational power for solving such
problems in a timely manner. One promising solution for this performance
problem is the parallelization of the usually time-consuming evaluation of
alternative solutions. In the present paper, a new generic and highly scalable
parallel method for unit commitment of distributed energy resources using
metaheuristic algorithms is presented. It is based on microservices, container
virtualization and the publish/subscribe messaging paradigm for scheduling
distributed energy resources. Scalability and applicability of the proposed
solution are evaluated by performing parallelized optimizations in a big data
environment for three distinct distributed energy resource scheduling
scenarios. The new method provides cluster or cloud parallelizability and is
able to deal with a comparably large number of distributed energy resources.
The application of the new proposed method results in very good performance for
scaling up optimization speed."
pub.1116876785,The Gem OB1/IC443/S249 Complex: A Case History of Stellar Evolution,"The extended cloud complex containing members of the Gem OB1 association, the supernova remnant IC443, and the H II region S249 has been studied with IRAS observations at 12,25,60 and 100 microns and WSRT observations at 327 and 1400 MHz and in the 21-cm H I line. A skeleton-like framework of cool dust delineates the boundaries of the region, and physical parameters have been derived for the entire complex, individual H II regions and the shocked and recombined gas within IC443 using the radio and infrared data. IC443 is shown to consist of three interconnected, roughly spherical subshells of vastly different radii and centroids. The geometry is fully constrained by the structural and kinematic data. Two of the subshells together define the usually assumed boundaries of IC443, while the third includes the optical filaments which extend beyond the northeastern rim and which are shown to have well-correlated nonthermal radio components. The available evidence implies that the SNR shock has encountered a pre-existing high density shell. It is shown that the system of subshells is fully consistent with formation by stellar wind driven bubbles generated by association members within the inhomogeneous environment of the complex."
pub.1147027188,Secure Digital Health Data Management in Internet of Things Using Blockchain and Machine Learning,"Advancements in technology have led to dramatic development in the healthcare industry leading to rising demand for better data security and privacy. The medical field is revolutionized by emerging technologies like Artificial Intelligence (AI) and Internet of Things (IoT) for digital health tracking and remote diagnosis. Recently, medical data management systems are encountering challenges in terms of transparency, provenance and traceability of data since most systems use centralized databases which require third-party involvement. The smart technology AI and the immutable technology blockchain can potentially reshape the secure handling of this data. In this chapter, a two-part novel model is proposed to allay any security and privacy concerns in Internet of Healthcare Things. The first part analyses a patient's health parameters using machine learning, detects and discards anomalous data and stores the non-anomalous data with findings in a transparent, secure fashion using blockchain. The second part provides a secure way to preserve critical medical records in blockchain using sandboxed Secure Virtual Machines to avoid any of the privacy issues and computational power constraints of currently existing solutions making this model very secure. Using our model, crucial healthcare data is managed in a secure and transparent fashion within the bounds of the IoHT ecosystem, while also preserving the confidentiality of the concerned parties. This chapter discusses the novel model is proposed to allay any security and privacy concerns in Internet of Healthcare Things. Advancements in technology have led to dramatic development in the healthcare industry leading to rising demand for better data security and privacy. The medical field is revolutionized by emerging technologies like Artificial Intelligence and Internet of Things (IoT) for digital health tracking and remote diagnosis. The chapter also discusses the integration of IoT, blockchain and cloud technologies in the medical environment for offering e-healthcare and e-medical services. It analyses a patient's health parameters using machine learning, detects and discards anomalous data and stores the non-anomalous data with findings in a transparent, secure fashion using blockchain. The typical client-server and cloud-based medical data management systems suffer from various difficulties which are, a single point of failure, centralized data stewardship, data privacy and system vulnerability."
pub.1148348065,Multi-Scale Image Preprocessing and Feature Tracking for Remote CME Characterization,"Coronal Mass Ejections (CMEs) influence the interplanetary environment over
vast distances in the solar system by injecting huge clouds of fast solar
plasma and energetic particles (SEPs). A number of fundamental questions remain
about how SEPs are produced, but current understanding points to CME-driven
shocks and compressions in the solar corona. At the same time, unprecedented
remote and in situ (Parker Solar Probe, Solar Orbiter) solar observations are
becoming available to constrain existing theories. Here we present a general
method for recognition and tracking on solar images of objects such as CME
shock waves and filaments. The calculation scheme is based on a multi-scale
data representation concept a trous wavelet transform, and a set of image
filtering techniques. We showcase its performance on a small set of CME-related
phenomena observed with the SDO/AIA telescope. With the data represented
hierarchically on different decomposition and intensity levels, our method
allows to extract certain objects and their masks from the imaging
observations, in order to track their evolution in time. The method presented
here is general and applicable to detecting and tracking various solar and
heliospheric phenomena in imaging observations. It holds potential to prepare
large training data sets for deep learning. We have implemented this method
into a freely available Python library."
pub.1160649181,Advances in Earth observation and machine learning for quantifying blue carbon,"Blue carbon ecosystems (mangroves, seagrasses and saltmarshes) are highly productive coastal habitats, and are considered some of the most carbon-dense ecosystems on Earth. They are an important nature-based solution for both climate change mitigation and adaptation. Quantifying blue carbon stocks and assessing their dynamics at large scales through remote sensing remains challenging due to difficulties of cloud coverage, spectral, spatial and temporal limitations of multispectral sensors and speckle noise of synthetic aperture radar (SAR). Recent advances in airborne and space-borne multispectral and SAR imagery and Light Detection and Ranging (LiDAR) data, sensor platforms such as unmanned aerial vehicles (UAVs), combined with novel machine learning techniques have offered different users with a wide-range of spectral, spatial, and multi-temporal information for quantifying blue carbon from space. However, a large number of challenges are posed by various traits such as atmospheric correction, water penetration, and water column transparency issues in coastal environments, the multi-dimensionality and size of the multispectral and LiDAR data, the limitation of training samples, and backscattering mechanisms of SAR imagery in the acquisition process. As a result, existing methodologies face major difficulties in accurately estimating blue carbon stocks using these datasets. In this context, emerging and innovative machine learning and artificial intelligence methodologies are often required for robustness and reliability of blue carbon estimates, particularly those using open-source software for signal processing and regression tasks. This review provides an overview of Earth Observation data, machine learning and state-of-the-art deep learning techniques that are currently being used to quantify above-ground carbon, below-ground carbon, and soil carbon stocks of mangroves, seagrasses and saltmarshes ecosystems. Some key limitations and future directions for the potential use of data fusion combined with advanced machine learning, deep learning, and metaheuristic optimisation techniques for quantifying blue carbon stocks are also highlighted. In , the quantification of blue carbon using remote sensing and machine learning approaches holds great potential in contributing to global efforts towards mitigating climate change and protecting coastal ecosystems."
pub.1036437492,The Structure and Evolution of a Numerically Simulated High-Precipitation Supercell Thunderstorm,"The structure and evolution of a high-precipitation (HP) supercell thunderstorm is investigated using a three-dimensional, nonhydrostatic, cloud-scale numerical model (TASS). The model is initialized with a sounding taken from a mesoscale modeling study of the environment that produced the 28 November 1988 Raleigh tornadic thunderstorm. TASS produces a long-lived convective system that compares favorably with the observed Raleigh tornadic thunderstorm. The simulated storm evolves from a multicell-type storm to a multiple-updraft supercell storm. The storm complex resembles a hybrid multicell-supercell thunderstorm and is consistent with the conceptual model of cool season strong dynamic HP supercells that are characterized by shallow mesocyclones. The origin of rotation in this type of storm is often in the lowest levels. Interactions between various cells in the simulated convective system are responsible for the transition to a supercellular structure. An intense low-level updraft core forms on the southwest flank of the simulated storm and moves over a region that is rich in vertical vorticity. The stretching of this preexisting vertical vorticity in the storm’s lowest levels is the most important vertical vorticity production mechanism during the initial stages of the main updraft’s development. Interactions with an extensive cold pool created by the storm complex are also important in producing vertical vorticity as the main updraft grows. Overall, the development of vorticity associated with the main updraft appears similar to nonsupercellular tornadic storms. However, classic supercell signatures are seen early in the simulation associated with other updrafts (e.g., formation of vortex couplet due to tilting of ambient horizontal vorticity, storm splitting, etc.) and are deemed important. In the storm’s supercell stage, rotation is sustained in the lowest levels of the storm despite large amounts of precipitation located near and within the main mesocyclone. Pulsating downdrafts periodically invigorate the storm and the gust front never occludes, thus allowing the main updraft to persist for a prolonged period of time. The storm’s intensity is also maintained by frequent updraft mergers."
pub.1117337151,"Ecotaxonomy: Linking traits, taxa, individuals and samples in a flexible virtual research environment for ecological studies","Major research progress in ecology is being achieved through large-scale collaborations across people, groups and countries. In large-scale projects harmonization of data is tedious and time-consuming, but needs to be done reliably and rapidly. This is especially true if projects investigate under-explored organism groups such as tropical invertebrates. To link taxa to their role in ecosystems, functional traits of the taxa need to be considered. However, despite the urgent need for a common database for invertebrate traits, this is yet to be established. We developed an open web platform, Ecotaxonomy (ecotaxonomy.org), that allows traits, taxa, individuals and samples to be linked within research projects. Ecotaxonomy includes a virtual research environment, allowing project members to work jointly online on the data input, integration and retrieval. The taxonomic system of Ecotaxonomy is based on the Global Biodiversity Information Facility (gbif.org), but may be complemented by morphospecies, pictures, literature and other parameters. Any parameters can be customized inside the system and attached either to taxa, individuals, or environmental samples (Fig. 1). As public output, the system provides interactive identification keys and web catalogs of traits and taxa. Ecotaxonomy is implemented on GCore platform, that is being developed by Complex Cloud Solutions (http://ccs.msk.ru/en/). The GCore is based on Node.js, allowing for fast and efficient standardised programming. Thus, custom modules can be implemented in the future by external developers in the framework of the platform. Ecotaxonomy is now open for beta-testing. After a public release (presumably in 2020), our goal is to keep the system and the code open and ensure data interoperability via Darwin core standards. The initial stage of Ecotaxonomy development (2016-2023) is funded in the framework of a DFG-funded project (SFB 990). To ensure long-term sustainability, we are involving ecological laboratories around the world and ultimately seek to establish a permanent funding by governmental or non-governmental organisations. Using and developing Ecotaxonomy, and linking it to existing open repositories will greatly improve the efficiency and integration of research in trait-based ecology."
pub.1028769349,The inter‐disciplinary modelling of supply chains in the context of collaborative multi‐structural cyber‐physical networks,"
                    Purpose
                    On modern markets, supply chains (SC) shape the competition landscape. At the same time, considerable research advancements have been recently achieved in the area of collaborative networks. Trends in information technology progress for networked systems include development of cyber‐physical networks, cloud service environments, etc. The purpose of this paper is to identify an inter‐disciplinary perspective and modelling tools for new generation SCs which will be collaborative cyber‐physical networks.
                  
                  
                    Design/methodology/approach
                    This study addresses the above‐mentioned research goal by first, developing a methodical vision of an inter‐disciplinary modelling framework for SCM based on the existing studies on SC operations, control and systems theories; and second, by integrating elements of different structures with structures dynamics within an adaptive framework based upon the authors' own research.
                  
                  
                    Findings
                    The inter‐disciplinary modelling framework for multi‐structural SCs has been developed. A new inter‐disciplinary level of model‐based decision‐making support in those SCs is claimed based on the integration of previously isolated problems and modelling tools developed in such disciplines like operations research, control theory, system dynamics, and artificial intelligence.
                  
                  
                    Originality/value
                    The novelty of this paper is the consideration of SC modelling in the context of collaborative cyber‐physical systems. This topic is particularly relevant for researchers and practitioners who are interested in future generation SCs. Particular focus is directed towards the multi‐structural SC modelling, structure dynamics, and inter‐disciplinary problems and models in future SCs. Challenges of integrated optimization in the organizational and informational context are discussed.
                  "
pub.1146437458,ASSESSMENT OF THE SECURITY OF CYBER-PHYSICAL SYSTEMS BASED ON A GENERAL GRAPH,"Динамичное развитие IT– отрасли, повышение автоматизации и технологичности бизнес - процессов, рост числа организаций внедряющих облачную инфраструктуру, а также повсеместная цифровизация, создает благоприятную среду для масштабирования хакерских атак в сфере кибербезопасности. При этом векторами целевых атак являются: социальная инженерия, неквалифицированные пользователи цифровых сервисов, эксплуатация уязвимостей основных систем и сопутствующей инфраструктуры. Вопросы своевременного реагирования, локализации и выявления киберинцидентов являются насущными, требующими временных и финансовых затрат. Для минимизации рисков утраты критических активов компании, необходимо построение эффективных организационных и технических мер, непрерывная адаптация под ландшафт угроз и изменения в объекте защиты. Мероприятия по предотвращению вторжений в защищаемую систему напрямую зависят от точности определения уязвимых мест, внедрения новых средств мониторинга и противодействия. В настоящем исследовании рассмотрен метод оценки защищенности киберфизических систем на основе ориентированного графа атак. Авторами предложен алгоритм определения последовательностей вершин, нахождения максимального количества переходов и выявления возможных связей между ними. Описаны метрики безопасности и векторы атак, определены пять групп категорий опасностей для новых и существующих уязвимостей в соответствии с актуальной версией CVSS 3.1. Проведена оценка рисков потенциальных потерь информационных активов при возникновении фатальных угроз безопасности информации. Особое внимание уделено вопросам совершенствования систем мониторинга и обнаружения вторжений в защищаемые объекты информатизации.
                  The dynamic development of IT - the industry, increasing the automation and technicality of business processes, the growth of the number of enterprise companies implementing cloud infrastructure, as well as widespread digitalization, creates a favorable environment for scaling hacker attacks in the field of cybersecurity. At the same time, the vectors of targeted attacks are: social engineering, unskilled users of digital services, the operation of vulnerabilities of basic systems and related infrastructure. Issues of timely response, localization and detection of cyber-incidents were urgent, requiring time and financial costs. To minimize the risk of loss of critical assets of the company, it is necessary to build effective organizational and technical measures, continuous adaptation to the threat landscape and changes in the protection object. Measures to prevent intrusions into the protected system directly depend on the accuracy of identifying vulnerabilities, the introduction of new monitoring and countermeasures. The present study discusses a method for assessing the security of cyberphysical systems based on an oriented attack graph. The authors propose an algorithm for determining sequences of vertices, finding the maximum number of transitions and identifying possib"
pub.1106848618,"Integrated, structured reporting and diagnosis of localized prostate cancer.","e16573
                  Background: To date 80-90% of men with primary prostate cancer are diagnosed with localized disease with a wide range of risks of future tumor progression. To optimize the balance between tumor removal and preserved continence and erectile function, it is essential to integrate all available information on tumor location, tumor characteristics, and baseline clinical data. Methods: We have developed a cloud-web-based technology allowing flexible access to the clinical data with close integration to the existing IT-environment like the hospital information management systems. The system allows the systematic and structured reporting of medical data generated by the involved disciplines of urology, laboratory, radiology, pathology, and (radiation) oncology, and patient reported health status and outcomes. Results: By use of the developed data integration platform we have collected diagnostic data of a prostate cancer case example scheduled for a multi-disciplinary primary treatment decision. The data comprises the patient health conditions, laboratory measures, multi-parametric MRI, localization information of MRI/US fusion guided biopsies, histo-pathology outcomes of prostate biopsy specimen, and patient reported outcomes on urinary and sexual function status. The collected data was presented in a visually integrated way and was used to discuss in a multi-disciplinary team, with the consistencies or discordances of findings across the various medical domains involved. Based on the discussion a primary treatment decision was concluded for this patient. Conclusions: The developed IT system to integrate heterogeneous medical data was successfully tested in a multi-disciplinary clinical setting. All required clinical variables to provide an informed primary treatment decision for a patient with primary localized prostate cancer was available in the system for discussion during the multi-disciplinary team (MDT) meeting and was presented in a clear visual way to support the interactive discussions between MDT members representing different clinical specialties."
pub.1093465009,Advanced integration of multimedia assistive technologies: A prospective outlook,"In the recent years several studies on population ageing in the most advanced countries argued that the share of people older than 65 years is steadily increasing. In order to tackle this phenomena, a significant effort has been devoted to the development of advanced technologies for supervising the domestic environments and their inhabitants to provide them assistance in their own home. In this context, the present paper aims to delineate a novel, highly-integrated system for advanced analysis of human behaviours. It is based on the fusion of the audio and vision frameworks, developed at the Multimedia Assistive Technology Laboratory (MATeLab) of the Università Politecnica delle Marche, in order to operate in the ambient assisted living context exploiting audio-visual domain features. The existing video framework exploits vertical RGB-D sensors for people tracking, interaction analysis and users activities detection in domestic scenarios. The depth information has been used to remove the affect of the appearance variation and to evaluate users activities inside the home and in front of the fixtures. In addition, group interactions are monitored and analysed. On the other side, the audio framework recognises voice commands by continuously monitoring the acoustic home environment. In addition, a hands-free communication to a relative or to a healthcare centre is automatically triggered when a distress call is detected. Echo and interference cancellation algorithms guarantee the high-quality communication and reliable speech recognition, respectively. The system we intend to delineate, thus, exploits multi-domain information, gathered from audio and video frameworks each, and stores them in a remote cloud for instant processing and analysis of the scene. Related actions are consequently performed."
pub.1109926729,Mobile Health Technologies for Diabetes Mellitus: Current State and Future Challenges,"The prevalence of diabetes is rising globally. Diabetes patients need continuous monitoring, and to achieve this objective, they have to be engaged in their healthcare management process. Mobile health (MH) is an information and communications technology trend to empower chronically ill patients in a smart environment. Discussing the current state of MH technologies is required in order to address their limitations. Existing review articles have evaluated the MH literature based on applicability and level of adoption by patients and healthcare providers. Most of these reviews asserted that MH apps and research have not reached a stable level yet. To the best of our knowledge, there is no clear description of solutions to these problems. In addition, no one has investigated and analyzed MH in its contextual environment in a detailed way. We conducted a comprehensive survey of MH research on diabetes management articles published between 2011 and September 27, 2017. In this survey, we discuss current challenges in MH, along with research gaps, opportunities, and trends. Our literature review searched three academic databases (ScienceDirect, IEEE Xplore, and SpringerLink). A total of 60 articles were analyzed, with 30% from ScienceDirect, 38% from IEEE Xplore, and 32% from SpringerLink. MH was analyzed in the context of the electronic health record (EHR) ecosystem. We consider dimensions such as clinical decision support systems, EHRs, cloud computing, semantic interoperability, wireless body area networks, and big data analytics. We propose specific metrics to analyze and evaluate MH from each of these dimensions. A comprehensive analysis of the literature from this viewpoint is valuable for both theoretical and developmental progress. This paper provides a critical analysis of challenges that have not been fully met and highlights directions for future research that could improve MH applicability."
pub.1053131738,Balanced Dynamics of Mesoscale Vortices Produced in Simulated Convective Systems,"Long-lived, mesoscale convective systems are known to occasionally produce mesoscale convective vortices (MCVs) in the lower to middle troposphere with horizontal scales averaging 100–200 km. The formation of MCVs is investigated using fully three-dimensional cloud model simulations of idealized, mesoscale convective systems (MCSs), initialized with a finite length line of unstable perturbations. In agreement with observations, the authors find that environmental conditions favoring MCV formation exhibit weak vertical shear confined to roughly the lowest 3 km, provided the Coriolis parameter (f) is chosen appropriate for midlatitudes. With f = 0, counterrotating vortices form on the line ends, positive to the north and negative to the south with westerly environmental shear. The MCV and end vortices are synonymous with anomalies of potential vorticity (PV). Using PV inversion techniques, the authors show that the vortices are nearly balanced, even with f = 0. However, the formation of mesoscale vortices depends upon the unbalanced, sloping, front-to-rear and rear inflow circulations of the mature squall line. End vortices form partly from the tilting of ambient shear but more from the tilting of the perturbation horizontal vorticity inherent in the squall line circulation. With the addition of earth's rotation, an asymmetric structure results with the cyclonic vortex dominant on the northern end of the line. The key to this MCV formation is organized convergence above the surface cold pool and associated mesoscale ascent and latent heating. A simulated MCV can even form in an environment with no ambient shear. Using a balanced model, the authors perform extended time integrations and show that the MCV produced in a sheared environment remains largely intact because the shear is confined to low levels and is relatively weak. In addition, the interaction of the vortex with the shear produces sufficient, mesoscale vertical motion on the downshear side of the vortex to trigger convection in typical, observed thermodynamic environments. Results suggest that balanced dynamical arguments may elucidate the long-term behavior of mesoscale vortices. However, because the balance equations neglect the irrotational velocity contribution to the horizontal vorticity, the formation of the mesoscale updraft that leads to an MCV and the generation of vertical vorticity through vortex tilting are both treated improperly. Thus, the authors believe that existing balanced models will have serious difficulty simulating MCS evolution and mesoscale vortex formation unless mesoscale environmental forcing determines the behavior of the convective system."
pub.1038300933,THE INNER STRUCTURE AND KINEMATICS OF THE SAGITTARIUS DWARF GALAXY AS A PRODUCT OF TIDAL STIRRING,"The tidal stirring model envisions the formation of dwarf spheroidal (dSph) galaxies in the Local Group and similar environments via the tidal interaction of disky dwarf systems with a larger host galaxy like the Milky Way. These progenitor disks are embedded in extended dark halos and during the evolution both components suffer strong mass loss. In addition, the disks undergo the morphological transformation into spheroids and the transition from ordered to random motion of their stars. Using collisionless N-body simulations, we construct a model for the nearby and highly elongated Sagittarius (Sgr) dSph galaxy within the framework of the tidal stirring scenario. Constrained by the present orbit of the dwarf, which is fairly well known, the model suggests that in order to produce the majority of tidal debris observed as the Sgr stream, but not yet transform the core of the dwarf into a spherical shape, Sgr must have just passed the second pericenter of its current orbit around the Milky Way. In the model, the stellar component of Sgr is still very elongated after the second pericenter and morphologically intermediate between the strong bar formed at the first pericenter and the almost spherical shape existing after the third pericenter. This is thus the first model of the evolution of the Sgr dwarf that accounts for its observed very elliptical shape. At the present time, there is very little intrinsic rotation left and the velocity gradient detected along the major axis is almost entirely of tidal origin. We model the recently measured velocity dispersion profile for Sgr assuming that mass traces light and estimate its current total mass within 5 kpc to be 5.2 × 108 M☉. To have this mass at present, the model requires that the initial virial mass of Sgr must have been as high as 1.6 × 1010 M☉, comparable to that of the Large Magellanic Cloud, which may serve as a suitable analog for the pre-interaction, Sgr progenitor."
pub.1158631622,BIM Perspectives in Real Estate Operations,"The chapter begins with a critical discussion of BIM in real estate (RE) operations. The number of product announcements, events and publications in the field of BIM far outweighs the number of implemented projects. The chapter shows what current and past research offers to remove existing obstacles to the use of BIM in RE operations.Based on an overview of most important initiatives in the field of standardization, the chapter first introduces research activities for digitally capturing existing buildings. Approaches to (partially) automated processing of 3D point clouds from 3D laser scans or photogrammetric surveys for the creation of BIM models of existing buildings are presented (Scan2BIM). Another research area concerns the management of BIM models during the operational phase. Innovative approaches to Common Data Environments (CDE) for RE operations based on virtual linked data integration are explained as well as the development of open platforms for the selection and support of continuous, digital tool chains for BIM processes. In the field of visualization and virtual, augmented or mixed reality, first practical implementations are presented based on selected research initiatives.BIM opens up new opportunities for using the facility manager’s know-how more easily within the planning phase. The section highlights research initiatives to develop assistance systems up to a framework for BIM-based knowledge management systems. It is also important to efficiently convey knowledge from RE operations and facility management (FM) to practitioners, trainees and students. For this purpose, new possibilities are presented using 3D gaming environments in so-called serious games.Finally, the chapter addresses research focussing on sustainability, energy efficiency and CO2 optimization. Approaches to the use of BIM for the simplified creation of ecological balance sheets with IFC-based building models are presented as well as the development of an open platform for the calculation and optimization of the CO2 footprint of facility services in RE operations. Approaches to research and testing of future scenarios for smart buildings finalize the chapter."
pub.1133886544,Bluetooth Enabled Miniaturized Temperature Controller Device for Electrochemical Sensing Applications,"Undeniably, Nanoparticles are of prodigious significance because of their tremendous applications in diverse fields. The structure and throughput of nanomaterials are profoundly reliant on the process utilized for their synthesis due to their unique physical and chemical properties. Synthesizing of the nanomaterials process plays a vital role in shaping the structures of nanoparticles effectively and efficiently for better yield, further these nanoparticles can be used for electrochemical sensing applications. The existing conventional technique requires very expensive and massive thermal instruments, a huge volume of reagents, a customized vessel like autoclave, stainless steel container, entails huge time with tedious and laborious approaches. In this process, the thermal management systems are considered to be an important and indispensable platform in several biological and biochemical applications. The miniaturized temperature controller device extends its usage with cost-effectiveness, rapidity, and portability. To make use of a thermal management device in micro-scale applications, it is very imperative to have an easy-to-use, effective and efficient design in the thermal management technique. This paper focuses on developing a miniaturized temperature controller system with key aspects such as cost-effective, precise, easy-to-operate, stability, automation, and compact device that can be used for electrochemical sensing applications. Such a platform is amenable to be used for biomedical or biochemical applications such as nanoparticle synthesis, DNA amplification using polymerase chain reaction (PCR) technique and rheological applications, etc. The portable device was incorporated with the arduino controller board which comes using a proportional-derivative-integral (PID) controlling approach. A PID controller is integrated with a response loop that includes an open-source library available in arduino IDE that offers a decent outcome performance constancy perceived through fine-tuning of KP=150, KI=1, and KD=70 as per the prerequisite with low tolerance. Further, a customized cartridge heater was calibrated to achieve a peak temperature of 300°C in 30 minutes, managed by a self-designed driver switching circuit and a k-type thermocouple sensor. The MAX6675 is a breakout module used with the k-type thermocouple, responsible to minimize the error, signal variation, and noise of the sensing parameters. The characterization of a cartridge heater was carried out using a feedback loop sensor. The setpoint temperature of the proposed device was around 75°C. The device showcased a temperature sensitivity of +/-2°C. Herein, an inexpensive, easy-to-operate automated and integrated device is being designed and developed for universal biological and biochemical applications. It includes a 32-bit arduino based microcontroller with 4 MB of memory and it operates in 3 – 5V range. Pro-mini usually works on 5V/16MHz and exhibits a maximum output current o"
pub.1136035899,Advanced Image Preprocessing and Feature Tracking for Remote CME Characterization,"<p>Coronal Mass Ejections (CMEs) influence the interplanetary environment over vast distances in the solar system by injecting huge clouds of fast solar plasma and energetic particles (SEPs). A number of fundamental questions remain about how SEPs are produced, but current understanding points to CME-driven shocks and compressions in the solar corona. At the same time, unprecedented remote (AIA, LOFAR, MWA) and in situ (Parker Solar Probe, Solar Orbiter) solar observations are becoming available to constrain existing theories. As part of the MOSAIICS project under the VIHREN programme, we are developing a suite of Python tools to reliably analyze radio and EUV remote imaging observations of CMEs and  shock. We present the method for smart characterization and tracking of solar eruptive features, based on the A-Trous wavelet decomposition technique, intensity rankings and a set of filtering techniques. We showcase its performance on a small set of CME-related phenomena observed with the SDO/AIA telescope. With the data represented hierarchically on different decomposition and intensity levels our method allows to extract certain objects and their masks from the series of initial images, in order to track their evolution in time. The method presented here is general and applicable to detecting and tracking various solar and heliospheric phenomena in imaging observations.</p>"
pub.1117625964,Investigating the influence of organizational factors on blockchain adoption," Purpose Blockchain possesses the potential to disrupt and reshape a plethora of industries in the next decade. However, blockchain adoption rates in technology developed countries, such as Ireland, are relatively low. Motivated by blockchain’s potential to transform sociotechnical systems, the lack of systematic inquiry pertaining to blockchain studies from an information system perspective, the authors propose the following research question: “How do organizational factors influence blockchain adoption in organizations based in a developed country?” Specifically, the purpose of this paper is to elucidate the impact of organizational factors on the adoption of blockchain and the adoption of blockchain in companies based in Ireland.   Design/methodology/approach A comprehensive literature review was conducted, and the methods of qualitative content analysis were used to identify the most important technology–organization–environment (TOE) blockchain adoption factors. Organizational factors are often viewed as the most significant determinants of IT innovation adoption in organizations. Consequently, using a multiple-case study of 20 companies based in Ireland, the authors investigate how the top three organizational factors identified from the blockchain literature affected these companies decision to adopt or not adopt blockchain.   Findings The literature review on blockchain adoption identified specific technological, organizational and environmental factors. Furthermore, the case study findings identified three patterns: top management support and organizational readiness are enablers for blockchain adoption, and large companies are more likely to adopt blockchain than small to medium-sized enterprises (SMEs). The authors explain these patterns by examining the nature of blockchain and the characteristics of Ireland as a developed country. Practical and scientific contributions are also presented.   Research limitations/implications This study makes several important scientific contributions. First, the findings revealed that top management support and organizational readiness are significant enablers of blockchain adoption. Ireland is recognized as a technology developed country; however, the findings in relation to top management support contradict existing IT adoption literature pertaining to developed countries. Second, previous IT innovation adoption literature suggests that organizations size has a positive influence on a company’s IT innovation adoption process. This study demonstrates that large organizations are more likely to not only adopt blockchain but are also more likely to conduct increased levels of blockchain research and development activities. Finally, and most significantly, the authors identified several patterns, which relate specifically to Ireland as a developed country that influenced the findings. These findings could hold particular relevance to governments and organizations of other developed countries in terms of"
pub.1148119849,Multi-scale image preprocessing and feature tracking for remote CME characterization,"Coronal Mass Ejections (CMEs) influence the interplanetary environment over vast distances in the solar system by injecting huge clouds of fast solar plasma and energetic particles (SEPs). A number of fundamental questions remain about how SEPs are produced, but current understanding points to CME-driven shocks and compressions in the solar corona. At the same time, unprecedented remote and in situ (Parker Solar Probe, Solar Orbiter) solar observations are becoming available to constrain existing theories. Here we present a general method for recognition and tracking solar images of objects such as CME shock waves and filaments. The calculation scheme is based on a multi-scale data representation concept à trous wavelet transform, and a set of image filtering techniques. We showcase its performance on a small set of CME-related phenomena observed with the SDO/AIA telescope. With the data represented hierarchically on different decomposition and intensity levels, our method allows extracting certain objects and their masks from the imaging observations in order to track their evolution in time. The method presented here is general and applicable to detecting and tracking various solar and heliospheric phenomena in imaging observations. It holds the potential to prepare large training data sets for deep learning. We have implemented this method into a freely available Python library."
pub.1137679996,In the Shadow of Platforms,"Introduction
This article explores the changing relational quality of “the shadow of hierarchy”, in the context of the merging of platforms with infrastructure as the source of the shadow of hierarchy. In governance and regulatory studies, the shadow of hierarchy (or variations thereof), describes the space of influence that hierarchal organisations and infrastructures have (Héritier and Lehmkuhl; Lance et al.). A shift in who/what casts the shadow of hierarchy will necessarily result in changes to the attendant relational values, logics, and (techno)socialities that constitute the shadow, and a new arrangement of shadow that presents new challenges and opportunities. This article reflects on relevant literature to consider two different ways the shadow of hierarchy has qualitatively changed as platforms, rather than infrastructures, come to cast the shadow of hierarchy – an increase in scalability; and new socio-technical arrangements of (non)participation – and the opportunities and challenges therein. The article concludes that more concerted efforts are needed to design the shadow, given a seemingly directionless desire to enact data-driven solutions.
The Shadow of Hierarchy, Infrastructures, and Platforms
The shadow of hierarchy refers to how institutional, infrastructural, and organisational hierarchies create a relational zone of influence over a particular space. This commonly refers to executive decisions and legislation created by nation states, which are cast over private and non-governmental actors (Héritier and Lehmkuhl, 2). Lance et al. (252–53) argue that the shadow of hierarchy is a productive and desirable thing. Exploring the shadow of hierarchy in the context of how geospatial data agencies govern their data, Lance et al. find that the shadow of hierarchy enables the networked governance approaches that agencies adopt. This is because operating in the shadow of institutions provides authority, confers bureaucratic legitimacy and top-down power, and offers financial support. The darkness of the shadow is thus less a moral or ethicopolitical statement (such as that suggested by Fisher and Bolter, who use the idea of darkness to unpack the morality of tourism involving death and human suffering), and instead a relationality; an expression of differing values, logics, and (techno)socialities internal and external to those infrastructures and institutions that cast it (Gehl and McKelvey). The shadow of hierarchy might therefore be thought of as a field of relational influences and power that a social body casts over society, by virtue of a privileged position vis-a-vis society. It modulates society’s “light”; the resources (Bourdieu) and power relationships (Foucault) that run through social life, as parsed through a certain institutional and infrastructural worldview (the thing that blocks the light to create the shadow). In this way the shadow of hierarchy is not a field of absolute blackness that obscures, but instead a gradient "
pub.1146987515,Development of a Virtual Learning Factory for Energy Efficiency Improvement,"Learning Factories (LF) is considered as one of the crucial approaches for upskilling Industry 4.0 knowledge. However, the LF concept does not only depend on technology advancements, but also on other aspects such as resource availability, learning approaches, and service strategies. This paper describes the LF implementation based on the prior energy monitoring project that aimed at improving energy efficiency in Thai factories. According to the lessons learned from implementation of 15 pilot factories, the results suggested that to achieve IT/OT convergence for Thai factories (usually ranked below Industry 3.0 level), basic IIoT and data management skills for energy efficiency improvement are needed. These basic skills are 1) IIoT information management, 2) integration of IT/OT subsystems (e.g., data acquisition, database, and control), and 3) data visualization. Thus, these basic materials were used in the construction of the virtual LF with a goal to educate or upskill factory employees or system integrators on the IIoT and data management topics. The learning process started with the understanding of how the monitored data are translated into correct formats and how to configure each module to create the information flow from devices to an IoT Cloud, and gained insight from visualization. The energy consumption information was then visualized via a web-based factory dashboard system. Since the factory environments of case examples were different, the virtual LF could educate or provide crucial ideas to the learners in order to adapt to their existing factory systems. The dashboard displayed the monitoring views of basic energy consumption data, energy efficiency indicator (i.e., Specific Energy Consumption), usage trends, and alert thresholds. This real-time energy and machine information could then be used in a decision-making process to optimize the utility and production systems."
pub.1158575255,How Health Information Technology Improved Patient Care and Treatment During the COVID-19 Pandemic: A Comparison Between International Case Studies and the Moroccan Context,"In the healthcare industry, health information technology (HIT) refers to the notion of cloud-based services, such as the internet, linked networks, and so on. It primarily makes use of electronic medical records, patient information, and data to deliver more efficient and sophisticated treatments and services. The aim of the research is to give roles and potential applications and also evaluate the influence of the HIT concept on care delivery during the existing COVID-19 epidemic. The scope of HIT in the health care industry and how countries such as Morocco can benefit from an international case study in a push to overhaul and modernize their hospital information systems.The writers perform their study with the use of reliable data sources such as IEEE, Springer, Elsevier, Taylor & Francis, Sprouts, and Google Books, as well as surveys of subject area experts. The authors used a methodical approach to study and diagnose the development of the hospital process digitization project in the Moroccan environment, taking into account past publications and studies on the subject, as well as an effective survey directed towards healthcare professionals.After Data synthesis and treatment, authors have defined Three main axes of the subject which are: 1. The role and impact of HIT on care delivery during the present COVID-19 crisis and The breadth of HIT in the health care sector during the pandemic from an international perspective 2. How developing countries, such as Morocco, have responded to Covid 19 by overhauling and modernizing their hospital information systems at the operational, tactical, and strategic levels 3. Morocco’s Accelerated Path to Digital Health Transformation.Finally, the authors presume that hospitals should consider improved data system integration, the use of customized warnings, and the growth of telehealth while employing information and communication technology to offer efficient medical care. In the Moroccan context, it was an excellent chance for health professionals to place greater emphasis on the health information system and to comprehend the critical role that information technology played throughout the pandemic era."
pub.1134853496,The Design Model for Robotic Waitress,"With the rapid development of traditional industries, intelligent robots have been widely used in the hospitality industry. Although the development of intelligent robots faces a positive trend and a good market in the hospitality industry, it also faces the problem that robots cannot effectively collect and use user data in the field of human–computer interaction. It not only affects the interaction experience between users and robots, but also prevents companies from getting valuable feedback in a timely manner. In order for intelligent robots to effectively utilize interactive information, the user experience of robot entertainment is improved. This paper proposes and establishes a basic technical model called iRCXM. Combining the iRCXM model with a decision tree classification algorithm is excepted effectively improve the interaction experience between humans and robots in hospitality. This paper designs a model of intelligent robot based on decision tree algorithm. The model divides the user into three sections, each corresponding to a different standard function. Using a decision tree classification algorithm model is excepted effectively judge users’ current stage and whether they can move to the next stage. When the user reaches the final stage, it proves that the user has obtained a good interactive experience. At the same time, for users at different stages, the model will provide strategies for downward transformation so that companies can adjust and improve existing problems in a timely manner. In addition, the research developed a robot user interaction system based on the existing technology. The system is based on Android. Using HTTP protocol and Baidu Cloud AI API to realize simple face recognition and Sanbot-OpenSDK to implement simple robot control, the development of this system is to verify the feasibility of the model. The developed samples were tested in a real environment and feedback from customer experience was collected through semi-structured interviews. Finally, the feasibility of the model is verified."
pub.1030416619,Toxic metal emissions from incineration: Mechanisms and control,"Toxic metals appear in the effluents of many combustion processes, and their release into the environment has come under regulatory scrutiny. This paper reviews the nature of the problems associated with toxic metals in combustion processes, and describes where these problems occur and how they are addressed through current and proposed regulations. Although emphasis in this paper is on problems associated with metals from incineration processes, conventional fossil fuel combustion is also considered, insofar as it pertains to mechanisms governing the fate of metals during combustion in general. This paper examines the release of metals into the vapor phase, with the particle dynamics of a nucleating, condensing, and coagulating aerosol that may be subsequently formed, and with the reactive scavenging of metals by sorbents.Metals can be introduced into combustion chambers in many physical and chemical forms. The subsequent transformations and vaporization of any volatile metal depend on the combustion environment, the presence of chlorine and other species (reducing or oxidizing), on the nature of the reactive metallic species formed within the furnace, and on the presence of other inorganic species such as alumino-silicates. Some insight into how these factors influence metal release can be gained by considering the release of organic sodium during coal char combustion.Once vaporized, a metal vapor cloud will normally pass through its dewpoint to form tiny nuclei, or condense around existing particles. These aerosols are then affected by other dynamic processes (including coagulation) as they evolve with time. This paper shows how current mathematical descriptions of aerosol dynamics are very useful in predicting metal aerosol size distributions in combustion systems. These models are applied to two prototype problems, namely: the prediction of the temporal evolution of a particle size distribution of a self-coagulating aerosol initially composed of nuclei; and the scavenging of nuclei by coagulation with larger sorbent particles.A metal vapor can also react with certain aluminosilicate sorbents. This process, which will occur at temperatures above the dewpoint, is described, and is important, since it allows the high temperatures in incineration processes to be exploited to allow the formation of water-unleachable metal-containing compounds that can be isolated from the environment. Future research problems are also identified."
pub.1153440382,5.2 Resource-Efficient Vehicle-to-Cloud Communications,"Big vehicular data is anticipated to become the new fuel for catalyzing the further development of connected and autonomous driving. Vehicles themselves will act as mobile sensor nodes that actively sense their environment and gather meaningful data for novel crowdsensing-enabled services such as the distributed generation of high-definition maps, traffic monitoring, and predictive maintenance. However, the implied tremendous increase in massive Machine-Type Communication (mMTC) represents an enormous challenge for the coexistence of different resource-consuming applications and entities within the limited radio spectrum. A promising approach for achieving relief through a more resource-efficient usage of existing network resources is the utilization of client-based intelligence. Novel communications paradigms such as anticipatory mobile networking aim to improve decision processes within wireless communication systems by explicitly taking context information into account. In the context of vehicular crowdsensing, these methods exploit the delay-tolerant nature of the targeted applications for scheduling the data transfer with respect to the expected resource efficiency. If the current radio channel and network load conditions do not allow a resource-efficient transmission, the data transfer process is postponed and the acquired data is aggregated locally in favor of a better transmission opportunity in the near future along the expected vehicular trajectory. In the following, the different evolution phases of the novel Channel-aware Transmission (CAT) scheme are presented. These are characterized by a sequential introduction of different machine learning methods. While the basis CAT approach applies a probabilistic channel-access mechanism based on measurements of the Signal-to-Noiseplus- Interference Ratio (SINR), Machine Learning CAT (ML-CAT) applies supervised learning for predicting the currently achievable data rate using features from the network context, the mobility context, and the application context domain. This approach is then further extended by Reinforcement Learning CAT (RL-CAT) through the autonomous detection and exploitation of favorable transmission opportunities. Finally, Blackspot-Aware Contextual Bandit (BC-CB) integrates a priori knowledge about the geospatially-dependent uncertainties of the prediction model, which is uncovered by unsupervised machine learning. It is shown that machine learning-aided opportunistic data transfer is not only able to increase the average data rate of the individual transmissions; it also achieves a massive reduction of the occupied network resources and the power consumption of the mobile device. The price to pay is an increase of the Age of Information (AoI) of the sensor measurements. In addition to the presentation of the novel opportunistic data-transfer approaches, new machine learning enabled methods for simulating these anticipatory mobile networks are presented, discussed, and valid"
pub.1171123867,"Land-Use Transitions Impact the Ecosystem Services Value in a Coastal Region by Coupling the Geo-Informatic Tupu and Benefit-Transfer Method: The Case of Ningde City, China","Exploring the mechanisms and processes of land-use transitions (LUTs) and their impact on ecosystem services can effectively elucidate the intricate interactions between human and natural systems, which is pivotal for advancing the sustainable development of regional economies and enhancing ecological environments. However, the existing literature lacks comprehensive analysis regarding the spatial and temporal evolution of LUTs, with insufficient integration of the “spatial pattern” and “time process”. Moreover, traditional assessments of the ecosystem services value (ESV) often overlook their negative costs. To address these gaps, this study first utilized the Google Earth Engine (GEE) cloud platform and employed the random forest algorithm to conduct supervised classification on Landsat remote-sensing images from the years 2000, 2010, and 2020 within the research area, thereby obtaining land-use data for three distinct periods. And then, we investigated the geographic features of LUTs and their ecological effects in the Ningde City of China from 2000 to 2020. The geo-informatic Tupu model and a newly revised method of benefit transfer were primarily employed for this purpose. The findings indicate the following: (1) Over the study period, the land-use structure of Ningde City predominantly comprised cultivated land and forest land, with continuous decreases in both types and a concurrent increase in built-up land. (2) Significant disparities exist in the spatial distribution of Tupu units, notably with “forest land → cultivated land” and “cultivated land → built-up land” as crucial units influencing ESV changes. (3) The ESV in Ningde City decreased from CNY 1105.54 × 108 to CNY 1020.47 × 108 over 2000–2020, while the ecosystem dis-services value exhibited an opposing trend, rising from CNY 12.68 × 108 to CNY 20.39 × 108. (4) The net ESV in Ningde City showed a decline over the same period, indicating a certain vulnerability in the city’s ecological system structure. This study aims to enhance our understanding of the influence of land-use patterns on ESV, offering valuable insights for regional ecological–environment management and land-use policy formulation, thereby fostering sustainable development in ecological, environmental, and socio-economic dimensions. Furthermore, the results serve as a reference for evaluating net ecosystem services value in other countries/regions."
pub.1135276168,"Nature-Inspired Optimization Algorithms, Recent Advances in Natural Computing and Biomedical Applications","Malware is continuously penetrating the current digital world. Even more alarming are the statistics that reveal that the biomedical industry is presently the most susceptible target of the attackers. The main reasons behind this disquieting situation of attacks on the biomedical industry are its sensitivity level and impact of harm. Moreover, the high cost of medical records is also a major reason for the upsurge in penetration and exploitation. This scenario calls for an effective prevention mechanism to ward off malware attacks on the biomedical or healthcare industry. This research initiative provides an overview of recent statistics of malware attacks in web-based biomedical applications and services. The study also provides a helpful mechanism called malware analysis for preventing malware issues. Further, the study analyzes the malware analysis approach for better and easy understanding and, more importantly, its adoption in biomedical industry. It also provides a ranking assessment/priority assessment of different malware analysis techniques for identifying the most prioritized approach through fuzzy analytic hierarchy process methodology. The study uses a scientifically proven approach for prioritization of analysis techniques and provides a novel idea and path for future researchers. Chronic kidney disease (CKD) occurs when the kidney fails to perform its functions and does not filter or purify the blood accurately. Various factors increase the risks of developing CKD. Hence, to detect this life-threatening disease at the fresh or initial stage, one has to monitor these risk factors regularly before the condition of the individual worsens. In this chapter, the detection of CKD, a deadly and serious disease, is discussed by using an adaptive neuro-fuzzy inference system (ANFIS). The main objective of this study is to enhance the accuracy of the diagnostic systems used for the detection of CKD. The developed ANFIS uses nephron functionality, blood sugar, diastolic blood pressure, systolic blood pressure, age, body mass index and smoking as input variables. The output variable describes the stage of the CKD of a particular patient. The proposed neuro-fuzzy inference system is implemented using the MATLAB software. The developed diagnostic system shows better results, an accuracy of 96% when compared with a fuzzy inference system. Image fusion is a combination of two or more images into one image to obtain essential information from the trigger images. The lousy or horrible information found in the resource images is reduced by image fusion. It can be broadly used in healthcare imaging, remote sensing, computer vision and military applications. The achievements of the fusion method are despite the noise within the resource images. An ABC (artificial bee colony) checks satellite-based images better using fusion enhancement by combining two multitemporal satellite images. Due to fusion, an enhanced contrast of images is available in the ABC. "
pub.1071472898,ION IRRADIATION OF ETHANE AND WATER MIXTURE ICE AT 15 K: IMPLICATIONS FOR THE SOLAR SYSTEM AND THE ISM,"Solid water has been observed on the surface of many different astronomical objects and is the dominant ice present in the universe, from the solar system (detected on the surface of some asteroids, planets and their satellites, trans-Neptunian objects [TNOs], comets, etc.) to dense cold interstellar clouds (where interstellar dust grains are covered with water-rich ices). Ethane has been detected across the solar system, from the atmosphere of the giant planets and the surface of Saturn’s satellite Titan to various comets and TNOs. To date, there were no experiments focused on icy mixtures of C2H6 and H2O exposed to ion irradiation simulating cosmic rays, a case study for many astronomical environments in which C2H6 has been detected. In this work, the radiolysis of a C2H6:H2O (2:3) ice mixture bombarded by a 40 MeV58Ni11+ ion beam is studied. The chemical evolution of the molecular species existing in the sample is monitored by a Fourier transform infrared spectrometer. The analysis of ethane, water, and molecular products in solid phase was performed. Induced chemical reactions in C2H6:H2O ice produce 13 daughter molecular species. Their formation and dissociation cross sections are determined. Furthermore, atomic carbon, oxygen, and hydrogen budgets are determined and used to verify the stoichiometry of the most abundantly formed molecular species. The results are discussed in the view of solar system and interstellar medium chemistry. The study presented here should be regarded as a first step in laboratory works dedicated to simulate the effect of cosmic radiation on multicomponent mixtures involving C2H6 and H2O."
pub.1181629780,A Comprehensive Study Of Incorporation Of Information Science Into Financial Management,"The incorporation of information science into financial management signifies a notable advancement in the financial sector, propelled by the growing intricacy and magnitude of financial data. This review paper analyses the most recent advancements, patterns, and methodologies in this integration, emphasising the revolutionary influence of cutting-edge technologies such as big data analytics, artificial intelligence, blockchain, robotic process automation, cloud computing, and natural language processing. These advancements have completely transformed the way financial decisions are made, risks are managed, and operations are streamlined. They have empowered institutions to handle massive amounts of data, automate intricate activities, and provide customised services. Although there are difficulties in guaranteeing data security, preserving data quality, merging new technologies with existing systems, and resolving talent deficiencies, the potential for development and enhancement is significant. This article thoroughly examines the aforementioned problems and investigates the possible advantages, such as better accuracy in decision-making, cost reduction through automation, the development of innovative financial products, greater consumer experiences, and more efficient compliance with regulatory requirements. The results emphasise the crucial significance of information science in influencing the future of financial management, facilitating the development of a more adaptable, streamlined, and protected financial ecosystem. As financial institutions evolve to keep up with technological breakthroughs, the incorporation of information science will be crucial in effectively navigating the intricacies of the contemporary financial environment and attaining long-term prosperity."
pub.1112614831,Design of a Technology-Enhanced Pedagogical Framework for a Systems and Networking Administration course incorporating a Virtual Laboratory,"Practical hands on lab activities are crucial to learning in fields such as computing, engineering and science. Advances in technologies have allowed the design and development of virtual and remote laboratories with many benefits overcoming constraints of physical laboratories. In literature, we observe a number of virtual labs implemented for systems-level courses in computing, using virtualization and cloud computing technologies. Although benefits of such technology-enhanced labs are well-known, the best approach to integrate these labs to achieve best outcomes for learning is still an exploratory area of research. In this paper, existing literature on virtual labs implementation for systems-level courses in computing is analyzed and classified into two stages of evolution. In the first stage, technical design & evaluation is the focus while in the second stage, pedagogy and learning theories and principles are used as the basis in designing technology innovations as well as teaching & learning activities. The second stage has the advantage of theoretical principles guiding the design of teaching and learning activities and technology artifacts with a focus to achieve learning outcomes and thus a higher potential to achieve learning goals. However, we observe only a few studies taking this approach. This paper presents the design of a holistic technology-enhanced pedagogical framework incorporating a virtual laboratory for a systems and network administration course. The framework applies theories such as Constructive Alignment for curriculum design and Kolb’s Experiential Learning Cycle, Bloom’s Taxonomy and Collaborative Learning to design teaching & learning activities and assessments. Technology artifacts such as virtual labs, a feedback tool, student and teacher dashboards, on-line quizzes and discussion boards are incorporated. Our future work will evaluate the framework in real class environments."
pub.1169587454,Unraveling the Origins of JFC-like Bodies: A Comparative Study of Comets and Meteoroids,"Jupiter-family comets (JFCs) originate from the Kuiper belt and scattered disk, characterized by short orbital periods and frequent interactions with Jupiter. Their icy composition and a chaotic transition to the inner solar system result in short dynamic and physical lifetimes. These features make JFCs key subjects for understanding the migration of celestial bodies and possibly the delivery of organic materials to the early Earth. Numerous studies of fireballs have historically posited a substantial contribution of large objects from JFC orbits, suggesting a significant presence of cometary material in the near-Earth environment. However, this prevalent belief necessitates a thorough re-examination, as the physical evolution of comets and the mechanisms governing their disintegration remain subjects of debate. Understanding the population of meteoroids and comets is crucial for evaluating this population's physical breakdown and evolution. Current dust models suggest that fragmentation and disintegration of comets play a significant role in populating the zodiacal cloud. However, the larger centimeter-meter scale debris observed by fireball networks has been shown to resemble more asteroidal sources dynamically, indicating that comets might be breaking down directly only into dust-sized fragments.  This study extends the scope of existing research by conducting a detailed analysis of both JFCs and comet-like fireball observations, aiming to elucidate the origins and dynamics of objects on JFC-like orbits across varying size scales. Utilizing extensive data from four major fireball networks (DFN, EFN, FRIPON, MORP) and ephemeris data of JFCs, the research comprises 646 fireball orbits and 661 JFCs. Methods include orbital stability analysis over 10,000 years, Lyapunov lifetime estimation, debiased NEO model source region estimation, meteorite fall identification, and meteor shower analysis. The analysis reveals that most meteoroids on JFC-like orbits do not align dynamically with typical JFCs. Instead, they predominantly originate from stable orbits in the outer main asteroid belt, challenging the notion that centimeter-to-meter scale meteoroids on JFC-like orbits primarily derive from JFCs. Furthermore, a subset of 24 JFCs in near-Earth orbits displayed unexpected orbital stability, suggesting a presence of asteroidal interlopers from the outer main belt within the JFC population. Our study demonstrates significant dynamical differences between kilometer-scale JFCs and smaller meteoroids. While the larger JFCs frequently encounter Jupiter and have dynamic, transient orbits, the smaller meteoroids detected by fireball networks originate primarily from stable orbits, indicating a predominant influence of asteroidal material from the outer main belt. This finding challenges conventional assumptions about the origins of JFC-like debris observed on Earth and highlights the complexity and diversity of the small-body environment in our solar system.  "
pub.1132233858,A Generic Scalable Method for Scheduling Distributed Energy Resources Using Parallelized Population-Based Metaheuristics,"Recent years have seen an increasing integration of distributed renewable energy resources into existing electric power grids. Due to the uncertain nature of renewable energy resources, network operators are faced with new challenges in balancing load and generation. In order to meet the new requirements, intelligent distributed energy resource plants can be used which provide as virtual power plants e.g. demand side management or flexible generation. However, the calculation of an adequate schedule for the unit commitment of such distributed energy resources is a complex optimization problem which is typically too complex for standard optimization algorithms if large numbers of distributed energy resources are considered. For solving such complex optimization tasks, population-based metaheuristics – as e.g. evolutionary algorithms – represent powerful alternatives. Admittedly, evolutionary algorithms do require lots of computational power for solving such problems in a timely manner. One promising solution for this performance problem is the parallelization of the usually time-consuming evaluation of alternative solutions. In the present paper, a new generic and highly scalable parallel method for unit commitment of distributed energy resources using metaheuristic algorithms is presented. It is based on microservices, container virtualization and the publish/subscribe messaging paradigm for scheduling distributed energy resources. Scalability and applicability of the proposed solution are evaluated by performing parallelized optimizations in a big data environment for three distinct distributed energy resource scheduling scenarios. Thereby, unlike all other optimization methods in the literature – to the best knowledge of the authors, the new method provides cluster or cloud parallelizability and is able to deal with a comparably large number of distributed energy resources. The application of the new proposed method results in very good performance for scaling up optimization speed."
pub.1114260999,The Integration of Satellite Data with Environmental Database,"The environmental database is becoming an indispensable source for various planning processes. However, it is a tough task to keep up the data with the ever changing environment. Especially factors like landuse are rapidly altered in Japan. On the other hand, constant renewal of such man-made data requires a lot of labor and time. Even if we have both, the precision of the data remains dubious because they have to depend on existing information such as maps, aerial photographs and occasional field trips. The integration of LANDSAT TM data with existing enviromental database of Kobe area is tried, and the results are discussed. First, the TM data covering Kobe area is extracted from a scene of 1984. Then band 2, 3 and 4 of the TM data was agreed to the coordinates of exiting database using affine approximation based on 10 corresponding ground control points. Thus agreed TM data was classified into several categories using maximum likelihood method and shown on the display attached to an image processing system named FIVIS. Finally the data was compared with some factors in Kobe environmental database. Strictly speaking, it is impossible to compare the two data because the TM data is based on an instantaneous view of the land surface, while man-made landuse and vegetation data is produced on various data of different stages. Satellite data can supply constant view of the area with reasonable cost though cloud-free data is rare. This is an advantage to follow the changing land surface. However, the view by satellite is often different from landuse we recongnize on vegetation-covered areas and mountain ranges. For instance, paddy fields are major agricultural landuse, but it is perceived as completely different landuses depending on the season by satellite. In addition, classifying landuse or vegetation is difficult due to shadows on the slope. For these reasons, both data should be supplemented each other to obtain more precise view of the area. Then satellite data will contribute a lot to improve current environmental and geographical databases."
pub.1169593253,"Climate change impact on rock avalanches in metamorphic rock masses in Tyrol, Austria","Rockfalls and rockslides are a common hazard in alpine terrain and are major factor of alpine landscape evolution. They are characterized by a complex combination of geological, hydrological, geomechanical and meteorolocical processes and occur in a wide variety of geological and structural settings and in response to various loading and triggering processes. In the Alps in particular, extremely rapid rock avalanches reaching a volume of several 10000 m3 or more have the potential to cause serious damage to both humans and infrastructure. As global warming progresses, the meteorological and climatological factors that influence rock avalanche formation will change. Especially, in the high mountain environment rock avalanches are strongly influenced by climate change due to thawing of permafrost and the retreat of glaciers. Less obvious is the influence of climate change on the formation of rock avalanches at lower altitudes, and thus there is a need for additional research. In this study, we investigate the impact of global warming on selected rock avalanche case studies with volumes above several tens of thousands of cubic meters. The study area covers approx. 3400 km2 in the metamorphic rock mass of the Ötztal Stubai Crystalline, the Silvretta and the Glockner Nappes as well as the units of the Engadin Window of the Tyrolian Alps, Austria. The aim of this work is to identify the processes that led to our case studies and if these processes are influenced by climate change factors, such as changes in temperature, precipitation, freeze-thaw cycles, snow coverage, etc. The climatic factors will be investigated in terms of both their short-term and long-term influence on the trigger mechanisms. Advanced remote sensing techniques were used on site to carry out small to large-scale investigations. Terrestrial laser scanning (TLS) and Airborne laser scanning (ALS) enables us to create high-resolution recordings of inaccessible rock faces, supported by 3D point cloud analyzing tools. In addition, where TLS campaigns are not possible, we use an unmanned aerial vehicle (UAV) photogrammetry system that provides 3D point clouds and delivers a 3D model of the site. Geological field investigations were performed to record lithological, hydrogeological and structural features. This results in a comprehensive geological model of the failure area. A 3D discontinuity network was developed based on the combined analyses of remote sensing and discontinuity mapping data, providing the basis for structural geological analyses and distinct element modelling studies. With regard to the above criteria, we have selected several case studies. Most of the case studies are located well above 2500 m above sea level in glaciated or recently glaciated areas. For all case studies, we were able to document at least one rock avalanche event with a volume exceeding several 10000 m3. A high-resolution climate model was created for the documented events. We then began to collect an"
pub.1051686770,The Extratropical Transitions of Hurricanes Felix and Iris in 1995,"The extratropical transitions of Hurricanes Felix and Iris in 1995 are examined and compared. Both systems affected northwest Europe but only Iris developed significantly as an extratropical system. In both cases the hurricane interacts with a preexisting extratropical system over the western Atlantic. The remnants of the exhurricanes can be identified and tracked across the Atlantic as separate low-level potential vorticity (PV) anomalies. The nature of the baroclinic wave involved in the extratropical transition is described from a PV perspective and shown to differ significantly between the two cases. The role of vertical shear in modifying the hurricane structure during the early phase of the transition is investigated. Iris moved into a region of strong shear. The high PV tower of Iris developed a marked downshear tilt. Felix moved into a vertically sheared environment also but the shear was weaker than for Iris and the PV tower of Felix did not tilt much. Iris maintained its warm-core structure as it tracked across relatively warm water. It moved into the center of a large-scale baroclinic cyclone. The superposition of the two systems gave rise to strong low-level winds. The resulting strong surface latent heat fluxes helped to keep the boundary layer equivalent potential temperature (θe) close to the saturated equivalent potential temperature of the underlying sea surface temperature. This high equivalent potential temperature air was redistributed in the vertical in association with deep convection, which helped maintain the warm core in a similar way to that in tropical cyclones. Felix did not maintain its warm-core structure as it tracked across the Atlantic. This has been shown to be linked to its more poleward track across colder water. It is argued that negative surface fluxes of latent and sensible heat decrease the boundary layer θe, resulting in low-cloud formation and a decoupling of the cyclone boundary layer from the the deep troposphere. In order to forecast these events there is a need for skill in predicting both the nature of the large-scale baroclinic wave development and the structural evolution of the exhurricane remnants."
pub.1021970070,The Legnaro-Padova distributed Tier-2: challenges and results,"The Legnaro-Padova Tier-2 is a computing facility serving the ALICE and CMS LHC experiments. It also supports other High Energy Physics experiments and other virtual organizations of different disciplines, which can opportunistically harness idle resources if available. The unique characteristic of this Tier-2 is its topology: the computational resources are spread in two different sites, about 15 km apart: the INFN Legnaro National Laboratories and the INFN Padova unit, connected through a 10 Gbps network link (it will be soon updated to 20 Gbps). Nevertheless these resources are seamlessly integrated and are exposed as a single computing facility. Despite this intrinsic complexity, the Legnaro-Padova Tier-2 ranks among the best Grid sites for what concerns reliability and availability. The Tier-2 comprises about 190 worker nodes, providing about 26000 HS06 in total. Such computing nodes are managed by the LSF local resource management system, and are accessible using a Grid-based interface implemented through multiple CREAM CE front-ends. dCache, xrootd and Lustre are the storage systems in use at the Tier-2: about 1.5 PB of disk space is available to users in total, through multiple access protocols. A 10 Gbps network link, planned to be doubled in the next months, connects the Tier-2 to WAN. This link is used for the LHC Open Network Environment (LHCONE) and for other general purpose traffic. In this paper we discuss about the experiences at the Legnaro-Padova Tier-2: the problems that had to be addressed, the lessons learned, the implementation choices. We also present the tools used for the daily management operations. These include DOCET, a Java-based webtool designed, implemented and maintained at the Legnaro-Padova Tier-2, and deployed also in other sites, such as the LHC Italian T1. DOCET provides an uniform interface to manage all the information about the physical resources of a computing center. It is also used as documentation repository available to the Tier-2 operations team. Finally we discuss about the foreseen developments of the existing infrastructure. This includes in particular the evolution from a Grid-based resource towards a Cloud-based computing facility."
pub.1028344326,Application and researches of Large Eddy Simulation of the low-level wind over complex terrain,"However, the distributed wind power closer to the user or load centers, leading to more complex elements of the surrounding environment. The changes of obstacles, surface roughness and topography have increased the difficulty of numerical weather prediction that used for distributed wind power forecast, which has seriously hampered the development of distributed wind power. Microscale weather modeling is an effective way to simulate distributed wind energy prediction. Although encouraging advances in micro-scale flow modeling, including the evolution of different technologies and flavors of LES and CFD (Computational Fluid Dynamics) models, have been made in the last decade, the modeling ability for micro-scale flows associated with real weather at distributed wind farm scales is still very limited. In fact, microscale weather flow models encounter many challenges. Therefore existing micro-scale models have mostly focused on idealized case study, with idealized initial conditions and/or boundary conditions and/or highly simplified atmospheric physics. The Real-Time Four Dimensional Data Assimilation (RTFDDA) weather forecasting system (Liu Y, 2006), built upon WRF (Liu Y, 2008a. The 1 INTRODUCTION Energy crisis, environmental issues and other factors have accelerated the development of distributed generation, especially the rapid development of distributed wind power. “Twelve Five” period, the development of China’s wind power has adjusted to “centralized” and “distributed”, the distributed wind power is with respect to the concentrated development large-scale wind farms (Wang C, 2005), it is the wind power production and used in the same location or limited to the local area. Distributed wind power generation with flexible, environmentally friendly features is more and more access to the distribution network (Linag Y, 2003), not only to save investment of the high-pressure lines and booster stations, but also to achieve the balance between wind power output and local load (Wang L, 2001; Xu Y, 2011; Dai J, 2011). The installed capacity of distributed wind power in 2015 will reach 500 million kilowatts and in 2020 is expected to reach 15 million kilowatts, accounting for 5% and 7.5% of the target installed capacity, respectively. However, the output with obvious intermittent and random volatility features, will affect the normal operation of the power system (Chen H, 2006; Hu H, 2006; Hadjsaid N, 1999; Puttgen H, 2003), when the large number of distributed wind power access to the distribution network. Studies have shown that accurately operational…part 1…; Liu Y, 2008b. The operational…part 2…). It has been downscaled to LES scale modeling grids, through the nested-down grid refinements, this modeling system provides a unique ability for simulating real micro-scale weather processes by incorporating realistic mesoscale weather forcing, high-resolution terrain and land use, and physical processes of solar and long wave radiation and cloud microphy"
pub.1142622093,Artificial Intelligence and Internet of Things for Renewable Energy Systems,"The sustainability incredibly insists in having innovations in renewable energy. To obtain an unsullied and a well-grounded environment, innovations in the present mechanisms have to be uplifted ensuring a predictive framework and an enormous outcome can be expected. The goal needs to be met to establish new research concepts and to move on the energy requirements in optimization of the existing machine learning (ML) framework. With the lot more renewable energy existing in nature, the two most variable and commonly used renewable energies are solar and wind. Its cons are nonuniformity of power and its dependency on external environmental factors. Due to this, sole dependence on renewable energy is not possible, and hence conventional power grid is also to be considered when any sort of predictive analysis needs to be done. Hence, more concentration is to be made on forecasting of renewable energy and on smart grids ensuring continuous equilibrium and balance within renewable energy and conventional grid. The electricity demand and supply of power can be predicted using ML algorithms ensuring better savings with operational costs. The two-way electricity and information flow will prove smart grid in future ensuring continuous monitoring of the network bringing more requirements for ML framework. The early warning systems incorporated by Germany symbolize a very good example of how ML algorithms analyze the realtime data from various renewable energy sources to analyze the total amount of energy requirement of the country. Google invention on DeepMind proves that the energy efficiency is 3.5 times more compared to the energy demands of the last 5 years. The innovations on intelligent home energy management systems prove promising energy usage with ML in real time. The consumer or the customer behavior can be predicted easily with intelligent techniques along with various other features like weather or climate modeling ensuring a complete and interoperable framework that suits the conventional grids. Artificial intelligence helps in sustainability of the grid with more focus on demand response. The energy management and operation cost management in smart grid is a promising feature incorporating ML algorithms in renewable energy. The optimization on managing the asset along with its maintenance ensures efficient management of power. The large amount of data collected brings data mining and prediction, thereby enabling more analytics on data. To establish such a kind of platform, various transfer models for solar and wind need to be applied along with irradiance to power models incorporating blending of information along with its categorization in ML algorithms. Feature selection methods along with the diversified algorithms in ML for diversified applications ensure continuous upgradation in framework rather than integrated approach. This chapter is devoted to the implementation of advanced controlled techniques to enhance the generation in wind ene"
pub.1150522631,FAIR Research Objects for realizing Open Science with RELIANCE EOSC project,"The H2020 Reliance project delivers a suite of innovative and interconnected services that extend European Open Science Cloud (EOSC)’s capabilities to support the management of the research lifecycle within Earth Science Communities and Copernicus Users. The project has delivered 3 complementary  technologies: Research Objects (ROs), Data Cubes and AI-based Text Mining.
                  RoHub is a Research Object management platform that implements these 3 technologies and enables researchers to collaboratively manage, share and preserve their research work.
                  RoHub implements the full RO model and paradigm: resources associated to a particular research work are aggregated into a single FAIR digital object, and metadata relevant for understanding and interpreting the content is represented as semantic metadata that are user and machine readable.
                  The development of RoHub is co-designed and validated through multidisciplinary and thematic real life use cases led by three different Earth Science communities: Geohazards, Sea Monitoring and Climate Change communities.
                  A RO commonly starts its life as an empty Live RO. ROs aggregate new objects through their whole lifecycle. This means, a RO is filled incrementally by aggregating new relevant resources such as workflows, datasets, documents according to its typology that are being created, reused or repurposed. These resources can be modified at any point in time.
                  We can copy and keep ROs in time through snapshots which reflect their status at a given point in time. Snapshots can have their own identifiers (DOIs) which facilitates tracking the evolution of a research. At some point in time, a RO can be published and archived (so called Archived RO) with a permanent identifier (DOI). New Live ROs can be derived based on an existing Archived RO, for instance by forking it.
                  To guide researchers, different types of Research Objects can be created:
                  
                    
                      
                        Bibliography-centric: includes manuals, anonymous interviews, publications, multimedia (video, songs) and/or other material that support research;
                      
                      
                        Data-centric: refers to datasets which can be indexed, discovered and manipulated;
                      
                      
                        Executable: includes the code, data and computational environment along with a description of the research object and in some cases a workflow. This type of ROs can be executed and is often used for scripts and/or Jupyter Notebooks;
                      
                      
                        Software-centric: also known as “Code as a Research Object”. Software-centric ROs include source codes and associated documentation. They often include sample datasets for running tests.
                      
                    "
pub.1068957036,Icebreaking Drillship for Offshore Exploratory Drilling in the Arctic,"An offshore drilling system designed expressly for the unique environment of the Arctic must take into account many parameters. This paper discusses design considerations for a drillship intended for extended offshore operation in the Arctic areas. Discussion of environmental protection and safety features for icebreakers, shipping rules, and icebreaking capability are included.  Introduction About 3 years ago it became apparent that the offshore oil potential in the Beaufort Sea and Canadian Arctic Islands areas was sufficient to give serious consideration to practically and economically providing a system capable of performing the needed exploratory drilling service. The practical approach was to use proven drilling techniques, systems, and equipment, proven drilling techniques, systems, and equipment, but to apply them to an environment not previously considered in their design, using imagination to accept opposing precepts to conceive a workable compromise. Economics dictate that the results must be reasonably capable of profitable application. A preference for ship-shaped units for exploratory drilling and the existing technology for breaking navigable ice channels with ships afforded a marriageable pair of disciplines. The only total unknown was how pair of disciplines. The only total unknown was how to maintain a drilling station in a slowly moving ice sheet.   Design Considerations Combining a drillship with an icebreaker provided the basis for the design considerations (Fig. 1). Throughout the design of the vessel, safety was the over-riding criterion - safety not only for the personnel, vessel, and equipment, but also safety personnel, vessel, and equipment, but also safety in terms of environmental protection. The design evolved around considerations of environmental parameters such as climatic conditions and their parameters such as climatic conditions and their impact; crew, equipment, and vessel safety; regulatory parameters; and icebreaking and operational requirements.   Environmental Parameters Experience with ship-shaped drilling vessels was a starting point for the design. This experience was adapted to the hostile climate of the Arctic. Where interfacing problems between the drillship and the icebreaker were encountered, the solution was normally an adaptation of drilling equipment or systems to the environment. The winters of the Beaufort Sea and Canadian Arctic Islands are long and severe. Although temperatures do not fall as far below zero as they do in the southern continental belt of the Arctic, the cold is persistent. The chill factor, coupled with the strong prevailing winds, makes this a very hostile region during the winter months. The long period of total darkness further complicates operations period of total darkness further complicates operations in the Arctic during this season. The summers in this area are cool and short. Cloudiness is prevalent along the coast because of the open water and abundant moisture, Th"
pub.1092891625,RStore,"Motivation.The iterative and exploratory nature of the data science process, combined with an increasing need to support debugging, historical queries, auditing, provenance, and reproducibility, warrants the need to store and query a large number of versions of a dataset. This realization has led to many efforts at building data management systems that support versioning as a first-class construct, both in academia [1, 3, 5, 6] and in industry (e.g., git, Datomic, noms). These systems typically support rich versioning/branching functionality and complex queries over versioned information but lack the capability to host versions of a collection of keyed records or documents in a distributed environment or a cloud. Alternatively, key-value stores1 (e.g., Apache Cassandra, HBase, MongoDB) are appealing in many collaborative scenarios spanning geographically distributed teams, since they offer centralized hosting of the data, are resilient to failures, can easily scale out, and can handle a large number of queries efficiently. However, those do not offer rich versioning and branching functionality akin to hosted version control systems (VCS) like GitHub. This work addresses the problem of compactly storing a large number of versions (snapshots) of a collection of keyed documents or records in a distributed environment, while efficiently answering a variety of retrieval queries over those. RStore Overview. Our primary focus here is to provide versioning and branching support for collections of records with unique identifiers. Like popular NoSQL systems, RStore supports a flexible data model; records with varying sizes, ranging from a few bytes to a few MBs; and a variety of retrieval queries to cover a wide range of use cases. Specifically, similar to NoSQL systems, our system supports efficient retrieval of a specific record in a specific version (given a key and a version identifier), or the entire evolution history for a given key. Similar to VCS, it supports retrieving all records belonging to a specific version to support use cases that require updating a large number of records (e.g., by applying a data cleaning step). Finally, since retrieving an entire version might be unnecessary and expensive, our system supports partial version retrieval given a range of keys and a version identifier. Challenges. Addressing the above desiderata poses many design and computational challenges, and natural baseline approaches (see full paper [2] for more details) that attempt to build this functionality on top of existing key-value stores suffer from critical limitations. First, most of those baseline approaches cannot directly support point queries targetting a specific record in a specific version (and by extension, full or partial version retrieval queries), without constructing and maintaining explicit indexes. Second, all the viable baselines fundamentally require too many back-and-forths between the retrieval module and the backend key-value store; this "
pub.1170825523,Research of the Amazon Lex V2 natural language recognition system,"In modern realities, systems with text and voice recognition are increasingly used, because the process of interaction with a person is increasingly automated. Human speech recognition technology is fundamental to the development of artificial intelligence. Systems that are built on this technology have the ability to solve a fairly large number of tasks related to data analysis, information search, and fulfillment of user requests. Voice assistants, smart home systems (such as Amazon Alexa or Google Assistant) are popular and quite common solutions for systems with NLP (Natural Language Processing). The study of this technology and the analysis of its integration in cloud environments provides an opportunity to independently use its features as automated assistants, or, for example, as part of an already existing intelligent system. The NLP core of the Amazon Lex V2 service provides an opportunity to deploy a self-configured chatbot on a given topic, therefore, from the point of view of development and research, it attracts the attention of developers and analysts. But, before that, many questions arise regarding its possibilities and limitations, which the integrator of this service will face.
У сучасних реаліях все більше використовуються системи з розпізнаванням тексту та голосу, бо процес взаємодії з людиною все більше автоматизується. Технологія розпізнавання людської мови є фундаментальним для розвитку штучного інтелекту. Системи, які побудовані на цій технології, мають можливість вирішувати досить велику кількість задач, пов’язаних з аналізом даних, пошуку інформації, виконанням запитів користувачів. Популярними і досить поширеними рішеннями систем з NLP (Natural Language Processing) є голосові помічники, системи розумного дому (такі як Amazon Alexa чи Google Assistant). Дослідження цієї технології та аналіз її інтеграції в хмарних середовищах надає можливість самостійно використати її особливості у якості автоматизованих ад’ютантів, чи наприклад, як частину вже існуючої інтелектуальної системи. NLP ядро сервісу Amazon Lex V2 надає можливість розгорнути власноручно налашто-ваний чат-бот на задану тему, тому з точки зору розробки та дослідження він привертає увагу розробників та аналітиків. Але, перед цим, постає багато питань щодо його можливостей та обмежень, перед якими зіткнеться інтегратор цієї служ-би."
pub.1111831871,The geography of poverty: Review and research prospects,"Geography of poverty (GOP) or poverty geography is a branch of human geography, which studies the geographical patterns, distribution characteristics, areal types and evolution mechanism of poverty and the relationship with geographical environment as well as antipoverty measures. Based on the systematical analysis on the significance of GOP research, this study firstly put forward the impoverished areal system (IAS), and then elaborated the main contents, research progresses and existing problems in GOP research, and finally proposed the possible key areas in the future. Results show that the IAS is an open system with structure and function and has its life-cycle law, which is composed of natural endowments, location conditions, human capital and geographical capital within a certain geographical area. The subsystem of human, land and industry is the core of the IAS. Poverty geography studies both regional (place) poverty and individual (people) poverty. Regional poverty is an external manifestation of the coupling maladjustment of human, land and industry elements in a particular area. There are 5W + H (What, Where, Why, When, Who and How) models in GOP research. Key areas of future GOP research include: 1) IAS's life cycle evolution law; 2) regional multidimensional poverty measurement; 3) geographical identification of poverty and its areal type; 4) dynamic simulation of impoverished and its mechanism; 5) poverty mapping; 6) antipoverty measure; and 7) poverty reduction effectiveness evaluation. Facing the UN's goal of eradicating poverty by 2030, poverty geography research in the new period should focus on the complexity, spatial heterogeneity and mechanism of poverty, and designs anti-poverty paths and models suitable for different countries. To adapt to the trend of globalization and informationization, poverty geographers should make use of modern technologies such as data platform, cloud computing, remote sensing and artificial intelligence to focus on the spatio-temporal pattern of poverty and its driving mechanism as well as antipoverty path, and to solve the global poverty problem and promote the internationalization, basification and engineering of geography."
pub.1103774929,Technology Focus: Intelligent Fields Technology (May 2018),"Technology Focus
                  I’m sure you’ve seen it very obviously happening all around us. Yet, looking at the details still surprises.  While reviewing the papers published in the intelligent-fields area this year, I was struck by the contrast I saw compared with just 3 years ago. Novel and niche are giving way to systemic and pervasive. What only recently was the domain of academics and research types with larger operators and service companies has broadened to an amazing diversity of practitioners.
                  The papers mirror what I see and hear in many of the companies I interact with. Yes, our companies have hired more people with formalized training in data science, but what I find impressive is the number of people who were hiding in normal discipline jobs only a few years ago who are coming out of the closet with their Python scripts. And, it’s working. In many ways, order is coming to the mess, efficiency is coming to tiresome manual activities, and richness is coming to our decisions.
                  So, what changed that we are more rapidly seeing the promised progress? 
                  I associate much of the acceleration to what I would call an open-source mentality, an approach that prefers to find an appropriate, available solution that is easily accessible, rather than developing or buying something fit-for-purpose. “There’s an app for that” has evolved to marketplace models, not only on your smart phone but also now in the Jupyter notebook on your desktop or in the marketplace of your cloud environment. As a result, or perhaps as a driving part of the changes, tech giants such as Amazon and Microsoft are finding their part in the energy sector by providing convenient and efficient marketplaces sup-porting integration of open-source and proprietary technologies. Smaller companies and startups can deliver low-cost solutions to such environments, and cooperative developments such as the Open Earth Consortium will bring further efficiencies by delivering standard oil-and-gas-specific frameworks. Instead of armies of developers delivering the next generation over 5 or 10 years, a capable community is emerging that can deliver a multitude of small advances that build on synergies of existing capabilities.
                  I hope to see you at the SPE workshop on Smart Integration in Production System Modeling on 19–20 June in Galveston, Texas, USA."
pub.1181829416,ОРГАНІЗАЦІЯ ЗАХИСТУ БУХГАЛТЕРСЬКОЇ ІНФОРМАЦІЇ В АВТОМАТИЗОВАНИХ СИСТЕМАХ ОБЛІКУ,"This article explores the organization of accounting information protection in automated accounting systems, a critical aspect for ensuring the confidentiality, integrity, and availability of financial data. As businesses increasingly rely on technology for managing their financial information, the potential risks and threats associated with data breaches have become more pronounced. The research identifies the primary threats that enterprises face, including cyberattacks, unauthorized access, and data corruption. Additionally, it analyzes modern technologies that can be implemented to enhance the security of accounting systems. Among these technologies, artificial intelligence (AI) stands out as a powerful tool for detecting anomalies and predicting potential security breaches before they occur. AI algorithms can analyze vast amounts of data in real time, providing organizations with the insights needed to respond swiftly to any irregularities. Furthermore, the integration of blockchain technology offers a decentralized and immutable ledger that ensures data integrity and transparency, making it extremely difficult for malicious actors to manipulate financial records. The article also emphasizes the importance of backup systems, particularly cloud-based solutions, which enable organizations to recover data quickly in case of system failures or cyber incidents. However, despite the advancements in technology, several unresolved issues persist in the realm of information security. One significant challenge is the insufficient integration of innovative technologies into existing systems, leading to vulnerabilities that can be exploited by cybercriminals. Moreover, the lack of unified security standards hampers the ability of organizations to implement effective protection measures. Establishing comprehensive security frameworks that outline best practices and compliance requirements is essential for enhancing the overall security posture of accounting information systems. Another critical aspect highlighted in the article is the need for ongoing employee training. Human error remains one of the leading causes of data breaches; therefore, regular training programs are vital to ensure that employees are aware of potential threats and equipped with the knowledge to mitigate risks. By identifying these unresolved issues, this study opens new avenues for further research in the field of information security in accounting. The findings underline the necessity for organizations to adopt a systematic approach to protect accounting information, incorporating advanced technological solutions while addressing the human factor. This holistic perspective aims to foster a secure information environment that not only safeguards financial data but also builds trust among stakeholders. In conclusion, the article calls for collaboration among researchers, practitioners, and regulatory bodies to develop comprehensive frameworks that ensure the security of accounting "
pub.1157613023,Evolution of titanium particle combustion in potassium perchlorate and air,"Understanding titanium particle combustion processes is critical not only for characterizing existing pyrotechnic systems but also for creating new igniter designs. In order to characterize titanium particle combustion processes, morphologies, and temperatures, simultaneous spatially-resolved electric field holography and imaging pyrometry techniques were used to capture post-ignition data at up to 7 kHz. Due to the phase and thermal distortions present in the combustion cloud, traditional digital in-line holography techniques fail to capture accurate data. In this work, electric field holography techniques are used in order to cancel distortions and capture the three-dimensional spatial locations and diameters of the particles. In order to estimate the projected surface temperatures of the titanium particles, an imaging pyrometry method that ratios emission at 750 and 850 nm is utilized. Using these diagnostics, joint statistics are collected for particle size, morphology, velocity, and temperature. Results show that, early in the combustion process, the titanium particles are primarily oxidized by potassium perchlorate inside the igniter cup, resulting in projected surface temperatures near 3000 K. Later in the process, the particles interact with ambient air, resulting in lower surface temperatures around 2400 K and the formation of flame zones. These results are consistent with adiabatic flame temperature predictions as well as particle morphology observations of a titanium core with a TiO2 surface. Late stage particle expansion, star fragmentation, and molten droplet breakup events are also observed using the time-resolved morphology and temperature diagnostics. These results illustrate the different stages of titanium particle combustion in pyrotechnic environments, which can be used to inform improvements in next-generation igniters."
pub.1157728000,LST-Bench: Benchmarking Log-Structured Tables in the Cloud,"Data processing engines increasingly leverage distributed file systems for
scalable, cost-effective storage. While the Apache Parquet columnar format has
become a popular choice for data storage and retrieval, the immutability of
Parquet files renders it impractical to meet the demands of frequent updates in
contemporary analytical workloads. Log-Structured Tables (LSTs), such as Delta
Lake, Apache Iceberg, and Apache Hudi, offer an alternative for scenarios
requiring data mutability, providing a balance between efficient updates and
the benefits of columnar storage. They provide features like transactions,
time-travel, and schema evolution, enhancing usability and enabling access from
multiple engines. Moreover, engines like Apache Spark and Trino can be
configured to leverage the optimizations and controls offered by LSTs to meet
specific business needs. Conventional benchmarks and tools are inadequate for
evaluating the transformative changes in the storage layer resulting from these
advancements, as they do not allow us to measure the impact of design and
optimization choices in this new setting.
  In this paper, we propose a novel benchmarking approach and metrics that
build upon existing benchmarks, aiming to systematically assess LSTs. We
develop a framework, LST-Bench, which facilitates effective exploration and
evaluation of the collaborative functioning of LSTs and data processing engines
through tailored benchmark packages. A package is a mix of use patterns
reflecting a target workload; LST-Bench makes it easy to define a wide range of
use patterns and combine them into a package, and we include a baseline package
for completeness. Our assessment demonstrates the effectiveness of our
framework and benchmark packages in extracting valuable insights across diverse
environments. The code for LST-Bench is open-sourced and is available at
https://github.com/microsoft/lst-bench/ ."
pub.1170173928,LST-Bench: Benchmarking Log-Structured Tables in the Cloud,"Data processing engines increasingly leverage distributed file systems for scalable, cost-effective storage. While the Apache Parquet columnar format has become a popular choice for data storage and retrieval, the immutability of Parquet files renders it impractical to meet the demands of frequent updates in contemporary analytical workloads. Log-Structured Tables (LSTs), such as Delta Lake, Apache Iceberg, and Apache Hudi, offer an alternative for scenarios requiring data mutability, providing a balance between efficient updates and the benefits of columnar storage. They provide features like transactions, time-travel, and schema evolution, enhancing usability and enabling access from multiple engines. Moreover, engines like Apache Spark and Trino can be configured to leverage the optimizations and controls offered by LSTs to meet specific business needs. Conventional benchmarks and tools are inadequate for evaluating the transformative changes in the storage layer resulting from these advancements, as they do not allow us to measure the impact of design and optimization choices in this new setting. In this paper, we propose a novel benchmarking approach and metrics that build upon existing benchmarks, aiming to systematically assess LSTs. We develop a framework, LST-Bench, which facilitates effective exploration and evaluation of the collaborative functioning of LSTs and data processing engines through tailored benchmark packages. A package is a mix of use patterns reflecting a target workload; LST-Bench makes it easy to define a wide range of use patterns and combine them into a package, and we include a baseline package for completeness. Our assessment demonstrates the effectiveness of our framework and benchmark packages in extracting valuable insights across diverse environments. The code for LST-Bench is open source and is available at https://github.com/microsoft/lst-bench/."
pub.1139288222,Impacts of groundwater and climate variability on terrestrial groundwater dependent ecosystems: a review of geospatial assessment approaches and challenges and possible future research directions,"Terrestrial groundwater dependent vegetation (TGDV) are crucial ecosystems which provide important goods and services such as carbon sequestration, habitat, water purification and aesthetic benefits in semi-arid environments. Global climate change and anthropogenic impacts on surface water resources have led to increased competing claims on groundwater resources to meet an exponential water demand for environmental needs, agricultural and developmental needs. This has led to the unsustainable exploitation of groundwater resources, resulting in groundwater table declines, threatening the sustainability of TGDV. It is on this premise that the review aims to provide a detailed overview on the progress in remote sensing of TGDV. More specifically, the paper provides a background on TGDV and threats, and then further explores recent knowledge on vegetation response to groundwater variability and climate change impacts on TGDV. This review also focuses on recent progress in remote sensing and geographic information systems (GIS) based techniques for mapping and monitoring of TGDV and explores the available satellite products and delineation techniques. Finally, the challenges of remote sensing and future research direction are explored. To date, research on TGDV has gained considerable interest with the year 2020 resulting in the most scientific journal publications. Of significant importance is an increase in studies integrating field measurements, model-based techniques with remotely sensed estimates. Despite this progress, only 0.06% of groundwater dependent ecosystems (GDE) research has utilized remote sensing techniques in the past 20 years, with the top three publishing countries namely, Australia, USA, and China. The literature reveals that TDGV are highly heterogenous, complex ecosystems with unique responses to varying groundwater levels. The vegetation responses differ with the landscape, vegetation type, and seasonality at specific groundwater table thresholds. Despite significant progress in TGDV scientific research, further remote sensing studies are required to understand the annual and inter-annual vegetation response to groundwater variability at local scales. Further, climate impacts are difficult to discriminate from other influences such as disturbances, management, and anthropogenic activities. Moreover, new generation remote sensing products integrated with machine learning techniques have the potential to improve TGDV delineation. Despite these challenges, the development of cloud computing technologies such as google earth engine (GEE) and artificial intelligence (AI) provide advanced computer-processing capabilities for long-term monitoring and integration of multi-source datasets required to capture the effects of climate and groundwater variability on TGDV."
pub.1160687324,Edge computing-based containerized deep-learning approach for intrusion detection in healthcare IoT,"Internet of Things (IoT) has accelerated the transformation of the healthcare sector significantly by improving people's standard of living by leading healthier lives. Distributed heterogeneous IoT devices with embedded sensors collect, process, and store vast amounts of patients’ data in a dynamic environment to facilitate timely and inexpensive health services. Though IoT has provided quality care to patients and enhanced healthcare facilities, it has also increased the cyber-attack surface in the healthcare sector. Most of the existing intrusion detection systems for healthcare IoT offer a cloud-based approach that suffers from latency and is unsuited for the healthcare sector which involves heterogeneous devices generating voluminous real-time data from a dynamic environment, requiring timely responses. The traditional machine-learning-based intrusion detection methodologies are shallow-learning methods that cannot detect unforeseen attacks in high-dimensional massive data from healthcare IoT devices. This forces the need for a distributed intrusion detection approach to healthcare IoT which can analyse the vast bulk of data from healthcare IoT devices and detect unforeseen attacks at a faster rate with greater accuracy. The proposed approach detects the attacks on healthcare IoT devices by implementing containerized deep-learning-based intrusion detection at distributed fog nodes for analysing massive data and detecting zero-day attacks with great accuracy at a faster rate. Disabilities can take on a variety of forms and dimensions. A clustered bar graph plots the percentage versus different age groups of 0 to 19 years, 20 to 39 years, 40 to 59 years, and above 60 and age not specified. People with significant underlying chronic medical conditions, such as chronic lung disease, a serious cardiac condition, or a compromised immune system, tend to be more susceptible to becoming unwell from COVID-19. Reports have noted difficulties suffered by the disabled in terms of acquiring food, necessities, and life-saving medical procedures during the lockdown, in addition to the difficulty accessing information and helplines. Social impediments play an important role in affecting a person's ability to function and on the environment in which they were born, raised, educated, and employed. The Indian government created a technological miracle with the Arogya Setu app."
pub.1128221246,An effective feature engineering for DNN using hybrid PCA-GWO for intrusion detection in IoMT architecture,"The entire computing paradigm is changed due to the technological advancements in Information and Communication Technology (ICT). Due to these advancements, various new communication channels are being introduced, out of which the Internet of Things (IoT) plays a significant role. The Internet of Medical Things (IoMT) is a special category of IoT in which the medical devices communicate with each other for sharing sensitive data. These advancements help the healthcare industry to have better contact and care towards their patients. But they too have certain drawbacks since there are so many security and privacy issues like replay, man-in-the-middle, impersonation, privileged-insider, remote hijacking, password guessing, denial of service (DoS) attacks and malware attacks. When the sensitive data is being attacked by any of these attacks, there is a chance of losing the authorized data to the attacker or getting altered due to which the data is not available for the authorized users and customers. Machine learning algorithms are widely used in the Intrusion Detection System (IDS) for detecting and classifying the attacks at the network and host level in a dynamic manner. Many supervised and unsupervised algorithms have been designed by researchers from the area of machine learning and data mining to identify the reliable detection of an anomaly. However, the main challenge in the IDS models are changed in dynamic and random behavior of malicious attacks and designing a scalable solution that can handle this behavior. The rapid change in network behavior and the fast evolution of various attacks paved the way for evaluating various datasets that are generated over the years and to design different dynamic approaches. In this paper, a deep neural network (DNN) is used to develop effective and efficient IDS in the IoMT environment to classify and predict unforeseen cyberattacks. The network parameter are preprocessed, optimized and tuned by hyperparameter selection methods. A comprehensive analysis of experiments in DNN with other machine learning algorithms are compared on the benchmark intrusion detection dataset. Through rigorous testing, it has proved that the proposed DNN model performs better than the existing machine learning approaches with an increase in accuracy by 15% and decreases in time complexity by 32%, which helps in faster alerts to avoid post effects of intrusion in sensitive cloud data storage."
pub.1118097954,“A journey as a flow”: A personal spatio-temporal projection of the world,"Abstract. When regarding the changing world of today, both artists and cartographers attempt to map what they perceive and are increasingly compelled to go beyond static and unchangeable geometric plans which represent a constrained version of the dynamics of our lives. In so doing these skilled researchers are questioning the existing models of representation of reality, by placing a stronger accent on the way all connections are established at geopolitical, social and personal levels.Therefore, this project aims to draw attention to this, how living consciously in the contemporary world involves rethinking our self-projection in time. What follows are ideas for an art exhibition that depicts dynamic space-time and thematic aspects of individual lives, drawing from geographic and artistic representation, conveying as well the distinction between plastic spaces and behavioral ones (Forer, 1978). In moving forward, many models could be adapted to the changing individual status of the life traveler, represented in a 3D space-time exhibition room (Hagerstrand, 1970). Within the meta-space-time we have individual lifetime space-time paths traced by spherical structures, which themselves contain a more temporally specific daily path (Fig 1a). Being inside the flow of different events is more a case of switching between interchangeable references (or multiple spherical structures - Fig 1b) instead of using behavioral patterns to pigeon-hole the traveler in only one of a number of possible lifestyles, which is the static, invariant take. Therefore, many possible destinations and routes of each individual flow (Fig 1c) on today’s map are changeable too, leaving projected traces in space-time, because of differences in perception inside of the total flows within big urban metropolises.If we attempt to re-define a journey as a way to create all the spatial connections with others by including as well a specific way of being in time, we will realize that as a consequence of our real time or virtual behavior, each day becomes a new type of a journey inside of different flows. Our self-projections are multifaceted and directly related to the way we establish connections in the world, subject to disrupting forces, such as hurricanes (an alternative representation - Fig 1d).Another perspective on this is to regard the balloon or cloud as a result of environmental processes to visualize different flows that are happening and in which we are participating. It is more a result of the flows than a flow in itself. It can have different color because of the intensity of the process that are operating behind the flow (expressed through cartographic means, linking color to theme or emotion, Figure 2). Because it depends on how it appears, it can have different ways to be expressed: as a result of the connections of the city flows, a result of a daily exploration of space-time in a mental sense etc. But it appears because of the flows, which are interrupting the regular"
pub.1106993241,A National Concept Dictionary,"Overall objectives or goalMost of the organizations that use population administrative data for research purposes have internal repository of validated definitions and algorithms of their own. Many of these concepts and definitions are applicable or at least adaptable to other organizations and jurisdictions. A comprehensive National (and potentially International) Concept Dictionary could help investigators to carry out methodologically sound work using consistent and validated algorithms using a shared pool of knowledge and resources. The Institute for Clinical Evaluative Sciences (ICES) in Ontario, Canada has recently modernized its internal Concept Dictionary by adopting standard templates based on the Manitoba Centre for Health Policy (MCHP) Concept Dictionary, reviewing and updating existing content and tagging the concept entries with appropriate MeSH terms and data sources, and adding standard computer code (e.g., SAS coding) where appropriate. A SharePoint® web-based application has been developed to provide advanced tagging, searching and browsing features. We envision a wiki-based Concept Dictionary hosted on a cloud-based environment with very granular access controls to provide enough flexibility for each participating organization to control their own content. This means each organization will be able to decide on how to share their own concepts (or part of them) with the public or internal users. All content will be tagged with MeSH terms and as well with the organization’s name that initially posts each entry. Other organizations which find the same concept applicable to their own use can tag the same entry with their organization name or refer to a secondary adapted entry if adaptation to fit their data and methodologies is required. The Search feature will allow refining the search criteria by MeSH terms, data sources, and also organization/jurisdiction name. Multiple layers of access controls will allow each organization to have their own groups of users with different standard privileges such as Local Administrators, Authors and Approvers (or Publishers). The Approver (Publisher) users within each organization can publish each entry for internal or public view. This way, for example, a definition/algorithm can be viewable only within the organization until the validation process is complete, and then the entry can be made publically available, while some sections, such as computer code, can remain restricted to the organization. We will discuss challenges in developing and maintaining such a platform including the costs, governance, intellectual property rights, copyrights and liabilities for the participating organizations. The intended output or outcomeWe aim to use this opportunity to form a working group from the interested organizations that are ready to participate and commit in developing this collaborative platform. After the conference, there will be follow up sessions with the members of the working group to plan and"
pub.1169736455,Omni-Scan2BIM: A ready-to-use Scan2BIM approach based on vision foundation models for MEP scenes,"Mechanical, electrical, and plumbing (MEP) systems play a crucial role in providing various services and creating comfortable environments for urban residents. In order to enhance the management efficiency of these highly complex MEP systems, as-built building information models (BIMs) are being increasingly adopted worldwide. As-built BIMs accurately represent the actual conditions of facilities, making as-built BIM reconstruction significantly important for construction progress tracking, quality assurance, subsequent facility management, and renewal. To create as-built BIMs for MEP systems, laser scanners are widely utilized to capture high-resolution images and dense 3D measurements of the environment in a fast and highly accurate manner. Despite research efforts to automatically achieve “Scan-to-BIM,” there are still gaps in applying current solutions to real-world scenarios. One of the major challenges are the limited generalization of existing methods to unseen scenarios without proper training or fine-tuning on custom-designed datasets. To address this issue, this study introduces Omni-Scan2BIM, a novel approach powered by large-scale pre-trained vision foundation models. Omni-Scan2BIM enables the recognition of MEP-related components with a single shot by integrating an all-purpose feature extraction model and a class-agnostic segmentation model. Firstly, given only a single image with a reference mask, the visual features are extracted for both the target component and the collected on-site images using the vision foundation model DINOv2. Secondly, through comparing pixel features, similarity maps are generated for the on-site images. The prior points for the class-agnostic segmentation model are sampled from the local maxima of the similarity map. Thirdly, Segment Anything Model (SAM) is leveraged to sequentially segment the target component. Finally, the target component is segmented out in 3D space based on the transformation matrix describing the spatial relationship between the 2D images and the 3D point clouds. Shape analysis and label fusion are conducted for as-built BIM modeling purpose. To validate the feasibility of the proposed technique, experiments were conducted using data collected from a real construction site in Hong Kong. The results demonstrate that the proposed Omni-Scan2BIM approach can easily generalize to unseen components with significantly improved accuracy and efficiency."
pub.1168760724,Identification of Safety Risk Factors in Metro Shield Construction,"Among the construction methods for subway projects, shield method construction technology has become a more widely used construction method for urban subway construction due to the advantages of a high degree of construction mechanization, low impact of the construction process on the environment, and strong adaptability of the shield machine to the stratum, etc. However, because of the complexity of the surrounding buildings (structures) in the subway construction, coupled with the diversity of the subway shield method construction activities and the uncertainties in the construction environment, to a certain extent, it is determined that the subway construction process is very complicated. The purpose of this study is based on the text mining method, where text is mined and utilized to realize the identification, extraction, and display of safety risk factors. Thus, it guides the safety management on site and provides a basis for knowledge reuse in other metro shield construction projects. Firstly, we analyze the shortcomings of safety risk management in domestic and international metro shield construction via a literature review, especially the utilization of safety risk text data. Secondly, we collect the risk reports submitted by all parties via the “Metro Project Safety Risk Early Warning System”, and manually screen the hidden danger statements with risk characterization to establish a corpus. Thirdly, we use the Jieba word separation package to extract and display the safety risk factors, so as to guide the on-site safety management. Subsequently, with the help of the Jieba word segmentation package for Chinese word segmentation, we develop a professional thesaurus to improve the effect of word segmentation; then, we use the TF-IDF parameter assignment to achieve the structural transformation of the text to extract high-frequency vocabulary; finally, from the high-frequency vocabulary to screen words containing the semantics of the risk to establish the risk of an initial set of words, we use the existing standards and norms to form the collection of safety risk factors of subway shield construction and generate the cloud diagram for visual display."
pub.1155763629,Generative Adversarial and Dual Layered Deep Classification Techniques for Improving Block Constructions in Public Cloud,"This paper provides the Generative Adversarial and Dual Layered Deep Classification techniques to improve the drawbacks in the methods of Absolute Moment BTC (AMBTC) technique in reconstruction error rate of standard BTC model. The image blocks generation and compression are the main phases of BTC model. This can be applied for both colour images and grey scale images. However, the conventional BTC procedures lacks for edge reconstructions and noise reductions in the output images. The first technique GABTC is developed with multi-layered Deep Neural Network (DNN) structures with GA neural models. The integration of both GA models and BTC principles improve the quality of block constructions and reconstructions significantly. The second proposed work is adopted the Dual layered Deep Classification Technique. Handling the image database with minimal storage complexity, minimal computational complexity and optimal quality is a significant task. To obtain these solutions, many image processing techniques are evolved. In the domain, image compression and decompression are more needed at any cost for effectively handling the complex image databases. E-Learning resources are widely used around the internet based knowledge sharing environments. In the E-Learning environment, multiple types of data resources are managed. Particularly, organizing the images is more crucial task where multiple qualities of images are appeared inside the E-Learning network databases. This problem expects solutions from effective image compression techniques. Block Truncation Coding (BTC) and Absolute Moment BTC (AMBTC) are the techniques provide useful and easy implementations of E-Learning based image compression platform. At the same time, they are limited to image dissimilarity rate. To maintain the quality of images in both compression and decompression phases, multilevel image analysis models and training phases are required. In this regard, this proposed system develops a Dual Layered Deep Classification and Truncation (DLDCT) technique. DLDCT comprises the baseline benefits of BTC, multi-layered Support Vector Machine (SVM) units and Deep Layered Convolutional Neural Network (DLCNN) for producing classified range of image pixels and compressing the images under controlled circumstances. This proposed DLDCT makes the image compression and decompression with determined observations. This reduces real time errors occur during image reconstruction phases. This proposed system has been implemented and compared with existing works with respect to significant performance parameters."
pub.1173537552,ASTRALE: an Artificial Intelligence and Citizen-Science driven project,"Our team is part of the Italian non-profit association AstronomiAmo, which aims at disseminating astronomy and promoting respect for the environment. The All-Sky TRacking, ALerts and Environment (ASTRALE) project merges both purposes, providing a beautiful view of the night sky, an automatic detection of meteors and an evaluation of the air quality index.  Specifically, ASTRALE consists of several devices installed across Italy, which collect data in the form of images or numbers, make a first processing and send them to a central server for storage, analysis and reporting. As shown in Figure 1, each peripheral device comes with two installation possibilities: ASTRALE Meteor: for those who have a good sky and visibility in all directions: the system is equipped with a Raspberry PI 4 camera for all-sky monitoring, as well as sensors for sky and air quality. ASTRALE Air: for those who lack a good sky, due to light pollution and / or reduced visibility, but wish to keep under control the quality of the air they breathe.  What makes ASTRALE unique?Automatic detection system: an auomtated detection tool has been developed to identify each transient event recorded by our devices. To date, over 10000 events have been visually inspected and classified by each volunteer citizen. This amount of data has been training an artificial intelligence algorithm that provides a real-time classification of each image. Citizen science is the driving engine to make this ambitious goal possible. Low-cost: As a non-profit association, we are fully committed to keep the cost as low as possible, while ensuring a high-quality performance. This cost simply matches the reimbursement for all hardware components purchased from the market. No earning is present, neither at personal nor at association level. User-friendly: our system can be distributed across Europe and easily installed at home, with no need for maintenance. The only requirements are Internet connection, a socket and an outdoor space. Our team provides full-time assistance to the user.  Interdisciplinary: this project has blossomed from the close interaction between amateur astronomers, professionals, outreach providers, who are all part of AstronomiAmo. The ASTRALE project encompasses complementary fields of expertise, such as astronomy, environmental science, software engineering, science communication and citizen science. For this reason, ASTRALE aims at engaging the widest possible audience, driven by a common passion for science and environmental awareness. How does it work?Images are captured with a Raspberry HQ Cam with a 12.3 Mpixel Sony IMX477 and a CS-mount with a 2.5mm fisheye lens. A dew heater is used to keep the lens and the dome free of moisture. The camera is coupled with a Raspberry PI4 2GB single board computer with a Debian based Linux distribution. The control software is developed in Python and OpenCV. The camera also provides the Sky Quality Meter (SQM) to measure the brightness of the sky "
pub.1169507582,Advanced Sea Ice Modeling for Short-Term Forecasting for Alaska’s Coasts,"Abstract  In Alaska’s coastal environment, accurate information of sea ice conditions is desired by operational forecasters, emergency managers, and responders. Complicated interactions among atmosphere, waves, ocean circulation, and sea ice collectively impact the ice conditions, intensity of storm surges, and flooding, making accurate predictions challenging. A collaborative work to build the Alaska Coastal Ocean Forecast System established an integrated storm surge, wave, and sea ice model system for the coasts of Alaska, where the verified model components are linked using the Earth System Modeling Framework and the National Unified Operational Prediction Capability. We present the verification of the sea ice model component based on the Los Alamos Sea Ice Model, version 6. The regional, high-resolution (3 km) configuration of the model was forced by operational atmospheric and ocean model outputs. Extensive numerical experiments were conducted from December 2018 to August 2020 to verify the model’s capability to represent detailed nearshore and offshore sea ice behavior, including landfast ice, ice thickness, and evolution of air–ice drag coefficient. Comparisons of the hindcast simulations with the observations of ice extent presented the model’s comparable performance with the Global Ocean Forecast System 3.1 (GOFS3.1). The model’s skill in reproducing landfast ice area significantly outperformed GOFS3.1. Comparison of the modeled sea ice freeboard with the Ice, Cloud, and Land Elevation Satellite-2 product showed a mean bias of −4.6 cm. Daily 5-day forecast simulations for October 2020–August 2021 presented the model’s promising performance for future implementation in the coupled model system.   Significance Statement Accurate sea ice information along Alaska’s coasts is desired by the communities for preparedness of hazardous events, such as storm surges and flooding. However, such information, in particular predicted conditions, remains to be a gap. This study presents the verification of the state-of-art sea ice model for Alaska’s coasts for future use in the more comprehensive coupled model system where ocean circulation, wave, and sea ice models are integrated. The model demonstrates comparable performance with the existing operational ocean–ice coupled model product in reproducing overall sea ice extent and significantly outperformed it in reproducing landfast ice cover. Comparison with the novel satellite product presented the model’s ability to capture sea ice freeboard in the stable ice season. "
pub.1136925474,Thunderstorm and fair-weather quasi-static electric fields over land and ocean,"Natural lightning and the associated clouds are known to behave differently over land and ocean, but many questions remain. We expand the related observational datasets by obtaining simultaneous quasi-static electric field observations over coastal land, near-shore water, and deep ocean regions during both fair-weather and thunderstorm periods. Oceanic observations were obtained using two 3-m NOAA buoys that were instrumented with Campbell Scientific electric field mills to measure the quasi-static electric fields. These data were compared to selected electric field records from the existing on-shore electric field mill suite of 31 sensors at Kennedy Space Center (KSC). Lightning occurrence times, locations and peak current estimates for both onshore and ocean were provided by the U.S. National Lightning Detection Network. The buoy instruments were first evaluated on-shore at the Florida coast, and the first system was calibrated for field enhancements and to confirm proper behavior of the system in elevated-field environments. The buoys were then moored 20 mi and 120 mi off the coast of KSC in February (20 mi) and August (120 mi) 2014. Diurnal fair-weather fields at both ocean sites matched will with each other and with those found during the Carnegie cruise, but mean values were 33% smaller, due at least in-part to constraints on the calibration procedure. Diurnal fair-weather fields variations at coastal and inland sites were a poorer match than offshore, likely because the offshore environment is “cleaner” with limited variations in local space charge, lower surface aerosol densities, little surface heating to disturb the surface charge layer during fair weather, and fewer local radioactive sources to modulate the near-surface electrical conductivity. Storm-related static fields were 4-5× larger at both oceanic sites than over land, likely due to decreased screening by near-surface space charge produced by corona current. The time-evolution of the electric field and field changes during storm approach are sufficiently different over land and ocean to warrant further study. This work shows the quality, accuracy, and reliability of these data, and has demonstrated the practicality of off-shore electric field measurements for safety- and launch-related decision making at KSC."
pub.1121803703,GEBCO-NF Alumni Team Technology Solution for Shell Ocean Discovery XPRIZE Final Round,"The GEBCO-NF Alumni Team is one of the five teams who completed the final round of the Shell Ocean Discovery XPRIZE challenge. This international team is made up of industry experts, advisors from within the GEBCO community and broader ocean community and is led by alumni from the Nippon Foundation / GEBCO Graduate Certificate Ocean Mapping Training Program at the Centre for Coastal and Ocean Mapping / Joint Hydrographic Center (CCOM/JHC) of the University of New Hampshire. The Team is distinguished by its extraordinary diversity with a global distribution of representatives from academic institutions, offshore survey and technology industries, academia as well as national hydrographic offices. The alumni worked closely with partners such as Hushcraft Ltd., Ocean Floor Geophysics Inc., Earth Analytic and Teledyne CARIS as well as equipment supplier Kongsberg Maritime AS to develop and advance the Team concept created for the Shell Ocean Discovery XPRIZE. The project was established and supervised at the University of New Hampshire. The Shell Ocean Discovery XPRIZE competition aimed to push the boundaries of ocean technologies by creating solutions to the grand challenge of mapping our ocean floor. The competition requirements had to be met within a short timeline of approximately one year per round. The Round 2 final field test was to demonstrate a complete system that could map 250 km2 in 24 hours to produce a grid with 5 m cell size and at least 10 images of the sea floor. Operations had to be remotely coordinated from a land-based operation center. The entire mapping system had to fit into a standard 40-foot shipping container. The aim of the GEBCO-NF Alumni Team has been to leverage existing technology, wherever possible, and to integrate them to achieve the competition requirements. The strategic approach is to develop strong partnerships with technology and services providers to augment the hardware, integration and software needs of the Team. The GEBCO-NF Alumni Team conceived a two-system, Autonomous Underwater Vehicle (AUV) and Remote-controlled or Unmanned Surface Vehicle (USV), concept to autonomously map the seafloor in a wide variety of ocean environments. Autonomous seafloor surveys, with remote AUV launch and recovery (human-in-the-loop) and with the USV autonomously tracking the AUV for a complete survey mission while being monitored from a remote shore station, were demonstrated to be a viable option for future offshore survey and inspection projects during Round 1. SEA-KIT, the Team’s USV, is an innovative new vessel that allows for autonomous management of AUV deployment and retrieval. In addition, the capability of being a stand-alone mapping platform was demonstrated during the competition using Kongsberg Maritime’s deep-water multibeam sonar EM304 mounted on the gondola below SEA-KIT. The Kongsberg Maritime HUGIN AUV was used for this competition to execute the underwater operations. The Team was confident that competition c"
pub.1162643049,DIGITALIZATION AS A POWERFUL FACTOR IN ENSURING HIGH COMPETITIVENESS OF THE ENTERPRISE,"The purpose of the article. In this article, ""digitalization"" is considered as a powerful factor in ensuring high competitiveness of the enterprise. The present requires the manager to make rapid changes and use the latest technologies to preserve, increase integrated development for a stable future. The transition of the economy to the digital era has led to the need to digitalize management processes, introduce digital products to meet market demand. In modern market conditions, the process of digitalization is one of determining factors of economic growth of enterprises and society as a whole. Changes in the external environment, in particular technological transformations, strengthening of information processes, the formation of new consumption demands, lead to the growth of digitalization around the world. The development of digitalization accelerates the achievement of scientific and technological progress and innovative technological methods of management.
 Methodology. In connection with numerous technological innovations, there was a need for changes in the management system. This led to the automation of technological and management processes and caused a high demand for high-quality business analysis in order to create flexible management systems.Companies need to provide highly qualified specialists, capable and ready to work with new products, as well as for further development using technological innovations and quality service. Today, every business entity seeks to increase its competitiveness. Leaders are experiencing rapid change and must use the latest technologies to support and enhance integrated development for a sustainable future. ""Digitalization"" is a factor that provides great advantages in this direction of development.
 Results. The digital dimension offers many new possibilities. Working on the Internet is more important today than ever before. Of course, not everyone is ready for it, but those who have experience of making money through the Internet can already outline its advantages.Digitalization also facilitates the creation of new systems and algorithms that will assist people and perform much of the monotonous work. This type of interaction between man and technology is called complementation. This is the so-called ""collaboration"" between humans and robotic systems, which aims to improve the result and transfer all the boring work to the systems. In particular, the work of a targetologist is very common today - a person who launches advertising on the Internet for a certain target group of people in order to sell a product to those who really need it. And if earlier advertisers had to analyze a huge amount of information about potential customers, today, thanks to the algorithms of various programs, it can be done independently.The digitalization process occurs in all areas of business and entrepreneurship at all stages of activities, from building and managing the company's reputation on the Internet to attracti"
pub.1132952562,Customer Sentiment Analysis Using Cloud App and Machine Learning Model,"The customer sentiments are very important to any business, as positive or negative feedback can affect the sales and adoption of the product in the market and subsequently define the product’s success. The monthly active usage of major social media platform such as Facebook is 2.32 billion monthly active users (MAU) and of Twitter is 126 million; hence, the market for understanding the customer sentiment through social media can be a game changer for a company and can help define the success of the company in the future. If the sentiments of the users are not captured correctly, it could lead to catastrophic failure of the product and hamper company’s reputation. Existing systems require a lot of manual tasks such as customer surveys, aggregating the sentiments then generating excel reports which are not very interactive and require a lot of time to gather results. These reports also do not show real-time data. People express their opinion on social media. Companies can use such platforms to capture honest and transparent opinions of the consumers. The cognitive service evaluates tweet texts and returns a sentiment score for each text, ranging from 0 (negative) to 1 (positive). This capability is useful for detecting positive and negative sentiments in social media such as Facebook, Twitter, customer reviews, and discussion forums. The machine learning model used by the cognitive service helps determine sentiment using data provided by the user. This feedback can allow the company to know the acceptance of the product in prototype stages and can use the same to modify the product as per the customer feedback before making the product generally available. The implementation environment uses Azure services (Logic apps, Cognitive Services, SQL Database, App Services) with Power BI used to generate real-time business intelligent reports to capture customer sentiment, and a Windows 10 workstation can be used to access all these services."
pub.1144129820,New requirements for technical acceptability of construction products as a component of the construction quality assurance system,"The analysis and generalization of theoretical approaches to the formation of the construction quality system, the generalization of the legislative and regulatory framework for quality control in construction. Analysis of the theoretical prerequisites for the formation of quality control of construction revealed that the digital transformation of construction is an integral part of the development of modern society, a prerequisite for changing the system of construction, including control, quality assurance and evaluation of construction processes, works, materials, products and structures.
 It was found that the quality in construction is directly affected by the following factors: quality and completeness of design documentation; the possibility and obligation to comply with legal requirements; quality of construction processes and works, strict compliance with the requirements of technology and construction organization; quality of construction products and structures; technical ability to provide a given level of quality with the help of machines, mechanisms, means of small mechanization, etc .; the level of qualification of management staff, workers and line ITP. The quality system itself can be presented in the format of a ""black box"", where the factor will be named at the entrance, and at the exit - buildings and structures that meet a given level of quality throughout the life cycle of the object.
 It is proposed to consider the construction quality management system in the form of a multicomponent formation and at the same time a dynamic environment, which is constantly changing under the influence of external and internal factors and has a high degree of scholasticism and uncertainty. With the introduction of digital technologies in the organization of construction, including VIM modeling, use of artificial intelligence, cloud services for information storage, quality control tools that can be performed without human intervention, such as drones, robotic elements, surveillance cameras and others, the construction quality assurance system has to change, taking into account the realities of time. Therefore, the system of quality formation in the article is proposed to be considered throughout the life cycle of the object - from pre-project research to liquidation. This will, in contrast to existing approaches, not only combine into a single system all the components of assurance, evaluation and quality control, which are used at the level of all participants in the construction, but also to ensure integrated process management."
pub.1149123781,Technology Focus: Intelligent Operations (May 2022),"The trend of increasing automation and integration of digital work flows continues in all disciplines within the industry. An obvious driver is the increasing digital nature of our world; today’s phones and tablets connected to the cloud perhaps have more computing capabilities than supercomputers from 5 or 10 years ago. Another key driver, arguably the reason why intelligent operations came on the radar of most companies, is being able to improve safety and efficiency in the complex operations that are routinely performed in the oil patch.
                  The last couple of years, I have highlighted efforts around remote operations of fields, smart wells, and the acquisition and integration of real-time data. This year, I have focused on recent drilling-related papers that highlight how safety and performance can be improved with more intelligent operations.
                  Paper SPE 208711 discusses a key requirement I see for successful intelligent operations, especially when the operations span disciplines and companies. Having a standard, consistent, and unambiguous lexicon is critical for automation and digital work flows. This paper highlights an effort to spur the standardization of codes across companies to describe drilling, completions, and other well activities. Anybody who has gone through old well files will recognize immediately the value and the need for such standardization of well activity codes.
                  The other two papers (SPE 208764 and SPE 208784) highlight how automation can lead to increased safety on rigs during operations. Companies are using a combination of cameras, wearables, and other technology to monitor personnel in safety zones on rigs to ensure no one is unknowingly in the wrong spot at the wrong time. A familiar analog to this would be the safety improvements in driving because of collision-avoidance systems in cars (where a combination of technologies including cameras, radar, and lidar are used).
                  Standardization of Well Activity Codes. - Paper SPE 208711 describes the efforts of one operator to standardize well activity reporting codes. A key highlighted improvement is a new coding system that captures both what was done (the typical focus of historical well activity codes) and the broader context on why the activity was performed.
                  The new codes have enabled more granular tracking of performance metrics and a simplification of reporting. Significantly, the operator has recognized the need for a standardized set of codes across the industry and has donated the codes they have developed to an industrywide open-source environment. I hope this spurs wider adoption and refinement of reporting standards across the industry.
                  Using Computer Vision To Monitor Safety Zones and Automate Drillpipe Tallies. - Paper SPE 208764 highlights how digitalization can result in safer operations and improved accuracy through the automation of manual tasks. The pap"
pub.1120053946,Effective High-level Coordination Programming for Decentralized and Distributed Ensembles,"Programming and coordinating decentralized ensembles of computing devices is extremely hard and error-prone. With cloud computing maturing and the emerging trend of embedding computation into mobile devices, the demand for building reliable distributed and decentralized systems is becoming increasingly common and complex. Because of these growing technical challenges, solutions for effective programming and coordination of decentralized ensembles remain elusive. Most main-stream programming methodologies only offer a node-centric view of programming, where a programmer specifies distributed computations from the perspective of each individual computing node (e.g., MPI, transactional memory, the Actor model, Linda Tuple Space, graph processing frameworks). When programming distributed computations in this style, programmers experience minimal shifts in paradigm but such concurrency primitives offer minimal support for the coordination problem. However, as systems grow in complexity and sophistication, maintaining code in this node-centric style of ten becomes costly, as the lack of concurrency abstraction means that programmers assumes all the responsibility of avoiding concurrency pitfalls (e.g., deadlocks and race-conditions). Because of this, ensemble-centric concurrency abstractions are now growing in popularity. In this style of programming, programmers are able to specify complex distributed computations from the perspective of entire collections of computing nodes as a whole (e.g., MapReduce, Google Web Tool-kit, choreographic programming), making implementations of distributed computations more concise and even making large classes of concurrency pitfalls syntactically impossible. However, programming distributed computations in this style typically require programmers to adopt a new perspective of computation. At times, they are overly restrictive and hence not applicable to a wider range of distributed coordination problems.  Our work centers on developing a concurrency abstraction to overcome the above challenges, by (1) providing a high-level ensemble-centric model of coordinating distributed computations, and (2) offering a clean and intuitive integration with traditional main-stream imperative programming languages. This framework as a whole, orthogonally combines a high-level concurrency abstraction together with established lower-level main-stream programming methodologies, maintaining a clean separation between the ensemble-centric concurrency model and the underlying sequential computation model, yet allowing them to interact with each other in a symbiotic manner. The benefit of this separation is twofold: first, a clear distinction of the elements of the coordination model from the computation model helps lower the learning curve of this new programming framework. Hence, developers familiar with the underlying main-stream computation model can incrementally build their technical understanding of the framework, by focusing solely"
pub.1164208616,Application of Machine Learning to In2O3-Based Semiconducting Oxide Gas Sensors for High-Performance Gas Discrimination Against Ambient Humidity and Temperature Variations,"Since the advent of the 4th industrial revolution characteristic of smart living standards, physical and/or chemical sensors have been gaining their academic/industrial interests in association with cloud-based data management, artificial intelligence and big data thanks to ever-increasing computing power and communication technology. In particular, machine learning-operated sensor networks are advancing to offer predictive, prescriptive, and even deductive analytics, overcoming basic descriptive functions. Regardless of the type of sensor, i.e., physical or chemical, homogeneously and/or heterogeneously configured sensor arrays can provide physical status and chemical information that have been impossible to achieve using single-mode sensors alone. This teaming of technology has opened up unprecedented applications that may be possible through sensor network implementation. Electronic nose with semiconducting gas sensors array can be regarded as a promising platform to find new functionality in the recognition of smells and odors through machine learning. Oxide semiconductor gas sensors with high sensitivity, simple structure, rapid response speed, excellent reversibility and facile integration have been widely employed to detect harmful, explosive, and toxic gases but the simple gas sensing mechanism involving charge transfer between the gas and oxide surfaces often leads to a lack of gas selectivity, hampering gas recognition. The machine learning ecosystem is capable of solving the pre-existing drawbacks encountered in chemical sensor domains. However, the recognition of gases under variations in ambient humidity and temperature has barely been investigated, and most studies have focused on the compensation of sensor signals using humidity and temperature sensor. Gas recognition under various humidity conditions by machine learning without the assistance of humidity sensors has never been achieved. Five In2O3-based semiconducting metal oxide (SMO) gas sensors were combined in the form of sensor arrays with machine learning methodologies with the aim to detecting and discriminating indoor volatile organic compounds (VOCs) such as benzene, xylene, toluene, formaldehyde, and ethanol against humidity and/or temperature variations. The SMO gas sensor performance was evaluated using principal component analysis (PCA) and neural network-based classification in terms of the gas sensor data type/amount, neural network algorithms, sensor combinations, and environmental factors. The PCA analyses revealed the limitations on the discrimination of VOCs under temperature- and/or humidity-interfered gas sensing environments. Gas detection/discrimination could be improved significantly by using neural network-based algorithms, i.e., artificial neural networks (ANNs), deep neural networks (DNNs), and 1-dimensional convolutional neural networks (1D CNNs). The neural network algorithm prediction based on the entire gas sensing/purge transient data outperforms de"
pub.1164262117,Prediction of Sensor Data in a Greenhouse for Cultivation of Paprika Plants Using a Stacking Ensemble for Smart Farms,"Ensuring food security has become of paramount importance due to the rising global population. In particular, the agriculture sector in South Korea faces several challenges such as an aging farming population and a decline in the labor force. These issues have led to the recognition of smart farms as a potential solution. In South Korea, the smart farm is divided into three generations. The first generation primarily concentrates on monitoring and controlling precise cultivation environments by leveraging information and communication technologies (ICT). This is aimed at enhancing convenience for farmers. Moving on to the second generation, it takes advantage of big data and artificial intelligence (AI) to achieve improved productivity. This is achieved through precise cultivation management and automated control of various farming processes. The most advanced level is the 3rd generation, which represents an intelligent robotic farm. In this stage, the entire farming process is autonomously managed without the need for human intervention. This is made possible through energy management systems and the use of robots for various farm operations. However, in the current Korean context, the adoption of smart farms is primarily limited to the first generation, resulting in the limited utilization of advanced technologies such as AI, big data, and cloud computing. Therefore, this research aims to develop the second generation of smart farms within the first generation smart farm environment. To accomplish this, data was collected from nine sensors spanning the period between 20 June to 30 September. Following that, we conducted kernel density estimation analysis, data analysis, and correlation heatmap analysis based on the collected data. Subsequently, we utilized LSTM, BI-LSTM, and GRU as base models to construct a stacking ensemble model. To assess the performance of the proposed model based on the analyzed results, we utilized LSTM, BI-LSTM, and GRU as the existing models. As a result, the stacking ensemble model outperformed LSTM, BI-LSTM, and GRU in all performance metrics for predicting one of the sensor data variables, air temperature. However, this study collected nine sensor data over a relatively short period of three months. Therefore, there is a limitation in terms of considering the long-term data collection and analysis that accounts for the unique seasonal characteristics of Korea. Additionally, the challenge of including various environmental factors influencing crops beyond the nine sensors and conducting experiments in diverse cultivation environments with different crops for model generalization remains. In the future, we plan to address these limitations by extending the data collection period, acquiring diverse additional sensor data, and conducting further research that considers various environmental variables."
pub.1175657991,Cyber-XAI-Block: an end-to-end cyber threat detection & fl-based risk assessment framework for iot enabled smart organization using xai and blockchain technologies,"The growing integration of the Internet of Things (IoT) in smart organizations is increasing the vulnerability of cyber threats, necessitating advanced frameworks for effective threat detection and risk assessment. Existing works provide achievable results but lack effective solutions, such as detecting Social Engineering Attacks (SEA). Using Deep Learning (DL) and Machine Learning (ML) methods whereas they are limited to validating user behaviors. Like high false positive rates, attack reoccurrence, and increases in numerous attacks. To overcome this problem, we use explainable (DL) techniques to increase cyber security in an IoT-enabled smart organization environment. This paper firstly, implements Capsule Network (CapsNet) to process employee fingerprints and blink patterns. Secondly, the Quantum Key Secure Communication Protocol (QKSCP) was also used to decrease communication channel vulnerabilities like Man In The Middle (MITM) and reply attacks. After Dual Q Network-based Asynchronous Advantage Actor-Critic algorithm DQN-A3C algorithm detects and prevents attacks. Thirdly, employed the explainable DQN-A3C model and the Siamese Inter Lingual Transformer (SILT) transformer for natural language explanations to boost social engineering security by ensuring the Artificial Intelligence (AI) model and human trustworthiness. After, we built a Hopping Intrusion Detection & Prevention System (IDS/IPS) using an explainable Harmonized Google Net (HGN) model with SHAP and SILT explanations to appropriately categorize dangerous external traffic flows. Finally, to improve global, cyberattack comprehension, we created a Federated Learning (FL)-based knowledge-sharing mechanism between Cyber Threat Repository (CTR) and cloud servers, known as global risk assessment. To evaluate the suggested approach, the new method is compared to the ones that already exist in terms of malicious traffic (65 bytes/sec), detection rate (97%), false positive rate (45%), prevention accuracy (98%), end-to-end response time (97 s), recall (96%), false negative rate (42%) and resource consumption (41). Our strategy's performance is examined using numerical analysis, and the results demonstrate that it outperforms other methods in all metrics."
pub.1140114991,Digital Oil Field; The NPDC Experience,"Abstract
                  This paper presents an overview of the implementation of a Digital Oilfield (DOF) system for the real-time management of the Oredo field in OML 111.
                  The Oredo field is predominantly a retrograde condensate field with a few relatively small oil reservoirs. The field operating philosophy involves the dual objective of maximizing condensate production and meeting the daily contractual gas quantities which requires wells to be controlled and routed such that the dual objectives are met.
                  An Integrated Asset Model (IAM) (or an Integrated Production System Model) was built with the objective of providing a mathematical basis for meeting the field's objective. The IAM, combined with a Model Management and version control tool, a workflow orchestration and automation engine, A robust data-management module, an advanced visualization and collaboration environment and an analytics library and engine created the Oredo Digital Oil Field (DOF).
                  The Digital Oilfield is a real-time digital representation of a field on a computer which replicates the behavior of the field. This virtual field gives the engineer all the information required to make quick, sound and rational field management decisions with models, workflows, and intelligently filtered data within a multi-disciplinary organization of diverse capabilities and engineering skill sets.
                  The creation of the DOF involved 4 major steps;
                  DATA GATHERING considered as the most critical in such engineering projects as it helps to set the limits of what the model can achieve and cut expectations. ENGINEERING MODEL REVIEW, UPDATE AND BENCHMARKING; Majorly involved engineering models review and update, real-time data historian deployment etc. SYSTEM PRECONFIGURATION AND DEPLOYMENT; Developed the DOF system architecture and the engineering workflow setup. POST DEPLOYMENT REVIEW AND UPDATE; Currently ongoing till date, this involves after action reviews, updates and resolution of challenges of the DOF, capability development by the operator and optimizing the system for improved performance.
                  The DOF system in the Oredo field has made it possible to integrate, automate and streamline the execution of field management tasks and has significantly reduced the decision-making turnaround time. Operational and field management decisions can now be made within minutes rather than weeks or months. The gains and benefits cuts across the entire production value chain from improved operational safety to operational efficiency and cost savings, real-time production surveillance, optimized production, early problem detection, improved Safety, Organizational/Cross-discipline collaboration, data Centralization and Efficiency.
                  The DOF system did not come without its peculiar challenges observed both at the planning, execution and post evaluation stages which includes selection of an "
pub.1042065782,Siberia Integrated Regional Study: multidisciplinary investigations of the dynamic relationship between the Siberian environment and global climate change,"This is an editorial overview of the Siberia Integrated Regional Study (SIRS), which is a large-scale investigation of ongoing and future environmental change in Siberia and its relationship to global processes, approaches, existing challenges and future direction. Introduction The SIRS is a mega-project within the Northern Eurasia Earth Science Partnership Initiative (NEESPI), which coordinates interdisciplinary, national and international activities in Northern Eurasia that follow the Earth System Science Program (ESSP) approach. Under the direction of the International Geosphere–Biosphere Program (IGBP), SIRS is one of the Integrated Regional Studies (IRS) that aims to investigate environmental change in Siberia under the current environment of global change, and the potential impact on Earth system dynamics [1]. The regions of interest are those that may function as 'choke or switch points' for the global Earth system, where changes in regional biophysical, biogeochemical and anthropogenic components may have significant consequences for the Earth system at the global scale. Siberia is a large and significant region that may compel change [2]. Regional consequences of global warming (e.g. anomalous increases in cold season temperatures) have already been documented for Siberia [3]. This result is also supported by climate modeling results for the 20th–22nd centuries [4]. Future climatic change threatens Siberia with the shift of permafrost boundaries northward, dramatic changes in land cover (redistribution among boreal forest, wetlands, tundra, and steppe zones often precipitated by fire regime change) and the entire hydrological regime of the territory [5–8]. These processes feed back to and influence climate dynamics through the exchange of energy, water, greenhouse gases and aerosols [9]. Even though there have been a handful of national and international projects focused on the Siberian environment, scientists have minimal knowledge about the processes that control change in this understudied region, particularly those concerning the primary components that influence regional climate (i.e. cloud cover, precipitation) and responses and feedbacks to and from terrestrial and aquatic systems. This provides a strong impetus for the SIRS project. SIRS was initiated at a boreal forest conference in Krasnoyarsk in 2002 under the auspices of the IGBP and ESSP regional strategy by Will Steffen (IGBP) and the Siberian Branch of the Russian Academy of Sciences (SB RAS). Russian and foreign scientific activities continued under the Siberian Center for Environmental Research and Training (SCERT) in 2003. In 2005, the Siberian Branch of the Russian National Committee (SB RNC) for IGBP endorsed these activities and recommended investigations focus on four major themes:  quantification of the terrestrial biota full greenhouse gas budget, with a focus on the exchange between biota and atmosphere; monitoring and modeling of regional climate change impacts;"
pub.1139850814,ASTRALE: a citizen-driven project for air quality and all-sky monitoring,"Our team is part of the Italian non-profit association AstronomiAmo (https://www.astronomiamo.it/Home), which aims at disseminating astronomy and promoting respect for the environment. The All-Sky TRacking, ALerts and Environment (ASTRALE) project merges both purposes, providing a beautiful view of the night sky, an automatic detection of meteors and an evaluation of the air quality index.  Specifically, ASTRALE (https://astrale.astronomiamo.it/) consists of several devices installed across Italy, which collect data in the form of images or numbers, make a first processing and send them to a central server for storage, analysis and reporting. Each peripheral device (see Figure 1) comes with two installation possibilities:ASTRALE Meteor: for those who have a good sky and visibility in all directions: the system is equipped with a webcam for the sky and sensors for the air. ASTRALE Air: for those who lack a good sky, due to light pollution and / or visibility, but wish to keep under control the quality of the air they breathe. What makes ASTRALE unique?Automatic detection system: An automatic analysis tool has been developed to identify each transient event. To date, nearly 2000 events have been visually inspected and classified by each volunteer citizen. This amount of data is feeding a machine-learning algorithm that will soon provide a real-time classification of each image. Citizen science is the driving engine to make this ambitious goal possible. Low-cost: As a non-profit association, we are fully committed to keep the cost as low as possible, while ensuring a high-quality performance. The expected cost for ASTRALE Meteor is 350 € maximum, while for ASTRALE Air it is about 180 €. This cost simply matches the reimbursement for all hardware components purchased from the market. Each device is then assembled for free by our team experts. No earning is present, neither at personal nor at association level. User-friendly: our system can be distributed across Europe and easily installed at home, with no need for maintenance. The only requirements are Internet connection, a socket and an outdoor space. Our team provides full-time assistance to the user.  Interdisciplinary: this project has blossomed from the close interaction between amateur astronomers, professionals, outreach providers, who are all part of AstronomiAmo. The ASTRALE project encompasses complementary fields of expertise, such as astronomy, environmental science, software engineering, science communication and citizen science. For this reason, ASTRALE aims at engaging the widest possible audience, driven by a common passion for science and environmental awareness.  How does it work?Images are captured with a Raspberry HQ Cam with a 12.3 Mpixel Sony IMX477 and a CS-mount with a 2.5mm fisheye lens. A dew heater is used to keep the lens and the dome free of moisture. The camera is coupled with a Raspberry PI4 2GB single board computer with a Debian based Linux distribution. The control "
pub.1134711704,"House of Samarin, a damaged rock carving station. Documentation, analysis and diagnosis in Los Llanos de Ifara, Granadilla, Tenerife","In the archaeology of the Canary Islands (Spain), there are many studies based on the usage of new technologies to contribute to the identification and description of rock art engravings through high-resolution digital models ( Martín, 2005 ; Martín, Velasco, González & Ramírez, 2007; Senén & Cuenca, 2016 ; Navarro & Cancel, 2019 ). This paper is supported by these documentation techniques and digital analysis in order to deepen into the characterization of the damaged rock art station Casa del Samarín (House of Samarín), or Tagoro del Rey, in Los Llanos de Ifara, south of the island of Tenerife (Figs. 1). Twenty-one panels conserved in situ were documented (Fig. 6). Geometric-linear, geometric with an oval and rectangular trend and figurative ones can be distinguished. The blocks  [1] that compose the engravings station belong to a rocky basalt outcrop, to which other free-standing blocks are attached, forming a circle. The shape that describes this set of blocks is defined as a ""cabin"" or circular-shaped structure. This set of engravings, made on a basalt rocky outcrop with a planar factory, show a tendency to suffer from exfoliation and are affected by internal stresses. The intrinsic characteristics of this stone support, together with their exposure to anthropic actions and strong insolation, condition its fragility, with the risk of losing part of the representations that it houses. Given the threat posed by its gradual deterioration, we seek to ensure its digital preservation through precision three-dimensional (3D) records, the engravings inventory, the record of their conservation state and the understanding of the degradation processes that are affecting the outcrop. What has been explained will be addressed quarterly, to observe the evolution of any material changes every three months. The registration work consisted of taking four photogrammetric surveys in eight months; the surveys were georeferenced by means of a centimetric Global Navigation Satellite System (GNSS) and a total station. Structure from Motion (SfM) technology enabled the researchers to generate high-precision 3D models in an affordable way, not only in terms of cost but also ease of use. Digital copies with Geographic Information System GIS technology were extracted from them, being exportable in shapefile format (Fig. 7). As regards the documentation of existing pathologies, assuming standardized lexicon and classification criteria ( IPCE, 2013 ), together with a rigorous information systematization, was key for achieving agile handling of the data collected and for facilitating monitoring tasks (Fig. 8). Damage maps were created for collecting the location and scope of the alterations. The complex volumetry of the outcrop and the varied orientation of the panels marked the need to resort to 3D editing so that all their faces could be properly registered (Fig. 10). This project was performed with a 3D design program, Blender®.  Thanks to an imaging analysis process,"
pub.1162752774,A One‐Dimensional Volcanic Plume Model for Predicting Ash Aggregation,"Abstract During explosive volcanic eruptions, volcanic ash is ejected into the atmosphere, impacting aircraft safety and downwind communities. These volcanic clouds tend to be dominated by fine ash (<63 μm in diameter), permitting transport over hundreds to thousands of kilometers. However, field observations show that much of this fine ash aggregates into clusters or pellets with faster settling velocities than individual particles. Models of ash transport and deposition require an understanding of aggregation processes, which depend on factors like moisture content and local particle collision rates. In this study, we develop a Plume Model for Aggregate Prediction, a one‐dimensional (1D) volcanic plume model that predicts the plume rise height, concentration of water phases, and size distribution of resulting ash aggregates from a set of eruption source parameters. The plume model uses a control volume approach to solve mass, momentum, and energy equations along the direction of the plume axis. The aggregation equation is solved using a fixed pivot technique and incorporates a sticking efficiency model developed from analog laboratory experiments of particle aggregation within a novel turbulence tower. When applied to the 2009 eruption of Redoubt Volcano, Alaska, the 1D model predicts that the majority of the plume is over‐saturated with water, leading to a high rate of aggregation. Although the mean grain size of the computed Redoubt aggregates is larger than the measured deposits, with a peak at 1 mm rather than 500 μm, the present results provide a quantitative estimate for the magnitude of aggregation in an eruption.
Plain Language Summary Volcanic eruptions produce significant quantities of ash, which can be hazardous to aircraft and the environment. Although small ash particles can remain airborne for long distances, many fall out earlier by combining into larger clusters. The clustering process depends on many factors, notably the moisture and the turbulence within the plume. We develop a new predictive tool that models the clustering process in a volcanic plume, which includes improved estimates of the clustering behavior from new experiments with a laboratory‐scale turbulence tower. We use this tool to study a 2009 eruption of Redoubt Volcano, which shows that ash particles rapidly cluster due to water within the plume. The model predicts larger clusters than observed in the field, but the estimates are improved from existing models that neglect this effect.
Key Points    A new 1D plume model predicts the evolution of ash size distribution due to aggregation   Analysis incorporates a new sticking efficiency model based on novel turbulence tower experiments   Application to a 2009 eruption of Redoubt Volcano displays a similar distribution peak and width to field measurements, albeit shifted larger   "
pub.1141897149,POST-COVID-19HEALTHCARE:TRANSITIONTOANEW NORMAL,"The COVID-19 pandemic has challenged the health care system to face extraordinary circumstances. These challenges bring forth a new era with a certain high point like the transition to the fusion of in person and digital health practice framework. Post COVID-19 there is a lot of sludge in health care, inthe form of administrative processes and requirements that slow down the core activities of providing care. COVID-19 crisis is used as an opportunity to reduce administrative burdens which most of the times made a primary care physician feel demoralized and burned out'. The focus of healthcare has shifted from hospitals to homes using telemedicine technologies which enabled virtual visits and remote care delivery. COVID-19 signified the telemedicine to be an essential component of healthcare delivery. It proved to an effective and safe way of treatment and avoiding nosocomial infections’. Previously, telemedicine was not considered to be a normal consultation method, nowadays it’s part of the normal lives of the patients as well as health care providers. The other big advantage of this great tool is that one can consult doctors even from very remote areas, which is impossible to consult physically. Different technological advancement like Artificial Intelligence-based diagnostics, cloud-based storage of medical records and integration of information in and outside hospitals were explored and adopted in the COVID-19 pandemic. Data is the key to advance research and refine health care process and outcomes for the COVID-19 patients. The digitalization of the health care system can provide significant benefit’. The phenomenon of digitalization of healthcare system is especially benefited for the developing world, because, in the resource-limited environment, a lot of cost and resources can be saved by opting paperless systems. When people were locked down, social media was pivotal in creating awareness and educating people in a short period*. There was a lot of stuff regarding COVID-19 pandemic on social media and most interestingly it was portrayedin a way that one could not be able to neglect the content. Consequently, intentionally or unintentionally many of the users opted some of the measures for prevention of the deadlyinfection. The pandemic has shown us that countries with the more robust public health system, primary care services and a healthier population perform well in the fight against the pandemic. Safeguard of voluntary and community organization is essential’. At the same time the countries with fewer resources and with limited measures for controlling massive disasters, in the form of this pandemic, got exposed. This is an alarming situation for global organizations like the World Health Organization and other health-related global leaders to work together for making sure that health resources be equally provided to all the countries across the globe. The COVID-19 pandemic educated the underdeveloped countries regarding infection con"
pub.1155664991,The Inclusion of RPA in the Digital Transformation,"Over time, technological evolution has had an impact on all business organizations where the need to be able to constantly innovate in all the internal processes of your company is created. Today the commercial market is very competitive and causes companies to see a deeper horizon and adopt certain methodologies and new processes such as digital transformation.According to the consulting firm Mckinzey, due to the pandemic, it has forced many entities to advance this adoption of new technologies, including the optimization of some manual processes, which have to be automated, but before talking about these technologies, we will verify some stumbling blocks that the digital transformation has that we detail below. according to (Weaver, 2021): 1)      The approval of the directors: This is the biggest obstacle of the 3 points because most probably the directors in their planning have invested technological tools that due to pandemic issues, these became obsolete, but according to studies at the beginning of the year 2020 ( KPMG, 2020), “67% of executives in the United States expressed a great concern regarding the migration of all their businesses to the cloud, however, this quickly turned from being a concern to a necessity where as a result , 70% of managers have an affirmation that the new digital models had a great advance as a result of the pandemic”. 2)      Resistance to change in human resources: this concept is not new in the market, because the lack of communication between the project leaders and the areas involved is not very effective, resulting in an environment of uncertainty and doubts about what that is being done. This exclusion of the staff and not making them part of our project, makes many people resist change, being a great obstacle for our technological project. It is always essential to be aware that the new technologies that are being implemented will most likely be used by these users, but it should also be considered that our users are used to executing and carrying out processes that are well known and it is the change in their routine that generates some fear or discomfort.   3)      Reduce dependence on traditional tools: for some authors, this point is the continuity of the previous one, since users know how to use the computer tools they have used for years and a new update means for them to start over from scratch. Migrating to new technological tools must mean that for the company it will bring many benefits, as well as improve the quality of life of its users. Contrary to the previous points, the benefits obtained in the digital transformation can be indicated according to (CMC, 2020):1) Faster response to changes.2) Improve the customer experience.3) Make better decisions.4) New business opportunities.5) Reduce risks.6) Increases the productivity of the company.7) It favors collaborative work.8) Promotes innovation.9) Advantage over the competition.10) Decentralization of work. The digital transformation combines"
pub.1181139209,Malware Detection using Deep Learning (DL),"The attack that occurred recently involved the utilization of malicious software, commonly referred to as malware, along with advanced techniques such as machine learning, specifically deep learning, code transformation, and polymorphism. This makes it harder for cyber experts to detect malware using traditional analysis methods. In view of the low accuracy and high false positive rate of traditional malware detection methods, this research proposes a fine-tuned deep learning model with a novel dataset that is compared with ANN, Support Vector Machine (SVM), random forest (RF), K-Nearest Neighbourhood (KNN) classifiers. Converting a malware code into an image could allow users to effectively identify the presence of malware, even if the original code is modified by the creator. This is due to the fact that the attributes of images remain unchanged, allowing for reliable identification. So, researchers used deep learning technology to detect malware, like detecting malaria from red blood cells. The deep learning model found that a more detailed analysis of malware data sets, focusing on RGB and greyscale images, is needed. These data sets currently rely on publicly available data, but the accuracy of the traditional model could be a lot higher. It also produces many false positive results. The main goal is to create a new data set and model using malware images to identify and categorize malware using deep learning without relying on existing image detection and transfer learning models. The researcher adjusted different hyper parameters, like the number of neurons, filters, stride, hidden units, layers, learning rate, batch size, activation function, optimizer, and epochs. Identifying and correcting issues in models, improving their clarity, stability, and fairness, as well as debugging and monitoring them, is a challenging task. To eliminate this obstacle, assess the model's behaviour and performance by employing various methods such as logging, profiling, testing, visualization, and model clarity. To overcome the challenges posed by the electricity shutdown, we utilized Google's cloud-based GPU and Python 3.7 language to conduct the experiment and train the model. Kali Linux is an operating system that can automatically encrypt file systems and has a lower chance of crashing the system due to malware in a virtual sandbox. The researcher used techniques like early stopping and cross-validation to prevent over fitting and assess generalization in addition to monitoring and evaluating the model's behaviour and performance. The researcher used different methods like L1 and L2 regularization, dropout, and batch normalization to improve the model's performance and avoid over fitting. The binary portable executable (PE) malware dataset is collected from Kaggle, Malimg, Virusshare, Malvis, MS Big2015, and VX-underground and finally converted to greyscale and RGB images to create a novel dataset to fill the lack of image dataset. The raw dataset was the"
pub.1119837691,"The Internet of Everything, Advances, Challenges and Applications","In the past, mobile ad hoc networks (MANET) have emerged due to their wide applicability in the field of disaster recovery, police operations, crowd management, emergency and military operations such as battle fields. Furthermore, through the advent of sensor-enabled intelligent mobile devices, MANETs have become a crucial element in the framework of Internet of things (IoT) and smart city developments. MANET is a decentralized system consisting of mobile nodes capable of forming a self-configurable, infrastructure-less and continuously evolving network. The lack of infrastructure empowers each mobile node to accomplish routing operation to confirm connectivity in MANET. Therefore, routing in MANET is an interesting operation. Most of the routing protocols used MANET as the basic broadcasting mechanism for flooding. In flooding, in order to find the route from source to destination, the packet is broadcasted to the neighboring nodes which in turn broadcast it to its neighboring nodes and this process sustains until the packet reaches to the destination. This neighborhood processing in MANET leads to broadcast storm problem. Traditional broadcast schemes have been presented to avoid broadcast storms by inhibiting some rebroadcasts. Another issue is the link failures caused by node mobility and energy exhaustion. In this chapter, we introduce a novel energy-efficient counter-based scheme and extend the scheme to reflect the mobility of node into an account to address these network challenges of MANET. In the proposed scheme, the decision of broadcasting is taken based on neighborhood, mobility and the energy of mobile nodes. The simulation results reveal that proposed schemes decrease the packet loss, the latency time and achieve lower energy consumption, better packet delivery and throughput when compared to ad hoc on-demand distance vector and hybrid counter-based broadcast routing protocol. Several biometric partial face recognition researches have been performed by many scientists. In this chapter, a novel technique has been recommended, which acknowledges students face to speed up the attendance procedures in a classroom. Students’ partial pictures have been used to prepare the image set, and preprocessed different partial faces to gray-level images. Initially, the technique like discrete wavelet transform has been used to obtain local features. Afterward, the effectiveness of the approach has been improved by employing image fusion with the averaging method. The fusion technique along with the correlation technique was executed to the contrast between the fused images, and the test images were selected from the entire image set. Results revealed practically 90% of the instances that were matched. The acceptance rate on an overall analysis has been found to prevail between 86.67% and 87.5%. The wide variety of Internet of thing (IoT) applications demands a secure and efficient communication channel that resists against a variety of modern atta"
pub.1083517186,E&P Notes (January 2017),"E&P Notes Hot Permian Play Gets Panel Focus Oil prices, fracturing fleet, staffing, and technology among topics aired. Joel Parshall, JPT Features Editor The successes and challenges of producers in the United States’ most active oil play received a wide-ranging discussion by three panelists recently at an SPE Gulf Coast Section meeting in Houston on the state of the Permian Basin. Exclusively following a question-and-answer format, the nearly hour-long program last month covered oil prices, the fracturing fleet, staffing, technology, reservoir issues, and other topics related to the basin that covers west Texas and southeastern New Mexico. “It’s exciting to see the Permian come around and actually be the hottest play and the hottest area probably in the world,” said Billy Smith, technology director for North America at Halliburton. “The encouraging thing about the Permian, at least with the operators I’ve spoken with, is that at USD 50[/bbl] oil, a lot of the play is very valid and very economic.” Some Permian plays are economic for their operators at USD 40/bbl oil, he said.   Oklahoma Official: Progress Being Made, but Induced Seismicity Will Not Stop Anytime Soon Trent Jacobs, JPT Senior Technology Writer One of Oklahoma’s top government officials announced recently that it could be many more months before the full scope of the state’s regulatory response plan for induced seismicity is proven effective. Oklahoma is a top-five oil and gas producing state in the US that has also seen a nearly 4,000% spike in earthquakes since 2009, making it among the most seismically active regions in the country. Scientists and industry experts have concluded that the earthquakes are instances of induced seismicity brought about by the injection of produced wastewater into a fault-connected layer of rock called the Arbuckle formation. Michael Teague, secretary of energy and environment for Oklahoma, explained that even though the state has ordered hundreds of disposal wells to cease operations or cut back their injection volumes, the earthquakes are slowing down but growing in strength. He suggested that it will take quite a bit of time for the built-up pressures inside the Arbuckle thought to be triggering the quakes to dissipate.   Beware of Bottlenecks Stephen Rassenfoss, JPT Emerging Technology Senior Editor Oil companies that have slashed the break-even cost of producing oil from shale plays now must figure out how to hold on to those hard-won gains. US producers can profitably produce oil from these difficult formations at prices that are 50% lower than they were during the boom, according to Rystad Energy. But roughly half of those gains are at risk as drilling activity rises. “Lower unit prices of service companies are a major reason for the drop,” said Jon Duesund, senior project manager for Rystad, during a recent briefing in Houston. Discounts squeezed out of suppliers are considered “nonsustainable” because prices will rise as demand rises, allowi"
pub.1139851161,Ulysses spacecraft data revisited: Detection of cometary meteoroid streams by following in situ dust impacts,"Cometary meteoroid streams (also referred to as trails) exist along the orbits of comets, forming fine structures of the interplanetary dust cloud. The streams consist predominantly of the largest cometary particles (with sizes of approximately (100 micrometer to 1 cm) which are ejected at low speeds and remain very close to the comet orbit for several revolutions around the Sun. The Interplanetary Meteoroid Environment for eXploration (IMEX) dust streams in space model (Soja et al., Astronomy & Astrophysics, 2015) is a universal model that simulates recently created cometary dust streams in the inner solar system, developed under ESA contract. IMEX is a physical model for dust dynamics and follows the orbital evolution of the streams of 420 comets. Particles are emitted when the comet is in the inner solar system, taking into account comet apparitions between the years 1700 and 2080. The dust ejection is described by an emission model, dust production rate and mass distribution covering the mass range from 10^-8 kg to 10^-2 kg (approximately corresponding to 100 micrometer to 1 cm particles). The dust production is calculated from the comet's absolute magnitude, the observed water production rate and dust-to-gas ratio. For each emitted particle, the trajectory is integrated individually including solar gravity, planetary perturbations as well as solar radiation pressure and Poynting-Robertson drag. The model calculates dust number density, flux and  velocity.We apply the IMEX model to study comet stream traverses by the Ulysses spacecraft. Ulysses was launched in 1990 and, after a Jupiter swing-by in 1992, became the first interplanetary spacecraft orbiting the Sun on a highly inclined  trajectory with an inclination of 80 degrees. The spacecraft was equipped with an impact ionization dust detector which provided the longest  data set of continuous in situ dust measurements in interplanetary space existing to date, covering 17 years  from 1990 to 2007. In addition to the interplanetary dust complex, several dust populations were investigated with the Ulysses dust instrument in the past: interstellar dust sweeping through our solar system, streams of approximately 10 nanometer-sized dust particles emanating from Jupiter's volcanically active moon Io, as well as sub-micrometer-sized particles driven away from the Sun by solar radiation pressure (so-called beta particles). Here we study the detection conditions for cometary meteoroid streams with the dust detector on board the Ulysses spacecraft and present first results from our attempt to identify cometary stream particles in the measured dust data set. Acknowledgements: The IMEX Dust Streams in Space model was developed under ESA funding (contract 4000106316/12/NL/AF - IMEX)."
pub.1096954035,OBS illumination: primary problems and mirror solutions," Abstract The world's demand for energy is accelerating, while its reserves of energy are diminishing. Producers are compelled to explore and produce oil and gas in more challenging environments and to maximize recovery in existing reservoirs. New seismic exploration technology has always been a key to success. One such new technology is Ocean Bottom Station (OBS) nodes. Since the cost of OBS deployment is high, a practical and relatively economical geometry for OBS is a sparse grid of nodes and a dense grid of shots. However, the sparse node geometry provides poor illumination, especially of reflectors whose depth under the seabed is less than the node interval. Fortunately, there is a good solution for this problem: the mirror imaging method. The sea surface acts as an acoustic convex mirror reflecting the image of subsurface structure. This mirror image offers wider illumination than a conventional image derived from primary reflections. We present the mirror imaging method and compare conventional and mirror imaging results from OBS data recorded in the North Sea, the Norwegian Sea and offshore West Africa. We then discuss the reasons why mirror imaging is superior to the conventional image of the primaries.  Introduction Ocean Bottom Station (OBS) nodes (Berg et al., 1994; Ronen et al., 2003; Amal et al., 2005; Docherty et al., 2005; Granger et al., 2005) are an emerging seismic exploration technology. Surface-towed streamers provide excellent seismic data for exploration, development and production monitoring. However, streamers have limitations and this motivates a quest for alternative technologies. When using streamers, obstacles such as production platforms require undershooting and the data lack near offsets and have anomalous azimuth distributions. OBS nodes are much less sensitive to obstacles and allow more complete coverage. When deployed by a Remotely-Operated Vehicle (ROV), they can be placed very accurately, right up to, or even under, production installations. Obstacles are not the only motivation for OBS technology. Even in the absence of obstacles, OBS nodes facilitate wide-azimuth geometries, important for imaging structures under complex overburdens such as salt environments. In particular, by moving the receivers to the seabed and recording a dense shooting grid on the sea surface, we can create a dataset that is suitable for wave-equation migration, is well-populated in azimuth and offset, and provides optimal subsurface illumination; conventional towed-streamer acquisition cannot provide this type of dataset because of the constraints imposed by the fixed source-receiver geometry. Indeed, while the Wide-Azimuth Towed-Streamer (WATS) method is becoming popular, and can also provide data suitable for wave-equation migration (Threadgold et al., 2006), it requires separate source vessels as well as repeated shot lines with different streamer locations. Such effort comes at a cost, and, for small areas (up to about 400 sq.km)"
pub.1146372169,"Tokenization, blockchain and web 3.0 technologies as research objects in innovation management","The e-mail allegedly attributed to Satoshi Nakamoto (supposedly a pseudonym) was transmitted 14 years ago, describing the development of an electronic currency (Nakamoto, 2008). The design of this electronic currency represented the solution of the general Byzantine problem, a well-known problem in computing, which, in general terms, defines that one of the parts of a system can intentionally fail, and with that, make the entire network unavailable. Therefore, the premise is that part of the system is corrupt (Dolev et al., 1982). In the few lines of the email, Satoshi Nakamoto described such a solution and published an article with the details made available on the same date. The article describes how to transmit information within a chain of blocks that are: synchronized with date and time (time stamp); combined with code that depends on a previous block (hash code); can be validated with public and private key cryptography framework anonymously and decentrally; but highly resilient to any tampering attempt and with public record. The concept of digital currency, in this case Bitcoin, consisted at that time of a code or token resulting from encryption and that could be included in these blocks. Blocks registered definitively in the ledgers distributed along the blockchain network that could be traced. The digital framework developed by Satoshi Nakamoto, although it emerged to make Bitcoin viable as a digital currency, has been separated over the last 14 years. Blockchain can be understood as a decentralized communication technology that gave rise to a family of other technological structures of encrypted communication such as ecosystems, public blockchain, private blockchain and blockchain networks, mainly (Mazumdar & Ruj, 2022). Digital currencies, on the other hand, have also developed in variety and quantity, so much so that as we write this editorial there are over 10,000 digital currencies in operation. The total capitalization value of digital currencies rose from USD 18 billion at the beginning of 2017, surpassing USD 1.4 trillion by mid-2021 (Su et al., 2022). Currently, there is no technological impediment for companies to create their own digital currencies using a Bitcoin network or an Etherium network, for example, as well as many other networks available.Obviously, even today, there are technical challenges related, mainly, to the scalability of these networks and currencies. Bitcoin, when created, had a capacity of 7 transactions per second, currently, as we write this editorial, the transaction capacity of the Bitcoin network (BTS) is 14 transactions per second. The Etherium (ETH) network was born with a capacity of 20 transactions per second and currently has a capacity of 35 transactions per second. For comparison purposes, the VISA network has a capacity of 1700 transactions per second, which shows that there is still some way to make blockchain networks the new communication backbone, scalable for more mass uses (Chauhan & Pa"
pub.1139850777,Interstellar Probe: A Mission to Explore the Heliospheric Boundary and Interstellar Medium,"During its evolution, the Sun and its protective magnetic bubble – the heliosphere - has completed nearly twenty revolutions around the Galactic Core. During this “Solar Journey” it has plowed through widely different interstellar environments that have all shaped the system we live in today. The orders-of-magnitude differences in interstellar properties have had dramatic consequences for the penetration of interstellar material and have affected elemental and isotopic abundances, atmospheric evolution and perhaps even conditions for habitability. As far as we know, only some 60, 000 years ago, the Sun entered what we call the Local Interstellar Cloud (LIC), and in less than 1,900 years the Sun will be entering a very different interstellar environment that will continue to shape its evolution and fate.The Interstellar Probe is a pragmatic mission with a possible launch already in the next decade that would explore the heliospheric boundary and how it interacts with the Very Local Interstellar Medium (VLISM) to understand the current state along this Solar Journey and, ultimately understand where our home came from, and where we are going. During its 50-year nominal design life, it would go far beyond where the Voyager missions have gone, out to about 400 astronomical units (au) and likely survive out to 1000 au. Therefore, the Interstellar Probe mission would represent humanity’s first explicit step in to the galaxy and become NASA's boldest step in space exploration.When the Voyager missions traversed the heliospheric boundary with their very limited payload it became clear that we are faced with a whole new regime of space physics that is not only decisive for our own heliosphere, but also for understanding the physics of other astrospheres as well. Today we still do not understand the force that is upholding the magnetic shell (the heliosheath) around our heliosphere, or the mechanisms that shield the solar system from galactic cosmic rays, and many other mysteries. Once beyond where the furthest Voyager spacecraft will cease operations (likely at ~170 au), Interstellar Probe would step in to the unknown, traverse the hydrogen wall and the complex magnetic topology at the very edge of the Sun’s sphere of influence, and then directly sample for the first time the interstellar material that has made all of us. There, measurements of the unperturbed gas, plasma, and fields would allow accurate determination of the current state of the LIC and how it affects the global heliosphere. Measurements of unshielded interstellar dust and galactic cosmic rays would provide unprecedented information on stellar and galactic evolution. The physical processes that occur as the solar wind and magnetic field interact with VLISM would also provide the only directly measurable prototypes for understanding the astrospheres surrounding other stars that control the atmospheres and habitability of their exoplanets. All this newly acquired knowledge would then enable "
pub.1125581371,The Global Long-term Microwave Vegetation Optical Depth Climate Archive VODCA,"<p>Since the late 1970s, spaceborne microwave radiometers have been providing measurements of radiation emitted by the Earth’s surface. From these measurements it is possible to derive vegetation optical depth (VOD), a model-based indicator related to the density, biomass, and water content of vegetation. Because of its high temporal resolution and long availability, VOD can be used to monitor short- to long-term changes in vegetation. However, studying long-term VOD dynamics is generally hampered by the relatively short time span covered by the individual microwave sensors. This can potentially be overcome by merging multiple VOD products into a single climate data record. However, combining multiple sensors into a single product is challenging as systematic differences between input products like biases, different temporal and spatial resolutions and coverage need to be overcome.</p><p><br>Here, we present a new series of long-term VOD products, the VOD Climate Archive (VODCA; Moesinger et al., 2019). VODCA combines VOD retrievals that have been derived from multiple sensors (SSM/I, TMI, AMSR-E, Windsat and AMSR-2) using the Land Parameter Retrieval Model. We produce separate VOD products for microwave observations in different spectral bands, namely Ku-band (period 1987-2017), X-band (1997-2018) and C-band (2002-2018). In this way, our multi-band VOD products preserve the unique characteristics of each frequency with respect to the structural elements of the canopy. Our merging approach builds on an existing approach that is used to merge satellite products of surface soil moisture<sup>1,2</sup>. </p><p><br>The characteristics of VODCA are assessed for self-consistency and against other products. Using an autocorrelation analysis, we show that the merging of the multiple data sets successfully reduces the random error compared to the input data sets. Spatio-temporal patterns and anomalies of the merged products show consistency between frequencies and with Leaf Area Index observations from the MODIS instrument as well as with Vegetation Continuous Fields from the AVHRR instruments. Long-term trends in Ku-Band VODCA show that since 1987 there has been a decline in VOD in the tropics and in large parts of east-central and north Asia, while a substantial increase is observed in India, large parts of Australia, southern Africa, southeastern China and central north America. In summary, VODCA shows vast potential for monitoring spatial-temporal ecosystem changes as it is sensitive to vegetation water content and unaffected by cloud cover or high sun zenith angles. As such it complements existing long-term optical indices of greenness and leaf area. </p><p><sup>1</sup>Gruber, A., Scanlon, T., van der Schalie, R., Wagner, W., Dorigo, W. (2019) Evolution of the CCI Soil Moisture Climate Data Records and their underlying merging methodology. Earth System Science Data 11, 717-739. https://doi.org/10.5194/essd-11-717-2019</p><p><sup>2</sup>Dorigo, W.A., W"
pub.1173538234,The M-MATISSE mission: Mars Magnetosphere ATmosphere Ionosphere and Space weather SciencE. An ESA Medium class (M7) candidate in Phase-A.  ,"The “Mars Magnetosphere ATmosphere Ionosphere and Space-weather SciencE (M-MATISSE)” mission is an ESA Medium class (M7) candidate currently in Phase A study by the European Space Agency (ESA) (Figure 1). M-MATISSE’s main scientific goal is to unravel the complex and dynamic couplings of the Martian Magnetosphere, Ionosphere and Thermosphere (M-I-T coupling) with relation to the Solar Wind (i.e. space weather) and the lower atmosphere, and the processes leading to this coupling, which are highly entangled between several regions of the system (Figure 2). The M-I-T coupling controls the dissipation of incoming energy from the solar wind, and therefore, the evolution of Mars’ atmosphere and climate (including atmospheric escape, auroral processes, and incoming radiation). Moreover, understanding the behavior of Mars’ M-I-T system and of the chain of processes that control Space Weather and Space Climate at Mars, as well as the radiation environment, is essential for exploration as it leads to accurate Space Weather forecasts and, thus, prevents hazardous situations for spacecraft and humans.Figure 1: The “Mars Magnetosphere ATmosphere Ionosphere and Space-weather SciencE (M-MATISSE)” mission is an ESA Medium class (M7) candidate.Mission goals: The mission has three main goals:Characterising the global dynamics of the M-I-T coupling by unravelling its temporal and spatial variabilities. This will be done with simultaneous observations of the solar wind (energy input) and ionosphere-magnetosphere (energy sink), and also, via investigating the coupling of the mesosphere with the ionosphere and solar energetic particles. Characterising the Radiation environment, by determining how the M-I-T absorbs the energy that reaches the planet and forecasting near-real time planetary Space Weather Characterising the Ionosphere-lower atmosphere coupling, which is a region barely explored but essential for solar energetic particles related phenomena as well as for communications in the HF wavelengths. In addition, M-MATISSE will significantly contribute to understand Mars climate and the lower atmosphere as two remote instruments have dedicated instrumentation to monitor dust, clouds, and get temperature and density profiles from the surface up to about 50 km. Moreover, the heliophysics community will count with a full-package solar wind monitor at Mars’ distances, contributing to understand solar wind and solar transient propagation in the inner Solar System.Figure 2: Mars regions that M-MATISSE will focus to understand the spatial-temporal variability of the M-I-T system and its couplings from the surface to space.Type of Mission: M-MATISSE is one of the current three candidates in competition at ESA at the Medium-size opportunity in ESA's Science Programme from the call in December 2021. From 27 initial responses, ESA down-selected 5 missions in 2022, which went through a Phase 0 study. In that phase, ESA evaluated the expected science that could be achieved wit"
pub.1160516042,Research on the Ageing-Friendly Design of Smart Entertainment Products Based on the Perceived Affordances Perspective,"Abstract“During the 13th Five-Year Plan period, China’s ageing will continue to deepen, with 254 million people aged 60 and above at the end of 2019 [2]. The continued growth of the ageing population has put enormous pressure on the social security and elderly services sectors. The Party Central Committee, with Comrade Xi Jinping at its core, has taken a holistic approach, insisting on combining the response to population ageing with the promotion of economic and social development, and promoting the concerted development of the elderly care industry. It is developing a silver-haired economy, developing age-appropriate technologies and products, solving the difficulties of the elderly in using smart technologies, and fostering a new business model for smart ageing. Smart health, smart ageing and smart entertainment are created through the collaboration of intelligent technologies such as big data, cloud computing, internet and internet of things. Through intelligent, digital services, the traditional elderly care model is deepened. It provides an opportunity for the management of health and mind in old age.Entertainment is an essential activity in our lives [1], and it is not only for young people, but should also be used throughout the later years of the elderly. Entertainment products are the direction of choice for older people for leisure, but there are very few products designed for older people in the existing market. The size of the product, the layout of the interface and the guidance system can all have an impact on the use of entertainment products by older people, and if they are not designed properly, there are many hidden dangers that can make older people fearful of the product and the development of modern technology. This not only increases the burden on the elderly themselves, but also on the family’s children and society [3, 4].This study first takes into account the physiological, psychological and cognitive characteristics of the elderly group, and constructs the design of elderly entertainment products under the perspective of perceptual schemability. How to truly make the intelligent entertainment system centred on elderly users and provide humane and friendly products and services through the theory of perceptual schemability is the focus of this study. The project will combine questionnaires, interviews, observations and other user research tools and methods to conduct in-depth user research, and carry out user group segmentation, build different types of user role models, and explore the needs of older people for wisdom products and systems in their cognitive environment. In user experience design, perceived schemability is a very important theoretical perspective. When designing products, we should abandon some subjective assumptions and pay full attention to the five dimensions of users’ perceived physical performance, perceived cognitive performance, perceived control performance, perceived emotional performance and pe"
pub.1167294763,Expert assessments in decision making: risks and safety,"The monograph “Expert assessments in decision making: risks and safety” presents studies that address the problems of expert assessments and the formation of decisions based on expert opinions in some areas of human activity. The results of decisions based on such assessments have a direct impact on safety and the risks that may follow. The problems are considered in the following aspects. An innovative approach to the formation of a list of types of forensic examinations and expert specialties.It has been established that the existing classification of forensic examinations by branches of specific examination used in their conduct is outdated and does not meet modern requirements. This creates difficulties not only for forensic experts and persons wishing to become them, but also for the court, participants in the trial and all citizens interested in obtaining a forensic expert's opinion. A discrepancy has been established between the types of forensic examination and the specialties of experts for which the qualification of a forensic expert is assigned in departmental lists. It has been proposed to unify the interdepartmental approach to the classification of forensic examinations in order to avoid errors that could lead to illegal and unfounded court decisions. The issue of unification can be resolved by developing a general approach to the classification of types and types of forensic examinations, enshrined in an interdepartmental regulatory act, which should be based on the criteria of the general theory of forensic examination. The creation of a unified list of types of forensic examinations and their corresponding expert specialties can open up ways to solve many issues facing the expert community. In particular, the creation of a forensic medical examination office and its integration with the Unified Forensic Information and Telecommunication System, the creation of a modern unified list of types of forensic medical examinations and the corresponding expert specialties are relevant. Blood cell image recognition using texture and neural networks for leukemia diagnosis.Morphological analysis of blood cell images is usually performed manually by an expert, but this method has many disadvantages, including slow analysis, low accuracy, and the results depend on the skill of the operator. This reduces the chance of a correct diagnosis in detecting acute lymphoblastic leukemia, a potentially fatal blood cancer if left untreated. The study developed and presented an automated method for identifying and classifying leukocytes using microscopic images of peripheral blood smears. The proposed neural random threshold classifier achieved a recognition rate of 98.3% when the data was divided into 80% training set and 20% test set. The proposed system can be implemented as a computational tool to detect other diseases in which blood cells undergo changes, such as Covid-19. This will eliminate the subjective factor that is invariably inherent in the c"
pub.1131720031,The Convergence Effect: Real and Virtual Encounters in Augmented Reality Art,"Augmented Reality—The Liminal Zone     Within the larger context of the post-desktop technological philosophy and practice, an increasing number of efforts are directed towards finding solutions for integrating as close as possible virtual information into specific real environments; a short list of such endeavors include Wi-Fi connectivity, GPS-driven navigation, mobile phones, GIS (Geographic Information System), and various technological systems associated with what is loosely called locative, ubiquitous and pervasive computing. Augmented Reality (AR) is directly related to these technologies, although its visualization capabilities and the experience it provides assure it a particular place within this general trend. Indeed, AR stands out for its unique capacity (or ambition) to offer a seamless combination—or what I call here an effect of convergence—of the real scene perceived by the user with virtual information overlaid on that scene interactively and in real time. The augmented scene is perceived by the viewer through the use of different displays, the most common being the AR glasses (head-mounted display), video projections or monitors, and hand-held mobile devices such as smartphones or tablets, increasingly popular nowadays. One typical example of AR application is Layar, a browser that layers information of public interest—delivered through an open-source content management system—over the actual image of a real space, streamed live on the mobile phone display. An increasing number of artists employ this type of mobile AR apps to create artworks that consist in perceptually combining material reality and virtual data: as the user points the smartphone or tablet to a specific place, virtual 3D-modelled graphics or videos appear in real time, seamlessly inserted in the image of that location, according to the user’s position and orientation.    In the engineering and IT design fields, one of the first researchers to articulate a coherent conceptualization of AR and to underlie its specific capabilities is Ronald Azuma. He writes that, unlike Virtual Reality (VR) which completely immerses the user inside a synthetic environment, AR supplements reality, therefore enhancing “a user’s perception of and interaction with the real world” (355-385). Another important contributor to the foundation of AR as a concept and as a research field is industrial engineer Paul Milgram. He proposes a comprehensive and frequently cited definition of “Mixed Reality” (MR) via a schema that includes the entire spectrum of situations that span the “continuum” between actual reality and virtual reality, with “augmented reality” and “augmented virtuality” between the two poles (283).     Important to remark with regard to terminology (MR or AR) is that especially in the non-scientific literature, authors do not always explain a preference for either MR or AR. This suggests that the two terms are understood as synonymous, but it also provides evidence for my arg"
pub.1139758473,The Role of Artificial Intelligence in Revolutionizing Frailty Diagnosis and Patient Care,"Artificial Intelligence (AI) refers to the design of computer programs and machines which simulate the rudiments of human intelligence independently [1]. Machine learning encompasses a multitude of deep learning algorithms, including Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) - both of which enable continuous analysis of large-scale data to make decisions consistent with previously detected patterns [1]. AI exhibits high potential for employment in the healthcare industry and research laboratories to accurately predict illness, maximize disease prevention, and refine treatment plans. As technological advancements are made, the application of AI will gradually become more feasible and appropriately lend itself to advancing quality care for frail patients even away from the hospital setting.  Frailty is somewhat of an ambiguous diagnosis due to lack of a universally agreed upon definition and frailty assessment tool. Efforts have been put forth to delineate frailty and standardize its method of measurement, but many physicians with minimal to none geriatric experience are more likely to eyeball the patient from the foot end of the bed. Although the Comprehensive Geriatric Assessment (CGA) is a gold standard for multidisciplinary and systematic approach of frailty recognition, it is time-consuming and depends upon administers’ expertise [2]. The integration of AI into a frailty assessment strategy would not only cause a paradigm shift in the approach of physicians to this syndrome, but it would also revolutionize pre-existing protocols for management of frail and pre-frail status patients.  Sufficient neglect of the variables that comprise frailty results in inefficacious treatment plans and fuels the cost of patient care. International guidelines have come to appreciate the reversibility of frailty and concur that it should be a mandatory component of patient evaluation [3]. AI may be the solution to pinpointing unidentified vulnerabilities that characterize frailty and ensuring that this entity of geriatric practice is more readily incorporated into other subspecialties, too. Chang et al. (2013) conducted research using “household goods” in hopes of facilitating “early detection of frailty and, hence, its early treatment” [4]. eChair, for example, was used to detect “slowness of movement, weakness and weight loss” [4]. Other devices were featured to detect long-term variations in frailty-determining elements and overall functional decline [4]. Pressure sensors, for example, have been embedded into walkers to measure “risk of fall” [4]. Similarly, Canadian Cardiovascular Society Guidelines (2017) encourage the monitoring of orthostatic vital signs to “identify individuals at risk of falls” [3]. Therefore, gradual integration of AI into day-to-day appliances can be exceptionally beneficial when monitoring patients for development of frailty-like “symptoms”.  The authors would like to emphasize that the safety and accurac"
pub.1111666057,Цифрові технології сучасної логістики та управління ланцюгами поставок,"The aim of the article. The purpose of this paper is to analyze and specify the existing aspects of using different kinds of digital technologies in logistics and supply chain management in the current state of economic development and in the near future (in the digital economy) and to formulate our own view towards this problem, in particular, researching the use of digital technologies in different types of logistics. In addition, the majority of papers on this topic are foreign, thus this article is written to compensate for the lack of research on this topic in the domestic academia. The results of the analysis.The analysis done with the use of different sources towards spread of using digital technologies in logistics and supply chain management currently and in the near future has shown that the first place is taken by the Internet of Things that represents some kind of circulatory system of supply chains in digital economy, in which there is a circulation of data that can be effectively processed and stored by using the Big Data technology (placed second) and Cloud Computing (placed third). The fourth place in the rating of using digital technologies in logistics and supply chain management is taken by the Blockchain technology which can improve the correctness and reliability of storage the huge arrays of data. The sixth place is taken by the Robots, and the seventh is given to Artificial Intelligence, which can replace a person in such logistic activities with the significant amount of operations as manufacturing, transportation and warehouse logistics and become a base for the cyber-physical systems and the systems that are able to learn and teach other machines. The eighth place is taken by the Augmented and Virtual Reality technologies that can combine the real and the virtual worlds which can be helpful in order to increase the effectiveness of supply chain functionality. Despite the fact that the Sensors have taken the last place in the rating, they have a significant connection with others technologies and it can be assumed that they are not discussed, but they are meant to be. Conclusions and direction for further research. The research done has shown that technologies that are already associated with logistics information flows are the digital technologies which are currently being widely used either have the logistics companies interested in them or will become this way in the near future. This is caused by the fact that information in the digital economy is taken the special status, thus by using the information correctly a company can get the competitive advantage over other companies. The list of those technologies contains the following items: the Internet of Things, Big Data, Blockchain, and Artificial Intelligence, which can be used in all the types of functional spheres of logistics. Manufacturing, transportation, and warehouse logistics are the functional spheres of logistics where there are much more opportunities for t"
pub.1140647631,유라시아경제연합(EAEU) 통합과정 평가와 한국의 협력전략(The Evaluation of the Integrating Process of the EAEU and the Economic Cooperation Strategy between Korea and EAEU),"Korean Abstract: EAEU 통합과정에서 나타난 성과는 다음과 같다. 첫째, 경제통합체로서 제도적 기반이 조성되었다. 둘째, 새로운 관세법이 제정되고, 금융, 전력, 석유ㆍ석유제품, 가스, 교통 서비스 분야에서 공동시장 조성을 위한 기반이 마련되었다. 셋째, 2017년부터 GDP 및 무역액이 증가하면서 통합의 효과가 서서히 나타나기 시작했다. 넷째, 다수의 비회원국과 자유무역협정 또는 무역ㆍ경제 협정이 체결되었다. 이러한 성과에도 불구하고 EAEU가 가진 한계는 다음과 같다. 첫째, EAEU는 초국적 경제통합체로 기능하는 데 구조적 한계를 보인다. 둘째, 낮은 관세 조화 수준으로 인해 ‘제한된 관세동맹’에 머물러 있고, 완전한 공동시장도 아직 가시화되지 않았다. 셋째, 러시아의 리더십 한계로 인해 통합의 추진력이 약화하고 있다. 넷째, 보호주의적 특징이 경제통합의 성과를 제한하고 있다. 이러한 성과와 한계를 고려할 때, EAEU 통합과정 발전에 대한 전망을 다음과 같이 제시할 수 있다. 첫째, EAEU가 해체될 가능성은 희박하지만 더 높은 수준의 경제통합을 달성하기도 쉽지 않아 보인다. 둘째, 공동시장의 완전한 작동, 정부 조달 분야의 발전, 거시경제 안정성 유지 등이 이루어진다면 역내 무역이 더욱 촉진될 수 있을 것이다. 셋째, 신규 회원국 확보를 통한 거시경제적 잠재력 강화가 필요할 것으로 보인다. 따라서 탈소비에트 국가들과의 협력 확대를 목표로 하는 ‘신북방정책’을 추진하는 한국의 입장에서 EAEU는 무역 확대 및 무역 다변화와 한국의 새로운 성장동력 창출을 지원하는 파트너가 될 수 있다고 판단된다. 본 보고서는 한-EAEU 경제협력 전략으로 민간 및 시장 주도형 경제협력 전략을 제안하며, 그 추진전략과 협력방안으로 첫째, 현재 시장에서 산업협력 중점 분야의 식별과 지원, 둘째, 미래지향적 산업협력 중점 분야의 선정과 지원, 셋째, 한-EAEU FTA 추진을 제시한다. 첫째, 현재 시장에서 산업협력 중점 분야 협력방안은 다음과 같다. 한국의 대세계 주력 수출산업의 EAEU 수직분업 산업(광물, 코크ㆍ정유ㆍ핵연료, 금속 등)과 상대 권역에서 수입되어 양 권역에서 내수로 소비되는 산업(광물 및 수송기기 등)은 시장에서 이미 성과를 보이고 있으므로, 즉 시장 주도 및 민간 주도의 무역 협력 구조와 체계가 작동하고 있으므로, 제3장에서 산업협력 중점분야로 선정되었다. 이 산업군에서의 산업협력 강화방안은 다음과 같다. ① 정부는 정부간 협력 차원에서 양 권역의 교역 잠재력 제약요인을 해소하기 위한 협력 및 소통 체계를 제도화하는 노력을 지속해야 할 것이다. ② 한국의 대EAEU 수입품 및 EAEU의 대한국 수입품에 대한 관세인하 노력이 필요하다. ③ 기존의 ODA 프로그램을 활용하여 EAEU 국가를 대상으로 수출 능력 경험 전수사업, 세관 등 무역원활화 지원사업, 각종 시장경제 능력 강화사업 등을 실행할 것을 권고한다. 둘째, 미래지향적 산업협력 중점 분야와 협력방안은 다음과 같다. EAEU 국가들은 현재 산업화를 통한 자국경제의 성장과 4차산업 시대의 변화에 부합하는 디지털경제와 신산업 육성이라는 과제에 직면하고 있다. 한국은 이미 산업화를 달성한 국가로서, 국내시장에서 한계에 부딪힌 우리 중소기업의 성장을 위한 협력파트너로서 EAEU 국가들과의 산업협력 강화가 필요하다. 또한 EAEU 전체에서 디지털경제 육성은 중요한 과제로 추진되고 있으므로 러시아 등 EAEU 내에서 주도적인 국가와의 협력을 통해 디지털경제 기반구축과 자율주행차, 인공지능(AI), 클라우드 등 상호보완적인 다양한 분야에서 공동의 산업생태계 구축이 필요하다. 최근의 코로나19 사태와 관련하여 의료ㆍ보건 분야에서의 협력은 어느 때보다 중요한 과제가 되었다. EAEU 국가들의 의료체계나 의약품, 의료기기 시장의 발전 가능성이 높다는 점을 고려하면 한국 의료기관의 경영 컨설팅과 위탁경영, 의료기기와 의약품 수출과 같은 분야에서 전망이 밝다. 셋째, 한-EAEU FTA 추진전략과 협력방안은 다음과 같다. 양국의 산업 환경을 고려하면서 단계적인 논의를 통해 상호이익이 되는 방향에서 한-EAEU FTA를 추진할 필요가 있다. CGE 연구 결과에 따르면 FTA가 발효될 경우 한국의 GDP를 소폭 증가시키지만 영향의 정도는 산업별로 다르다. 한국 곡물산업의 피해가 가장 크고, 제조업 분야에서는 특히 금속, 전기ㆍ전자, 기계 산업에 부정적 효과가 발생할 것으로 예상된다. 반면 육류, 가공식품, 수송기기 산업에는 긍정적인 효과가 나타날 것으로 예측된다. 서비스업을 크게 도소매, 운송, 보건복지, 사업서비스 등으로 구분하여 FTA의 영향 정도를 분석한 결과 대체적으로 서비스업의 생산은 증가할 것으로 예상된다. 반면 EAEU 국가 중에서는 러시아와 키르기스스탄의 GDP는 증가하는 반면, 카자흐스탄, 벨라루스, 아르메니아의 GDP는 감소한다. EAEU 국가의 제조업 및 서비스업에서의 산업생산이 감소할 것으로 나타나므로 향후 FTA 혹은 경제협력에서 이 분야들에 대한 정밀한 협력방안을 강구할 필요가 있다. 종합하면 한국과 EAEU FTA가 어느 일국에게만 일방적으로 유리한 결과를 낳지 않으므로 양국이 FTA를 통해 서로가 윈-윈 할 수 있는 협력방안을 모색해야 할 것이다. 특히 한국이 높은 경쟁력을 가지고 있는 수송기기 산업에서의 기술지원, 신기술협력사업, ODA 등을 통해 우리나라가 피해를 볼 것으로 예상되는 산업의 피해를 최소화하고 상대국의 피해산업도 보완할 수 있는 양국간 협력관계를 형성해야 할 것이다.English Abstract: 1. Evaluation of the integration process of the EAEUThe results of the integration process of the EAEU are as follows: first, the EAEU has created an institutional foundation as an economic integrator. Second, the EAEU has enacted new tariff laws and laid the foundation "
pub.1111071323,Nowe trendy w systemie regulowania rynku usług bankowych,"The severity of the last financial crisis for the European financial markets, the economy, and society makes the scientists and financial analysts start to seek answers with great openness not only to the question of how to reduce its negative effects in the future, but also of how the system regulating financial institutions will look like in the future. The article discusses three options of the positions on the future regulatory tendencies: theoretical alternative, option presented in reports and expert studies, and the version arising from observations of the current practices of functioning of the European banks. The aim of the article is to confront the views on the future trends in the regulation of the banking sector from theoretical, consulting point of view, and the view formulated on the basis of evalua-tion of banking practices. The severity of the last financial crisis for the European financial markets, the economy, and society, which lasts almost 8 years, makes the scientists and financial analysts start to seek answers with great openness not only to the question of how to reduce its negative effects in the future, but also to consider how the system regulating financial institutions will look like in the future. The article presents three options of the positions on the future regulatory tendencies: theoretical alternative, the option presented in reports and expert studies, and the version arising from observations of the current practices of functioning of the European banks. The aim of the article is to confront the views on the future trends in the regulation of the banking sector from the theoretical, consulting point of view, and the view formulated on the basis of evaluation of banking practices. The elaboration presents a working hypothesis that the implemented financial innovations will be the main force driving and influencing concepts, methodology, and the operation of regulatory institutions of the banking sector. The article consists of four, logically related sections. The first assumes that the vision of the regulatory system for bank institutions will result from the interactions of banks and the actions of the regulators. Therefore, a synthetic assessment of the changes that have taken place in the bank regulatory system is presented. The second part highlights the changes in the post-crisis regulations, as the crisis had a strong impact on the shape of the new regulatory architecture. The third part of the elaboration presents the scenarios of the future development of the banking sector and the directions of changes to its regulations. The last part exposes the expectations of the bankers in relation to the key actions of the regulators in the perspective of the nearest 10-15 years. Visions of regulating the banking sector The issue of predicting trends in regulating the banking sector is a serious scientific and research challenge, yet highly troubling and controversial [Gosee, Philon, 2014]. From the nature a"
pub.1131719913,The Real Future of the Media,"When George Orwell encountered ideas of a technological utopia sixty-five years ago, he acted the grumpy middle-aged man  Reading recently a batch of rather shallowly optimistic “progressive” books, I was struck by the automatic way in which people go on repeating certain phrases which were fashionable before 1914. Two great favourites are “the abolition of distance” and “the disappearance of frontiers”. I do not know how often I have met with the statements that “the aeroplane and the radio have abolished distance” and “all parts of the world are now interdependent” (1944).  It is worth revisiting the old boy’s grumpiness, because the rhetoric he so niftily skewers continues in our own time. Facebook features “Peace on Facebook” and even claims that it can “decrease world conflict” through inter-cultural communication. Twitter has announced itself as “a triumph of humanity” (“A Cyber-House” 61). Queue George.  In between Orwell and latter-day hoody cybertarians, a whole host of excitable public intellectuals announced the impending end of materiality through emergent media forms. Marshall McLuhan, Neil Postman, Daniel Bell, Ithiel de Sola Pool, George Gilder, Alvin Toffler—the list of 1960s futurists goes on and on. And this wasn’t just a matter of punditry: the OECD decreed the coming of the “information society” in 1975 and the European Union (EU) followed suit in 1979, while IBM merrily declared an “information age” in 1977. Bell theorized this technological utopia as post-ideological, because class would cease to matter (Mattelart). Polluting industries seemingly no longer represented the dynamic core of industrial capitalism; instead, market dynamism radiated from a networked, intellectual core of creative and informational activities. The new information and knowledge-based economies would rescue First World hegemony from an “insurgent world” that lurked within as well as beyond itself (Schiller).  Orwell’s others and the Cold-War futurists propagated one of the most destructive myths shaping both public debate and scholarly studies of the media, culture, and communication. They convinced generations of analysts, activists, and arrivistes that the promises and problems of the media could be understood via metaphors of the environment, and that the media were weightless and virtual. The famous medium they wished us to see as the message —a substance as vital to our wellbeing as air, water, and soil—turned out to be no such thing. Today’s cybertarians inherit their anti-Marxist, anti-materialist positions, as a casual glance at any new media journal, culture-industry magazine, or bourgeois press outlet discloses.  The media are undoubtedly important instruments of social cohesion and fragmentation, political power and dissent, democracy and demagoguery, and other fraught extensions of human consciousness. But talk of media systems as equivalent to physical ecosystems—fashionable among marketers and media scholars alike—is predicated on the n"
pub.1131720065,Cute and Monstrous Furbys in Online Fan Production,"Image 1: Hasbro/Tiger Electronics 1998 Furby. (Photo credit: Author)  Introduction  Since the mid-1990s robotic and digital creatures designed to offer social interaction and companionship have been developed for commercial and research interests. Integral to encouraging positive experiences with these creatures has been the use of cute aesthetics that aim to endear companions to their human users. During this time there has also been a growth in online communities that engage in cultural production through fan fiction responses to existing cultural artefacts, including the widely recognised electronic companion, Hasbro’s Furby (image 1).  These user stories and Furby’s online representation in general, demonstrate that contrary to the intentions of their designers and marketers, Furbys are not necessarily received as cute, or the embodiment of the helpless and harmless demeanour that goes along with it. Furbys’ large, lash-framed eyes, small, or non-existent limbs, and baby voice are typical markers of cuteness but can also evoke another side of cuteness—monstrosity, especially when the creature appears physically capable instead of helpless (Brzozowska-Brywczynska 217). Furbys are a particularly interesting manifestation of the cute aesthetic because it is used as tool for encouraging attachment to a socially interactive electronic object, and therefore intersects with existing ideas about technology and nonhuman companions, both of which often embody a sense of otherness.  This paper will explore how cuteness intersects withand transitions into monstrosity through online representations of Furbys, troubling their existing design and marketing narrative by connecting and likening them to other creatures, myths, and anecdotes. Analysis of narrative in particular highlights the instability of cuteness, and cultural understandings of existing cute characters, such as the gremlins from the film Gremlins (Dante) reinforce the idea that cuteness should be treated with suspicion as it potentially masks a troubling undertone. Ultimately, this paper aims to interrogate the cultural complexities of designing electronic creatures through the stories that people tell about them online.    Fan Production  Authors of fan fiction are known to creatively express their responses to a variety of media by appropriating the characters, settings, and themes of an original work and sharing their cultural activity with others (Jenkins 88). On a personal level, Jenkins (103) argues that “[i]n embracing popular texts, the fans claim those works as their own, remaking them in their own image, forcing them to respond to their needs and to gratify their desires.”  Fan fiction authors are motivated to write not for financial or professional gains but for personal enjoyment and fan recognition, however, their production does not necessarily come from favourable opinions of an existing text. The antifan is an individual who actively hates a text or cultural artefact and is m"
pub.1131718884,A Risky Business? The Role of Incentives and Runaway Production in Securing a Screen Industries Production Base in Scotland,"IntroductionDespite claims that the importance of distance has been reduced due to technological and communications improvements (Cairncross; Friedman; O’Brien), the ‘power of place’ still resonates, often intensifying the role of geography (Christopherson et al.; Morgan; Pratt; Scott and Storper). Within the film industry, there has been a decentralisation of production from Hollywood, but there remains a spatial logic which has preferenced particular centres, such as Toronto, Vancouver, Sydney and Prague often led by a combination of incentives (Christopherson and Storper; Goldsmith and O’Regan; Goldsmith et al.; Miller et al.; Mould). The emergence of high end television, television programming for which the production budget is more than £1 million per television hour, has presented new opportunities for screen hubs sharing a very similar value chain to the film industry (OlsbergSPI with Nordicity).In recent years, interventions have proliferated with the aim of capitalising on the decentralisation of certain activities in order to attract international screen industries production and embed it within local hubs. Tools for building capacity and expertise have proliferated, including support for studio complex facilities, infrastructural investments, tax breaks and other economic incentives (Cucco; Goldsmith and O’Regan; Jensen; Goldsmith et al.; McDonald; Miller et al.; Mould). Yet experience tells us that these will not succeed everywhere. There is a need for a better understanding of both the capacity for places to build a distinctive and competitive advantage within a highly globalised landscape and the relative merits of alternative interventions designed to generate a sustainable production base.This article first sets out the rationale for the appetite identified in the screen industries for co-location, or clustering and concentration in a tightly drawn physical area, in global hubs of production. It goes on to explore the latest trends of decentralisation and examines the upturn in interventions aimed at attracting mobile screen industries capital and labour. Finally it introduces the Scottish screen industries and explores some of the ways in which Scotland has sought to position itself as a recipient of screen industries activity. The paper identifies some key gaps in infrastructure, most notably a studio, and calls for closer examination of the essential ingredients of, and possible interventions needed for, a vibrant and sustainable industry.A Compulsion for ProximityIt has been argued that particular spatial and place-based factors are central to the development and organisation of the screen industries. The film and television sector, the particular focus of this article, exhibit an extraordinarily high degree of spatial agglomeration, especially favouring centres with global status. It is worth noting that the computer games sector, not explored in this article, slightly diverges from this trend displaying more spatial patterns"
pub.1160087211,Training of qualified specialists in the conditions of digitalization of education,"Modern professions offered high demands on the intellectual skills of employees to students’ place. Information technologies, especially multimedia, provide an opportunity to develop the intellectual skills of students. High requirements for the intellectual development of students occupy one of the leading positions in the international labor market. A delay in the development of intellectual skills means a delay in life. Therefore, in professional preparation of students for the modern digital society, it is necessary, first of all, to develop their intellectual skills and teach them how to apply them in practice"
pub.1131718985,Without a True North: Tactical Approaches to Self-Published Fiction,"IntroductionOver three days in November 2017, 400 people gathered for a conference at the Sam’s Town Hotel and Gambling Hall in Las Vegas, Nevada. The majority of attendees were fiction authors but the conference program looked like no ordinary writer’s festival; there were no in-conversation interviews with celebrity authors, no panels on the politics of the book industry and no books launched or promoted. Instead, this was a gathering called 20Books2017, a self-publishing conference about the business of fiction ebooks and there was expertise in the room.Among those attending, 50 reportedly earned over $100,000 US per annum, with four said to be earning in excess of $1,000,000 US year. Yet none of these authors are household names. Their work is not adapted to film or television. Their books cannot be found on the shelves of brick-and-mortar bookstores. For the most part, these authors go unrepresented by the publishing industry and literary agencies, and further to which, only a fraction have ever actively pursued traditional publishing. Instead, they write for and sell into a commercial fiction market dominated by a single retailer and publisher: online retailer Amazon.While the online ebook market can be dynamic and lucrative, it can also be chaotic. Unlike the traditional publishing industry—an industry almost stoically adherent to various gatekeeping processes: an influential agent-class, formalized education pathways, geographic demarcations of curatorial power (see Thompson)—the nascent ebook market is unmapped and still somewhat ungoverned. As will be discussed below, even the markets directly engineered by Amazon are subject to rapid change and upheaval. It can be a space with shifting boundaries and thus, for many in the traditional industry both Amazon and self-publishing come to represent a type of encroaching northern dread.In the eyes of the traditional industry, digital self-publishing certainly conforms to the barbarous north of European literary metaphor: Orwell’s ‘real ugliness of industrialism’ (94) governed by the abject lawlessness of David Peace’s Yorkshire noir (Fowler). But for adherents within the day-to-day of self-publishing, this unruly space also provides the frontiers and gold-rushes of American West mythology.What remains uncertain is the future of both the traditional and the self-publishing sectors and the degree to which they will eventually merge, overlap and/or co-exist. So-called ‘hybrid-authors’ (those self-publishing and involved in traditional publication) are becoming increasingly common—especially in genre fiction—but the disruption brought about by self-publishing and ebooks appears far from complete.To the contrary, the Amazon-led ebook iteration of this market is relatively new. While self-publishing and independent publishing have long histories as modes of production, Amazon launched both its Kindle e-reader device and its marketplace Kindle Direct Publishing (KDP) a little over a decade ago. In th"
pub.1131718952,Ways of Depicting: The Presentation of One’s Self as a Brand,"Ways of Seeing""Images … define our experiences more precisely in areas where words are inadequate."" (Berger 33)""Different skins, you know, different ways of seeing the world."" (Morrison)The research question animating this article is: 'How does an individual creative worker re-present themselves as a contemporary - and evolving - brand?' Berger notes that the ""principal aim has been to start a process of questioning"" (5), and the raw material energising this exploration is the life's work of Richard Morrison, the creative director and artist who is the key moving force behind The Morrison Studio collective of designers, film makers and visual effects artists, working globally but based in London. The challenge of maintaining currency in this visually creative marketplace includes seeing what is unique about your potential contribution to a larger project, and communicating it in such a way that this forms an integral part of an evolving brand - on trend, bleeding edge, but reliably professional. One of the classic outputs of Morrison's oeuvre, for example, is the title sequence for Terry Gilliam's Brazil.Passion cannot be seen yet Morrison conceives it as the central engine that harnesses skills, information and innovative ways of working to deliver the unexpected and the unforgettable. Morrison's perception is that the design itself can come after the creative artist has really seen and understood the client's perspective. As he says: ""What some clients are interested in is 'How can we make money from what we're doing?'"" Seeing the client, and the client's motivating needs, is central to Morrison's presentation of self as a brand: ""the broader your outlook as a creative, the more chance you have of getting it right"". Jones and Warren draw attention to one aspect of this dynamic: ""Wealthy and private actors, both private and state, historically saw creative practice as something that money was spent on - commissioning a painting or a sculpture, giving salaries to composers to produce new works and so forth. Today, creativity has been reimagined as something that should directly or indirectly make money"" (293). As Berger notes, ""We never look at just one thing; we are always looking at the relation between things and ourselves…The world-as-it-is is more than pure objective fact, it includes consciousness"" (9, 11). What is our consciousness around the creative image?Individuality is central to Berger's vision of the image in the ""specific vision of the image-maker…the result of an increasing consciousness of individuality, accompanying an increasing awareness of history"" (10). Yet, as Berger argues ""although every image embodies a way of seeing, our perception or appreciation of an image depends also upon our own way of seeing"" (10). Later, Berger links the meanings viewers attribute to images as indicating the ""historical experience of our relation to the past…the experience of seeking to give meaning to our lives"" (33). The seeing and the seeking"
pub.1036003893,"Monday, December 4, 2006Poster Session IV7:30 a.m. – 4:30 p.m.","1 Véronique M. André, 1 Carlos Cepeda, 1,3 Vinters V. Harry, 1,2 Mathern W. Gary, and 1 Levine S. Michael ( 1 Mental Retardation Research Center, David Geffen School of Medicine at UCLA, Los Angeles, CA ; 2 Division of Neurosurgery ; and 3 Department of Neuropathology, UCLA, Los Angeles, CA ) Rationale: Based on histopathology, cortical dysplasias (CD) are classified into mild CD if the cortex shows architectural abnormalities only and severe CD if the cortex also presents cytomegalic neurons and balloon cells in addition to architectural changes. Both types induce intractable seizures in children. We showed that GABAergic cell numbers and GABA terminals were altered in severe CD but not in mild CD. GABA peak currents, densities and desensitization time constants were also differentially altered in mild and severe CD, suggesting that mechanisms of seizure induction might be different in those two types of pediatric CD (Andre et al., Epilepsia 46 Suppl. 8: 5, 2005). The present study characterized further postsynaptic GABAA receptor function in non‐CD, mild and severe CD human brain. Methods: Cortical samples resected for the treatment of pharmaco‐resistant epilepsy were collected from non‐CD (n = 10), mild (n = 6) and severe CD (n = 10) patients. Brain tissue slices were acutely dissociated and electrophysiological recordings were performed on isolated pyramidal neurons. Patch electrodes were filled with N‐methyl‐D‐glucamine for whole‐cell voltage clamp recordings. Different concentrations of GABA and GABAA receptor modulators were applied to the cells. Results: EC50 values were similar in non‐CD and severe CD but higher in mild CD cells suggesting less sensitive GABAA receptors. Zinc reversibly decreased GABA peak currents in all cells. However, severe CD cells had a smaller sensitivity to zinc compared to non‐CD cells. Zolpidem, a benzodiazepine (BZ) of type I that binds specifically to α1 subunits, reversibly enhanced GABA peak currents in all cells. Zolpidem sensitivity was significantly smaller in severe CD cells compared to mild and non‐CD cells. Conclusions: GABA sensitivity determined by EC50 values was similar in non‐CD and severe CD cells. However, GABAA receptors responded differently to modulators in severe CD vs non‐CD. Sensitivity to zinc was smaller in severe CD, suggesting higher expression of γ2 subunits. Zolpidem effect was also decreased in severe CD vs non‐CD indicating a decreased expression of α1. As α1 and γ2 subunits are part of the BZ binding site, alteration in their expression is likely to change the efficacy of BZ used to treat seizures in severe CD. In mild CD, effects of modulators binding to α1‐ and γ2‐containing receptors were not different from non‐CD indicating those subunits are not altered. However, GABA sensitivity was lower in mild CD, which could implicate a loss of inhibition and lead to seizures. Together, these results indicate that GABAA receptor subunit composition is differentially altered in mild and "
