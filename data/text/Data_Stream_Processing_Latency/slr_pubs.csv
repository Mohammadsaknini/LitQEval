id,title,abstract
pub.1002913244,Complementary fMRI and EEG evidence for more efficient neural processing of rhythmic vs. unpredictably timed sounds,"The brain's fascinating ability to adapt its internal neural dynamics to the temporal structure of the sensory environment is becoming increasingly clear. It is thought to be metabolically beneficial to align ongoing oscillatory activity to the relevant inputs in a predictable stream, so that they will enter at optimal processing phases of the spontaneously occurring rhythmic excitability fluctuations. However, some contexts have a more predictable temporal structure than others. Here, we tested the hypothesis that the processing of rhythmic sounds is more efficient than the processing of irregularly timed sounds. To do this, we simultaneously measured functional magnetic resonance imaging (fMRI) and electro-encephalograms (EEG) while participants detected oddball target sounds in alternating blocks of rhythmic (e.g., with equal inter-stimulus intervals) or random (e.g., with randomly varied inter-stimulus intervals) tone sequences. Behaviorally, participants detected target sounds faster and more accurately when embedded in rhythmic streams. The fMRI response in the auditory cortex was stronger during random compared to random tone sequence processing. Simultaneously recorded N1 responses showed larger peak amplitudes and longer latencies for tones in the random (vs. the rhythmic) streams. These results reveal complementary evidence for more efficient neural and perceptual processing during temporally predictable sensory contexts. "
pub.1112630502,High-Throughput Line Buffer Microarchitecture for Arbitrary Sized Streaming Image Processing,"Parallel hardware designed for image processing promotes vision-guided intelligent applications. With the advantages of high-throughput and low-latency, streaming architecture on FPGA is especially attractive to real-time image processing. Notably, many real-world applications, such as region of interest (ROI) detection, demand the ability to process images continuously at different sizes and resolutions in hardware without interruptions. FPGA is especially suitable for implementation of such flexible streaming architecture, but most existing solutions require run-time reconfiguration, and hence cannot achieve seamless image size-switching. In this paper, we propose a dynamically-programmable buffer architecture (D-SWIM) based on the Stream-Windowing Interleaved Memory (SWIM) architecture to realize image processing on FPGA for image streams at arbitrary sizes defined at run time. D-SWIM redefines the way that on-chip memory is organized and controlled, and the hardware adapts to arbitrary image size with sub-100 ns delay that ensures minimum interruptions to the image processing at a high frame rate. Compared to the prior SWIM buffer for high-throughput scenarios, D-SWIM achieved dynamic programmability with only a slight overhead on logic resource usage, but saved up to 56 % of the BRAM resource. The D-SWIM buffer achieves a max operating frequency of 329.5 MHz and reduction in power consumption by 45.7 % comparing with the SWIM scheme. Real-world image processing applications, such as 2D-Convolution and the Harris Corner Detector, have also been used to evaluate D-SWIM's performance, where a pixel throughput of 4.5 Giga Pixel/s and 4.2 Giga Pixel/s were achieved respectively in each case. Compared to the implementation with prior streaming frameworks, the D-SWIM-based design not only realizes seamless image size-switching, but also improves hardware efficiency up to 30 × ."
pub.1158060058,Modelling novelty detection in the thalamocortical loop,"In complex natural environments, sensory systems are constantly exposed to a large stream of inputs. Novel or rare stimuli, which are often associated with behaviorally important events, are typically processed differently than the steady sensory background, which has less relevance. Neural signatures of such differential processing, commonly referred to as novelty detection, have been identified on the level of EEG recordings as mismatch negativity (MMN) and on the level of single neurons as stimulus-specific adaptation (SSA). Here, we propose a multi-scale recurrent network with synaptic depression to explain how novelty detection can arise in the whisker-related part of the somatosensory thalamocortical loop. The ""minimalistic"" architecture and dynamics of the model presume that neurons in cortical layer 6 adapt, via synaptic depression, specifically to a frequently presented stimulus, resulting in reduced population activity in the corresponding cortical column when compared with the population activity evoked by a rare stimulus. This difference in population activity is then projected from the cortex to the thalamus and amplified through the interaction between neurons of the primary and reticular nuclei of the thalamus, resulting in rhythmic oscillations. These differentially activated thalamic oscillations are forwarded to cortical layer 4 as a late secondary response that is specific to rare stimuli that violate a particular stimulus pattern. Model results show a strong analogy between this late single neuron activity and EEG-based mismatch negativity in terms of their common sensitivity to presentation context and timescales of response latency, as observed experimentally. Our results indicate that adaptation in L6 can establish the thalamocortical dynamics that produce signatures of SSA and MMN and suggest a mechanistic model of novelty detection that could generalize to other sensory modalities."
pub.1013139537,Evolution of aquatic insect behaviours across a gradient of disturbance predictability,"Natural disturbance regimes--cycles of fire, flood, drought or other events--range from highly predictable (disturbances occur regularly in time or in concert with a proximate cue) to highly unpredictable. While theory predicts how populations should evolve under different degrees of disturbance predictability, there is little empirical evidence of how this occurs in nature. Here, we demonstrate local adaptation in populations of an aquatic insect occupying sites along a natural gradient of disturbance predictability, where predictability was defined as the ability of a proximate cue (rainfall) to signal a disturbance (flash flood). In controlled behavioural experiments, populations from predictable environments responded to rainfall events by quickly exiting the water and moving sufficiently far from the stream to escape flash floods. By contrast, populations from less predictable environments had longer response times and lower response rates, reflecting the uncertainty inherent to these environments. Analysis with signal detection theory showed that for 13 out of 15 populations, observed response times were an optimal compromise between the competing risks of abandoning versus remaining in the stream, mediated by the rainfall-flood correlation of the local environment. Our study provides the first demonstration that populations can evolve in response to differences in disturbance predictability, and provides evidence that populations can adapt to among-stream differences in flow regime."
pub.1092132554,The role of multisensory interplay in enabling temporal expectations,"Temporal regularities can guide our attention to focus on a particular moment in time and to be especially vigilant just then. Previous research provided evidence for the influence of temporal expectation on perceptual processing in unisensory auditory, visual, and tactile contexts. However, in real life we are often exposed to a complex and continuous stream of multisensory events. Here we tested - in a series of experiments - whether temporal expectations can enhance perception in multisensory contexts and whether this enhancement differs from enhancements in unisensory contexts. Our discrimination paradigm contained near-threshold targets (subject-specific 75% discrimination accuracy) embedded in a sequence of distractors. The likelihood of target occurrence (early or late) was manipulated block-wise. Furthermore, we tested whether spatial and modality-specific target uncertainty (i.e. predictable vs. unpredictable target position or modality) would affect temporal expectation (TE) measured with perceptual sensitivity (d<sup>'</sup>) and response times (RT). In all our experiments, hidden temporal regularities improved performance for expected multisensory targets. Moreover, multisensory performance was unaffected by spatial and modality-specific uncertainty, whereas unisensory TE effects on d<sup>'</sup> but not RT were modulated by spatial and modality-specific uncertainty. Additionally, the size of the temporal expectation effect, i.e. the increase in perceptual sensitivity and decrease of RT, scaled linearly with the likelihood of expected targets. Finally, temporal expectation effects were unaffected by varying target position within the stream. Together, our results strongly suggest that participants quickly adapt to novel temporal contexts, that they benefit from multisensory (relative to unisensory) stimulation and that multisensory benefits are maximal if the stimulus-driven uncertainty is highest. We propose that enhanced informational content (i.e. multisensory stimulation) enables the robust extraction of temporal regularities which in turn boost (uni-)sensory representations."
pub.1046720626,Emergence of neural encoding of auditory objects while listening to competing speakers,"A visual scene is perceived in terms of visual objects. Similar ideas have been proposed for the analogous case of auditory scene analysis, although their hypothesized neural underpinnings have not yet been established. Here, we address this question by recording from subjects selectively listening to one of two competing speakers, either of different or the same sex, using magnetoencephalography. Individual neural representations are seen for the speech of the two speakers, with each being selectively phase locked to the rhythm of the corresponding speech stream and from which can be exclusively reconstructed the temporal envelope of that speech stream. The neural representation of the attended speech dominates responses (with latency near 100 ms) in posterior auditory cortex. Furthermore, when the intensity of the attended and background speakers is separately varied over an 8-dB range, the neural representation of the attended speech adapts only to the intensity of that speaker but not to the intensity of the background speaker, suggesting an object-level intensity gain control. In summary, these results indicate that concurrent auditory objects, even if spectrotemporally overlapping and not resolvable at the auditory periphery, are neurally encoded individually in auditory cortex and emerge as fundamental representational units for top-down attentional modulation and bottom-up neural adaptation."
pub.1138770051,Cognitive load influences oculomotor behavior in natural scenes,"Cognitive neuroscience researchers have identified relationships between cognitive load and eye movement behavior that are consistent with oculomotor biomarkers for neurological disorders. We develop an adaptive visual search paradigm that manipulates task difficulty and examine the effect of cognitive load on oculomotor behavior in healthy young adults. Participants (N = 30) free-viewed a sequence of 100 natural scenes for 10 s each, while their eye movements were recorded. After each image, participants completed a 4 alternative forced choice task in which they selected a target object from one of the previously viewed scenes, among 3 distracters of the same object type but from alternate scenes. Following two correct responses, the target object was selected from an image increasingly farther back (N-back) in the image stream; following an incorrect response, N decreased by 1. N-back thus quantifies and individualizes cognitive load. The results show that response latencies increased as N-back increased, and pupil diameter increased with N-back, before decreasing at very high N-back. These findings are consistent with previous studies and confirm that this paradigm was successful in actively engaging working memory, and successfully adapts task difficulty to individual subject’s skill levels. We hypothesized that oculomotor behavior would covary with cognitive load. We found that as cognitive load increased, there was a significant decrease in the number of fixations and saccades. Furthermore, the total duration of saccades decreased with the number of events, while the total duration of fixations remained constant, suggesting that as cognitive load increased, subjects made fewer, longer fixations. These results suggest that cognitive load can be tracked with an adaptive visual search task, and that oculomotor strategies are affected as a result of greater cognitive demand in healthy adults."
pub.1132133347,Primary tumor and metastasis—sectioning the different steps of the metastatic cascade,"Patients with lung cancer in the majority die of metastases. Treatment options include surgery, chemo- and radiotherapy, targeted therapy by tyrosine kinase inhibitors (TKIs), and immuno-oncologic treatment. Despite the success with these treatment options, cure of lung cancer is achieved in only a very small proportion of patients. In most patients' recurrence and metastasis will occur, and finally kill the patient. Metastasis is a multistep procedure. It requires a change in adhesion of tumor cells for detachment from their neighboring cells. The next step is migration either as single cells [epithelial-mesenchymal transition (EMT)], or as cell clusters (hybrid-EMT or bulk migration). A combination of genetic changes is required to facilitate migration. Then tumor cells have to orient themselves along matrix proteins, detect oxygen concentrations, prevent attacks by immune cells, and induce a tumor-friendly switch of stroma cells (macrophages, myofibroblasts, etc.). Having entered the blood stream tumor cells need to adapt to shear stress, avoid being trapped by coagulation, but also use coagulation in small veins for adherence to endothelia, and express homing molecules for extravasation. Within a metastatic site, tumor cells need a well-prepared niche to establish a metastatic focus. Tumor cells again have to establish a vascular net for maintaining nutrition and oxygen supply, communicate with stroma cells, grow out and set further metastases. In this review the different steps will be discussed with a focus on pulmonary carcinomas. The vast amount of research manuscripts published so far are not easy to analyze: in most reports' single steps of the metastatic cascade are interpreted as evidence for the whole process; for example, migration is interpreted as evidence for metastasis. In lung cancer most often latency periods are shorter, in between 1-5 years. In other cases, despite widespread migration occurs, tumor cells die within the circulation and do not reach a metastatic site. Therefore, migration is a requisite, but does not necessarily predict metastasis. The intention of this review is to point to these different aspects and hopefully provoke research directed into a more functional analysis of the metastatic process."
pub.1144653577,A Framework for Decentralized Parallel Complex Event Processing on Heterogeneous Infrastructures,"Low-latency stream processing applications (SPA) distribute the event streams on parallel instances of the application’s operators to avoid queuing. Each operator has a set of processing nodes for its instances. Edge and fog infrastructures promise, among other things, low communication latency and are thus of interest for low-latency SPAs. Yet, they include heterogeneous, even volatile nodes which makes the parallel processing of SPAs challengingAvailable solutions for parallel SPAs often assume homogeneity and stable infrastructures.Thus, we aim to balance the stream processing among heterogeneous, instable infrastructures using a decentralized control mechanism. We propose a decentralized, dynamic, and lightweight approach to re-balance SPA-load according to the nodes current performance, while minimizing latencies. In doing so, we introduce the Coda-EFC framework for window-based stream processing on heterogeneous distributed infrastructures. It dynamically adapts its load balancing to node performance without computing quickly outdated schedules and aims for latency-optimal load balancing. We introduce two adaptive load balancing strategiesone tailored for heterogeneous, dynamic infrastructures, the other an adaptation of a state-of-the art approach."
pub.1014248013,Efficient and Adaptive Processing of Multiple Continuous Queries,"Continuous queries are queries executed on data streams within a potentially open-ended time interval specified by the user and are usually long running. The data streams are likely to exhibit fluctuating characteristics such as varying inter-arrival times, as well as varying data characteristics during the query execution. In the presence of such unpredictable factors, continuous query systems must still be able to efficiently handle large number of queries, as well as to offer acceptable individual query performance. In this paper, we propose and discuss a novel framework, called AdaptiveCQ, for the efficient processing of multiple continuous queries. In our framework, multiple queries share intermediate results at a fine level of granularity. Unlike previous approaches to sharing or reusing that relied on materialization to disk, AdaptiveCQ allows on-the-fly sharing of results. We show that this feature improves both the initial query response time, and the overall response time. Finally, AdaptiveCQ, which extrapolates the idea proposed by the eddy query-processing model, adapts well to fluctuations of the data streams characteristics by this combination of fine grain and on-the-fly sharing. We implemented AdaptiveCQ from scratch in Java and made use of it to conduct the experiments. We present experimental results that substantiate our claim that AdaptiveCQ can provide substantial performance improvements over existing methods of reusing intermediate results that relied on materialization to disk. In addition, we also show that AdaptiveCQ can adapt well to fluctuations in the query environment."
pub.1092569607,Drizzle,"Large scale streaming systems aim to provide high throughput and low latency. They are often used to run mission-critical applications, and must be available 24x7. Thus such systems need to adapt to failures and inherent changes in workloads, with minimal impact on latency and throughput. Unfortunately, existing solutions require operators to choose between achieving low latency during normal operation and incurring minimal impact during adaptation. Continuous operator streaming systems, such as Naiad and Flink, provide low latency during normal execution but incur high overheads during adaptation (e.g., recovery), while micro-batch systems, such as Spark Streaming and FlumeJava, adapt rapidly at the cost of high latency during normal operations. Our key observation is that while streaming workloads require millisecond-level processing, workload and cluster properties change less frequently. Based on this, we develop Drizzle, a system that decouples the processing interval from the coordination interval used for fault tolerance and adaptability. Our experiments on a 128 node EC2 cluster show that on the Yahoo Streaming Benchmark, Drizzle can achieve end-to-end record processing latencies of less than 100ms and can get 2-3x lower latency than Spark. Drizzle also exhibits better adaptability, and can recover from failures 4x faster than Flink while having up to 13x lower latency during recovery."
pub.1095646008,Query Plan Optimization and Migration Strategy over Data Stream,"Continuous queries are the more interesting class of data stream queries. The answer to a continuous query is continuously produced over time unless stopped artificially. During an executed continuous query, the run environment changes continuously, meanwhile some properties of data stream itself change too, such as input rate, selectivity of operators and execute time and so on. In order to adapt the continuity of queries and variability of system environment and date itself during continuous query, this paper presents a query plan optimization and migration strategy based on multi-factors and gives corresponding algorithm. Experiments have proved that this optimization and migration strategy ensures output results correctness and time sequence, meanwhile validly increases throughput and output rate and reduces output time latency."
pub.1007287400,MCEP,"With the proliferation of mobile devices and sensors, complex event proceesing (CEP) is becoming increasingly important to scalably detect situations in real time. Current CEP systems are not capable of dealing efficiently with highly dynamic mobile consumers whose interests change with their location. We introduce the distributed mobile CEP (MCEP) system which automatically adapts the processing of events according to a consumer's location. MCEP significantly reduces latency, network utilization, and processing overhead by providing on-demand and opportunistic adaptation algorithms to dynamically assign event streams and computing resources to operators of the MCEP system."
pub.1044962254,Dynamic Load Balancing Techniques for Distributed Complex Event Processing Systems,"Applying real-time, cost-effective Complex Event processing (CEP) in the cloud has been an important goal in recent years. Distributed Stream Processing Systems (DSPS) have been widely adopted by major computing companies such as Facebook and Twitter for performing scalable event processing in streaming data. However, dynamically balancing the load of the DSPS’ components can be particularly challenging due to the high volume of data, the components’ state management needs, and the low latency processing requirements. Systems should be able to cope with these challenges and adapt to dynamic and unpredictable load changes in real-time. Our approach makes the following contributions: (i) we formulate the load balancing problem in distributed CEP systems as an instance of the job-shop scheduling problem, and (ii) we present a novel framework that dynamically balances the load of CEP engines in real-time and adapts to sudden changes in the volume of streaming data by exploiting two balancing policies. Our detailed experimental evaluation using data from the Twitter social network indicates the benefits of our approach in the system’s throughput."
pub.1145611846,Cost-aware & Fault-tolerant Geo-distributed Edge Computing for Low-latency Stream Processing,"The number of Internet-of-Things (IoT) devices is rapidly increasing with the growth of IoT applications in various domains. As IoT applications have a strong demand for low latency and high throughput computing, stream processing using edge computing resources is a promising approach to support low latency processing of large-scale data. Edge-based stream processing extends the capability of cloud-based stream processing by processing the data streams near the edge of the network. In this vision paper, we discuss a distributed stream processing framework that optimizes the performance of stream processing applications through a careful allocation of geo-distributed computing and network resources available in edge computing environments. The framework includes key optimizations in both the platform layer and the infrastructure layer. While the platform layer is responsible for converting the user program into a stream processing physical plan and optimizing the physical plan and operator placement, the infrastructure layer is responsible for provisioning geo-distributed resources to the platform layer. The framework optimizes the performance of stream query processing at the platform layer through its careful consideration of data locality and resource constraints during physical plan generation and operator placement and by incorporating resilience to deal with failures. The framework also includes techniques to dynamically determine the level of parallelism to adapt to changing workload conditions. At the infrastructure layer, the framework includes a novel model for allocating computing resources in edge and geo-distributed cloud computing environments by carefully considering latency and cost. End users benefit from the platform through reduced cost and improved user experience in terms of response time and latency."
pub.1147085963,Neural Adaptive IoT Streaming Analytics with RL-Adapt,"The emerging IoT stream processing is a key enabling technology for the time-critical IoT applications, which often require high accuracy and low latency. Existing stream processing engines are insufficient to meet these requirements, since they could not integrate and respond timely to variable network conditions in the dynamic wireless environment. Recent efforts focusing on adaptive streaming support user-specified policies to adapt to the variable network conditions. However, those manual-policies can hardly achieve optimal performance across a broad set of network conditions and quality of experience (QoE) objectives. In this paper, we present a Reinforcement Learning-based Adaptive streaming system (RL-Adapt) that is capable of generating adaption policies using RL-strategy and providing declarative APIs for efficient development. RL-Adapt trains a neural network model that can automatically select the optimal policy based on the observed network conditions. RL-Adapt does not rely on pre-defined models or assumptions on the environment. Instead, it learns to make decisions solely through observations of the resulting performance of past decisions. We implemented RL-Adapt and evaluated its performance extensively in three representative real-world IoT applications. Our results show that RL-Adapt outperforms the state-of-the-art scheme, with 20% improvements on average QoE."
pub.1061280719,Predictable Low-Latency Event Detection With Parallel Complex Event Processing,"The tremendous number of sensors and smart objects being deployed in the Internet of Things (IoT) pose the potential for IT systems to detect and react to live-situations. For using this hidden potential, complex event processing (CEP) systems offer means to efficiently detect event patterns (complex events) in the sensor streams and therefore, help in realizing a “distributed intelligence” in the IoT. With the increasing number of data sources and the increasing volume at which data is produced, parallelization of event detection is crucial to limit the time events need to be buffered before they actually can be processed. In this paper, we propose a pattern-sensitive partitioning model for data streams that is capable of achieving a high degree of parallelism in detecting event patterns, which formerly could only consistently be detected in a sequential manner or at a low parallelization degree. Moreover, we propose methods to dynamically adapt the parallelization degree to limit the buffering imposed on event detection in the presence of dynamic changes to the workload. Extensive evaluations of the system behavior show that the proposed partitioning model allows for a high degree of parallelism and that the proposed adaptation methods are able to meet a buffering limit for event detection under high and dynamic workloads."
pub.1163156429,Neural adaptive IoT streaming analytics with RL-Adapt,"IoT stream processing is an emerging technology that plays a crucial role in enabling time-critical IoT applications, which often demand high accuracy and low latency. For example, Augmented Reality (AR) applications require high object detection precision and object localization precision, as well as low latency. Note that the exact definition of accuracy varies depending on the specific application Existing stream processing engines are insufficient to meet these requirements, since they could not integrate and respond timely to variable network conditions in the dynamic wireless environment. Recent efforts focusing on adaptive streaming support user-specified policies to adapt to the variable network conditions. However, existing works suffer from manual policies or simplified models of the deployment environment which limit their ability to achieve optimal performance across a broad set of network conditions and quality of experience (QoE) objectives. In this paper, we present a Reinforcement Learning-based Adaptive streaming system (RL-Adapt) that is capable of generating adaption policies using RL-strategy and providing declarative APIs for efficient development. RL-Adapt trains a neural network model that can automatically select the optimal policy based on the observed network conditions. RL-Adapt does not rely on pre-defined models or assumptions about the environment. Instead, it learns to make decisions solely through observations of the resulting performance of past decisions. We implemented RL-Adapt and evaluated its performance extensively in three representative real-world IoT applications. Our results show that RL-Adapt outperforms the state-of-the-art schemes in terms of QoE metrics."
pub.1174031385,AgileDART: An Agile and Scalable Edge Stream Processing Engine,"Edge applications generate a large influx of sensor data at massive scales.
Under many time-critical scenarios, these massive data streams must be
processed in a very short time to derive actionable intelligence. However,
traditional data processing systems (e.g., stream processing systems,
cloud-based IoT data processing systems) are not well-suited for these edge
applications. This is because they often do not scale well with a large number
of concurrent stream queries, do not support low-latency processing under
limited edge computing resources, and do not adapt to the level of
heterogeneity and dynamicity commonly present in edge computing environments.
These gaps suggest a need for a new edge stream processing system that advances
the stream processing paradigm to achieve efficiency and flexibility under the
constraints presented by edge computing architectures.
  We present AgileDart, an agile and scalable edge stream processing engine
that enables fast stream processing of a large number of concurrently running
low-latency edge applications' queries at scale in dynamic, heterogeneous edge
environments. The novelty of our work lies in a dynamic dataflow abstraction
that leverages distributed hash table (DHT) based peer-to-peer (P2P) overlay
networks to automatically place, chain, and scale stream operators to reduce
query latencies, adapt to workload variations, and recover from failures; and a
bandit-based path planning model that can re-plan the data shuffling paths to
adapt to unreliable and heterogeneous edge networks. We show analytically and
empirically that AgileDart outperforms Storm and EdgeWise on query latency and
significantly improves scalability and adaptability when processing a large
number of real-world edge stream applications' queries."
pub.1094407272,A Preventive Auto-Parallelization Approach for Elastic Stream Processing,"Nowadays, more and more sources (connected devices, social networks, etc.) emit real-time data with fluctuating rates over time. Existing distributed stream processing engines (SPE) have to resolve a difficult problem: deliver results satisfying end-users in terms of quality and latency without over-consuming resources. This paper focuses on parallelization of operators to adapt their throughput to their input rate. We suggest an approach which prevents operator congestion in order to limit degradation of results quality. This approach relies on an automatic and dynamic adaptation of resource consumption for each continuous query. This solution takes advantage of i) a metric estimating the activity level of operators in the near future ii) the AUTOSCALE approach which evaluates the need to modify parallelism degrees at local and global scope iii) an integration into the Apache Storm solution. We show performance tests comparing our approach to the native solution of this SPE."
pub.1095268207,Adaptive Class-Based Scheduling of Continuous Queries,"The emergence of Data Stream Management Systems (DSMS) facilitates implementing many types of monitoring applications via continuous queries (CQs). However, these applications usually have different quality-of-service requirements for different CQs. In this work, we are proposing the Adaptive Broadcast Disks (ABD) scheduler, a new scheduling policy which employs two-level scheduling that can handle different ranks of CQ classes. The ABD scheduler optimizes the weighted average response time of the CQ classes while still preserving the relative importance of each class. We demonstrate that ABD outperforms state-of-the-art schedulers and adapts to changes in the workload without manual intervention."
pub.1094420070,The Proactive Promotion Engine,"Given the nature of high volume streaming environments, not all tuples can be processed within the required response time. In such instances, it is crucial to dedicate resources to producing the most important results. We will demonstrate the Proactive Promotion Engine (PP) which employs a new preferential resource allocation methodology for priority processing of stream tuples. Our key contributions include: 1) our promotion continuous query language allows the specification of priorities within a query, 2) our promotion query algebra supports proactive promotion query processing, 3) our promotion query optimization locates an optimized PP query plan, and 4) our adaptive promotion control adapts online which subset of tuples are given priority online within a single physical query plan. Our “Portland Home Arrest” demonstration facilitates the capture of in-flight criminals using data generated by the Virginia Tech Network Dynamics and Simulation Science Laboratory via simulation-based modeling techniques."
pub.1106072342,Dual-Paradigm Stream Processing,"Existing stream processing frameworks operate either under data stream paradigm processing data record by record to favor low latency, or under operation stream paradigm processing data in micro-batches to desire high throughput. For complex and mutable data processing requirements, this dilemma brings the selection and deployment of stream processing frameworks into an embarrassing situation. Moreover, current data stream or operation stream paradigms cannot handle data burst efficiently, which probably results in noticeable performance degradation. This paper introduces a dual-paradigm stream processing, called DO (Data and Operation) that can adapt to stream data volatility. It enables data to be processed in micro-batches (i.e., operation stream) when data burst occurs to achieve high throughput, while data is processed record by record (i.e., data stream) in the remaining time to sustain low latency. DO embraces a method to detect data bursts, identify the main operations affected by the data burst and switch paradigms accordingly. Our insight behind DO's design is that the trade-off between latency and throughput of stream processing frameworks can be dynamically achieved according to data communication among operations in a fine-grained manner (i.e., operation level) instead of framework level. We implement a prototype stream processing framework that adopts DO. Our experimental results show that our framework with DO can achieve 5x speedup over operation stream under low data stream sizes, and outperforms data stream on throughput by 2.1x to 3.2x under data burst."
pub.1040230848,Tuning Performance of P2P Mesh Streaming System Using a Network Evolution Approach,"Resilience and startup delay are the most important performance metrics to evaluate the P2P streaming systems. To simultaneously improve the two metrics, we propose several mechanisms at different system evolution stages. At the first stage, media server encodes the stream into multiple sub-streams of the same length. Redundancy is introduced by using Reed-Solomon (RS) coding before distributing the sub-streams to different successors. Each peer in the network establishes a cooperative relationship with others to obtain all required sub-streams. At the stage of new peer arrival, a parent selection algorithm with relatively lower complexity is proposed which takes full advantage of redundant coding. After the peer builds up streaming transmission, it replaces some parents with a latency-based decision mechanism. In case of node failure, a swap-in-turn repairing algorithm between different sub-stream sources is proposed to ensure the high continuity of steaming transmission. Simulation results show that 1) the redundant coding and the parent replace algorithm in case of node failure can effectively reduce interruption of data streams; 2) the codes with higher redundant degree can adapt to more dynamic scenario. Meanwhile, the codes with redundancy does not significantly decrease the effective transmission ratio when network is dynamic; 3) transmission achieves higher performance when the number of substreams is between 8 and 16; and 4) the parent switching mechanism can significantly decrease the startup latency for a big proportion of peers."
pub.1154180247,Shepherd: Seamless Stream Processing on the Edge,"Next generation applications such as augmented/vir-tual reality, autonomous driving, and Industry 4.0, have tight latency constraints and produce large amounts of data. To address the real-time nature and high bandwidth usage of new applications, edge computing provides an extension to the cloud infrastructure through a hierarchy of datacenters located between the edge devices and the cloud. Outside of the cloud and closer to the edge, the network becomes more dynamic requiring stream processing frameworks to adapt more frequently. Cloud based frameworks adapt very slowly because they employ a stop-the-world approach and it can take several minutes to reconfigure jobs resulting in downtime. In this paper, we propose Shepherd, a new stream processing framework for edge computing. Shepherd minimizes downtime during application reconfiguration, with almost no impact on data processing latency. Our experiments show that, compared to Apache Storm, Shepherd reduces application downtime from several minutes to a few tens of milliseconds."
pub.1093344134,FFWD: Latency-Aware Event Stream Processing via Domain-Specific Load-Shedding Policies,"Tools and applications for event stream processing and real-time analytics are getting a huge hype these days on a wide range of application scenarios, from the smallest Internet of Things (IoT) embedded sensor to the most popular Social Network feed. Unfortunately, dealing with this kind of input rises some issues that can easily mine the real-time analysis requirement due to an unexpected overload of the system; this happens as the processing time may strongly depend on the single event content, while the event arrival rate may vary unpredictably over time. In this work, we propose Fast Forward With Degradation (FFWD), a latency-aware load shedding framework that exploits performance degradation techniques to adapt the throughput of the application to the size of the input, allowing the system to have a fast and reliable response time in case of overloading. Moreover, we show how different domain-specific policies can guarantee a reasonable accuracy of the aggregated output metrics."
pub.1138899733,The synergy of complex event processing and tiny machine learning in industrial IoT,"Focusing on comprehensive networking, the Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness in factory operations. Various intelligent sensors play a central role, as they generate a vast amount of real-time data that can provide insights into manufacturing. Complex event processing (CEP) and machine learning (ML) have been developed actively in the last years in IIoT to identify patterns in heterogeneous data streams and fuse raw data into tangible facts. In a traditional compute-centric paradigm, the raw field data are continuously sent to the cloud and processed centrally. As IIoT devices become increasingly pervasive, concerns are raised since transmitting such an amount of data is energy-intensive, vulnerable to be intercepted, and subjected to high latency. Decentralized on-device ML and CEP provide a solution where data is processed primarily on edge devices. Thus communications can be minimized. However, this is no mean feat because most IIoT edge devices are resource-constrained with low power consumption. This paper proposes a framework that exploits ML and CEP's synergy at the edge in distributed sensor networks. By leveraging tiny ML and CEP, we now shift the computation from the cloud to the resource-constrained IIoT devices and allow users to adapt on-device ML models and CEP reasoning rules flexibly on the fly. Lastly, we demonstrate the proposed solution and show its effectiveness and feasibility using an industrial use case of machine safety monitoring."
pub.1118773365,Foveated Video Streaming for Cloud Gaming,"Good user experience with interactive cloud-based multimedia applications,
such as cloud gaming and cloud-based VR, requires low end-to-end latency and
large amounts of downstream network bandwidth at the same time. In this paper,
we present a foveated video streaming system for cloud gaming. The system
adapts video stream quality by adjusting the encoding parameters on the fly to
match the player's gaze position. We conduct measurements with a prototype that
we developed for a cloud gaming system in conjunction with eye tracker
hardware. Evaluation results suggest that such foveated streaming can reduce
bandwidth requirements by even more than 50% depending on parametrization of
the foveated video coding and that it is feasible from the latency perspective."
pub.1023581144,Dynamic Load Balancing for Ordered Data-Parallel Regions in Distributed Streaming Systems,"Distributed stream computing has emerged as a technology that can satisfy the low latency, high throughput demands of big data. Stream computing naturally exposes pipeline, task and data parallelism. Meeting the throughput and latency demands of online big data requires exploiting such parallelism across heterogeneous clusters. When a single job is running on a homogeneous cluster, load balancing is important. When multiple jobs are running across a heterogeneous cluster, load balancing becomes critical. The data parallel regions of distributed streaming applications are particularly sensitive to load imbalance, as their overall speed is gated by the slowest performer. We propose a dynamic load balancing technique based on a system artifact: the TCP blocking rate per connection. We build a function for each connection based on this blocking rate, and obtain a balanced load distribution by modeling the problem as a minimax separable resource allocation problem. In other words, we minimize the maximum value of these functions. Our model achieves local load balancing that does not require any global information. We test our model in a real streaming system, and demonstrate that it is able to detect differences in node capacities, determine the correct load distribution for those capacities and dynamically adapt to changes in the system."
pub.1126567515,Near Real-Time Big Data Stream Processing Platform Using Cassandra,"Users are always impatient to get answers instantly from analytics system. If time to insight exceeds 10s of milliseconds, then the value is lost. Applications such as stock market, sensors, Twitter feed data or fraud detection can't afford to wait. This often means analyzing the inflow of data before it even stored to the database of records. Coupled with zero tolerance for data loss and the challenge gets even more daunting. In realtime Big Data scenario rather waiting for data to be collected as a whole at a long periodic interval, streaming analysis let us identify patterns and make informed decisions based on them-as data start arriving. When data are non-stationary, and patterns change with time, streaming systems adapt itself. This work describes near real-time data storage and processing approaches to analyze streams of data with respect to Cassandra NoSQL datastore. It provides an insight into optimizing Cassandra on a multi data center setup for near Real-Time Responses. The classic trade-off between low-latency and high-accuracy is conceptualized. The theoretical claims are corroborated with several thorough experimental analysis in Apache and Datastax distribution of Cassandra."
pub.1094268471,100 Gbit/s End-to-End Communication: Designing Scalable Protocols with Soft Real-Time Stream Processing,"With the recent roll-out of 100 Gbit Ethernet technology for high-performance computing applications and the technology for 100 Gbit wireless communication emerging on the horizon, it is just a matter of time until non-high performance computing applications will have to utilize these data rates. Since 10 Gbit/s protocol processing is already challenging for current server machines and simply upscaling the computing resources is no solution, new approaches are needed. In this paper, we present a stream processing based design approach for scalable communication protocols. The stream processing paradigm enables us to adapt the communication protocol processing for a certain hardware configuration without touching the protocol's implementation. We use this design technique to develop a prototype communication protocol for ultra-high throughput applications and we demonstrate how to adapt the protocol processing for a Stable Throughput as well as for a Low Latency scenario. Last but not least, we present the evaluation results of the experiments, which show that the measured throughput respectively latency of the adapted protocol, scales nearly linear with the number of provided interfaces."
pub.1095854003,Foveated Video Streaming for Cloud Gaming,"Good user experience with interactive cloud-based multimedia applications, such as cloud gaming and cloud-based VR, requires low end-to-end latency and large amounts of downstream network bandwidth at the same time. In this paper, we present a foveated video streaming system for cloud gaming. The system adapts video stream quality by adjusting the encoding parameters on the fly to match the player's gaze position. We conduct measurements with a prototype that we developed for a cloud gaming system in conjunction with eye tracker hardware. Evaluation results suggest that such foveated streaming can reduce bandwidth requirements by even more than 50% depending on parametrization of the foveated video coding and that it is feasible from the latency perspective."
pub.1030280444,Adaptive Speculative Processing of Out-of-Order Event Streams,"Distributed event-based systems are used to detect meaningful events with low latency in high data-rate event streams that occur in surveillance, sports, finances, etc. However, both known approaches to dealing with the predominant out-of-order event arrival at the distributed detectors have their shortcomings: buffering approaches introduce latencies for event ordering, and stream revision approaches may result in system overloads due to unbounded retraction cascades.
                  This article presents an adaptive speculative processing technique for out-of-order event streams that enhances typical buffering approaches. In contrast to other stream revision approaches developed so far, our novel technique encapsulates the event detector, uses the buffering technique to delay events but also speculatively processes a portion of it, and adapts the degree of speculation at runtime to fit the available system resources so that detection latency becomes minimal.
                  Our technique outperforms known approaches on both synthetical data and real sensor data from a realtime locating system (RTLS) with several thousands of out-of-order sensor events per second. Speculative buffering exploits system resources and reduces latency by 40% on average."
pub.1028205933,Reliable speculative processing of out-of-order event streams in generic publish/subscribe middlewares,"In surveillance, sports, finances, etc., distributed event-based systems are used to detect meaningful events with low latency in high data rate event streams. Both known approaches to deal with the predominant out-of-order event arrival at the distributed detectors have their shortcomings: buffering approaches introduce latencies for event ordering and stream revision approaches may result in system overloads due to unbounded retraction cascades. This paper presents a speculative processing technique for out-of-order event streams that enhances typical buffering approaches. In contrast to other stream revision approaches our novel technique encapsulates the event detector, uses the buffering technique to delay events but also speculatively processes a portion of it, and adapts the degree of speculation at runtime to fit the available system resources so that detection latency becomes minimal. Our technique outperforms known approaches on both synthetical data and real sensor data from a Realtime Locating System (RTLS) with several thousands of out-of-order sensor events per second. Speculative buffering exploits system resources and reduces latency by 40% on average."
pub.1127378981,Latency‐aware adaptive micro‐batching techniques for streamed data compression on graphics processing units,"Summary Stream processing is a parallel paradigm used in many application domains. With the advance of graphics processing units (GPUs), their usage in stream processing applications has increased as well. The efficient utilization of GPU accelerators in streaming scenarios requires to batch input elements in microbatches, whose computation is offloaded on the GPU leveraging data parallelism within the same batch of data. Since data elements are continuously received based on the input speed, the bigger the microbatch size the higher the latency to completely buffer it and to start the processing on the device. Unfortunately, stream processing applications often have strict latency requirements that need to find the best size of the microbatches and to adapt it dynamically based on the workload conditions as well as according to the characteristics of the underlying device and network. In this work, we aim at implementing latency‐aware adaptive microbatching techniques and algorithms for streaming compression applications targeting GPUs. The evaluation is conducted using the Lempel‐Ziv‐Storer‐Szymanski compression application considering different input workloads. As a general result of our work, we noticed that algorithms with elastic adaptation factors respond better for stable workloads, while algorithms with narrower targets respond better for highly unbalanced workloads."
pub.1095518382,Autonomic Parallel Data Stream Processing,"Data Stream Processing (Da SP) is a recent and highly active research field, applied in various real world scenarios. Differently than traditional applications, input data is seen as transient continuous streams that must be processed “on the fly”, with critical requirements on throughput, latency and memory occupancy. A parallel solution is often advocated, but the problem of designing and implementing high throughput and low latency DaSP applications is complex per se and because of the presence of multiple streams characterized by high volume, high velocity and high variability. Moreover, parallel DaSP applications must be able to adapt themselves to data dynamics in order to satisfy desired QoS levels. The aim of our work is to study these problems in an integrated way, providing to the programmers a methodological framework for the parallelization of DaSP applications."
pub.1022138468,Adaptive Stream Processing using Dynamic Batch Sizing,"The need for real-time processing of ""big data"" has led to the development of frameworks for distributed stream processing in clusters. It is important for such frameworks to be robust against variable operating conditions such as server failures, changes in data ingestion rates, and workload characteristics. To provide fault tolerance and efficient stream processing at scale, recent stream processing frameworks have proposed to treat streaming workloads as a series of batch jobs on small batches of streaming data. However, the robustness of such frameworks against variable operating conditions has not been explored. In this paper, we explore the effects of the batch size on the performance of streaming workloads. The throughput and end-to-end latency of the system can have complicated relationships with batch sizes, data ingestion rates, variations in available resources, workload characteristics, etc. We propose a simple yet robust control algorithm that automatically adapts the batch size as the situation necessitates. We show through extensive experiments that it can ensure system stability and low latency for a wide range of workloads, despite large variations in data rates and operating conditions."
pub.1095836752,Popularity-Aware Differentiated Distributed Stream Processing on Skewed Streams,"Real-world stream data with skewed distribution raises unique challenges to distributed stream processing systems. Existing stream workload partitioning schemes usually use a “one size fits all” design, which leverage either a shuffle grouping or a key grouping strategy for partitioning the stream workloads among multiple processing units, leading to notable problems of unsatisfied system throughput and processing latency. In this paper, we show that the key grouping based schemes result in serious load imbalance and low computation efficiency in the presence of data skewness while the shuffle grouping schemes are not scalable in terms of memory space. We argue that the key to efficient stream scheduling is the popularity of the stream data. We propose and implement a differentiated distributed stream processing system, call DStream, which assigns the popular keys using shuffle grouping while assigns unpopular ones using key grouping. We design a novel efficient and light-weighted probabilistic counting scheme for identifying the current hot keys in dynamic real-time streams. Two factors contribute to the power of this design: 1) the probabilistic counting scheme is extremely computation and memory efficient, so that it can be well integrated in processing instances in the system; 2) the scheme can adapt to the popularity changes in the dynamic stream processing environment. We implement the DStream system on top of Apache Storm. Experiment results using large-scale traces from real-world systems show that DStream achieves a 2.3 × improvement in terms of processing throughput and reduces the processing latency by 64% compared to state-of-the-art designs."
pub.1137884519,The Synergy of Complex Event Processing and Tiny Machine Learning in Industrial IoT,"Focusing on comprehensive networking, big data, and artificial intelligence,
the Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness
in factory operations. Various sensors and field devices play a central role,
as they generate a vast amount of real-time data that can provide insights into
manufacturing. The synergy of complex event processing (CEP) and machine
learning (ML) has been developed actively in the last years in IIoT to identify
patterns in heterogeneous data streams and fuse raw data into tangible facts.
In a traditional compute-centric paradigm, the raw field data are continuously
sent to the cloud and processed centrally. As IIoT devices become increasingly
pervasive and ubiquitous, concerns are raised since transmitting such amount of
data is energy-intensive, vulnerable to be intercepted, and subjected to high
latency. The data-centric paradigm can essentially solve these problems by
empowering IIoT to perform decentralized on-device ML and CEP, keeping data
primarily on edge devices and minimizing communications. However, this is no
mean feat because most IIoT edge devices are designed to be computationally
constrained with low power consumption. This paper proposes a framework that
exploits ML and CEP's synergy at the edge in distributed sensor networks. By
leveraging tiny ML and micro CEP, we shift the computation from the cloud to
the power-constrained IIoT devices and allow users to adapt the on-device ML
model and the CEP reasoning logic flexibly on the fly without requiring to
reupload the whole program. Lastly, we evaluate the proposed solution and show
its effectiveness and feasibility using an industrial use case of machine
safety monitoring."
pub.1128045903,Prompt: Dynamic Data-Partitioning for Distributed Micro-batch Stream Processing Systems,"Advances in real-world applications require high-throughput processing over large data streams. Micro-batching has been proposed to support the needs of these applications. In micro-batching, the processing and batching of the data are interleaved, where the incoming data tuples are first buffered as data blocks, and then are processed collectively using parallel function constructs (e.g., Map-Reduce). The size of a micro-batch is set to guarantee a certain response-time latency that is to conform to the application's service-level agreement. In contrast to tuple-at-a-time data stream processing, micro-batching has the potential to sustain higher data rates. However, existing micro-batch stream processing systems use basic data-partitioning techniques that do not account for data skew and variable data rates. Load-awareness is necessary to maintain performance and to enhance resource utilization. A new data partitioning scheme termed Prompt is presented that leverages the characteristics of the micro-batch processing model. In the batching phase, a frequency-aware buffering mechanism is introduced that progressively maintains run-time statistics, and provides online key-based sorting as data tuples arrive. Because achieving optimal data partitioning is NP-Hard in this context, a workload-aware greedy algorithm is introduced that partitions the buffered data tuples efficiently for the Map stage. In the processing phase, a load-aware distribution mechanism is presented that balances the size of the input to the Reduce stage without incurring inter-task communication overhead. Moreover, Prompt elastically adapts resource consumption according to workload changes. Experimental results using real and synthetic data sets demonstrate that Prompt is robust against fluctuations in data distribution and arrival rates. Furthermore, Prompt achieves up to 200% improvement in system throughput over state-of-the-art techniques without degradation in latency."
pub.1130371931,PStream: A Popularity-Aware Differentiated Distributed Stream Processing System,"Real-world stream data with skewed distributions raises unique challenges to distributed stream processing systems. Existing stream workload partitioning schemes usually use a “one size fits all” design, which leverages either a shuffle grouping or a key grouping strategy for partitioning the stream workloads among multiple processing units, leading to notable problems of unsatisfied system throughput and processing latency. In this article, we show that the key grouping based schemes result in serious load imbalance and low computation efficiency in the presence of data skewness while the shuffle grouping schemes are not scalable in terms of memory space. We argue that the key to efficient stream scheduling is the popularity of the stream data. We propose PStream, a popularity-aware differentiated distributed stream processing system which assigns the hot keys using shuffle grouping while assigns rare ones using key grouping. PStream leverages a novel light-weighted probabilistic counting scheme for identifying the currently hot keys in dynamic real-time streams. The scheme is extremely efficient in computation and memory consumption, so that the predictor based on it can be well integrated into processing instances in the system. We further design an adaptive threshold configuration scheme, which can quickly adapt to the dynamical popularity changes in highly dynamical real-time streams. We implement PStream on top of Apache Storm and conduct comprehensive experiments using large-scale traces from real-world systems to evaluate the performance of this design. Results show that PStream achieves a 2.3× improvement in terms of processing throughput and reduces the processing latency by 64 percent compared to state-of-the-art designs."
pub.1005124287,Biologically-Inspired Distributed Middleware Management for Stream Processing Systems,"We present a decentralized and dynamic biologically-inspired algorithm for placing dataflow graphs composed of stream processing tasks onto a distributed network of machines, while minimizing the end-to-end latency. Our algorithm responds on-the-fly to placement requests of new flow graphs or to modifications of an already running stream processing flow graph, and dynamically adapts to changes in performance characteristics such as message rates or service times as well as to changes in processor availability or link performance during runtime. Our algorithm is derived by analogy to pheromone-based cooperation between ants to fulfill goals such as food discovery. We have conducted extensive simulation experiments to show the scalability and adaptability of our algorithm."
pub.1115049134,Reliable stream data processing for elastic distributed stream processing systems,"Distributed stream processing system (DSPS) has proven to be an effective way to process and analyze large-scale data streams in real-time fashions. The reliability problem of DSPS is becoming a popular topic in recent years. Novel elastic DSPSs provide the ability to seamlessly adapt to stream workload changes, which introduce new reliability challenges: (1) operators can be scaled up and down at runtime, requiring fault tolerant methods to maintain data backup consistency under the runtime dynamics. (2) Rollback recovery to the last checkpoint may undo recent auto-scaling adjustments, which will introduce high cost and unacceptable impact to the system. In this paper, we put forward a novel fault-tolerant mechanism to deal with these issues. In particular, we propose a self-adaptive backup unit, elastic data slice (EDS), that can partition and merge data backups according to operator auto-scaling at runtime. The consistency of recovery is guaranteed by new upstream backup protocols, which restart the system from the status after auto-scaling instead of last checkpoint and avoid high recovery latency. Based on them, we implement a prototype system named SPATE. Evaluations on SPATE show that our mechanism supports auto-scaling changes with similar overhead compared to existing approaches, while achieving low recovery latency despite auto-scaling."
pub.1118960775,Henge: Intent-driven Multi-Tenant Stream Processing,"We present Henge, a system to support intent-based multi-tenancy in modern
stream processing applications. Henge supports multi-tenancy as a first-class
citizen: everyone inside an organization can now submit their stream processing
jobs to a single, shared, consolidated cluster. Additionally, Henge allows each
tenant (job) to specify its own intents (i.e., requirements) as a Service Level
Objective (SLO) that captures latency and/or throughput. In a multi-tenant
cluster, the Henge scheduler adapts continually to meet jobs' SLOs in spite of
limited cluster resources, and under dynamic input workloads. SLOs are soft and
are based on utility functions. Henge continually tracks SLO satisfaction, and
when jobs miss their SLOs, it wisely navigates the state space to perform
resource allocations in real time, maximizing total system utility achieved by
all jobs in the system. Henge is integrated in Apache Storm and we present
experimental results using both production topologies and real datasets."
pub.1107307625,Henge,"We present Henge, a system to support intent-based multi-tenancy in modern distributed stream processing systems. Henge supports multi-tenancy as a first-class citizen: everyone in an organization can now submit their stream processing jobs to a single, shared, consolidated cluster. Secondly, Henge allows each job to specify its own intents (i.e., requirements) as a Service Level Objective (SLO) that captures latency and/or throughput needs. In such an intent-driven multi-tenant cluster, the Henge scheduler adapts continually to meet jobs' respective SLOs in spite of limited cluster resources, and under dynamically varying workloads. SLOs are soft and are based on utility functions. Henge's overall goal is to maximize the total system utility achieved by all jobs in the system. Henge is integrated into Apache Storm and we present experimental results using both production jobs from Yahoo! and real datasets from Twitter."
pub.1133450913,WASP,"Adaptability is critical for stream processing systems to ensure stable, low-latency, and high-throughput processing of long-running queries. Such adaptability is particularly challenging for wide-area stream processing due to the highly dynamic nature of the wide-area environment, which includes unpredictable workload patterns, variable network bandwidth, occurrence of stragglers, and failures. Unfortunately, existing adaptation techniques typically achieve these performance goals by compromising the quality/accuracy of the results, and they are often application-dependent. In this work, we rethink the adaptability property of wide-area stream processing systems and propose a resource-aware adaptation framework, called WASP. WASP adapts queries through a combination of multiple techniques: task re-assignment, operator scaling, and query re-planning, and applies them in a WAN-aware manner. It is able to automatically determine which adaptation action to take depending on the type of queries, dynamics, and optimization goals. We have implemented a WASP prototype on Apache Flink. Experimental evaluation with the YSB benchmark and a real Twitter trace shows that WASP can handle various dynamics without compromising the quality of the results."
pub.1106072382,AWStream,"The emerging class of wide-area streaming analytics faces the challenge of scarce and variable WAN bandwidth. Non-adaptive applications built with TCP or UDP suffer from increased latency or degraded accuracy. State-of-the-art approaches that adapt to network changes require developer writing sub-optimal manual policies or are limited to application-specific optimizations. We present AWStream, a stream processing system that simultaneously achieves low latency and high accuracy in the wide area, requiring minimal developer efforts. To realize this, AWStream uses three ideas: (i) it integrates application adaptation as a first-class programming abstraction in the stream processing model; (ii) with a combination of offline and online profiling, it automatically learns an accurate profile that models accuracy and bandwidth trade-off; and (iii) at runtime, it carefully adjusts the application data rate to match the available bandwidth while maximizing the achievable accuracy. We evaluate AWStream with three real-world applications: augmented reality, pedestrian detection, and monitoring log analysis. Our experiments show that AWStream achieves sub-second latency with only nominal accuracy drop (2-6%)."
pub.1110420290,Network-Aware Grouping in Distributed Stream Processing Systems,"Distributed Stream Processing (DSP) systems have recently attracted much attention because of their ability to process huge volumes of real-time stream data with very low latency on clusters of commodity hardware. Existing workload grouping strategies in a DSP system can be classified into four categories (i.e. raw and blind, data skewness, cluster heterogeneity, and dynamic load-aware). However, these traditional stream grouping strategies do not consider network distance between two communicating operators. In fact, the traffic from different network channels makes a significant impact on performance. How to grouping tuples according to network distances to improve performance has been a critical problem.In this paper, we propose a network-aware grouping framework called Squirrel to improve the performance under different network distances. Identifying the network location of two communicating operators, Squirrel sets a weight and priority for each network channel. It introduces Weight Grouping to assign different numbers of tuples to each network channel according to channel’s weight and priority. In order to adapt to changes in network conditions, input load, resources and other factors, Squirrel uses Dynamic Weight Control to adjust network channel’s weight and priority online by analyzing runtime information. Experimental results prove Squirrel’s effectiveness and show that Squirrel can achieve 1.67x improvement in terms of throughput and reduce the latency by 47%."
pub.1128542729,OSCAR: An Optical Stochastic Computing AcceleRator for Polynomial Functions,"Approximate computing allows improving design energy efficiency at the cost of computing accuracy. Stochastic computing is an approximate computing technique, where numbers are represented as probabilities using stochastic bit streams. The serial processing of the bit streams leads to reduced hardware complexity but induces high processing latency. Silicon photonics has the potential to overcome this limitation thanks to high propagation speed of signals and high bandwidth. However, the technology remains costly, which calls for optical accelerators capable to adapt to application-specific requirements. In this paper, we propose a reconfigurable optical accelerator capable to adapt to computing accuracy, energy efficiency, and throughput objectives. The architecture can be configured to execute i) 4th order function for high accuracy processing or ii) 2nd order function for high-energy efficiency or high throughput purposes. Evaluations are carried out using image processing Gamma correction application. Compared to a static architecture for which accuracy is defined at design time, the proposed architecture leads to 36.8% energy overhead but increases the range of reachable accuracy by 65%."
pub.1090855694,Scalable Online Analytics on Cloud Infrastructures,"The need for low latency analysis of high velocity real time continuous data streams has led to the emergence of Stream Processing Systems (SPSs). Contemporary SPSs allow a stream processing application to be hosted on Cloud infrastructures and dynamically scaled so as to adapt to the fluctuating data rates. However, the run time scalability incorporated in these SPSs are in their early adaptations and are based on simple local/global threshold based controls. This work studies the issues with the local and global auto scaling techniques that may lead to performance inefficiencies in real time traffic analysis on Cloud platforms and presents an efficient hybrid auto scaling strategy StreamScale which addresses the identified issues. The proposed StreamScale auto-scaling algorithm accounts for the gaps in the local/global scaling approaches and effectively identifies (de)parallelization opportunities in stream processing applications for maintaining QoS at reduced costs. Simulation based experimental evaluation on representative stream application topologies indicate that the proposed StreamScale auto-scaling algorithm exhibits better performance in comparison to both local and global auto-scaling approaches."
pub.1145130981,A Multi-Metric Adaptive Stream Processing System,"Stream processing systems (SPS) have to deal with highly dynamic scenarios where its adaptation is mandatory in order to accomplish realistic applications requirements. In this work, we propose a new adaptive SPS for real-time processing that, based on input data rate variation, dynamically adapts the number of active operator replicas. Our SPS extends Storm by pre-allocating, for each operator, a set of inactive replicas which are activated (or deactivated) when necessary without the Storm reconfiguration cost. We exploit the MAPE model and define a new metric that aggregates the value of multiple metrics to dynamically changes the number of replicas of an operator. We deploy our SPS over Google Cloud Platform and results confirm that our metric can tolerate highly dynamic conditions, improving resource usage while preserving high throughput and low latency."
pub.1118328435,The Power of Both Choices: Practical Load Balancing for Distributed Stream Processing Engines,"We study the problem of load balancing in distributed stream processing
engines, which is exacerbated in the presence of skew. We introduce Partial Key
Grouping (PKG), a new stream partitioning scheme that adapts the classical
""power of two choices"" to a distributed streaming setting by leveraging two
novel techniques: key splitting and local load estimation. In so doing, it
achieves better load balancing than key grouping while being more scalable than
shuffle grouping. We test PKG on several large datasets, both real-world and
synthetic. Compared to standard hashing, PKG reduces the load imbalance by up
to several orders of magnitude, and often achieves nearly-perfect load balance.
This result translates into an improvement of up to 60% in throughput and up to
45% in latency when deployed on a real Storm cluster."
pub.1095544345,The Power of Both Choices: Practical Load Balancing for Distributed Stream Processing Engines,"We study the problem of load balancing in distributed stream processing engines, which is exacerbated in the presence of skew. We introduce Partial Key Grouping (pkg), a new stream partitioning scheme that adapts the classical “power of two choices” to a distributed streaming setting by leveraging two novel techniques: key splitting and local load estimation. In so doing, it achieves better load balancing than key grouping while being more scalable than shuffle grouping. We test PKG on several large datasets, both real-world and synthetic. Compared to standard hashing, PKG reduces the load imbalance by up to several orders of magnitude, and often achieves nearly-perfect load balance. This result translates into an improvement of up to 60% in throughput and up to 45% in latency when deployed on a real Storm cluster."
pub.1165570753,An adaptive load balancing strategy for stateful join operator in skewed data stream environments,"As one of the most computationally intensive operations in stream processing applications, join operation can cause severe load imbalance problem when dealing with skewed data. Most of the popular solutions focused on monitoring-based dynamic balancing strategies, making it difficult to quickly adapt to the changing frequency of data stream, and sometimes failing the balancing strategies that try to address the skewed load in the cluster. To address these issues, we propose to use the prediction results of a deep reinforcement learning model and adjust the grouping strategy in advance before the frequency change of data stream. It will enable the system to quickly adapt to data stream fluctuation, while managing the resources for effective resource utilization. The following contributions are made in this paper: 1) Explore the main factors that trigger the load skewness problem in distributed stream join systems and carefully model the load balancing problem at the application level. 2) Develop a Gated Recurrent Unit Sequence to Sequence model to predict key frequency distribution of streams, and propose a dynamic grouping algorithm and a feedback-based resource elasticity scaling algorithm to solve the load imbalance problem caused by hot keys in real time. 3) Design and implement an adaptive stream join system Aj-Stream based on the prediction model and the proposed algorithm on Apache Storm. 4) Evaluate the system performance through extensive experiments on a large scale real-world dataset and multiple synthetic datasets. The experimental results demonstrate that the Aj-Stream proposed in this paper exhibits stable throughput and latency performance with both static data streams of varying skewnesses and dynamic data streams. In comparison to existing stream-connected systems, Aj-Stream demonstrated a 22.1% increase in system throughput and a 45.5% decrease in system latency when dealing with frequently fluctuating data streams."
pub.1156368923,ANMS: Asynchronous Non-Maximum Suppression in Event Stream,"The non-maximum suppression (NMS) is widely used in frame-based tasks as an
essential post-processing algorithm. However, event-based NMS either has high
computational complexity or leads to frequent discontinuities. As a result, the
performance of event-based corner detectors is limited. This paper proposes a
general-purpose asynchronous non-maximum suppression pipeline (ANMS), and
applies it to corner event detection. The proposed pipeline extract fine
feature stream from the output of original detectors and adapts to the speed of
motion. The ANMS runs directly on the asynchronous event stream with extremely
low latency, which hardly affects the speed of original detectors.
Additionally, we evaluate the DAVIS-based ground-truth labeling method to fill
the gap between frame and event. Evaluation on public dataset indicates that
the proposed ANMS pipeline significantly improves the performance of three
classical asynchronous detectors with negligible latency. More importantly, the
proposed ANMS framework is a natural extension of NMS, which is applicable to
other asynchronous scoring tasks for event cameras."
pub.1174739857,Emma: Elastic Multi-Resource Management for Realtime Stream Processing,"In stream processing applications, an operator is often instantiated into multiple parallel execution instances, referred to as executors, to facilitate large-scale data processing. Due to unpredictable changes in executor workloads, data tuples processed by different executors may exhibit varying latency. In particular, within the same operator, the executor with the maximum latency significantly impacts the end-to-end (E2E) latency of the application. Existing solutions, such as load balancing and horizontal scaling, which involve workload migration, often incur substantial time overhead induced by state migration and synchronization. In contrast, elastically scaling up/down resources of executors rather than moving workloads can not only effectively handle workload fluctuations but also offer rapid adjustments; however, prior works only considered CPU scaling with the assumption of sufficient memory. In this paper, we propose Emma, an elastic multi-resource manager. Emma leverages the resource elasticity of lightweight virtualization containers, e.g., Linux containers, to resize the resource of executors at runtime. The core of Emma is a multi-resource provisioning plan that conducts performance analysis and resource adjustment in real-time. We explore the relationship between resources and performance experimentally and theoretically, guiding the plan to adaptively allocate the appropriate combination of resources to each executor to 1) accommodate the dynamic workload; 2) efficiently utilize resources to enhance the performance of as many executors as possible. Additionally, we propose an online learning method that makes the manager seamlessly adapt to diverse stream applications. We integrate Emma with Apache Samza, and our experiments show that compared to existing solutions, Emma can significantly reduce latency by orders of magnitude in real-world applications."
pub.1112461494,State and runtime-aware scheduling in elastic stream computing systems,"State and runtime-aware scheduling is one of the problems that is hard to resolve in elastic big data stream computing systems, as the state of each vertex is different, and the arrival rate of data streams fluctuates over time. A state and runtime-aware scheduling framework should be able to dynamically adapt to the fluctuation of the arrival rate of data streams and be aware of vertex states and resource availability. Currently, there is an increasing number of research work focusing on application scheduling in stream computing systems, however, this problem is still far from being completely solved. In this paper, we focus on the state of vertex in applications and the runtime feature of resources in a data center, and propose a state and runtime-aware scheduling framework (Sra-Stream) for elastic streaming computing systems, which incorporates the following features: (1) Profiling mathematical relationships between the system response time and the arrival rate of data streams, and identifying relevant resource constraints to meet the low response time and high throughput objectives. (2) Classifying vertex into stateless vertex or stateful vertex from a quantitative perspective, and achieving vertex parallelization by considering the state of the vertex. (3) Demonstrating a proposed stream application scheduling scheme consisting of a modified first-fit based runtime-aware data tuple scheduling strategy at the initial stage, and a maximum latency-sensitive based runtime-aware data stream scheduling strategy at the online stage, by considering the current scheduling status of the application. (4) Evaluating the achievement levels of low response time and high throughput objectives in a real-world elastic stream computing system. Experimental results conclusively demonstrate that the proposed Sra-Stream provides significant performance improvements on achieving the low system response time and high system throughput."
pub.1128045373,R-MStorm: A Resilient Mobile Stream Processing System for Dynamic Edge Networks,"Mobile Stream Processing (MSP) provides a promising approach to run computation-intensive stream applications, e.g., video face recognition, on a cluster of mobile devices at the edge. However, the performance of MSP is severely restricted by the fluctuating bandwidth and intermittent connectivity of the wireless networks connecting those devices. Therefore, to achieve a good MSP performance, implementing a resilient MSP system that adapts to dynamic edge networks is essential. In this paper, we present R-MStorm, a resilient MSP system deployed at the edge. R-MStorm improves the system survivability by (1) assigning tasks to mobile devices with higher availability to improve the availability of whole system; (2) assigning tasks of the same application components to different devices to increase the diversity of physical stream paths. Besides, to efficiently divide the output of upstream tasks to downstream tasks, R-MStorm adopts adaptive stream grouping , which considers both the transmission rate to and processing rate at each downstream task. Moreover, to alleviate congestion caused by network disconnection and stream redirection, adaptive stream selection is applied to skip some data to achieve a short response time. We conduct extensive experiments on R-MStorm by executing a video face recognition App under different network conditions. The experimental results show that, compared with baseline approaches, R-MStorm achieves up to 1.5x higher throughput, 75% lower response time, at a cost of 3.3% accuracy loss."
pub.1181267673,CLAP: Cost and Latency-Aware Placement of microservices on the computing continuum,"For microservices-based real-time stream processing applications, computing at the edge delivers fast responses for low workloads, but as workload increases, the response time starts to slow down due to limited compute capacity. Abundant compute capacity in the cloud delivers fast responses even for higher workloads, but incurs very high cost of operation. For applications which can tolerate latencies up to a certain limit, using either of them has one or the other drawback and for different applications and edge infrastructures, it is non-trivial to decide when to use only edge resources and when to leverage cloud resources. In this paper, we propose CLAP, which dynamically understands the relationship between workload and application latency, and automatically adjusts placement of microservices across edge and cloud computing continuum, with the goal of jointly reducing latency as well as cost of running microservices-based streaming applications. CLAP leverages Reinforcement Learning (RL) technique to learn the optimal placement for a given workload and based on the learnings, adjusts placement of microservices as the application workload changes. We conduct experiments with real-world video analytics applications and show that CLAP adapts placement of microservices in response to varying workloads and achieves low latency for applications in a cost-efficient manner. Particularly, we show that for two real-world video analytics applications i.e. human attributes and face recognition, CLAP is able to reduce average cost (across 4 days at different locations) by 47% and 58% for human attributes detection and face recognition application, respectively, while consistently maintaining latency below the tolerable limit."
pub.1125159573,Dynamic Tuple Scheduling with Prediction for Data Stream Processing Systems,"For data stream processing systems such as Apache Heron, workload imbalance across processing instances often causes significant system performance degradation. To mitigate such issues, Apache Heron leverages a naive throttling-based back-pressure scheme, which may lead to unexpected system disruption. This calls for a finer-grained control to distribute data stream units (tuples) between successive instances, a.k.a. tuple scheduling, which well adapts to data stream variations and workload discrepancy. Besides, the benefits of predictive scheduling to data stream processing systems still remain unexplored. In this paper, we formulate tuple scheduling problem as a stochastic network optimization problem, with careful choices in the granularity of system modeling and decision making. With non-trivial transformation, we decouple the problem into a series of online subproblems. By exploiting unique subproblem structure, we propose POTUS, an efficient, online, and distributed scheduling scheme that employs the power of predictive scheduling but requires only limited system dynamics to achieve a tunable trade-off between communication cost reduction and system queue stability. Theoretical analysis and simulations show that POTUS effectively shortens response time with mild-value of future information, even in face of mis-prediction. Our solution is also applicable to other data stream processing systems."
pub.1061296841,Peer-to-Peer Live Multicast: A Video Perspective,"Peer-to-peer multicast is promising for large-scale streaming video distribution over the Internet. Viewers contribute their resources to a peer-to-peer overlay network to act as relays for the media streams, and no dedicated infrastructure is required. As packets are transmitted over long, unreliable multipeer transmission paths, it is particularly challenging to achieve consistently high video quality and low end-to-end delay. In this paper, we focus on error-resilient transport for peer-to-peer video streaming. The algorithms we describe are representative of three broad categories of robust video streaming schemes: forward error correction, multiple descriptions, and prioritized automatic repeat request. We analyze how these techniques can be employed for live peer-to-peer multicast and discuss their relative merits. Our results show that significant gains can be obtained when systems are designed to adapt to the encoding structure of the video streams they are transmitting. They also reveal the importance of avoiding congestion at every peer participating in the multicast to obtain a low-latency system. Finally, we provide insights as to which are the important metrics to compare different peer-to-peer streaming systems."
pub.1117328355,Haren,"In modern Stream Processing Engines (SPEs), numerous diverse applications, which can differ in aspects such as cost, criticality or latency sensitivity, can co-exist in the same computing node. When these differences need to be considered to control the performance of each application, custom scheduling of operators to threads is of key importance (e.g., when a smart vehicle needs to ensure that safety-critical applications always have access to computational power, while other applications are given lower, variable priorities). Many solutions have been proposed regarding schedulers that allocate threads to operators to optimize specific metrics (e.g., latency) but there is still lack of a tool that allows arbitrarily complex scheduling strategies to be seamlessly plugged on top of an SPE. We propose Haren to fill this gap. More specifically, we (1) formalize the thread scheduling problem in stream processing in a general way, allowing to define ad-hoc scheduling policies, (2) identify the bottlenecks and the opportunities of scheduling in stream processing, (3) distill a compact interface to connect Haren with SPEs, enabling rapid testing of various scheduling policies, (4) illustrate the usability of the framework by integrating it into an actual SPE and (5) provide a thorough evaluation. As we show, Haren makes it is possible to adapt the use of computational resources over time to meet the goals of a variety of scheduling policies."
pub.1101062623,Model-based Scheduling for Stream Processing Systems,"Stream processing is emerging to react to the changing business situations of real-time processing. The main aim of this paradigm is to deal with the huge volume of data in the format of information flows originating from distributed devices. This consequently poses challenges to the scheduling problem in cloud data centers regarding the time-varying velocity of data ingesting and processing. In response to the uncertainties and complexities of streaming data, we propose a model-based scheduling scheme for stream processing systems, capturing the system behavior and providing an optimal allocation strategy to adapt to the changing work conditions. The proposed scheduling policy is implemented in Apache Storm, and micro-benchmarks with various shapes (e.g line, star, and diamond) were used in the evaluation. A topology that tracks trending topics on Twitter is also used, where the input is feeding with tweets in realtime. Experimental results show that the proposed solution can perform estimations that are well aligned with the system performance. The proposed scheduling policy achieves an improved performance with regards throughput and latency under varying ingesting rates."
pub.1132062169,Big Data Velocity Management–From Stream to Warehouse via High Performance Memory Optimized Index Join,"Efficient resource optimization is critical to manage the velocity and volume of real-time streaming data in near-real-time data warehousing and business intelligence. This article presents a memory optimisation algorithm for rapidly joining streaming data with persistent master data in order to reduce data latency. Typically during the transformation phase of ETL (Extraction, Transformation, and Loading) a stream of transactional data needs to be joined with master data stored on disk. To implement this process, a semi-stream join operator is commonly used. Most semi-stream join operators cache frequent parts of the master data to improve their performance, this process requires careful distribution of allocated memory among the components of the join operator. This article presents a cache inequality approach to optimise cache size and memory. To test this approach, we present a novel Memory Optimal Index-based Join (MOIJ) algorithm. MOIJ supports many-to-many types of joins and adapts to dynamic streaming data. We also present a cost model for MOIJ and compare the performance with existing algorithms empirically as well as analytically. We envisage the enhanced ability of processing near-real-time streaming data using minimal memory will reduce latency in processing big data and will contribute to the development of high-performance real-time business intelligence systems."
pub.1100893786,Stateful Load Balancing for Parallel Stream Processing,"Timely processing of streams in parallel requires dynamic load balancing to diminish skewness of data. In this paper we study this problem for stateful operators with key grouping for which the process of load balancing involves a lot of state movements. Consequently, load balancing is a bi-objective optimization problem, namely Minimum-Cost-Load-Balance (MCLB). We address MCLB with two approximate algorithms by a certain relaxation of the objectives: (1) a greedy algorithm ELB performs load balancing eagerly but relaxes the objective of load imbalance to a range; and (2) a periodic algorithm CLB aims at reducing load imbalance via a greedy procedure of minimizing the covariance of substreams but ignores the objective of state movement by amortizing the overhead of it over a relative long period. We evaluate our approaches with both synthetic and real data. The results show that they can adapt effectively to load variations and improve latency efficiently comparing to the existing solutions whom ignored the overhead of state movement in stateful load balancing."
pub.1092910784,Lever,"With the vast involvement of streaming big data in many applications (e.g., stock market data, sensor data, social network data, etc.), quickly mining and analyzing such data is becoming more and more important. To provide fault tolerance and efficient stream processing at scale, recent stream processing frameworks have proposed to adapt batch processing systems, such as MapReduce and Spark, to handle streaming data by putting the streams into micro-batches and treating the workloads as a continuous series of small jobs [1]. The fundamental challenge of building a batched stream processing system is to minimize the processing latency of each micro-batch. In this paper, we focus on the straggler problem, where a subset of workers are straggling behind and significantly affecting the job completion time. The straggler problem is a well-known critical problem in parallel processing systems. In comparing to large batch processing, the straggler problems in micro-batch processing are more severe and harder to tackle. We argue that the problem of using the existing straggler mitigation solutions for micro-batch processing is that they detect (or predict) stragglers and re-schedule stragglers too late in the data handling pipeline. The re-scheduling actions are carried out during the task execution period, hence it would inevitably increase the processing time of the micro-batches. Furthermore, as the data have already been dispatched, re-scheduling would inherently incur expensive data relocation. Such overhead would become significant in micro-batch processing due to the short processing time of each micro-batch. We refer to this type of methods as post-scheduling techniques. To address the problem, we propose a new pre-scheduling framework, called Lever, which predicts stragglers and makes timely scheduling decisions to minimize the processing latency. As shown in Figure 1, Lever periodically collects and analyzes the historical job profiles of the recurring micro-batch jobs. Based on such information, Lever pre-schedules the data through three main steps, i.e. identify potential stragglers, evaluate node capacity and choose suitable helpers. More importantly, Lever makes the re-scheduling decisions before the batching module dispatches the data. As the scheduling is done while the data are being batched, it would not increase the processing time of the micro-batch. We implemented Lever in Spark Streaming, which has been contributed to the open source community as an extension of Apache Spark Streaming. To the best of our knowledge, this is the first work specifically addressing the straggler problem in continuous micro-batch processing. We conduct various experiments to validate the effectiveness of Lever. The experimental results demonstrate that Lever reduces job completion time by 30.72% to 42.19% and outperforms traditional techniques significantly."
pub.1144949650,Naive Importance Weighting for Data Stream with Intermediate Latency,"In the literature, it is often assumed that given a data stream, every instance is provided with its correct label after the prediction for performance evaluation (i.e., the null latency scenario). However, in several real applications, this assumption is invalid. For example, in the problem of predicting if it will rain tomorrow, the actual label will only be available the day after tomorrow, i.e., the actual label will be made available with an unavoidable delay. This scenario is called intermediate latency. Unfortunately, learning with intermediate latency is little investigated in the literature. Furthermore, the data stream can be non-stationary, i.e., their data distribution can change over time. This phenomenon is called concept drift. Therefore, a classification model in a data stream with concept drift will need a set of recent instances with their respective labels to adapt to the new concept. However, in an intermediate latency scenario, the delayed instance labels may not be timely for concept drift detection. Thus, the classification model will not be capable of adapting immediately, implying a reduction of performance. In this paper, we propose a framework that can improve the performance of a classification model in a data stream with intermediate latency through a domain adaptation approach called importance weighting. The framework is called NIW-DSIL (Naive Importance Weighting for Data Stream with Intermediate Latency). The experiments showed that our approach is promising in dealing with this scenario. For the real datasets, the NIW - DSIL got better results in 6 of 8 intermediate latency scenarios."
pub.1118383783,Partial Key Grouping: Load-Balanced Partitioning of Distributed Streams,"We study the problem of load balancing in distributed stream processing
engines, which is exacerbated in the presence of skew. We introduce Partial Key
Grouping (PKG), a new stream partitioning scheme that adapts the classical
""power of two choices"" to a distributed streaming setting by leveraging two
novel techniques: key splitting and local load estimation. In so doing, it
achieves better load balancing than key grouping while being more scalable than
shuffle grouping.
  We test PKG on several large datasets, both real-world and synthetic.
Compared to standard hashing, PKG reduces the load imbalance by up to several
orders of magnitude, and often achieves nearly-perfect load balance. This
result translates into an improvement of up to 175% in throughput and up to 45%
in latency when deployed on a real Storm cluster. PKG has been integrated in
Apache Storm v0.10."
pub.1165899737,Collaborative Inference in DNN-based Satellite Systems with Dynamic Task Streams,"As a driving force in the advancement of intelligent in-orbit applications,
DNN models have been gradually integrated into satellites, producing daily
latency-constraint and computation-intensive tasks. However, the substantial
computation capability of DNN models, coupled with the instability of the
satellite-ground link, pose significant challenges, hindering timely completion
of tasks. It becomes necessary to adapt to task stream changes when dealing
with tasks requiring latency guarantees, such as dynamic observation tasks on
the satellites. To this end, we consider a system model for a collaborative
inference system with latency constraints, leveraging the multi-exit and model
partition technology. To address this, we propose an algorithm, which is
tailored to effectively address the trade-off between task completion and
maintaining satisfactory task accuracy by dynamically choosing early-exit and
partition points. Simulation evaluations show that our proposed algorithm
significantly outperforms baseline algorithms across the task stream with
strict latency constraints."
pub.1175016641,Collaborative Inference in DNN-Based Satellite Systems with Dynamic Task Streams,"As a driving force in the advancement of intel-ligent in-orbit applications, DNN models have been gradually integrated into satellites, producing daily latency-constraint and computation-intensive tasks. However, the substantial computation capability of DNN models, coupled with the instability of the satellite-ground link, pose significant challenges, hindering the timely completion of tasks. It becomes necessary to adapt to task stream changes when dealing with tasks requiring latency guarantees, such as dynamic observation tasks on the satellites. To this end, we consider a system model for a collaborative inference system with latency constraints, leveraging the multi-exit and model partition technology. To address this, we propose an algorithm, which is tailored to effectively address the trade-off between task completion and maintaining satisfactory task accuracy by dynamically choosing early-exit and partition points. Simulation evaluations show that our proposed algorithm signif-icantly outperforms baseline algorithms across the task stream with strict latency constraints."
pub.1107851996,Impatience is a Virtue: Revisiting Disorder in High-Performance Log Analytics,"There is a growing interest in processing real-time queries over out-of-order streams in this big data era. This paper presents a comprehensive solution to meet this requirement. Our solution is based on Impatience sort, an online sorting technique that is based on an old technique called Patience sort. Impatience sort is tailored for incrementally sorting streaming datasets that present themselves as almost sorted, usually due to network delays and machine failures. With several optimizations, our solution can adapt to both input streams and query logic. Further, we develop a new Impatience framework that leverages Impatience sort to reduce the latency and memory usage of query execution, and supports a range of user latency requirements, without compromising on query completeness and throughput, while leveraging existing efficient in-order streaming engines and operators. We evaluate our proposed solution in Trill, a high-performance streaming engine, and demonstrate that our techniques significantly improve sorting performance and reduce memory usage - in some cases, by over an order of magnitude."
pub.1135478214,Graceful Performance Degradation in Apache Storm,"The concept of stream data processing is becoming challenging in most business sectors where try to improve their operational efficiency by deriving valuable information from unstructured, yet, contentiously generated high volume raw data in an expected time spans. A modern streamlined data processing platform is required to execute analytical pipelines over a continues flow of data-items that might arrive in a high rate. In most cases, the platform is also expected to dynamically adapt to dynamic characteristics of the incoming traffic rates and the ever-changing condition of underlying computational resources while fulfill the tight latency constraints imposed by the end-users. Apache Storm has emerged as an important open source technology for performing stream processing with very tight latency constraints over a cluster of computing nodes. To increase the overall resource utilization, however, the service provider might be tempted to use a consolidation strategy to pack as many applications as possible in a (cloud-centric) cluster with limited number of working nodes. However, collocated applications can negatively compete with each other, for obtaining the resource capacity in a shared platform that, in turn, the result may lead to a severe performance degradation among all running applications.The main objective of this work is to develop an elastic solution in a modern stream processing ecosystem, for addressing the shared resource contention problem among collocated applications. We propose a mechanism, based on design principles of Model Predictive Control theory, for coping with the extreme conditions in which the collocated analytical applications have different quality of service (QoS) levels while the shared-resource interference is considered as a key performance limiting parameter. Experimental results confirm that the proposed controller can successfully enhance the p$$p$$-99 latency of high priority applications by 67%, compared to the default round robin resource allocation strategy in Storm, during the high traffic load, while maintaining the requested quality of service levels."
pub.1137410962,Deadline-Aware Offloading for High-Throughput Accelerators,"Contemporary GPUs are widely used for throughput-oriented data-parallel workloads and increasingly are being considered for latency-sensitive applications in datacenters. Examples include recurrent neural network (RNN) inference, network packet processing, and intelligent personal assistants. These data parallel applications have both high throughput demands and real-time deadlines (40μs-7ms). Moreover, the kernels in these applications have relatively few threads that do not fully utilize the device unless a large batch size is used. However, batching forces jobs to wait, which increases their latency, especially when realistic job arrival times are considered. Previously, programmers have managed the tradeoffs associated with concurrent, latency-sensitive jobs by using a combination of GPU streams and advanced scheduling algorithms running on the CPU host. Although GPU streams allow the accelerator to execute multiple jobs concurrently, prior state-of-the-art solutions use the relatively distant CPU host to prioritize the latency-sensitive GPU tasks. Thus, these approaches are forced to operate at a coarse granularity and cannot quickly adapt to rapidly changing program behavior. We observe that fine-grain, device-integrated kernel schedulers efficiently meet the deadlines of concurrent, latency-sensitive GPU jobs. To overcome the limitations of software-only, CPU-side approaches, we extend the GPU queue scheduler to manage real-time deadlines. We propose a novel laxity-aware scheduler (LAX) that uses information collected within the GPU to dynamically vary job priority based on how much laxity jobs have before their deadline. Compared to contemporary GPUs, 3 state-of-the-art CPU-side schedulers and 6 other advanced GPU-side schedulers, LAX meets the deadlines of 1.7X – 5.0X more jobs and provides better energy-efficiency, throughput, and 99-percentile tail latency."
pub.1095491128,An Improved Xen Credit Scheduler for I/O Latency-Sensitive Applications on Multicores,"It has long been recognized that the Credit scheduler favors CPU-bound applications while for the latency-sensitive workloads such as those related to stream-based audio/video services, its performance is far from satisfactory. In this paper we present an improved Credit scheduler in Xen to facilitate such tasks on multicore platforms. To this end, we improve the Credit scheduler from three perspectives. First, given the identified Simultaneous Multi-Boost problem, we minimize the system response time by load balancing the virtual CPUs with the BOOST priority between the cores. Second, we address the Premature Preemption problem by monitoring the received network packets in the driver domain and deliberately preventing it from being prematurely preempted during the packet delivery to further reduce and stabilize the I/O latency. Finally, we optimize the frequency of CPU switch by utilizing time-variant slice instead of the existing long time-invariant one to adapt to the dynamic fluctuation of the number of virtual CPUs in the run queue associated with each physical CPU. Our empirical studies show that the proposed improvement can significantly improve the performance of the Credit scheduler for scheduling the I/O latency-sensitive applications."
pub.1093219578,Data Availability in P2P Streaming Systems,"Nowadays, sharing multimedia contents in P2P systems has become a common practice. In general, it is done in “download” mode as any other type of data. However, due to their huge volume and real time constraint, multimedia data present another consumption mode which is the streaming mode. In this mode, rather than downloading the entire data before viewing it, it initiates a stream from the source peer to the destination one. The advantages of this mode are numerous such as latency reduction, copyright preservation (in the sense that the consumption peer does not hold the multimedia content), etc. Nevertheless, due to the unpredictable departure of peers and the real time constraint of the consumed data, the problem of maintaining data availability remains open. In this paper we propose to set up a cache to save the “last part” of the “now playing” videos. We demonstrate that such a cache can enhance the data availability in these systems while introducing an acceptable additional traffic. Additionally, we develop a cache management policy that automatically adapts the size of the cached data to the system workload. The simulations we conducted show the effectiveness of our approach."
pub.1019351157,BACS: Split Channel Based Overlay Multicast for Multimedia Streaming,"Multipath multicast has focused on how to deal with bandwidth instability and unfairness in forwarding overhead. Creating multiple disjoint trees is good for applied applications requiring aggregated throughput, such as content distribution. However, the path heterogeneity of different trees may cause data asynchrony in the receiver’s view, making it difficult to use in real time applications. In this paper, we propose a new delivery structure named cluster tree that utilizes bandwidth efficiently and lessens asynchrous sub-stream arrival. Cluster is composed of the interconnection of nodes within a latency boundary to each other, and the parent-child relationship between clusters forms a tree. Members of a cluster exchange disjoint sub-streams with peers in the same cluster and adapt to network dynamics cooperatively. This rate control mechanism can adapt to bandwidth fluctuation. The simulation result shows that cluster tree increases effective packets and reduces average source-to-leaf latency."
pub.1135675249,AMVP: Adaptive CNN-based Multitask Video Processing on Mobile Stream Processing Platforms,"The popularity of video cameras has spawned a new type of application called multitask video processing, which uses multiple CNNs to obtain different information of interests from a raw video stream. Unfortunately, the huge resource requirements of CNNs make the concurrent execution of multiple CNNs on a single resource-constrained mobile device challenging. Existing solutions solve this challenge by offloading CNN models to the cloud or edge server, compressing CNN models to fit the mobile device, or sharing some common parts of multiple CNN models. Most of these solutions, however, use the above offloading, compression or sharing strategies in a separate manner, which fail to adapt to the complex edge computing scenario well. In this paper, to solve the above limitation, we propose AMVP, an adaptive execution framework for CNN-based multitask video processing, which elegantly integrates the strategies of CNN layer sharing, feature compression, and model offloading. First, AMVP reduces the total computation workload of multiple CNN inference by sharing some common frozen CNN layers. Second, AMVP supports distributed CNN inference by splitting big CNNs into smaller components running on different devices. Third, AMVP leverages a quantization-based feature compression mechanism to reduce the feature transmission traffic size between two separate CNN components. We conduct extensive experiments on AMVP and the experimental results show that our AMVP framework can adapt to different performance goals and execution environments. Compared to two baseline approaches that only share or offload CNN layers, AMVP achieves up to 61% lower latency and 10% higher throughput with comparative accuracy."
pub.1004255096,Distributed gradient-domain processing of planar and spherical images,"Gradient-domain processing is widely used to edit and combine images. In this article we extend the framework in two directions. First, we adapt the gradient-domain approach to operate on a spherical domain, to enable operations such as seamless stitching, dynamic-range compression, and gradient-based sharpening over spherical imagery. An efficient streaming computation is obtained using a new spherical parameterization with bounded distortion and localized boundary constraints. Second, we design a distributed solver to efficiently process large planar or spherical images. The solver partitions images into bands, streams through these bands in parallel within a networked cluster, and schedules computation to hide the necessary synchronization latency. We demonstrate our contributions on several datasets including the Digitized Sky Survey, a terapixel spherical scan of the night sky."
pub.1142512617,CANS: Communication Limited Camera Network Self-Configuration for Intelligent Industrial Surveillance,"Realtime and intelligent video surveillance via camera networks involve computation-intensive vision detection tasks with massive video data, which is crucial for safety in the edge-enabled industrial Internet of Things (IIoT). Multiple video streams compete for limited communication resources on the link between edge devices and camera networks, resulting in considerable communication congestion. It postpones the completion time and degrades the accuracy of vision detection tasks. Thus, achieving high accuracy of vision detection tasks under the communication constraints and vision task deadline constraints is challenging. Previous works focus on single camera configuration to balance the tradeoff between accuracy and processing time of detection tasks by setting video quality parameters. In this paper, an adaptive camera network self-configuration method (CANS) of video surveillance is proposed to cope with multiple video streams of heterogeneous quality of service (QoS) demands for edge-enabled IIoT. Moreover, it adapts to video content and network dynamics. Specifically, the tradeoff between two key performance metrics, i.e., accuracy and latency, is formulated as an NP-hard optimization problem with latency constraints. A low-complexity algorithm is proposed to solve the optimization problem based on greedy searching. Simulation on real-world surveillance datasets demonstrates that the proposed CANS method achieves low end-to-end latency (13 ms on average) with high accuracy (92%) with network dynamics, which validates its effectiveness."
pub.1141111858,CANS: Communication Limited Camera Network Self-Configuration for Intelligent Industrial Surveillance,"Realtime and intelligent video surveillance via camera networks involve
computation-intensive vision detection tasks with massive video data, which is
crucial for safety in the edge-enabled industrial Internet of Things (IIoT).
Multiple video streams compete for limited communication resources on the link
between edge devices and camera networks, resulting in considerable
communication congestion. It postpones the completion time and degrades the
accuracy of vision detection tasks. Thus, achieving high accuracy of vision
detection tasks under the communication constraints and vision task deadline
constraints is challenging. Previous works focus on single camera configuration
to balance the tradeoff between accuracy and processing time of detection tasks
by setting video quality parameters. In this paper, an adaptive camera network
self-configuration method (CANS) of video surveillance is proposed to cope with
multiple video streams of heterogeneous quality of service (QoS) demands for
edge-enabled IIoT. Moreover, it adapts to video content and network dynamics.
Specifically, the tradeoff between two key performance metrics, \emph{i.e.,}
accuracy and latency, is formulated as an NP-hard optimization problem with
latency constraints. Simulation on real-world surveillance datasets
demonstrates that the proposed CANS method achieves low end-to-end latency (13
ms on average) with high accuracy (92\% on average) with network dynamics. The
results validate the effectiveness of the CANS."
pub.1167360203,An Evolving Population Approach to Data-Stream Classification with Extreme Verification Latency,"Recognising and reacting to change in non-stationary data-streams is a
challenging task. The majority of research in this area assumes that the true
class label of incoming points are available, either at each time step or
intermittently with some latency. In the worse case this latency approaches
infinity and we can assume that no labels are available beyond the initial
training set. When change is expected and no further training labels are
provided the challenge of maintaining a high classification accuracy is very
great. The challenge is to propagate the original training information through
several timesteps, possibly indefinitely, while adapting to underlying change
in the data-stream. In this paper we conduct an initial study into the
effectiveness of using an evolving, population-based approach as the mechanism
for adapting to change. An ensemble of one-class-classifiers is maintained for
each class. Each classifier is considered as an agent in the sub-population and
is subject to selection pressure to find interesting areas of the feature
space. This selection pressure forces the ensemble to adapt to the underlying
change in the data-stream."
pub.1167576275,An Evolving Population Approach to Data-Stream Classification with Extreme Verification Latency,"Recognising and reacting to change in non-stationary data-streams is a challenging task. The majority of research in this area assumes that the true class label of incoming points are available, either at each time step or intermittently with some latency. In the worse case this latency approaches infinity and we can assume that no labels are available beyond the initial training set. When change is expected and no further training labels are provided the challenge of maintaining a high classification accuracy is very great. The challenge is to propagate the original training information through several timesteps, possibly indefinitely, while adapting to underlying change in the data-stream. In this paper we conduct an initial study into the effectiveness of using an evolving, population-based approach as the mechanism for adapting to change. An ensemble of one-class-classifiers is maintained for each class. Each classifier is considered as an agent in the sub-population and is subject to selection pressure to find interesting areas of the feature space. This selection pressure forces the ensemble to adapt to the underlying change in the data-stream."
pub.1042603016,JetStream: Enabling high throughput live event streaming on multi-site clouds,"Scientific and commercial applications operate nowadays on tens of cloud datacenters around the globe, following similar patterns: they aggregate monitoring or sensor data, assess the QoS or run global data mining queries based on inter-site event stream processing. Enabling fast data transfers across geographically distributed sites allows such applications to manage the continuous streams of events in real time and quickly react to changes. However, traditional event processing engines often consider data resources as second-class citizens and support access to data only as a side-effect of computation (i.e. they are not concerned by the transfer of events from their source to the processing site). This is an efficient approach as long as the processing is executed in a single cluster where nodes are interconnected by low latency networks. In a distributed environment, consisting of multiple datacenters, with orders of magnitude differences in capabilities and connected by a WAN, this will undoubtedly lead to significant latency and performance variations. This is namely the challenge we address in this paper, by proposing JetStream, a high performance batch-based streaming middleware for efficient transfers of events between cloud datacenters. JetStream is able to self-adapt to the streaming conditions by modeling and monitoring a set of context parameters. It further aggregates the available bandwidth by enabling multi-route streaming across cloud sites, while at the same time optimizing resource utilization and increasing cost efficiency. The prototype was validated on tens of nodes from US and Europe datacenters of the Windows Azure cloud with synthetic benchmarks and a real-life application monitoring the ALICE experiment at CERN. The results show a 3× increase of the transfer rate using the adaptive multi-route streaming, compared to state of the art solutions."
pub.1123599449,Performance Assay of Big IoT Data Analytics Framework,"Evaluation of Internet of Things (IoT) technologies in real life has scaled the enumeration of data in huge volumes and that too with high velocity, and thus a new issue has come into picture that is of management & analytics of this BIG IOT STREAM data. In order to optimize the performance of the IoT Machines and services provided by the vendors, industry is giving high priority to analyze this big IoT Stream Data for surviving in the competitive global environment. Thses analysis are done through number of applications using various Data Analytics Framework, which require obtaining the valuable information intelligently from a large amount of real-time produced data. This paper, discusses the challenges and issues faced by distributed stream analytics frameworks at the data processing level and tries to recommend a possible a Scalable Framework to adapt with the volume and velocity of Big IoT Stream Data. Experiments focus on evaluating the performance of three Distributed Stream Analytics Here Analytics frameworks, namely Apache Spark, Splunk and Apache Storm are being evaluated over large steam IoT data on latency & throughput as parameters in respect to concurrency. The outcome of the paper is to find the best possible existing framework and recommend a possible scalable framework."
pub.1122707092,Haren,"In modern Stream Processing Engines (SPEs), numerous diverse applications, which can differ in aspects such as cost, criticality or latency sensitivity, can co-exist in the same computational node. When these differences need to be considered to control the performance of each application, custom scheduling of operators to threads is of key importance. Many solutions have been proposed regarding schedulers that allocate threads to operators to optimize specific metrics (e.g., latency) but there is still lack of a middleware that allows arbitrarily complex scheduling strategies to be seamlessly plugged on top of an SPE. We demonstrate Haren, a general scheduling middleware that fills this gap. Haren can be integrated into SPEs through a compact interface and efficiently enforce user-defined scheduling rules. This demo shows how Haren makes it is possible to adapt the use of computational resources over time to meet the goals of a variety of user-defined scheduling policies."
pub.1117328204,Mobile VR on edge cloud,"In this paper we design and implement MEC-VR, a mobile VR system that uses a Mobile Edge Cloud (MEC) to deliver high quality VR content to today's mobile devices using 4G/LTE cellular networks. Our main contribution is in realizing a low latency control loop that streams VR scenes containing only the user's Field of View (FoV) and a latency-adaptive margin area around the FoV. This allows the clients to render locally at a high refresh rate to accommodate and compensate for the head movements before the next motion update arrives. Compared with prior approaches, our MEC-VR design requires no viewpoint prediction, supports dynamic and live VR content, and adapts to the real-world latency experienced in cellular networks between the MEC and mobile devices. We implement a prototype of MEC-VR and evaluate its performance on a MEC node connected to an LTE testbed. We demonstrate that MEC-VR can effectively stream live VR content up to 8K resolution over 4G/LTE networks and achieve more than 80% of bandwidth savings."
pub.1139080632,Low-latency speculative inference on distributed multi-modal data streams,"While multi-modal deep learning is useful in distributed sensing tasks like human tracking, activity recognition, and audio and video analysis, deploying state-of-the-art multi-modal models in a wirelessly networked sensor system poses unique challenges. The data sizes for different modalities can be highly asymmetric (e.g., video vs. audio), and these differences can lead to significant delays between streams in the presence of wireless dynamics. Therefore, a slow stream can significantly slow down a multi-modal inference system in the cloud, leading to either increased latency (when blocked by the slow stream) or degradation in inference accuracy (if inference proceeds without waiting). In this paper, we introduce speculative inference on multi-modal data streams to adapt to these asymmetries across modalities. Rather than blocking inference until all sensor streams have arrived and been temporally aligned, we impute any missing, corrupt, or partially-available sensor data, then generate a speculative inference using the learned models and imputed data. A rollback module looks at the class output of speculative inference and determines whether the class is sufficiently robust to incomplete data to accept the result; if not, we roll back the inference and update the model's output. We implement the system in three multi-modal application scenarios using public datasets. The experimental results show that our system achieves 7 -- 128 latency speedup with the same accuracy as six state-of-the-art methods."
pub.1162804832,Enhancing Cloud-Based Video Streaming Efficiency using Neural Networks,"The growing demand for high-quality video streaming services has increased the need for efficient utilization of cloud resources. This research proposes a novel solution to this challenge by utilizing neural networks to enhance video streaming performance in the cloud. Specifically, the research introduces a neural network architecture capable of dynamically adjusting video quality based on network conditions and user preferences. By combining convolutional and recurrent neural networks, the proposed model learns patterns in video streams and predicts the optimal bit rate for each frame. The architecture also considers network conditions such as bandwidth, latency, and packet loss to adapt video quality in real-time. The proposed approach aims to improve quality of service while reducing cloud resource utilization, leading to cost-effective and scalable video streaming services. Extensive simulations and experiments were conducted to evaluate the feasibility and effectiveness of the approach. The proposed neural network architecture achieved up to 30% and 40% higher PSNR and SSIM scores, respectively, than the baseline approach. These results demonstrate the effectiveness of the proposed approach in enhancing video streaming efficiency in the cloud. Overall, this research provides valuable insights into designing efficient video streaming systems and highlights the potential benefits of neural networks in cloud-based video streaming services."
pub.1173968326,Towards Railways Remote Driving: Analysis of Video Streaming Latency and Adaptive Rate Control,"Remote driving aims to improve transport systems by promoting efficiency, sustainability, and accessibility. In the railway sector, remote driving makes it possible to increase flexibility, as the driver no longer has to be in the cab. However, this brings several challenges, as it has to provide at least the same level of safety obtained when the driver is in the cab. To achieve it, wireless networks and video streaming technologies gain importance as they should provide real-time track visualization and obstacle detection capabilities to the remote driver. Low latency camera capture, onboard media processing devices, and streaming protocols adapted for wireless links are the necessary enablers to be developed and integrated into the railway infrastructure. This paper compares video streaming protocols such as Real-Time Streaming Protocol (RTSP) and Web Real-Time Communication (WebRTC), as they are the main alternatives based on Real-time Transport Protocol (RTP) protocol to enable low latency. As latency is the main performance metric, this paper also provides a solution to calculate the End-to-End video streaming latency analytically. Finally, the paper proposes a rate control algorithm to adapt the video stream depending on the network capacity. The objective is to keep the latency as low as possible while avoiding any visual artifacts. The proposed solutions are tested in different setups and scenarios to prove their effectiveness before the planned field testing."
pub.1061697921,Optimizing Selective ARQ for H.264 Live Streaming: A Novel Method for Predicting Loss-Impact in Real Time,"This work proposes a quality-oriented, real-time capable prioritization technique for media units of H.264/AVC video streams. The derivation of estimates is based on the analysis of the macroblock partitioning, the spatial extents of temporal dependencies, and the length and strength of prediction chains existing among macroblocks, thus incorporating the expected impact of error propagation. It is demonstrated how the prioritization scheme can be beneficially integrated into live streaming systems which are characterized by tight timing constraints, with the focus on content-aware selective automatic repeat request mechanisms. Additionally, it is shown how potentially limited feedback can be used to adapt the estimation process to leverage prediction preciseness. The approach is compared against existing techniques in terms of practicability and efficiency, and tested under independent and bursty loss conditions in a wired and a wireless test setup. Moreover, the performance is examined when low-latency and constant bitrate video settings are enforced by using x264's novel encoding feature periodic-intra-refresh. Results of both experiments and simulations indicate that the proposed technique outperforms all reference techniques in nearly all test cases, and that the video quality can be further improved by incorporating receiver feedback."
pub.1172481757,Towards Railways Remote Driving: Analysis of Video Streaming Latency and Adaptive Rate Control,"Remote driving aims to improve transport systems by promoting efficiency,
sustainability, and accessibility. In the railway sector, remote driving makes
it possible to increase flexibility, as the driver no longer has to be in the
cab. However, this brings several challenges, as it has to provide at least the
same level of safety obtained when the driver is in the cab. To achieve it,
wireless networks and video streaming technologies gain importance as they
should provide real-time track visualization and obstacle detection
capabilities to the remote driver. Low latency camera capture, onboard media
processing devices, and streaming protocols adapted for wireless links are the
necessary enablers to be developed and integrated into the railway
infrastructure. This paper compares video streaming protocols such as Real-Time
Streaming Protocol (RTSP) and Web Real-Time Communication (WebRTC), as they are
the main alternatives based on Real-time Transport Protocol (RTP) protocol to
enable low latency. As latency is the main performance metric, this paper also
provides a solution to calculate the End-to-End video streaming latency
analytically. Finally, the paper proposes a rate control algorithm to adapt the
video stream depending on the network capacity. The objective is to keep the
latency as low as possible while avoiding any visual artifacts. The proposed
solutions are tested in different setups and scenarios to prove their
effectiveness before the planned field testing."
pub.1160813382,Large-Scale Response-Aware Online ANN Search in Dynamic Datasets,"Similarity search is an key operation in Content-based multimedia retrieval(CBMR) applications. Online CBMR applications, which is the focus of thiswork, have to search in large and dynamic datasets that are updated during theexecution while offering low response times. Additionally, these applications aresubmitted to workloads that vary at runtime. The computing demands in thisscenario exceeds the processing power of a single computer, motivating the large-scale machines in the domain. Thus, in this work, we proposed a distributedmemory parallelization of similarity search that addresses the mentioned chal-lenges. Our solution employs the efficient Inverted File System with AsymmetricDistance Computation algorithm (IVFADC) algorithm as the baseline, which isextended here to support dynamic datasets. Further, we developed a dynamicresource management algorithm, called Multi-Stream Adaptation (MS-ADAPT),that is executed at run-time to change the computing resource assignment withthe goal of minimizing response times. We have evaluated our system solutionwith multiple data partitioning strategies using up to 160 compute nodes anda dataset with 344 billion multimedia descriptors. It demonstrated superlinearscalability for our Spatial-Aware data partition algorithms and MS-ADAPT out-performed the best static approach (oracle) by reducing the response times upto 32× on high-load cases."
pub.1013255654,Multi-Dimensional Analysis of Data Streams Using Stream Cubes,"Large volumes of dynamic stream data pose great challenges to its analysis. Besides its dynamic and transient behavior, stream data has another important characteristic: multi-dimensionality. Much of stream data resides at a multidimensional space and at rather low level of abstraction, whereas most analysts are interested in relatively high-level dynamic changes in some combination of dimensions. To discover high-level dynamic and evolving characteristics, one may need to perform multi-level, multi-dimensional on-line analytical processing (OLAP) of stream data. Such necessity calls for the investigation of new architectures that may facilitate on-line analytical processing of multi-dimensional stream data.In this chapter, we introduce an interesting stream_cube architecture that effectively performs on-line partial aggregation of multi-dimensional stream data, captures the essential dynamic and evolving characteristics of data streams, and facilitates fast OLAP on stream data. Three important techniques are proposed for the design and implementation of stream cubes. First, a tilted time frame model is proposed to register time-related data in a multi-resolution model: The more recent data are registered at finer resolution, whereas the more distant data are registered at coarser resolution. This design reduces the overall storage requirements of time-related data and adapts nicely to the data analysis tasks commonly encountered in practice. Second, instead of materializing cuboids at all levels, two critical layers: observation layer and minimal interesting layer, are maintained to support routine as well as flexible analysis with minimal computation cost. Third, an efficient stream data cubing algorithm is developed that computes only the layers (cuboids) along a popular path and leaves the other cuboids for on-line, query-driven computation. Based on this design methodology, stream data cube can be constructed and maintained incrementally with reasonable memory space, computation cost, and query response time. This is verified by our substantial performance study.Stream cube architecture facilitates online analytical processing of stream data. It also forms a preliminary structure for online stream mining. The impact of the design and implementation of stream cube in the context of stream mining is also discussed in the chapter."
pub.1004710075,"Adaptive, best-effort delivery of digital audio and video across packet-switched networks","We present an overview of a “best-effort” transport protocol that supports conferencing with digital audio and video across interconnected packet switched networks. The protocol delivers the highest quality conference service possible given the current load in the network. Quality is defined in terms of synchronization between audio and video, the number of frames played out of order, and the end-to-end latency in the conference. High quality conferences are realized through four transport and display mechanisms and a real-time implementation of these mechanisms that integrates operating system services (e.g., scheduling and resource allocation, and device management) with network communication services (e.g., transport protocols). In concert these mechanisms dynamically adapt the conference frame rate to the bandwidth available in the network, minimize the latency in the displayed streams while avoiding discontinuities, and provide quasi-reliable delivery of audio frames."
pub.1003253904,Stream Cube: An Architecture for Multi-Dimensional Analysis of Data Streams,"Real-time surveillance systems, telecommunication systems, and other dynamic environments often generate tremendous (potentially infinite) volume of stream data: the volume is too huge to be scanned multiple times. Much of such data resides at rather low level of abstraction, whereas most analysts are interested in relatively high-level dynamic changes (such as trends and outliers). To discover such high-level characteristics, one may need to perform on-line multi-level, multi-dimensional analytical processing of stream data. In this paper, we propose an architecture, called stream_cube, to facilitate on-line, multi-dimensional, multi-level analysis of stream data.For fast online multi-dimensional analysis of stream data, three important techniques are proposed for efficient and effective computation of stream cubes. First, a tilted time frame model is proposed as a multi-resolution model to register time-related data: the more recent data are registered at finer resolution, whereas the more distant data are registered at coarser resolution. This design reduces the overall storage of time-related data and adapts nicely to the data analysis tasks commonly encountered in practice. Second, instead of materializing cuboids at all levels, we propose to maintain a small number of critical layers. Flexible analysis can be efficiently performed based on the concept of observation layer and minimal interesting layer. Third, an efficient stream data cubing algorithm is developed which computes only the layers (cuboids) along a popular path and leaves the other cuboids for query-driven, on-line computation. Based on this design methodology, stream data cube can be constructed and maintained incrementally with a reasonable amount of memory, computation cost, and query response time. This is verified by our substantial performance study."
pub.1135461313,Dynamic Auto Reconfiguration of Operator Placement in Wireless Distributed Stream Processing Systems,"The data is generated at significant speed and volume by devices in real-time. The data generation and the growth of fog and edge computing infrastructure have led to the noteworthy development of the corresponding distributed stream processing systems (DSPS). A DSPS application has Quality of Service (QoS) restrictions in terms of resource cost and time. The physical resources are distributed and heterogeneous. The resource-constrained scheduling problem has considerable implications on the performance of the system and QoS violations. The static deployment of applications in fog or edge scenario has to be monitored continuously for runtime issues, and actions have to be taken accordingly. In this paper, we propose an adaptation capability with reinforcement learning techniques to an existing stream processing framework scheduler. This functionality enables the scheduler to make decisions on its own when the system model or knowledge of the environment is not known upfront. The reinforcement learning methods adapt to the system when the system model for different states is not available. We consider applications whose workload cannot be characterized or predicted. In such applications, predictions of input load are not helpful for online scheduling. The Q-Learning based online scheduler learns to make dynamic scaling decisions at runtime when there is performance degradation. We validated the proposed approach with real-time and benchmark applications on a DSPS cluster. We obtained an average of 6% reduction in the response time and a 15% increase in the throughput when the Q Learning module is employed in the scheduler."
pub.1095350153,Adaptive multithreaded H.264/AVC decoding,"The current trend towards multi-core processors imposes the necessity of finding viable strategies to exploit the additional computational resources in media processing. Among the challenges for video decoding are the appropriate partitioning of decoder steps, efficient tracking of dependencies and resource allocation/synchronization for multiple threads with respect to the resulting overhead. In this paper, we propose two variants of multithreading with distributed synchronization. The first method is optimized for minimum latency decoding, necessary for conversational applications. The second method aims to maximize the total throughput at the cost of a higher latency. In addition, we propose a method of dynamic core usage in order to reduce the total allocated processing resources due to inter-process communication overhead. This method is based on a coarse grained complexity estimation. To implicitly adapt to different combinations of processor architectures, associated memory interfaces and power-saving states, the scheme is feedback assisted. By correlating the initial estimate with the actual required processing time, a sufficiently accurate prediction of the required number of cores for the image processing part can be obtained. Experimental results demonstrate the scaling abilities of up to factor 3.5 on a quad-core machine, as well as the limits of the proposed approach regarding the complexity of sequential bitstream processing. We demonstrate that real-time 4k resolution decoding is feasible on current mid-range PC hardware. For less demanding streams, the adaptive mode reduces the total required CPU resources by up to 10% compared to the greedy approach."
pub.1150793464,Stream-Based Active Learning with Verification Latency in Non-stationary Environments,"Data stream classification is an important problem in the field of machine learning. Due to the non-stationary nature of the data where the underlying distribution changes over time (concept drift), the model needs to continuously adapt to new data statistics. Stream-based Active Learning (AL) approaches address this problem by interactively querying a human expert to provide new data labels for the most recent samples, within a limited budget. Existing AL strategies assume that labels are immediately available, while in a real-world scenario the expert requires time to provide a queried label (verification latency), and by the time the requested labels arrive they may not be relevant anymore. In this article, we investigate the influence of finite, time-variable, and unknown verification delay, in the presence of concept drift on AL approaches. We propose PRopagate (PR), a latency independent utility estimator which also predicts the requested, but not yet known, labels. Furthermore, we propose a drift-dependent dynamic budget strategy, which uses a variable distribution of the labelling budget over time, after a detected drift. Thorough experimental evaluation, with both synthetic and real-world non-stationary datasets, and different settings of verification latency and budget are conducted and analyzed. We empirically show that the proposed method consistently outperforms the state-of-the-art. Additionally, we demonstrate that with variable budget allocation in time, it is possible to boost the performance of AL strategies, without increasing the overall labeling budget."
pub.1174393396,Energy efficient and low-latency spiking neural networks on embedded microcontrollers through spiking activity tuning,"In this work, we target the efficient implementation of spiking neural networks (SNNs) for low-power and low-latency applications. In particular, we propose a methodology for tuning SNN spiking activity with the objective of reducing computation cycles and energy consumption. We performed an analysis to devise key hyper-parameters, and then we show the results of tuning such parameters to obtain a low-latency and low-energy embedded LSNN (eLSNN) implementation. We demonstrate that it is possible to adapt the firing rate so that the samples belonging to the most frequent class are processed with less spikes. We implemented the eLSNN on a microcontroller-based sensor node and we evaluated its performance and energy consumption using a structural health monitoring application processing a stream of vibrations for damage detection (i.e. binary classification). We obtained a cycle count reduction of 25% and an energy reduction of 22% with respect to a baseline implementation. We also demonstrate that our methodology is applicable to a multi-class scenario, showing that we can reduce spiking activity between 68 and 85% at iso-accuracy."
pub.1164602151,Vidaptive: Efficient and Responsive Rate Control for Real-Time Video on Variable Networks,"Real-time video streaming relies on rate control mechanisms to adapt video
bitrate to network capacity while maintaining high utilization and low delay.
However, the current video rate controllers, such as Google Congestion Control
(GCC), are very slow to respond to network changes, leading to link
under-utilization and latency spikes. While recent delay-based congestion
control algorithms promise high efficiency and rapid adaptation to variable
conditions, low-latency video applications have been unable to adopt these
schemes due to the intertwined relationship between video encoders and rate
control in current systems.
  This paper introduces Vidaptive, a new rate control mechanism designed for
low-latency video applications. Vidaptive decouples packet transmission
decisions from encoder output, injecting ``dummy'' padding traffic as needed to
treat video streams akin to backlogged flows controlled by a delay-based
congestion controller. Vidaptive then adapts the target bitrate of the encoder
based on delay measurements to align the video bitrate with the congestion
controller's sending rate. Our evaluations atop Google's implementation of
WebRTC show that, across a set of cellular traces, Vidaptive achieves ~1.5x
higher video bitrate and 1.4 dB higher SSIM, 1.3 dB higher PSNR, and 40% higher
VMAF, and it reduces 95th-percentile frame latency by 2.2 s with a slight 17 ms
increase in median frame latency."
pub.1147144994,Stream-based Active Learning with Verification Latency in Non-stationary Environments,"Data stream classification is an important problem in the field of machine
learning. Due to the non-stationary nature of the data where the underlying
distribution changes over time (concept drift), the model needs to continuously
adapt to new data statistics. Stream-based Active Learning (AL) approaches
address this problem by interactively querying a human expert to provide new
data labels for the most recent samples, within a limited budget. Existing AL
strategies assume that labels are immediately available, while in a real-world
scenario the expert requires time to provide a queried label (verification
latency), and by the time the requested labels arrive they may not be relevant
anymore. In this article, we investigate the influence of finite,
time-variable, and unknown verification delay, in the presence of concept drift
on AL approaches. We propose PRopagate (PR), a latency independent utility
estimator which also predicts the requested, but not yet known, labels.
Furthermore, we propose a drift-dependent dynamic budget strategy, which uses a
variable distribution of the labelling budget over time, after a detected
drift. Thorough experimental evaluation, with both synthetic and real-world
non-stationary datasets, and different settings of verification latency and
budget are conducted and analyzed. We empirically show that the proposed method
consistently outperforms the state-of-the-art. Additionally, we demonstrate
that with variable budget allocation in time, it is possible to boost the
performance of AL strategies, without increasing the overall labeling budget."
pub.1131701998,GSLICE,"The increasing demand for cloud-based inference services requires the use of Graphics Processing Unit (GPU). It is highly desirable to utilize GPU efficiently by multiplexing different inference tasks on the GPU. Batched processing, CUDA streams and Multi-process-service (MPS) help. However, we find that these are not adequate for achieving scalability by efficiently utilizing GPUs, and do not guarantee predictable performance. GSLICE addresses these challenges by incorporating a dynamic GPU resource allocation and management framework to maximize performance and resource utilization. We virtualize the GPU by apportioning the GPU resources across different Inference Functions (IFs), thus providing isolation and guaranteeing performance. We develop self-learning and adaptive GPU resource allocation and batching schemes that account for network traffic characteristics, while also keeping inference latencies below service level objectives. GSLICE adapts quickly to the streaming data's workload intensity and the variability of GPU processing costs. GSLICE provides scalability of the GPU for IF processing through efficient and controlled spatial multiplexing, coupled with a GPU resource re-allocation scheme with near-zero (< 100s) downtime. Compared to default MPS and TensorRT, GSLICE improves GPU utilization efficiency by 60--800% and achieves 2--13X improvement in aggregate throughput."
pub.1094051646,Decentralized Hash-Based Coordination of Distributed Multimedia Caches,"We present a new approach to decentralized and cooperative caching of multimedia streams, based on the notion of virtual hierarchies, which result in very uniform distributions of loads across the system of caches. We show through simulations that our method greatly reduces loads at the server as well as latencies at the client. Our approach is robust, scalable and adapts quickly to changes in object popularity."
pub.1175491948,Distributed Federated and Incremental Learning for Electric Vehicles Model Development in Kafka-ML,"With the increasing development and deployment of new systems for efficient and clean mobility, Electric Vehicles (EVs) are becoming more and more common among people. Those produce large amounts of data streams that need to be collected and analyzed to understand user needs and improve their performance. For this purpose, Artificial Intelligence (AI) techniques are playing a very important role. Within this context, Kafka-ML is a Machine Learning (ML) framework that enables the consumption and processing of data streams and allows the flexible management and deployment of neural networks throughout their entire life cycle. Kafka-ML can work with Distributed Neural Networks (DNN) which reduce latency and response times, perform incremental training over time allowing models to adapt to data on the fly, and carry out Federated Learning (FL) processes for this type of algorithms so a more robust global model can be created while maintaining data privacy and security, but all this separately. This work has considered the joint implementation of FL, for anonymous data sharing, incremental learning for continuous training of the models, and DNN for distribution of the models across different points on the map. All this applied within a Vehicle-to-everything (V2X) domain where EV usage and charge data can be shared to improve the user experience, as well as to better understand the behavior of this type of vehicles and their charging points to achieve savings, and how it affects people daily lives. An evaluation of the system related to this EV use case is presented to demonstrate the viability of the tool."
pub.1094866416,An Adaptive Multi-Path Video Streaming Scalable Video Coding Algorithm,"In order to provide end users with better video quality, This paper presents an adaptive extension H264/AVC video streams based multipath scalable video coding algorithm, with the path diversity provided by video distribution network (VDN). Our method adapts to the variety of end-users using the scalable video coding. Moreover, it adapts to network bandwidth fluctuation by observing the changes of the available bandwidth over the multiple overlay paths. Experimental results show that the algorithm is more effective to estimate network congestion, reduce video packet loss rate and reducing network latency, and thus more effectively ensure the quality of the video network transmission."
pub.1173625184,AiGAS-dEVL: An Adaptive Incremental Neural Gas Model for Drifting Data Streams under Extreme Verification Latency,"The ever-growing speed at which data are generated nowadays, together with
the substantial cost of labeling processes cause Machine Learning models to
face scenarios in which data are partially labeled. The extreme case where such
a supervision is indefinitely unavailable is referred to as extreme
verification latency. On the other hand, in streaming setups data flows are
affected by exogenous factors that yield non-stationarities in the patterns
(concept drift), compelling models learned incrementally from the data streams
to adapt their modeled knowledge to the concepts within the stream. In this
work we address the casuistry in which these two conditions occur together, by
which adaptation mechanisms to accommodate drifts within the stream are
challenged by the lack of supervision, requiring further mechanisms to track
the evolution of concepts in the absence of verification. To this end we
propose a novel approach, AiGAS-dEVL (Adaptive Incremental neural GAS model for
drifting Streams under Extreme Verification Latency), which relies on growing
neural gas to characterize the distributions of all concepts detected within
the stream over time. Our approach exposes that the online analysis of the
behavior of these prototypical points over time facilitates the definition of
the evolution of concepts in the feature space, the detection of changes in
their behavior, and the design of adaptation policies to mitigate the effect of
such changes in the model. We assess the performance of AiGAS-dEVL over several
synthetic datasets, comparing it to that of state-of-the-art approaches
proposed in the recent past to tackle this stream learning setup. Our results
reveal that AiGAS-dEVL performs competitively with respect to the rest of
baselines, exhibiting a superior adaptability over several datasets in the
benchmark while ensuring a simple and interpretable instance-based adaptation
strategy."
pub.1164954321,Large-scale response-aware online ANN search in dynamic datasets,"Similarity search is a key operation in content-based multimedia retrieval (CBMR) applications. Online CBMR applications, which are the focus of this work, perform a large number of search operations on dynamic datasets, which are updated at run-time. Additionally, the rates of search and data insertion (updated) operations vary during the execution. Such applications that rely on similarity search are required to fulfill these demands while also offering low response times. Thus, it is common for the computing demands in such applications to exceed the processing power of a single computer, motivating the usage of large-scale compute systems. As such, we propose in this work a distributed memory parallelization of similarity search that addresses these challenges. Our solution employs the efficient Inverted File System with Asymmetric Distance Computation algorithm (IVFADC) as the baseline, which is extended to support dynamic datasets. A dynamic resource management algorithm, called Multi-Stream Adaptation (MS-ADAPT) is proposed. It allows run-time changes on resource assignment with the goal of reducing response times. We evaluate our solution with multiple data partitioning strategies using up to 160 compute nodes and a dataset with 344 billion multimedia descriptors. Our experiments demonstrate superlinear scalability and MS-ADAPT outperforms the best static approach (oracle) by improving the response times up to 32×$$32\times$$ on high-load cases."
pub.1174846337,Do Cloud Games Adapt to Client Settings and Network Conditions?,"Cloud gaming relieves gamers from acquiring highly configured gaming hardware to smoothly play high-profile games. To this objective, cloud gaming platforms operate cloud servers that receive inputs from client devices, render gaming graphics, and stream the gaming scenes back to the client devices in real-time. While significantly expanding the gaming industry, this business model imposes high demand on the Internet that transports gaming video for good user experience (QoE) with a typical requirement of more than 10-20Mbps bandwidth, 100ms or less latency, and less than 5% packet drops. In this paper, focusing on two leading platforms, Nvidia's GeForce NOW and Microsoft's Xbox Cloud Gaming, we systematically profile and compare how cloud games adapt their streaming characteristics to client settings (including game streaming frame rate and graphic resolution) and network conditions like bandwidth, latency, and packet drop. Key insights are obtained such as Nvidia's GeForce NOW optimizes its game streaming frame rate and graphic resolution for a smooth user experience, particularly for users using its native application; Xbox does not reactively adapt and shows better tolerance for poor network conditions on PC browsers compared to its proprietary gaming console."
pub.1149310144,Novel Adaptive DNN Partitioning Method Based on Image-Stream Pipeline Inference between the Edge and Cloud,"The cloud-only and edge-computing approaches have recently been proposed to satisfy the requirements of complex neural networks. However, the cloud-only approach generates a latency challenge because of the high data volumes that must be sent to a centralized location in the cloud. Less-powerful edge computing resources require a compression model for computation reduction, which degrades the model trading accuracy. To address this challenge, deep neural network (DNN) partitioning has become a recent trend, with DNN models being sliced into head and tail portions executed at the mobile edge devices and cloud server, respectively. We propose Edgepipe, a novel partitioning method based on pipeline inference with an image stream to automatically partition DNN computation between the edge device and cloud server, thereby reducing the global latency and enhancing the system-wide real-time performance. This method adapts to various DNN architectures, hardware platforms, and networks. Here, when evaluated on a suite of five DNN applications, Edgepipe achieves average latency speedups of 1.241× and 1.154× over the cloud-only approach and the state-of-the-art approach known as “Neurosurgeon”, respectively."
pub.1111520523,STARLORD: Sliding window Temporal Accumulate-Retract Learning for Online Reasoning on Datastreams,"Nowadays, data sources, such as IoT devices, financial markets, and online services, continuously generate large amounts of data. Such data is usually generated at high frequencies and is typically described by non-stationary distributions. Querying these data sources brings new challenges for machine learning algorithms, which now need to be considered from the perspective of an evolving stream and not a static dataset. Under such scenarios, where data flows continuously, the challenge is how to transform the vast amount of data into information and knowledge, and how to adapt to data changes (i.e. drifts) and accumulate experience over time to support online decision-making. In this paper, we introduce STARLORD, a novel incremental computation method and system acting on data streams and capable of achieving low-latency (millisecond level) and high-throughput (thousands events/second/core) when learning from data streams. Moreover, the approach is able to adapt to data drifts and accumulate experience over time, and to use such knowledge to improve future learning and prediction performance, with resource usage guarantees. This is proven by our preliminary experiments where we built-in the framework in an open source stream engine (i.e, Apache Flink)."
pub.1107336674,Adaptive multicast streaming for videoconferences on software-defined networks,"Real-time applications, such as video conferences, have strong Quality of Service requirements for ensuring a decent Quality of Experience. Nowadays, most of these conferences are performed over wireless devices. Thus, an appropriate management of both heterogeneous mobile devices and network dynamics is necessary. Software Defined Networking enables the use of multicasting and stream layering inside the network nodes, two techniques able to enhance the quality of live video streams. In this paper, we propose two algorithms for building and maintaining multicast sessions in a software-defined network. The first algorithm sets up the initial multicast trees for a given call. It optimally places the stream layer adaptation function inside the core network in order to minimize the bandwidth consumption. This algorithm has two versions: the first one, based on shortest path trees is minimizing the latency, while the second one, based on spanning trees is minimizing the bandwidth consumption. The second algorithm adapts the multicast trees according to the network changes occurring during a call. It does not recompute the trees, but only relocates the stream layer adaptation functions. It requires very low computation at the controller, thus making our proposal fast and highly reactive. Extensive simulation results confirm the efficiency of our solution in terms of processing time and bandwidth savings compared to existing solutions such as multiple unicast connections, Multipoint Control Unit solutions and application layer multicast."
pub.1175067248,Optimizing Service Replication and Placement for IoT Applications in Fog Computing Systems,"Fog Computing extends Cloud Computing to the network edge, enhancing distributed computing to meet the growing needs of Internet of Things (IoT) applications requiring real-time or near-real-time analysis. This research focuses on efficiently managing the vast amounts of data generated by IoT devices and the continuous data streams they produce, employing an advanced replication and placement strategy for application components across distributed Fog Computing nodes. This approach enables scalable and parallel data processing to adapt to demand fluctuations, prevent over-provisioning, and maintain low response times, making it particularly effective for the dynamic nature of data stream processing in IoT applications. In this paper, we propose an Optimal IoT Service Replication and Placement (SRP) model, formulated as a constraint satisfaction problem, that considers the diverse requirements of IoT applications and the available infrastructure resources. Our model is designed to be adaptive and extensible, addressing the challenge of workload variability through real-time optimization. Numerical evaluations confirm the superior performance and scalability of our model over existing methods, while maintaining quality of service constraints. This highlights the potential of our approach to improve efficiency and resource management in Fog Computing environments."
pub.1014975606,Video transmission adaptation on mobile devices,"The development of multimedia streaming over wireless network is facing a lot of challenges. Taking into account mobility and highly variable bandwidth are the two major ones. Using scalable video content can solve the variable bandwidth problem only if the streaming architecture is able to react without latency. In this article, we present NetMoVie, an intermediate architecture based on real-time protocol which is able to adapt streams to the constraints of the wireless channel."
pub.1158536712,Towards Effective Multipath Scheduling with Multipath QUIC in Heterogeneous Paths,"Quick UDP Internet Connection (QUIC) is a network transmission protocol proposed by Google which has lower latency, higher flexibility and stronger security compared with TCP. Multipath QUIC (MPQUIC) further enhance the user experience and adapt the large-scale network traffic by using multiple paths. Different from Multipath TCP (MPTCP), MPQUIC scheduler can perform finer granularity stream scheduling. In heterogeneous paths, MPQUIC not only has to consider inter-stream HoL blocking, but also encounters severe intra-stream HoL blocking within individual stream; in addition, MPQUIC scheduler needs to control scheduling rate to ensure that stream data is not backed up in sub-path queue, but also the bandwidth of sub-path is fully utilized. However, existing scheduling algorithms cannot solve the problems above. Therefore, we propose multi-stream hierarchical scheduling algorithm(MS-HS), which speeds up the transmission of high-priority streams by controlling data allocation rate and giving each stream two priority levels. We evaluate the benefits of MS-HS under heterogeneous paths by comparing it with the classical scheduler of MPQUIC. Our evaluation shows that our scheduler can reduce up to 31.4% of the high-priority stream completion time, while MS-HS achieves higher throughput than the original best scheduler."
pub.1033398463,Direct Memory Access Optimization in Wireless Terminals for Reduced Memory Latency and Energy Consumption,"Today, wireless networks are becoming increasingly ubiquitous. Usually several complex multi-threaded applications are mapped on a single embedded system and all of them are triggered by a single wireless stream (which corresponds to the dynamic run-time behavior of the user). It is almost impossible to analyze these systems fully at design-time. Therefore, run-time information has also to be used in order to produce an efficient design. This introduces new challenges, especially for embedded system designers using a Direct Memory Access (DMA) module, who have to know in advance the memory transfer behavior of the whole system, in order to design and program their DMA efficiently. In this paper, we propose a mixed Hardware/Software optimization at system level. More specifically, we propose to adapt DMA usage parameters automatically at run-time based on online information. With our proposed optimization approach we manage to reduce the mean latency of the memory transfers while optimizing energy consumption and system responsiveness. We evaluate our approach using a set of real-life applications and real wireless dynamic streams."
pub.1170751912,LAECIPS: Large Vision Model Assisted Adaptive Edge-Cloud Collaboration for IoT-based Perception System,"Recent large vision models (e.g., SAM) enjoy great potential to facilitate
intelligent perception with high accuracy. Yet, the resource constraints in the
IoT environment tend to limit such large vision models to be locally deployed,
incurring considerable inference latency thereby making it difficult to support
real-time applications, such as autonomous driving and robotics. Edge-cloud
collaboration with large-small model co-inference offers a promising approach
to achieving high inference accuracy and low latency. However, existing
edge-cloud collaboration methods are tightly coupled with the model
architecture and cannot adapt to the dynamic data drifts in heterogeneous IoT
environments. To address the issues, we propose LAECIPS, a new edge-cloud
collaboration framework. In LAECIPS, both the large vision model on the cloud
and the lightweight model on the edge are plug-and-play. We design an
edge-cloud collaboration strategy based on hard input mining, optimized for
both high accuracy and low latency. We propose to update the edge model and its
collaboration strategy with the cloud under the supervision of the large vision
model, so as to adapt to the dynamic IoT data streams. Theoretical analysis of
LAECIPS proves its feasibility. Experiments conducted in a robotic semantic
segmentation system using real-world datasets show that LAECIPS outperforms its
state-of-the-art competitors in accuracy, latency, and communication overhead
while having better adaptability to dynamic environments."
pub.1142933764,Scrooge,"Advances in deep learning (DL) have prompted the development of cloud-hosted DL-based media applications that process video and audio streams in real-time. Such applications must satisfy throughput and latency objectives and adapt to novel types of dynamics, while incurring minimal cost. Scrooge, a system that provides media applications as a service, achieves these objectives by packing computations efficiently into GPU-equipped cloud VMs, using an optimization formulation to find the lowest cost VM allocations that meet the performance objectives, and rapidly reacting to variations in input complexity (e.g., changes in participants in a video). Experiments show that Scrooge can save serving cost by 16-32% (which translate to tens of thousands of dollars per year) relative to the state-of-the-art while achieving latency objectives for over 98% under dynamic workloads."
pub.1158095473,Hierarchical Auto-scaling Policies for Data Stream Processing on Heterogeneous Resources,"Data Stream Processing (DSP) applications analyze data flows in near real-time by means of operators, which process and transform incoming data. Operators handle high data rates running parallel replicas across multiple processors and hosts. To guarantee consistent performance without wasting resources in the face of variable workloads, auto-scaling techniques have been studied to adapt operator parallelism at run-time. However, most of the effort has been spent under the assumption of homogeneous computing infrastructures, neglecting the complexity of modern environments. We consider the problem of deciding both how many operator replicas should be executed and which types of computing nodes should be acquired. We devise heterogeneity-aware policies by means of a two-layered hierarchy of controllers. While application-level components steer the adaptation process for whole applications, aiming to guarantee user-specified requirements, lower-layer components control auto-scaling of single operators. We tackle the fundamental challenge of performance and workload uncertainty, exploiting Bayesian optimization (BO) and reinforcement learning (RL) to devise policies. The evaluation shows that our approach is able to meet users’ requirements in terms of response time and adaptation overhead, while minimizing the cost due to resource usage, outperforming state-of-the-art baselines. We also demonstrate how partial model information is exploited to reduce training time for learning-based controllers."
pub.1158097540,Intelligent resource allocation scheme for cloud-edge-end framework aided multi-source data stream,"To support multi-source data stream generated from Internet of Things devices, edge computing emerges as a promising computing pattern with low latency and high bandwidth compared to cloud computing. To enhance the performance of edge computing within limited communication and computation resources, we study a cloud-edge-end computing architecture, where one cloud server and multiple computational access points can collaboratively process the compute-intensive data streams that come from multiple sources. Moreover, a multi-source environment is considered, in which the wireless channel and the characteristic of the data stream are time-varying. To adapt to the dynamic network environment, we first formulate the optimization problem as a markov decision process and then decompose it into a data stream offloading ratio assignment sub-problem and a resource allocation sub-problem. Meanwhile, in order to reduce the action space, we further design a novel approach that combines the proximal policy optimization (PPO) scheme with convex optimization, where the PPO is used for the data stream offloading assignment, while the convex optimization is employed for the resource allocation. The simulated outcomes in this work can help the development of the application of the multi-source data stream."
pub.1094744767,A Game-Theoretic Based QoS-Aware Capacity Management for Real-Time EdgeIoT Applications,"More and more real-time IoT applications such as smart cities or autonomous vehicles require big data analytics with reduced latencies. However, data streams produced from distributed sensing devices may not suffice to be processed traditionally in the remote cloud due to: (i) longer Wide Area Network (WAN) latencies and (ii) limited resources held by a single Cloud. To solve this problem, a novel Software-Defined Network (SDN) based InterCloud architecture is presented for mobile edge computing environments, known as IoT An adaptive resource capacity management approach is proposed to employ a policy-based QoS control framework using principles in coalition games with externalities. To optimise resource capacity policy, the proposed QoS management technique solves, adaptively, a lexicographic ordering bi-criteria Coalition Structure Generation (CSG) problem. It is an onerous task to guarantee in a deterministic way that a real-time EdgeIoT application satisfies low latency requirement specified in Service Level Agreements (SLA). CloudSim 4.0 toolkit is used to simulate an SDN-based InterCloud scenario, and the empirical results suggest that the proposed approach can adapt, from an operational perspective, to ensure low latency QoS for real-time EdgeIoT application instances."
pub.1122979520,Reducing Handover Latency of PMIPv6 using Extended Open-Flow Technique,"This proposed technique adapts PMIPv6 to the Extended Open-Flow architecture, and this technique is referred to as the Extended Open-Flow Technique of PMIPv6 (EOFT-PMIPv6). This method isolates the versatility capacities from the PMIPv6 segments, for example, the Local Mobility Anchor (LMA) and Mobile Access Gateway (MAG), and recreates the parts to take points of interest of the Open-Flow design. The parts that contain the LMA work set the stream table of the switches situated in the way as the controller of Open-Flow, and as such, the area of the MN is kept up. The entrance switches with the MAG capacities tell the connection of a MN and introduce the portability related motioning of MAG. The fundamental commitments of this proposed strategy are twofold: 1) isolating the control and data planes and 2) reducing handover latency."
pub.1095120562,Implementation of Multi-Format Adaptive Streaming Server,"This paper presents the implementation details of multi-format streaming server. Supported protocols are HLS (HTTP live streaming) protocol and MPEG-DASH (Moving Pictures Experts Group - Dynamic Adaptive Streaming over HTTP) protocol, which allow the server to adapt to different network conditions and client requirements in order to achieve the best possible QoE (Quality of Experience) with low latency and seamless transitions between streams of different quality and bitrates. Compliance of the solution to proposed standards was tested with commercial applications that support client side of these protocols, while QoE was measured through surveys by subjective evaluation of participants."
pub.1164004012,Multiple Description Video Coding for Real-Time Applications using HEVC,"Remote control vehicles require the transmission of large amounts of data, and video is one of the most important sources for the driver. To ensure reliable video transmission, the encoded video stream is transmitted simultaneously over multiple channels. However, this solution incurs a high transmission cost. To address this issue, it is necessary to use more efficient video encoding methods that can make the video stream robust to noise. Moreover it should have a less complexity to adapt to the real time requirement. In this paper, we propose a low-complexity, low-latency 2-channel Multiple Description Coding (MDC) solution with an adaptive Instantaneous Decoder Refresh (IDR) frame period, which is compatible with the HEVC standard with adaptive redundancy adjustment. This method shows a better resistance to high packet loss rates with lower complexity."
pub.1105963398,Adaptive Video with SCReAM over LTE for Remote‐Operated Working Machines,"Remote operation is a step toward the automation of mobile working machines. Safe and efficient teleremote operation requires good‐quality video feedback. Varying radio conditions make it desirable to adapt the video sending rate of cameras to make the best use of the wireless capacity. The adaptation should be able to prioritize camera feeds in different directions depending on motion, ongoing tasks, and safety concerns. Self‐Clocked Rate Adaptation for Multimedia (SCReAM) provides a rate adaptation algorithm for these needs. SCReAM can control the compression used for multiple video streams using differentiating priorities and thereby provide sufficient congestion control to achieve both low latency and high video throughput. We present results from the testing of prioritized adaptation of four video streams with SCReAM over LTE and discuss how such adaptation can be useful for the teleremote operation of working machines."
pub.1170173095,CAC: Content-aware Adaptive Configuration for Edge Video Analytics,"Edge video analytics, which offloads video stream processing tasks to edge servers, is a key enabling technology for deep neural networks (DNNs)-based intelligent video applications. Unfortunately, it often suffers from dynamic network conditions that incur long transmission latency. Recent efforts focusing on adaptive configuration, however, rarely consider the content dynamics, resulting in suboptimal configuration decisions. In this paper, we present CAC, a Content-aware Adaptive Configuration for edge video analytics supporting both network and content adaptation. CAC employs deep reinforcement learning (DRL) techniques to learn the control policy for configuration tunning that optimizes application performance while accommodating fluctuations in network conditions and video content. Specifically, to adapt to the content dynamic, we quantify the relationship between content dynamics and the configuration settings in terms of impact coefficients to instruct the configuration decision. We compare CAC to state-of-the-art methods on a variety of real-world network traces. Results show that CAC outperforms the best state-of-the-art scheme, with improvements in average Quality of Experience (QoE) of 3.56%, 40.6%, and 59.6% in three real-world applications."
pub.1171515925,Edge-Cloud Collaborative Streaming Video Analytics with Multi-agent Deep Reinforcement Learning,"Streaming video analytics focuses on the real-time analysis of streaming video data from multiple resources, such as security cameras, and IoT devices with video capabilities. It involves applications of various techniques to extract valuable information from live video streams. Edge computing and cloud computing facilitate video stream analytics by utilizing computation resources across both ends, enabling both high accuracy and low latency. However, video streaming behaviours are dynamic and constantly evolving across the edge and the cloud. The network conditions, computing resources, and video content can change rapidly, making it crucial to continuously adjust the analytics methods to provide accurate results. Previous works both based on deep neural networks (DNNs) or heuristic algorithms learn a suitable deployment plan for streaming video analytics applications from historical data or synthetic data and therefore are not able to capture the dynamics. Hence, we propose reinforcement learning-based methods that can adapt to ongoing changes in video streaming behaviours. To ensure the scalability of video analytics in distributed environments, we implement OSMOTICGATE2, a distributed streaming video analytics system that features optimized processing pipelines and multi-agent RL-based controllers for fast adapting the system configurations across the edge and the cloud. Experiments on a real testbed show that our method outperforms baselines, assuring real-time video analysis and high accuracy in dynamic and distributed environments."
pub.1150029814,Towards low latency live streaming,"Over-the-Top (OTT) service providers need faster, cheaper, and Digital Rights Management (DRM)-capable video streaming solutions. Recently, HTTP Adaptive Streaming (HAS) has become the dominant video delivery technology over the Internet. In HAS, videos are split into short intervals called segments, and each segment is encoded at various qualities/bitrates (i.e., representations) to adapt to the available bandwidth. Utilizing different HAS-based technologies with various segment formats imposes extra cost, complexity, and latency to the video delivery system. Enabling a unified format for transmitting and storing segments at Content Delivery Network (CDN) servers can alleviate the aforementioned issues. To this end, MPEG Common Media Application Format (CMAF) is presented as a standard format for cost-effective and low latency streaming. However, CMAF has not been adopted by video streaming providers yet and it is incompatible with most legacy end-user players. This paper reveals some useful steps for achieving low latency live video streaming that can be implemented for non-DRM sensitive contents before jumping to CMAF technology. We first design and instantiate our testbed in a real OTT provider environment, and then investigate the impact of changing format, segment duration, and Digital Video Recording (DVR) window length on a real live event. The results illustrate that replacing the transport stream (.ts) format with fragmented MP4 (.fMP4) and shortening segments' duration reduces live latency significantly."
pub.1117620627,Chaos-based Delay-Constrained Green Security Communications for Fog-enabled Information-Centric Multimedia Network,"The Information-Centric Network (ICN), possessing the content-centric features, is the innovative architecture of the next generation of network. Collaborating with fog computing characterized by its strong edge power, ICN will become the development trend of the future network. The emergence of Information-Centric Multimedia Network (ICMN) can meet the increasing demand for transmission of multimedia streams in the current Internet environment. The data transmission has become more delay-constrained and convenient because of the distributed storage, the separation between the location of information and terminals, and the strong cacheability of each node in ICN. However, at the same time, the security of the multimedia streams in the delivery process still requires further protection against wiretapping, interception or attacking. In this paper, we propose the delay-constrained green security communications for ICMN based on chaotic encryption and fog computing so as to transmit multimedia streams in a more secure and time-saving way. We adapt a chaotic cryptographic method to ICMN, implementing the encryption and decryption of multimedia streams. Meanwhile, the network edge’s capability to process the encryption and decryption is enhanced. Thanks to the fog computing, the strengthened transmission speed of the multimedia streams can fulfill the need for short latency. The work in the paper is of great significance to improve the green security communications of multimedia streams in ICMN."
pub.1168659282,Evolving Mobile Cloud Gaming with 5G Standalone Network Telemetry,"Mobile cloud gaming places the simultaneous demands of high capacity and low
latency on the wireless network, demands that Private and Metropolitan-Area
Standalone 5G networks are poised to meet. However, lacking introspection into
the 5G Radio Access Network (RAN), cloud gaming servers are ill-poised to cope
with the vagaries of the wireless last hop to a mobile client, while 5G network
operators run mostly closed networks, limiting their potential for co-design
with the wider internet and user applications. This paper presents Telesa, a
passive, incrementally-deployable, and independently-deployable Standalone 5G
network telemetry system that streams fine-grained RAN capacity, latency, and
retransmission information to application servers to enable better millisecond
scale, application-level decisions on offered load and bit rate adaptation than
end-to-end latency measurements or end-to-end packet losses currently permit.
We design, implement, and evaluate a Telesa telemetry-enhanced game streaming
platform, demonstrating exact congestion-control that can better adapt game
video bitrate while simultaneously controlling end-to-end latency, thus
maximizing game quality of experience. Our experimental evaluation on a
production 5G Standalone network demonstrates a 178-249% Quality of Experience
improvement versus two state-of-the-art cloud gaming applications."
pub.1039306799,Approximate similarity search for online multimedia services on distributed CPU–GPU platforms,"Similarity search in high-dimensional spaces is a pivotal operation for several database applications, including online content-based multimedia services. With the increasing popularity of multimedia applications, these services are facing new challenges regarding (1) the very large and growing volumes of data to be indexed/searched and (2) the necessity of reducing the response times as observed by end-users. In addition, the nature of the interactions between users and online services creates fluctuating query request rates throughout execution, which requires a similarity search engine to adapt to better use the computation platform and minimize response times. In this work, we address these challenges with Hypercurves, a flexible framework for answering approximate k-nearest neighbor (kNN) queries for very large multimedia databases. Hypercurves executes in hybrid CPU–GPU environments and is able to attain massive query-processing rates through the cooperative use of these devices. Hypercurves also changes its CPU–GPU task partitioning dynamically according to the observed load, aiming for optimal response times. In our empirical evaluation, dynamic task partitioning reduced query response times by approximately 50 % compared to the best static task partition. Due to a probabilistic proof of equivalence to the sequential kNN algorithm, the CPU–GPU execution of Hypercurves in distributed (multi-node) environments can be aggressively optimized, attaining superlinear scalability while still guaranteeing, with high probability, results at least as good as those from the sequential algorithm."
pub.1132888659,Distream,"Video cameras have been deployed at scale today. Driven by the breakthrough in deep learning (DL), organizations that have deployed these cameras start to use DL-based techniques for live video analytics. Although existing systems aim to optimize live video analytics from a variety of perspectives, they are agnostic to the workload dynamics in real-world deployments. In this work, we present Distream, a distributed live video analytics system based on the smart camera-edge cluster architecture, that is able to adapt to the workload dynamics to achieve low-latency, high-throughput, and scalable live video analytics. The key behind the design of Distream is to adaptively balance the workloads across smart cameras and partition the workloads between cameras and the edge cluster. In doing so, Distream is able to fully utilize the compute resources at both ends to achieve optimized system performance. We evaluated Distream with 500 hours of distributed video streams from two real-world video datasets with a testbed that consists of 24 cameras and a 4-GPU edge cluster. Our results show that Distream consistently outperforms the status quo in terms of throughput, latency, and latency service level objective (SLO) miss rate."
pub.1112226868,FlexSaaS,"Web search engines deploy large-scale selection services on CPUs to identify a set of web pages that match user queries. An FPGA-based accelerator can exploit various levels of parallelism and provide a lower latency, higher throughput, more energy-efficient solution than commodity CPUs. However, maintaining such a customized accelerator in a commercial search engine is challenging because selection services are changed often. This article presents our design for FlexSaaS (Flexible Selection as a Service), an FPGA-based accelerator for web search selection. To address efficiency and flexibility challenges, FlexSaaS abstracts computing models and separates memory access from computation. Specifically, FlexSaaS (i) contains a reconfigurable number of matching processors that can handle various possible query plans, (ii) decouples index stream reading from matching computation to fetch and decode index files, and (iii) includes a universal memory accessor that hides the complex memory hierarchy and reduces host data access latency. Evaluated on FPGAs in the selection service of a commercial web search--the Bing web search engine—FlexSaaS can be evolved quickly to adapt to new updates. Compared to the software baseline, FlexSaaS on Arria 10 reduces average latency by 30% and increases throughput by 1.5×."
pub.1095736055,Adaptive end-to-end optimization of mobile video streaming using QoS negotiation,"Video streaming over wireless links is a non-trivial problem due to the large and frequent changes in the quality of the underlying radio channel combined with latency constraints. We believe that every layer in a mobile system must be prepared to adapt its behavior to its environment. Hence, a layer must be capable of operating in multiple modes; each mode will show a different quality and resource usage. Selecting the right mode of operation requires exchange of information between interacting layers. For example, selecting the best channel coding requires information about the quality of the channel (capacity, bit error rate) as well as the requirements (latency, reliability) of the compressed video stream generated by the source encoder. In this paper we study the application of our generic QoS negotiation scheme to a specific configuration for mobile video transmission. We describe the results of experiments studying the overall effectiveness, stability, and dynamics of adaptation of our distributed optimization approach."
pub.1094613414,Bandwidth-Aware Data Filtering in Edge-Assisted Wireless Sensor Systems,"By placing processing-capable devices at the edge of local wireless access networks, Edge Computing architectures have been recently proposed to connect mobile devices to computational power through a one-hop low-latency wireless link. In this paper, we propose a new design where edge assistance is used to control local data filtering at the mobile devices in bandwidth and energy constrained systems. We focus on realtime monitoring applications, where the video input from mobile devices is processed to centrally detect and recognize objects. The edge processor controls the activation and deactivation of local classifiers implemented by the mobile devices to remove useless portions of video frames. The objective is to adapt the video stream to time-varying bandwidth constraints, while minimizing the additional energy consumption introduced by local processing. To this end, an optimization problem is formulated for a loss function embodying the balance between the risk of violating the available bandwidth and the cost of overly-conservative data filtering. The edge assists the local decision by extracting parameters of the video, such as density of objects of interest in a frame, which influence the output of the sensor. Numerical results, obtained by performing a measurement campaign based on a real implementation, illustrate the tension between energy and bandwidth use for a Haar-feature based cascade classifier."
pub.1149360878,Novel Big Data Networking Framework Using Multihoming Optimization for Distributed Stream Computing,"One of the main technologies for big data networking framework is online multihoming optimization that is large-scale dimension table association technology in a distributed environment. It is often used in applications like real-time suggestion and research. Big data is concerned with the quality of large datasets that are distributed. These datasets demand sophisticated network technologies to adequately transmit massive share files. Dimension table association is the process of integrating multihoming stream data with offline stored dimension table data and executing data processing using novel big data frameworks, as described in this study. The current technological options for dimension table connection are assessed first, followed by accompanying optimization technologies and the design route of mainstream distributed engines. The dimension table data query is the one that has been optimized with the greatest performance. Nonetheless, the typical optimization approach is influenced by the dimension, table size, and the design route of the mainstream distributed engine—limits on data flow rate. Second, due to the limitations of existing optimization technologies for the overall consideration of the cluster in a distributed environment, a computing model suited for hybrid computing of offline batch data and real-time streaming data is provided, followed by a single-point reading. Dimension table data, the dimension table associated data technique for distribution and calculation after segmentation, and optimization of the dimension table associated calculation logic adapt to a larger dimension table scale and are no longer restricted to data connections. Since optimizing the query of dimension table, data is employed to reduce the I/O overhead and delay caused by querying dimension table in big data. Finally, both the suggested and standard dimension table association technologies are implemented on the Apache Flink stream computing engine. Through trials, the throughput and latency on data created by Alibaba’s “Double Eleven” are compared, demonstrating the usefulness of dimension table association techniques for Distributed Stream Computing optimization by utilizing multihoming networks."
pub.1168505393,Accelerated Real-Time Classification of Evolving Data Streams using Adaptive Random Forests,"Machine learning is increasingly applied to a wide range of real-time applications, with classification tasks playing a critical role in enabling intelligent decision-making. However, the phenomenon of concept drift, in which the underlying data distribution changes over time, presents a significant challenge for maintaining the accuracy of machine learning models in applications with evolving data streams, such as health monitoring or sensor data analysis. The Adaptive Random Forest (ARF) algorithm addresses this issue by coupling multiple Hoeffding Trees with a drift detector to adapt to concept drift. As training a forest of growing decision trees is a high-latency operation, custom-hardware acceleration is needed to meet the stringent latency requirements for real-time use of ARF. To the best of our knowledge, this work describes the first FPGA implementation of the ARF algorithm, focusing on achieving high hardware efficiency, scalability, and adaptability to different datasets. We present a parameterized design that incorporates various levels of parallelism, resource sharing, and pipelining, and delivers $15 \mathrm{x}-79 \mathrm{x}$ faster execution than a 40 -core CPU with a maximum accuracy loss of $13 \%$. Furthermore, our design outperforms a state-of-the-art GPU implementation, achieving $3 \mathrm{x}-21 \mathrm{x}$ faster execution while maintaining accuracy scores in the range of $0.3 \%$ to $15 \%$ of the GPU ARF implementation."
pub.1124348513,Toward Truly Immersive Holographic-Type Communication: Challenges and Solutions,"With significant advances in holographic display technology, a plethora of interactive applications, such as tele-conferencing and tele-surgery, are well on their way to integrating holographic technologies. However, hologram-based applications will place significant demands on networking infrastructure, which are not supported today. These include support for ultra-low delays, high bandwidth, and the ability to coordinate, synchronize, and dynamically adapt multiple data streams. This article articulates these challenges and points out gaps in existing networks that solutions must address. In addition, it provides an experimental analysis of novel network architectures that address one of these challenges, namely the ability to dynamically set up new flows with very low latency incurred by the first packet."
pub.1105435636,Streaming Botnet Traffic Analysis using Bio-Inspired Active Learning,"Non-stationary network traffic, together with stealth occurrences of malicious behaviors, make analyzing network traffic challenging. In this research, a machine learning framework is used to incrementally learn the network behavior and adapt to the changes in the traffic. This framework works under two main constraints: 1) label budget, 2) class imbalance; which makes it suitable for real-world network scenarios. Evaluations are performed on a public dataset with multiple Botnet scenarios under 0.5% and 5% label budgets; only around 2.2% of traffic is Botnet. Our results demonstrate the significance of the proposed Stream Genetic Programming solution and a general robustness to factors such as long latencies between instances of the same Botnet."
pub.1181497593,AI-powered threat detection in surveillance systems: A real-time data processing framework,"The increasing need for enhanced security has driven the adoption of AI-powered threat detection in surveillance systems. Traditional surveillance methods, reliant on manual monitoring, are often inefficient in detecting complex, evolving threats in real time. This review proposes a comprehensive real-time data processing framework for AI-powered threat detection in surveillance systems, designed to automate and optimize threat identification, classification, and response. The framework integrates AI algorithms, including machine learning and deep learning models, to analyze vast amounts of surveillance data from various sources such as video feeds, audio recordings, and sensor inputs. It utilizes techniques like object detection, facial recognition, and anomaly detection to identify potential threats, while leveraging stream processing frameworks (e.g., Apache Kafka, Apache Flink) to ensure low-latency, real-time analysis. Edge computing is incorporated to reduce network bottlenecks and enable faster decision-making closer to the data source. The framework also addresses the challenges of high data volume and velocity, as well as the need for scalable, flexible infrastructure. Security measures such as encryption, identity and access management (IAM), and compliance with data privacy regulations ensure that sensitive information is protected. The inclusion of continuous model training allows the system to adapt to emerging threats and reduce false positives and negatives. Case studies from urban environments, critical infrastructure, and law enforcement demonstrate the practical applications and effectiveness of this AI-driven approach. By integrating real-time data processing with advanced AI models, the framework provides a robust solution for improving the efficiency and accuracy of threat detection in modern surveillance systems. This research contributes to the growing field of AI-enhanced security, paving the way for future advancements in intelligent surveillance."
pub.1127430374,FEPDS: A Proposal for the Extraction of Fuzzy Emerging Patterns in Data Streams,"Nowadays, most data is generated by devices that produce data continuously. These kinds of data can be categorized as data streams and valuable insights can be extracted from them. In particular, the insights extracted by emerging patterns (EPs) are interesting in a data stream context as easy, fast, and reliable decisions can be made. However, their extraction is a challenge due to the necessary response time, memory, and continuous model updates. In this article, an approach for the extraction of EPs in data streams is presented. It processes the instances by means of batches following an adaptive approach. The learning algorithm is an evolutionary fuzzy system where previous knowledge is employed in order to adapt to concept drift. A wide experimental study has been performed in order to show both the suitability of the approach in combating concept drift and the quality of the knowledge extracted. Finally, the proposal is applied to a case study related to the continuous determination of the profiles of New York City cab customers according to their fare amount, in order to show its potential."
pub.1012200365,Energy efficient wireless packet scheduling and fair queuing," As embedded systems are being networked, often wirelessly, an increasingly larger share of their total energy budget is due to the communication. This necessitates the development of power management techniques that address communication subsystems, such as radios, as opposed to computation subsystems, such as embedded processors, to which most of the research effort thus far has been devoted. In this paper, we present techniques for energy efficient packet scheduling and fair queuing in wireless communication systems. Our techniques are based on an extensive slack management approach that dynamically adapts the output rate of the system in accordance with the input packet arrival rate. We use a recently proposed radio power management technique, dynamic modulation scaling (DMS), as a control knob to enable energy-latency trade-offs during wireless packet transmission. We first analyze a single input stream scenario, and describe a rate adaptation technique that results in significantly lower energy consumption (reductions of up to 10 ×), while still bounding the resulting packet delays. By appropriately setting the various parameters of our algorithm, the system can be made to traverse the energy-latency-fidelity trade-off space. We extend our techniques to a multiple input stream scenario, and present E 2 WFQ , an energy efficient version of the weighted fair queuing (WFQ) algorithm for fair packet scheduling. Simulation results show that large energy savings can be obtained through the use of E 2 WFQ , with only a small, bounded increase in worst case packet latency. Further, our results demonstrate that E 2 WFQ does not adversely affect the throughput allocation (and hence, fairness) of WFQ. "
pub.1150362393,Optimal Latency-Oriented Coding and Scheduling in Parallel Queuing Systems,"The evolution of 5G and Beyond networks has enabled new applications with stringent end-to-end latency requirements, but providing reliable low-latency service with high throughput over public wireless networks is still a significant challenge. One of the possible ways to solve this is to exploit path diversity, encoding the information flow over multiple streams across parallel links. The challenge presented by this approach is the design of joint coding and scheduling algorithms that adapt to the state of links to take full advantage of path diversity. In this paper, we address this problem for a synchronous traffic source that generates data blocks at regular time intervals (e.g., a video with constant frame rate) and needs to deliver each block within a predetermined deadline. We first develop a closed-form performance analysis in the simple case of two parallel servers without any buffering and single-packet blocks, and propose a model for the general problem based on a Markov Decision Process (MDP). We apply policy iteration to obtain the coding and scheduling policy that maximizes the fraction of source blocks delivered within the deadline: our simulations show the drawbacks of different commonly applied heuristic solutions, drawing general design insights on the optimal policy."
pub.1147687241,Optimizing ADWIN for steady streams,"With the ever-growing data generation rates and stringent constraints on the latency of analyzing such data, stream analytics is overtaking. Learning from data streams, aka online machine learning, is no exception. However, online machine learning comes with many challenges for the different aspects of the learning process, starting from the algorithm design to the evaluation method. One of these challenges is the ability of a learning system to adapt to the change in data distribution, known as concept drift, to maintain the accuracy of the predictions. Over time, several drift detection approaches have been proposed. A prominent approach is adaptive windowing (ADWIN) which can detect changes in features data distribution without explicit feedback on the correctness of the prediction. Several variants for ADWIN have been proposed to enhance its runtime performance, w.r.t throughput, and latency. However, the drift detection accuracy of these variants was not compared with the original algorithm. Moreover, there is no study concerning the memory consumption of the variants and the original algorithm. Additionally, the evaluation was done on synthetic datasets with a considerable number of drifts not covering all types of drifts or steady streams, those that do not have drifts at all or almost negligible drifts. The contribution of this paper is two-fold. First, we compare the original Adaptive Window (ADWIN) and its variants: Serial, HalfCut, and Optimistic in terms of drift detection accuracy, detection speed, and memory consumption, represented in the internal window size. We compare them using synthetic data sets covering different types of concept drifts, namely: incremental, gradual, abrupt, and steady. We also use two real-life datasets whose drifts are unknown. Second, we present ADWIN++. We use an adaptive bucket dropping technique to control window size. We evaluate our technique on the same data sets above and new datasets with fewer drifts. Experiments show that our approach saves about 80% of memory consumption. Moreover, it takes less time to detect concept drift and maintains the drift detection accuracy."
pub.1140805304,Optimal Latency-Oriented Scheduling in Parallel Queuing Systems,"The evolution of 5G and Beyond networks has enabled new applications with
stringent end-to-end latency requirements, but providing reliable low-latency
service with high throughput over public wireless networks is still a
significant challenge. One of the possible ways to solve this is to exploit
path diversity, encoding the information flow over multiple streams across
parallel links. The challenge presented by this approach is the design of joint
coding and scheduling algorithms that adapt to the state of links to take full
advantage of path diversity. In this paper, we address this problem for a
synchronous traffic source that generates data blocks at regular time intervals
(e.g., a video with constant frame rate) and needs to deliver each block within
a predetermined deadline. We first develop a closed-form performance analysis
in the simple case of two parallel servers without any buffering and
single-packet blocks, and propose a model for the general problem based on a
Markov Decision Process (MDP). We apply policy iteration to obtain the coding
and scheduling policy that maximizes the fraction of source blocks delivered
within the deadline: our simulations show the drawbacks of different commonly
applied heuristic solutions, drawing general design insights on the optimal
policy."
pub.1132783378,Concept learning using one-class classifiers for implicit drift detection in evolving data streams,"Data stream mining has become an important research area over the past decade due to the increasing amount of data available today. Sources from various domains generate a near-limitless volume of data in temporal order. Such data are referred to as data streams, and are generally nonstationary as the characteristics of data evolves over time. This phenomenon is called concept drift, and is an issue of great importance in the literature, since it makes models obsolete by decreasing their predictive performance. In the presence of concept drift, it is necessary to adapt to change in data to build more robust and effective classifiers. Drift detectors are designed to run jointly with classification models, updating them when a significant change in data distribution is observed. In this paper, we present an implicit (unsupervised) algorithm called One-Class Drift Detector (OCDD), which uses a one-class learner with a sliding window to detect concept drift. We perform a comprehensive evaluation on mostly recent 17 prevalent concept drift detection methods and an adaptive classifier using 13 datasets. The results show that OCDD outperforms the other methods by producing models with better predictive performance on both real-world and synthetic datasets."
pub.1012459099,LiveTraj,"We present LiveTraj, a novel system for tracking trajectories in a live video stream in real time, backed by a cloud platform. Although trajectory tracking is a well-studied topic in computer vision, so far most attention has been devoted to improving the accuracy of trajectory tracking, rather than the efficiency. To our knowledge, LiveTraj is the first that achieves real-time efficiency in trajectory tracking, which can be a key enabler in many important applications such as video surveillance, action recognition and robotics. LiveTraj is based on a state-of-the-art approach to (offline) trajectory tracking; its main innovation is to adapt this base solution to run on an elastic cloud platform to achieve real-time tracking speed at an affordable cost. The video demo shows the offline base solution and LiveTraj side by side, both running on a video stream containing human actions. Besides demonstrating the real-time efficiency of LiveTraj, our video demo also exhibits important system parameters to the audience such as latency and cloud resource usage for different components of the system. Further, if the conference venue provides sufficiently fast Internet connection to our cloud platform, we also plan to demonstrate LiveTraj on-site, during which we will show LiveTraj identifying and tracking trajectories from a live video stream captured by a camera."
pub.1061714294,ITP: An Image Transport Protocol for the Internet,"Images account for a significant and growing fraction of Web downloads. The traditional approach to transporting images uses TCP, which provides a generic reliable in-order byte-stream abstraction, but which is overly restrictive for image data. We analyze the progression of image quality at the receiver with time, and show that the in-order delivery abstraction provided by a TCP-based approach prevents the receiver application from processing and rendering portions of an image when they actually arrive. The end result is that an image is rendered in bursts interspersed with long idle times rather than smoothly. This paper describes the design, implementation, and evaluation of the image transport protocol (ITP) for image transmission over loss-prone congested or wireless networks. ITP improves user-perceived latency using application-level framing (ALF) and out-of-order application data unit (ADU) delivery, achieving significantly better interactive performance as measured by the evolution of peak signal-to-noise ratio (PSNR) with time at the receiver. ITP runs over UDP, incorporates receiver-driven selective reliability, uses the congestion manager (CM) to adapt to network congestion, and is customizable for specific image formats (e.g., JPEG and JPEG2000). ITP enables a variety of new receiver post-processing algorithms such as error concealment that further improve the interactivity and responsiveness of reconstructed images. Performance experiments using our implementation across a variety of loss conditions demonstrate the benefits of ITP in improving the interactivity of image downloads at the receiver."
pub.1094581571,An image transport protocol for the Internet,"Images account for a significant and growing fraction of Web downloads. The traditional approach to transporting images uses TCP, which provides a generic reliable, in-order byte-stream abstraction, but which is overly, restrictive for image data. We analyze the progression of image quality at the receiver with time and show that the in-order delivery abstraction provided by a TCP-based approach prevents the receiver application from processing and rendering portions of an image when they, actually, arrive. The end result is that an image is rendered in bursts interspersed with long idle times rather than smoothly. This paper describes the design, implementation, and evaluation of the Image Transport Protocol (ITP) for image transmission over loss-prone congested or wireless networks. ITP improves user-perceived latency using application level framing (ALF) and out-of-order pplication data unit (ADU) delivery achieving significantly better interactive performance as measured by the evolution of peak signal-to-noise ratio (PSNR) with time at the receiver ITP runs over UDP, incorporates receiver-driven selective reliability uses the congestion manager (CM) to adapt to network congestion, and is customizable for specific image formats (e.g., JPEG and JPEG2000). ITP enables a variety of new receiver post-processing algorithms such as error concealment that further improve the interactivity and responsiveness of reconstructed images. Performance experiments using our implementation across a variety of loss conditions demonstrate the benefits of ITP in improving the interactivity of image downloads at the receiver."
pub.1037010563,Hierarchical Synchronized Multimedia Multicast for Mobile Hosts in Heterogeneous Wireless Networks,"For supporting handoff mobile users on heterogeneous wireless networks to synchronously receive and play out multicast multimedia stream data, we propose a two-layer Hierarchical Synchronized Multimedia Multicast (HSMM) architecture to enhance the single-layer Synchronized Multimedia Multicast (SMM) [1]. In HSMM, each wireless network operator can adapt its own management mechanism, such as routing protocol, access control, etc., and further define the range of Guarantee Region (GR) to satisfy different management requirements. Compared to SMM and the traditional Remote Subscription (RS) protocol, HSMM will significantly reduce total amounts of synchronization buffer of foreign agents, join latency and buffer replenishment time of mobile users, and finally achieve a better playback quality."
pub.1134828698,An Adaptive Video Streaming Framework for Peer-To-Peer 5G Networks: Paving the Road to 5G-IMS,"The IMS architecture presents many disadvantages which are centralized control, low efficiency, and low scalability in terms of core network equipment compared to network infrastructures using Cloud Computing. Cloud Computing is a new information technology paradigm, offering dynamically scalable resources, often through virtual machines and accessible as services on the Internet. IMS migration to the cloud can improve the performance of the IMS infrastructure. Video streaming in the IMS architecture of 5G technology is proposed for use in high-quality multimedia applications, the use of adaptive streaming stream is used to adapt the quality for a heterogeneous network of equipment. Knowing that 5G technology is based on cloud computing techniques and virtualization, coexistence between IMS and Peer to Peer is possible with the BitTorrent protocol which offers a solution compatible with 5G technology, by offering BitTorrent Sync a solution based on Cloud. Use of a video streaming adaptation with an architecture based on 5G technology for both to distribute the load between the elements of the P2P IMS architecture, decrease the response time for the procedures (User authentication, time required for “a Peers joined the network”, time to download songs (Reduce latency in the network)."
pub.1094921892,A multithreaded multimedia processor merging on-chip multiprocessors and distributed vector pipelines,"This paper introduces a new multimedia processor architecture that exploits all sorts of parallelism that can be found in multimedia applications, namely data, instruction, thread and transfer level parallelism. The proposed architecture issues simultaneously several scalar and vector threads on an array of functional units to mask pipeline stalls due to functional and memory units latencies or cache misses. The clustering of the architecture in several processors communicating through register channels and stream Buffer allows high clock frequency implementation and reduces the ever growing interconnection delay influence. Several motion estimation algorithms have already been implemented to show the efficiency of the architecture to adapt its resources allocation to better fulfil the application intrinsic vectorizing ratio."
pub.1094059379,An adaptive cryptographic engine for IPSec architectures,"Architectures that implement the Internet Protocol Security (IPSec) standard have to meet the enormous computing demands of cryptographic algorithms. In addition, IPSec architectures have to be flexible enough to adapt to diverse security parameters. This paper proposes an FPGA-based Adaptive Cryptographic Engine (ACE) for IPSec architectures. By taking advantage of FPGA technology, ACE can adapt to diverse security parameters on the fly while providing superior performance compared with software-based approaches. For example, for the final candidate algorithms of the Advanced Encryption Standard (AES), our techniques lead to throughput speed-up of 4-20 while the key-setup latency time is reduced by a factor of 20-700 compared with software-based approaches. We also develop a compression technique that reduces the memory requirements of ACE without the need for dedicated hardware. Though data compression has been extensively studied before, we are not aware of any prior work that addresses the compression problem of FPGA-based embedded systems with respect to the implementation cost. Using our technique, we demonstrate up to 40% savings in memory for various configuration bit-streams."
pub.1101003973,Cross Layer Image Optimization (CLIO) for Wireless Video Transmission Over 802.11ad Multi-Gigabit Channels,"Wireless display is known to be the future of display technology where a world full of inexpensive displays is used for streaming various multimedia applications. Due to the stringent requirements for high quality video transmission with low latency, a cross layer approach is needed to adapt to fast changes of the wireless link. Here, we consider cross layer approaches over IEEE 802.11ad multi-gigabit wireless networks to increase QoS guarantee for real time multimedia applications. This includes dynamic layering or partitioning on top of IEEE 802.11ad to switch among a set of procedures/parameters in the transmitter and receiver, including error correction technique, compression, or packetization on a packet by packet basis. We propose a layered-based compression designed for an unequal error protection for treating ordered bit streams of different layers differently. Dynamic importance aware tagging for packets is proposed as the third method. IEEE 802.11ad MAC and transport layer protocols and signaling are modified to adapt for importance levels and corresponding information exchange."
pub.1038818126,Flexible TDM-based resource management in on-chip networks,"Time-division multiplexing (TDM) is the commonly used and well established solution to the problem of sharing resources in real-time Networks-on-Chip (NoCs). TDM timing is well predictable, simplifies worst-case analysis and is easy to implement. However, it introduces a constant, periodic and non-work-conserving resource sharing scheme. This challenges resource efficiency whenever the applications expose dynamics in execution time, communication volume and whenever the system is not highly loaded. In this work, we present a flexible TDM approach for NoCs where the TDM cycle adapts dynamically to the current load of data streams accessing the network. This is performed using a global arbitration layer managed by a scheduling unit called Resource Manager (RM) and a dedicated protocol. We present a formal worst-case timing analysis of this approach which allows to provide guarantees to all streams using the NoC. Finally, we demonstrate that our mechanism not only exhibits higher average performance but, even more importantly, consistently reaches smaller formally guaranteed worst-case network latencies than static TDM for realistic levels of utilization."
pub.1158872008,Dynamic Probabilistic Reliable Broadcast,"Byzantine reliable broadcast is a fundamental primitive in distributed
systems that allows a set of processes to agree on a message broadcast by a
dedicated process, even when some of them are malicious (Byzantine). It
guarantees that no two correct processes deliver different messages, and if a
message is delivered by a correct process, every correct process eventually
delivers one. Byzantine reliable broadcast protocols are known to scale poorly,
as they require $\Omega(n^2)$ message exchanges, where $n$ is the number of
system members. The quadratic cost can be explained by the inherent need for
every process to relay a message to every other process. In this paper, we
explore ways to overcome this limitation, by casting the problem to the
probabilistic setting. We propose a solution in which every broadcast message
is validated by a small set of witnesses, which allows us to maintain low
latency and small communication complexity. In order to tolerate the slow
adaptive adversary, we dynamically select the witnesses through a novel
stream-local hash function: given a stream of inputs, it generates a stream of
output hashed values that adapts to small deviations of the inputs. Our
performance analysis shows that the proposed solution exhibits significant
scalability gains over state-of-the-art protocols."
pub.1017716873,The next database revolution,"Database system architectures are undergoing revolutionary changes. Most importantly, algorithms and data are being unified by integrating programming languages with the database system. This gives an extensible object-relational system where non-procedural relational operators manipulate object sets. Coupled with this, each DBMS is now a web service. This has huge implications for how we structure applications. DBMSs are now object containers. Queues are the first objects to be added. These queues are the basis for transaction processing and workflow applications. Future workflow systems are likely to be built on this core. Data cubes and online analytic processing are now baked into most DBMSs. Beyond that, DBMSs have a framework for data mining and machine learning algorithms. Decision trees, Bayes nets, clustering, and time series analysis are built in; new algorithms can be added. There is a rebirth of column stores for sparse tables and to optimize bandwidth. Text, temporal, and spatial data access methods, along with their probabilistic reasoning have been added to database systems. Allowing approximate and probabilistic answers is essential for many applications. Many believe that XML and xQuery will be the main data structure and access pattern. Database systems must accommodate that perspective. External data increasingly arrives as streams to be compared to historical data; so stream-processing operators are being added to the DBMS. Publish-subscribe systems invert the data-query ratios; incoming data is compared against millions of queries rather than queries searching millions of records. Meanwhile, disk and memory capacities are growing much faster than their bandwidth and latency, so the database systems increasingly use huge main memories and sequential disk access. These changes mandate a much more dynamic query optimization strategy - one that adapts to current conditions and selectivities rather than having a static plan. Intelligence is moving to the periphery of the network. Each disk and each sensor will be a competent database machine. Relational algebra is a convenient way to program these systems. Database systems are now expected to be self-managing, self-healing, and always-up. We researchers and developers have our work cut out for us in delivering all these features."
pub.1164763508,Adaptive Network Configuration for Efficient and Accurate Neural Video Inference,"Cameras are widely used in many fields, e.g., intelligent transportation, autonomous driving, surveillance, etc. It is thus vital to conduct video analytics in an efficient and accurate manner. However, camera’s built-in capacity is insufficient to support neural network processing, while offloading video streams incurs prohibitive latency and communication cost. In this paper, we find that frame rate, resolution, and neural network inference model, have an intertwined impact on network resource demand. The optimal configuration of these factors also varies with video content feature. To address these challenges, we propose a dynamic configuration update scheme based on predictive video perception using a long short-term memory (LSTM) neural network, to adapt configuration to content changes. This scheme consists of a change detector and a configuration profiler. Through theoretical modeling and analysis, we derive the detection thresholds for both dynamic and stationary video contents, considering the LSTM prediction error. The configuration profiler then updates system by solving an optimization problem, which maximizes the overall utility considering analytics accuracy and resource consumption. Extensive real-world traces-based experiments show that the proposed scheme can save profiling resources by up to 95% while ensuring high accuracy compared with other benchmarks."
pub.1035006073,Paceline,"Increasingly, multimedia applications need higher bandwidth to provide better quality, for example in multi-party HD video conferencing. This demanding class of interactive applications simultaneously require high bandwidth and low end-to-end latency, a conflicting combination that is poorly supported in existing transports. Conventional wisdom dictates that network applications have a choice of transport protocols between TCP, if a reliable service model is desired, or UDP, if control over timing is required. In this paper we present Paceline, an enhanced transport we have devised to support interactive, high-bandwidth applications. Paceline enhances the transport service model to support application adaptation, through prioritization to provide timely delivery of important data, and cancellation to adapt the application rate to match available bandwidth. However, contrary to conventional wisdom, Paceline has not been implemented over UDP, nor does Paceline propose changes to TCP. We believe that the deployment obstacles and duplication of effort faced by solutions that alter or replace TCP entirely outweigh the challenges of mitigating its impairments. Instead, Paceline employs several mechanisms to support timely priority order delivery and cancellation above TCP: an application-level rate controller to reduce queueing delay due to excessive socket buffering, failover among connections to handle extreme cases of congestion, and message fragmentation to reduce the granularity of preemption. Our evaluation shows that Paceline improves upon conventional end-to-end latency shortcomings of using TCP, by factor of 3 in median latency and a factor of 4 in worst case latency. Meanwhile, Paceline is able to preserve TCP's performance in terms of fairness and utilization. Finally, we compare application performance with Paceline to a representative TCP alternative, Structured Stream Transport (SST), showing Paceline to be highly competitive."
pub.1046823189,A similarity-based approach for data stream classification,"Incremental learning techniques have been used extensively to address the data stream classification problem. The most important issue is to maintain a balance between accuracy and efficiency, i.e., the algorithm should provide good classification performance with a reasonable time response. This work introduces a new technique, named Similarity-based Data Stream Classifier (SimC), which achieves good performance by introducing a novel insertion/removal policy that adapts quickly to the data tendency and maintains a representative, small set of examples and estimators that guarantees good classification rates. The methodology is also able to detect novel classes/labels, during the running phase, and to remove useless ones that do not add any value to the classification process. Statistical tests were used to evaluate the model performance, from two points of view: efficacy (classification rate) and efficiency (online response time). Five well-known techniques and sixteen data streams were compared, using the Friedman’s test. Also, to find out which schemes were significantly different, the Nemenyi’s, Holm’s and Shaffer’s tests were considered. The results show that SimC is very competitive in terms of (absolute and streaming) accuracy, and classification/updating time, in comparison to several of the most popular methods in the literature."
pub.1146522402,Adaptive Aggregation For Federated Learning,"Advances in federated learning (FL) algorithms,along with technologies like
differential privacy and homomorphic encryption, have led to FL being
increasingly adopted and used in many application domains. This increasing
adoption has led to rapid growth in the number, size (number of
participants/parties) and diversity (intermittent vs. active parties) of FL
jobs. Many existing FL systems, based on centralized (often single) model
aggregators are unable to scale to handle large FL jobs and adapt to parties'
behavior.
  In this paper, we present a new scalable and adaptive architecture for FL
aggregation. First, we demonstrate how traditional tree overlay based
aggregation techniques (from P2P, publish-subscribe and stream processing
research) can help FL aggregation scale, but are ineffective from a resource
utilization and cost standpoint. Next, we present the design and implementation
of AdaFed, which uses serverless/cloud functions to adaptively scale
aggregation in a resource efficient and fault tolerant manner. We describe how
AdaFed enables FL aggregation to be dynamically deployed only when necessary,
elastically scaled to handle participant joins/leaves and is fault tolerant
with minimal effort required on the (aggregation) programmer side. We also
demonstrate that our prototype based on Ray scales to thousands of
participants, and is able to achieve a >90% reduction in resource requirements
and cost, with minimal impact on aggregation latency."
pub.1128357142,Context sensitivity across multiple time scales with a flexible frequency bandwidth,"ABSTRACT Everyday auditory streams are complex, including spectro-temporal content that varies at multiple time scales. Using EEG, we investigate the sensitivity of human auditory cortex to the content of past stimulation in unattended sequences of equiprobable tones. In 3 experiments including 82 participants overall, we found that neural responses measured at different latencies after stimulus onset were sensitive to frequency intervals computed over distinct time scales. Importantly, early responses were sensitive to a longer history of stimulation than later responses. To account for these results, we tested a model consisting of neural populations with frequency-specific but broad tuning that undergo adaptation with exponential recovery. We found that the coexistence of neural populations with distinct recovery rates can explain our results. Furthermore, the adaptation bandwidth of these populations depended on spectral context – it was wider when the stimulation sequence had a wider frequency range. Our results provide electrophysiological evidence as well as a possible mechanistic explanation for dynamic and multi-scale context-dependent auditory processing in the human cortex.  SIGNIFICANCE STATEMENT It has become clear that brain processing of sensory stimuli depends on their temporal context, but context can be construed at time scales from the recent millisecond to life-long. How do different contextual time scales affect sensory processing? We show that auditory context is integrated across at least two separate time scales, and that at both of these time scales responses dynamically adapt to a varying frequency stimulation range. Using computational modeling, we develop a rigorous methodology to estimate the time and frequency scales of context integration for separate response components. Our robust results replicated across 3 EEG experiments, and contribute to the understanding of neural mechanisms supporting complex and dynamic context integration. "
pub.1139099544,LATEST: Learning-Assisted Selectivity Estimation Over Spatio-Textual Streams,"Selectivity and cardinality estimation are main driving factors for developing cheap query plans and ultimately faster query processing. Traditionally, database systems use estimation data structures, e.g., histograms, to maintain data summaries. Machine learning models have recently been employed, acting as black boxes, in several database tasks, including cardinality estimation. In the dynamic streaming environments, both estimation data structures and machine learning models struggle with adaptation for dynamic changes in data and query workloads. This paper proposes LATEST; a system module that uses machine learning to enable dynamic adaptation of estimation data structures. For spatial-keyword queries in a streaming environment, it shows on par or better performance than the state-of-the-art estimators. LATEST builds an incremental supervised learning model over a moving time window that helps the underlying system to switch among several estimation structures to keep estimation accuracy high at all times. As an incremental learner, LATEST effectively adapts to dynamic changes of both data and queries in streaming environments. Our extensive experiments on three real datasets and various query workloads verify the effectiveness of LATEST with higher accuracy and lower response times over the state-of-the-art estimators."
pub.1051563090,QoE-Based Low-Delay Live Streaming Using Throughput Predictions,"
                    Recently, Hypertext Transfer Protocol (HTTP)-based adaptive streaming has become the de facto standard for video streaming over the Internet. It allows clients to dynamically adapt media characteristics to the varying network conditions to ensure a high quality of experience (QoE)—that is, minimize playback interruptions while maximizing video quality at a reasonable level of quality changes. In the case of live streaming, this task becomes particularly challenging due to the latency constraints. The challenge further increases if a client uses a wireless access network, where the throughput is subject to considerable fluctuations. Consequently, live streams often exhibit latencies of up to 20 to 30 seconds. In the present work, we introduce an adaptation algorithm for HTTP-based live streaming called
                    LOLYPOP
                    (short for low-latency prediction-based adaptation), which is designed to operate with a transport latency of a few seconds. To reach this goal, LOLYPOP leverages Transmission Control Protocol throughput predictions on multiple time scales, from 1 to 10 seconds, along with estimations of the relative prediction error distributions. In addition to satisfying the latency constraint, the algorithm heuristically maximizes the QoE by maximizing the average video quality as a function of the number of skipped segments and quality transitions. To select an efficient prediction method, we studied the performance of several time series prediction methods in IEEE 802.11 wireless access networks. We evaluated LOLYPOP under a large set of experimental conditions, limiting the transport latency to 3 seconds, against a state-of-the-art adaptation algorithm called
                    FESTIVE
                    . We observed that the average selected video representation index is by up to a factor of 3 higher than with the baseline approach. We also observed that LOLYPOP is able to reach points from a broader region in the QoE space, and thus it is better adjustable to the user profile or service provider requirements.
                  "
pub.1118500353,QoE-Based Low-Delay Live Streaming Using Throughput Predictions,"Recently, HTTP-based adaptive streaming has become the de facto standard for
video streaming over the Internet. It allows clients to dynamically adapt media
characteristics to network conditions in order to ensure a high quality of
experience, that is, minimize playback interruptions, while maximizing video
quality at a reasonable level of quality changes. In the case of live
streaming, this task becomes particularly challenging due to the latency
constraints. The challenge further increases if a client uses a wireless
network, where the throughput is subject to considerable fluctuations.
Consequently, live streams often exhibit latencies of up to 30 seconds. In the
present work, we introduce an adaptation algorithm for HTTP-based live
streaming called LOLYPOP (Low-Latency Prediction-Based Adaptation) that is
designed to operate with a transport latency of few seconds. To reach this
goal, LOLYPOP leverages TCP throughput predictions on multiple time scales,
from 1 to 10 seconds, along with an estimate of the prediction error
distribution. In addition to satisfying the latency constraint, the algorithm
heuristically maximizes the quality of experience by maximizing the average
video quality as a function of the number of skipped segments and quality
transitions. In order to select an efficient prediction method, we studied the
performance of several time series prediction methods in IEEE 802.11 wireless
access networks. We evaluated LOLYPOP under a large set of experimental
conditions limiting the transport latency to 3 seconds, against a
state-of-the-art adaptation algorithm from the literature, called FESTIVE. We
observed that the average video quality is by up to a factor of 3 higher than
with FESTIVE. We also observed that LOLYPOP is able to reach a broader region
in the quality of experience space, and thus it is better adjustable to the
user profile or service provider requirements."
pub.1093263825,Adaptive resource allocation for streaming applications,"Streaming applications often have latency and throughput requirements due to timing critical signal processing, or the time critical interaction with their environment. Mapping such applications to a multi-core architecture is commonly done at design-time to be able to analyze the complex design-space. However, such design-flows cannot deal with a dynamic platform or a dynamic set of applications. Hardware faults and resources claimed by other applications may render the assumed available resources inaccessible. To avoid the assumptions posed on the state of the platform by a fixed resource allocation, applications should be designed with location transparency in mind. Applications must be analyzed at design-time to determine the required resource budget, independent of which specific resources will be allocated. Sufficient performance can be guaranteed when such applications are mapped onto an architecture in which each resource is arbitrated using a budget scheduler. Within the Cutting edge Reconfigurable ICs for Stream Processing (CRISP) project, a many-core platform is developed that adheres to these requirements. Using the configuration features of the platform, the system is able to control at run-time what resources are being used by the applications. This paper shows that run-time resource allocation can effectively adapt to the available set of resources, providing partial distribution transparency to the user. As an example, a GNSS receiver is mapped to the platform containing faulty hardware components. A few resources remain critical, but in most cases the faulty components can be circumvented, such that adequate resources can be allocated to the application at run-time."
pub.1169110764,Content Caching Utilizing Scalable Video Coding (SVC) in Wireless Edge Networks,"It is possible to enhance video streaming efficiency and lessen the stress on origin servers by prefetching well-liked videos into the local caches of edge nodes. Scalable Video Coding (SVC) is a video coding standard that allows a video stream to be encoded into multiple layers or “scalability layers,” each providing a different level of video quality and bit rate. This makes it possible to adapt the video stream to different network conditions and device capabilities. Integrating SV C with edge caching can provide several benefits for video streaming. By storing multiple scalability layers in the edge cache, the video stream can be adapted to the network conditions and device capabilities of the end-user. Furthermore, by caching multiple scalability layers, the edge cache can provide more granular control over the video quality delivered to end-users. In this study, we talk about the difficulties that come with scaled videos unique watching requirements and varying content popularity. We give a brief review of the current caching strategies and list the factors to consider when choosing a caching strategy. After that, a case study is presented in which the transmission latency is measured as a performance parameter. In comparison to the current caching systems running without SV C, simulation findings show that taking into account the practical requirements of end users can drastically reduce the content transmission delay."
pub.1154227292,The stream data warehouse: Page replacement algorithms and quality of service metrics,"The stream data warehouse is an answer to the rapidly changing world of data analysis, which demands reliable and up-to-date results, obtained in a near real-time manner. Therefore it is a subject of recent research involving such areas as continuous updates and low-latency response, for example. In this paper, we study the stream adaptation of the OLAP cube and, in particular — its memory paging mechanism. It is driven by the page replacement algorithm, which manages the efficient data transfer and thus supplies users with constantly updatable data cubes. The following paper introduces an entirely novel approach to this topic. By perceiving the page replacement process as a multi-objective optimization problem, we propose three new algorithms that constantly analyze their varying environment and adapt to those changes by adjusting their behavior. Moreover, they consider user-provided constraints, which impose maximal values of specific parameters that cannot be exceeded. In addition to the page replacement algorithms, we propose two distinct quality of service metrics that measure the overall efficiency of data transfer inside the stream data warehouse. In order to verify and compare the new algorithms with their older counterparts, a series of experiments were conducted. Their results have confirmed that the proposed algorithms meet their requirements and visibly outperform the original solutions. The average wait time decreased between 25% and 66% (from 1.3x to 3.0x respectively, depending on the chosen algorithm), whereas the peak wait time decreased by approximately 99% (between 100x and 190x respectively)."
pub.1143049930,Scheduling Massive Camera Streams to Optimize Large-Scale Live Video Analytics,"In smart cities, more and more government departments will make use of live analytics of videos from surveillance cameras in their tasks, such as vehicle traffic monitoring and criminal detection. Obviously, it is costly for each individual department to deploy its own infrastructure, i.e., cameras and analytics system. In this paper, we consider a scenario in which a city deploys an infrastructure and departments submit requests to access and analyze videos for their own purposes. The live analytics of massive streams is computation-intensive and the tasks might be latency-critical, which makes scheduling massive streams to optimize all tasks an essential and challenging work. We exploit an end-edge-cloud architecture and propose an adaptive system to schedule the massive camera streams and tasks, which considers all factors affecting the computation and networking resource consumption, e.g., sharing of model computation, video quality, model partition, and task placement. Particularly, the resource consumption of Faster R-CNN + ResNet101 under each partition scheme is profiled for the first time and we notice the partition must be used together with lossless compression techniques to be beneficial. Furthermore, sometimes tasks might be required to migrate because the scheduling decision made by the system changes to adapt to the changing resource supply and demand. In order to avoid the performance degradation during migration, we propose a non-destructive migration scheme and implement it in the system. Simulations demonstrate our system achieves a total utility close to the maximum and our analytics system performs better than state-of-the-art solutions."
pub.1095215765,ROI based video streaming for 3D remote rendering,"This paper proposes a low computational method to perform ROI (Region Of Interest) based video encoding and adaptive streaming for remote rendering applications. The main objective of the proposed solution is to minimize the latency in the interactive loop even when facing poor transmission conditions. In order to do that, the knowledge of the depth map information provided by the rendering engine is exploited by the real-time video encoder to adapt the bitrate of the transmitted stream. Especially, thanks to an efficient coupling between the rendering and the video encoding stages, the macroblocks of each video frame are encoded with different quantization steps that follow an ROI partitioning. The details of this partitioning algorithm are provided as well with some implementation considerations. The simulation results demonstrate the benefit of our adaptive approach from the user experience point of view."
pub.1138615371,Cost-Efficient Request Dispatching in Geo-distributed Cloud Gaming Infrastructure,"Cloud gaming is a recent approach to gaming, where processing for the games occurs on well-equipped servers within the cloud and a video stream is returned to the user, meaning users can play high-end games on devices that lack computational power. Different games require different amounts of computational resources and computation times. It would be desirable to efficiently pack a number of servers with multiple games at once, however this is complicated within a geo-distributed cloud system as we must consider that not every data center can fulfil every game request due to latency requirements. Within this work, we present shadow routing algorithms to distribute game requests to cloud data centers and also to pack the servers within the data centers with these game requests. These algorithms are designed to operate in order to minimize total cost from server hire and bandwidth usage, and we prove their performance is asymptotically close to optimal. An experiment using realistic arrival rates is given, and the results verify our theory within a realistic context. Also shown using proof and experimentation is that the algorithms can adapt themselves to periodic changes as demand raises and falls while remaining close to the optimal, which is a particular weak point of other schemes."
pub.1044746552,Streaming Video Using Dynamic Rate Shaping and TCP Congestion Control,"We present a new technique for streaming real time video on today's Internet, based ondynamic rate shapingandTCP congestion control. Dynamic rate shaping is a signal processing technique that adapts the rate of compressed video (MPEG-1, MPEG-2, H.26x) to dynamically varying bandwidth constraints. This provides an interface (or filter) between the source and the network, with which the encoder's output (either live or stored) can be perfectly matched to the network's available bandwidth. We couple this adaptation capability with the use of a new semi-reliable protocol that uses the TCP congestion window to pace the delivery of data into the network, but without using other TCP algorithms that are poorly suited to real time media. Use of TCP congestion control ensures that the protocol competes fairly with all other TCP data and that it optimally shares the available bandwidth. It also avoids the latency problems commonly associated with TCP. In addition, we describe a real application that uses this approach to stream MPEG video on the Internet. We present several experiments, performed in both a controlled environment and the wide area Internet, that were used to evaluate the effectiveness and fairness of the scheme. The results show that the proposed solution achieves superior video quality while at the same time providing fairness by sharing bandwidth equally with other non-real-time connections."
pub.1094875738,Cross-Layer Transmission Scheme for Wireless H.264 Using Distortion Measure and MAC-Level Error-Control,"Video transmission over wireless channels has been a challenging task since it is difficult to adapt the video stream requiring relatively high bandwidth and low latency to bandlimited and frequent bit/packet error-prone physical channels. There have been a lot of research activities to improve the end-video quality in wireless video transmission over the error-prone network environments. In this paper, we propose a distortion measure of encoded H.264 slice and a cross-layer UEP scheme using the proposed distortion measure. In addition, we propose a MAC-level ARQ scheme combined with FEC to improve that of the IEEE 802.11 legacy WLAN. The proposed MAC-level ARQ scheme is utilized in the cross-layer UEP scheme to improve transmission efficiency. The performance of the proposed cross-layer UEP scheme is demonstrated in comparison with a generic EEP scheme."
pub.1095482948,Statistical Multiplexing in Fronthaul-Constrained Massive MIMO,"Despite the promising benefits of the cloud-radio access network (C-RAN), the fronthaul (FH) imposes stringent requirements in terms of data rate, latency, jitter and synchronisation. In the classical C-RAN, the FH capacity scales linearly with the number of the transmitting antennas, which has posed severe demands on the FH capacity, especially due to emerging 5G technologies such as massive MIMO. However, this can be relaxed by performing precoding at the remote radio units (RRUs) instead of centrally, leading to FH traffic which depends on the number of currently served users. This paper adapts queueing theory and spatial traffic models to derive statistical multiplexing gain enabled by varying number of user streams. Through this, we showed that the required FH capacity can be reduced dramatically, depending on traffic demand and its statistical properties."
pub.1165259626,PArtNNer: Platform-Agnostic Adaptive Edge-Cloud DNN Partitioning for Minimizing End-to-End Latency," The last decade has seen the emergence of Deep Neural Networks (DNNs) as the de facto algorithm for various computer vision applications. In intelligent edge devices, sensor data streams acquired by the device are processed by a DNN application running on either the edge device itself or in the cloud. However, “edge-only” and “cloud-only” execution of State-of-the-Art DNNs may not meet an application’s latency requirements due to the limited compute, memory, and energy resources in edge devices, dynamically varying bandwidth of edge-cloud connectivity networks, and temporal variations in the computational load of cloud servers. This work investigates distributed (partitioned) inference across edge devices (mobile/end device) and cloud servers to minimize end-to-end DNN inference latency. We study the impact of temporally varying operating conditions and the underlying compute and communication architecture on the decision of whether to run the inference solely on the edge, entirely in the cloud, or by partitioning the DNN model execution among the two. Leveraging the insights gained from this study and the wide variation in the capabilities of various edge platforms that run DNN inference, we propose PArtNNer , a platform-agnostic adaptive DNN partitioning algorithm that finds the optimal partitioning point in DNNs to minimize inference latency. PArtNNer can adapt to dynamic variations in communication bandwidth and cloud server load without requiring pre-characterization of underlying platforms. Experimental results for six image classification and object detection DNNs on a set of five commercial off-the-shelf compute platforms and three communication standards indicate that PArtNNer results in 10.2× and 3.2× (on average) and up to 21.1× and 6.7× improvements in end-to-end inference latency compared to execution of the DNN entirely on the edge device or entirely on a cloud server, respectively. Compared to pre-characterization-based partitioning approaches, PArtNNer converges to the optimal partitioning point 17.6× faster. "
pub.1173741553,On Elastic Language Models," Large-scale pretrained language models have achieved compelling performance in a wide range of language understanding and information retrieval tasks. While their large scales ensure capacity, they also hinder deployment. Knowledge distillation offers an opportunity to compress a large language model to a small one, in order to reach a reasonable latency-performance tradeoff. However, for scenarios where the number of requests (e.g., queries submitted to a search engine) is highly variant, the static tradeoff attained by the compressed language model might not always fit. Once a model is assigned with a static tradeoff, it could be inadequate in that the latency is too high when the number of requests is large, or the performance is too low when the number of requests is small. To this end, we propose an elastic language model ( ElasticLM ) that elastically adjusts the tradeoff according to the request stream. The basic idea is to introduce a compute elasticity to the compressed language model, so that the tradeoff could vary on-the-fly along a scalable and controllable compute. Specifically, we impose an elastic structure to equip ElasticLM with compute elasticity and design an elastic optimization method to learn ElasticLM under compute elasticity. To serve ElasticLM , we apply an elastic schedule. Considering the specificity of information retrieval, we adapt ElasticLM to dense retrieval and reranking, and present an ElasticDenser and an ElasticRanker, respectively. Offline evaluation is conducted on a language understanding benchmark GLUE, and several information retrieval tasks including Natural Question, Trivia QA and MS MARCO. The results show that ElasticLM along with ElasticDenser and ElasticRanker can perform correctly and competitively compared with an array of static baselines. Furthermore, an online simulation with concurrency is also carried out. The results demonstrate that ElasticLM can provide elastic tradeoffs with respect to varying request stream. "
pub.1012564940,Environmental Change and Human Adaptational Failure at the End of the Early Bronze Age in the Southern Levant,"Failure to adapt to changing climatic conditions can be as much a social problem as a technological one. This is illustrated by the collapse of Early Bronze Age society in the southern Levant. The agricultural system which supported these socially stratified communities was adapted to floodwater farming of cereals and the production of cash crops including olives and grapes. The climatic change at the end of the third millennium BC resulted in a decrease in precipitation which led to lowered stream base levels, and the hydraulic regime changed from an aggrading system to one of stream incision. Floodwater farming was no longer possible, yields became lower and less predictable, leading to the collapse of the agricultural sector. Technological change to a system of canal irrigation from springs would have maintained higher yields, and such a system was indeed used by Middle Bronze Age inhabitants in the same marginal environment. Although canal irrigation was known, it was not implemented as an adaptive measure by the Early Bronze Age society. Some of the reasons for this failure to adapt might include slow response time in the perception of catastrophe, the ability of the elite managers to profit from short-term environmental stress, and the direction of energy toward increased religious activity rather than technological change."
pub.1165938611,On Elastic Language Models,"Large-scale pretrained language models have achieved compelling performance
in a wide range of language understanding and information retrieval tasks.
Knowledge distillation offers an opportunity to compress a large language model
to a small one, in order to reach a reasonable latency-performance tradeoff.
However, for scenarios where the number of requests (e.g., queries submitted to
a search engine) is highly variant, the static tradeoff attained by the
compressed language model might not always fit. Once a model is assigned with a
static tradeoff, it could be inadequate in that the latency is too high when
the number of requests is large or the performance is too low when the number
of requests is small. To this end, we propose an elastic language model
(ElasticLM) that elastically adjusts the tradeoff according to the request
stream. The basic idea is to introduce a compute elasticity to the compressed
language model, so that the tradeoff could vary on-the-fly along scalable and
controllable compute. Specifically, we impose an elastic structure to enable
ElasticLM with compute elasticity and design an elastic optimization to learn
ElasticLM under compute elasticity. To serve ElasticLM, we apply an elastic
schedule. Considering the specificity of information retrieval, we adapt
ElasticLM to dense retrieval and reranking and present ElasticDenser and
ElasticRanker respectively. Offline evaluation is conducted on a language
understanding benchmark GLUE; and several information retrieval tasks including
Natural Question, Trivia QA, and MS MARCO. The results show that ElasticLM
along with ElasticDenser and ElasticRanker can perform correctly and
competitively compared with an array of static baselines. Furthermore, online
simulation with concurrency is also carried out. The results demonstrate that
ElasticLM can provide elastic tradeoffs with respect to varying request stream."
pub.1149159376,ElasticEdge: An Intelligent Elastic Edge Framework for Live Video Analytics,"Cloud computing and edge computing models are popularly applied in emerging applications, such as smart homes, smart parks, and connected autonomous vehicles for large-scale live video analytics. Cloud computing-based models transfer all data to the cloud for video analytics, which burdens network bandwidth and increases the data transmission overhead. Edge computing mode enables video data to be processed at the edge node, thereby reducing the bandwidth overhead. Existing edge computing-based models optimize the performance, but they still have defects in three perspectives: 1) enabling end users to control video content in a real-time format; 2) efficiently locating and transferring the user region of interest (ROI) video data in the video stream; and 3) adapting to various network conditions. To tackle these challenges, we proposed an intelligent elastic edge framework for live video analytics, known as ElasticEdge. ElasticEdge enables the interaction between the end user and the edge node. Elasticity is reflected in two perspectives: 1) the dynamic changes of user requirements and 2) the dynamic changes in network conditions. In addition, ElasticEdge transmits the video stream to the end users based on the tradeoff between the amount of video data and users’ ROI to meet various network conditions. To validate ElasticEdge, we conducted experiments to study its performance in comparison to RTFace. The experimental results show that ElasticEdge has a significant edge over RTFace in terms of data transmission. Using 1/16 reserved images, ElasticEdge saves 75% bandwidth and reduces latency by approximately 10% compared with RTFace. We also find that ElasticEdge adapts to various network conditions when streaming videos, i.e., it can reliably obtain essential information with low latency even when the network condition is poor."
pub.1144440700,Towards intelligent P2P IPTV overlay management through classification of peers,"IPTV has emerged as one of the popular Internet applications attracting immense interest of academia and industry. Among others, Peer-to-Peer (P2P) approach enables IPTV service with ease of deployment and low cost. P2P systems involve end-hosts, called peers, to share their resources for dissemination of stream. In these systems, user activities, such as join and quit operations, translate as activities of peers. Due to this reason, these systems become highly dynamic as the behavior of users has a significant impact on the stream delivery performance of the whole system. Earlier P2P approaches deal with user behavior indirectly through enabling resilience in the system, which leads to other issues such as increased latency. Latter approaches attempt to address user behavior directly through proposing user behavior models. Such models focus to learn and predict user behavior in order to adapt the overlay network accordingly. Towards this, one important aspect is the classification of users. However, machine-learning classifiers have not been extensively evaluated for their accuracy over the classification problem. In this paper, we extensively evaluate numerous classifiers for their accuracy over different parameters. These results may be used to design intelligent P2P overlays for IPTV services providing better performance in terms of efficient stream delivery to users. Our results show that the Decision tree classifier performs better than other classifiers for this particular problem."
pub.1095203313,Performance issues in CD-ROM-based storage systems for multimedia,"CD-ROMs have proliferated as a distribution media for desktop machines for a large variety of multimedia applications. We look at issues related to the single-user desktop environment. Since these multimedia applications are highly interactive in nature, we take a pragmatic approach, and have made a detailed study of the multimedia application behavior in terms of the I/O request patterns generated to the CD-ROM subsystem by tracing these patterns. We discuss prefetch buffer design and seek time characteristics in the context of the analysis of these traces. We show that it is best to place multimedia streams near the center of the CD-ROM. We present the parameters of an adaptive main-memory hosted cache that receives caching hints from the application to reduce the latency when the user moves from one node of the hypergraph to another. We show that short-throw seeks of distance 100 are the most important. We look at the use of CD-ROMs in a video on demand server (VoD) and discuss the problem of scheduling multiple request streams and buffer management in this scenario. We adapt C-SCAN (Circular SCAN) algorithm to suit the CD-ROM drive characteristics and prove that it is optimal in terms of buffer size management. We provide computationally inexpensive relations by which this algorithm can be implemented. We then propose a 'constant-full-load' admission control algorithm which admits new request streams from a pool of dummy requests without disrupting the continuity of playback of the previous request streams. The algorithm also supports operations such as fast forward and replay."
pub.1095087499,On receiver-driven congestion control for multicast streaming delivery,"In streaming services nowadays, distortion of received data is mainly caused by congestion in traversing routers within the best-effort Internet. Thus, mechanisms to control the rate of transmitted data are crucial to improve user's perception and acceptance of these evolving services. Layered multicast is considered as a promising technique to easily adapt to locally available bandwidth by striping a stream into different layers of information, which in combination result in the highest quality of reception under the currently available network resources. For that, each layer is transmitted within a separate multicast group, and receivers join or leave layers appropriately for bandwidth adaptation, i.e., implementing the adaptation decision in each receiver. Several approaches have been proposed to realize this adaptation decision. This paper presents an overview of these proposals and also briefly outlines an approach to improve the response time of the adaptation process by extending the Internet Group Management Protocol appropriately. This extension promises an improved congestion control response time and a simplification of the receiver-driven adaptation decisions."
pub.1169921296,Enhancing Networked Embedded Systems Using Deep Learning Techniques,"Improving performance and flexibility is crucial in the field of networked embedded systems. Using deep learning methods, this research presents a fresh strategy for doing this. Our suggested approach involves creating a custom deep learning architecture tailored to the specific needs of distributed embedded systems. This strategy makes use of Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Long Short-Term Memory Networks (LSTMs) to evaluate real-time data streams, expedite decision-making, and intelligently adapt to ever-changing environments. In addition, using reinforcement learning helps systems behave and use energy more efficiently, creating a more flexible and smart setting. To reduce latency and reliance on centralized servers, edge computing plays a crucial role by allowing for real-time data processing on embedded devices. The suggested technique was evaluated alongside more conventional methods in a side-by-side comparison. Imaginary numbers were utilized for demonstration purposes. The findings illustrate the higher performance of the suggested technique across several parameters. When compared to baseline deep learning methods like Convolutional Neural Networks, Recurrent Neural Networks, Long Short-Term Memory Networks, Generative Adversarial Networks, Federated Learning, Transfer Learning, and Time Series Analysis with Deep Learning, the proposed method shows marked improvements in accuracy, latency, energy efficiency, robustness, security, scalability, and resource utilization. Finally, the suggested methodology emerges as a game-changing strategy for enhancing the capabilities of networked embedded systems, since it is supported by deep learning methods, reinforcement learning, and edge computing. Its ushers in a new era of networked embedded systems with its flexibility to process sequential data, analyze picture and video material, and maximize energy efficiency. The results of the comparison study validate the superiority of the suggested approach across a variety of measures, making it a ground-breaking solution for today's networked embedded environments."
pub.1121899125,Software-defined Flow Reservation: Configuring IEEE 802.1Q Time-Sensitive Networks by the Use of Software-Defined Networking,"Communication systems are core elements of future cyber-physical systems (Industrial Internet, Industry 4.0, Internet of Things) which form the basis for applications such as smart production, smart grid, smart city, and the like. These applications will comprise data streams with very different communication requirements regarding data rate, latency, jitter, and reliability that may also vary over time. This calls for communication solutions that on one hand can adapt flexibly and on-demand to these needs and on the other hand can seamlessly support data streams with different quality-of-service requirements. Within this paper we present Software-defined Flow Reservation (SDFR), an approach that combines Software-defined Networking (SDN) technologies with Time-Sensitive Networking. Special focus is on the configuration of the time-sensitive network. SDFR implements the IEEE 802.1Qcc standard in the logically centralized SDN controller and, thus, allows for a flexible software-driven configuration of the underlying network. This flexibility enables operators to support multiple configuration interfaces simultaneously and to easily modify traffic classes, scheduling approaches, and network behavior. SDFR allows to re-use existing solutions for well-understood SDN use cases while simultaneously supporting novel TSN-specific solutions."
pub.1021229050,A Dynamic Time Warping Approach to Real-Time Activity Recognition for Food Preparation,"We present a dynamic time warping based activity recognition system for the analysis of low-level food preparation activities. Accelerometers embedded into kitchen utensils provide continuous sensor data streams while people are using them for cooking. The recognition framework analyzes frames of contiguous sensor readings in real-time with low latency. It thereby adapts to the idiosyncrasies of utensil use by automatically maintaining a template database. We demonstrate the effectiveness of the classification approach by a number of real-world practical experiments on a publically available dataset. The adaptive system shows superior performance compared to a static recognizer. Furthermore, we demonstrate the generalization capabilities of the system by gradually reducing the amount of training samples. The system achieves excellent classification results even if only a small number of training samples is available, which is especially relevant for real-world scenarios."
pub.1147392156,Dynamic Bandwidth Slicing for Time-Critical IoT Data Streams in the Edge-Cloud Continuum,"Edge computing has gained momentum in recent years, as complementary to cloud computing, for supporting applications (e.g., industrial control systems) that require time-critical communication guarantees. While edge computing can provide immediate analysis of streaming data from Internet of Things devices, those devices lack computing capabilities to guarantee reasonable performance for time-critical applications. To alleviate this critical problem, the prevalent trend is to offload these data analytic tasks from the edge devices to the cloud. However, existing offloading approaches are static in nature as they are unable to adapt varying workload and network conditions. To handle these issues, we present a novel distributed and quality of services based multilevel queue traffic scheduling system that can undertake semiautomatic bandwidth slicing to process time-critical incoming traffic in the edge-cloud environments. Our developed system shows a great enhancement in latency and throughput as well as reduction in energy consumption for edge-cloud environments."
pub.1158500360,Machine Learning for Intrusion Detection: Stream Classification Guided by Clustering for Sustainable Security in IoT,"The Internet of Things (IoT) has brought about unprecedented connectivity and convenience in our daily lives, but with this newfound interconnectedness comes the threat of cyber-attacks. With ever-increasing IoT devices being connected to the internet, securing IoT devices is becoming increasingly urgent. Machine learning (ML) is among the most popular techniques used by intrusion detection systems (IDS) to enhance their detection performance when securing IoT. However, a key obstacle of ML-based IDS for IoT is learning from nonstationary streaming data, also known as concept drift. One of the most challenging learning scenarios under concept drift is extreme verification latency (EVL), which occurs when only unlabeled nonstationary streaming data is available after a small set of initial labeled data. Stream Classification Algorithm Guided by Clustering (SCARGC) is an algorithm that can effectively deal with the nonstationary data streams in EVL scenarios. Applying an EVL implementation provides the capability of adapting to nonstationary environments within the IoT domain. The SCARGC model, as an integrated IoT intrusion detection system, allows for sustainable security as new threats are identified in this non-stationary environment. Hence, in this project, we develop an innovative IoT intrusion detection approach by natively integrating SCARGC and intrusion detection to address the EVL challenges to provide sustainable security as the model adapts to nonstationary environments. We evaluated the proposed approach on real-world IoT cybersecurity datasets. The results demonstrate the feasibility of the proposed approach, which can lead to the development of sophisticated intrusion detection systems for IoT."
pub.1123270084,SENSE: Scalable Data Acquisition from Distributed Sensors with Guaranteed Time Coherence,"Data analysis in the Internet of Things (IoT) requires us to combine event
streams from a huge amount of sensors. This combination (join) of events is
usually based on the time stamps associated with the events. We address two
challenges in environments which acquire and join events in the IoT: First, due
to the growing number of sensors, we are facing the performance limits of
central joins with respect to throughput, latency, and network utilization.
Second, in the IoT, diverse sensor nodes are operated by different
organizations and use different time synchronization techniques. Thus, events
with the same timestamps are not necessarily recorded at the exact same time
and joined data tuples have an unknown time incoherence. This can cause
undetected failures, such as false correlations and wrong predictions. We
present SENSE, a system for scalable data acquisition from distributed sensors.
SENSE introduces time coherence measures as a fundamental data characteristic
in addition to common time synchronization techniques. The time coherence of a
data tuple is the time span in which all values contained in the tuple have
been read from sensors. We explore concepts and algorithms to quantify and
optimize time coherence and show that SENSE scales to thousands of sensors,
operates efficiently under latency and coherence constraints, and adapts to
changing network conditions."
pub.1163411972,Digital twin‐based framework for wireless multimodal interactions over long distance,"Summary Wireless multimodal interactions over long distance (WMILD) would give rise to numerous thrilling applications, such as remote touching and immersive teleoperations. However, long distances can induce large propagation delays, which makes it difficult to meet the ultra‐low latency requirements in haptic‐visual interactions. Considering existing works mainly focused on the wireless access part, this paper designs an end‐to‐end framework for general WMILD applications based on the digital twin (DT) technology and proposes an intelligent resource allocation and parameter compression scheme to guarantee WMILD performance under constraint network resources. In the framework, user device can acquire real‐time remote interactions by performing local interactions with nearby base station (BS), where a DT of the remote side is deployed to predict the remote haptic‐visual feedbacks. A reliable DT updating process is carefully designed to guarantee the DT accurately model its dynamic physical counterpart. To optimize the updating reliability, we formulate the resource allocation and parameter compression to be a constraint‐Markov decision problem, under the constraints on energy consumption, multimodal interactions and updating latencies. Then, a safe deep reinforcement learning algorithm is proposed to adapt resources and compression according to the dynamic DT updating workload, multimodal data‐streams and remote transmission capacities. Simulation shows the framework can achieve high updating reliability compared with baselines."
pub.1109929191,IBM z14: Processor Characterization and Power Management for High-Reliability Mainframe Systems,"The IBM z14 is the latest update in the storied history of IBM mainframes. Reliability, availability, security, and scalability are the foundation of the IBM mainframe line. System reliability and availability targets are in excess of 10 years, requiring rigorous chip characterization processes. In this paper, we discuss some of the many processes used to ensure that lifetime. An additional part of this reliability is power management (PM). The 5.2-GHz high-power design of the central processor chip requires advanced on-die PM capabilities to adapt to power intensive instruction streams. We also discuss a number of improvements to the critical path monitoring design used to manage power supply voltage droops, reducing response time and the impact on system performance. Finally, we compare a set of simulations and hardware results to validate our power fluctuation models."
pub.1141969063,How to Learn a Domain-Adaptive Event Simulator?,"The low-latency streams captured by event cameras have shown impressive potential in addressing vision tasks such as video reconstruction and optical flow estimation. However, these tasks often require massive training event streams, which are expensive to collect and largely bypassed by recently proposed event camera simulators. To align the statistics of synthetic events with that of target event cameras, existing simulators often need to be heuristically tuned with elaborative manual efforts and thus become incompetent to automatically adapt to various domains. To address this issue, this work proposes one of the first learning-based, domain-adaptive event simulator. Given a specific domain, the proposed simulator learns pixel-wise distributions of event contrast thresholds that, after stochastic sampling and paralleled rendering, can generate event representations well aligned with those from the data from realistic event cameras. To achieve such domain-specific alignment, we design a novel divide-and-conquer discrimination scheme that adaptively evaluates the synthetic-to-real consistency of event representations according to the local statistics of images and events. Trained with the data synthesized by the proposed simulator, the performances of state-of-the-art event-based video reconstruction and optical flow estimation approaches are boosted up to 22.9% and 2.8%, respectively. In addition, we show significantly improved domain adaptation capability over existing event simulators and tuning strategies, consistently on three real event datasets."
pub.1061787022,Flow Classification for Software-Defined Data Centers Using Stream Mining,"Traffic management is known to be important to effectively utilize the high bandwidth provided by datacenters. Recent works have focused on identifying elephant flows and rerouting them to improve network utilization. These approaches however require either a significant monitoring overhead or hardware/end-host modifications. In this paper, we propose FlowSeer, a fast, low-overhead elephant flow detection and scheduling system using data stream mining. Our key idea is that the features from flows first few packets allow us to train the streaming classification models that can accurately and quickly predict the rate and duration of any initiated flow. With these predicted information, FlowSeercan adapt routing polices of elephant flows to their demands and dynamic network conditions. Another nice property of FlowSeeris its capability of enabling the controller and switches to perform cooperative prediction. Most of decisions can be made by switches locally, thereby reducing both detection latency and signaling overhead. FlowSeerrequires less than 100 flow table entries at each switch to enable cooperative prediction, and hence can be implemented on off-the-shelf switches. The evaluation via both experiments in realistic virtual networks and trace-driven simulations shows that FlowSeerimproves the throughput by multiple times over Hedera, which pulls flow statistics, and performs comparably to Mahout, which needs end-host modification."
pub.1134328086,Block Data Delivery Time Control for Reliable Wireless Operation of Digital Factory Tools,"Wireless communications have become an essential component of modern factory operations. Compared with wired communications, wireless communications can flexibly adapt to physical layout changes and movement of machines and human operators. However, wireless communications are more susceptible to influences from the surrounding environment, interference from other wireless systems and changes in signal strength due to changes in the physical environment. This requires the development of smarter communication systems that can guarantee the reliability of wireless links. In particular, the reliability of wireless links for time-sensitive communications is an important issue. This paper examines a scenario with mixed traffic of video and digital assembly tool in a factory which results in congestion where they merge at a wireless link, with increase in latency due to long waits in the wireless transmission queue. A method for traffic control at a bridge before the wireless link is proposed that balances packet delivery time (PDT) for the video streams and a new measure of block delivery time (BDT) for the digital tools. Analysis and simulations are used to show that traffic shaping with adjustable transmission weights can be effective for tuning the tradeoff between PDT of video streams and BDT of tool data. An example is presented that achieves 75% reduction of PDT for video packets to 5ms while keeping the BDT of tool data below 150ms."
pub.1031972575,Dynamic Transmission Scheduling for Streaming Applications via P2P Overlay,"In current Peer-to-Peer streaming applications, a lot of research attempts to provide timely stream services to the end users. Most of them focus on how to organize the peer connections into an efficient overlay network. Due to the variation in the network, a lot of overhead is generated when the overlay structure adapts to the changes. In order to minimize the impact of network fluctuation, we proposed a scheduling algorithm which helps to distribute stream data efficiently through the fluctuating networks. It needs only partial information about overlay structure and provides the services to satisfy most number of users. The global latency experienced by peers in the system is thus minimized. To alleviate the problem of packet loss along the overlay, we introduce retransmission requests into our scheduling algorithm. Requests for much needed data are treated with higher priority. Parent peers which receive the request will re-send the missing data to minimize the loss impact. The missing data which are needed by more peers will have a larger cumulative impact through the tree overlay and these are thus scheduled and sent earlier. Simulation results showed that our prioritization and scheduling algorithm minimizes the negative impact of fluctuation and data loss in a dynamic network environment."
pub.1128471849,Molecular switch architecture drives response properties,"Abstract  Many intracellular signaling pathways are composed of molecular switches, proteins that transition between two states— on and off . Typically, signaling is initiated when an external stimulus activates its cognate receptor that in turn causes downstream switches to transition from off to on using one of the following mechanisms: activation, in which the transition rate from the off state to the on state increases; derepression, in which the transition rate from the on state to the off state decreases; and concerted, in which activation and derepression operate simultaneously. We use mathematical modeling to compare these signaling mechanisms in terms of their dose-response curves, response times, and abilities to process upstream fluctuations. Our analysis elucidates several general principles. First, activation increases the sensitivity of the pathway, whereas derepression decreases sensitivity. Second, activation generates response times that decrease with signal strength, whereas derepression causes response times to increase with signal strength. These opposing features allow the concerted mechanism to not only show dose-response alignment, but also to decouple the response time from stimulus strength. However, these potentially beneficial properties come at the expense of increased susceptibility to up-stream fluctuations. In addition to above response metrics, we also examine the effect of receptor removal on switches governed by activation and derepression. We find that if inactive (active) receptors are preferentially removed then activation (derepression) exhibits a sustained response whereas derepression (activation) adapts. In total, we show how the architecture of molecular switches govern their response properties. We also discuss the biological implications of our findings. "
pub.1175096866,Distributed Threat Intelligence at the Edge Devices: A Large Language Model-Driven Approach,"With the proliferation of edge devices, there is a significant increase in attack surface on these devices. The decen-tralized deployment of threat intelligence on edge devices, coupled with adaptive machine learning techniques such as the in-context learning feature of Large Language Models (LLMs), represents a promising paradigm for enhancing cybersecurity on resource-constrained edge devices. This approach involves the deployment of lightweight machine learning models directly onto edge devices to analyze local data streams, such as network traffic and system logs, in real-time. Additionally, distributing computational tasks to an edge server reduces latency and improves responsiveness while also enhancing privacy by processing sensitive data locally. LLM servers can enable these edge servers to autonomously adapt to evolving threats and attack patterns, continuously updating their models to improve detection accuracy and reduce false positives. Furthermore, collaborative learning mechanisms facilitate peer-to-peer secure and trustworthy knowledge sharing among edge devices, enhancing the collective intelligence of the network and enabling dynamic threat mitigation measures such as device quarantine in response to detected anomalies. The scalability and flexibility of this approach make it well-suited for diverse and evolving network environments, as edge devices only send suspicious information such as network traffic and system log changes, offering a resilient and efficient solution to combat emerging cyber threats at the network edge. Thus, our proposed framework can improve edge computing security by providing better security in cyber threat detection and mitigation by isolating the edge devices from the network."
pub.1171634639,Distributed Threat Intelligence at the Edge Devices: A Large Language Model-Driven Approach,"With the proliferation of edge devices, there is a significant increase in
attack surface on these devices. The decentralized deployment of threat
intelligence on edge devices, coupled with adaptive machine learning techniques
such as the in-context learning feature of Large Language Models (LLMs),
represents a promising paradigm for enhancing cybersecurity on
resource-constrained edge devices. This approach involves the deployment of
lightweight machine learning models directly onto edge devices to analyze local
data streams, such as network traffic and system logs, in real-time.
Additionally, distributing computational tasks to an edge server reduces
latency and improves responsiveness while also enhancing privacy by processing
sensitive data locally. LLM servers can enable these edge servers to
autonomously adapt to evolving threats and attack patterns, continuously
updating their models to improve detection accuracy and reduce false positives.
Furthermore, collaborative learning mechanisms facilitate peer-to-peer secure
and trustworthy knowledge sharing among edge devices, enhancing the collective
intelligence of the network and enabling dynamic threat mitigation measures
such as device quarantine in response to detected anomalies. The scalability
and flexibility of this approach make it well-suited for diverse and evolving
network environments, as edge devices only send suspicious information such as
network traffic and system log changes, offering a resilient and efficient
solution to combat emerging cyber threats at the network edge. Thus, our
proposed framework can improve edge computing security by providing better
security in cyber threat detection and mitigation by isolating the edge devices
from the network."
pub.1128045786,Timely Reporting of Heavy Hitters using External Memory,"Given an input stream of size N, a φ-heavy hitter is an item that occurs at least φ N times in S. The problem of finding heavy-hitters is extensively studied in the database literature. We study a real-time heavy-hitters variant in which an element must be reported shortly after we see its T = φ N-th occurrence (and hence becomes a heavy hitter). We call this the Timely Event Detection (TED) Problem. The TED problem models the needs of many real-world monitoring systems, which demand accurate (i.e., no false negatives) and timely reporting of all events from large, high-speed streams, and with a low reporting threshold (high sensitivity). Like the classic heavy-hitters problem, solving the TED problem without false-positives requires large space (Ω(N) words). Thus in-RAM heavy-hitters algorithms typically sacrifice accuracy (i.e., allow false positives), sensitivity, or timeliness (i.e., use multiple passes). We show how to adapt heavy-hitters algorithms to external memory to solve the TED problem on large high-speed streams while guaranteeing accuracy, sensitivity, and timeliness. Our data structures are limited only by I/O-bandwidth (not latency) and support a tunable trade-off between reporting delay and I/O overhead. With a small bounded reporting delay, our algorithms incur only a logarithmic I/O overhead. We implement and validate our data structures empirically using the Firehose streaming benchmark. Multi-threaded versions of our structures can scale to process 11M observations per second before becoming CPU bound. In comparison, a naive adaptation of the standard heavy-hitters algorithm to external memory would be limited by the storage device's random I/O throughput, i.e., ~100K observations per second."
pub.1143224627,Timely Reporting of Heavy Hitters Using External Memory,"
                    Given an input stream
                    S
                    of size
                    N
                    , a
                    
                      ɸ-heavy hitter
                    
                    is an item that occurs at least
                    ɸN
                    times in
                    S
                    . The problem of finding heavy-hitters is extensively studied in the database literature.
                  
                  
                    We study a real-time heavy-hitters variant in which an element must be reported shortly after we see its T = ɸ N-th occurrence (and hence it becomes a heavy hitter). We call this the Timely Event Detection (
                    TED
                    ) Problem. The
                    TED
                    problem models the needs of many real-world monitoring systems, which demand accurate (i.e., no false negatives) and timely reporting of all events from large, high-speed streams with a low reporting threshold (high sensitivity).
                  
                  
                    Like the classic heavy-hitters problem, solving the
                    TED
                    problem without false-positives requires large space (Ω (N) words). Thus in-RAM heavy-hitters algorithms typically sacrifice accuracy (i.e., allow false positives), sensitivity, or timeliness (i.e., use multiple passes).
                  
                  
                    We show how to adapt heavy-hitters algorithms to external memory to solve the
                    TED
                    problem on large high-speed streams while guaranteeing accuracy, sensitivity, and timeliness. Our data structures are limited only by I/O-bandwidth (not latency) and support a tunable tradeoff between reporting delay and I/O overhead. With a small bounded reporting delay, our algorithms incur only a logarithmic I/O overhead.
                  
                  We implement and validate our data structures empirically using the Firehose streaming benchmark. Multi-threaded versions of our structures can scale to process 11M observations per second before becoming CPU bound. In comparison, a naive adaptation of the standard heavy-hitters algorithm to external memory would be limited by the storage device’s random I/O throughput, i.e., ≈100K observations per second."
pub.1131701684,Adaptive Strategy to Improve the Quality of Communication for IoT Edge Devices,"In an IoT system, the response time of edge devices is calculated during the design time. These edge devices continuously provide data streams to ensure the smooth execution of a real-time IoT system. However, edge devices are prone to errors, and very often suffer issues when trying to maintain a certain level of communication quality in the presence of external interference. Any loss of communication at the edge device level can lead to a failure of the entire system or to misleading information being provided. Due to there being a large number of heterogeneous devices within the IoT system, it is not a trivial matter to monitor all of these devices from a centralised location or to explore system logs to determine any loss of communication. Hence, in order to maintain the highest level of of communication quality in as close as possible to the best theoretical response time, there is a need for a lightweight intelligent layer on the edge devices which could adapt depending on changes in the context. In this work, we propose an adaptive algorithm, which can predict the quality of communication of WiFi and BLE with an accuracy of 94.14% and 92.25% respectively. The adaptive layer can recommend the next best alternative available wireless communication protocol in case the existing wireless protocol’s quality degrades. Edge devices within IoT systems can be equipped with our proposed adaptive layer, which can help them to adapt according to dynamic context whilst ensuring the highest level of communication quality, thus, improving the overall resilience of the entire IoT system."
pub.1093773910,InVerse: Designing an interactive universe architecture for scalability and extensibility,"Faster networks, faster processors, and standardized protocols have enabled the emergence of interactive applications running over commercial networks such as the Internet. In such applications, multiple users interact with one another by exchanging real-time information such as user position and orientation in a virtual world, live and recorded audio, video, and text. These applications include interactive shopping, team training, virtual meeting rooms, and multi-player games. However, to date, these interactive systems have supported a limited number of information types, offered limited scalability, and have failed to account for a heterogeneous network and processor environment. In this paper, we describe the design and implementation of InVerse, an infrastructure that supports real-time interactive applications on the Internet. InVerse provides a common backplane for disseminating and managing multiple real-time data streams. Within this general-purpose structure, the InVerse system maximizes scalability by implementing a hybrid communications architecture that adapts itself to available network bandwidth, observed network latency, installed network security measures, and available services such as multicast."
pub.1149763800,SQP: Congestion Control for Low-Latency Interactive Video Streaming,"This paper presents the design and evaluation of SQP, a congestion control
algorithm (CCA) for interactive video streaming applications that need to
stream high-bitrate compressed video with very low end-to-end frame delay (eg.
AR streaming, cloud gaming). SQP uses frame-coupled, paced packet trains to
sample the network bandwidth, and uses an adaptive one-way delay measurement to
recover from queuing, for low, bounded queuing delay. SQP rapidly adapts to
changes in the link bandwidth, ensuring high utilization and low frame delay,
and also achieves competitive bandwidth shares when competing with
queue-building flows within an acceptable delay envelope. SQP has good fairness
properties, and works well on links with shallow buffers.
  In real-world A/B testing of SQP against Copa in Google's AR streaming
platform, SQP achieves 27% and 15% more sessions with high bitrate and low
frame delay for LTE and Wi-Fi, respectively. When competing with queue-building
traffic like Cubic and BBR, SQP achieves 2-3X higher bandwidth compared to
GoogCC (WebRTC), Sprout, and PCC-Vivace, and comparable performance to Copa
(with mode switching)."
pub.1084764811,A Multiplexing Scheme for Multimodal Teleoperation,"This article proposes an application-layer multiplexing scheme for teleoperation systems with multimodal feedback (video, audio, and haptics). The available transmission resources are carefully allocated to avoid delay-jitter for the haptic signal potentially caused by the size and arrival time of the video and audio data. The multiplexing scheme gives high priority to the haptic signal and applies a preemptive-resume scheduling strategy to stream the audio and video data. The proposed approach estimates the available transmission rate in real time and adapts the video bitrate, data throughput, and force buffer size accordingly. Furthermore, the proposed scheme detects sudden transmission rate drops and applies congestion control to avoid abrupt delay increases and converge promptly to the altered transmission rate. The performance of the proposed scheme is measured objectively in terms of end-to-end signal latencies, packet rates, and peak signal-to-noise ratio (PSNR) for visual quality. Moreover, peak-delay and convergence time measurements are carried out to investigate the performance of the congestion control mode of the system."
pub.1112855420,PASCAL: An architecture for proactive auto-scaling of distributed services,"One of the main characteristics that today makes cloud services so popular is their ability to be elastic, i.e., they can adapt their provisioning to variable workloads, thus increasing resource utilization and reducing operating costs. At the core of any elastic service lies an automatic scaling mechanism that drives provisioning on the basis of a given strategy. In this paper we propose PASCAL , an architecture for Proactive Auto-SCALing of generic distributed services. PASCAL combines a proactive approach, to forecast incoming workloads, with a profiling system, to estimate required provision. Scale-in/out operations are decided according to an application-specific strategy, which aims at provisioning the minimum number of resources needed to sustain the foreseen workload. The main novelties introduced with PASCAL architecture are: (i) a strategy to proactively auto-scale a distributed stream processing system (namely, Apache Storm) with the aim of load balancing operators through an accurate system performance estimation model, and (ii) a strategy to proactively auto-scale a distributed datastore (namely, Apache Cassandra), focused on how to choose when executing scaling actions on the basis of the time needed for the activation/deactivation of storage nodes so as to have the configuration ready when needed. We provide a prototype implementation of PASCAL for both use cases and, through an experimental evaluation conducted on a private cloud, we validate our approach and demonstrate the effectiveness of the proposed strategies in terms of saved resources and response time."
pub.1045019738,Ultra-low duty cycle MAC with scheduled channel polling,"Energy is a critical resource in sensor networks. MAC protocols such as S-MAC and T-MAC coordinate sleep schedules to reduce energy consumption. Recently, lowpower listening (LPL) approaches such as WiseMAC and B-MAC exploit very brief polling of channel activity combined with long preambles before each transmission, saving energy particularly during low network utilization. Synchronization cost, either explicitly in scheduling, or implicitly in long preambles, limits all these protocols to duty cycles of 1-2%. We demonstrate that ultra-low duty cycles of 0.1% and below are possible with a new MAC protocol called scheduled channel polling (SCP). This work prompts three new contributions: First, we establish optimal configurations for both LPL and SCP under fixed conditions, developing a lower bound of energy consumption. Under these conditions, SCP can extend lifetime of a network by a factor of 3-6 times over LPL. Second, SCP is designed to adapt well to variable traffic. LPL is optimized for known, periodic traffic, and long preambles become very costly when traffic varies. In one experiment, SCP reduces energy consumption by a factor of 10 under bursty traffic. We also show how SCP adapts to heavy traffic and streams data in multi-hop networks, reducing latency by 85% and energy by 95% at 9 hops. Finally, we show that SCP can operate effectively on recent hardware such as 802.15.4 radios. In fact, power consumption of SCP decreases with faster radios, but that of LPL increases."
pub.1028733956,A self-managing data cache for edge-of-network web applications,"Database caching at proxy servers enables dynamic content to be generated at the edge of the network, thereby improving the scalability and response time of web applications. The scale of deployment of edge servers coupled with the rising costs of their administration demand that such caching middleware be adaptive and self-managing. To achieve this, a cache must be dynamically populated and pruned based on the application query stream and access pattern. In this paper, we describe such a cache which maintains a large number of materialized views of previous query results. Cached ""views"" share physical storage to avoid redundancy, and are usually added and evicted dynamically to adapt to the current workload and to available resources. These two properties of large scale (large number of cached views) and overlapping storage introduce several challenges to query matching and storage management which are not addressed by traditional approaches. In this paper, we describe an edge data cache architecture with a flexible query matching algorithm and a novel storage management policy which work well in such an environment. We perform an evaluation of a prototype of such an architecture using the TPC-W benchmark and find that it reduces query response times by up to 75%, while reducing network and server load."
pub.1151140727,Analysis of the capabilities of MPLS technology for managing traffic in communication networks,"To support a growing number of users and multiple classes of applications with different performance requirements and characteristics, service providers have been forced to adapt to new technologies. To improve traffic management and Internet service quality, the Internet Engineering Task Force (IETF) proposed MPLS technology to support several classes of latency-critical applications. Traditional IP networks use a hop-by-pop principle for transmitting traffic. This leads to aggregation of heterogeneous traffic on links in different parts of the network, which causes considerable possible growth of congestion and leaves the network with both unbalanced use of resources and link failure in congested parts. This raises the need for traffic engineering to ensure bandwidth guarantees and efficient use of network resources. To overcome these problems, the IETF has proposed a new data transmission mechanism, which is MPLS (Multi protocol label switching), in accordance with the current requirements. The application of MPLS (Multi protocol label switching) technology in modern communication networks is defined by the author as a research task. The report discusses the prospects of MPLS as a universal technology that supports several protocols. The features of construction of virtual private networks (VPN) on MPLS are considered, and how traffic engineering in MPLS takes into account the use of resources, which makes the development of routes based on separate streams or different streams between the same endpoints more effective."
pub.1006780268,Intelligent information dissemination services in hybrid satellite‐wireless networks,"The need for rapid deployment and user mobility suggest the use of a hybrid satellite‐wireless network infrastructure for important situation awareness and emergency response applications. An Intelligent Information Dissemination Service (IIDS) has been developed to support the dissemination and maintenance of extended situation awareness throughout such a network information infrastructure in a seamless manner. One of the goals of IIDS is to transparently handle the mismatches in characteristics of satellite and terrestrial wireless networks, allow effective utilization of available bandwidth, and support timely delivery of highly relevant information. IIDS achieves the above by implementing user profile aggregation that incrementally aggregates users into communities sharing common interests to enable multicast‐based information dissemination. Based on the user grouping, semantic profile matching customizes information streams based on matching user group interest profiles. By taking into account expected changes in user profiles, profile‐oriented data dissemination achieves predictive push and caching that anticipates future user needs and minimizes latency of data request by making data available before they are explicitly requested. Finally, bandwidth‐aware filtering adapts information streams to resource bandwidth availability to gracefully hide the bandwidth mismatch between the satellite and wireless links in the hybrid network infrastructure. The IIDS software has been deployed on the Digital Wireless Battlefield Network (DWBN) that integrates commercial off‐the‐shelf satellite and wireless products into a heterogeneous satellite/wireless hybrid network for supporting wireless mobile multimedia services."
pub.1140188258,Improving service-level agreements for critical systems using big data monitoring techniques,"The proliferation of big data in virtually every branch of society and industry comes with the need to adapt and develop monitoring and alerting systems in such a way that the system can cope with any kind of data stream, whilst also ensuring rapid response times. This paper presents a framework based on modern open-source technologies that can be used to improve the quality and reliability of a connected system (such as an industrial control system), through effective monitoring and alerting. Service level agreements are crucial in our modern society, where failures need to be detected quickly and effectively, especially when one is providing a service and every moment of downtime means a large quantity of lost money and potential customers, thus monitoring is essential. Benefits in terms of responsiveness and lower downtime are also discussed, with an emphasis on a prototype implementation for a major non-profit organization."
pub.1095125583,NADA: A Unified Congestion Control Scheme for Low-Latency Interactive Video,"Low-latency, interactive media applications (e.g., video conferencing) present a unique set of challenges for congestion control. Unlike TCP, the transport mechanism for interactive media needs to adapt fast to abrupt changes in available bandwidth, accommodate sluggish responses and output rate fluctuations of a live video encoder, and avoid high queuing delay over the network. An ideal scheme should also make effective use of all types of congestion signals from the network, including packet losses, queuing delay, and explicit congestion notification (ECN) markings. This paper presents a unified approach for congestion control of interactive video: network-assisted dynamic adaptation (NADA). In NADA, the sender regulates its sending rate based on a composite congestion signal calculated and reported by the receiver, which combines both implicit (e.g., loss and delay) and explicit (e.g., ECN marking) congestion indications from the network. Via a consistent set of sender adaptation rules, the scheme can reap the full benefits of proactive, explicit congestion notifications supported by advanced queue management schemes. It remains equally responsive in the absence of such notifications. Extensive simulation studies show that NADA interact well with a wide variety of queue management schemes: conventional drop-tail, random early detection (RED), recently proposed CoDel (controlled delay) and PIE (Proportional Integral controller Enhanced), as well as a token-bucket-based random marking scheme based on Pre-Congestion Notification (PCN). Furthermore, NADA reacts fast to changes over the network, allows for weighted bandwidth sharing among multiple competing video streams, and sustains a substantial share of bottleneck bandwidth when coexisting with TCP."
pub.1157480052,EdgeBOL: A Bayesian Learning Approach for the Joint Orchestration of vRANs and Mobile Edge AI,"Future mobile networks need to support intelligent services which collect and process data streams at the network edge, so as to offer real-time and accurate inferences to users. However, the widespread deployment of these services is hindered by the unprecedented energy cost they induce to the network, and by the difficulties in optimizing their end-to-end operation. To address these challenges, we propose a Bayesian learning framework for jointly configuring the service and the Radio Access Network (RAN), aiming to minimize the total energy consumption while respecting accuracy and latency service requirements. Using a fully-fledged prototype with a software-defined base station (vBS) and a GPU-enabled edge server, we profile a typical video analytics service and identify new performance trade-offs and optimization opportunities. Accordingly, we tailor the proposed learning framework to account for the (possibly varying) network conditions, user needs, and service metrics, and apply it to a range of experiments with real traces. Our findings suggest that this approach effectively adapts to different hardware platforms and service requirements, and outperforms state-of-the-art benchmarks based on neural networks."
pub.1136542270,Adaptive Rate Control for Live streaming using SRT protocol,"Media delivery represents one of the main challenges for future networks which aim to converge Broadcast and Broadband video traffic into a common telecommunication network architecture. Nowadays, contents streamed over Internet are delivered in two different manners depending on the application: Video on Demand and Live Streaming. For the former, HTTP-based streaming technologies, such as Dynamic Adaptive Streaming over HTTP (MPEG-DASH), are widely employed for unicast and broadcast communications. It also enables Adaptive Rate Control on the client device allowing players to select a representation and bitrate matching the capabilities of the network at any moment. For the latter, MPEGDASH does not provide low latency for Live streaming when compared to a Broadcast service. Secure Reliable Transport (SRT) is proposed by SRT Alliance to overcome such limitations of unicast and broadcast communications. Nevertheless, it misses the adaptation of the content to the available network resources. In this paper, we show an implementation of Adaptive Rate Control for SRT protocol which exploits periodical network reports in order to adapt the content encoding process. The evaluation includes a real deployment of the solution and a comparison with a legacy SRT stream."
pub.1149732014,Adaptive QoS of WebRTC for Vehicular Media Communications,"Vehicles shipping sensors for onboard systems are gaining connectivity. This enables information sharing to realize a more comprehensive understanding of the environment. However, peer communication through public cellular networks brings multiple networking hurdles to address, needing in-network systems to relay communications and connect parties that cannot connect directly. Web Real-Time Communication (WebRTC) is a good candidate for media streaming across vehicles as it enables low latency communications, while bringing standard protocols to security handshake, discovering public IPs and transverse Network Address Translation (NAT) systems. However, the end-to-end Quality of Service (QoS) adaptation in an infrastructure where transmission and reception are decoupled by a relay, needs a mechanism to adapt the video stream to the network capacity efficiently. To this end, this paper investigates a mechanism to apply changes on resolution, framerate and bitrate by exploiting the Real Time Transport Control Protocol (RTCP) metrics, such as bandwidth and round-trip time. The solution aims to ensure that the receiving onboard system gets relevant information in time. The impact on end-to-end throughput efficiency and reaction time when applying different approaches to QoS adaptation are analyzed in a real 5G testbed."
pub.1093087825,Fast and Smart Network Resource Management for Datacenters and Beyond,"Modern networked systems give rise to many challenging resource management problems, from congestion control in datacenters to bitrate adaptation for video streams to scheduling workloads on computer clusters. In this talk, I will describe some of our work on network resource management systems and algorithms across two paradigms: (1) a classical paradigm, characterized by simplified system models, low-level design goals, and fixed algorithms that achieve these goals in specified conditions; (2) a learning paradigm, characterized by unknown and uncertain system dynamics, high-level resource management goals, and algorithms that learn and adapt to their environment and workload. For the classical paradigm, I will describe some of our work on packet transport mechanisms for datacenters that enable the construction of large-scale fabrics that provide 100s of Tb/s of bandwidth, microsecond-level packet latencies, and resilience to link failures. For the learning paradigm, I will discuss our recent work on systems that learn to make resource management decisions entirely from experience using modern deep reinforcement techniques. I will demonstrate the power of this approach for problems such as adaptive bitrate selection for video streaming and scheduling jobs in computer clusters."
pub.1150487259,Adaptive QoS of WebRTC for Vehicular Media Communications,"Vehicles shipping sensors for onboard systems are gaining connectivity. This
enables information sharing to realize a more comprehensive understanding of
the environment. However, peer communication through public cellular networks
brings multiple networking hurdles to address, needing in-network systems to
relay communications and connect parties that cannot connect directly. Web
Real-Time Communication (WebRTC) is a good candidate for media streaming across
vehicles as it enables low latency communications, while bringing standard
protocols to security handshake, discovering public IPs and transverse Network
Address Translation (NAT) systems. However, the end-to-end Quality of Service
(QoS) adaptation in an infrastructure where transmission and reception are
decoupled by a relay, needs a mechanism to adapt the video stream to the
network capacity efficiently. To this end, this paper investigates a mechanism
to apply changes on resolution, framerate and bitrate by exploiting the Real
Time Transport Control Protocol (RTCP) metrics, such as bandwidth and
round-trip time. The solution aims to ensure that the receiving onboard system
gets relevant information in time. The impact on end-to-end throughput
efficiency and reaction time when applying different approaches to QoS
adaptation are analyzed in a real 5G testbed."
pub.1167171025,Deep Learning-Enabled Efficient Storage and Retrieval of Video Streams in the Cloud,"In recent years, the rapid growth of video data has presented challenges for efficient storage and retrieval in cloud-based systems. This research proposes a novel approach that utilizes machine learning techniques to address these challenges. The objective is to develop a robust framework that optimizes storage utilization, enhances retrieval efficiency, and maintains video quality with minimal latency. Machine learning algorithms are employed for tasks such as video compression, content analysis, and metadata extraction, enabling effective storage and retrieval mechanisms. The framework leverages advanced video analytics algorithms to extract relevant features, incorporating them into the storage and retrieval process for efficient indexing and organization of video content. The framework adapts to changing workloads and dynamically allocates resources for optimal performance in cloud environments. Extensive experiments and comparisons with existing methods validate the proposed framework, demonstrating significant improvements in storage utilization, retrieval performance, and scalability. Specifically, our framework achieves an average improvement of 25% in storage efficiency compared to another method. It also demonstrates faster retrieval speeds, with an improvement of approximately 10-20%, and achieves higher video quality ratings, with an improvement of approximately 10-20% compared to existing methods."
pub.1128533842,OASIS: A Framework for Enhanced Live Video Streaming over Integrated LTE Wi-Fi Networks,"Live Video Streaming has a strict low latency requirement. Current multipath techniques like Multipath RTP (MPRTP) and Multipath TCP (MPTCP) are not optimized to meet this requirement. This is especially true when they are operating over a wireless medium, whose channel conditions change very rapidly and unpredictably. These multipath techniques are unable to adapt quickly to the dynamic channel conditions. LTE WLAN Aggregation (LWA) is capable of dealing with this problem since it makes its decisions at the edge of the network. In this paper, we propose the bOunded delAy baSed steerIng with timelineSs (OASIS) framework over LWA for streaming live video with low latency. The OASIS framework comprises of a novel traffic steering algorithm, BOunded deLay based sTeering (BOLT). BOLT calculates the maximum traffic load which can be transmitted on each of the links (LTE and Wi-Fi) before the packets experience higher delay i.e., above a certain delay threshold. OASIS also comprises of a timeliness model, which prioritizes the packets belonging to Intra-coded picture (I-frames) over other packets. OASIS is implemented and tested in NS-3. We use NS-3 in emulation mode which allows us to stream real videos over the simulated topology. OASIS is compared with MPRTP, LWA, LTE, and Wi-Fi. We show that the proposed framework performs 1.4× better than MPRTP. Also, it outperforms all the other comparative techniques when tested under different scenarios. These scenarios are designed to test the different techniques under rapidly changing wireless medium. We also show that the quality of the video improves with the inclusion of the timeliness model."
pub.1172557153,EdgeSync: Faster Edge-model Updating via Adaptive Continuous Learning for Video Data Drift,"Real-time video analytics systems typically place models with fewer weights
on edge devices to reduce latency. The distribution of video content features
may change over time for various reasons (i.e. light and weather change) ,
leading to accuracy degradation of existing models, to solve this problem,
recent work proposes a framework that uses a remote server to continually train
and adapt the lightweight model at edge with the help of complex model.
However, existing analytics approaches leave two challenges untouched: firstly,
retraining task is compute-intensive, resulting in large model update delays;
secondly, new model may not fit well enough with the data distribution of the
current video stream. To address these challenges, in this paper, we present
EdgeSync, EdgeSync filters the samples by considering both timeliness and
inference results to make training samples more relevant to the current video
content as well as reduce the update delay, to improve the quality of training,
EdgeSync also designs a training management module that can efficiently adjusts
the model training time and training order on the runtime. By evaluating real
datasets with complex scenes, our method improves about 3.4% compared to
existing methods and about 10% compared to traditional means."
pub.1168664458,Octopus: In-Network Content Adaptation to Control Congestion on 5G Links,"It is challenging to meet the bandwidth and latency requirements of interactive real-time applications (e.g., virtual reality, cloud gaming, etc.) on time-varying 5G cellular links. Today's feedback-based congestion controllers try to match the sending rate at the endhost with the estimated network capacity. However, such controllers cannot precisely estimate the cellular link capacity that changes at timescales smaller than the feedback delay. We instead propose a different approach for controlling congestion on 5G links. We send real-time data streams using an imprecise controller (that errs on the side of overestimating network capacity) to ensure high throughput, and then adapt the transmitted content by dropping appropriate packets in the cellular base stations to match the actual capacity and minimize delay. We build a system called Octopus to realize this approach. Octopus provides parameterized primitives that applications at the endhost can configure differently to express different content adaptation policies. Octopus transport encodes the corresponding app-specified parameters in packet header fields, which the base-station logic can parse to execute the desired dropping behavior. Our evaluation shows how real-time applications involving standard and volumetric videos can be designed to exploit Octopus, and achieve 1.5--18× better performance than state-of-the-art schemes."
pub.1166097076,Autonomous solution for Controller Placement Problem of Software-Defined Networking using MuZero based intelligent agents,"Controller placement is a critical issue in Software-Defined Networking, which has been identified as a potential approach to achieve more flexible network control and management. Deep Reinforcement Learning holds immense promise in achieving favorable outcomes through its ability to explore solution spaces and adapt to swiftly changing data streams. Furthermore, the advancement of reinforcement learning algorithms, like chess or Go, has inspired the concept of treating controller placement problem as a Markov game. These algorithms are employed to train intelligent agents capable of autonomously resolving this problem. This paper presents an intelligent system capable of optimizing the placement of controllers in Software-Defined Networking. The Muzero Reinforcement learning algorithm is used to train a model via self-competition which, by combining a tree search with a learned model, achieves superhuman performance in complex domains. Once trained, the model is integrated with an Software-Defined Networking controller, so that it is able to find the optimal placement of the game in a real network by incorporating performance metrics including latency, load, and overhead communication into the training process of intelligent agents. In order to show that our approach is feasible and efficient in practice, experiment results are provided as a benchmark."
pub.1094184950,A Network-Layer Proxy for Bandwidth Aggregation and Reduction of IP Packet Reordering,"With today's widespread deployment of wireless technologies, it is often the case that a single communication device can select from a variety of access networks. At the same time, there is an ongoing trend towards integration of multiple network interfaces into end-hosts, such as cell phones with HSDPA, Bluetooth and WLAN. By using multiple Internet connections concurrently, network applications can benefit from aggregated bandwidth and increased fault tolerance. However, the heterogeneity of wireless environments introduce challenges with respect to implementation, deployment, and protocol compatibility. Variable link characteristics cause reordering when sending IP packets of the same flow over multiple paths. This paper introduces a multilink proxy that is able to transparently stripe traffic destined for multihomed clients. Operating on the network layer, the proxy uses path monitoring statistics to adapt to changes in throughput and latency. Experimental results obtained from a proof-of-concept implementation verify that our approach is able to fully aggregate the throughput of heterogeneous downlink streams, even if the path characteristics change over time. In addition, our novel method of equalizing delays by buffering packets on the proxy significantly reduces IP packet reordering and the buffer requirements of clients."
pub.1167687009,Caching User-Generated Content in Distributed Autonomous Networks via Contextual Bandit,"The escalating proliferation of user generated contents such as videos and images are dominating the network traffic. The optimal strategy for mitigating backbone congestion and minimizing user request latency lies in prudent caching at edge stations within distributed autonomous networks, obviating the necessity to transmit data to the cloud. However, accurately caching content based on distributed autonomous networks requires elaborative collaboration between edge servers, which remains a great challenge, especially when content is highly dynamic and the storage resources of edge stations are limited. To tackle this challenge, this paper proposes a contextual bandit-based online caching algorithm for evaluating the optimal content hit rate reward, which can adapt to the constantly changing stream of emerging content. We build the content space, BS space, and a fine-grained space searching method to cache contents and corresponding edge stations. Furthermore, to perform collaborative caching and sharing between edges, we propose a federated autonomous multi-layer caching framework, whereby each server can locally learn the model for accurate caching and a synchronous mechanism is set up for global updating, further improving the hit rates. Finally, we perform theoretical proofs and simulations, demonstrating that our regret is sublinear and our caching algorithm outperforms several state-of-the-art algorithms."
pub.1148060187,An active adaptation strategy for streaming time series classification based on elastic similarity measures,"In streaming time series classification problems, the goal is to predict the label associated to the most recently received observations over the stream according to a set of categorized reference patterns. In on-line scenarios, data arise from non-stationary processes, which results in a succession of different patterns or events. This work presents an active adaptation strategy that allows time series classifiers to accommodate to the dynamics of streamed time series data. Specifically, our approach consists of a classifier that detects changes between events over streaming time series. For this purpose, the classifier uses features of the dynamic time warping measure computed between the streamed data and a set of reference patterns. When classifying a streaming series, the proposed pattern end detector analyzes such features to predict changes and adapt off-line time series classifiers to newly arriving events. To evaluate the performance of the proposed scheme, we employ the pattern end detection model along with dynamic time warping-based nearest neighbor classifiers over a benchmark of ten time series classification problems. The obtained results present exciting insights into the detection accuracy and latency performance of the proposed strategy."
pub.1098899935,Architectural Mechanisms for Explicit Communication in Shared Memory Multiprocessors,"The goal of this work is to explore architectural mechanisms for supporting explicit communication in cache-coherent shared memory multiprocessors. The motivation stems from the observation that applications display wide diversity in terms of sharing characteristics and hence impose different communication requirements on the system. Explicit communication mechanisms would allow tailoring the coherence management under software control to match these differing needs and strive to provide a close approximation to a zero overhead machine from the application perspective. Toward achieving these goals, we first analyze the characteristics of sharing observed in certain specific applications. We then use these characteristics to synthesize explicit communication primitives. The proposed primitives allow selectively updating a set of processors, or requesting a stream of data ahead of its intended use. These primitives are essentially generalizations of prefetch and poststore, with the ability to specify the sharer set for poststore either statically or dynamically. The proposed primitives are to be used in conjunction with an underlying invalidation based protocol. Used in this manner, the resulting memory system can dynamically adapt itself to performing either invalidations or updates to match the communication needs. Through application driven performance study we show the utility of these mechanisms in being able to reduce and tolerate communication latencies."
pub.1094154439,EMS: Encoded Multipath Streaming for Real-time Live Streaming Applications,"Multipath streaming protocols have recently attracted much attention because they provide an effective means to provide high-quality streaming over the Internet. However, many existing schemes require a long start-up delay and thus are not suitable for interactive applications such as video conferencing and tele-presence. In this paper, we focus on real-time live streaming applications with stringent end-to-end latency requirement, say several hundreds of milliseconds. To address these challenges, we take a joint multipath and FECapproach that intelligently splits the FEC-encoded stream among multiple available paths. We develop an analytical model and use asymptotic analysis to derive closed-form, optimal load splitting solutions, which are surprisingly simple yet insightful. To our best knowledge, this is the first work that provides such closed-form optimal solutions. Based on the analytical insights, we have designed and implemented a novel Encoded Multipath Streaming (EMS) scheme for real-time live streaming. EMS strives to continuously satisfy the application's QoS requirements by dynamically adjusting the load splitting decisions and the FEC settings. Our simulation results have shown that EMS can not only outperform the existing multipath streaming schemes, but also adapt to the dynamic loss and delay characteristics of the network with minimal overhead."
pub.1172746474,Maximizing training efficiency through intelligent data exposure,"Edge computing in remote sensing often necessitates on-device learning due to bandwidth and latency constraints. However, limited memory and computational power on edge devices pose challenges for traditional machine learning approaches with large datasets and complex models. Continuous learning offers a potential solution for these scenarios by enabling models to adapt to evolving data streams. This paper explores the concept of leveraging a strategically selected subset of archival training data to improve performance in continual learning. We introduce a feedback-based intelligent data sampling method that utilizes a log-normal distribution to prioritize informative data points from the original training set, focusing on samples which the model struggled with during initial training. This simulation-based exploration investigates the trade-off between accuracy gains and resource utilization with different data inclusion rates, paving the way for the deployment of this approach in real-world edge devices. This approach can lead to better decision making in the field, improved operational efficiency through reduced reliance on cloud resources, and greater system autonomy for remotely sensing tasks. This will lead to the development of robust and efficient edge-based learning systems that enable real-time, autonomous, and data-driven decisions for critical tasks in remote locations."
pub.1107570777,Chapter 21 Greening and Cooling the City Using Novel Urban Water Systems A European Perspective,"In today's cities water appears as drinking water, wastewater, rainwater, and runoff, as well as natural and artificial waterbodies. These water streams play a key role in the urban metabolism. The management of the water streams is challenging, especially in dense urban areas and in the context of climate change. Moreover, additional requirements have evolved including adapting to climate change, improving the quality of urban life, creating urban cooling and green areas in the cities, increasing resource protection, and flood protection and prevention. To tackle these challenges, current water infrastructure needs to strongly adapt or even transform its essential character. It has to become more flexible regarding its response time to adaptation and provide services more targeted toward the specific local needs. Here, recent innovations in water infrastructures, also called novel urban water systems, come into the picture. They provide possibilities able to react both faster and more specifically, and to build strong bridges to other technical infrastructure and urban planning. Water sensitive urban design (WSUD) focuses on the management of all water streams within the city. Although the focus of this approach is mostly identified in stormwater management, WSUD also includes the sustainable management of domestic wastewater. When it comes to the projects built under WSUD design principles, stormwater is usually considered, whereas communal/domestic wastewater is often not taken into account. This chapter argues that there are specific cases in novel urban water systems where an active integration of wastewater into the WSUD concept should be considered, as it provides clear advantages and benefits for both. Moreover, it provides further details on the integration of novel urban water systems into a WSUD approach and shows examples where such integration is already practiced."
pub.1101926069,Software-defined board- and chip-level optical interconnects for multi-socket communication and disaggregated computing,"The vast amount of new data being generated is outpacing the development of infrastructures and continues to grow at much higher rates than Moores law, a problem that is commonly referred to as the IJdata deluge problem. This brings current computational machines in the struggle to exceed Exascale processing powers by 2020 and this is where the energy boundary is setting the second, bottom-side alarm: A reasonable power envelope for future Super-computers has been projected to be 20MW, while worlds current No. 1 Supercomputer Sunway TaihuLight provides 93 Pflops and requires already 15.37 MW. This simply means that we have reached so far below 10% of the Exascale target but we consume already more than 75% of the tar-geted energy limit! The way to escape is currently following the paradigm of disaggregating and disintegrating resources, massively introducing at the same time optical technologies for interconnect purposes. Disaggregating computing from memory and storage modules can allow for flexible and modular settings where hardware requirements can be tailored to meet the certain energy and performance metrics targeted per application. At the same time, optical interconnect and photonic integration technologies are rapidly replacing electrical interconnects continuously penetrating at deeper hierarchy levels: Silicon photonics have enabled the penetration of optical technology to the computing environment, starting from rack-to-rack and gradually shifting towards board-level communications. In this article, we present our recent work towards implementing on-board single-mode optical interconnects that can support Software Defined Networking allowing for programmable and flexible computational settings that can quickly adapt to the application requirements. We present a programmable 44 Silicon Photonic switch that supports SDN through the use of Bloom filter (BF) labeled router ports. Our scheme significantly simplifies packet forwarding as it negates the need for large forwarding tables, supporting at the same time network size and topol-ogy changes through simple modifications in the assigned BF labels. We demonstrate 14 switch operation controlling the Si-Pho switch by a Stratix V FPGA board that is responsible for processing the packet ID and correlating its destination with the appropriate BF-labeled switch output port. Moving towards high-capacity board-level settings, we discuss the architecture and technology being currently promoted by the recently started H2020 project ICT-STREAMS, where single-mode optical PCBs hosting Si-based routing modules and mid-board transceiver optics expect to enable a massive any-to-any, buffer-less, collision-less and extremely low latency routing platform with 25.6Tb/s aggregate through-put. This architecture and technology are also extended to support resource disaggregation in data centers as currently being pursued in the H2020 project dREDBox, where the any-to-any collisionless routing scheme is prop"
pub.1150159274,Bandwidth-efficient multi-task AI inference with dynamic task importance for the Internet of Things in edge computing,"Over the past years, artificial intelligence (AI) models have been utilized for the Internet of Things (IoT) in applications such as remote assistance based on augmented reality (AR) in smart factories, as well as powerline inspection and precision agriculture missions performed by unmanned aerial vehicles (UAVs). Due to the limited battery capacity and computing power of these devices (e.g., AR glasses and UAVs), edge computing is recognized as a means to empower the Internet of Things (IoT) with AI. Considering that multiple AI model inference tasks (e.g., point cloud classification and fault detection) are typically performed on the same stream of sensory data (e.g., UAV camera feed), we propose TORC (Tasks-Oriented Edge Computing) to reduce the bandwidth requirement. By incorporating AI into data transmission, the lightweight framework of TORC preserves edge computing servers’ ability to reconstruct/restore data into the original form, ensuring the proper coexistence of AI inference tasks and traditional non-AI tasks like human inspection, as well as simultaneous localization and mapping. It encodes and decodes sensory data with neural networks, whose training is driven by the AI inference tasks, in order to reduce bandwidth consumption and latency without impairing the accuracy of the AI inference tasks. Additionally, taking into account the mobility of the IoT and changes in the environment, TORC can adapt to variation in the bandwidth budget, as well as the temporally dynamic importance of AI inference tasks, without the need to train multiple neural networks for each setting. As a demonstration, empirical results conducted on the Cityscapes dataset and tasks related to autonomous driving show that, at the same level of accuracy, TORC reduces the bandwidth consumption by up to 48% and latency by up to 26%."
pub.1165165902,Access Priority Adaptation for Triggered Uplink Channel Access in 802.11ax WLANs,"Uplink Multi-User (MU) MIMO transmissions allow clients to simultaneously transmit independent data streams to the Access Point (AP), effectively multiplying the capacity of the wireless channel for uplink access. Due to inherent limitations of the distributed wireless networks, extra coordination is required for effective implementation of uplink MU-MIMO. Triggered uplink access (TUA) is the only mechanism that can initiate an uplink MU-MIMO transmission in Wi-Fi: it enables an access point (AP) to start simultaneous uplink multi-user (MU) transmissions. To trigger a MU uplink transmission, the AP must first contend for the channel using the enhanced distributed channel access (EDCA) and win channel access to broadcast the trigger frame in the downlink direction. At the same time, clients that have traffic buffered for uplink transmission also contend for channel access using the same EDCA method. However, the aforementioned mechanism introduces a fundamental conflict in the network. There are potentially two network entities competing for the channel for the same packet, namely, the AP contends for the channel to broadcast the trigger frame, while the clients that have traffic buffered for uplink transmission also contend for single-user (SU) channel access. Yet, while TUA MU transmission is preferable to SU uplink, one cannot disable the latter entirely. In this paper, we introduce Client-side Access Manipulation (CAM) as a mechanism to enable clients to dynamically adapt their channel access priority in order to realize an efficient uplink MU-MIMO WLAN. Through experiments in an end-to-end testbed with the TUA mechanism, an 11ax compliant network, and traffic from bursty closed-loop applications we show that CAM achieves gains in throughput and up to 65% reduction in average latency. Moreover, we show that, on the same scenarios, the aggregate throughput decreases and the average latency increases sharply with the use of the standard's defined access adaptation mechanism."
pub.1174178786,Defining and testing the value proposition of outside-in planning processes,"Traditional planning processes are inside-out. The focus is on mining the patterns of orders and shipments and synchronising demand and supply based on enterprise data. The problem with this approach is with product proliferation and item complexity, demand latency increases, making the order signal out of step with the market by weeks and months. In addition, as supply chain leaders attempt to better align supply chains with markets in the face of months after months of disruption, historical patterns of orders and shipments are irrelevant. The reason is simple. In a volatile world, history is not a good signal to drive decisions. To find a solution, o9 Solutions and Supply Chain Insights partnered with a group of business leaders and academics to test the use of multiple streams of demand data and determine the value of using market, or channel, data versus driving demand off of order or shipment patterns. In this paper, we share insights and the journey. While the opportunity is tremendous and the value clear, the biggest issue for technologists and business leaders is unlearning conventional paradigms. As the use of market data makes many of the traditional planning assumptions obsolete. Here the reader will learn the lessons from the testing and gain insights to adapt their planning processes to be more outside-in."
pub.1094617493,Fast Event-based Harris Corner Detection Exploiting the Advantages of Event-driven Cameras,"The detection of consistent feature points in an image is fundamental for various kinds of computer vision techniques, such as stereo matching, object recognition, target tracking and optical flow computation. This paper presents an event-based approach to the detection of corner points, which benefits from the high temporal resolution, compressed visual information and low latency provided by an asynchronous neuromorphic event-based camera. The proposed method adapts the commonly used Harris corner detector to the event-based data, in which frames are replaced by a stream of asynchronous events produced in response to local light changes at μs temporal resolution. Responding only to changes in its field of view, an event-based camera naturally enhances edges in the scene, simplifying the detection of corner features. We characterised and tested the method on both a controlled pattern and a real scenario, using the dynamic vision sensor (DVS) on the neuromorphic iCub robot. The method detects corners with a typical error distribution within 2 pixels. The error is constant for different motion velocities and directions, indicating a consistent detection across the scene and over time. We achieve a detection rate proportional to speed, higher than frame-based technique for a significant amount of motion in the scene, while also reducing the computational cost."
pub.1063202547,Distortion-Based Link Adaptation for Wireless Video Transmission,"Wireless local area networks (WLANs) such as IEEE 802.11a/g utilise numerous transmission modes, each providing different throughputs and reliability levels. Most link adaptation algorithms proposed in the literature (i) maximise the error-free data throughput, (ii) do not take into account the content of the data stream, and (iii) rely strongly on the use of ARQ. Low-latency applications, such as real-time video transmission, do not permit large numbers of retransmission. In this paper, a novel link adaptation scheme is presented that improves the quality of service (QoS) for video transmission. Rather than maximising the error-free throughput, our scheme minimises the video distortion of the received sequence. With the use of simple and local rate distortion measures and end-to-end distortion models at the video encoder, the proposed scheme estimates the received video distortion at the current transmission rate, as well as on the adjacent lower and higher rates. This allows the system to select the link-speed which offers the lowest distortion and to adapt to the channel conditions. Simulation results are presented using the MPEG-4/AVC H.264 video compression standard over IEEE 802.11g. The results show that the proposed system closely follows the optimum theoretic solution."
pub.1061574966,Supporting Interactive Video-on-Demand with Adaptive Multicast Streaming,"Recent advances in multicast video streaming algorithms have opened up new ways to provision video-on-demand services to potentially millions of users. However, the spectacular efficiency of multicast streaming algorithms can only be realized by restricting or even prohibiting interactive playback control. Experiments reveal that the performance of current state-of-the-art multicast streaming algorithms will degrade significantly even at very low levels of interactivity (e.g., one control per five users). This study tackles this challenge by investigating the fundamental limitations of multicast streaming algorithms in supporting interactive playback control and presents a general solution—static full stream scheduling (SFSS)—which can be applied to many of the existing multicast streaming algorithms to substantially improve their performance when interactive playback control is to be supported. Moreover, to solve the problem of optimizing the algorithm for the often unknown client access patterns (e.g., arrival rates and interactivity rates), we present a novel just-in-time simulation (JTS) scheme to dynamically and automatically tune operating parameters of the SFSS algorithm while the system is online. This JTS scheme not only eliminates the need for a priori knowledge of the often unknown system parameters, but also can adapt to changes in the client access pattern over time. Extensive simulation results show that the proposed adaptive algorithm can reduce the admission and interactive control latencies by as much as 90%."
pub.1168380368,Fast and scalable all-optical network architecture for distributed deep learning," With the ever-increasing size of training models and datasets, network communication has emerged as a major bottleneck in distributed deep learning training. To address this challenge, we propose an optical distributed deep learning (ODDL) architecture. ODDL utilizes a fast yet scalable all-optical network architecture to accelerate distributed training. One of the key features of the architecture is its flow-based transmit scheduling with fast reconfiguration. This allows ODDL to allocate dedicated optical paths for each traffic stream dynamically, resulting in low network latency and high network utilization. Additionally, ODDL provides physically isolated and tailored network resources for training tasks by reconfiguring the optical switch using LCoS-WSS technology. The ODDL topology also uses tunable transceivers to adapt to time-varying traffic patterns. To achieve accurate and fine-grained scheduling of optical circuits, we propose an efficient distributed control scheme that incurs minimal delay overhead. Our evaluation on real-world traces showcases ODDL’s remarkable performance. When implemented with 1024 nodes and 100 Gbps bandwidth, ODDL accelerates VGG19 training by 1.6× and 1.7× compared to conventional fat-tree electrical networks and photonic SiP-Ring architectures, respectively. We further build a four-node testbed, and our experiments show that ODDL can achieve comparable training time compared to that of an ideal electrical switching network. "
pub.1131976411,NeuroIV: Neuromorphic Vision Meets Intelligent Vehicle Towards Safe Driving With a New Database and Baseline Evaluations,"Neuromorphic vision sensors such as the Dynamic and Active-pixel Vision Sensor (DAVIS) using silicon retina are inspired by biological vision, they generate streams of asynchronous events to indicate local log-intensity brightness changes. Their properties of high temporal resolution, low-bandwidth, lightweight computation, and low-latency make them a good fit for many applications of motion perception in the intelligent vehicle. However, as a younger and smaller research field compared to classical computer vision, neuromorphic vision is rarely connected with the intelligent vehicle. For this purpose, we present three novel datasets recorded with DAVIS sensors and depth sensor for the distracted driving research and focus on driver drowsiness detection, driver gaze-zone recognition, and driver hand-gesture recognition. To facilitate the comparison with classical computer vision, we record the RGB, depth and infrared data with a depth sensor simultaneously. The total volume of this dataset has 27360 samples. To unlock the potential of neuromorphic vision on the intelligent vehicle, we utilize three popular event-encoding methods to convert asynchronous event slices to event-frames and adapt state-of-the-art convolutional architectures to extensively evaluate their performances on this dataset. Together with qualitative and quantitative results, this work provides a new database and baseline evaluations named NeuroIV in cross-cutting areas of neuromorphic vision and intelligent vehicle."
pub.1004920191,A grid density based framework for classifying streaming data in the presence of concept drift,"Mining data streams is the process of extracting information from non-stopping, rapidly flowing data records to provide knowledge that is reliable and timely. Streaming data algorithms need to be one pass and operate under strict limitations of memory and response time. In addition, the classification of streaming data requires learning in an environment where the data characteristics might change constantly. Many of the classification algorithms presented in literature assume a 100 % labeling rate, which is impractical and expensive when data records are rapidly flowing in. In this paper, a new incremental grid density based learning framework, the GC3 framework, is proposed to perform classification of streaming data with concept drift and limited labeling. The proposed framework uses grid density clustering to detect changes in the input data space. It maintains an evolving ensemble of classifiers to learn and adapt to the model changes over time. The framework also uses a uniform grid density sampling mechanism to obtain a uniform subset of samples for better classification performance with a lower labeling rate. The entire framework is designed to be one-pass, incremental and work with limited memory to perform any-time classification on demand. Experimental comparison with state of the art concept drift handling systems demonstrate the GC3 frameworks ability to provide high classification performance, using fewer models in the ensemble and with only 4-6 % of the samples labeled. The results show that the GC3 framework is effective and attractive for use in real world data stream classification applications."
pub.1092752303,Dynamic data-driven learning for self-healing avionics,"In sensor-based systems, spatio-temporal data streams are often related in non-trivial ways. For example in avionics, while the airspeed that an aircraft attains in cruise phase depends on the weight it carries, it also depends on many other factors such as engine inputs, angle of attack, and air density. It is therefore a challenge to develop failure models that can help recognize errors in the data, such as an incorrect fuel quantity or an incorrect airspeed. In this paper, we present a highly-declarative programming framework that facilitates the development of self-healing avionics applications, which can detect and recover from data errors. Our programming framework enables specifying expert-created failure models using error signatures, as well as learning failure models from data. To account for unanticipated failure modes, we propose a new dynamic Bayes classifier, that detects outliers and upgrades them to new modes when statistically significant. We evaluate error signatures and our dynamic Bayes classifier for accuracy, response time, and adaptability of error detection. While error signatures can be more accurate and responsive than dynamic Bayesian learning, the latter method adapts better due to its data-driven nature."
pub.1006054251,Parallel Ada for Symmetrical Multiprocessors,"ABSTRACT A recent trend in computer engineering has been the replacement of large uniprocessor based proprietary architectures by multiple microprocessor based designs employing various interconnection strategies. While these multiprocessor based systems offer significant performance and economic advantages over uniprocessor systems, not all prospective users are able or willing to adapt their applications to execute as multiple concurrent streams. The Ada programming language is well suited to multiprocessor systems as it allows the programmer to direct the use of concurrency through the use of the Ada tasking mechanism. The avoidance of automatic distribution of the program by the compiler and the choice of the Ada task as the unit of distribution greatly simplify the development of Ada software for multiprocessor architectures. For performance reasons, the inter-processor communications path should offer low latency and high transfer rates. Shared memory supports these characteristics and a multiprocessor system, where all memory can be accessed by all processors, has proven to be a suitable platform for a parallel Ada implementation. This paper discusses the implementation and architecture of a parallel Ada system that allows up to twenty processors to co-execute the same Ada program with true concurrency. Particular attention is given to the design of the Ada runtime and the interface between the runtime and the underlying operating system, as these parts of the system must be “multi-threaded” throughout in order to minimize bottle-necks. The paper concludes with the description of a 1000 MIPS Ada engine currently under development."
pub.1137893252,Automatic Sublining for Efficient Sparse Memory Accesses,"Sparse memory accesses, which are scattered accesses to single elements of a large data structure, are a challenge for current processor architectures. Their lack of spatial and temporal locality and their irregularity makes caches and traditional stream prefetchers useless. Furthermore, performing standard caching and prefetching on sparse accesses wastes precious memory bandwidth and thrashes caches, deteriorating performance for regular accesses. Bypassing prefetchers and caches for sparse accesses, and fetching only a single element (e.g., 8 B) from main memory (subline access), can solve these issues. Deciding which accesses to handle as sparse accesses and which as regular cached accesses, is a challenging task, with a large potential impact on performance. Not only is performance reduced by treating sparse accesses as regular accesses, not caching accesses that do have locality also negatively impacts performance by significantly increasing their latency and bandwidth consumption. Furthermore, this decision depends on the dynamic environment, such as input set characteristics and system load, making a static decision by the programmer or compiler suboptimal.  We propose the Instruction Spatial Locality Estimator ( ISLE ), a hardware detector that finds instructions that access isolated words in a sea of unused data. These sparse accesses are dynamically converted into uncached subline accesses, while keeping regular accesses cached. ISLE does not require modifying source code or binaries, and adapts automatically to a changing environment (input data, available bandwidth, etc.). We apply ISLE to a graph analytics processor running sparse graph workloads, and show that ISLE outperforms the performance of no subline accesses, manual sublining, and prior work on detecting sparse accesses. "
pub.1023432130,Distributed optimization of P2P live streaming overlays,"Peer-to-peer live media streaming over the Internet is becoming increasingly more popular, though it is still a challenging problem. Nodes should receive the stream with respect to intrinsic timing constraints, while the overlay should adapt to the changes in the network and the nodes should be incentivized to contribute their resources. In this work, we meet these contradictory requirements simultaneously, by introducing a distributed market model to build an efficient overlay for live media streaming. Using our market model, we construct two different overlay topologies, tree-based and mesh-based, which are the two dominant approaches to the media distribution. First, we build an approximately minimal height multiple-tree data dissemination overlay, called Sepidar. Next, we extend our model, in GLive, to make it more robust in dynamic networks by replacing the tree structure with a mesh. We show in simulation that the mesh-based overlay outperforms the multiple-tree overlay. We compare the performance of our two systems with the state-of-the-art NewCoolstreaming, and observe that they provide better playback continuity and lower playback latency than that of NewCoolstreaming under a variety of experimental scenarios. Although our distributed market model can be run against a random sample of nodes, we improve its convergence time by executing it against a sample of nodes taken from the Gradient overlay. The evaluations show that the streaming overlays converge faster when our market model works on top of the Gradient overlay."
pub.1170405944,Differential-Matching Prefetcher for Indirect Memory Access,"Indirect memory access is a critical bottleneck for modern CPUs, especially for graph analysis and sparse linear algebra applications, where the values of one data array are used to generate the fetching addresses of another array. It often causes irregular data accesses with poor temporal and spatial locality that are difficult to be captured by conventional hardware prefetchers. For many complex workloads, such indirect access patterns may have different types and are nested in a multiplelevel form. Moreover, branch mispredictions would further disturb their patterns, making them even harder to detect. As a result, existing hardware prefetchers are unable to fully prefetch complex indirect patterns. This paper proposes DMP, a low-cost hardware prefetcher to improve the memory latency in several representative irregular workloads. DMP targets four types of indirect memory access patterns including single, range, multi-level, and multi-way indirect access. DMP uses differential matching to identify an indirect access pattern in pair with its corresponding index stream. Then DMP uses a flexible prefetching mechanism to dynamically adapt the prefetching degree to maintain prefetching coverage. We evaluate the performance, energy consumption, and transistor cost of DMP among various algorithms from GAP, NAS, and HPCG benchmarks. DMP improves performance by 1.8 × (up to 5.6 ×) on average against state-of-the-art hardware prefetchers and 1.2 × (up to 2.3 ×) speedup against state-of-the-art compiler-based prefetcher Prodigy. Besides, the proposed design is optimized to take only 0.9KB of storage, making it feasible to be integrated into current CPU designs."
pub.1157312347,Dynamic frequency slot allocation on IP-over-EON access links with multiple-type and time-varying traffic,"Access links that connect client networks to public networks have to carry multiple-type and time-varying traffic. Those hybrid traffic flows compete for limited bandwidth, yet at the same time have drastically different performance requirements. As planning the link capacity for peak traffic demand is not economically viable, it is important that bandwidth on the access links is shared between different traffic classes in a way that maximal network utility can be achieved. In this paper, we study the frequency slot allocation problem on IP-over-elastic optical network (EON) access links that carry three types of traffic, namely, packet streams, latency critical circuit connections (e.g., video conferencing), and delay tolerant circuit connections (e.g., bulk data transfers). We define four network operation states; in each of which an access link serves traffic with different levels of fulfillment. We then formulate the allocation problem into a weak-constrained optimization problem and propose a genetic algorithm to solve it in real time. Numerical results show that the relative error of the genetic algorithm is within 3% and the access link keeps maintaining the optimal achievable network operation state. We also show that, by increasing the storage size, the access link can adapt to the increasing traffic load within a certain range without upgrading the expensive access link bandwidth. Our study provides useful insights in managing and operating IP-over-EON access links, and the concept of multiple network operation states can be generalized to networks that serve more than one type of traffic."
pub.1105041215,Determination of the response time of new generation electromagnetic injectors as a function of fuel pressure using the internal photoelectric effect,"Very strict requirements referring to emission standards, the pollution of which is closely related to the quality of the combustion process, forces the manufacturers to introduce new solutions in the field of fuel injection, as well as adapt existing to more stringent standards. A noticeable trend in self-ignition engines is the return to injectors with electromagnetic control, due to the longer durability of the control element in relation to injectors with piezoelectric control. The use of electromagnetic injectors, however, required the introduction of numerous changes, resulting in a reduction in the number and weight of moving elements, which allowed to achieve greater precision of control and multiplication of the fuel dose. Due to the wide range of injector work pressures, it is necessary to determine the delay between the coil override and the beginning of the injection process, so that it is possible to precisely control the supply of fuel at each point of the engine's operation. In order to characterize the delay as a function of fuel pressure, the authors used the photoelectric effect. At the moment when the fuel stream interrupts the light source in the photoactive element, there is a voltage drop, on the basis of which it is possible to determine the actual start of the injection. The authors managed to show that the increase in fuel pressure in the reservoir significantly reduces the response time of the injector. Using the waveform curves, the relationship between the increase in the fuel dose for the measured cross section and the voltage drop in the photoactive element was also determined. It was also found, that the method basing on the use of a radiation source and a photoactive element can be used in the diagnostics of injectors to identify damaged friction pairs needle - body."
pub.1118443932,Performance-oriented DevOps: A Research Agenda,"DevOps is a trend towards a tighter integration between development (Dev) and
operations (Ops) teams. The need for such an integration is driven by the
requirement to continuously adapt enterprise applications (EAs) to changes in
the business environment. As of today, DevOps concepts have been primarily
introduced to ensure a constant flow of features and bug fixes into new
releases from a functional perspective. In order to integrate a non-functional
perspective into these DevOps concepts this report focuses on tools,
activities, and processes to ensure one of the most important quality
attributes of a software system, namely performance.
  Performance describes system properties concerning its timeliness and use of
resources. Common metrics are response time, throughput, and resource
utilization. Performance goals for EAs are typically defined by setting upper
and/or lower bounds for these metrics and specific business transactions. In
order to ensure that such performance goals can be met, several activities are
required during development and operation of these systems as well as during
the transition from Dev to Ops. Activities during development are typically
summarized by the term Software Performance Engineering (SPE), whereas
activities during operations are called Application Performance Management
(APM). SPE and APM were historically tackled independently from each other, but
the newly emerging DevOps concepts require and enable a tighter integration
between both activity streams. This report presents existing solutions to
support this integration as well as open research challenges in this area."
pub.1164411459,PATCH,"Recent advancements in deep learning have shown that multimodal inference can be particularly useful in tasks like autonomous driving, human health, and production line monitoring. However, deploying state-of-the-art multimodal models in distributed IoT systems poses unique challenges since the sensor data from low-cost edge devices can get corrupted, lost, or delayed before reaching the cloud. These problems are magnified in the presence of asymmetric data generation rates from different sensor modalities, wireless network dynamics, or unpredictable sensor behavior, leading to either increased latency or degradation in inference accuracy, which could affect the normal operation of the system with severe consequences like human injury or car accident. In this paper, we propose PATCH, a framework of speculative inference to adapt to these complex scenarios. PATCH serves as a plug-in module in the existing multimodal models, and it enables speculative inference of these off-the-shelf deep learning models. PATCH consists of 1) a Masked-AutoEncoder-based cross-modality imputation module to impute missing data using partially-available sensor data, 2) a lightweight feature pair ranking module that effectively limits the searching space for the optimal imputation configuration with low computation overhead, and 3) a data alignment module that aligns multimodal heterogeneous data streams without using accurate timestamp or external synchronization mechanisms. We implement PATCH in nine popular multimodal models using five public datasets and one self-collected dataset. The experimental results show that PATCH achieves up to 13% mean accuracy improvement over the state-of-art method while only using 10% of training data and reducing the training overhead by 73% compared to the original cost of retraining the model."
pub.1163150443,Real-Time Video and Audio in the World Wide Web,"The architecture of World Wide Web (WWW) browsers and servers supports full-file transfer for document retrieval. TCP is used for data transfers by Web browsers and their associated Hypertext Transfer Protocol (HTTP) servers. Full-file transfer and TCP are unsuitable for continuous media, such as realtime audio and video. In order for the WWW to support continuous media, we require the transmission of video and audio on demand and in real time, as well as new protocols for real-time data. We extend the architecture of the WWW to encompass the dynamic, real-time information space of video and audio. Our WWW browser Vosaic, short for Video Mosaic, incorporates real-time video and audio into standard hypertext pages that are displayed in place. Video and audio transfers occur in real time; there is no file-retrieval latency. The video and audio result in compelling Web pages. Real-time video and audio data can be effectively served over the present day Internet with the proper transmission protocol. We have developed a real-time protocol called VDP that we specialized for handling real-time video over the WWW. VDP reduces inter-frame jitter and dynamically adapts to the client CPU load and network congestion. Our WWW server dynamically changes transfer protocols, adapting to the request stream and the metainformation in requested documents. Experiments show a 44-fold increase in received video-frame rate (0.2 frames-per-second (fps) to 9 fps) with the use of VDP in lieu of TCP, with a commensurate improvement in observed video quality. Our work enables a video-enhanced Web."
pub.1130850497,Age of Information Aware Trajectory Planning of UAVs in Intelligent Transportation Systems: A Deep Learning Approach,"Unmanned aerial vehicles (UAVs) are envisioned to play a key role in intelligent transportation systems to complement the communication infrastructure in future smart cities. UAV-assisted vehicular networking research typically adopts throughput and latency as the main performance metrics. These conventional metrics, however, are not adequate to reflect the freshness of the information, an attribute that has been recently identified as a critical requirement to enable services such as autonomous driving and accident prevention. In this paper, we consider a UAV-assisted single-hop vehicular network, wherein sensors (e.g., LiDARs and cameras) on vehicles generate time sensitive data streams, and UAVs are used to collect and process this data while maintaining a minimum age of information (AoI). We aim to jointly optimize the trajectories of UAVs and find scheduling policies to keep the information fresh under minimum throughput constraints. The formulated optimization problem is shown to be mixed integer non-linear program (MINLP) and generally hard to be solved. Motivated by the success of machine learning (ML) techniques particularly deep learning in solving complex problems with low complexity, we reformulate the trajectories and scheduling policies problem as a Markov decision process (MDP) where the system state space considers the vehicular network dynamics. Then, we develop deep reinforcement learning (DRL) to learn the vehicular environment and its dynamics in order to handle UAVs trajectory and scheduling policy. In particular, we leverage Deep Deterministic Policy Gradient (DDPG) for learning the trajectories of the deployed UAVs to efficiently minimize the Expected Weighted Sum AoI (EWSA). Simulations results demonstrate the effectiveness of the proposed design and show the deployed UAVs adapt their velocities during the data collection mission in order to minimize the AoI."
pub.1173929525,Thunderbird: Efficient Homomorphic Evaluation of Symmetric Ciphers in 3GPP by combining two modes of TFHE,"Hybrid homomorphic encryption (a.k.a., transciphering) can alleviate the ciphertext size expansion inherent to fully homomorphic encryption by integrating a specific symmetric encryption scheme, which requires selected symmetric encryption scheme that can be efficiently evaluated homomorphically. While there has been a recent surge in the development of FHE-friendly ciphers, concerns have arisen regarding their security. A significant challenge for the transciphering community remains the efficient evaluation of symmetric encryption algorithms that have undergone extensive study and standardization.In this paper, we present an evaluation framework, dubbed Thunderbird, which for the first time presents efficient homomorphic implementations of stream ciphers SNOW 3G and ZUC that are standardized in the 3G Partnership Project (3GPP). Specifically, Thunderbird combines gate bootstrapping mode and leveled evaluation mode of TFHE to cater to various function types within symmetric encryption algorithms. In the gate bootstrapping mode, we propose a variant of the homomorphic full adder that consumes only a single blind rotation, which may be of independent interest. In the leveled evaluation mode, we employ the CMux gate combining with hybrid packing technique to efficiently achieve lookup tables, significantly reducing the need for gate bootstrapping, and adapt the current optimal circuit bootstrapping to expedite the Thunderbird framework. We have implemented the Thunderbird framework in the TFHEpp public library. Experimental results demonstrate that SNOW 3G and ZUC can homomorphically generate a keyword in only 7 seconds and 9.5 seconds, which are 52x and 32x faster than the trivial gate bootstrapping mode, respectively. For the homomorphic evaluation of the AES-128 algorithm using Thunderbird, we achieve a speedup of 1.9x in terms of latency and use less evaluation key compared to the state-of-the-art work."
pub.1170104958,Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in Event Cameras,"Event cameras are increasingly popular in robotics due to beneficial features
such as low latency, energy efficiency, and high dynamic range. Nevertheless,
their downstream task performance is greatly influenced by the optimization of
bias parameters. These parameters, for instance, regulate the necessary change
in light intensity to trigger an event, which in turn depends on factors such
as the environment lighting and camera motion. This paper introduces feedback
control algorithms that automatically tune the bias parameters through two
interacting methods: 1) An immediate, on-the-fly \textit{fast} adaptation of
the refractory period, which sets the minimum interval between consecutive
events, and 2) if the event rate exceeds the specified bounds even after
changing the refractory period repeatedly, the controller adapts the pixel
bandwidth and event thresholds, which stabilizes after a short period of noise
events across all pixels (\textit{slow} adaptation). Our evaluation focuses on
the visual place recognition task, where incoming query images are compared to
a given reference database. We conducted comprehensive evaluations of our
algorithms' adaptive feedback control in real-time. To do so, we collected the
QCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366
repeated traversals of a Scout Mini robot navigating through a 100 meter long
indoor lab setting (totaling over 35km distance traveled) in varying brightness
conditions with ground truth location information. Our proposed feedback
controllers result in superior performance when compared to the standard bias
settings and prior feedback control methods. Our findings also detail the
impact of bias adjustments on task performance and feature ablation studies on
the fast and slow adaptation mechanisms."
pub.1120053954,Video Demo of LiveAR: Real-Time Human Action Recognition over Live Video Streams,"We propose to present a video demonstration of LiveAR at the ARC'16 conference. For this purpose, we have prepared three demo videos, which can be found in the submission files. These video demos show the effectiveness and efficiency of LiveAR running on video streams containing a diverse set of human actions. Additionally, the demo also exhibits important system performance parameters such as latency and resource usage. LiveAR is a novel system for recognizing human actions, such as running and fighting, in a video stream in real time, backed by a massively-parallel processing (MPP) platform. Although action recognition is a well-studied topic in computer vision, so far most attention has been devoted to improving accuracy, rather than efficiency. To our knowledge, LiveAR is the first that achieves real-time efficiency in action recognition, which can be a key enabler in many important applications, e.g., video surveillance and monitoring over critical infrastructure such as water reservoirs. LiveAR is based on a state-of-the-art method for offline action recognition which obtains high accuracy; its main innovation is to adapt this base solution to run on an elastic MPP platform to achieve real-time speed at an affordable cost. The main objectives in the design of LiveAR are to (i) minimize redundant computations, (ii) reduce communication costs between nodes in the cloud, (iii) allow a high degree of parallelism and (iv) enable dynamic node additions and removals to match the current workload. LiveAR is based on an enhanced version of Apache Storm. Each video manipulation operation is implemented as a bolt (i.e., logical operator) executed by multiple nodes, while the input frame arrive at the system via a spout (i.e., streaming source). The output of the system is presented on screen using FFmpeg. Next we briefly explain the main operations in LiveAR. The dense point extraction bolt is a first step for video processing, which has two input streams: the input video frame and the current trajectories. The output of this operator consists of dense points sampled in the video frame that are not already on any of the current trajectories. In particular, LiveAR partitions the frame into different regions, and assigns one region to a dense point evaluator, each running in a separate thread. Then, the sampled coordinates are grouped according to the partitioning, and routed to the corresponding dense point evaluator. Meanwhile, coordinates on current trajectories are similarly grouped by a point dispatcher, and routed accordingly. Such partitioning and routing minimizes network transmissions as each node is only fed the pixels and trajectory points it needs. The optic flow generation operator is executed by multiple nodes in parallel similarly to the dense point extractor. An additional challenge here is that the generation of optic flows involves (i) comparing two frames at consecutive time instances and (ii) multiple pixels in determining the value "
pub.1163334288,A multi-scenario approach to continuously learn and understand norm violations,"Using norms to guide and coordinate interactions has gained tremendous attention in the multiagent community. However, new challenges arise as the interest moves towards dynamic socio-technical systems, where human and software agents interact, and interactions are required to adapt to changing human needs. For instance, different agents (human or software) might not have the same understanding of what it means to violate a norm (e.g., what characterizes hate speech), or their understanding of a norm might change over time (e.g., what constitutes an acceptable response time). The challenge is to address these issues by learning to detect norm violations from the limited interaction data and to explain the reasons for such violations. To do that, we propose a framework that combines Machine Learning (ML) models and incremental learning techniques. Our proposal is equipped to solve tasks in both tabular and text classification scenarios. Incremental learning is used to continuously update the base ML models as interactions unfold, ensemble learning is used to handle the imbalance class distribution of the interaction stream, Pre-trained Language Model (PLM) is used to learn from text sentences, and Integrated Gradients (IG) is the interpretability algorithm. We evaluate the proposed approach in the use case of Wikipedia article edits, where interactions revolve around editing articles, and the norm in question is prohibiting vandalism. Results show that the proposed framework can learn to detect norm violation in a setting with data imbalance and concept drift."
pub.1093738242,A network measurement architecture for adaptive applications,"The quality of network connectivity between a pair of Internet hosts can vary greatly. Adaptive applications can cope with these differences in connectivity by choosing alternate representations of objects or streams or by downloading the objects from alternate locations. In order to effectively adapt, applications must discover the condition of the network before communicating with distant hosts. Unfortunately, the ability to predict or report the quality of connectivity is missing in today's suite of Internet services. To address this limitation, we have developed SPAND (shared passive network performance discovery), a system that facilitates the development of adaptive network applications. In each domain, applications make passive application specific measurements of the network and store them in a local centralized repository of network performance information. Other applications may retrieve this information from the repository and use the shared experiences of all hosts in a domain to predict future performance. In this way, applications can make informed decisions about adaptation choices as they communicate with distant hosts. In this paper, we describe and evaluate the SPAND architecture and implementation. We show how the architecture makes it easy to integrate new applications into our system and how the architecture has been used with specifics types of data transport. Finally, we describe LookingGlass, a WWW mirror site selection tool that uses SPAND. LookingGlass meets the conflicting goals of collecting passive network performance measurements and maintaining good client response times. In addition, LookingGlass's server selection algorithms based on application level measurements perform much better than techniques that rely on geographic location or route metrics."
pub.1142539309,Modelling novelty detection in the thalamocortical loop,"Abstract
                In complex natural environments, sensory systems are constantly exposed to a large stream of inputs. Novel or rare stimuli, which are often associated with behaviorally important events, are typically processed differently than the steady sensory background, which has less relevance. Neural signatures of such differential processing, commonly referred to as novelty detection, have been identified on the level of EEG recordings as mismatch negativity and the level of single neurons as stimulus-specific adaptation. Here, we propose a multi-scale recurrent network with synaptic depression to explain how novelty detection can arise in the whisker-related part of the somatosensory thalamocortical loop. The architecture and dynamics of the model presume that neurons in cortical layer 6 adapt, via synaptic depression, specifically to a frequently presented stimulus, resulting in reduced population activity in the corresponding cortical column when compared with the population activity evoked by a rare stimulus. This difference in population activity is then projected from the cortex to the thalamus and amplified through the interaction between neurons of the primary and reticular nuclei of the thalamus, resulting in spindle-like, rhythmic oscillations. These differentially activated thalamic oscillations are forwarded to cortical layer 4 as a late secondary response that is specific to rare stimuli that violate a particular stimulus pattern. Model results show a strong analogy between this late single neuron activity and EEG-based mismatch negativity in terms of their common sensitivity to presentation context and timescales of response latency, as observed experimentally. Our results indicate that adaptation in L6 can establish the thalamocortical dynamics that produce signatures of SSA and MMN and suggest a mechanistic model of novelty detection that could generalize to other sensory modalities.
                
                  Author summary
                  Cortical sensory neurons have been shown to be capable of novelty detection, that is they respond more vigorously when a novel, unexpected stimulus is presented, and less so when the stimulus is part of a predictable sequence. However, the neural mechanism underlying this capability is not yet fully understood. Here, we developed a thalamocortical network model that accounts for novelty detection and reproduces physiologically observed neural response patterns in the somatosensory cortex. Specifically, our results demonstrate that the novelty signal arises from the complex recurrent interplay between thalamic neurons and cortical neurons in layers 4 and 6. This work therefore provides a concrete mechanism that can serve as a starting point for further investigating the neural circuit mechanisms underlying novelty detection.
                "
pub.1167037092,Is this a violation? Learning and understanding norm violations in online communities,"Using norms to guide and coordinate interactions has gained tremendous attention in the multi-agent community. However, new challenges arise as the interest moves towards dynamic socio-technical systems, where human and software agents interact, and interactions are required to adapt to human's changing needs. For instance, different agents (human or software) might not have the same understanding of what it means to violate a norm (e.g., what characterizes hate speech), or their understanding of a norm might change over time (e.g., what constitutes an acceptable response time). The challenge is to address these issues by learning the meaning of a norm violation from limited interaction data. For this, we use batch and incremental learning to train an ensemble of classifiers. Ensemble learning and data-sampling handle the imbalanced class distribution of the interaction stream. At the same time, the training approaches use different strategies to ensure that the ensemble models reflect the latest community view on the meaning of norm violation. Batch learning uses weight assignment, while incremental learning continuously updates the ensemble models as community members interact. Here, we extend our previous work by creating a different balance strategy for online learning and integrating interpretability to understand norm violations. Additionally, we evaluate the proposed approaches in the context of Wikipedia article edits, where interactions revolve around editing articles, and the norm in question is prohibiting vandalism. Lastly, we conduct ablation studies to compare the ensemble's performance against a single model approach and to examine the behavior of two data sampling techniques. Results indicate that the different machine learning frameworks can learn the meaning of a norm violation in a setting with data imbalance and concept drift, although with significant differences."
pub.1120986696,A vision-based deep on-device intelligent bus stop recognition system,"Intelligent public transportation systems are the cornerstone to any smart city, given the advancements made in the field of self-driving autonomous vehicles - particularly for autonomous buses, where it becomes really difficult to systematize a way to identify the arrival of a bus stop on-the-fly for the bus to appropriately halt and notify its passengers. This paper proposes an automatic and intelligent bus stop recognition system built on computer vision techniques, deployed on a low-cost single-board computing platform with minimal human supervision. The on-device recognition engine aims to extract the features of a bus stop and its surrounding environment, which eliminates the need for a conventional Global Positioning System (GPS) look-up, thereby alleviating network latency and accuracy issues. The dataset proposed in this paper consists of images of 11 different bus stops taken at different locations in Chennai, India during day and night. The core engine consists of a convolutional neural network (CNN) of size ~260 kB that is computationally lightweight for training and inference. In order to automatically scale and adapt to the dynamic landscape of bus stops over time, incremental learning (model updation) techniques were explored on-device from real-time incoming data points. Real-time incoming streams of images are unlabeled, hence suitable ground truthing strategies (like Active Learning), should help establish labels on-the-fly. Light-weight Bayesian Active Learning strategies using Bayesian Neural Networks using dropout (capable of representing model uncertainties) enable selection of the most informative images to query from an oracle. Intelligent rendering of the inference module by iteratively looking for better images on either sides of the bus stop environment propels the system towards human-like behavior. The proposed work can be integrated seamlessly into the widespread existing vision-based self-driving autonomous vehicles."
pub.1135427622,Does Cognitive Load Affect Eye Movements/Oculomotor Behavior in Natural Scenes?,"<p>Cognitive neuroscience researchers have identified relationships between cognitive load and eye movement behavior that are consistent with oculomotor biomarkers for neurological disorders. We develop an adaptive visual search paradigm that manipulates task difficulty and examine the effect of cognitive load on oculomotor behavior in healthy young adults. Participants (N=30) free-viewed a sequence of 100 natural scenes for 10 seconds each, while their eye movements were recorded. After each image, participants completed a 4 alternative forced choice task in which they selected a target object from the previously viewed scene, among 3 distracters of the same object type but from alternate scenes. Following two correct responses, the target object was selected from an image increasingly farther back (N-back) in the image stream; following an incorrect response, N decreased by 1. N-back thus quantifies and individualizes cognitive load. The results show that response latencies increased as N-back increased, and pupil diameter increased with N-back, before decreasing at very high N-back. These findings are consistent with previous studies and confirm that this paradigm was successful in actively engaging working memory, and successfully adapts task difficulty to individual subject’s skill levels. We hypothesized that oculomotor behavior would covary with cognitive load. However, there were no significant differences between the number or duration of fixations and saccades for high/low performing subjects, or between high/low performing trials for a given subject. Similarly, oculomotor behavior did not act as a predictor of correct/incorrect responses with increasing demand from the N-back task. Similarly, the proportion of each scene viewed was not related to N-back and was not a significant predictor of accuracy. These results suggest that cognitive load can be tracked with an adaptive visual search task, but that oculomotor strategies generally do not change as a result of greater cognitive demand in healthy adults.</p>"
pub.1156230293,Smart Facilities and 5G-Supported Systems in Social IoB,"5G is the fifth-generation mobile network that delivers high data speeds, large network capacity, low latency, and more reliability to users. Our phones can stream multiple movies simultaneously or download one in 4K high quality at speeds of about 10 gigabits per second thanks to 5G. Although 5G runs on the same radio frequencies used by your devices, its enormous potential has taken technology to the next level. By advancing societies, transforming industries, and elevating experiences, it is giving innovation a better shape. It takes connectivity to the next level by delivering cloud services to clients’ connected experience. With 5G as the new global standard network after 4G, there has been a significant change in the working and efficiency of many industries, the Internet of Behavior (IoB) being the most developed one among them. IoB is a network of interconnected, web-connected devices that can gather and send data across a wireless network without the need for human interaction. It’s a major advancement because it makes connected cities safer, asset tracking more affordable, healthcare more individualized, and energy usage more efficient. For its services to be more effective, it needs a dependable, inexpensive, and quick Internet. Therefore, IoB connectivity needs are served excellently with faster network and higher capacity. By increasing the capacity of cellular systems, the 5G spectrum widens the frequencies on which data is carried, enabling more devices to connect. IoB also enables businesses to optimize value propositions because 5G’s significant Internet bandwidth and speed allow for the connection and data exchange of physical assets’ sensors, software, and other technologies with other systems. Businesses that are 5G and IoB connected will not only spur creativity and improve efficiency across the board, they will also contribute to the fight against climate change by lowering carbon pollution. The potential of IoB and its numerous next-generation services has been increased by 5G. Advanced wireless modulation techniques, network slicing capabilities, automated network application lifecycle management, software-defined infrastructure, and network (SDN) virtualization are all being accelerated by 5G. By providing a huge number of IoB connections to traffic signals, cameras, and sensors, 5G will also enable improved traffic control. Smart meters will track energy use and contribute to consumption reduction. They are assisted by 5G low-cost IoB sensors and connectivity. In a way, 5G has enabled IoB and improved its working. IoB is a vast area to analyze and understand, and with 5G coming along, the area is expanding, and we need to deeply understand how 5G is boosting the growth of IoB. Therefore, in this chapter, we will discuss how 5G is empowering IoB and making it the greatest network in the world. 5G is the fifth-generation mobile network that delivers high data speeds, large network capacity, low latency, and more reliabilit"
pub.1145668638,FlexEdge: Dynamic Task Scheduling for a UAV-Based On-Demand Mobile Edge Server,"With the large number of cameras deployed in smart industrial parks and smart campuses, edge devices and location-fixed edge servers are deployed near to these cameras and help transmit video streams to data center for video analytics; however, location-fixed edge servers are difficult to adapt to computation-intensive and delay-sensitive video analytics tasks in hot spot, and it is also challenging to execute tasks in natural disasters in which the infrastructure is damaged. Moreover, task migration methods are used to balance the load of edge servers caused by irregular movement of detected objects, but it results in extra data transmission overhead. Therefore, unmanned aerial vehicles (UAVs) with computing and communication resources are widely used to optimize mobile edge video analysis; however, existing solutions formulate the UAV-based lowest latency and energy consumption by jointly optimizing the task allocation strategy and UAV location to be a multiobjective optimization problem, based on which the Pareto optimum solution set, including task allocation strategies and UAV locations, can find multiple solutions but not a unique solution. It makes the solution difficult to be applied in video analytics with the UAV hover location decision-making scheme and task allocation strategy. In this article, we propose a flexible cloud-edge collaborative scheduling strategy based on a UAV named FlexEdge. We first normalize values of execution time and energy consumption, and then convert the multiobjective optimization problem into a single-objective optimization problem by using the weighted sum of the two metrics as the optimization objective. We also proved the task allocation strategy based on execution time, energy consumption, and the UAV hover location decision-making scheme as an NP-hard problem. We propose a flexible and lightweight genetic algorithm (FGA) based on a polysomy-strengthening elitist genetic algorithm in FlexEdge to address the NP-hard problem. FlexEdge not only achieves optimal task allocation and UAV location to minimize the weighted sum of execution time and energy consumption but also provides computing resources and reliable network connection to reduce task offloading overload, which is validated by comprehensive performance evaluation."
pub.1031501982,Platform Sessions,"Monday July 9th, 2007
10:00 – 11:30
Room 209
Platform Session 1
Social Issues Shah U 1 , D'Souza C 2 , Shah P 1 , Saxena V 3 ( 1 Neurology Department, K.E.M. Hospital, Mumbai, India , 2 Indian Epilepsy Association, Mumbai, India , 3 Indian Epilepsy Association, Mumbai, India) Purpose: The Indian Epilepsy Association is lobbying for inclusion of chronic epilepsy under the Indian Disability Act. In several countries ‘intractability’ is a criterion for certification. A national survey was conducted to i) explore whether ‘intractability’ suffices to select the ‘disabled’; ii) determine appropriate criteria for disability certification. Method: Nine hundred and eight patients in 27 epilepsy clinics answered a questionnaire, reporting demographic, seizure and medication details and rated impact across various quality of life domains. Analysis involved: 1) dividing the sample on the basis of accepted intractability criteria and calculating the percentage of highly impacted people (disabling epilepsy); 2) dividing the sample on the basis of impact scores disability criteria and analysing differences; 3) eliciting predictor variables for impact. Results: 1) 153/908 (16.8%) were intractable, of whom 34/153 (22.2%) had high impact scores; of the 755 (83.2%) not intractable, 119/755 (15.8%) had high impact scores. 2) Significant differences (p < 0.05) emerged between the high and low impact group for education, occupation, marital status, seizure onset, duration, seizure control, medication and overall life satisfaction. 3) A significant regression model (adjusted R square = .207; F 11,580= 15.057, p < 0.0005) revealed low education (Beta =?.149, p < 0.0005), low work status (Beta =?.281, p < 0.0005), single marital status (Beta =?.200, p < 0.0005) and drug polytherapy (Beta =?173, p < 0.0005) to be significant predictor variables. Conclusion: Disability certification using intractability criteria alone may result in exclusion of people experiencing disabling epilepsy and inclusion of those not disabled. Disability rating should include demographic criteria in addition to seizure and medication criteria as these appear to be significant predictors of disabling epilepsy. Funded by the Indian Epilepsy Association 18th International Epilepsy Congress Trust and approved by the IEA‐IES Ethics Committee. Seshadri V 1 , Murthy J 1 , Ravi Raju C 2 ( 1 The Institute of Neurological Sciences, Care Hospital, Hyderabad, Andhra Pradesh, India , 2 The Institute of Neurological Sciences, Care Hospital, Hyderabad, Andhra Pradesh, India,) Purpose: In developing countries reasons for high treatment gap mainly are low purchasing capacity of the population and no access to appropriate medical care. Methods: All prevalence cases of epilepsy identified in 22 villages in a province in south India were registered in village medical‐center run by Byrraju Foundation, an organization working for rural transformation. Type of epilepsy and epilepsy syndrome was identified by the epilept"
pub.1013370252,Poster Session 211:00 a.m.‐7:30 p.m.Professionals in Epilepsy care,"Marlene A. Blackman*, Elaine Wirrell† and N. Thornton*
*Neurosciences, Alberta Children's Hospital, Calgary, AB, Canada and †Neurosciences, Mayo Clinic, Rochester, MN Rationale: To examine the impact of epilepsy on quality of life of children compared to nearest aged siblings, as well as to further investigate the impact of epilepsy thru the use of the HARCES and ICND scales. Methods: All children with epilepsy ages 3–17 whom had a non‐epileptic sibling in same age group seen thru the Neurology Clinic at the Alberta Children's Hospital were identified as potential participants for the study. Parents were asked to complete Global Quality of Life Linear scales for both children and Hague Restrictions in Epilepsy Scales and Impact of Child Neurologic Disability Scales for the child with epilepsy. Results: Fifty children with epilepsy (age range 3–17, gender M 25, F 25) and Fifty siblings who acted as the controls (age range 3–17 gender M 20 – F 30) participated in the study. Quality of life measurement using the Global Quality of Life Linear Scale was significantly different (p < 0.0001) betwen children with epilepsy (4.57 ± 0.92) and their siblings (5.30 ± 0.71). The HARCES found the average score of 18.89 Of the 49 children who completed this scale 3 (6%)scored 10 (no disability) 23 (47%) scored 11–15 (slight disability) 13 (26%) scored between 16–25,Mild 4 (8%) 26–30 moderate 6 (12%) 31–40 severe disability. On the ICNDS average was 40 with a range of (0–108) 14 scored than 20 (less impact), (28%) 20 scored 21–50 (40%) (moderate impact),with 16 scoring 50–108 (32%) (severe impact). Conclusions: Epilepsy has a negative impact on the quality of life of children, children in the same family with the same parents have statistically significant differnces in quality of life scores. Bryn M. Corbett, J. Gerke, Joseph I. Sirven, D. Shulman, G. Long, T. Pipe and M. Griffin 
 5W Neuro/ENT, Mayo Clinic Hospital, Arizona, Phoenix, AZ Rationale: Transforming Care at the Bedside (TCAB) is a nation‐wide research initiative, funded by the Robert Wood Johnson Foundation (RWJ) and supported by the Institute for Health Care Improvement. The overall aim of TCAB is to help hospital based nurses redesign delivery of work processes to improve patient care. Outcomes are assessed through comparison of baseline and ongoing measures including: work environment and culture; patient centeredness; number and types of innovations tested and sustained; and nursing variables that include, but are not limited to, adverse events (drug events, patient falls, and pressure ulcers) Patients admitted to the EMU are at greater risk for falls and injury than the general medical surgical patient population therefore any initiative to enhance an already comprehensive safety protocol for EMU patients is welcomed. Methods: 5W a medical‐surgical unit with an integrated EMU embraced the TCAB philosophy in June 2007. As just one TCAB innovation, RN Hourly Rounds specifically pertains to the iss"
