id,title,abstract
pub.1173111464,SPSC: Stream Processing Framework Atop Serverless Computing for Industrial Big Data,"With the advance of smart manufacturing and information technologies, the volume of data to process is increasing accordingly. Current solutions for big data processing resort to distributed stream processing systems, such as Apache Flink and Spark. However, such frameworks face challenges of resource underutilization and high latency in big data application scenarios. In this article, we propose SPSC, a serverless-based stream computing framework where events are discretized into the atomic stream and stateless Lambda functions are taken as context-irrelevant operators, achieving task parallelism and inherent data parallelism in processing. Also, we implement a prototype of the framework on Amazon Web service (AWS) using AWS Lambda, AWS simple queue service, and AWS DynamoDB. The evaluation shows that compared with Alibaba's real-time computing Flink version, SPSC outperforms by 10.12% when the overhead is close."
pub.1136296950,An efficient approach for low latency processing in stream data,"Stream data is the data that is generated continuously from the different data sources and ideally defined as the data that has no discrete beginning or end. Processing the stream data is a part of big data analytics that aims at querying the continuously arriving data and extracting meaningful information from the stream. Although earlier processing of such stream was using batch analytics, nowadays there are applications like the stock market, patient monitoring, and traffic analysis which can cause a drastic difference in processing, if the output is generated in levels of hours and minutes. The primary goal of any real-time stream processing system is to process the stream data as soon as it arrives. Correspondingly, analytics of the stream data also needs consideration of surrounding dependent data. For example, stock market analytics results are often useless if we do not consider their associated or dependent parameters which affect the result. In a real-world application, these dependent stream data usually arrive from the distributed environment. Hence, the stream processing system has to be designed, which can deal with the delay in the arrival of such data from distributed sources. We have designed the stream processing model which can deal with all the possible latency and provide an end-to-end low latency system. We have performed the stock market prediction by considering affecting parameters, such as USD, OIL Price, and Gold Price with an equal arrival rate. We have calculated the Normalized Root Mean Square Error (NRMSE) which simplifies the comparison among models with different scales. A comparative analysis of the experiment presented in the report shows a significant improvement in the result when considering the affecting parameters. In this work, we have used the statistical approach to forecast the probability of possible data latency arrives from distributed sources. Moreover, we have performed preprocessing of stream data to ensure at-least-once delivery semantics. In the direction towards providing low latency in processing, we have also implemented exactly-once processing semantics. Extensive experiments have been performed with varying sizes of the window and data arrival rate. We have concluded that system latency can be reduced when the window size is equal to the data arrival rate."
pub.1138967502,QoS-Aware Approximate Query Processing for Smart Cities Spatial Data Streams,"Large amounts of georeferenced data streams arrive daily to stream processing systems. This is attributable to the overabundance of affordable IoT devices. In addition, interested practitioners desire to exploit Internet of Things (IoT) data streams for strategic decision-making purposes. However, mobility data are highly skewed and their arrival rates fluctuate. This nature poses an extra challenge on data stream processing systems, which are required in order to achieve pre-specified latency and accuracy goals. In this paper, we propose ApproxSSPS, which is a system for approximate processing of geo-referenced mobility data, at scale with quality of service guarantees. We focus on stateful aggregations (e.g., means, counts) and top-N queries. ApproxSSPS features a controller that interactively learns the latency statistics and calculates proper sampling rates to meet latency or/and accuracy targets. An overarching trait of ApproxSSPS is its ability to strike a plausible balance between latency and accuracy targets. We evaluate ApproxSSPS on Apache Spark Structured Streaming with real mobility data. We also compared ApproxSSPS against a state-of-the-art online adaptive processing system. Our extensive experiments prove that ApproxSSPS can fulfill latency and accuracy targets with varying sets of parameter configurations and load intensities (i.e., transient peaks in data loads versus slow arriving streams). Moreover, our results show that ApproxSSPS outperforms the baseline counterpart by significant magnitudes. In short, ApproxSSPS is a novel spatial data stream processing system that can deliver real accurate results in a timely manner, by dynamically specifying the limits on data samples."
pub.1169165180,PaCHNOC: Packet and Circuit Hybrid Switching NoC for Real-Time Parallel Stream Signal Processing,"Real-time heterogeneous parallel embedded digital signal processor (DSP) systems process multiple data streams in parallel in a stringent time interval. This type of system on chip (SoC) requires the network on chip (NoC) to establish multiple symbiotic parallel data transmission paths with ultra-low transmission latency in real time. Our early NoC research PCCNOC meets this need. The PCCNOC uses packet routing to establish and lock a transmission circuit, so that PCCNOC is perfectly suitable for ultra-low latency and high-bandwidth transmission of long data packets. However, a parallel multi-data stream DSP system also needs to transmit roughly the same number of short data packets for job configuration and job execution status reports. While transferring short data packets, the link establishment routing delay of short data packets becomes relatively obvious. Our further research, thus, introduced PaCHNOC, a hybrid NoC in which long data packets are transmitted through a circuit established and locked by routing, and short data packets are attached to the routing packet and the transmission is completed during the routing process, thus avoiding the PCCNOC setup delay. Simulation shows that PaCHNOC performs well in supporting real-time heterogeneous parallel embedded DSP systems and achieves overall latency reduction 65% compared with related works. Finally, we used PaCHNOC in the baseband subsystem of a real 5G base station, which proved that our research is the best NoC for baseband subsystem of 5G base stations, which reduce 31% comprehensive latency in comparison to related works."
pub.1164181505,Decision-Change Informed Rejection Improves Robustness in Pattern Recognition-Based Myoelectric Control,"Post-processing techniques have been shown to improve the quality of the decision stream generated by classifiers used in pattern-recognition-based myoelectric control. However, these techniques have largely been tested individually and on well-behaved, stationary data, failing to fully evaluate their trade-offs between smoothing and latency during dynamic use. Correspondingly, in this work, we survey and compare 8 different post-processing and decision stream improvement schemes in the context of continuous and dynamic class transitions: majority vote, Bayesian fusion, onset locking, outlier detection, confidence-based rejection, confidence scaling, prior adjustment, and adaptive windowing. We then propose two new temporally aware post-processing schemes that use changes in the decision and confidence streams to better reject uncertain decisions. Our decision-change informed rejection (DCIR) approach outperforms existing schemes during both steady-state and transitions based on error rates and decision stream volatility whether using conventional or deep classifiers. These results suggest that added robustness can be gained by appropriately leveraging temporal context in myoelectric control."
pub.1150671415,Synchronization of ear-EEG and audio streams in a portable research hearing device,"Recent advancements in neuroscientific research and miniaturized ear-electroencephalography (EEG) technologies have led to the idea of employing brain signals as additional input to hearing aid algorithms. The information acquired through EEG could potentially be used to control the audio signal processing of the hearing aid or to monitor communication-related physiological factors. In previous work, we implemented a research platform to develop methods that utilize EEG in combination with a hearing device. The setup combines currently available mobile EEG hardware and the so-called Portable Hearing Laboratory (PHL), which can fully replicate a complete hearing aid. Audio and EEG data are synchronized using the Lab Streaming Layer (LSL) framework. In this study, we evaluated the setup in three scenarios focusing particularly on the alignment of audio and EEG data. In Scenario I, we measured the latency between software event markers and actual audio playback of the PHL. In Scenario II, we measured the latency between an analog input signal and the sampled data stream of the EEG system. In Scenario III, we measured the latency in the whole setup as it would be used in a real EEG experiment. The results of Scenario I showed a jitter (standard deviation of trial latencies) of below 0.1 ms. The jitter in Scenarios II and III was around 3 ms in both cases. The results suggest that the increased jitter compared to Scenario I can be attributed to the EEG system. Overall, the findings show that the measurement setup can time-accurately present acoustic stimuli while generating LSL data streams over multiple hours of playback. Further, the setup can capture the audio and EEG LSL streams with sufficient temporal accuracy to extract event-related potentials from EEG signals. We conclude that our setup is suitable for studying closed-loop EEG & audio applications for future hearing aids."
pub.1153708566,Event-based imaging polarimeter simulation with a single DoFP image.,"An event camera is a neuromorphic vision sensor with a high dynamic range (HDR), high temporal resolution (HTR), low latency, and low power consumption. A polarimeter is an instrument for measuring the state of polarization of light. Currently, most imaging polarimeters are limited in dynamic range and frame rate when used with frame-based cameras. We can establish an event-based imaging polarimeter using the principle of the event camera to obtain HDR and HTR polarized event streams for processing polarization information. However, because of the short history and high cost of event cameras, developing an event-based imaging polarimeter requires substantial resources. We propose an event-based imaging polarimeter simulation method with a single division of focal plane image based on the existing research on event simulation. This method can easily convert existing data into a polarized event stream. It is beneficial to lower the requirement of processing polarized event streams and to create large datasets for deep learning."
pub.1079206316,Stream Computing for Biomedical Signal Processing: A QRS Complex Detection Case-Study,"Recent developments in ""Big Data"" have brought significant gains in the ability to process large amounts of data on commodity server hardware. Stream computing is a relatively new paradigm in this area, addressing the need to process data in real time with very low latency. While this approach has been developed for dealing with large scale data from the world of business, security and finance, there is a natural overlap with clinical needs for physiological signal processing. In this work we present a case study of streams processing applied to a typical physiological signal processing problem: QRS detection from ECG data. "
pub.1163256786,BRAND: A platform for closed-loop experiments with deep network models,"Artificial neural networks (ANNs) are state-of-the-art tools for modeling and decoding neural activity, but deploying them in closed-loop experiments with tight timing constraints is challenging due to their limited support in existing real-time frameworks. Researchers need a platform that fully supports high-level languages for running ANNs (e.g., Python and Julia) while maintaining support for languages that are critical for low-latency data acquisition and processing (e.g., C and C++). To address these needs, we introduce the Backend for Realtime Asynchronous Neural Decoding (BRAND). BRAND comprises Linux processes, termed <i>nodes</i> , which communicate with each other in a <i>graph</i> via streams of data. Its asynchronous design allows for acquisition, control, and analysis to be executed in parallel on streams of data that may operate at different timescales. BRAND uses Redis to send data between nodes, which enables fast inter-process communication and supports 54 different programming languages. Thus, developers can easily deploy existing ANN models in BRAND with minimal implementation changes. In our tests, BRAND achieved &lt;600 microsecond latency between processes when sending large quantities of data (1024 channels of 30 kHz neural data in 1-millisecond chunks). BRAND runs a brain-computer interface with a recurrent neural network (RNN) decoder with less than 8 milliseconds of latency from neural data input to decoder prediction. In a real-world demonstration of the system, participant T11 in the BrainGate2 clinical trial performed a standard cursor control task, in which 30 kHz signal processing, RNN decoding, task control, and graphics were all executed in BRAND. This system also supports real-time inference with complex latent variable models like Latent Factor Analysis via Dynamical Systems. By providing a framework that is fast, modular, and language-agnostic, BRAND lowers the barriers to integrating the latest tools in neuroscience and machine learning into closed-loop experiments."
pub.1166840495,Publish/Subscribe Method for Real-Time Data Processing in Massive IoT Leveraging Blockchain for Secured Storage,"In the Internet of Things (IoT) era, the surge in Machine-Type Devices (MTDs) has introduced Massive IoT (MIoT), opening new horizons in the world of connected devices. However, such proliferation presents challenges, especially in storing and analyzing massive, heterogeneous data streams in real time. In order to manage Massive IoT data streams, we utilize analytical database software such as Apache Druid version 28.0.0 that excels in real-time data processing. Our approach relies on a publish/subscribe mechanism, where device-generated data are relayed to a dedicated broker, effectively functioning as a separate server. This broker enables any application to subscribe to the dataset, promoting a dynamic and responsive data ecosystem. At the core of our data transmission infrastructure lies Apache Kafka version 3.6.1, renowned for its exceptional data flow management performance. Kafka efficiently bridges the gap between MIoT sensors and brokers, enabling parallel clusters of brokers that lead to more scalability. In our pursuit of uninterrupted connectivity, we incorporate a fail-safe mechanism with two Software-Defined Radios (SDR) called Nutaq PicoLTE Release 1.5 within our model. This strategic redundancy enhances data transmission availability, safeguarding against connectivity disruptions. Furthermore, to enhance the data repository security, we utilize blockchain technology, specifically Hyperledger Fabric, known for its high-performance attributes, ensuring data integrity, immutability, and security. Our latency results demonstrate that our platform effectively reduces latency for 100,000 devices, qualifying as an MIoT, to less than 25 milliseconds. Furthermore, our findings on blockchain performance underscore our model as a secure platform, achieving over 800 Transactions Per Second in a dataset comprising 14,000 transactions, thereby demonstrating its high efficiency."
pub.1170505467,Intracranial Mapping of Response Latencies and Task Effects for Spoken Syllable Processing in the Human Brain,"Prior lesion, noninvasive-imaging, and intracranial-electroencephalography (iEEG) studies have documented hierarchical, parallel, and distributed characteristics of human speech processing. Yet, there have not been direct, intracranial observations of the latency with which regions <i>outside the temporal lobe</i> respond to speech, or how these responses are impacted by task demands. We leveraged human intracranial recordings via stereo-EEG to measure responses from diverse forebrain sites during (i) passive listening to /bi/ and /pi/ syllables, and (ii) active listening requiring /bi/-versus-/pi/ categorization. We find that neural response latency increases from a few tens of ms in Heschl's gyrus (HG) to several tens of ms in superior temporal gyrus (STG), superior temporal sulcus (STS), and early parietal areas, and hundreds of ms in later parietal areas, insula, frontal cortex, hippocampus, and amygdala. These data also suggest parallel flow of speech information dorsally and ventrally, from HG to parietal areas and from HG to STG and STS, respectively. Latency data also reveal areas in parietal cortex, frontal cortex, hippocampus, and amygdala that are not responsive to the stimuli during passive listening but are responsive during categorization. Furthermore, multiple regions-spanning auditory, parietal, frontal, and insular cortices, and hippocampus and amygdala-show greater neural response amplitudes during active versus passive listening (a task-related effect). Overall, these results are consistent with hierarchical processing of speech at a macro level and parallel streams of information flow in temporal and parietal regions. These data also reveal regions where the speech code is stimulus-faithful and those that encode task-relevant representations."
pub.1170482874,BRAND: a platform for closed-loop experiments with deep network models,"<i>Objective.</i>Artificial neural networks (ANNs) are state-of-the-art tools for modeling and decoding neural activity, but deploying them in closed-loop experiments with tight timing constraints is challenging due to their limited support in existing real-time frameworks. Researchers need a platform that fully supports high-level languages for running ANNs (e.g. Python and Julia) while maintaining support for languages that are critical for low-latency data acquisition and processing (e.g. C and C++).<i>Approach.</i>To address these needs, we introduce the Backend for Realtime Asynchronous Neural Decoding (BRAND). BRAND comprises Linux processes, termed<i>nodes</i>, which communicate with each other in a<i>graph</i>via streams of data. Its asynchronous design allows for acquisition, control, and analysis to be executed in parallel on streams of data that may operate at different timescales. BRAND uses Redis, an in-memory database, to send data between nodes, which enables fast inter-process communication and supports 54 different programming languages. Thus, developers can easily deploy existing ANN models in BRAND with minimal implementation changes.<i>Main results.</i>In our tests, BRAND achieved &lt;600 microsecond latency between processes when sending large quantities of data (1024 channels of 30 kHz neural data in 1 ms chunks). BRAND runs a brain-computer interface with a recurrent neural network (RNN) decoder with less than 8 ms of latency from neural data input to decoder prediction. In a real-world demonstration of the system, participant T11 in the BrainGate2 clinical trial (ClinicalTrials.gov Identifier: NCT00912041) performed a standard cursor control task, in which 30 kHz signal processing, RNN decoding, task control, and graphics were all executed in BRAND. This system also supports real-time inference with complex latent variable models like Latent Factor Analysis via Dynamical Systems.<i>Significance.</i>By providing a framework that is fast, modular, and language-agnostic, BRAND lowers the barriers to integrating the latest tools in neuroscience and machine learning into closed-loop experiments."
pub.1113526771,Efficient State Management for Scaling Out Stateful Operators in Stream Processing Systems,"Many big data applications require real-time analysis of continuous data streams. Stream Processing Systems (SPSs) are designed to act on real-time streaming data using continuous queries consisting of interconnected operators. The dynamic nature of data streams, for example, fluctuation in data arrival rates and uneven data distribution, can cause an operator to be a bottleneck one. Scalability is an important factor in SPS, but detecting bottleneck operator correctly and scaling it without affecting application execution are challenging. A stateful operator such as aggregation or join makes scaling operation more difficult as it involves state management. Current research does not address the issue of scaling stateful operators efficiently as mostly stop application for handling state, which results in significant overheads to the performance. In this article, the key idea is to detect bottleneck operator correctly using the runtime bottleneck detection approach and then scale out this operator and manage its internal state in a way that we can achieve almost zero latency. During the bottleneck detection process, we have defined alarming_threshold, a parameter for the operators that can be bottleneck operators in the future and scale_out_threshold, when the operator is bottleneck. To scale out, we have presented two techniques, active backup and checkpointing, the former one will start a Secondary Execution (SE) in back end by partitioning state and input streams to multiple nodes at alarming_threshold; this SE will replace primary node at scale_out_threshold. In the latter technique, a State Manager (SM) module will start state checkpointing at alarming_threshold to external store and perform scale out by managing state and input stream at scale_out_threshold. The first approach will help us to achieve almost zero latency goal, while the latter one is a resource efficient technique. Our results show that both techniques are working while providing desired goals of reducing overall latency during scale out and improving resource utilization."
pub.1160520399,Dual-wavelength off-axis digital holography in ImageJ: toward real-time phase retrieval using CUDA streams.,"An ImageJ plug-in is developed to realize automatic real-time phase reconstruction for dual-wavelength digital holography (DH). This plug-in assembles the algorithms, including automatic phase reconstruction based on the division algorithm and post-processing. These algorithms are implemented and analyzed using a CPU and GPU, respectively. To hide the CPU-to-GPU data transfer latency, an optimization scheme using Compute Unified Device Architecture (CUDA) streams is proposed in ImageJ. Experimental results show that the proposed plug-in can perform faster reconstruction for dual-wavelength DH, resulting in frame rates up to 48 fps even for one-megapixel digital holograms on a normal PC. In other words, the proposed plug-in can realize real-time phase reconstruction for dual-wavelength digital holographic videos."
pub.1146676133,LHC physics dataset for unsupervised New Physics detection at 40 MHz,"In the particle detectors at the Large Hadron Collider, hundreds of millions of proton-proton collisions are produced every second. If one could store the whole data stream produced in these collisions, tens of terabytes of data would be written to disk every second. The general-purpose experiments ATLAS and CMS reduce this overwhelming data volume to a sustainable level, by deciding in real-time whether each collision event should be kept for further analysis or be discarded. We introduce a dataset of proton collision events that emulates a typical data stream collected by such a real-time processing system, pre-filtered by requiring the presence of at least one electron or muon. This dataset could be used to develop novel event selection strategies and assess their sensitivity to new phenomena. In particular, we intend to stimulate a community-based effort towards the design of novel algorithms for performing unsupervised new physics detection, customized to fit the bandwidth, latency and computational resource constraints of the real-time event selection system of a typical particle detector."
pub.1047682650,Overlapping visual response latency distributions in visual cortices and LP-pulvinar complex of the cat,"The visual system of the cat is considered to be organized in both a serial and parallel manner. Studies of visual onset latencies generally suggest that parallel processing occurs throughout the dorsal stream. These studies are at odds with the proposed hierarchies of visual areas based on termination patterns of cortico–cortical projections. In previous studies, a variety of stimuli have been used to compute latencies, and this is problematic as latencies are known to depend on stimulus parameters. This could explain the discrepancy between latency and neuroanatomical based studies. Therefore, the first aim of the present study was to determine whether latencies increased along the hierarchy of visual areas when the same stimuli are used. In addition, the effect of stimulus complexity was assessed. Visual onset latencies were calculated for area 17, PMLS, AMLS, and AEV neurons. Latencies were also computed from neurons in the lateral posterior (LP)-pulvinar complex given the importance of this extrageniculate complex in cortical intercommunication. Latency distributions from all regions overlapped substantially, and no significant difference was present, regardless of the type of stimulus used. The onset latencies in the LP–pulvinar complex were comparable to those seen in cortical areas. The data suggest that the initial processing of information in the visual system is parallel, despite the presence of a neuroanatomical hierarchy. Simultaneous response onsets among cortical areas and the LP-pulvinar suggest that the latter is more than a simple relay station for information headed to cortex. The data are consistent with proposals of the LP-pulvinar as a center for the integration and distribution of information from/to multiple cortical areas."
pub.1077368633,Latency Reduction During Telemetry Transmission in Brain-Machine Interfaces,Advanced array processing techniques are becoming an indispensable requirement for integrating the rapid developments in wireless high-density electronic interfaces to the central nervous system with computational neuroscience. This work aims at describing a systems approach for latency reduction in telemetry-linked brain machine interfaces to enable real-time transmission of high volumes of neural data. We show that the tradeoff between transmission bit rate and processing complexity requires a smart processing mechanism to strip the redundancy and extract the useful information early in the data stream. The results presented demonstrate that space-time processing offers tremendous savings in communication costs compared to on-chip spike detection followed by off-chip classification. They also demonstrate that the performance asymptotically approaches that of on-chip spike detection and sorting. Detailed performance evaluation is described.
pub.1171330241,Unsupervised Characterization of Prediction Error Markers in Unisensory and Multisensory Streams Reveal the Spatiotemporal Hierarchy of Cortical Information Processing,"Elicited upon violation of regularity in stimulus presentation, mismatch negativity (MMN) reflects the brain's ability to perform automatic comparisons between consecutive stimuli and provides an electrophysiological index of sensory error detection whereas P300 is associated with cognitive processes such as updating of the working memory. To date, there has been extensive research on the roles of MMN and P300 individually, because of their potential to be used as clinical markers of consciousness and attention, respectively. Here, we intend to explore with an unsupervised and rigorous source estimation approach, the underlying cortical generators of MMN and P300, in the context of prediction error propagation along the hierarchies of brain information processing in healthy human participants. The existing methods of characterizing the two ERPs involve only approximate estimations of their amplitudes and latencies based on specific sensors of interest. Our objective is twofold: first, we introduce a novel data-driven unsupervised approach to compute latencies and amplitude of ERP components accurately on an individual-subject basis and reconfirm earlier findings. Second, we demonstrate that in multisensory environments, MMN generators seem to reflect a significant overlap of ""modality-specific"" and ""modality-independent"" information processing while P300 generators mark a shift toward completely ""modality-independent"" processing. Advancing earlier understanding that multisensory contexts speed up early sensory processing, our study reveals that temporal facilitation extends to even the later components of prediction error processing, using EEG experiments. Such knowledge can be of value to clinical research for characterizing the key developmental stages of lifespan aging, schizophrenia, and depression."
pub.1169078205,Adaptive Unsupervised Learning-Based 3D Spatiotemporal Filter for Event-Driven Cameras,"In the evolving landscape of robotics and visual navigation, event cameras have gained important traction, notably for their exceptional dynamic range, efficient power consumption, and low latency. Despite these advantages, conventional processing methods oversimplify the data into 2 dimensions, neglecting critical temporal information. To overcome this limitation, we propose a novel method that treats events as 3D time-discrete signals. Drawing inspiration from the intricate biological filtering systems inherent to the human visual apparatus, we have developed a 3D spatiotemporal filter based on unsupervised machine learning algorithm. This filter effectively reduces noise levels and performs data size reduction, with its parameters being dynamically adjusted based on population activity. This ensures adaptability and precision under various conditions, like changes in motion velocity and ambient lighting. In our novel validation approach, we first identify the noise type and determine its power spectral density in the event stream. We then apply a one-dimensional discrete fast Fourier transform to assess the filtered event data within the frequency domain, ensuring that the targeted noise frequencies are adequately reduced. Our research also delved into the impact of indoor lighting on event stream noise. Remarkably, our method led to a 37% decrease in the data point cloud, improving data quality in diverse outdoor settings."
pub.1134779435,Industry 4.0 towards Forestry 4.0: Fire Detection Use Case †,"Forestry 4.0 is inspired by the Industry 4.0 concept, which plays a vital role in the next industrial generation revolution. It is ushering in a new era for efficient and sustainable forest management. Environmental sustainability and climate change are related challenges to promote sustainable forest management of natural resources. Internet of Forest Things (IoFT) is an emerging technology that helps manage forest sustainability and protect forest from hazards via distributing smart devices for gathering data stream during monitoring and detecting fire. Stream processing is a well-known research area, and recently, it has gained a further significance due to the emergence of IoFT devices. Distributed stream processing platforms have emerged, e.g., Apache Flink, Storm, and Spark, etc. Querying windowing is the heart of any stream-processing platform which splits infinite data stream into chunks of finite data to execute a query. Dynamic query window-based processing can reduce the reporting time in case of missing and delayed events caused by data drift.In this paper, we present a novel dynamic mechanism to recommend the optimal window size and type based on the dynamic context of IoFT application. In particular, we designed a dynamic window selector for stream queries considering input stream data characteristics, application workload and resource constraints to recommend the optimal stream query window configuration. A research gap on the likelihood of adopting smart IoFT devices in environmental sustainability indicates a lack of empirical studies to pursue forest sustainability, i.e., sustainable forestry applications. So, we focus on forest fire management and detection as a use case of Forestry 4.0, one of the dynamic environmental management challenges, i.e., climate change, to deliver sustainable forestry goals. According to the dynamic window selector's experimental results, end-to-end latency time for the reported fire alerts has been reduced by dynamical adaptation of window size with IoFT stream rate changes."
pub.1172092720,Integrated photonic encoder for low power and high-speed image processing,"Modern lens designs are capable of resolving greater than 10 gigapixels, while advances in camera frame-rate and hyperspectral imaging have made data acquisition rates of Terapixel/second a real possibility. The main bottlenecks preventing such high data-rate systems are power consumption and data storage. In this work, we show that analog photonic encoders could address this challenge, enabling high-speed image compression using orders-of-magnitude lower power than digital electronics. Our approach relies on a silicon-photonics front-end to compress raw image data, foregoing energy-intensive image conditioning and reducing data storage requirements. The compression scheme uses a passive disordered photonic structure to perform kernel-type random projections of the raw image data with minimal power consumption and low latency. A back-end neural network can then reconstruct the original images with structural similarity exceeding 90%. This scheme has the potential to process data streams exceeding Terapixel/second using less than 100 fJ/pixel, providing a path to ultra-high-resolution data and image acquisition systems."
pub.1065110626,Parallel algorithms and architectures based on pipelined optical buses.,"Optical signals have some unique properties, such as unidirectional propagation and precisely predictable path delays in waveguides, which are not shared with their electronic counterparts. By taking advantage of these unique properties, we can use optical interconnections to achieve speed improvements in single-instruction stream, multiple-data streams (SIMD) computations. We first show how optical buses can be utilized advantageously in SIMD architectures to obtain fast solutions to several computational problems, including integer addition, counting and logical XOR, sorting, and fast Fourier transforms. We then present a new implementation of the optical buses to meet the unique requirements in highperformance optical-electronic computing systems. Such an implementation allows the transmission of messages at speeds ideal for optics and, in the meantime, the processing of data at speeds ideal for electronics, dealing successfully with the speed limitation by electronics in optical-electronic computers. The primary effects of this bimodal optical bus are twofold: reduction of fiber lengths and reduction of system latency. Reduced latency is a unique advantage to an optical bimodal bus. Together, these observations make optical-bus-based architectures appear to be a promising approach to SIMD processing."
pub.1061526680,A systems approach for data compression and latency reduction in cortically controlled brain machine interfaces,"This paper suggests a new approach for data compression during extracutaneous transmission of neural signals recorded by high-density microelectrode array in the cortex. The approach is based on exploiting the temporal and spatial characteristics of the neural recordings in order to strip the redundancy and infer the useful information early in the data stream. The proposed signal processing algorithms augment current filtering and amplification capability and may be a viable replacement to on chip spike detection and sorting currently employed to remedy the bandwidth limitations. Temporal processing is devised by exploiting the sparseness capabilities of the discrete wavelet transform, while spatial processing exploits the reduction in the number of physical channels through quasi-periodic eigendecomposition of the data covariance matrix. Our results demonstrate that substantial improvements are obtained in terms of lower transmission bandwidth, reduced latency and optimized processor utilization. We also demonstrate the improvements qualitatively in terms of superior denoising capabilities and higher fidelity of the obtained signals."
pub.1141128453,Hybrid FPGA-CPU pupil tracker.,"An off-axis monocular pupil tracker designed for eventual integration in ophthalmoscopes for eye movement stabilization is described and demonstrated. The instrument consists of light-emitting diodes, a camera, a field-programmable gate array (FPGA) and a central processing unit (CPU). The raw camera image undergoes background subtraction, field-flattening, 1-dimensional low-pass filtering, thresholding and robust pupil edge detection on an FPGA pixel stream, followed by least-squares fitting of the pupil edge pixel coordinates to an ellipse in the CPU. Experimental data suggest that the proposed algorithms require raw images with a minimum of ∼32 gray levels to achieve sub-pixel pupil center accuracy. Tests with two different cameras operating at 575, 1250 and 5400 frames per second trained on a model pupil achieved 0.5-1.5 μm pupil center estimation precision with 0.6-2.1 ms combined image download, FPGA and CPU processing latency. Pupil tracking data from a fixating human subject show that the tracker operation only requires the adjustment of a single parameter, namely an image intensity threshold. The latency of the proposed pupil tracker is limited by camera download time (latency) and sensitivity (precision)."
pub.1059323172,Wireless Image Streaming in Mobile Ultrasound,"This work evaluates the feasibility of using 802.11 g ad hoc and 3G cellular broadband networks to wirelessly stream ultrasound video in real-time. Telemedicine ultrasound applications in events such as disaster relief and first-response triage can incorporate these technologies, enabling onsite medical personnel to receive assistance with diagnostic decisions by remote medical experts. The H.264 scalable video codec was used to encode echocardiographic video streams at various image resolutions (video graphics array [VGA] and quarter video graphics array [QVGA]) and frame rates (10, 15, 20, and 30 frames/s). The video stream was transmitted using 802.11 g and 3G cellular technologies, and pertinent transmission parameters such as data rate, packet loss, delay jitter, and latency were measured. 802.11 g permits high frame rate and VGA resolution and has low latency and jitter, but it is suitable only for short communication ranges, whereas the 3G cellular network allows medium to low frame rate streaming at QVGA image resolution with medium latency. However, video streaming can take place from any location with 3G service to any other site with Internet connectivity. The transmitted ultrasound video streams were subsequently recorded and evaluated by physicians with expertise in medical ultrasonography who evaluated the diagnostic value of the received video streams relative to the original videos. They expressed the opinion that image quality in the case of both 802.11 g and 3G was fully to adequately preserved, but missed frames could momentarily decrease the diagnostic value. This research demonstrates that 3G and 802.11 g wireless networks combined with efficient video compression make diagnostically valuable wireless streaming of ultrasound video feasible."
pub.1084497983,An Initial Investigation Into the Real-Time Conversion of Facial Surface EMG Signals to Audible Speech,"This paper presents early-stage results of our investigations into the direct conversion of facial surface electromyographic (EMG) signals into audible speech in a real-time setting, enabling novel avenues for research and system improvement through real-time feedback. The system uses a pipeline approach to enable online acquisition of EMG data, extraction of EMG features, mapping of EMG features to audio features, synthesis of audio waveforms from audio features and output of the audio waveforms via speakers or headphones. Our system allows for performing EMG-to-Speech conversion with low latency and on a continuous stream of EMG data, enabling near instantaneous audio output during audible as well as silent speech production. In this paper, we present an analysis of our systems components for latency incurred, as well as the tradeoffs between conversion quality, latency and training duration required."
pub.1048422737,Flow of activation from V1 to frontal cortex in humans,"Abstract. This study provides a time frame for the initial trajectory of activation flow along the dorsal and ventral visual processing streams and for the initial activation of prefrontal cortex in the human. We provide evidence that this widespread system of sensory, parietal, and prefrontal areas is activated in less than 30 ms, which is considerably shorter than typically assumed in the human event-related potential (ERP) literature and is consistent with recent intracranial data from macaques. We find a mean onset latency of activity over occipital cortex (C1e) at 56 ms, with dorsolateral frontal cortex subsequently active by just 80 ms. Given that activity in visual sensory areas typically continues for 100–400 ms prior to motor output, this rapid system-wide activation provides a time frame for the initiation of feedback processes onto sensory areas. There is clearly sufficient time for multiple iterations of interactive processing between sensory, parietal, and frontal areas during brief (e.g., 200 ms) periods of information processing preceding motor output. High-density electrical mapping also suggested activation in dorsal stream areas preceding ventral stream areas. Our data suggest that multiple visual generators are active in the latency range of the traditional C1 component of the ERP, which has often been taken to represent V1 activity alone. Based on the temporal pattern of activation shown in primate recordings and the evidence from these human recordings, we propose that only the initial portion of the C1 component (approximately the first 10–15 ms; C1e) is likely to represent a response that is predominated by V1 activity. These data strongly suggest that activity represented in the ""early"" ERP components such as P1 and N1 (and possibly even C1) is likely to reflect relatively late processing, after the initial volley of sensory afference through the visual system and involving top-down influences from parietal and frontal regions."
pub.1126577157,Mining Massive E-Health Data Streams for IoMT Enabled Healthcare Systems,"With the increasing popularity of the Internet-of-Medical-Things (IoMT) and smart devices, huge volumes of data streams have been generated. This study aims to address the concept drift, which is a major challenge in the processing of voluminous data streams. Concept drift refers to overtime change in data distribution. It may occur in the medical domain, for example the medical sensors measuring for general healthcare or rehabilitation, which may switch their roles for ICU emergency operations when required. Detecting concept drifts becomes trickier when the class distributions in data are skewed, which is often true for medical sensors e-health data. Reactive Drift Detection Method (RDDM) is an efficient method for detecting long concepts. However, RDDM has a high error rate, and it does not handle class imbalance. We propose an Enhanced Reactive Drift Detection Method (ERDDM), which systematically generates strategies to handle concept drift with class imbalance in data streams. We conducted experiments to compare ERDDM with three contemporary techniques in terms of prediction error, drift detection delay, latency, and ability to handle data imbalance. The experimentation was done in Massive Online Analysis (MOA) on 48 synthetic datasets customized to possess the capabilities of data streams. ERDDM can handle abrupt and gradual drifts and performs better than all benchmarks in almost all experiments."
pub.1047525103,Cortical visual processing is temporally dispersed by luminance in human subjects,"Increasing the intensity of a stimulus such as luminance results in faster processing of the signal and therefore decreases simple motor reaction time (RT). We studied the latencies of visual evoked potentials (VEPs, N80, P100, N130) and RTs in eight subjects to flashing spots of light while varying the luminance of the spots from 1 to 1000 cd/m2. The data show that processing time as a function of intensity is modified not only at the retina but also at later processing sites. This indicates a temporal dispersion of the visual signal over the whole processing stream from visual input all the way to motor output."
pub.1173776793,A High-Performance Pixel-Level Fully Pipelined Hardware Accelerator for Neural Networks,"The design of convolutional neural network (CNN) hardware accelerators based on a single computing engine (CE) architecture or multi-CE architecture has received widespread attention in recent years. Although this kind of hardware accelerator has advantages in hardware platform deployment flexibility and development cycle, it is still limited in resource utilization and data throughput. When processing large feature maps, the speed can usually only reach 10 frames/s, which does not meet the requirements of application scenarios, such as autonomous driving and radar detection. To solve the above problems, this article proposes a full pipeline hardware accelerator design based on pixel. By pixel-by-pixel strategy, the concept of the layer is downplayed, and the generation method of each pixel of the output feature map (Ofmap) can be optimized. To pipeline the entire computing system, we expand each layer of the neural network into hardware, eliminating the buffers between layers and maximizing the effect of complete connectivity across the entire network. This approach has yielded excellent performance. Besides that, as the pixel data stream is a fundamental paradigm in image processing, our fully pipelined hardware accelerator is universal for various CNNs (MobileNetV1, MobileNetV2 and FashionNet) in computer vision. As an example, the accelerator for MobileNetV1 achieves a speed of 4205.50 frames/s and a throughput of 4787.15 GOP/s at 211 MHz, with an output latency of 0.60 ms per image. This extremely shorts processing time and opens the door for AI's application in high-speed scenarios."
pub.1127533675,Explora: Interactive Querying of Multidimensional Data in the Context of Smart Cities,"Citizen engagement is one of the key factors for smart city initiatives to remain sustainable over time. This in turn entails providing citizens and other relevant stakeholders with the latest data and tools that enable them to derive insights that add value to their day-to-day life. The massive volume of data being constantly produced in these smart city environments makes satisfying this requirement particularly challenging. This paper introduces Explora, a generic framework for serving interactive low-latency requests, typical of visual exploratory applications on spatiotemporal data, which leverages the stream processing for deriving-on <i>ingestion time</i>-synopsis data structures that concisely capture the spatial and temporal trends and dynamics of the sensed variables and serve as compacted data sets to provide fast (approximate) answers to visual queries on smart city data. The experimental evaluation conducted on proof-of-concept implementations of Explora, based on traditional database and distributed data processing setups, accounts for a decrease of up to 2 orders of magnitude in query latency compared to queries running on the base raw data at the expense of less than 10% query accuracy and 30% data footprint. The implementation of the framework on real smart city data along with the obtained experimental results prove the feasibility of the proposed approach."
pub.1140776987,Preparatory delta phase response is correlated with naturalistic speech comprehension performance,"While human speech comprehension is thought to be an active process that involves top-down predictions, it remains unclear how predictive information is used to prepare for the processing of upcoming speech information. We aimed to identify the neural signatures of the preparatory processing of upcoming speech. Participants selectively attended to one of two competing naturalistic, narrative speech streams, and a temporal response function (TRF) method was applied to derive event-related-like neural responses from electroencephalographic data. The phase responses to the attended speech at the delta band (1–4 Hz) were correlated with the comprehension performance of individual participants, with a latency of − 200–0 ms relative to the onset of speech amplitude envelope fluctuations over the fronto-central and left-lateralized parietal electrodes. The phase responses to the attended speech at the alpha band also correlated with comprehension performance but with a latency of 650–980 ms post-onset over the fronto-central electrodes. Distinct neural signatures were found for the attentional modulation, taking the form of TRF-based amplitude responses at a latency of 240–320 ms post-onset over the left-lateralized fronto-central and occipital electrodes. Our findings reveal how the brain gets prepared to process an upcoming speech in a continuous, naturalistic speech context."
pub.1173304042,Enhancing security in smart healthcare systems: Using intelligent edge computing with a novel Salp Swarm Optimization and radial basis neural network algorithm,"A smart healthcare system (SHS) is a health service system that employs advanced technologies such as wearable devices, the Internet of Things (IoT), and mobile internet to dynamically access information and connect people and institutions related to healthcare, thereby actively managing and responding to medical ecosystem needs. Edge computing (EC) plays a significant role in SHS as it enables real-time data processing and analysis at the data source, which reduces latency and improves medical intervention speed. However, the integration of patient information, including electronic health records (EHRs), into the SHS framework induces security and privacy concerns. To address these issues, an intelligent EC framework was proposed in this study. The objective of this study is to accurately identify security threats and ensure secure data transmission in the SHS environment. The proposed EC framework leverages the effectiveness of Salp Swarm Optimization and Radial Basis Functional Neural Network (SS-RBFN) for enhancing security and data privacy. The proposed methodology commences with the collection of healthcare information, which is then pre-processed to ensure the consistency and quality of the database for further analysis. Subsequently, the SS-RBFN algorithm was trained using the pre-processed database to distinguish between normal and malicious data streams accurately, offering continuous monitoring in the SHS environment. Additionally, a Rivest-Shamir-Adelman (RSA) approach was applied to safeguard data against security threats during transmission to cloud storage. The proposed model was trained and validated using the IoT-based healthcare database available at Kaggle, and the experimental results demonstrated that it achieved 99.87 % accuracy, 99.76 % precision, 99.49 % f-measure, 98.99 % recall, 97.37 % throughput, and 1.2s latency. Furthermore, the results achieved by the proposed model were compared with the existing models to validate its effectiveness in enhancing security."
pub.1110811717,Adaptation and spectral enhancement at auditory temporal perceptual boundaries - Measurements via temporal precision of auditory brainstem responses,"In human and animal auditory perception the perceived quality of sound streams changes depending on the duration of inter-sound intervals (ISIs). Here, we studied whether adaptation and the precision of temporal coding in the auditory periphery reproduce general perceptual boundaries in the time domain near 20, 100, and 400 ms ISIs, the physiological origin of which are unknown. In four experiments, we recorded auditory brainstem responses with five wave peaks (P1 -P5) in response to acoustic models of communication calls of house mice, who perceived these calls with the mentioned boundaries. The newly introduced measure of average standard deviations of wave latencies of individual animals indicate the waves' temporal precision (latency jitter) mostly in the range of 30-100 μs, very similar to latency jitter of single neurons. Adaptation effects of response latencies and latency jitter were measured for ISIs of 10-1000 ms. Adaptation decreased with increasing ISI duration following exponential or linear (on a logarithmic scale) functions in the range of up to about 200 ms ISIs. Adaptation effects were specific for each processing level in the auditory system. The perceptual boundaries near 20-30 and 100 ms ISIs were reflected in significant adaptation of latencies together with increases of latency jitter at P2-P5 for ISIs < ~30 ms and at P5 for ISIs < ~100 ms, respectively. Adaptation effects occurred when frequencies in a sound stream were within the same critical band. Ongoing low-frequency components/formants in a sound enhanced (decrease of latencies) coding of high-frequency components/formants when the frequencies concerned different critical bands. The results are discussed in the context of coding multi-harmonic sounds and stop-consonants-vowel pairs in the auditory brainstem. Furthermore, latency data at P1 (cochlea level) offer a reasonable value for the base-to-apex cochlear travel time in the mouse (0.342 ms) that has not been determined experimentally."
pub.1120655607,ReconSocket: a low-latency raw data streaming interface for real-time MRI-guided radiotherapy,"With the recent advent of hybrid MRI-guided radiotherapy systems, continuous intra-fraction MR imaging for motion monitoring has become feasible. The ability to perform real-time custom image reconstructions is however often lacking. In this work we present a low-latency streaming solution, ReconSocket, which provides a real-time stream of k-space data from the magnetic resonance imaging (MRI) to custom reconstruction servers. We determined the performance of the data streaming by measuring the streaming latency (i.e. non-zero time delay due to data transfer and processing) and jitter (i.e. deviations from periodicity) using an ultra-fast 1D MRI acquisition of a moving phantom. Simultaneously, its position was recorded with near-zero time delay. The feasibility of low-latency custom reconstructions was tested by measuring the imaging latency (i.e. time delay between physical change and appearance of that change on the image) for several non-Cartesian 2D and 3D acquisitions using an in-house implemented reconstruction server. The measured streaming latency of the ReconSocket interface was [Formula: see text] ms. 98% of the incoming data packets arrived within a jitter range of 367 [Formula: see text]s. This shows that the ReconSocket interface can provide reliable real-time access to MRI data, acquired during the course of a MRI-guided radiotherapy fraction. The total imaging latency was measured to be 221 ms (2D) and 3889 ms (3D) for exemplary acquisitions, using the custom image reconstruction server. These imaging latencies are approximately equal to half of the temporal footprint (T <sub>acq</sub> /2) of the respective 2D and 3D golden-angle radial sequences. For radial sequences, it was previously showed that T <sub>acq</sub> /2 is the expected contribution of only the data acquisition to the total imaging latency. Indeed, the contribution of the non-Cartesian reconstruction to the total imaging latency was minor (&lt;10%): 21 ms for 2D, 300 ms for 3D, indicating that the acquisition, i.e. the physical encoding of the image itself is the major contributor to the total imaging latency."
pub.1141135689,An Extended Modular Processing Pipeline for Event-Based Vision in Automatic Visual Inspection,"Dynamic Vision Sensors differ from conventional cameras in that only intensity changes of individual pixels are perceived and transmitted as an asynchronous stream instead of an entire frame. The technology promises, among other things, high temporal resolution and low latencies and data rates. While such sensors currently enjoy much scientific attention, there are only little publications on practical applications. One field of application that has hardly been considered so far, yet potentially fits well with the sensor principle due to its special properties, is automatic visual inspection. In this paper, we evaluate current state-of-the-art processing algorithms in this new application domain. We further propose an algorithmic approach for the identification of ideal time windows within an event stream for object classification. For the evaluation of our method, we acquire two novel datasets that contain typical visual inspection scenarios, i.e., the inspection of objects on a conveyor belt and during free fall. The success of our algorithmic extension for data processing is demonstrated on the basis of these new datasets by showing that classification accuracy of current algorithms is highly increased. By making our new datasets publicly available, we intend to stimulate further research on application of Dynamic Vision Sensors in machine vision applications."
pub.1083280501,Signal Timing Across the Macaque Visual System,"The onset latencies of single-unit responses evoked by flashing visual stimuli were measured in the parvocellular (P) and magnocellular (M) layers of the dorsal lateral geniculate nucleus (LGNd) and in cortical visual areas V1, V2, V3, V4, middle temporal area (MT), medial superior temporal area (MST), and in the frontal eye field (FEF) in individual anesthetized monkeys. Identical procedures were carried out to assess latencies in each area, often in the same monkey, thereby permitting direct comparisons of timing across areas. This study presents the visual flash-evoked latencies for cells in areas where such data are common (V1 and V2), and are therefore a good standard, and also in areas where such data are sparse (LGNd M and P layers, MT, V4) or entirely lacking (V3, MST, and FEF in anesthetized preparation). Visual-evoked onset latencies were, on average, 17 ms shorter in the LGNd M layers than in the LGNd P layers. Visual responses occurred in V1 before any other cortical area. The next wave of activation occurred concurrently in areas V3, MT, MST, and FEF. Visual response latencies in areas V2 and V4 were progressively later and more broadly distributed. These differences in the time course of activation across the dorsal and ventral streams provide important temporal constraints on theories of visual processing."
pub.1142413353,Hybrid-based framework for COVID-19 prediction via federated machine learning models,"The COronaVIrus Disease 2019 (COVID-19) pandemic is unfortunately highly transmissible across the people. In order to detect and track the suspected COVID-19 infected people and consequently limit the pandemic spread, this paper entails a framework integrating the machine learning (ML), cloud, fog, and Internet of Things (IoT) technologies to propose a novel smart COVID-19 disease monitoring and prognosis system. The proposal leverages the IoT devices that collect streaming data from both medical (e.g., X-ray machine, lung ultrasound machine, etc.) and non-medical (e.g., bracelet, smartwatch, etc.) devices. Moreover, the proposed hybrid fog-cloud framework provides two kinds of federated ML as a service (federated MLaaS); (i) the distributed batch MLaaS that is implemented on the cloud environment for a long-term decision-making, and (ii) the distributed stream MLaaS, which is installed into a hybrid fog-cloud environment for a short-term decision-making. The stream MLaaS uses a shared federated prediction model stored into the cloud, whereas the real-time symptom data processing and COVID-19 prediction are done into the fog. The federated ML models are determined after evaluating a set of both batch and stream ML algorithms from the Python’s libraries. The evaluation considers both the quantitative (i.e., performance in terms of accuracy, precision, root mean squared error, and F1 score) and qualitative (i.e., quality of service in terms of server latency, response time, and network latency) metrics to assess these algorithms. This evaluation shows that the stream ML algorithms have the potential to be integrated into the COVID-19 prognosis allowing the early predictions of the suspected COVID-19 cases."
pub.1156178669,A key role of the hippocampal P3 in the attentional blink,"The attentional blink (AB) refers to an impaired identification of target stimuli (T2), which are presented shortly after a prior target (T1) within a rapid serial visual presentation (RSVP) stream. It has been suggested that the AB is related to a failed transfer of T2 into working memory and that hippocampus (HC) and entorhinal cortex (EC) are regions crucial for this transfer. Since the event-related P3 component has been linked to inhibitory processes, we hypothesized that the hippocampal P3 elicited by T1 may impact on T2 processing within HC and EC. To test this hypothesis, we reanalyzed microwire data from 21 patients, who performed an RSVP task, during intracranial recordings for epilepsy surgery assessment (Reber et al., 2017). We identified T1-related hippocampal P3 components in the local field potentials (LFPs) and determined the temporal onset of T2 processing in HC/EC based on single-unit response onset activity. In accordance with our hypothesis, T1-related single-trial P3 amplitudes at the onset of T2 processing were clearly larger for unseen compared to seen T2-stimuli. Moreover, increased T1-related single-trial P3 peak latencies were found for T2[unseen] versus T2[seen] trials in case of lags 1 to 3, which was in line with our predictions. In conclusion, our findings support inhibition models of the AB and indicate that the hippocampal P3 elicited by T1 plays a central role in the AB."
pub.1003310115,Target consolidation under high temporal processing demands as revealed by MEG,"We investigated the nature of resource limitations during visual target processing by imposing high temporal processing demands on the cognitive system. This was achieved by embedding target stimuli into rapid-serial-visual-presentation-streams (RSVP). In RSVP streams, it is difficult to report the second of two targets (T2) if the second follows the first (T1) within 500 ms. This effect is known as the attentional blink (AB). For the AB to occur, it is essential that T1 is followed by a mask, as without such a stimulus, the AB is significantly attenuated. Usually, it is thought that T1 processing is delayed by the mask, which in turn delays T2 processing, increasing the likelihood for T2 failures (AB). Predictions regarding amplitudes and latencies of cortical responses (M300, the magnetic counterpart to the P300) to targets were tested by investigating the neurophysiological effects of the post-T1 item (mask) by means of magnetoencephalography (MEG). Cortical M300 responses to targets drawn from prefrontal sources--areas associated with working memory--revealed accelerated T1 yet delayed T2 processing with an intervening mask. The explanation we are proposing assumes that ""protection"" of ongoing T1 processing necessitated by the occurrence of the mask suppresses other activation patterns, which boosts T1 yet also hinders further processing. Our data shed light on the mechanisms employed by the human brain for ensuring visual target processing under high temporal processing demands, which is hypothesized to occur at the expense of subsequently presented information."
pub.1143754991,Wireless Multi-sensor Physio-Motion Measurement and Synchronization System and Method for HRI Research,"There is a strong demand for acquisition, processing and understanding of a variety of physiological and behavioral signals from the measurements in human-robot interface (HRI). However, multiple data streams from these measurements bring considerable challenges for their synchronizations, either for offline analysis or for online HRI applications, especially when the sensors are wirelessly connected, without synchronization mechanisms, such as a network-time-protocol. In this paper, we presented a full wireless multi-modality sensor system comprising biopotential measurements such as EEG, EMG and inertial parameter data of articulated body-limb motions. In the paper, we propose two methods to synchronize and calibrate the transmission latencies from different wireless channels. The first method employs the traditional artificial electrical timing signal. The other one employs the force-acceleration relationship governed by Newton's Second Law to facilitate reconstruction of the sample-to-sample alignment between the two wireless sensors. The measured latencies are investigated and the result show that they could be determined consistently and accurately by the devised techniques."
pub.1152351357,A High-Performance and Flexible Architecture for Accelerating SDN on the MPSoC Platform,"Software-defined networking has been developing in recent years and the separation of the control plane and the data plane has made networks more flexible. Due to its flexibility, the software method is used to implement the data plane. However, with increasing network speed, the CPU is becoming unable to meet the requirements of high-speed packet processing. FPGAs are usually used as dumb switches to accelerate the data plane, with all intelligence centralized in the remote controller. However, the cost of taking the intelligence out of the switch is the increased latency between the controller and the switch. Therefore, we argue that the control decisions should be made as locally as possible. In this paper, we propose a novel architecture with high performance and flexibility for accelerating SDN based on the MPSoC platform. The control plane is implemented in the on-chip CPU and the data plane is implemented in the FPGA logic. The communication between the two components is performed using Ethernet communication. We design a high-performance TCAM based on distributed RAM. The architecture employs a pipeline design with modules connected via the AXI Stream interface. The designed architecture is flexible enough to support multiple network functions while achieving high performance at 100 Gbps. As far as we know, the architecture is the first proposed in the design of a 100 Gbps system."
pub.1015231113,Does a warning signal accelerate the processing of sensory information? Evidence from recognition potential responses to high and low frequency words,"Electrophysiological and behavioral data were obtained in 12 subjects who detected valid words in a background stream of random letter strings. Behavioral reaction time (RT) showed significant effects of warning signal presentation and the frequency of word usage in printed literature. Cross-correlation functions were used to estimate delays of electrophysiological responses. The critical response was the recognition potential (RP). The RP is a response of the brain that occurs when a person views recognizable images, such as words, pictures, or faces. Its latency is usually less than 300 ms. Both the RP and longer latency activity occurring at approximately 400--600 ms were delayed more for low than for high frequency words. The longer latency responses showed shorter delay if a warning signal was presented, but the RP did not. The results supported the idea that a non-informative warning signal decreases RT by altering response-related processes without facilitating sensory processes."
pub.1133123586,Motion-Aware Interplay between WiGig and WiFi for Wireless Virtual Reality,"Wireless virtual reality (VR) is a promising direction for future VR systems that offloads heavy computation to a remote processing entity and wirelessly receives high-quality streams. WiGig and WiFi are representative solutions to implement wireless VR; however, they differ in communication bandwidth and reliability. Our testbed experiments show that the performance of WiGig and VR traffic generation strongly correlates with and consequently can be predicted from a user's motion. Based on this observation, we develop a wireless VR system that exploits the benefits of both links by switching between them and controlling the VR frame encoding for latency regulation and image quality enhancement. The proposed system predicts the performance of the links and selects the one with a higher capacity in an opportunistic manner. It adjusts the encoding rate of the host based on the motion-aware prediction of the frame size and estimated latency of the selected link. By evaluating the testbed data, we demonstrate that the proposed system outperforms a WiGig-only system with a fixed encoding rate in terms of latency regulation and image quality."
pub.1049862376,A Potential VEP Biomarker for Mild Cognitive Impairment: Evidence from Selective Visual Deficit of Higher-Level Dorsal Pathway,"Visual dysfunctions are common in Alzheimer's disease (AD). Our aim was to establish a neurophysiological biomarker for amnestic mild cognitive impairment (aMCI). Visual evoked potentials (VEPs) were recorded in aMCI patients who later developed AD (n = 15) and in healthy older (n = 15) and younger controls (n = 15). Visual stimuli were optimized to separately activate lower and higher levels of the ventral and dorsal streams. We compared VEP parameters across the three groups of participants and conducted a linear correlation analysis between VEPs and data from neuropsychological tests. We then used a receiver operating characteristic (ROC) analysis to discriminate those with aMCI from those who were healthy older adults. The latency and phase of VEPs to lower-level stimuli (chromatic and achromatic gratings) were significantly affected by age but not by cognitive decline. Conversely, VEP latencies for higher-ventral (faces and kanji-words) and dorsal (kana-words and optic flow motion) stimuli were not affected by age, but they were significantly prolonged in aMCI patients. Interestingly, VEPs for higher-dorsal stimuli were related to outcomes of neuropsychological tests. Furthermore, the ROC analysis showed that the highest areas under the curve were obtained for VEP latencies in response to higher-dorsal stimuli. These results suggest aMCI-related functional impairment specific to higher-level visual processing. Further, dysfunction in the higher-level of the dorsal stream could be an early indicator of cognitive decline. Therefore, we conclude that VEPs associated with higher-level dorsal stream activity can be a sensitive biomarker for early detection of aMCI."
pub.1061812562,Data Streaming in Telepresence Environments,"In this paper, we discuss data transmission in telepresence environments for collaborative virtual reality applications. We analyze data streams in the context of networked virtual environments and classify them according to their traffic characteristics. Special emphasis is put on geometry-enhanced (3D) video. We review architectures for real-time 3D video pipelines and derive theoretical bounds on the minimal system latency as a function of the transmission and processing delays. Furthermore, we discuss bandwidth issues of differential update coding for 3D video. In our telepresence system-the blue-c-we use a point-based 3D video technology which allows for differentially encoded 3D representations of human users. While we discuss the considerations which lead to the design of our three-stage 3D video pipeline, we also elucidate some critical implementation details regarding decoupling of acquisition, processing and rendering frame rates, and audio/video synchronization. Finally, we demonstrate the communication and networking features of the blue-c system in its full deployment. We show how the system can possibly be controlled to face processing or networking bottlenecks by adapting the multiple system components like audio, application data, and 3D video."
pub.1092446898,Cooperative cortical network for categorical processing of Chinese lexical tone,"In tonal languages such as Chinese, lexical tone with varying pitch contours serves as a key feature to provide contrast in word meaning. Similar to phoneme processing, behavioral studies have suggested that Chinese tone is categorically perceived. However, its underlying neural mechanism remains poorly understood. By conducting cortical surface recordings in surgical patients, we revealed a cooperative cortical network along with its dynamics responsible for this categorical perception. Based on an oddball paradigm, we found amplified neural dissimilarity between cross-category tone pairs, rather than between within-category tone pairs, over cortical sites covering both the ventral and dorsal streams of speech processing. The bilateral superior temporal gyrus (STG) and the middle temporal gyrus (MTG) exhibited increased response latencies and enlarged neural dissimilarity, suggesting a ventral hierarchy that gradually differentiates the acoustic features of lexical tones. In addition, the bilateral motor cortices were also found to be involved in categorical processing, interacting with both the STG and the MTG and exhibiting a response latency in between. Moreover, the motor cortex received enhanced Granger causal influence from the semantic hub, the anterior temporal lobe, in the right hemisphere. These unique data suggest that there exists a distributed cooperative cortical network supporting the categorical processing of lexical tone in tonal language speakers, not only encompassing a bilateral temporal hierarchy that is shared by categorical processing of phonemes but also involving intensive speech-motor interactions over the right hemisphere, which might be the unique machinery responsible for the reliable discrimination of tone identities."
pub.1156588928,Head-Mounted Miniature Motorized Camera and Laser Pointer Driven by Eye Movements,"Recording a video scene as seen by an observer, materializing where is focused his visual attention and allowing an external person to point at a given object in this scene, could be beneficial for various applications such as medical education or remote training. Such a versatile device, although tested at the experimental laboratory demonstrator stage, has never been integrated in a compact and portable way in a real environment. In this context, we built a low-cost, light-weight, head-mounted device integrating a miniature camera and a laser pointer that can be remotely controlled or servo-controlled by an eye tracker. Two motorizations were implemented and tested (pan/tilt and Rilsey-prisms-based). The video was both recorded locally and transmitted wirelessly. Risley prisms allowed finer remote control of camera or laser pointer orientation (0.1° vs. 0.35°), but data processing and Wi-Fi transmission incur significant latency (~0.5 s) limiting the servo-controlling by eye movements. The laser beam was spatially shaped by a Diffractive Optical Element to facilitate object illumination or recognition. With this first proof-of-concept prototype, the data stream needs to be optimized to make full use of the eye tracker, but this versatile device can find various applications in education, healthcare or research."
pub.1138805685,Recurrent neural network FPGA hardware accelerator for delay-tolerant indoor optical wireless communications.,"The optical wireless communication (OWC) system has been widely studied as a promising solution for high-speed indoor applications. The transmitter diversity scheme has been proposed to improve the performance of high-speed OWC systems. However, the transmitter diversity is vulnerable to the delay of multiple channels. Recently neural networks have been studied to realize delay-tolerant indoor OWC systems, where long-short term memory (LSTM) and attention-augmented LSTM (ALSTM) recurrent neural networks (RNNs) have shown their capabilities. However, they have high computation complexity and long computation latency. In this paper, we propose a low complexity delay-tolerant RNN scheme for indoor OWC systems. In particular, an RNN with parallelized structure is proposed to reduce the computation cost. The proposed RNN schemes show comparable capability to the more complicated ALSTM, where a bit-error-rate (BER) performance within the forward-error-correction (FEC) limit is achieved for up to 5.5 symbol periods delays. In addition, previously studied LSTM/ALSTM schemes are implemented using high-end GPUs, which have high cost, high power consumption, and long processing latency. To solve these practical limitations, in this paper we further propose and demonstrate the FPGA-based RNN hardware accelerator for delay-tolerant indoor OWC systems. To optimize the processing latency and power consumption, we also propose two optimization methods: the parallel implementation with triple-phase clocking and the stream-in based computation with additive input data insertion. Results show that the FPGA-based RNN hardware accelerator with the proposed optimization methods achieves 96.75% effective latency reduction and 90.7% lower energy consumption per symbol compared with the FPGA-based RNN hardware accelerator without optimization. Compared to the GPU implementation, the latency is reduced by about 61% and the power consumption is reduced by about 58.1%."
pub.1169633131,Attention to audiovisual speech shapes neural processing through feedback-feedforward loops between different nodes of the speech network,"Selective attention-related top-down modulation plays a significant role in separating relevant speech from irrelevant background speech when vocal attributes separating concurrent speakers are small and continuously evolving. Electrophysiological studies have shown that such top-down modulation enhances neural tracking of attended speech. Yet, the specific cortical regions involved remain unclear due to the limited spatial resolution of most electrophysiological techniques. To overcome such limitations, we collected both electroencephalography (EEG) (high temporal resolution) and functional magnetic resonance imaging (fMRI) (high spatial resolution), while human participants selectively attended to speakers in audiovisual scenes containing overlapping cocktail party speech. To utilise the advantages of the respective techniques, we analysed neural tracking of speech using the EEG data and performed representational dissimilarity-based EEG-fMRI fusion. We observed that attention enhanced neural tracking and modulated EEG correlates throughout the latencies studied. Further, attention-related enhancement of neural tracking fluctuated in predictable temporal profiles. We discuss how such temporal dynamics could arise from a combination of interactions between attention and prediction as well as plastic properties of the auditory cortex. EEG-fMRI fusion revealed attention-related iterative feedforward-feedback loops between hierarchically organised nodes of the ventral auditory object related processing stream. Our findings support models where attention facilitates dynamic neural changes in the auditory cortex, ultimately aiding discrimination of relevant sounds from irrelevant ones while conserving neural resources."
pub.1129770822,RDMA data transfer and GPU acceleration methods for high‐throughput online processing of serial crystallography images,"The continual evolution of photon sources and high-performance detectors drives cutting-edge experiments that can produce very high throughput data streams and generate large data volumes that are challenging to manage and store. In these cases, efficient data transfer and processing architectures that allow online image correction, data reduction or compression become fundamental. This work investigates different technical options and methods for data placement from the detector head to the processing computing infrastructure, taking into account the particularities of modern modular high-performance detectors. In order to compare realistic figures, the future ESRF beamline dedicated to macromolecular X-ray crystallography, EBSL8, is taken as an example, which will use a PSI JUNGFRAU 4M detector generating up to 16 GB of data per second, operating continuously during several minutes. Although such an experiment seems possible at the target speed with the 100 Gb s<sup>-1</sup> network cards that are currently available, the simulations generated highlight some potential bottlenecks when using a traditional software stack. An evaluation of solutions is presented that implements remote direct memory access (RDMA) over converged ethernet techniques. A synchronization mechanism is proposed between a RDMA network interface card (RNIC) and a graphics processing unit (GPU) accelerator in charge of the online data processing. The placement of the detector images onto the GPU is made to overlap with the computation carried out, potentially hiding the transfer latencies. As a proof of concept, a detector simulator and a backend GPU receiver with a rejection and compression algorithm suitable for a synchrotron serial crystallography (SSX) experiment are developed. It is concluded that the available transfer throughput from the RNIC to the GPU accelerator is at present the major bottleneck in online processing for SSX experiments."
pub.1107707479,Towards a Cascading Reasoning Framework to Support Responsive Ambient-Intelligent Healthcare Interventions,"In hospitals and smart nursing homes, ambient-intelligent care rooms are equipped with many sensors. They can monitor environmental and body parameters, and detect wearable devices of patients and nurses. Hence, they continuously produce data streams. This offers the opportunity to collect, integrate and interpret this data in a context-aware manner, with a focus on reactivity and autonomy. However, doing this in real time on huge data streams is a challenging task. In this context, cascading reasoning is an emerging research approach that exploits the trade-off between reasoning complexity and data velocity by constructing a processing hierarchy of reasoners. Therefore, a cascading reasoning framework is proposed in this paper. A generic architecture is presented allowing to create a pipeline of reasoning components hosted locally, in the edge of the network, and in the cloud. The architecture is implemented on a pervasive health use case, where medically diagnosed patients are constantly monitored, and alarming situations can be detected and reacted upon in a context-aware manner. A performance evaluation shows that the total system latency is mostly lower than 5 s, allowing for responsive intervention by a nurse in alarming situations. Using the evaluation results, the benefits of cascading reasoning for healthcare are analyzed."
pub.1013154458,Low latency breathing frequency detection and monitoring on a personal computer,"We demonstrate a low latency respiratory/breathing frequency detection system that is fast (<5 ms), easy to operate, requires no batteries or external power supply and operates fully via computer-standard USB connection. Exercises in controlling ones breathing frequency, usually referred to as paced-breathing exercises, have shown positive effects in treating pulmonary diseases, cardiovascular diseases and stress/anxiety-related disorders. We developed a breathing frequency detection system which uses two pairs of microphones to detect exhalation activity, eliminate noise from the environment and stream the recording data via USB connection to a personal computer. It showed 97.1% reliability (10 subjects) when monitoring breathing activity in non-guided free breathing and 100% reliability (10 subjects) when monitoring breathing activity during interactive paced-breathing exercises. We also evaluated the breathing frequency detection systems noise elimination functionality which showed a reduction of 84.2 dB for stationary (white noise) and a reduction of 79.3 dB for non-stationary (hands clapping) noise."
pub.1044800096,"The effects of parts, wholes, and familiarity on face-selective responses in MEG","Although face perception is commonly characterized as holistic, as opposed to part-based, we have recently shown that both face parts and wholes are represented in ""face-selective"" cortical regions, with greater adaptation of holistic representations for familiar faces (A. Harris & G. K. Aguirre, 2008). Here we investigate the time course of these holistic and part-based face processing effects using magnetoencephalography (MEG). We examined ""face-selective"" components at early (approximately 170-200 ms) and later (approximately 250-450 ms) latencies in occipitotemporal sensors. While both ""M170"" and ""M400"" components showed significantly larger responses for familiar versus unfamiliar faces, neither exhibited a main effect of holistic versus part-based processing. These data affirm the existence of part-based ""face-selective"" representations, and additionally demonstrate that such representations are present from relatively early stages of face processing. However, only the later M400 component showed a modulatory effect of familiarity similar to that previously seen with fMRI, with a larger response to familiar faces in the holistic condition. Likewise, behavioral recognition was significantly correlated with the M400, not the M170, and only in the holistic condition. Together, these data suggest that, while face parts are represented from the earliest stages of face perception, modulatory effects of familiarity occur later in the face processing stream."
pub.1014376008,Longitudinal study of preterm and full-term infants: High-density EEG analyses of cortical activity in response to visual motion,"Electroencephalogram (EEG) was used to investigate brain electrical activity of full-term and preterm infants at 4 and 12 months of age as a functional response mechanism to structured optic flow and random visual motion. EEG data were recorded with an array of 128-channel sensors. Visual evoked potentials (VEPs) and temporal spectral evolution (TSE, time-dependent amplitude changes) were analysed. VEP results showed a significant improvement in full-term infants' latencies with age for forwards and reversed optic flow but not random visual motion. Full-term infants at 12 months significantly differentiated between the motion conditions, with the shortest latency observed for forwards optic flow and the longest latency for random visual motion, while preterm infants did not improve their latencies with age, nor were they able to differentiate between the motion conditions at 12 months. Differences in induced activities were also observed where comparisons between TSEs of the motion conditions and a static non-flow pattern showed desynchronised theta-band activity in both full-term and preterm infants, with synchronised alpha-beta band activity observed only in the full-term infants at 12 months. Full-term infants at 12 months with a substantial amount of self-produced locomotor experience and neural maturation coupled with faster oscillating cell assemblies, rely on the perception of structured optic flow to move around efficiently in the environment. The poorer responses in the preterm infants could be related to impairment of the dorsal visual stream specialized in the processing of visual motion. "
pub.1140181915,Oculomotor target selection is mediated by complex objects,"Oculomotor target selection often requires discriminating visual features, but it remains unclear how oculomotor substrates encoding saccade vectors functionally contribute to this process. One possibility is that oculomotor vector representations (observed directly as physiological activation or inferred from behavioral interference) of potential targets are continuously reweighted by task relevance computed elsewhere in specialized visual modules, whereas an alternative possibility is that oculomotor modules use local featural analyses to actively discriminate potential targets. Strengthening the former account, oculomotor vector representations have longer onset latencies for ventral- (i.e., color) than dorsal-stream features (i.e., luminance), suggesting that oculomotor vector representations originate from featurally relevant specialized visual modules. Here, we extended this reasoning by behaviorally examining whether the onset latency of saccadic interference elicited by visually complex stimuli is greater than is commonly observed for simple stimuli. We measured human saccade metrics (saccade curvature, endpoint deviations, saccade frequency, and error proportion) as a function of time after abrupt distractor onset. Distractors were novel, visually complex, and had to be discriminated from targets to guide saccades. The earliest saccadic interference latency was ∼110 ms, considerably longer than previous experiments, suggesting that sensory representations projected into the oculomotor system are gated to allow for sufficient featural processing to satisfy task demands. Surprisingly, initial oculomotor vector representations encoded features, as we manipulated the visual similarity between targets and distractors and observed increased vector modulation response magnitude and duration when the distractor was highly similar to the target. Oculomotor vector modulation was gradually extinguished over the time course of the experiment.<b>NEW &amp; NOTEWORTHY</b> We challenge the role of the oculomotor system in discriminating features during saccadic target selection. Our data suggest that the onset latency of oculomotor vector representations is scaled by task difficulty and featural complexity, suggesting that featural computations are performed outside of the oculomotor system, which receives the output of these computations only after sufficient visual and cognitive processing. We also challenge the convention that initial oculomotor vector representations are feature invariant, as they encoded task relevance."
pub.1041410296,The BOLD correlates of the visual P1 and N1 in single-trial analysis of simultaneous EEG-fMRI recordings during a spatial detection task,"Simultaneous EEG-fMRI measurements can combine the high spatial resolution of fMRI with the high temporal resolution of EEG. Therefore, we applied this approach to the study of peripheral vision. More specifically, we presented visual field quadrant fragments of checkerboards and a full central checkerboard in a simple detection task. A technique called ""integration-by-prediction"" was used to integrate EEG and fMRI data. In particular, we used vectors of single-trial ERP amplitude differences between left and right occipital electrodes as regressors in an ERP-informed fMRI analysis. The amplitude differences for the regressors were measured at the latencies of the visual P1 and N1 components. Our results indicated that the traditional event-related fMRI analysis revealed mostly activations in the vicinity of the primary visual cortex and in the ventral visual stream, while both P1 and N1 regressors revealed activation of areas in the temporo-parietal junction. We conclude that simultaneous EEG-fMRI in a spatial detection task can separate visual processing at 100-200 ms from stimulus onset from the rest of the information processing in the brain."
pub.1027346494,"AmsterdamASTRON radio transient facility and analysis centre: towards a 24 7, all-sky monitor for the low-frequency array (LOFAR)","The Amsterdam-ASTRON Radio Transient Facility And Analysis Centre (AARTFAAC) project aims to implement an all-sky monitor (ASM), using the low-frequency array (LOFAR) telescope. It will enable real-time, 24 × 7 monitoring for low-frequency radio transients over most of the sky locally visible to the LOFAR at time scales ranging from seconds to several days, and rapid triggering of follow-up observations with the full LOFAR on detection of potential transient candidates. These requirements pose several implementation challenges: imaging of an all-sky field of view, low latencies of processing, continuous availability and autonomous operation of the ASM. The first of these has already resulted in the correlator for the ASM being the largest in the world in terms of the number of input data streams. We have carried out test observations using existing LOFAR infrastructure, in order to quantify and constrain crucial instrumental design criteria for the ASM. In this study, we present an overview of the AARTFAAC data-processing pipeline and illustrate some of the aforementioned challenges by showing all-sky images obtained from one of the test observations. These results provide quantitative estimates of the capabilities of the instrument."
pub.1061812808,A Streaming-Based Solution for Remote Visualization of 3D Graphics on Mobile Devices,"Mobile devices such as Personal Digital Assistants, Tablet PCs, and cellular phones have greatly enhanced user capability to connect to remote resources. Although a large set of applications are now available bridging the gap between desktop and mobile devices, visualization of complex 3D models is still a task hard to accomplish without specialized hardware. This paper proposes a system where a cluster of PCs, equipped with accelerated graphics cards managed by the Chromium software, is able to handle remote visualization sessions based on MPEG video streaming involving complex 3D models. The proposed framework allows mobile devices such as smart phones, Personal Digital Assistants (PDAs), and Tablet PCs to visualize objects consisting of millions of textured polygons and voxels at a frame rate of 30 fps or more depending on hardware resources at the server side and on multimedia capabilities at the client side. The server is able to concurrently manage multiple clients computing a video stream for each one; resolution and quality of each stream is tailored according to screen resolution and bandwidth of the client. The paper investigates in depth issues related to latency time, bit rate and quality of the generated stream, screen resolutions, as well as frames per second displayed."
pub.1014887957,The cortical representation of the speech envelope is earlier for audiovisual speech than audio speech,"Visual speech can greatly enhance a listener's comprehension of auditory speech when they are presented simultaneously. Efforts to determine the neural underpinnings of this phenomenon have been hampered by the limited temporal resolution of hemodynamic imaging and the fact that EEG and magnetoencephalographic data are usually analyzed in response to simple, discrete stimuli. Recent research has shown that neuronal activity in human auditory cortex tracks the envelope of natural speech. Here, we exploit this finding by estimating a linear forward-mapping between the speech envelope and EEG data and show that the latency at which the envelope of natural speech is represented in cortex is shortened by >10 ms when continuous audiovisual speech is presented compared with audio-only speech. In addition, we use a reverse-mapping approach to reconstruct an estimate of the speech stimulus from the EEG data and, by comparing the bimodal estimate with the sum of the unimodal estimates, find no evidence of any nonlinear additive effects in the audiovisual speech condition. These findings point to an underlying mechanism that could account for enhanced comprehension during audiovisual speech. Specifically, we hypothesize that low-level acoustic features that are temporally coherent with the preceding visual stream may be synthesized into a speech object at an earlier latency, which may provide an extended period of low-level processing before extraction of semantic information. "
pub.1003528502,Electrophysiological correlates of auditory change detection and change deafness in complex auditory scenes,"Change deafness describes the failure to perceive even intense changes within complex auditory input, if the listener does not attend to the changing sound. Remarkably, previous psychophysical data provide evidence that this effect occurs independently of successful stimulus encoding, indicating that undetected changes are processed to some extent in auditory cortex. Here we investigated cortical representations of detected and undetected auditory changes using electroencephalographic (EEG) recordings and a change deafness paradigm. We applied a one-shot change detection task, in which participants listened successively to three complex auditory scenes, each of them consisting of six simultaneously presented auditory streams. Listeners had to decide whether all scenes were identical or whether the pitch of one stream was changed between the last two presentations. Our data show significantly increased middle-latency Nb responses for both detected and undetected changes as compared to no-change trials. In contrast, only successfully detected changes were associated with a later mismatch response in auditory cortex, followed by increased N2, P3a and P3b responses, originating from hierarchically higher non-sensory brain regions. These results strengthen the view that undetected changes are successfully encoded at sensory level in auditory cortex, but fail to trigger later change-related cortical responses that lead to conscious perception of change."
pub.1017646441,Emotional facial expression processing in depression: Data from behavioral and event-related potential studies,"Behavioral literature investigating emotional processes in depressive populations (i.e., unipolar and bipolar depression) states that, compared to healthy controls, depressive subjects exhibit disrupted emotional processing, indexed by lower performance and/or delayed response latencies. The development of brain imaging techniques, such as functional magnetic resonance imaging (fMRI), provided the possibility to visualize the brain regions engaged in emotional processes and how they fail to interact in psychiatric diseases. However, fMRI suffers from poor temporal resolution and cognitive function involves various steps and cognitive stages (serially or in parallel) to give rise to a normal performance. Thus, the origin of a behavioral deficit may result from the alteration of a cognitive stage differently situated along the information-processing stream, outlining the importance of access to this dynamic ""temporal"" information. In this paper, we will illustrate, through depression, the role that should be attributed to cognitive event-related potentials (ERPs). Indeed, owing to their optimal temporal resolution, ERPs can monitor the neural processes engaged in disrupted cognitive function and provide crucial information for its treatment, training of the impaired cognitive functions and guidelines for clinicians in the choice and monitoring of appropriate medication for the patient. "
pub.1052568008,Attention selectively modulates cortical entrainment in different regions of the speech spectrum,"Recent studies have uncovered a neural response that appears to track the envelope of speech, and have shown that this tracking process is mediated by attention. It has been argued that this tracking reflects a process of phase-locking to the fluctuations of stimulus energy, ensuring that this energy arrives during periods of high neuronal excitability. Because all acoustic stimuli are decomposed into spectral channels at the cochlea, and this spectral decomposition is maintained along the ascending auditory pathway and into auditory cortex, we hypothesized that the overall stimulus envelope is not as relevant to cortical processing as the individual frequency channels; attention may be mediating envelope tracking differentially across these spectral channels. To test this we reanalyzed data reported by Horton et al. (2013), where high-density EEG was recorded while adults attended to one of two competing naturalistic speech streams. In order to simulate cochlear filtering, the stimuli were passed through a gammatone filterbank, and temporal envelopes were extracted at each filter output. Following Horton et al. (2013), the attended and unattended envelopes were cross-correlated with the EEG, and local maxima were extracted at three different latency ranges corresponding to distinct peaks in the cross-correlation function (N1, P2, and N2). We found that the ratio between the attended and unattended cross-correlation functions varied across frequency channels in the N1 latency range, consistent with the hypothesis that attention differentially modulates envelope-tracking activity across spectral channels."
pub.1003458616,Dysfunction of the magnocellular stream in Alzheimer's disease evaluated by pattern electroretinograms and visual evoked potentials,"BACKGROUND: Visuo-spatial disturbances could represent a clinical feature of early stage Alzheimer's disease (AD). The magnocellular (M) pathway has anatomo-physiological characteristic which make it more suitable for detecting form, motion and depth compared with parvocellular one (P).
OBJECTIVE: Aim of our study was to evaluate specific visual subsystem involvement in a group of AD patients, recording isoluminant chromatic and luminance pattern electroretinograms and pattern visual evoked potentials.
MATERIAL AND METHODS: data were obtained from 15 AD patients (9 females and 6 males, mean age+/-1SD: 77.6+/-4.01 years) not yet undergoing any treatment, and from 10 age-matched healthy controls. Diagnosis of probable AD was clinically and neuroradiologically established. PERGs were recorded monocularly in response to equiluminant red-green (R-G), blue-yellow (B-Y) and luminance yellow-black (Y-Bk) horizontal square gratings of 0.3c/deg and 90% contrast, reversed at 1Hz. VEPs were recorded in response to full-field (14 deg) equiluminant chromatic R-G, B-Y and luminance Y-Bk sinusoidal gratings of 2c/deg, presented in onset (300ms)-offset (700ms) mode, at the contrast levels of 90%.
RESULTS: All data were retrieved in terms of peak-amplitude and latency and assessed using the Student's t-test for paired data. Temporal differences of PERGs and VEPs, evoked by Y-Bk grating in AD patients compared with controls, suggest a specific impairment of the magnocellular stream.
CONCLUSIONS: Our study support the hypothesis that the impairment of the PERGs and VEPs arising from the magnocellular streams of visual processing may indicate a primary dysfunction of the M-pathways in AD."
pub.1037494347,Lateralized auditory spatial perception and the contralaterality of cortical processing as studied with functional magnetic resonance imaging and magnetoencephalography,"Functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) were used to study the relationships between lateralized auditory perception in humans and the contralaterality of processing in auditory cortex. Subjects listened to rapidly presented streams of short FM-sweep tone bursts to detect infrequent, slightly deviant tone bursts. The stimulus streams consisted of either monaural stimuli to one ear or the other or binaural stimuli with brief interaural onset delays. The onset delay gives the binaural sounds a lateralized auditory perception and is thought to be a key component of how our brains localize sounds in space. For the monaural stimuli, fMRI revealed a clear contralaterality in auditory cortex, with a contralaterality index (contralateral activity divided by the sum of contralateral and ipsilateral activity) of 67%. In contrast, the fMRI activations from the laterally perceived binaural stimuli indicated little or no contralaterality (index of 51%). The MEG recordings from the same subjects performing the same task converged qualitatively with the fMRI data, confirming a clear monaural contralaterality, with no contralaterality for the laterally perceived binaurals. However, the MEG monaural contralaterality (55%) was less than the fMRI and decreased across the several hundred millisecond poststimulus time period, going from 57% in the M50 latency range (20-70 ms) to 53% in the M200 range (170-250 ms). These data sets provide both quantification of the degree of contralaterality in the auditory pathways and insight into the locus and mechanism of the lateralized perception of spatially lateralized sounds."
pub.1065190162,"Low-power, transparent optical network interface for high bandwidth off-chip interconnects.","The recent emergence of multicore architectures and chip multiprocessors (CMPs) has accelerated the bandwidth requirements in high-performance processors for both on-chip and off-chip interconnects. For next generation computing clusters, the delivery of scalable power efficient off-chip communications to each compute node has emerged as a key bottleneck to realizing the full computational performance of these systems. The power dissipation is dominated by the off-chip interface and the necessity to drive high-speed signals over long distances. We present a scalable photonic network interface approach that fully exploits the bandwidth capacity offered by optical interconnects while offering significant power savings over traditional E/O and O/E approaches. The power-efficient interface optically aggregates electronic serial data streams into a multiple WDM channel packet structure at time-of-flight latencies. We demonstrate a scalable optical network interface with 70% improvement in power efficiency for a complete end-to-end PCI Express data transfer."
pub.1012135611,Auditory ERPs to Stimulus Deviance in an Awake Chimpanzee (Pan troglodytes): Towards Hominid Cognitive Neurosciences,"BACKGROUND: For decades, the chimpanzee, phylogenetically closest to humans, has been analyzed intensively in comparative cognitive studies. Other than the accumulation of behavioral data, the neural basis for cognitive processing in the chimpanzee remains to be clarified. To increase our knowledge on the evolutionary and neural basis of human cognition, comparative neurophysiological studies exploring endogenous neural activities in the awake state are needed. However, to date, such studies have rarely been reported in non-human hominid species, due to the practical difficulties in conducting non-invasive measurements on awake individuals.
METHODOLOGY/PRINCIPAL FINDINGS: We measured auditory event-related potentials (ERPs) of a fully awake chimpanzee, with reference to a well-documented component of human studies, namely mismatch negativity (MMN). In response to infrequent, deviant tones that were delivered in a uniform sound stream, a comparable ERP component could be detected as negative deflections in early latencies.
CONCLUSIONS/SIGNIFICANCE: The present study reports the MMN-like component in a chimpanzee for the first time. In human studies, various ERP components, including MMN, are well-documented indicators of cognitive and neural processing. The results of the present study validate the use of non-invasive ERP measurements for studies on cognitive and neural processing in chimpanzees, and open the way for future studies comparing endogenous neural activities between humans and chimpanzees. This signifies an essential step in hominid cognitive neurosciences."
pub.1092070766,Towards Edge-Aware Spatio-Temporal Filtering in Real-Time,"Spatio-temporal edge-aware (STEA) filtering methods have recently received increased attention due to their ability to efficiently solve or approximate important image-domain problems in a temporally consistent manner - which is a crucial property for video-processing applications. However, existing STEA methods are currently unsuited for real-time, embedded stream-processing settings due to their high processing latency, large memory, and bandwidth requirements, and the need for accurate optical flow to enable filtering along motion paths. To this end, we propose an efficient STEA filtering pipeline based on the recently proposed permeability filter (PF), which offers high quality and halo reduction capabilities. Using mathematical properties of the PF, we reformulate its temporal extension as a causal, non-linear infinite impulse response filter, which can be efficiently evaluated due to its incremental nature. We bootstrap our own accurate flow using the PF and its temporal extension by interpolating a quasi-dense nearest neighbour field obtained with an improved PatchMatch algorithm, which employs binarized octal orientation maps (BOOM) descriptors to find correspondences among subsequent frames. Our method is able to create temporally consistent results for a variety of applications such as optical flow estimation, sparse data upsampling, visual saliency computation and disparity estimation. We benchmark our optical flow estimation on the MPI Sintel dataset, where we currently achieve a Pareto optimal quality-efficiency tradeoff with an average endpoint error of 7.68 at 0.59 s single-core execution time on a recent desktop machine."
pub.1010828257,Comparison of Neural Responses to Cat Meows and Human Vowels in the Anterior and Posterior Auditory Field of Awake Cats,"For humans and animals, the ability to discriminate speech and conspecific vocalizations is an important physiological assignment of the auditory system. To reveal the underlying neural mechanism, many electrophysiological studies have investigated the neural responses of the auditory cortex to conspecific vocalizations in monkeys. The data suggest that vocalizations may be hierarchically processed along an anterior/ventral stream from the primary auditory cortex (A1) to the ventral prefrontal cortex. To date, the organization of vocalization processing has not been well investigated in the auditory cortex of other mammals. In this study, we examined the spike activities of single neurons in two early auditory cortical regions with different anteroposterior locations: anterior auditory field (AAF) and posterior auditory field (PAF) in awake cats, as the animals were passively listening to forward and backward conspecific calls (meows) and human vowels. We found that the neural response patterns in PAF were more complex and had longer latency than those in AAF. The selectivity for different vocalizations based on the mean firing rate was low in both AAF and PAF, and not significantly different between them; however, more vocalization information was transmitted when the temporal response profiles were considered, and the maximum transmitted information by PAF neurons was higher than that by AAF neurons. Discrimination accuracy based on the activities of an ensemble of PAF neurons was also better than that of AAF neurons. Our results suggest that AAF and PAF are similar with regard to which vocalizations they represent but differ in the way they represent these vocalizations, and there may be a complex processing stream between them."
pub.1018795270,Just Swap Out of Negative Vibes? Rumination and Inhibition Deficits in Major Depressive Disorder: Data from Event-Related Potentials Studies,"Major depression is a serious disorder of impaired emotion regulation. Emotion hyperactivity leads to excessive negative ruminations that daily hijack the patient's mental life, impacting their mood. Evidence from past researches suggest that depressive patients present several cognitive impairments in attention and working memory, leading to a more acute selective attention for negative stimuli and a greater accessibility of negative memories. Recently, is has been proposed that impaired inhibitory functioning with regard to emotional information processing might be one of the mechanisms of ruminations linking memory, attention and depression. It seems that inhibition deficit is present at both the input level (i.e., the ability to reduce the interference from emotional distracters) and the higher level (i.e., the ability to direct the attention away from emotional material that has already been processed) of emotional information processing. Event-related potentials (ERP) have widely been used to study inhibition in adults suffering from various psychopathological states. In particular, depressive disorder has been linked to ERPs modulations, at early as well as at latter stages of the information-processing stream, when processing affective material. For instance, deficits in inhibiting negative information have been indexed by changes in the parameters (amplitudes and latencies) of early P2, P1 and N1 components while other ERP studies have shown an ability to differentiate depressed patients from normal controls based upon response inhibition difficulties in go-nogo tasks, indexed by later NoGo P3 differences. In this review, we will focus on results of ERP studies investigating inhibition and its interaction with emotional related cue processing in depressive populations. Implications for future research and theoretical perspectives will be discussed within the framework of current models of depressive disorder, based upon the hypothesis that negative ruminations are at the center of depression processes. "
pub.1129790364,A novel gateway-based solution for remote elderly monitoring,"Internet of Things (IoT) technologies have been applied to various fields such as manufacturing, automobile industry and healthcare. IoT-based healthcare has a significant impact on real-time remote monitoring of patients' health and consequently improving treatments and reducing healthcare costs. In fact, IoT has made healthcare more reliable, efficient, and accessible. Two major drawbacks which IoT suffers from can be expressed as: first, thelimited battery capacityof thesensorsis quickly depleted due to the continuous stream of data; second, the dependence of the system on the cloud for computations and processing causes latency in data transmission which is not accepted in real-time monitoring applications. This research is conducted to develop a real-time, secure, and energy-efficient platform which provides a solution for reducing computation load on the cloud and diminishing data transmission delay. In the proposed platform, the sensors utilize a state-of-the-art power saving technique known as Compressive Sensing (CS). CS allows sensors to retrieve the sensed data using fewer measurements by sending a compressed signal. In this framework, the signal reconstruction and processing are computed locally on a Heterogeneous Multicore Platform (HMP) device to decrease the dependency on the cloud. In addition, a framework has been implemented to control the system, set different parameters, display the data as well as send live notifications to medical experts through the cloud in order to alert them of any eventual hazardous event or abnormality and allow quick interventions. Finally, a case study of the system is presented demonstrating the acquisition and monitoring of the data for a given subject in real-time. The obtained results reveal that the proposed solution reduces 15.4% of energy consumption in sensors, that makes this prototype a good candidate for IoT employment in healthcare."
pub.1127378378,Efficient Processing of Spatio-Temporal Data Streams With Spiking Neural Networks,"Spiking neural networks (SNNs) are potentially highly efficient models for inference on fully parallel neuromorphic hardware, but existing training methods that convert conventional artificial neural networks (ANNs) into SNNs are unable to exploit these advantages. Although ANN-to-SNN conversion has achieved state-of-the-art accuracy for static image classification tasks, the following subtle but important difference in the way SNNs and ANNs integrate information over time makes the direct application of conversion techniques for sequence processing tasks challenging. Whereas all connections in SNNs have a certain propagation delay larger than zero, ANNs assign different roles to feed-forward connections, which immediately update all neurons within the same time step, and recurrent connections, which have to be rolled out in time and are typically assigned a delay of one time step. Here, we present a novel method to obtain highly accurate SNNs for sequence processing by modifying the ANN training before conversion, such that delays induced by ANN rollouts match the propagation delays in the targeted SNN implementation. Our method builds on the recently introduced framework of streaming rollouts, which aims for fully parallel model execution of ANNs and inherently allows for temporal integration by merging paths of different delays between input and output of the network. The resulting networks achieve state-of-the-art accuracy for multiple event-based benchmark datasets, including N-MNIST, CIFAR10-DVS, N-CARS, and DvsGesture, and through the use of spatio-temporal shortcut connections yield low-latency approximate network responses that improve over time as more of the input sequence is processed. In addition, our converted SNNs are consistently more energy-efficient than their corresponding ANNs."
pub.1032419780,"Closed-Loop, Multichannel Experimentation Using the Open-Source NeuroRighter Electrophysiology Platform","Single neuron feedback control techniques, such as voltage clamp and dynamic clamp, have enabled numerous advances in our understanding of ion channels, electrochemical signaling, and neural dynamics. Although commercially available multichannel recording and stimulation systems are commonly used for studying neural processing at the network level, they provide little native support for real-time feedback. We developed the open-source NeuroRighter multichannel electrophysiology hardware and software platform for closed-loop multichannel control with a focus on accessibility and low cost. NeuroRighter allows 64 channels of stimulation and recording for around US $10,000, along with the ability to integrate with other software and hardware. Here, we present substantial enhancements to the NeuroRighter platform, including a redesigned desktop application, a new stimulation subsystem allowing arbitrary stimulation patterns, low-latency data servers for accessing data streams, and a new application programming interface (API) for creating closed-loop protocols that can be inserted into NeuroRighter as plugin programs. This greatly simplifies the design of sophisticated real-time experiments without sacrificing the power and speed of a compiled programming language. Here we present a detailed description of NeuroRighter as a stand-alone application, its plugin API, and an extensive set of case studies that highlight the system's abilities for conducting closed-loop, multichannel interfacing experiments."
pub.1106898198,Neural Correlates of Conscious Motion Perception,"The nature of the proper neural signature of conscious perception remains a topic of active debate. Theoretical support from integrative theories of consciousness is consistent with such signature being P3b, one of the main candidates in the literature. Recent work has also put forward a mid-latency and more localized component, the Visual Awareness Negativity (VAN), as a proper Neural Correlate of Consciousness (NCC). Early local components like P1 have also been proposed. However, experiments exploring visual NCCs are conducted almost exclusively using static images as the content to be consciously perceived, favoring ventral stream processing, therefore limiting the scope of the NCCs that have been identified. Here we explored the visual NCCs isolating local motion, a dorsally processed feature, as the primary feature being consciously perceived. Physical equality between Seen and Unseen conditions in addition to a minimal contrast difference between target and no-target displays was employed. In agreement with previous literature, we found a P3b with a wide centro-parietal distribution that strongly correlated with the detection of the stimuli. P3b magnitude was larger for Seen vs. Unseen conditions, a result that was consistently observed at the single subject level. In contrast, we were unable to detect VAN in our data, regardless of whether the subject perceived or not the stimuli. In the 200-300 ms time window we found a N2pc component, consistent with the high attentional demands of our task. Early components like P1 were not observed in our data, in agreement with their proposed role in the processing of visual features, but not as proper NCCs. Our results extend the role of P3b as a content independent NCC to conscious visual motion perception."
pub.1083365308,Temporal and Spatial Response to Second-Order Stimuli in Cat Area 18,"Temporal and spatial response to second-order stimuli in cat area 18. J. Neurophysiol. 80: 2811-2823, 1998. Approximately one-half of the neurons in cat area 18 respond to contrast envelope stimuli, consisting of a sinewave carrier whose contrast is modulated by a drifting sinewave envelope of lower spatial frequency. These stimuli should fail to elicit a response from a conventional linear neuron because they are designed to contain no spatial frequency components within the cell's luminance-defined frequency passband. We measured neurons' responses to envelope stimuli by varying both the drift rate and spatial frequency of the contrast modulation. These data were then compared with the same neurons' spatial and temporal properties obtained with luminance-defined sinewave gratings. Most neurons' responses to the envelope stimuli were spatially and temporally bandpass, with bandwidths comparable with those measured with luminance gratings. The temporal responses of these neurons (temporal frequency tuning and latency) were systematically slower when tested with envelope stimuli than with luminance gratings. The simplest kind of model that can accommodate these results is one having separate, parallel streams of bandpass processing for luminance and envelope stimuli."
pub.1024391497,Stream-based Hebbian eigenfilter for real-time neuronal spike discrimination,"BackgroundPrincipal component analysis (PCA) has been widely employed for automatic neuronal spike sorting. Calculating principal components (PCs) is computationally expensive, and requires complex numerical operations and large memory resources. Substantial hardware resources are therefore needed for hardware implementations of PCA. General Hebbian algorithm (GHA) has been proposed for calculating PCs of neuronal spikes in our previous work, which eliminates the needs of computationally expensive covariance analysis and eigenvalue decomposition in conventional PCA algorithms. However, large memory resources are still inherently required for storing a large volume of aligned spikes for training PCs. The large size memory will consume large hardware resources and contribute significant power dissipation, which make GHA difficult to be implemented in portable or implantable multi-channel recording micro-systems.MethodIn this paper, we present a new algorithm for PCA-based spike sorting based on GHA, namely stream-based Hebbian eigenfilter, which eliminates the inherent memory requirements of GHA while keeping the accuracy of spike sorting by utilizing the pseudo-stationarity of neuronal spikes. Because of the reduction of large hardware storage requirements, the proposed algorithm can lead to ultra-low hardware resources and power consumption of hardware implementations, which is critical for the future multi-channel micro-systems. Both clinical and synthetic neural recording data sets were employed for evaluating the accuracy of the stream-based Hebbian eigenfilter. The performance of spike sorting using stream-based eigenfilter and the computational complexity of the eigenfilter were rigorously evaluated and compared with conventional PCA algorithms. Field programmable logic arrays (FPGAs) were employed to implement the proposed algorithm, evaluate the hardware implementations and demonstrate the reduction in both power consumption and hardware memories achieved by the streaming computingResults and discussionResults demonstrate that the stream-based eigenfilter can achieve the same accuracy and is 10 times more computationally efficient when compared with conventional PCA algorithms. Hardware evaluations show that 90.3% logic resources, 95.1% power consumption and 86.8% computing latency can be reduced by the stream-based eigenfilter when compared with PCA hardware. By utilizing the streaming method, 92% memory resources and 67% power consumption can be saved when compared with the direct implementation of GHA.ConclusionStream-based Hebbian eigenfilter presents a novel approach to enable real-time spike sorting with reduced computational complexity and hardware costs. This new design can be further utilized for multi-channel neuro-physiological experiments or chronic implants."
pub.1110634477,Stronger responses to darks along the ventral pathway of the cat visual cortex,"Light increments (brights) and decrements (darks) are differently processed throughout the early visual system. It is well known that a bias towards faster and stronger responses to darks is present in the retina, lateral geniculate nucleus and primary visual cortex. In humans, psychophysical and neurophysiological data indicate that darks are better detected than brights, suggesting that the dark bias found in early visual areas is transmitted across the cortical hierarchy. Here, we tested this assumption by investigating the spatiotemporal features of responses to brights and darks in area 21a, a gateway area of the cat ventral stream, using reverse correlation analysis of a sparse noise stimulus. The receptive field of most 21a neurons exhibited larger dark subfields. Additionally, the amplitude of the responses to darks was considerably greater than those evoked by brights. In the temporal domain, no differences were found between the response peak latency. Thus, the present study supports the notion that bright/dark asymmetries are transmitted throughout the cortical hierarchy and further, that the luminance processing varies as a function of the position in the cortical hierarchy, dark preference being strongly enhanced (in the spatial domain and response amplitude) along the ventral pathway."
pub.1041614278,The iso-response method: measuring neuronal stimulus integration with closed-loop experiments,"Throughout the nervous system, neurons integrate high-dimensional input streams and transform them into an output of their own. This integration of incoming signals involves filtering processes and complex non-linear operations. The shapes of these filters and non-linearities determine the computational features of single neurons and their functional roles within larger networks. A detailed characterization of signal integration is thus a central ingredient to understanding information processing in neural circuits. Conventional methods for measuring single-neuron response properties, such as reverse correlation, however, are often limited by the implicit assumption that stimulus integration occurs in a linear fashion. Here, we review a conceptual and experimental alternative that is based on exploring the space of those sensory stimuli that result in the same neural output. As demonstrated by recent results in the auditory and visual system, such iso-response stimuli can be used to identify the non-linearities relevant for stimulus integration, disentangle consecutive neural processing steps, and determine their characteristics with unprecedented precision. Automated closed-loop experiments are crucial for this advance, allowing rapid search strategies for identifying iso-response stimuli during experiments. Prime targets for the method are feed-forward neural signaling chains in sensory systems, but the method has also been successfully applied to feedback systems. Depending on the specific question, ""iso-response"" may refer to a predefined firing rate, single-spike probability, first-spike latency, or other output measures. Examples from different studies show that substantial progress in understanding neural dynamics and coding can be achieved once rapid online data analysis and stimulus generation, adaptive sampling, and computational modeling are tightly integrated into experiments."
pub.1005611656,Distinct Visual Evoked Potential Morphological Patterns for Apparent Motion Processing in School-Aged Children,"Measures of visual cortical development in children demonstrate high variability and inconsistency throughout the literature. This is partly due to the specificity of the visual system in processing certain features. It may then be advantageous to activate multiple cortical pathways in order to observe maturation of coinciding networks. Visual stimuli eliciting the percept of apparent motion and shape change is designed to simultaneously activate both dorsal and ventral visual streams. However, research has shown that such stimuli also elicit variable visual evoked potential (VEP) morphology in children. The aim of this study was to describe developmental changes in VEPs, including morphological patterns, and underlying visual cortical generators, elicited by apparent motion and shape change in school-aged children. Forty-one typically developing children underwent high-density EEG recordings in response to a continuously morphing, radially modulated, circle-star grating. VEPs were then compared across the age groups of 5-7, 8-10, and 11-15 years according to latency and amplitude. Current density reconstructions (CDR) were performed on VEP data in order to observe activated cortical regions. It was found that two distinct VEP morphological patterns occurred in each age group. However, there were no major developmental differences between the age groups according to each pattern. CDR further demonstrated consistent visual generators across age and pattern. These results describe two novel VEP morphological patterns in typically developing children, but with similar underlying cortical sources. The importance of these morphological patterns is discussed in terms of future studies and the investigation of a relationship to visual cognitive performance. "
pub.1113047026,Functional Development of Principal Neurons in the Anteroventral Cochlear Nucleus Extends Beyond Hearing Onset,"Sound information is transduced into graded receptor potential by cochlear hair cells and encoded as discrete action potentials of auditory nerve fibers. In the cochlear nucleus, auditory nerve fibers convey this information through morphologically distinct synaptic terminals onto bushy cells (BCs) and stellate cells (SCs) for processing of different sound features. With expanding use of transgenic mouse models, it is increasingly important to understand the <i>in vivo</i> functional development of these neurons in mice. We characterized the maturation of spontaneous and acoustically evoked activity in BCs and SCs by acquiring single-unit juxtacellular recordings between hearing onset (P12) and young adulthood (P30) of anesthetized CBA/J mice. In both cell types, hearing sensitivity and characteristic frequency (CF) range are mostly adult-like by P14, consistent with rapid maturation of the auditory periphery. In BCs, however, some physiological features like maximal firing rate, dynamic range, temporal response properties, recovery from post-stimulus depression, first spike latency (FSL) and encoding of sinusoid amplitude modulation undergo further maturation up to P18. In SCs, the development of excitatory responses is even more prolonged, indicated by a gradual increase in spontaneous and maximum firing rates up to P30. In the same cell type, broadly tuned acoustically evoked inhibition is immediately effective at hearing onset, covering the low- and high-frequency flanks of the excitatory response area. Together, these data suggest that maturation of auditory processing in the parallel ascending BC and SC streams engages distinct mechanisms at the first central synapses that may differently depend on the early auditory experience."
pub.1024174066,"Motmot, an open-source toolkit for realtime video acquisition and analysis","BackgroundVideo cameras sense passively from a distance, offer a rich information stream, and provide intuitively meaningful raw data. Camera-based imaging has thus proven critical for many advances in neuroscience and biology, with applications ranging from cellular imaging of fluorescent dyes to tracking of whole-animal behavior at ecologically relevant spatial scales.ResultsHere we present 'Motmot': an open-source software suite for acquiring, displaying, saving, and analyzing digital video in real-time. At the highest level, Motmot is written in the Python computer language. The large amounts of data produced by digital cameras are handled by low-level, optimized functions, usually written in C. This high-level/low-level partitioning and use of select external libraries allow Motmot, with only modest complexity, to perform well as a core technology for many high-performance imaging tasks. In its current form, Motmot allows for: (1) image acquisition from a variety of camera interfaces (package motmot.cam_iface), (2) the display of these images with minimal latency and computer resources using wxPython and OpenGL (package motmot.wxglvideo), (3) saving images with no compression in a single-pass, low-CPU-use format (package motmot.FlyMovieFormat), (4) a pluggable framework for custom analysis of images in realtime and (5) firmware for an inexpensive USB device to synchronize image acquisition across multiple cameras, with analog input, or with other hardware devices (package motmot.fview_ext_trig). These capabilities are brought together in a graphical user interface, called 'FView', allowing an end user to easily view and save digital video without writing any code. One plugin for FView, 'FlyTrax', which tracks the movement of fruit flies in real-time, is included with Motmot, and is described to illustrate the capabilities of FView.ConclusionMotmot enables realtime image processing and display using the Python computer language. In addition to the provided complete applications, the architecture allows the user to write relatively simple plugins, which can accomplish a variety of computer vision tasks and be integrated within larger software systems. The software is available at http://code.astraw.com/projects/motmot"
pub.1124082084,IRIS: A Modular Platform for Continuous Monitoring and Caretaker Notification in the Intensive Care Unit,"OBJECTIVE: New approaches are needed to interpret large amounts of physiologic data continuously recorded in the ICU. We developed and prospectively validated a versatile platform (IRIS) for real-time ICU physiologic monitoring, clinical decision making, and caretaker notification.
METHODS: IRIS was implemented in the neurointensive care unit to stream multimodal time series data, including EEG, intracranial pressure (ICP), and brain tissue oxygenation (P<sub>bt</sub>O<sub>2</sub>), from ICU monitors to an analysis server. IRIS was applied for 364 patients undergoing continuous EEG, 26 patients undergoing burst suppression monitoring, and four patients undergoing intracranial pressure and brain tissue oxygen monitoring. Custom algorithms were used to identify periods of elevated ICP, compute burst suppression ratios (BSRs), and detect faulty or disconnected EEG electrodes. Hospital staff were notified of clinically relevant events using our secure API to route alerts through a password-protected smartphone application.
RESULTS: Sustained increases in ICP and concordant decreases in P<sub>bt</sub>O<sub>2</sub> were reliably detected using user-defined thresholds and alert throttling. BSR trends computed by the platform correlated highly with manual neurologist markings (r<sup>2</sup> 0.633-0.781; p &lt; 0.0001). The platform identified EEG electrodes with poor signal quality with 95% positive predictive value, and reduced latency of technician response by 93%.
CONCLUSION: This study validates a flexible real-time platform for monitoring and interpreting ICU data and notifying caretakers of actionable results, with potential to reduce the manual burden of continuous monitoring services on care providers.
SIGNIFICANCE: This work represents an important step toward facilitating translational medical data analytics to improve patient care and reduce health care costs."
pub.1028627697,The aging brain shows less flexible reallocation of cognitive resources during dual-task walking: A mobile brain/body imaging (MoBI) study,"Aging is associated with reduced abilities to selectively allocate attention across multiple domains. This may be particularly problematic during everyday multitasking situations when cognitively demanding tasks are performed while walking. Due to previous limitations in neuroimaging technology, much remains unknown about the cortical mechanisms underlying resource allocation during locomotion. Here, we utilized an EEG-based mobile brain/body imaging (MoBI) technique that integrates high-density event-related potential (ERP) recordings with simultaneously acquired foot-force sensor data to monitor gait patterns and brain activity concurrently. To assess effects of motor load on cognition we evaluated young (N=17; mean age=27.2) and older adults (N=16; mean age=63.9) and compared behavioral and ERP measures associated with performing a Go/No-Go response inhibition task as participants sat stationary or walked on a treadmill. Stride time and variability were also measured during task performance and compared to stride parameters obtained without task performance, thereby assessing effects of cognitive load on gait. Results showed that older, but not young adults' accuracy dropped significantly when performing the inhibitory task while walking. Young adults revealed ERP modulations at relatively early (N2 amplitude reduction) and later (earlier P3 latency) stages within the processing stream as motor load increased while walking. In contrast, older adults' ERP modulations were limited to later processing stages (increased P3 amplitude) of the inhibitory network. The relative delay and attenuation of ERP modulations accompanied by behavioral costs in older participants might indicate an age-associated loss in flexible resource allocation across multiple tasks. Better understanding of the neural underpinnings of these age-related changes may lead to improved strategies to reduce fall risk and enhance mobility in aging. "
pub.1138129517,Augmenting Speech Quality Estimation in Software-Defined Networking Using Machine Learning Algorithms,"With the increased number of Software-Defined Networking (SDN) installations, the data centers of large service providers are becoming more and more agile in terms of network performance efficiency and flexibility. While SDN is an active and obvious trend in a modern data center design, the implications and possibilities it carries for effective and efficient network management are not yet fully explored and utilized. With most of the modern Internet traffic consisting of multimedia services and media-rich content sharing, the quality of multimedia communications is at the center of attention of many companies and research groups. Since SDN-enabled switches have an inherent feature of monitoring the flow statistics in terms of packets and bytes transmitted/lost, these devices can be utilized to monitor the essential statistics of the multimedia communications, allowing the provider to act in case of network failing to deliver the required service quality. The internal packet processing in the SDN switch enables the SDN controller to fetch the statistical information of the particular packet flow using the PacketIn and Multipart messages. This information, if preprocessed properly, can be used to estimate higher layer interpretation of the link quality and thus allowing to relate the provided quality of service (QoS) to the quality of user experience (QoE). This article discusses the experimental setup that can be used to estimate the quality of speech communication based on the information provided by the SDN controller. To achieve higher accuracy of the result, latency characteristics are added based on the exploiting of the dummy packet injection into the packet stream and/or RTCP packet analysis. The results of the experiment show that this innovative approach calculates the statistics of each individual RTP stream, and thus, we obtain a method for dynamic measurement of speech quality, where when quality decreases, it is possible to respond quickly by changing routing at the network level for each individual call. To improve the quality of call measurements, a Convolutional Neural Network (CNN) was also implemented. This model is based on two standard approaches to measuring the speech quality: PESQ and E-model. However, unlike PESQ/POLQA, the CNN-based model can take delay into account, and unlike the E-model, the resulting accuracy is much higher."
pub.1020372268,Soldiers With Posttraumatic Stress Disorder See a World Full of Threat: Magnetoencephalography Reveals Enhanced Tuning to Combat-Related Cues,"BACKGROUND: Posttraumatic stress disorder (PTSD) is linked to elevated arousal and alterations in cognitive processes. Yet, whether a traumatic experience is linked to neural and behavioral differences in selective attentional tuning to traumatic stimuli is not known. The present study examined selective awareness of threat stimuli and underlying temporal-spatial patterns of brain activation associated with PTSD.
METHODS: Participants were 44 soldiers from the Canadian Armed Forces, 22 with PTSD and 22 without. All completed neuropsychological tests and clinical assessments. Magnetoencephalography data were collected while participants identified two targets in a rapidly presented stream of words. The first target was a number and the second target was either a combat-related or neutral word. The difference in accuracy for combat-related versus neutral words was used as a measure of attentional bias.
RESULTS: All soldiers showed a bias for combat-related words. This bias was enhanced in the PTSD group, and behavioral differences were associated with distinct patterns of brain activity. At early latencies, non-PTSD soldiers showed activation of midline frontal regions associated with fear regulation (90-340 ms after the second target presentation), whereas those with PTSD showed greater visual cortex activation linked to enhanced visual processing of trauma stimuli (200-300 ms).
CONCLUSIONS: These findings suggest that attentional biases in PTSD are linked to deficits in very rapid regulatory activation observed in healthy control subjects. Thus, sufferers with PTSD may literally see a world more populated by traumatic cues, contributing to a positive feedback loop that perpetuates the effects of trauma."
pub.1084099000,Hard-wired feed-forward visual mechanisms of the brain compensate for affine variations in object recognition,"Humans perform object recognition effortlessly and accurately. However, it is unknown how the visual system copes with variations in objects' appearance and the environmental conditions. Previous studies have suggested that affine variations such as size and position are compensated for in the feed-forward sweep of visual information processing while feedback signals are needed for precise recognition when encountering non-affine variations such as pose and lighting. Yet, no empirical data exist to support this suggestion. We systematically investigated the impact of the above-mentioned affine and non-affine variations on the categorization performance of the feed-forward mechanisms of the human brain. For that purpose, we designed a backward-masking behavioral categorization paradigm as well as a passive viewing EEG recording experiment. On a set of varying stimuli, we found that the feed-forward visual pathways contributed more dominantly to the compensation of variations in size and position compared to lighting and pose. This was reflected in both the amplitude and the latency of the category separability indices obtained from the EEG signals. Using a feed-forward computational model of the ventral visual stream, we also confirmed a more dominant role for the feed-forward visual mechanisms of the brain in the compensation of affine variations. Taken together, our experimental results support the theory that non-affine variations such as pose and lighting may need top-down feedback information from higher areas such as IT and PFC for precise object recognition."
pub.1040014581,Functional Connections Between Auditory Cortex on Heschl's Gyrus and on the Lateral Superior Temporal Gyrus in Humans,"Functional connections between auditory fields on Heschl's gyrus (HG) and the acoustically responsive posterior lateral superior temporal gyrus (field PLST) were studied using electrical stimulation and recording methods in patients undergoing diagnosis and treatment of intractable epilepsy. Averaged auditory (click-train) evoked potentials were recorded from multicontact subdural recording arrays chronically implanted over the lateral surface of the superior temporal gyrus (STG) and from modified depth electrodes inserted into HG. Biphasic electrical pulses (bipolar, constant current, 0.2 ms) were delivered to HG sites while recording from the electrode array over acoustically responsive STG cortex. Stimulation of sites along the mediolateral extent of HG resulted in complex waveforms distributed over posterolateral STG. These areas overlapped each other and field PLST. For any given HG stimulus site, the morphology of the electrically evoked waveform varied across the STG map. A characteristic waveform was recorded at the site of maximal amplitude of response to stimulation of mesial HG [presumed primary auditory field (AI)]. Latency measurements suggest that the earliest evoked wave resulted from activation of connections within the cortex. Waveforms changed with changes in rate of electrical HG stimulation or with shifts in the HG stimulus site. Data suggest widespread convergence and divergence of input from HG to posterior STG. Evidence is presented for a reciprocal functional projection, from posterolateral STG to HG. Results indicate that in humans there is a processing stream from AI on mesial HG to an associational auditory field (PLST) on the lateral surface of the superior temporal gyrus."
pub.1113327747,Auditory sensory memory span for duration is severely curtailed in females with Rett syndrome,"Rett syndrome (RTT), a rare neurodevelopmental disorder caused by mutations in the MECP2 gene, is typified by profound cognitive impairment and severe language impairment, rendering it very difficult to accurately measure auditory processing capabilities behaviorally in this population. Here we leverage the mismatch negativity (MMN) component of the event-related potential to measure the ability of RTT patients to decode and store occasional duration deviations in a stream of auditory stimuli. Sensory memory for duration, crucial for speech comprehension, has not been studied in RTT.High-density electroencephalography was successfully recorded in 18 females with RTT and 27 age-matched typically developing (TD) controls (aged 6–22 years). Data from seven RTT and three TD participants were excluded for excessive noise. Stimuli were 1 kHz tones with a standard duration of 100 ms and deviant duration of 180 ms. To assess the sustainability of sensory memory, stimulus presentation rate was varied with stimulus onset asynchronies (SOAs) of 450, 900, and 1800 ms. MMNs with maximum negativity over fronto-central scalp and a latency of 220–230 ms were clearly evident for each presentation rate in the TD group, but only for the shortest SOA in the RTT group. Repeated-measures ANOVA revealed a significant group by SOA interaction. MMN amplitude correlated with age in the TD group only. MMN amplitude was not correlated with the Rett Syndrome Severity Scale. This study indicates that while RTT patients can decode deviations in auditory duration, the span of this sensory memory system is severely foreshortened, with likely implications for speech decoding abilities."
pub.1107843459,Detectability of stop‐signal determines magnitude of deceleration in saccade planning,"An inhibitory control is exerted when the context in which a movement has been planned changes abruptly making the impending movement inappropriate. Neurons in the frontal eye field and superior colliculus steadily increase activity before a saccadic eye movement, but cease the rise below a threshold when an impending saccade is withheld in response to an unexpected stop-signal. This type of neural modulation has been majorly considered as an outcome of a race between preparatory and inhibitory processes ramping up to reach a decision criterion. An alternative model claims that the rate of saccade planning is diminished exclusively when the stop-signal is detected within a stipulated period. However, due to a dearth of empirical evidence in support of the latter model, it remains unclear how the detectability of the stop-signal influences saccade inhibition. In our study, human participants selected a visual target to look at by discriminating a go-cue. Infrequently they cancelled saccade and reported whether they saw the stop-signal. The go-cue and stop-signal both were embedded in a stream of irrelevant stimuli presented in rapid succession. Participants exhibited difficulty in detection of the stop-signal when presented almost immediately after the go-cue. We found a robust relationship between the detectability of the stop-signal and the odds of saccade inhibition. Saccade latency increased exponentially with the maximum time available for processing the stop-signal before gaze shifted. A model in which the stop-signal onset spontaneously decelerated progressive saccade planning with the magnitude proportional to its detectability accounted for the data."
pub.1021527252,Multisensory processing of naturalistic objects in motion: A high-density electrical mapping and source estimation study,"In everyday life, we continuously and effortlessly integrate the multiple sensory inputs from objects in motion. For instance, the sound and the visual percept of vehicles in traffic provide us with complementary information about the location and motion of vehicles. Here, we used high-density electrical mapping and local auto-regressive average (LAURA) source estimation to study the integration of multisensory objects in motion as reflected in event-related potentials (ERPs). A randomized stream of naturalistic multisensory-audiovisual (AV), unisensory-auditory (A), and unisensory-visual (V) ""splash"" clips (i.e., a drop falling and hitting a water surface) was presented among non-naturalistic abstract motion stimuli. The visual clip onset preceded the ""splash"" onset by 100 ms for multisensory stimuli. For naturalistic objects early multisensory integration effects beginning 120-140 ms after sound onset were observed over posterior scalp, with distributed sources localized to occipital cortex, temporal lobule, insular, and medial frontal gyrus (MFG). These effects, together with longer latency interactions (210-250 and 300-350 ms) found in a widespread network of occipital, temporal, and frontal areas, suggest that naturalistic objects in motion are processed at multiple stages of multisensory integration. The pattern of integration effects differed considerably for non-naturalistic stimuli. Unlike naturalistic objects, no early interactions were found for non-naturalistic objects. The earliest integration effects for non-naturalistic stimuli were observed 210-250 ms after sound onset including large portions of the inferior parietal cortex (IPC). As such, there were clear differences in the cortical networks activated by multisensory motion stimuli as a consequence of the semantic relatedness (or lack thereof) of the constituent sensory elements."
pub.1018254440,"Ascending Inputs to, and Internal Organization of, Cortical Motor Areas","Modern anatomical studies show that, contrary to the long-held dogma, there appears to be essentially no convergence of lemniscal, cerebellar, pallidal, or substantial nigral afferents in the thalamus. Each afferent stream defines its own thalamic territory and, through the projection of these thalamic territories to separate cortical territories, the independence of the projections of subcortical motor nuclei upon the cortex is preserved. Only the spinothalamic system appears to gain access to both sensory and motor cortex. A further principle of organization in the sensorimotor thalamus is the presence of individual anatomically and physiologically defined channels, composed of separate afferent inputs and groups of neurons relaying to the cortex. In the somatic sensory relay nuclei the dissociation of cutaneous, deep slowly and rapidly adapting channels is clear-cut in the thalamus and at the input level of the cortex. In the motor system, inputs from each of the deep cerebellar nuclei appear to be dissociated from one another in the thalamus and these in turn from the vestibular and spinothalamic systems. Just as pallidal, nigral and cerebellar pathways are in position to control separate premotor and motor areas of the cortex, so separate channels leading through VLp appear to be in a position to control separate functional units in area 4. Within the cortex itself the absence of corticocortical connections passing from areas 3a and 3b to area 4 appears to indicate that information flow out of these areas is back to areas 1 and 2 for further processing before transmission to area 4 with all the consequences that entails for sensory convergence. Presumably, this route is sufficiently rapid for sensory inputs to reach area 4 at short latency. Although many data are beginning to accrue on the intrinsic structure and connectivity of the sensorimotor cortex, we are still distant from a complete wiring diagram. Circuitry involving thalamic afferents is becoming known slowly and the nature of the cells that are present and their transmitter characteristics are becoming evident from morphological and immunocytochemical studies, along with information on the patterns of axonal ramification of specific cell types, especially of GABAergic cells and of excitatory corticocortical cells."
pub.1119761910,Topology-aware task allocation for online distributed stream processing applications with latency constraints,"There have been increasing demands for real time processing of the ever-growing data. In order to meet this requirement and ensure the reliable processing of streaming data, a variety of distributed stream processing architectures and platforms have been developed, which handles the fundamental task of allocating processing tasks to the currently available physical resources and routing streaming data between these resources. However, many stream processing systems lack an intelligent scheduling mechanism, in which their default schedulers allocate tasks without taking resource demands and availability, or the transfer latency between resources into consideration. Besides, stream processing has a strict request for latency. Thus it is important to give latency guarantee for distributed stream processing. In this paper, we propose two new algorithms for stream processing with latency guarantee, both the algorithms consider transfer latency and resource demand in task allocation. Both algorithms can guarantee latency constraints. Algorithm AHA reduces more than 21.3% and 58.9% resources compared with the greedy and the round-robin algorithms, and algorithm PHA further improves the resource utilization to 32.1% and 73.2%."
pub.1107560572,Topology-Aware Task Allocation for Distributed Stream Processing with Latency Guarantee,"Nowadays it becomes more and more critical to process the increasingly large amounts of data in timely manner. In order to meet this requirement and ensure the reliable processing of streaming data, a variety of distributed stream processing architectures and platforms have been developed, which handles the fundamental task of allocating processing tasks to the currently available physical resources and routing streaming data between these resources. However, many stream processing systems lack an intelligent scheduling mechanism, in which their default schedulers allocate tasks without taking resource demands and availability, or the transfer latency between resources into consideration. Besides stream processing has a strict request for latency. Thus it's important to give latency guarantee for distributed stream processing. In this paper, we propose a new algorithm for stream processing with latency guarantee, the algorithm both consider transfer latency and resource demand in the process of task allocation. Extensive experiments verify the correctness and effectiveness of our approach. Under the condition of satisfying the latency constraints, the heuristic algorithm AHA on average, reduce more than 21.3% and 58.9% resources compared with the greedy and the round-robin algorithms."
pub.1162763937,CompressStreamDB: Fine-Grained Adaptive Stream Processing without Decompression,"Stream processing prevails and SQL query on streams has become one of the most popular application scenarios. For example, in 2021, the global number of active IoT endpoints reaches 12.3 billion. Unfortunately, the increasing scale of data and strict user requests place much pressure on existing stream processing systems, requiring high processing throughput with low latency. To further improve the performance of current stream processing systems, we propose a compression-based stream processing engine, called CompressStreamDB, which enables adaptive fine-grained stream processing directly on compressed streams, without decompression. Particularly, CompressStreamDB involves eight compression methods targeting various data types in streams, and it also provides a cost model for dynamically selecting the appropriate compression methods. By exploring data redundancy among streams, CompressStreamDB not only saves space in data transmission between client and server, but also achieves high throughput with low latency in SQL query on stream processing. Our experimental results show that compared to the state-of-the-art stream processing system on uncompressed streams, CompressStreamDB achieves 3.24× throughput improvement and 66.0% lower latency on average. Besides, CompressStreamDB saves 66.8% space."
pub.1068339496,Efficiency of Stream Processing Engines for Processing BIGDATA Streams,"Background/Objectives:Inthis paper,there are real-time stream &big data stream applications running ona large amount of clusters & live migration of data using intensive & interactive tasks. Methods/Statistical Analysis: Traditionally, there are many stream processing engines which are going to manage the processing data in this streaming scenario, ongoing stream processing the data must be handle due to faults & stragglers within a second-scale latency. At present, there are efficient stream processing engines which can avoid these faults & stragglers by using the stream processing engines like APACHE SPARK & APACHE FLINK. Findings: In the real-time data stream processing there are some problem occurrences while processing the data, those are data latency, fault-tolerance, mutable data-sets and more stragglers. To avoid all these issues there is an efficient stream processing engines with additional added mechanisms like dolly retreat mechanism for avoiding stragglers and process data efficiently. Application/Improvements: Now a day’s demand for stream processing is increasing a lot which may be real-time streaming or normal cluster data transformation. Data has to be processed fast, so that companies are integrating these stream processing engines to changing their business conditions effectively in realtime for analyze their data more effectively with help of Fast Stream Processing Engines. Keywords: Fault-Tolerance, RDD, Second-Scale Latency and Immutable Datasets, Stragglers"
pub.1106072342,Dual-Paradigm Stream Processing,"Existing stream processing frameworks operate either under data stream paradigm processing data record by record to favor low latency, or under operation stream paradigm processing data in micro-batches to desire high throughput. For complex and mutable data processing requirements, this dilemma brings the selection and deployment of stream processing frameworks into an embarrassing situation. Moreover, current data stream or operation stream paradigms cannot handle data burst efficiently, which probably results in noticeable performance degradation. This paper introduces a dual-paradigm stream processing, called DO (Data and Operation) that can adapt to stream data volatility. It enables data to be processed in micro-batches (i.e., operation stream) when data burst occurs to achieve high throughput, while data is processed record by record (i.e., data stream) in the remaining time to sustain low latency. DO embraces a method to detect data bursts, identify the main operations affected by the data burst and switch paradigms accordingly. Our insight behind DO's design is that the trade-off between latency and throughput of stream processing frameworks can be dynamically achieved according to data communication among operations in a fine-grained manner (i.e., operation level) instead of framework level. We implement a prototype stream processing framework that adopts DO. Our experimental results show that our framework with DO can achieve 5x speedup over operation stream under low data stream sizes, and outperforms data stream on throughput by 2.1x to 3.2x under data burst."
pub.1154644585,An Adaptive Partitioning Method for Distributed Stream Processing Systems,"In recent years, stream processing systems have attracted attention for their ability to process big data, that is constantly generated, with low latency. However, the performance of stream processing deteriorates due to the bias in the amount of input data for parallel processing. This paper proposes an adaptive partitioning method based on the latency of each parallel process. Experimental results using a prototype system show that the latency bias can be reduced to half."
pub.1149964506,Latency and Energy-Awareness in Data Stream Processing for Edge Based IoT Systems,"LE-STREAM is a framework for IoT data stream processing. Data processing in IoT is challenging due to its dynamic and heterogeneous nature, and the massive amount of generated data. Sensor data suffers from uncertainty and inconsistency issues, that can affect its accuracy. Several IoT applications are time sensitive, requiring fast data processing. Finally, as IoT devices are often battery powered, processing tasks must be performed in an energy-efficient way. Therefore, challenges in IoT data stream processing span three dimensions: accuracy, latency and energy; and LE-STREAM jointly addresses them. It leverages edge computing to bring the data processing closer to the data sources, thus minimizing latency. Adaptive sampling combined with data prediction model reduce the energy consumption of devices without compromising data accuracy. An active node selection schema improves the workload distribution among devices, also tackling the energy dimension by promoting a graceful degradation of device resources."
pub.1139810587,Latency-Aware Strategies for Deploying Data Stream Processing Applications on Large Cloud-Edge Infrastructure,"Internet of Things (IoT) applications often require the processing of data streams generated by devices dispersed over a large geographical area. Traditionally, these data streams are forwarded to a distant cloud for processing, thus resulting in high application end-to-end latency. Recent work explores the combination of resources located in clouds and at the edges of the Internet, called cloud-edge infrastructure, for deploying Data Stream Processing (DSP) applications. Most previous work, however, fails to scale to very large IoT settings. This paper introduces deployment strategies for the placement of Data Stream Processing (DSP) applications onto cloud-edge infrastructure. The strategies split an application graph into regions and consider regions with stringent time requirements for edge placement. The proposed Aggregate End-to-End Latency Strategy with Region Patterns and Latency Awareness (AELS+RP+LA) decreases the number of evaluated resources when computing an operator's placement by considering the communication overhead across computing resources. Simulation results show that, unlike the state-of-the-art, Aggregate End-to-End Latency Strategy with Region Patterns and Latency Awareness (AELS+RP+LA) scales to environments with more than 100k resources with negligible impact on the application end-to-end latency."
pub.1095517323,A Highly Efficient Consolidated Platform for Stream Computing and Hadoop,"Data Stream Processing or stream computing is the new computing paradigm for processing a massive amount of streaming data in real-time without storing them in secondary storage. In this paper we propose an integrated execution platform for Data Stream Processing and Hadoop with dynamic load balancing mechanism to realize an efficient operation of computer systems and reduction of latency of Data Stream Processing. Our implementation is built on top of System S, a distributed data stream processing system developed by IBM Research. Our experimental results show that our load balancing mechanism could increase CPU usage from 47.77% to 72.14% when compared to the one with no load balancing. Moreover, the result shows that latency for stream processing jobs are kept low even in a bursty situation by dynamically allocating more compute resources to stream processing jobs."
pub.1181596879,A Distributed and Scalable Framework for Low-Latency Continuous Trajectory Stream Processing,"In recent times, the widespread adoption of GPS-enabled devices has resulted in an exponential surge in the amount of spatio-temporal data streams. These data streams, often referred to as trajectory data streams, represent a sequence of spatial locations of mobile objects. As these streams continue to grow, so does the demand for its efficient processing and analysis. Various government agencies and businesses now require real-time or low-latency solutions for continuous query processing and interactive analysis of trajectory streams, driven by applications like real-time route guidance during disasters, tracking critically ill patients to prevent mishaps, ongoing road traffic management to avert accidents and congestion, and more. Trajectory streams, due to their volume and velocity, necessitate scalable Stream Processing Engines (SPEs) for effective handling. However, currently available state-of-the-art scalable spatial SPEs do not adequately support low-latency and continuous processing of trajectories. Conversely, while scalable SPEs do exist, they lack the necessary data structures, operators, and indices essential for processing trajectory streams. To bridge this gap, our work introduces TStream, a distributed, low-latency, and continuous trajectory stream processing system. TStream addresses this challenge by offering support for spatial data types, spatial indices, and spatial continuous queries such as range, nearest neighbor searches (kNN), and joins over spatial trajectory streams. To substantiate the efficacy of TStream, we present an extensive experimental study conducted on two real-world trajectory datasets."
pub.1094880379,Adaptive Block and Batch Sizing for Batched Stream Processing System,"The need for real-time and large-scale data processing has led to the development of frameworks for distributed stream processing in the cloud. To provide fast, scalable, and fault tolerant stream processing, recent Distributed Stream Processing Systems (DSPS) treat streaming workloads as a series of batch jobs, instead of a series of records. Batch-based stream processing systems could process data at high rate but lead to large end-to-end latency. In this paper we concentrate on minimizing the end-to-end latency of batched streaming system by leveraging adaptive batch sizing and execution parallelism tuning. We propose, DyBBS, a heuristic algorithm integrated with isotonic regression to automatically learn and adjust batch size and execution parallelism according to workloads and operating conditions. Our approach does not require workload specific knowledge. The experimental results show that our algorithm significantly reduces end-to-end latency compared to state-of-the-art: i) for Reduce workload, the latency can be reduced by 34.97% and 48.02% for sinusoidal and Markov chain data input rates, respectively; and ii) for Join workload, the latency reductions are 63.28 % and 67.51% for sinusoidal and Markov chain data input rates, respectively."
pub.1157778355,Intelligent computing for WPT–MEC-aided multi-source data stream,"Due to its low latency and energy consumption, edge computing technology is essential in processing multi-source data streams from intelligent devices. This article investigates a mobile edge computing network aided by wireless power transfer (WPT) for multi-source data streams, where the wireless channel parameters and the characteristic of the data stream are varied. Moreover, we consider a practical communication scenario, where the devices with limited battery capacity cannot support the executing and transmitting of computational data streams under a given latency. Thus, WPT technology is adopted for this considered network to enable the devices to harvest energy from the power beacon. In further, by considering the device’s energy consumption and latency constraints, we propose an optimization problem under energy constraints. To solve this problem, we design a customized particle swarm optimization-based algorithm, which aims at minimizing the latency of the device processing computational data stream by jointly optimizing the charging and offloading strategies. Furthermore, simulation results illustrate that the proposed method outperforms other benchmark schemes in minimizing latency, which shows the proposed method’s superiority in processing the multi-source data stream."
pub.1169294572,Research on IIoT Cloud-Edge Collaborative Stream Processing Architecture for Intelligent Factory,"Intelligent factories (IF) represented by unmanned workshops and light out workshops point out the development direction of future industrial production which is known as Industrial Internet of Things (IIoT) and cloud-edge collaborative services. IIoT must concurrently send various types of data in real time to the cloud for adequately analyzing and processing the data to support the intelligent decision-making needs of IF, which makes massive highly concurrent heterogeneous data streams. The traditional IIoT platform of directly connecting devices to the cloud, which cannot satisfy the high concurrency heterogeneous data stream transmission requirements of IF, result in data loss and high latency. In this article, cloud-edge collaborative stream processing architecture is proposed for efficiently processing high concurrency heterogeneous data streams generated by IIoT in the production process with reliable, confidential, and low latency data transmission from devices to edges and clouds. The real-time display of AGV status and workshop environment through the cloud-edge collaborative stream processing platform prototype system has demonstrated that the latency of data stream transmission processing is millisecond level, fully satisfying the practical application requirements of IF."
pub.1156684974,Dynamic Load Balancing in Stream Processing Pipelines Containing Stream-Static Joins,"Data stream processing systems are used to continuously run mission-critical applications for real-time monitoring and alerting. These systems require high throughput and low latency to process incoming data streams in real time. However, changes in the distribution of incoming data streams over time can cause partition skew, which is defined as an unequal distribution of data partitions among workers, resulting in sub-optimal processing due to an unbalanced load. This paper presents the first solution designed specifically to address partition skew in the context of joining streaming and static data. Our solution uses state-of-the-art principles to monitor processing load, detect load imbalance, and dynamically redistribute partitions, to achieve optimal load balance. To accomplish this, our solution leverages the collocation of streaming and static data, while considering the processing load of the join and the subsequent stream processing operations. Finally, we present the results of an experimental evaluation, in which we compared the throughput and latency of four stream processing pipelines containing such a join. The results show that our solution achieved significantly higher throughput and lower latency than the competing approaches."
pub.1140131302,Resilient Stream Processing in Edge Computing,"The proliferation of Internet-of-Things (IoT) devices is rapidly increasing the demands for efficient processing of low latency stream data generated close to the edge of the network. A large number of IoT applications require continuous processing of data streams in real-time. Examples include virtual reality applications, connected autonomous vehicles and smart city applications. Although current distributed stream processing systems offer various forms of fault tolerance, existing schemes do not understand the dynamic characteristics of edge computing infrastructures and the unique requirements of edge computing applications. Optimizing fault tolerance techniques to meet latency requirements while minimizing resource usage becomes a critical dimension of resource allocation and scheduling when dealing with latency-sensitive IoT applications in edge computing. In this paper, we present a novel resilient stream processing framework that achieves system-wide fault tolerance while meeting the latency requirements for edge-based applications. The proposed approach employs a novel resilient physical plan generation for stream queries and optimizes the placement of operators to minimize the processing latency during recovery and reduces the overhead of checkpointing. We implement a prototype of the proposed techniques in Apache Storm and evaluate it in a real testbed. Our results demonstrate that the proposed approach is highly effective and scalable while ensuring low latency and low-cost recovery for edge-based stream processing applications."
pub.1110033157,A Task Allocation Method for Stream Processing with Recovery Latency Constraint,"Stream processing applications continuously process large amounts of online streaming data in real time or near real time. They have strict latency constraints. However, the continuous processing makes them vulnerable to any failures, and the recoveries may slow down the entire processing pipeline and break latency constraints. The upstream backup scheme is one of the most widely applied fault-tolerant schemes for stream processing systems. It introduces complex backup dependencies to tasks, which increases the difficulty of controlling recovery latencies. Moreover, when dependent tasks are located on the same processor, they fail at the same time in processor-level failures, bringing extra recovery latencies that increase the impacts of failures. This paper studies the relationship between the task allocation and the recovery latency of a stream processing application. We present a correlated failure effect model to describe the recovery latency of a stream topology in processor-level failures under a task allocation plan. We introduce a recovery-latency aware task allocation problem (RTAP) that seeks task allocation plans for stream topologies that will achieve guaranteed recovery latencies. We discuss the difference between RTAP and classic task allocation problems and present a heuristic algorithm with a computational complexity of O(n log2n) to solve the problem. Extensive experiments were conducted to verify the correctness and effectiveness of our approach. It improves the resource usage by 15%–20% on average."
pub.1094125679,Scalable and Low-Latency Data Processing with Stream MapReduce,"We present StreamMapReduce, a data processing approach that combines ideas from the popular MapReduce paradigm and recent developments in Event Stream Processing. We adopted the simple and scalable programming model of MapReduce and added continuous, low-latency data processing capabilities previously found only in Event Stream Processing systems. This combination leads to a system that is efficient and scalable, but at the same time, simple from the user's point of view. For latency-critical applications, our system allows a hundred-fold improvement in response time. Notwithstanding, when throughput is considered, our system offers a ten-fold per node throughput increase in comparison to Hadoop. As a result, we show that our approach addresses classes of applications that are not supported by any other existing system and that the MapReduce paradigm is indeed suitable for scalable processing of real-time data streams."
pub.1136541454,Effective Stream Data Processing using Asynchronous Iterative Routing Protocol,"In the last decade, various distributed stream processing engines (DSPEs) were developed in order to process data streams in a flexible, scalable, fast and resilient manner. Coping with the increasing high-throughput and low-latency requirements of modern applications led to a careful investigation and re-design of new tools for stream processing. The first generation of tools, such as Apache Hadoop [19], Spark [20], Storm [18] and Kafka [14], were designed to split an incoming data stream into batches and to then synchronously execute their analytical workflows over these data batches. To overcome the limitations—primarily, the high latency—of this iterative form of bulk-synchronous processing (BSP), asynchronous stream-processing (ASP) engines such as Apache Flink [17] and Samza [15] have also recently emerged."
pub.1174031385,AgileDART: An Agile and Scalable Edge Stream Processing Engine,"Edge applications generate a large influx of sensor data at massive scales.
Under many time-critical scenarios, these massive data streams must be
processed in a very short time to derive actionable intelligence. However,
traditional data processing systems (e.g., stream processing systems,
cloud-based IoT data processing systems) are not well-suited for these edge
applications. This is because they often do not scale well with a large number
of concurrent stream queries, do not support low-latency processing under
limited edge computing resources, and do not adapt to the level of
heterogeneity and dynamicity commonly present in edge computing environments.
These gaps suggest a need for a new edge stream processing system that advances
the stream processing paradigm to achieve efficiency and flexibility under the
constraints presented by edge computing architectures.
  We present AgileDart, an agile and scalable edge stream processing engine
that enables fast stream processing of a large number of concurrently running
low-latency edge applications' queries at scale in dynamic, heterogeneous edge
environments. The novelty of our work lies in a dynamic dataflow abstraction
that leverages distributed hash table (DHT) based peer-to-peer (P2P) overlay
networks to automatically place, chain, and scale stream operators to reduce
query latencies, adapt to workload variations, and recover from failures; and a
bandit-based path planning model that can re-plan the data shuffling paths to
adapt to unreliable and heterogeneous edge networks. We show analytically and
empirically that AgileDart outperforms Storm and EdgeWise on query latency and
significantly improves scalability and adaptability when processing a large
number of real-world edge stream applications' queries."
pub.1145611846,Cost-aware & Fault-tolerant Geo-distributed Edge Computing for Low-latency Stream Processing,"The number of Internet-of-Things (IoT) devices is rapidly increasing with the growth of IoT applications in various domains. As IoT applications have a strong demand for low latency and high throughput computing, stream processing using edge computing resources is a promising approach to support low latency processing of large-scale data. Edge-based stream processing extends the capability of cloud-based stream processing by processing the data streams near the edge of the network. In this vision paper, we discuss a distributed stream processing framework that optimizes the performance of stream processing applications through a careful allocation of geo-distributed computing and network resources available in edge computing environments. The framework includes key optimizations in both the platform layer and the infrastructure layer. While the platform layer is responsible for converting the user program into a stream processing physical plan and optimizing the physical plan and operator placement, the infrastructure layer is responsible for provisioning geo-distributed resources to the platform layer. The framework optimizes the performance of stream query processing at the platform layer through its careful consideration of data locality and resource constraints during physical plan generation and operator placement and by incorporating resilience to deal with failures. The framework also includes techniques to dynamically determine the level of parallelism to adapt to changing workload conditions. At the infrastructure layer, the framework includes a novel model for allocating computing resources in edge and geo-distributed cloud computing environments by carefully considering latency and cost. End users benefit from the platform through reduced cost and improved user experience in terms of response time and latency."
pub.1147192382,Evaluating Micro-batch and Data Frequency for Stream Processing Applications on Multi-cores,"In stream processing, data arrives constantly and is often unpredictable. It can show large fluctuations in arrival frequency, size, complexity, and other factors. These fluctuations can strongly impact application latency and throughput, which are critical factors in this domain. Therefore, there is a significant amount of research on self-adaptive techniques involving elasticity or micro-batching as a way to mitigate this impact. However, there is a lack of benchmarks and tools for helping researchers to investigate micro-batching and data stream frequency implications. In this paper, we extend a benchmarking framework to support dynamic micro-batching and data stream frequency management. We used it to create custom benchmarks and compare latency and throughput aspects from two different parallel libraries. We validate our solution through an extensive analysis of the impact of micro-batching and data stream frequency on stream processing applications using Intel TBB and FastFlow, which are two libraries that leverage stream parallelism on multi-core architectures. Our results demonstrated up to 33% throughput gain over latency using micro-batches. Additionally, while TBB ensures lower latency, FastFlow ensures higher throughput in the parallel applications for different data stream frequency configurations."
pub.1169922449,Data-Aware Adaptive Compression for Stream Processing,"Stream processing has been in widespread use, and one of the most common application scenarios is SQL query on streams. By 2021, the global deployment of IoT endpoints reached 12.3 billion, indicating a surge in data generation. However, the escalating demands for high throughput and low latency in stream processing systems have posed significant challenges due to the increasing data volume and evolving user requirements. We present a compression-based stream processing engine, called CompressStreamDB, which enables adaptive fine-grained stream processing directly on compressed streams, to significantly enhance the performance of existing stream processing solutions. CompressStreamDB utilizes nine diverse compression methods tailored for different stream data types and integrates a cost model to automatically select the most efficient compression schemes. CompressStreamDB provides high throughput with low latency in stream SQL processing by identifying and eliminating redundant data among streams. Our evaluation demonstrates that CompressStreamDB improves average performance by 3.84× and reduces average delay by 68.0% compared to the state-of-the-art stream processing solution for uncompressed streams, along with 68.7% space savings. Besides, our edge trials show an average throughput/price ratio of 9.95× and a throughput/power ratio of 7.32× compared to the cloud design."
pub.1094429876,Elastic Stream Computing with Clouds,"Stream computing, also known as data stream processing, has emerged as a new processing paradigm that processes incoming data streams from tremendous numbers of sensors in a real-time fashion. Data stream applications must have low latency even when the incoming data rate fluctuates wildly. This is almost impossible with a local stream computing environment because its computational resources are finite. To address this kind of problem, we have devised a method and an architecture that transfers data stream processing to a Cloud environment as required in response to the changes of the data rate in the input data stream. Since a trade-off exists between application's latency and the economic costs when using the Cloud environment, we treat it as an optimization problem that minimizes the economic cost of using the Cloud. We implemented a prototype system using Amazon EC2 and an IBM System S stream computing system to evaluate the effectiveness of our approach. Our experimental results show that our approach reduces the costs by 80% while keeping the application's response latency low."
pub.1106821367,Reducing Tail Latencies While Improving Resiliency to Timing Errors for Stream Processing Workloads,"Stream processing is an increasingly popular model for online data processing that can be partitioned into streams of elements. It is commonly used in data analytics services, such as processing Twitter tweets. Current stream processing frameworks boast high throughput and low average latency. However, lower tail latencies and better real-time performance are desirable to stream processing users. In practice, there are issues that can affect the performance of these applications and cause unacceptable violations of real-time constraints. Some examples of these issues are garbage collection pauses and resource contention. In this paper, we propose applying redundancy in the data processing pipeline to increase the resiliency of stream processing applications to timing errors. This results in better real-time performance and a reduction in tail latency. We present a methodology and apply this redundancy in a framework based on Twitter's Heron. Then, we then evaluate the effectiveness of this technique against a range of injected timing errors using benchmarks from Intel's Storm Benchmark. Furthermore, we also study the potential effects of duplication when applied at different stages in the topology. Finally, we evaluate the additional overhead that duplicating tuples brings to a stream processing topology. Our results show that redundant tuple processing can effectively reduce the tail latency by up to 63% and that the number of missed deadlines can also be reduced by up to 94% in the best case. Overall we conclude that redundancy through duplicated tuples is indeed a powerful tool for increasing the resiliency to intermittent runtime timing errors."
pub.1144339476,A Topology-Aware Scheduling Strategy for Distributed Stream Computing System,"Reducing latency has become the focus of task scheduling research in distributed big data stream computing systems. Currently, most task schedulers in big data stream computing systems mainly focus on tasks assignment and implicitly ignore task topology which can have significant impact on the latency and energy efficiency. This paper proposes a topology-aware scheduling strategy to reduce the processing latency of stream processing systems. We construct the data stream graph as a directed acyclic graph and then, divide it using the graph Laplace algorithm. On the divided graph, tasks will be assigned with a low-latency scheduling strategy. We also provide a computing node selection strategy, which enables the system to run tasks on the topology with the least number of computing nodes. Based on this scheduling strategy, the tasks of the data stream graph can be redistributed and the scheduling mechanism can be optimized to minimize the system latency. The experimental results demonstrate the efficiency and effectiveness of the proposed strategy."
pub.1120986457,Latency-Aware Secure Elastic Stream Processing with Homomorphic Encryption,"Increasingly organizations are elastically scaling their stream processing applications into the infrastructure as a service clouds. However, state-of-the-art approaches for elastic stream processing do not consider the potential threats of exposing their data to third parties in cloud environments. We present the design and implementation of an Elastic Switching Mechanism for data stream processing which is based on homomorphic encryption (HomoESM). The HomoESM not only elastically scales data stream processing applications into public clouds but also preserves the privacy of such applications. Using a real-world test setup, which includes an E-mail Filter benchmark and a Web server access log processor benchmark (EDGAR), we demonstrate the effectiveness of our approach. Experiments on Amazon EC2 indicate that the proposed approach for homomorphic encryption provides a significant result which is 10–17% improvement in average latency in the case of E-mail Filter benchmark and EDGAR benchmark, respectively. Furthermore, EDGAR add/subtract operations, multiplication, and comparison operations showed up to 6.13%, 7.81%, and 26.17% average latency improvements, respectively. Finally, we evaluate the potential of scaling the homomorphic stream processor in the public cloud. These results indicate the potential for real-world deployments of secure elastic data stream processing applications."
pub.1134896062,RealSync: A Synchronous Multimodality Media Stream Analytic Framework for Real-Time Communications Applications,"While advancements in computing algorithms and hardware have enabled real-time stream analytics in videos, the information-rich audio bonded with video is still usually dropped and wasted. Processing both audio and video stream is not trivial as synchronizing multiple data streams creates much more difficulties than processing only one stream. In this paper, we designed and implemented a lightweight multimodal stream processing system that keeps both streams synchronized in processing and tested in a typical use case profanity filter. While it is inevitable to slow down certain processors to keep the steams synchronized, by careful butter setting, the overall latency is not affected (still 400-500ms). Besides achieving real-time processing, we located a core problem causing bursty audio latency and gave directions for further latency improvements."
pub.1106862596,Streams and Tables,"Stream processing has emerged as a paradigm for applications that require low-latency evaluation of operators over unbounded sequences of data. Defining the semantics of stream processing is challenging in the presence of distributed data sources. The physical and logical order of data in a stream may become inconsistent in such a setting. Existing models either neglect these inconsistencies or handle them by means of data buffering and reordering techniques, thereby compromising processing latency. In this paper, we introduce the Dual Streaming Model to reason about physical and logical order in data stream processing. This model presents the result of an operator as a stream of successive updates, which induces a duality of results and streams. As such, it provides a natural way to cope with inconsistencies between the physical and logical order of streaming data in a continuous manner, without explicit buffering and reordering. We further discuss the trade-offs and challenges faced when implementing this model in terms of correctness, latency, and processing cost. A case study based on Apache Kafka illustrates the effectiveness of our model in the light of real-world requirements."
pub.1132889849,Multi-directional CPU resource control in edge computing,"Edge computing promises a considerable reduction in latency and data volume by placing edge frameworks near the data source. Stream processing framework is a critical use case for edge computing, presenting in-situ data processing and low latency. However, careful CPU resource control is more important than cloud computing to provide effective multi-tenancy in resource-constrained edge computing. In this work, we investigate practical ways of controlling CPU resources of stream processing frameworks. Evaluation results from the different stream processor parallelism and cgroup parameters give a clear direction of designing an essential controller for stream processing frameworks in edge computing."
pub.1084030366,Optimization of data-intensive workflows in stream-based data processing models,"Stream computing applications require minimum latency and high throughput for efficiently processing real-time data. Typically, data-intensive applications where large datasets are required to be moved across execution nodes have low latency requirements. In this paper, a stream-based data processing model is adopted to develop an algorithm for optimal partitioning the input data such that the inter-partition data flow remains minimal. The proposed algorithm improves the execution of the data-intensive workflows in heterogeneous computing environments by partitioning the data-intensive workflow and mapping each partition on the available heterogeneous resources that offer minimum execution time. Minimum data movement between the partitions reduces the latency, which can be further reduced by applying advanced data parallelism techniques. In this paper, we apply data parallelism technique to the bottleneck (most compute-intensive) task in each partition that significantly reduces the latency. We study the effectiveness and the performance of the proposed approach by using synthesized workflows and real-world applications, such as Montage and Cybershake. Our evaluation shows that the proposed algorithm provides schedules with approximately 12% reduced latency and nearly 17% enhanced throughput as compared to the existing state of the art algorithms."
pub.1095668733,SpanEdge: Towards Unifying Stream Processing over Central and Near-the-Edge Data Centers,"In stream processing, data is streamed as a continuous flow of data items, which are generated from multiple sources and geographical locations. The common approach for stream processing is to transfer raw data streams to a central data center that entails communication over the wide-area network (WAN). However, this approach is inefficient and falls short for two main reasons: (i) the burst in the amount of data generated at the network edge by an increasing number of connected devices, (ii) the emergence of applications with predictable and low latency requirements. In this paper, we propose SpanEdge, a novel approach that unifies stream processing across a geo-distributed infrastructure, including the central and near-the-edge data centers. SpanEdge reduces or eliminates the latency incurred by WAN links by distributing stream processing applications across the central and the near-the-edge data centers. Furthermore, SpanEdge provides a programming environment, which allows programmers to specify parts of their applications that need to be close to the data source. Programmers can develop a stream processing application, regardless of the number of data sources and their geographical distributions. As a proof of concept, we implemented and evaluated a prototype of SpanEdge. Our results show that SpanEdge can optimally deploy the stream processing applications in a geo-distributed infrastructure, which significantly reduces the bandwidth consumption and the response latency."
pub.1127976036,Message Latency-Based Load Shedding Mechanism in Apache Kafka,"Apache Kafka is a distributed message queuing platform that delivers data streams in real time. Through the distributed processing technology, Kafka has the advantage of delivering very large data streams very fast. However, when the data explosion occurs, the message latency largely increases and the system might be interrupted. This paper proposes a load shedding engine of Kafka that solves this message latency problem. The load shedding engine solves the data explosion problem by introducing a simple mechanism that restricts the transmission of some messages when the latency exceeds the given threshold in the Kafka’s producer. Experiments with Apache Storm-based real-time applications show that the latency does not continuously increase due to the load shedding function in both single and multiple data streams, and maintains a constant level. This is the first attempt to apply a load shedding technique to Kafka-based real-time stream processing, providing simple and efficient data explosion control."
pub.1095425645,Stream processing in data-driven computational science,"The use of real-time data streams in data-driven computational science is driving the need for stream processing tools that work within the architectural framework of the larger application. Data stream processing systems are beginning to emerge in the commercial space, but these systems fail to address the needs of large-scale scientific applications. In this paper we illustrate the unique needs of large-scale data driven computational science through an example taken from weather prediction and forecasting. We apply a realistic workload from this application against our Calder stream processing system to determine effective throughput, event processing latency, data access scalability, and deployment latency.11This work is supported in part by NSF grants EIA-0202048 and CDA-0116050, and DOE DE-FG02-04ER25600. This work is supported in part by NSF grants EIA-0202048 and CDA-0116050, and DOE DE-FG02-04ER25600."
pub.1015268608,Managing parallelism for stream processing in the cloud,"Stream processing applications run continuously and have varying load. Cloud infrastructures present an attractive option to meet these fluctuating computational demands. Coordinating such resources to meet end-to-end latency objectives efficiently is important in preventing the frivolous use of cloud resources. We present a framework that parallelizes and schedules workflows of stream operators, in real-time, to meet latency objectives. It supports data- and task-parallel processing of all workflow operators, by all computing nodes, while maintaining the ordering properties of sorted data streams. We show that a latency-oriented operator scheduling policy coupled with the diversification of computing node responsibilities encourages parallelism models that achieve end-to-end latency-minimization goals. We demonstrate the effectiveness of our framework with preliminary experimental results using a variety of real-world applications on heterogeneous clusters."
pub.1095218809,Task Allocation for Stream Processing with Recovery Latency Guarantee,"Stream processing applications continuously process large amounts of online streaming data in real-time or near real-time. They have strict latency constraints, but they are also vulnerable to failures. Failure recoveries may slow down the entire processing pipeline and break latency constraints. Upstream backup is one of the most widely applied fault-tolerant schemes for stream processing systems. It introduces complex backup dependencies to tasks, and increases the difficulty of controlling recovery latencies. Moreover, when dependent tasks are located on the same processor, they fail at the same time in processor-level failures, bringing extra recovery latencies that increase the impacts of failures. This paper presents a correlated failure effect model to describe the recovery latency of a stream topology in processor-level failures for an allocation plan. We introduce a Recovery-latency-aware Task Allocation Problem (RTAP) that seeks task allocation plans for stream topologies that will achieve guaranteed recovery latencies. We present a heuristic algorithm with a computational complexity of O(nlog2n) to solve the problem. Extensive experiments were conducted to verify the correctness and effectiveness of our approach."
pub.1087303136,Minimum Backups for Stream Processing With Recovery Latency Guarantees,"The stream processing model continuously processes online data in an on-pass fashion that can be more vulnerable to failures than other big-data processing schemes. Existing fault-tolerant (FT) approaches have been presented to enhance the reliability of stream processing systems. However, the fundamental tradeoff between recovery latency and FT overhead is still unclear, so these scheme cannot provide recovery latency guarantees. This paper introduces the FT Configuration (FTC) problem and presents a solution for guaranteed recovery latency with minimum backups. A failure effect model is presented to describe the relationship between recovery latency and FTC (the amount and locations of backups). With this model, we design an algorithm to compute FTCs for different types of stream topologies according to recovery latency requirements. Extensive experiments are conducted to verify the correctness and effectiveness of our approach. We prove that our algorithm guarantees recovery latencies for all directed acyclic graph (DAG) stream topologies. For line(s) and tree topologies, our algorithm solves the FTC problem with a time complexity of ${O(N)}$ . For a general DAG topology, a heuristic function is used to generate FTCs. This causes fewer than 10 more backups on average compared to the optimal solution with a time complexity of $O(N^2)$."
pub.1009316917,Demo,"MapReduce is a popular scalable processing framework for large-scale data. In this paper we demonstrate Enorm, which represents our efforts on rectifying the traditional batch-oriented MapReduce framework for low-latency data stream processing. Most existing work have focused on how to extend the MapReduce framework for low-latency data stream processing, but overlooked the problem of obtaining runtime elasticity. The demonstration focuses on two important features in Enorm. (1) sharing aggregate computations among overlapping windows and (2) runtime elasticity."
pub.1142447581,Amnis: Optimized stream processing for edge computing,"The proliferation of Internet-of-Things (IoT) devices is rapidly increasing the demands for efficient processing of low latency stream data generated close to the edge of the network. Edge computing-based stream processing techniques that carefully consider the heterogeneity of the computational and network resources available in the infrastructure provide significant benefits in optimizing the throughput and end-to-end latency of the data streams. In this paper, we propose a novel stream query processing framework called Amnis that optimizes the performance of the stream processing applications through a careful allocation of computational and network resources available at the edge. The Amnis approach differentiates itself through its consideration of data locality and resource constraints during physical plan generation and operator placement for the stream queries. Additionally, Amnis considers the coflow dependencies to optimize the network resource allocation through an application-level rate control mechanism. We implement a prototype of Amnis in Apache Storm. Our performance evaluation carried out in a real testbed shows that the proposed techniques achieve as much as 200X improvement on the end-to-end latency and 10X improvement on the overall throughput compared to the default resource aware scheduler in Storm."
pub.1126759702,Towards comparison of real time stream processing engines,"The real time stream processing engines are developed for different specific use cases, which incorporates various domains such as, IOT, finance, advertisement, telecommunications, healthcare etc. These stream processing engines are based on distributed processing models, where unbounded data streams are processed. Semantics of data stream is determined after complete scanning of whole data sets, which becomes inconvenient in real time stream processing to process entire data stream at once. Windowing mechanisms are used for processing data stream in a predefine topology with fixed number of operations such as, join, aggregate, filter etc. In this paper, a comparative study is performed with existing stream processing engines. This comparison provides a direction for choosing an appropriate stream processing engine. A modified master-slave model for stream processing is discussed for reducing latency, improving scalability and fault tolerance."
pub.1170724029,Latency-Aware Placement of Stream Processing Operators,"The rise of the Internet of Things and Fog computing has increased substantially the number of interconnected devices at the edge of the network. As a result, a large amount of computations is now performed in the fog generating vast amounts of data. To process this data in near real time, stream processing is typically employed due to its efficiency in handling continuous streams of information in a scalable manner. However, most stream processing approaches do not consider the underlying network devices as candidate resources for processing data. Moreover, many existing works do not take into account the incurred network latency of performing computations on multiple devices in a distributed way. Consequently, the fog computing resources may not be fully exploited by existing stream processing approaches. To avoid this, we formulate an optimization problem for utilizing the existing fog resources, and we design heuristics for solving this problem efficiently. Furthermore, we integrate our heuristics into Apache Storm, and we perform experiments that show latency-related benefits compared to alternatives."
pub.1017208547,Quality-driven processing of sliding window aggregates over out-of-order data streams,"One fundamental challenge in data stream processing is to cope with the ubiquity of disorder of tuples within a stream caused by network latency, operator parallelization, merging of asynchronous streams, etc. High result accuracy and low result latency are two conflicting goals in out-of-order stream processing. Different applications may prefer different extent of trade-offs between the two goals. However, existing disorder handling solutions either try to meet one goal to the extreme by sacrificing the other, or try to meet both goals but have shortcomings including unguaranteed result accuracy or increased complexity in operator implementation and application logic. To meet different application requirements on the latency versus result accuracy trade-off in out-of-order stream processing, in this paper, we propose to make this trade-off user-configurable. Particularly, focusing on sliding window aggregates, we introduce AQ-K-slack, a buffer-based quality-driven disorder handling approach. AQ-K-slack leverages techniques from the fields of sampling-based approximate query processing and control theory. It can adjust the input buffer size dynamically to minimize the result latency, while respecting user-specified threshold on relative errors in produced query results. AQ-K-slack requires no a priori knowledge of disorder characteristics of data streams, and imposes no changes to the query operator implementation or the application logic. Experiments over real-world out-of-order data streams show that, compared to the state-of-art, AQ-K-slack can reduce the average buffer size, thus the average result latency, by at least 51% while respecting user-specified requirement on the accuracy of query results."
pub.1110930824,A Pareto-Efficient Algorithm for Data Stream Processing at Network Edges,"Data stream processing has received considerable attention from both research community and industry over the last years. Since latency is a key issue in data stream processing environments, the majority of the works existing in the literature focus on minimizing the latency experienced by the users. The aforementioned minimization takes place by assigning the data stream processing components close to data sources. Server consolidation is also a key issue for drastically reducing energy consumption in computing systems. Unfortunately, energy consumption and latency are two objective functions that may be in conflict with each other. Therefore, when the target function is to minimize energy consumption, the delay experienced by users may be considerable high, and the opposite. For the above reason there is a dire need to design strategies such that by targeting the minimization of energy consumption, there is a graceful degradation in latency, as well as the opposite. To achieve the above, we propose a Pareto-efficient algorithm that tackles the problem of data processing tasks placement simultaneously in both dimensions regarding the energy consumption and latency. The proposed algorithm outputs a set of solutions that are not dominated by any solution within the set regarding energy consumption and latency. The experimental results show that the proposed approach is superior against single-solution approaches because by targeting one objective function the other one can be gracefully degraded by choosing the appropriate solution."
pub.1120929295,NARPCA: Neural Accumulate-Retract PCA for Low-Latency High-Throughput Processing on Datastreams,"The increasingly interconnected and instrumented world, provides a deluge of data generated by multiple sensors in the form of continuous streams. Efficient stream processing needs control over the number of useful variables. This is because maintaining data structure in reduced sub-spaces, given that data is generated at high frequencies and is typically follows non-stationary distributions, brings new challenges for dimensionality reduction algorithms. In this work we introduce NARPCA, a neural network streaming PCA algorithm capable to explain the variance-covariance structure of a set of variables in a stream through linear combinations. The essentially neural-based algorithm is leveraged by a novel incremental computation method and system operating on data streams and capable of achieving low-latency and high-throughput when learning from data streams, while maintaining resource usage guarantees. We evaluate NARPCA in real-world data experiments and demonstrate low-latency (millisecond level) and high-throughput (thousands events/second) for simultaneous eigenvalues and eigenvectors estimation in a multi-class classification task."
pub.1038684258,Nephele streaming: stream processing under QoS constraints at scale,"The ability to process large numbers of continuous data streams in a near-real-time fashion has become a crucial prerequisite for many scientific and industrial use cases in recent years. While the individual data streams are usually trivial to process, their aggregated data volumes easily exceed the scalability of traditional stream processing systems.At the same time, massively-parallel data processing systems like MapReduce or Dryad currently enjoy a tremendous popularity for data-intensive applications and have proven to scale to large numbers of nodes. Many of these systems also provide streaming capabilities. However, unlike traditional stream processors, these systems have disregarded QoS requirements of prospective stream processing applications so far.In this paper we address this gap. First, we analyze common design principles of today’s parallel data processing frameworks and identify those principles that provide degrees of freedom in trading off the QoS goals latency and throughput. Second, we propose a highly distributed scheme which allows these frameworks to detect violations of user-defined QoS constraints and optimize the job execution without manual interaction. As a proof of concept, we implemented our approach for our massively-parallel data processing framework Nephele and evaluated its effectiveness through a comparison with Hadoop Online.For an example streaming application from the multimedia domain running on a cluster of 200 nodes, our approach improves the processing latency by a factor of at least 13 while preserving high data throughput when needed."
pub.1148837560,Elastic Resource Allocation Based on Dynamic Perception of Operator Influence Domain in Distributed Stream Processing,"With the development of distributed stream processing systems, elastic resource allocation has become a powerful means to deal with the fluctuating data stream. The existing methods either focus on a single operator or only consider the static correlation between operators to perform elastic scaling. However, they ignore the dynamic correlation between operators in data stream processing applications, which leads to lagging and inaccuracy resource allocation, increasing processing latency. To address these issues, we propose an elastic resource allocation method, which is based on the dynamic perception of operator influence domain, to perform resource allocation dynamically and in advance. The experimental results show that compared with the existing methods, our method not only guarantees that the end-to-end latency meets QoS requirements but also reduces resource utilization."
pub.1108059986,Latency-Aware Placement of Data Stream Analytics on Edge Computing,"The interest in processing data events under stringent time constraints as they arrive has led to the emergence of architecture and engines for data stream processing. Edge computing, initially designed to minimize the latency of content delivered to mobile devices, can be used for executing certain stream processing operations. Moving operators from cloud to edge, however, is challenging as operator-placement decisions must consider the application requirements and the network capabilities. In this work, we introduce strategies to create placement configurations for data stream processing applications whose operator topologies follow series parallel graphs. We consider the operator characteristics and requirements to improve the response time of such applications. Results show that our strategies can improve the response time in up to 50% for application graphs comprising multiple forks and joins while transferring less data and better using the resources."
pub.1093187521,Buffering Strategies for Ultra High-Throughput Stream Processing,"The insatiable demand for more data content and higher video resolution over the Internet requires a corresponding increase in communication data rates. Handling the massive acquisition, aggregation, and propagation of these communication signals requires high-throughput data processing. ASICs and FPGAs are the only devices capable of processing the data in real time at the highest throughput requirements. Traditionally, when data throughput exceeds the clock rate of these devices, data are sourced in parallel streams. However, current high-level synthesis tools fail to efficiently handle data distributed across parallel streams while trying to meet low latency and real time requirements. This paper presents an algorithm to automate efficient allocation of data across parallel streams and computations with varying memory access requirements. The proposed approach exceeds the performance of current high-level tools by 57% in latency and 55% memory usage for two samples per clock, with more impressive reductions of 88% in latency and 83% in memory usage at eight samples per clock."
pub.1142694898,Q-Spark: QoS Aware Micro-batch Stream Processing System Using Spark,"Unlike the event-driven stream processing systems, the micro-batch stream processing systems collect input data for a certain period of time before processing. This is because they focus on improving the throughput of the entire system rather than reducing the latency of each data. However, ingesting a continuous stream of data and its real-time analysis is also necessary in micro-batch stream processing systems where reducing the latency is more important than improving the throughput. This paper presents Q-Spark, a QoS (Quality of Service) aware micro-batch stream processing system that is implemented on Apache Spark. The main idea of Q - Spa rk design is to set a deadline time for each query and dynamically adjust the batch size so as not to exceed it. Since Q - Spa r k executes a micro-batch by buffering as much as possible until the deadline set for each query is exceeded, it guarantees the QoS requirement of each query while maintaining the throughput as much as the original Spark batching mechanism. Experimental results show that the tail latency of Q - Spa rk is always bound to the deadline compared to the original Spark where data is buffered using triggers for a certain period. As a result, Q - Spa r k reduces the tail latency per query by up to 75%, while maintaining the throughput stably compared to the original Spark without the concept of a deadline."
pub.1105897819,Deterministic Model for Distributed Speculative Stream Processing,Users of modern distributed stream processing systems have to choose between non-deterministic computations and high latency due to a need in excessive buffering. We introduce a speculative model based on MapReduce-complete set of operations that allows us to achieve determinism and low-latency. Experiments show that our prototype can outperform existing solutions due to low overhead of optimistic synchronization.
pub.1108003713,Cutting the Tail: Designing High Performance Message Brokers to Reduce Tail Latencies in Stream Processing,"Over the last decade, organizations have become heavily reliant on providing near-instantaneous insights to the end user based on vast amounts of data collected from various sources in real-time. In order to accomplish this task, a stream processing pipeline is constructed, which in its most basic form, consists of a Stream Processing Engine (SPE) and a Message Broker (MB). The SPE is responsible for performing actual computations on the data and providing insights from it. MB, on the other hand, acts as an intermediate queue to which data is written by ephemeral sources and then fetched by the SPE to perform computations on. Due to the inherent real-time nature of such a pipeline, low latency is a highly desirable feature for them. Thus, several existing research works in the community focus on improving latency and throughput of the streaming pipeline. However, there is a dearth of studies optimizing the tail latencies of such pipelines. Moreover, the root cause of this high tail latency is still vague. In this paper, we propose a model-based approach to analyze in-depth the reasons behind high tail latency in streaming systems such as Apache Kafka. Having found the MB to be a major contributor of messages with high tail latencies in a streaming pipeline, we design and implement an RDMA-enhanced high-performance MB, called Frieda, with the higher goal of accelerating any arbitrary stream processing pipeline regardless of the SPE used. Our experiments show a reduction of up to 98% in 99.9th percentile latency for micro-benchmarks and up to 31% for full-fledged stream processing pipeline constructed using Yahoo! Streaming Benchmark."
pub.1111258594,Reducing Tail Latencies While Improving Resiliency to Timing Errors for Stream Processing Workloads,"Stream processing is an increasingly popular model for online data processing that can be partitioned into streams of elements. It is commonly used in real-time data analytics services, such as processing Twitter tweets and Internet of Things (IoT) device feeds. Current stream processing frameworks boast high throughput and low average latency. However, users of these frameworks may desire lower tail latencies and better real-time performance for their applications. In practice, there are a number of errors that can affect the performance of stream processing applications, such as garbage collection and resource contention. For some applications, these errors may cause unacceptable violations of real-time constraints. In this paper we propose applying redundancy in the data processing pipeline to increase the resiliency of stream processing applications to timing errors. This results in better realtime performance and a reduction in tail latency. We present a methodology and apply this redundancy in a framework based on Twitter's Heron. Finally, we evaluate the effectiveness of this technique against a range of injected timing errors using benchmarks from Intel's Storm Benchmark. Our results show that redundant tuple processing can effectively reduce the tail latency, and that the number of missed deadlines can also be reduced by up to 94% in the best case. We also study the potential effects of duplication when applied at different stages in the topology. For the topologies in this paper, we further observe that duplication is most effective when computation is redundant at the first bolt. Finally, we evaluate the additional overhead that duplicating tuples brings to a stream processing topology. Our results also show that computation overhead scales slower than communication, and that the real-time performance is improved in spite of the overheads. Overall we conclude that redundancy through duplicated tuples is indeed a powerful tool for increasing the resiliency to intermittent runtime timing errors."
pub.1090555555,"Online Scheduling and Interference Alleviation for Low-Latency, High-Throughput Processing of Data Streams","Data Streams occur naturally in several observational settings and often need to be processed with a low latency. Streams pose unique challenges: they have no preset lifetimes, the traffic on these streams may be bursty, and data arrival rates on these streams can be quite high. Furthermore, stream processing computations are generally stateful where the outcome of processing a data stream packet depends on the state that builds up within the computation over multiple, successive rounds of execution. As the number of streams increases, stream processing computations need to be orchestrated over a collection of machines. Achieving timeliness and high throughput in such settings is a challenge. Optimal scheduling of stream processing computations is an instance of the resource constrained scheduling problem, and depending on the precise formulation of the problem can be characterized as either NP-Complete or NP-Hard. We have designed an algorithm for online scheduling of stream processing computations. Our algorithm focuses on reducing interference that adversely impacts performance of stream processing computations. Our measure of interference is based on stream packet arrivals at a particular machine, the accompanying resource utilization encompassing CPU, memory and network utilization, and the resource utilization at machines comprising the cluster. Our algorithm performs continuous, incremental detection of interference experienced by computations and performing migrations to alleviate them."
pub.1019780188,Massively-parallel stream processing under QoS constraints with Nephele,"Today, a growing number of commodity devices, like mobile phones or smart meters, is equipped with rich sensors and capable of producing continuous data streams. The sheer amount of these devices and the resulting overall data volumes of the streams raise new challenges with respect to the scalability of existing stream processing systems. At the same time, massively-parallel data processing systems like MapReduce have proven that they scale to large numbers of nodes and efficiently organize data transfers between them. Many of these systems also provide streaming capabilities. However, unlike traditional stream processors, these systems have disregarded QoS requirements of prospective stream processing applications so far. In this paper we address this gap. First, we analyze common design principles of today's parallel data processing frameworks and identify those principles that provide degrees of freedom in trading off the QoS goals latency and throughput. Second, we propose a scheme which allows these frameworks to detect violations of user-defined latency constraints and optimize the job execution without manual interaction in order to meet these constraints while keeping the throughput as high as possible. As a proof of concept, we implemented our approach for our parallel data processing framework Nephele and evaluated its effectiveness through a comparison with Hadoop Online. For a multimedia streaming application we can demonstrate an improved processing latency by factor of at least 15 while preserving high data throughput when needed."
pub.1095852983,Intelligent Clustering Guided Adaptive Prefetching and Buffer Management for Stream Processing,"Real-time stream data processing is required to handle big data with low latency to process a large amount of incessant streaming data. We propose a clustering based intel-igent prefetching scheme and its associated DRAM-PCM (phase change memory) hybrid memory management policy especially for stream processing. To alleviate stream processing's burden of requests accessing main memory, an intelligence prefetching technique is designed to reflect stream processing behavior to reduce the amount of memory access by improving buffer hit ratio. Because stream processing has to guarantee relatively small latency to the users, it is especially important for stream processsing to process the stream data in strict time. By using a hybrid memory structure, we take such advantages like high performance, low energy consumption, and memory scalability. And by using clustering based smart prefetching, we could improve buffer hit rate and because of this, overall system performance can be enhanced. Our proposed architecture and clustering based prefetching method can improve system performance by 1.15 times, compared with hybrid memory and buffer architecture without any prefetching scheme of conventional model and also energy consumption by 1.23 times, compared with DRAM only conventional model."
pub.1120328282,Towards a Framework for Data Stream Processing in the Fog,"In volatile data streams as encountered in the Internet of Things (IoT), the data volume to be processed changes permanently. Hence, to ensure timely data processing, there is a need to reconfigure the computational resources used for processing data streams. Up to now, mostly cloud-based computational resources have been utilized for this. However, cloud data centers are usually located far away from IoT data sources, which leads to an increase in latency since data needs to be sent from the data sources to the cloud and back. With the advent of fog computing, it is possible to perform data processing in the cloud as well as at the edge of the network, i. e., by exploiting the computational resources offered by networked devices. This leads to decreased latency and a lower communication overhead. Despite this, there is currently a lack of approaches to data stream processing which explicitly exploit the computational resources available in the fog.Within this paper, we consider the usage of fog-based computational resources for the purposes of data stream processing in the IoT. For this, we introduce a representative application scenario in the field of Industry 4.0 and present a framework for stream processing in the fog."
pub.1086126522,Maximizing Determinism in Stream Processing Under Latency Constraints,"The problem of coping with the demands of determinism and meeting latency constraints is challenging in distributed data stream processing systems that have to process high volume data streams that arrive from different unsynchronized input sources. In order to deterministically process the streaming data, they need mechanisms that synchronize the order in which tuples are processed by the operators. On the other hand, achieving real-time response in such a system requires careful tradeoff between determinism and low latency performance. We build on a recently proposed approach to handle data exchange and synchronization in stream processing, namely ScaleGate, which comes with guarantees for determinism and an efficient lock-free implementation, enabling high scalability. Considering the challenge and trade-offs implied by real-time constraints, we propose a system which comprises (a) a novel data structure called Slack-ScaleGate (SSG), along with its algorithmic implementation; SSG enables us to guarantee the deterministic processing of tuples as long as they are able to meet their latency constraints, and (b) a method to dynamically tune the maximum amount of time that a tuple can wait in the SSG data-structure, relaxing the determinism guarantees when needed, in order to satisfy the latency constraints. Our detailed experimental evaluation using a traffic monitoring application deployed in the city of Dublin, illustrates the working and benefits of our approach."
pub.1166701719,SPinDP: A High-Speed Distributed Processing Platform for Sampling and Filtering Data Streams,"Recently, there has been an explosive generation of streaming data in various fields such as IoT and network attack detection, medical data monitoring, and financial trend analysis. These domains require precise and rapid analysis capabilities by minimizing noise from continuously generated raw data. In this paper, we propose SPinDP (Stream Purifier in Distributed Platform), an open source-based high-speed stream purification platform, to support real-time stream purification. SPinDP consists of four major components, Data Stream Processing Engine, Purification Library, Plan Manager, and Shared Storage, and operates based on open-source systems including Apache Storm and Apache Kafka. In these components, stream processing throughput and latency are critical performance metrics, and SPinDP significantly enhances distributed processing performance by utilizing the ultra-high-speed network RDMA (Remote Direct Memory Access). For the performance evaluation, we use a distributed cluster environment consisting of nine nodes, and we show that SPinDP’s stream processing throughput is more than 28 times higher than that of the existing Ethernet environment. SPinDP also significantly reduces the processing latency by more than 2473 times on average. These results indicate that the proposed SPinDP is an excellent integrated platform that can efficiently purify high-speed and large-scale streams through RDMA-based distributed processing."
pub.1054506200,Dynamic Low-Latency Distributed Event Processing of Sensor Data Streams,"Event-based systems (EBS) are used to detect meaningful events with low latency in surveillance, sports, finances, etc. However, with rising data and event rates and with correlations among these events, processing can no longer be sequential but it needs to be distributed. However, naively distributing existing approaches not only cause failures as their order-less processing of events cannot deal with the ubiquity of out-of-order event arrival. It is also hard to achieve a minimal detection latency.This paper illustrates the combination of our building blocks towards a scalable publish/ subscribe-based EBS that analyzes high data rate sensor streams with low latency: a parameter calibration to put out-of-order events in order without a-priori knowledge on event delays, a runtime migration of event detectors across system resources, and an online optimization algorithm that uses migration to improve performance.We evaluate our EBS and its building blocks on position data streams from a Realtime Locating System in a sports application."
pub.1122235045,When FPGA-accelerator meets Stream Data Processing in the Edge,"Today, stream data applications represent the killer applications for Edge computing: placing computation close to the data source facilitates real-time analysis. Previous efforts have focused on introducing light-weight distributed stream processing (DSP) systems and dividing the computation between Edge servers and the clouds. Unfortunately, given the limited computation power of Edge servers, current efforts may fail in practice to achieve the desired latency of stream data applications. In this vision paper, we argue that by introducing FPGAs in Edge servers and integrating them into DSP systems, we might be able to realize stream data processing in Edge infrastructures. We demonstrate that through the design, implementation, and evaluation of F-Storm, an FPGA-accelerated and general-purpose distributed stream processing system on Edge servers. F-Storm integrates PCIe-based FPGAs into Edge-based stream processing systems and provides accelerators as a service for stream data applications. We evaluate F-Storm using different representative stream data applications. Our experiments show that compared to Storm, F-Storm reduces the latency by 36% and 75% for matrix multiplication and grep application. It also obtains 1.4x and 2.1x improvement for these two applications, respectively. We expect this work to accelerate progress in this domain."
pub.1094978770,An FPGA-Based Low-Latency Network Processing for Spark Streaming,"Low-latency stream data processing is a key enabler for on-line data analysis applications, such as detecting anomaly conditions and change points from stream data continuously generated from sensors and networking services. Existing stream processing frameworks are classified into micro-batch and one-at-a-time processing methodology. Apache Spark Streaming employs the micro-batch methodology, where data analysis is repeatedly performed for a series of data arrived during a short time period, called a micro batch. A rich set of data analysis libraries provided by Spark, such as machine learning and graph processing, can be applied for the micro batches. However, a drawback of the micro-batch processing methodology is a high latency for detecting anomaly conditions and change points. This is because data are accumulated in a micro batch (e.g., 1 sec length) and then data analysis is performed for the micro batch. In this paper, we propose to offload one-at-a-time methodology analysis functions on an FPGA-based 10Gbit Ethernet network interface card (FPGA NIC) in cooperation with Spark Streaming framework, in order to significantly reduce the processing latency and improve the processing throughput. We implemented word count and change-point detection applications on Spark Streaming with our FPGA NIC, where a one-at-a-time methodology analysis logic is implemented. Experiment results demonstrates that the word count throughput is improved by 22x and the change-point detection latency is reduced by 94.12 % compared to the original Spark Streaming. Our approach can complement the existing micro-batch methodology data analysis framework with ultra low latency one-at-a-time methodology logic."
pub.1138148988,A Two-Sided Matching Model for Data Stream Processing in the Cloud-Fog Continuum,"Latency-sensitive and bandwidth-intensive stream processing applications are
dominant traffic generators over the Internet network. A stream consists of a
continuous sequence of data elements, which require processing in nearly
real-time. To improve communication latency and reduce the network congestion,
Fog computing complements the Cloud services by moving the computation towards
the edge of the network. Unfortunately, the heterogeneity of the new Cloud-Fog
continuum raises important challenges related to deploying and executing data
stream applications. We explore in this work a two-sided stable matching model
called Cloud-Fog to data stream application matching (CODA) for deploying a
distributed application represented as a workflow of stream processing
microservices on heterogeneous Cloud-Fog computing resources. In CODA, the
application microservices rank the continuum resources based on their
microservice stream processing time, while resources rank the stream processing
microservices based on their residual bandwidth. A stable many-to-one matching
algorithm assigns microservices to resources based on their mutual preferences,
aiming to optimize the complete stream processing time on the application side,
and the total streaming traffic on the resource side. We evaluate the CODA
algorithm using simulated and real-world Cloud-Fog scenarios. We achieved 11 to
45% lower stream processing time and 1.3 to 20% lower streaming traffic
compared to related state-of-the-art approaches."
pub.1140131303,A Two-Sided Matching Model for Data Stream Processing in the Cloud – Fog Continuum,"Latency-sensitive and bandwidth-intensive stream processing applications are dominant traffic generators over the Internet network. A stream consists of a continuous sequence of data elements, which require processing in nearly real-time. To improve communication latency and reduce the network congestion, Fog computing complements the Cloud services by moving the computation towards the edge of the network. Unfortunately, the heterogeneity of the new Cloud – Fog continuum raises important challenges related to deploying and executing data stream applications. We explore in this work a two-sided stable matching model called Cloud – Fog to data stream application matching (CODA) for deploying a distributed application rep-resented as a workflow of stream processing microservices on heterogeneous computing continuum resources. In CODA, the application microservices rank the continuum resources based on their microservice stream processing time, while resources rank the stream processing microservices based on their residual bandwidth. A stable many-to-one matching algorithm assigns microservices to resources based on their mutual preferences, aiming to optimize the complete stream processing time on the application side, and the total streaming traffic on the resource side. We evaluate the CODA algorithm using simulated and real-world Cloud – Fog experimental scenarios. We achieved 11-45% lower stream processing time and 1.3-20% lower streaming traffic compared to related state-of-the-art approaches."
pub.1131449164,Deterministic Time-Series Joins for Asynchronous High-Throughput Data Streams,"A variety of data stream problems that affect two or more data streams rely on joining them based on a common or similar timing attribute. With the advent of stream processing frameworks like Apache Spark and Apache Flink within the last years, processing of streamed data has become much easier. Repeated processing of relatively small data batches in so-called windows increases flexibility with respect to implementation and task distribution across multiple nodes. Using event times instead of ingestion times avoids, among other problems, incorrect joins. However, in this work we argue that batch-processing leads to a significant trade-off between increased computational complexity and latency of the resulting join pairs. A concept for time-series joins of streaming data is presented. This concept, which is built upon a resilient data stream framework, minimizes both the computational costs and latency times. It uses the guarantees associated with this underlying framework to join the data records deterministically according to event times instead of processing times. This work represents a work-in-progress paper, as detailed benchmarks are pending."
pub.1094580924,Elastic Scaling for Distributed Latency-Sensitive Data Stream Operators,"High-volume data streams are straining the limits of stream processing frameworks which need advanced parallel processing capabilities to withstand the actual incoming bandwidth. Parallel processing must be synergically integrated with elastic features in order dynamically scale the amount of utilized resources by accomplishing the Quality of Service goals in a cost-effective manner. This paper proposes a control-theoretic strategy to drive the elastic behavior of latency-sensitive streaming operators in distributed environments. The strategy takes scaling decisions in advance by relying on a predictive model-based approach. Our ideas have been experimentally evaluated on a cluster using a real-world streaming application fed by synthetic and real datasets. The results show that our approach takes the strictly necessary reconfigurations while providing reduced resource consumption. Furthermore, it allows the operator to meet desired average latency requirements with a significant reduction in the experienced latency jitter."
pub.1121476508,A latency-sensitive elastic adaptive scheduling in distributed stream computing systems,"With the massive growth of big data applications, the requirement for data processing speed is getting higher and higher in stream computing systems. The Storm, as one of the most popular distributed stream computing systems, has received more attention. However, the Storm’s traditional scheduling strategy is not ideal for processing a large volume of streaming data. The resource scheduling in a distributed stream computing system should consider not only node allocation status but also fluctuating input rates of data stream. To address this problem, this paper has completed the following work: (1) A performance model La-Stream (latency-sensitive elastic adaptive scheduling) is proposed and built by adopting a quantitative method for calculating the amount of computation required between task map nodes and node communication. (2) A La-Stream based algorithm is proposed. The algorithm dynamically plans a resource allocation scheme with minimal data processing latency among available resources to achieve optimal allocation. (3) Three functional modules of La-steam are proposed and implemented: module Monitor, module optimizer and module Scheduler. The three modules are integrated into the Storm platform with minimal overhead. Several sets of experiments are conducted, verifying the feasibility and effectiveness of La-Stream."
pub.1100847652,Ultra-Low Latency Continuous Block-Parallel Stream Windowing Using FPGA On-Chip Memory,"In this paper, we propose and demonstrate a realtime ultra-fast multi-data stream processing methodology on FPGA called “SWIM” (Stream Windowing on Interleaved Memory). The method exploits the flexible on-chip block memory fabric on existing FPGA architectures to achieve ultra-low-latency and fully pipelined continuous data flow while maintaining linear spatial locality of data for efficient data addressing and processing. The SWIM method is directly applicable to many practical applications such as real-time stencil computing, streaming image data processing, as well as closed loop-control systems that require ultra-low latency interleaved access and processing of high-speed sensor data. We demonstrate two practical cases on actual FPGA for generic 3-by-3 2-D convolution filter and image super-resolution method using pixel interleaving. Both memory usage and latency scales linearly with window height, or width of the 2-D input data set. The generic implementation of SWIM on FPGA showed impressive worst-case operation frequency of 410 MHz and uses $9.0\times$ and $5.6\times$ less Register and LUT resources respectively compared with a high-level synthesis solution."
pub.1012718775,Chapter 20 Parallel Streaming Signal Processing,"Streaming signals or streaming data are digital signals arriving periodically within a regular time interval. The unit of streaming data is usually a data packet instead of a data sample. Streaming signals either are sampled with fixed data rate, transmitted through communication channels, or stored in storage devices, and are consumed with a fixed data rate. Real-time behavior is the most important feature. Signal processing on the current data packet must be finished before the new data packet arrives. Streaming signal is seldom represented using 64-bit or 32-bit floating-point data. Most streaming DSP applications have custom data types and finite data precision, which is enough to satisfy the SNR requirement. The stream processing latency is the sum of all the processing time from the time “input packet available” to the time “output packet available.” There are n tasks connected in serial, and the task Tn–1 uses two P. The total processing time is P(n+1). The output packet transfer time is much less than P, so that the stream processing latency is almost (yet slightly more than) P(n+1). If the feedback is required after streaming signal processing, the latency of the pipelined stream processing might be limited."
pub.1119553856,Large Scale Estimation in Cyberphysical Systems using Streaming Data: a Case Study with Smartphone Traces,"Controlling and analyzing cyberphysical and robotics systems is increasingly
becoming a Big Data challenge. Pushing this data to, and processing in the
cloud is more efficient than on-board processing. However, current cloud-based
solutions are not suitable for the latency requirements of these applications.
We present a new concept, Discretized Streams or D-Streams, that enables
massively scalable computations on streaming data with latencies as short as a
second.
  We experiment with an implementation of D-Streams on top of the Spark
computing framework. We demonstrate the usefulness of this concept with a novel
algorithm to estimate vehicular traffic in urban networks. Our online EM
algorithm can estimate traffic on a very large city network (the San Francisco
Bay Area) by processing tens of thousands of observations per second, with a
latency of a few seconds."
pub.1093856711,Finding Space-Time Stream Permutations for Minimum Memory and Latency,"Processing of parallel data streams requires permutation units for many algorithms where the streams are not independent. Such algorithms include transforms, multirate signal processing, and Viterbi decoding. The absolute order of data elements from the permutation is not important, only that data elements are located correctly for the next processing step. This paper describes a method to find permutations that require a minimum amount of memory and latency. The required permutations are generated based on the data dependencies of a computation set. Additional constraints are imposed so that the parallel streaming architecture processes the data without flow control. Results show agreement with brute force methods, which become computationally infeasible for large permutation sets."
pub.1137103334,Towards Elastic and Sustainable Data Stream Processing on Edge Infrastructure,"Much of the data produced today is processed as it is generated by data stream processing systems. Although the cloud is often the target infrastructure for deploying data stream processing applications, resources located at the edges of the Internet have increasingly been used to offload some of the processing performed in the cloud and hence reduce the end-to-end latency when handling data events. In this work, I highlight some of the challenges in executing data stream processing applications on edge computing infrastructure and discuss directions for future research on making such applications more elastic and sustainable."
pub.1113642200,Privacy Preserving Elastic Stream Processing with Clouds Using Homomorphic Encryption,"Prevalence of the Infrastructure as a Service (IaaS) clouds has enabled organizations to elastically scale their stream processing applications to public clouds. However, current approaches for elastic stream processing do not consider the potential security vulnerabilities in cloud environments. In this paper we describe the design and implementation of an Elastic Switching Mechanism for data stream processing which is based on Homomorphic Encryption (HomoESM). The HomoESM not only does elastically scale data stream processing applications into public clouds but also preserves the privacy of such applications. Using a real world test setup, which includes an email filter benchmark and a web server access log processor benchmark (EDGAR) we demonstrate the effectiveness of our approach. Multiple experiments on Amazon EC2 indicate that the proposed approach for Homomorphic encryption provides significant results which is 10% to 17% improvement of average latency in the case of email filter benchmark and EDGAR benchmarks respectively. Furthermore, EDGAR add/subtract operations and comparison operations showed 6.13% and 26.17% average latency improvements respectively. These promising results pave the way for real world deployments of privacy preserving elastic stream processing in the cloud."
pub.1144630863,SDN-based fog and cloud interplay for stream processing,"This paper focuses on SDN-based approaches for deploying stream processing workloads on heterogeneous environments comprising wide-area networks, cloud and fog resources. Stream processing applications impose strict latency requirements to operate appropriately. Deploying workloads in the fog reduces unnecessary delays, but its computational resources may not handle all the tasks. On the other hand, offloading the tasks to the cloud is constrained by limited network resources and involves additional transmission delays that exceed latency thresholds. Adaptive workload deployment may solve these issues by ensuring that resource and latency requirements are satisfied for all the data streams processed by an application. This paper’s main contribution consists of dynamic workload placement algorithms operating on stream processing requests with latency constraints. Provisioning of computing infrastructure exploits the interplay between fog and cloud under limited network capacity. The algorithms aim to maximize the ratio of successfully handled requests by effectively utilizing available resources while meeting application latency constraints. Experiments demonstrate that the goal can be achieved by detailed analysis of requests and ensuring balanced computing and network resources utilization. As a result, up to 30% improvement over the reference algorithms in success rate is observed."
pub.1004442723,Toward a Grid-Based Zero-Latency Data Warehousing Implementation for Continuous Data Streams Processing,"Continuous data streams are information sources in which data arrives in high volume in unpredictable rapid bursts. Processing data streams is a challenging task due to (1) the problem of random access to fast and large data streams using present storage technologies and (2) the exact answers from data streams often being too expensive. A framework of building a Grid-based Zero-Latency Data Stream Warehouse (GZLDSWH) to overcome the resource limitation issues in data stream processing without using approximation approaches is specified. The GZLDSWH is built upon a set of Open Grid Service Infrastructure (OGSI)-based services and Globus Toolkit 3 (GT3) with the capability of capturing and storing continuous data streams, performing analytical processing, and reacting autonomously in near real time to some kinds of events based on a well-established knowledge base. The requirements of a GZLDSWH, its Grid-based conceptual architecture, and the operations of its service are described in this paper. Furthermore, several challenges and issues in building a GZLDSWH, such as the Dynamic Collaboration Model between the Grid services, the Analytical Model, and the Design and Evaluation aspects of the Knowledge Base Rules are discussed and investigated."
pub.1118708684,Privacy Preserving Stream Analytics: The Marriage of Randomized Response and Approximate Computing,"How to preserve users' privacy while supporting high-utility analytics for
low-latency stream processing? To answer this question: we describe the design,
implementation, and evaluation of PRIVAPPROX, a data analytics system for
privacy-preserving stream processing. PRIVAPPROX provides three properties: (i)
Privacy: zero-knowledge privacy guarantees for users, a privacy bound tighter
than the state-of-the-art differential privacy; (ii) Utility: an interface for
data analysts to systematically explore the trade-offs between the output
accuracy (with error-estimation) and query execution budget; (iii) Latency:
near real-time stream processing based on a scalable ""synchronization-free""
distributed architecture. The key idea behind our approach is to marry two
existing techniques together: namely, sampling (used in the context of
approximate computing) and randomized response (used in the context of
privacy-preserving analytics). The resulting marriage is complementary - it
achieves stronger privacy guarantees and also improves performance, a necessary
ingredient for achieving low-latency stream analytics."
pub.1018154770,Toward a Grid-Based Zero-Latency Data Warehousing Implementation for Continuous Data Streams Processing,"<p>Continuous data streams are information sources in which data arrives in high volume in unpredictable rapid bursts. Processing data streams is a challenging task due to (1) the problem of random access to fast and large data streams using present storage technologies and (2) the exact answers from data streams often being too expensive. A framework of building a Grid-based Zero-Latency Data Stream Warehouse (GZLDSWH) to overcome the resource limitation issues in data stream processing without using approximation approaches is specified. The GZLDSWH is built upon a set of Open Grid Service Infrastructure (OGSI)-based services and Globus Toolkit 3 (GT3) with the capability of capturing and storing continuous data streams, performing analytical processing, and reacting autonomously in near real time to some kinds of events based on a well-established knowledge base. The requirements of a GZLDSWH, its Grid-based conceptual architecture, and the operations of its service are described in this paper. Furthermore, several challenges and issues in building a GZLDSWH, such as the Dynamic Collaboration Model between the Grid services, the Analytical Model, and the Design and Evaluation aspects of the Knowledge Base Rules are discussed and investigated.</p>"
pub.1093369362,Stream Processing Performance for Blue Gene/P Supercomputer,"Stream processing systems are designed to support applications that use real time data. Examples of streaming applications include security agencies processing data from communications media, battlefield management systems for military operations, consumer fraud detection based on online transactions, and automated trading based on financial market data. Many stream processing applications are faced with the challenge of increasingly large volumes of data and the requirement to deliver low-latency responses predicated by analysis of that data. In this paper, we assess the applicability of the Blue Gene architecture for stream computing applications. This work is part of a larger effort to demonstrate the efficacy of using a Blue Gene for streaming applications. Blue Gene supercomputers provide a high-bandwidth low-latency network connecting a set of I/O and compute nodes. We examine Blue Gene's suitability for stream computing applications by assessing its messaging capability for typical stream computing messaging workloads. In particular, this paper presents results from micro-benchmarks we used to evaluate the raw performance of Blue Gene/P (Blue Gene/P) supercomputer under loads produced by high volumes of streaming data. We measure the performance of data streams that originate outside the supercomputer, are directed through the I/O nodes to the compute nodes and then terminate outside. Our performance experiments demonstrate that the Blue Gene/P hardware delivers low-latency and high-throughput capability in a manner usable by streaming applications."
pub.1086111693,Hardware Support for Secure Stream Processing in Cloud Environments,"Many-core microprocessor architectures are quickly becoming prevalent in data centers, due to their demonstrated processing power and network flexibility. However, this flexibility comes at a cost; co-mingled data from disparate users must be kept secure, which forces processor cycles to be wasted on cryptographic operations. This paper introduces a novel, secure, stream processing architecture which supports efficient homomorphic authentication of data and enforces secrecy of individuals' data. Additionally, this architecture is shown to secure time-series analysis of data from multiple users from both corruption and disclosure. Hardware synthesis shows that security-related circuitry incurs less than 10% overhead, and latency analysis shows an increase of 2 clocks per hop. However, despite the increase in latency, the proposed architecture shows an improvement over stream processing systems that use traditional security methods."
pub.1099680522,An Adaptive SLA-Based Data Flow Mechanism for Stream Processing Engines,"With the upsurge in the volume of data and profuse cloud applications, big data analytics becomes very popular in the research communities in industry as well as in academia. This led to the emergence of real-time distributed stream processing systems such as Flink, Storm, Spark, and Samza. These systems sanction complex queries on streaming data to be distributed across multiple worker nodes in a cluster. Some of the stream processing systems provide basic supports for controlling the latency and throughput of the system as well as correctness of the results. We present an intelligent and efficient adaptive watermarking and dynamic buffering timeout mechanism for modern distributed stream processing engines. It is designed to increase the overall throughput by making the watermarks of the system adaptive according to incoming workload streams, and dynamically scale the buffering timeout for every task tracker on the fly while maintaining the SLA-based end-to-end latency of the system. Apache Flink is used as testing distributed processing engine in the paper. However, the proposed mechanism can be applied to other streaming frameworks. Our preliminary results indicate that the proposed system outperforms the existing stream processing framework."
pub.1094698667,Towards Service Collaboration Model in Grid-based Zero Latency Data Stream Warehouse (GZLDSWH),"A Grid-based Zero-Latency Data Stream Warehouse (GZLDSWH), built upon a set of OGS/-based grid services and GT3 Toolkit, overcomes the resource limitation issue for data stream processing without using traditional approximate approaches. However, due to its “automated event-based reaction” characteristic, the GZLDSWH requires a mechanism which allows the grid services to be able to work together to fulfil the common tasks. This paper describes the Collaboration Model for the Grid Services which enables the automation of the GZLDSWH in capturing and storing continuous data streams, making analytical processing, and reacting autonomously in near real time with some kinds of events based on well-established Knowledge Base."
pub.1099756267,A Distributed Stream Processing based Architecture for IoT Smart Grids Monitoring,"Sensor networks have become ubiquitous -- being present from personal smartphones to smart cities deployments -- and are producing large volumes of data at increasing rates. Distributed event stream processing systems, in its turn, are a specific kind of systems that help us to parallelize event processing. Therefore, they provide us capabilities to produce quick insights and decisions, in near real-time, on top of multiple data streams. However, current systems for large scale processing do not focus on Internet of Things and Sensor Network workloads, which makes the performance decrease quickly as the workload size increases. In order to process large scale events with acceptable latency percentiles and high throughput, special systems are needed, such as distributed event stream processing systems. In this work, we propose an architecture for Internet of Things data workloads, in a combination of sensor networks data sources and distributed event stream processing systems, focused on smart grid data profiles. In our evaluations, the system was able to process up to 45K messages per second using 8 processing nodes, while providing stable latencies for micro-batches above 30 seconds."
pub.1136509674,Hazelcast Jet: Low-latency Stream Processing at the 99.99th Percentile,"Jet is an open-source, high-performance, distributed stream processor built
at Hazelcast during the last five years. Jet was engineered with millisecond
latency on the 99.99th percentile as its primary design goal. Originally Jet's
purpose was to be an execution engine that performs complex business logic on
top of streams generated by Hazelcast's In-memory Data Grid (IMDG): a set of
high-performance, in-memory, partitioned and replicated data structures. With
time, Jet evolved into a full-fledged, scale-out stream processor that can
handle out-of-order streams and exactly-once processing guarantees. Jet's
end-to-end latency lies in the order of milliseconds, and its throughput in the
order of millions of events per CPU-core. This paper presents main design
decisions we made in order to maximize the performance per CPU-core, alongside
lessons learned, and an empirical performance evaluation."
pub.1128045913,Rhino: Efficient Management of Very Large Distributed State for Stream Processing Engines,"Scale-out stream processing engines (SPEs) are powering large big data applications on high velocity data streams. Industrial setups require SPEs to sustain outages, varying data rates, and low-latency processing. SPEs need to transparently reconfigure stateful queries during runtime. However, state-of-the-art SPEs are not ready yet to handle on-the-fly reconfigurations of queries with terabytes of state due to three problems. These are network overhead for state migration, consistency, and overhead on data processing. In this paper, we propose Rhino, a library for efficient reconfigurations of running queries in the presence of very large distributed state. Rhino provides a handover protocol and a state migration protocol to consistently and efficiently migrate stream processing among servers. Overall, our evaluation shows that Rhino scales with state sizes of up to TBs, reconfigures a running query 15 times faster than the state-of-the-art, and reduces latency by three orders of magnitude upon a reconfiguration."
pub.1127378981,Latency‐aware adaptive micro‐batching techniques for streamed data compression on graphics processing units,"Summary Stream processing is a parallel paradigm used in many application domains. With the advance of graphics processing units (GPUs), their usage in stream processing applications has increased as well. The efficient utilization of GPU accelerators in streaming scenarios requires to batch input elements in microbatches, whose computation is offloaded on the GPU leveraging data parallelism within the same batch of data. Since data elements are continuously received based on the input speed, the bigger the microbatch size the higher the latency to completely buffer it and to start the processing on the device. Unfortunately, stream processing applications often have strict latency requirements that need to find the best size of the microbatches and to adapt it dynamically based on the workload conditions as well as according to the characteristics of the underlying device and network. In this work, we aim at implementing latency‐aware adaptive microbatching techniques and algorithms for streaming compression applications targeting GPUs. The evaluation is conducted using the Lempel‐Ziv‐Storer‐Szymanski compression application considering different input workloads. As a general result of our work, we noticed that algorithms with elastic adaptation factors respond better for stable workloads, while algorithms with narrower targets respond better for highly unbalanced workloads."
pub.1030280444,Adaptive Speculative Processing of Out-of-Order Event Streams,"Distributed event-based systems are used to detect meaningful events with low latency in high data-rate event streams that occur in surveillance, sports, finances, etc. However, both known approaches to dealing with the predominant out-of-order event arrival at the distributed detectors have their shortcomings: buffering approaches introduce latencies for event ordering, and stream revision approaches may result in system overloads due to unbounded retraction cascades.
                  This article presents an adaptive speculative processing technique for out-of-order event streams that enhances typical buffering approaches. In contrast to other stream revision approaches developed so far, our novel technique encapsulates the event detector, uses the buffering technique to delay events but also speculatively processes a portion of it, and adapts the degree of speculation at runtime to fit the available system resources so that detection latency becomes minimal.
                  Our technique outperforms known approaches on both synthetical data and real sensor data from a realtime locating system (RTLS) with several thousands of out-of-order sensor events per second. Speculative buffering exploits system resources and reduces latency by 40% on average."
pub.1030788531,Real-Time Stream Processing in Java,"This paper presents a streaming data framework for the Real-Time Specification for Java, with the goal of levering as much as possible the Java 8 Stream processing framework whilst delivering bounded latency. Our approach is to buffer the incoming streaming data into micro batches which are then converted to collections for processing by the Java 8 infrastructure which is configured with a real-time ForkJoin thread pool. Deferrable servers are used to limit the impact of stream processing activity on hard real-time activities."
pub.1100169217,Identifying the Most Recent Heavy Hitters in Large-Scale Streams Using Block-Wise Counting,"Identifying the most recent heavy hitters, i.e., finding the items with the highest appearances in a high speed data stream is a fundamental problem in real-time stream processing. The requirement of real-time stream applications raises significant challenges to this problem in terms of the processing latency, the space usage and the precision. Traditional schemes leverage the sliding windows based design which is hard to support both high precision, and low space usage of heavy hitters identification. In this work, we propose a novel Block-wise Counting scheme, which can partition the streams into tiny blocks to support high precision and low latency of heavy hitters identification with low space cost. The experiment results show that our scheme significantly improves the identification precision by 65% and reduces the processing latency by 87% compared to state-of-the-art desions."
pub.1028205933,Reliable speculative processing of out-of-order event streams in generic publish/subscribe middlewares,"In surveillance, sports, finances, etc., distributed event-based systems are used to detect meaningful events with low latency in high data rate event streams. Both known approaches to deal with the predominant out-of-order event arrival at the distributed detectors have their shortcomings: buffering approaches introduce latencies for event ordering and stream revision approaches may result in system overloads due to unbounded retraction cascades. This paper presents a speculative processing technique for out-of-order event streams that enhances typical buffering approaches. In contrast to other stream revision approaches our novel technique encapsulates the event detector, uses the buffering technique to delay events but also speculatively processes a portion of it, and adapts the degree of speculation at runtime to fit the available system resources so that detection latency becomes minimal. Our technique outperforms known approaches on both synthetical data and real sensor data from a Realtime Locating System (RTLS) with several thousands of out-of-order sensor events per second. Speculative buffering exploits system resources and reduces latency by 40% on average."
pub.1148092062,Stream processing technology based on FPGA network service,"The ever-increasing amount of data has brought severe pressure to the data center. Facing the large-scale data processing requirements, the data center not only needs to increase the data bandwidth, but also needs to ensure the timeliness of data processing. It is increasingly unable to meet the processing requirements of high throughput and low latency. This topic adopts the stream processing architecture of heterogeneous collaborative computing based on FPGA and CPU, designs a dual-channel separated stream processing system based on FPGA network offload processing, improves the management and interaction capabilities of FPGA, reduces processing delay, and provides a set of manageable The general data real-time processing platform."
pub.1142947510,Hazelcast jet,"Jet is an open source, high performance, distributed stream processor built at Hazelcast during the last five years. Jet was engineered with millisecond latency on the 99.99th percentile as its primary design goal. Originally Jet's purpose was to be an execution engine that performs complex business logic on top of streams generated by Hazelcast's In-memory Data Grid (IMDG): a set of in-memory, partitioned and replicated data structures. With time, Jet evolved into a full-fledged, scale-out stream processor that can handle out-of-order streams and provide exactly-once processing guarantees. Jet's end-to-end latency lies in the order of milliseconds, and its throughput in the order of millions of events per CPU-core. This paper presents the main design decisions we made in order to maximize the performance per CPU-core, alongside lessons learned, and an empirical performance evaluation."
pub.1094582325,Fault Tolerant State Management for High-Volume Low-Latency Data Stream Workloads,"One of the major challenges in performing incremental computations on parallel distributed stream processing systems is in the implementation of a mechanism for passing state values across successive runs. One approach is to enhance the granularity from record-at-a-time processing to processing at micro-batch level. A contrasting approach is to follow the record-at-a-time semantics and ensure scalability by means of distributed state management. Both approaches, however, require observing high degree of fault tolerance. In this paper, we study the problem of process state management against non-terminating data stream workloads for low-latency computing using the micro-batch stream processing approach. We attempt to examine methods that could yield optimum levels of state retentions with high degree of fault tolerance for typical processing workloads and propose a three-pronged approach to harness the demand."
pub.1107230701,Optimising Kafka for stream processing in latency sensitive systems," Many problems, like recommendation services, sensor networks, anti-crime protection, sophisticated AI services, need online data processing coming from the environment in the form of data streams consisting of events. The novelty of the approach in the field of stream processing lies in a synergistic effort toward optimization of such systems and additionally needed client components working as a whole. Building a message passing system for gathering information from mission-critical systems can be beneficial, but it is required to pay close attention to the impact it has on these systems. In this paper, we present the Apache Kafka optimization process for usage Kafka as a messaging system in latency sensitive systems. We propose a set of performance tests that can be used to measure Kafka impact on the system and performance test results of KafkaProducer Java API. KafkaProducer has almost no impact on system overall latency and it has a severe impact on resource consumption in terms of CPU. Optimising Kafka for stream processing in latency sensitive systems we reduce KafkaProducer negative impact by 75%. The tests are performed on an isolated production system."
pub.1093838451,Latency Reduction of Selected Data Streams in Network-an-Chips for Adaptive Manycore Systems,"This paper reviews Network-on-Chip architectures with prioritization of selected data streams targeting runtime reconfigurable manycore systems. The common idea of these architectures is to minimize the latency of selected packet transmissions by either bypassing or parallelizing processing stages in routers or by using dedicated links bypassing complete routers. Potential classes of selected data streams are latency critical messages, i.e. cache accesses in multiprocessor systems, or systems with semi-static data streams, i.e. systems in which the same components continuously exchange data for a longer period. The review categorizes the diverse architectures and evaluates their pros and cons in terms of universality, hardware efficiency and support of changing traffic patterns."
pub.1171398737,ShuffleBench: A Benchmark for Large-Scale Data Shuffling Operations with Distributed Stream Processing Frameworks,"Distributed stream processing frameworks help building scalable and reliable applications that perform transformations and aggregations on continuous data streams. This paper introduces ShuffleBench, a novel benchmark to evaluate the performance of modern stream processing frameworks. In contrast to other benchmarks, it focuses on use cases where stream processing frameworks are mainly employed for shuffling (i.e., re-distributing) data records to perform state-local aggregations, while the actual aggregation logic is considered as black-box software components. ShuffleBench is inspired by requirements for near real-time analytics of a large cloud observability platform and takes up benchmarking metrics and methods for latency, throughput, and scalability established in the performance engineering research community. Although inspired by a real-world observability use case, it is highly configurable to allow domain-independent evaluations. ShuffleBench comes as a ready-to-use open-source software utilizing existing Kubernetes tooling and providing implementations for four state-of-the-art frameworks. Therefore, we expect ShuffleBench to be a valuable contribution to both industrial practitioners building stream processing applications and researchers working on new stream processing approaches. We complement this paper with an experimental performance evaluation that employs ShuffleBench with various configurations on Flink, Hazelcast, Kafka Streams, and Spark in a cloud-native environment. Our results show that Flink achieves the highest throughput while Hazelcast processes data streams with the lowest latency."
pub.1085044268,Latency Aware Elastic Switching-based Stream Processing Over Compressed Data Streams,Elastic scaling of event stream processing systems has gained significant attention recently due to the prevalence of cloud computing technologies. We investigate on the complexities associated with elastic scaling of an event processing system in a private/public cloud scenario. We develop an Elastic Switching Mechanism (ESM) which reduces the overall average latency of event processing jobs by significant amount considering the cost of operating the system. ESM is augmented with adaptive compressing of upstream data. The ESM conducts one of the two types of switching where either part of the data is sent to the public cloud (data switching) or a selected query is sent to the public cloud (query switching) based on the characteristics of the query. We model the operation of the ESM as the function of two binary switching functions. We show that our elastic switching mechanism with compression is capable of handling out-of-order events more efficiently compared to techniques which does not involve compression. We used two application benchmarks called EmailProcessor and a Social Networking Benchmark (SNB2016) to conduct multiple experiments to evaluate the effectiveness of our approach. In a single query deployment with EmailProcessor benchmark we observed that our elastic switching mechanism provides 1.24 seconds average latency improvement per processed event which is 16.70% improvement compared to private cloud only deployment. When presented the option of scaling EmailProcessor with four public cloud VMs ESM further reduced the average latency by 37.55% compared to the single public cloud VM. In a multi-query deployment with both EmailProcessor and SNB2016 we obtained a reduction of average latency of both the queries by 39.61 seconds which is a decrease of 7% of overall latency. These performance figures indicate that our elastic switching mechanism with compressed data streams can effectively reduce the average elapsed time of stream processing happening in private/public clouds.
pub.1129393941,Pre-processing and data validation in IoT data streams,"In the last few years, distributed stream processing engines have been on the rise due to their crucial impacts on real-time data processing with guaranteed low latency in several application domains such as financial markets, surveillance systems, manufacturing, smart cities, etc. Stream processing engines are run-time libraries to process data streams without knowing the lower level streaming mechanics. Apache Storm, Apache Flink, Apache Spark, Kafka Streams and Hazelcast Jet are some of the popular stream processing engines. Nowadays, critical systems like energy systems, are interconnected and automated. As a result, these systems are vulnerable to cyber-attacks. In real-world applications, the sensing values come from sensor devices contains missing values, redundant data, data outliers, manipulated data, data failures, etc. Therefore, our system must be resilient to these conditions. In this paper, we present an approach to check if there is any above mentioned conditions by pre-processing data streams using a stream processing engine like Apache Flink which will be updated as a library in future. Then, the pre-processed streams are forwarded to other stream processing engines like Apache Kafka for real stream processing. As a result, data validation, data consistency and integrity for a resilient system can be accomplished before initiating the actual stream processing."
pub.1169524643,ShuffleBench: A Benchmark for Large-Scale Data Shuffling Operations with Distributed Stream Processing Frameworks,"Distributed stream processing frameworks help building scalable and reliable
applications that perform transformations and aggregations on continuous data
streams. This paper introduces ShuffleBench, a novel benchmark to evaluate the
performance of modern stream processing frameworks. In contrast to other
benchmarks, it focuses on use cases where stream processing frameworks are
mainly employed for shuffling (i.e., re-distributing) data records to perform
state-local aggregations, while the actual aggregation logic is considered as
black-box software components. ShuffleBench is inspired by requirements for
near real-time analytics of a large cloud observability platform and takes up
benchmarking metrics and methods for latency, throughput, and scalability
established in the performance engineering research community. Although
inspired by a real-world observability use case, it is highly configurable to
allow domain-independent evaluations. ShuffleBench comes as a ready-to-use
open-source software utilizing existing Kubernetes tooling and providing
implementations for four state-of-the-art frameworks. Therefore, we expect
ShuffleBench to be a valuable contribution to both industrial practitioners
building stream processing applications and researchers working on new stream
processing approaches. We complement this paper with an experimental
performance evaluation that employs ShuffleBench with various configurations on
Flink, Hazelcast, Kafka Streams, and Spark in a cloud-native environment. Our
results show that Flink achieves the highest throughput while Hazelcast
processes data streams with the lowest latency."
pub.1158505810,Two-stage scheduling for a fluctuant big data stream on heterogeneous servers with multicores in a data center,"Rapid processing with low-latency and high-throughput is a critical requirement for the applications of big data streams. However, the interferences among stream processing tasks in a data center decrease the utilization of the computational resources and prolong the latency of the tasks. Thus, we study an optimal scheduling method for processing a big data stream on heterogeneous servers with multicores in a data center. We model the big data stream processing and the scheduling problem with four objects or factors which are streaming data items, processing tasks, computational nodes and the cores inside each computational node. An interference model based on regression analysis and a prediction model based on the Autoregressive Integrated Moving Average are presented. Then, we propose a two-stage scheduling method including the fine-grained core scheduling and the coarse-grained node scheduling. In the core scheduling stage, we design a core scheduling algorithm named CS_TDF. In the node scheduling stage, we design a node scheduling algorithm named NS_ITF for a single time window and a continuous scheduling algorithm named PS_UIM for the entire data stream in all time windows. The experimental results show that our scheduling method achieves low interference and high computational resource utilization."
pub.1135691283,Poster: Dependency-Aware Operator Placement of Distributed Stream Processing IoT Applications Deployed at the Edge,"In the last few years, the number of IoT applications that rely on stream processing has increased significantly. These applications process continuous streams of data with a low delay and provide valuable information. To meet the stringent latency requirements and the need for real-time results that they require, the components of the stream processing pipeline can be deployed directly onto the edge layer to benefit from the resources and capabilities that the swarm of edge devices can provide. In this poster, we outline some ongoing research ideas into deploying stream processing operators onto edge nodes, with the goal of minimizing latency while ensuring that the constraints of the devices and their network capabilities are respected. More precisely, we provide a modeling of the semantics of the operators that considers the interactions between different operators, the parallelism of concurrent operators, as well as the latency and bandwidth usage."
pub.1155838979,Choosing an effective setup for stream processing,"This project aims to study the feasibility and cost-effectiveness of using
edge computing for stream data processing in the context of Internet of Things
(IoT) in manufacturing in Europe. Two scenarios were considered: using edge
computing to reduce latency and using a popular public cloud provider. Both
scenarios demonstrated high throughput, with the edge computing scenario
slightly outperforming the public cloud scenario. The impact on resource
utilization was also measured, with the edge node showing slightly lower
resource usage than the cloud node. The experiment concluded that running the
system at the edge is more cost-efficient, but only using any Infrastructure as
a Service (IaaS) provider acting as the infrastructure provider. IaaS providers
will be crucial in offering edge solutions and identifying geographical areas
where regional data centers could be used as points of presence for low-latency
applications.
  Keywords: edge computing, stream data processing, Internet of Things (IoT),
manufacturing, Europe, latency, throughput, resource utilization,
cost-efficiency, infrastructure as a service (IaaS), regional data centers,
low-latency applications, cloud computing, feasibility study."
pub.1112685903,Enactment of adaptation in data stream processing with latency implications—A systematic literature review,"Context Stream processing is a popular paradigm to continuously process huge amounts of data. Runtime adaptation plays a significant role in supporting the optimization of data processing tasks. In recent years runtime adaptation has received significant interest in scientific literature. However, so far no categorization of the enactment approaches for runtime adaptation in stream processing has been established. Objective This paper identifies and characterizes different approaches towards the enactment of runtime adaptation in stream processing with a main focus on latency as quality dimension. Method We performed a systematic literature review (SLR) targeting five main research questions. An automated search, resulting in 244 papers, was conducted. 75 papers published between 2006 and 2018 were finally included. From the selected papers, we extracted data like processing problems, adaptation goals, enactment approaches of adaptation, enactment techniques, evaluation metrics as well as evaluation parameters used to trigger the enactment of adaptation in their evaluation. Results We identified 17 different enactment approaches and categorized them into a taxonomy. For each, we extracted the underlying technique used to implement this enactment approach. Further, we identified 9 categories of processing problems, 6 adaptation goals, 9 evaluation metrics and 12 evaluation parameters according to the extracted data properties. Conclusion We observed that the research interest on enactment approaches to the adaptation of stream processing has significantly increased in recent years. The most commonly applied enactment approaches are parameter adaptation to tune parameters or settings of the processing, load balancing used to re-distribute workloads, and processing scaling to dynamically scale up and down the processing. In addition to latency, most adaptations also address resource fluctuation / bottleneck problems. For presenting a dynamic environment to evaluate enactment approaches, researchers often change input rates or processing workloads."
pub.1174753300,Optimizing Real-Time Data Processing in Azure Stream Analytics with .NET: Techniques and Best Practices,"In the era of big data, real-time data processing is crucial for businesses to derive actionable insights. Azure Stream Analytics, integrated with .NET, provides a robust platform for processing streaming data. This paper explores various tech- niques and best practices to optimize real-time data processing in Azure Stream Analytics using .NET. Through a comprehensive study, we identify performance bottlenecks and propose opti- mization strategies. Experimental results demonstrate significant improvements in processing efficiency and latency reduction. This research provides a practical guide for developers and engineers to enhance their real-time data processing capabilities using Azure Stream Analytics and .NET."
pub.1100899173,Viper: Communication-Layer Determinism and Scaling in Low-Latency Stream Processing,"Stream Processing Engines (SPEs) process continuous streams of data and produce up-to-date results in a real-time fashion, typically through one-at-a-time tuple analysis. When looking into the vital SPE processing properties required from applications, determinism has a strong position besides scalability in throughput and low processing latency. SPEs scale in throughput and latency by relying on shared-nothing parallelism, deploying multiple copies of each operator to which tuples are distributed based on the semantics of the operator. The coordination of the asynchronous analysis of parallel operators required to enforce determinism is then carried out by additional dedicated sorting operators. In this work we shift such costly coordination to the communication layer of the SPE. Specifically, we extend earlier work on shared-memory implementations of deterministic operators and provide a communication module (Viper) which can be integrated in the SPE communication layer. Using Apache Storm and the Linear Road benchmark, we show the benefits that can be achieved by our approach in terms of throughput and energy efficiency of SPEs implementing one-at-a-time analysis."
pub.1093391499,On-line Data Stream Query Processing Using Finite State Automata,"In data stream systems, data does not appear in form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying streams. Although, the nature of data streams is on-line, the well-known data stream models don't consider on-line query processing. This is a major deficit in these environments. In this paper an online query processing model for data streams is presented in which finite state automata is used. Architecture of the proposed query processing model, structure of the finite state automata, and relevant algorithms for on-line data stream query processing are presented. It is tested using an on-line auction environment and its performance in term of response time, tuple latency, number of operations and required memory space is evaluated and compared with the existing system via simulation."
pub.1136732073,Big Data Stream Discretization Using ChiMerge Algorithm,"In this paper, we propose a new approach of the ChiMerge algorithm. We have added a new layer to remediate the threshold limitation issue and process the incoming data in real-time with a minimal latency. Our empirical results show that ChiMerge stream discretization helps to improve the subsequent pre-processing and algorithm execution time by 3.5×$$\times $$ compared to batch processing, with a slightly different accuracy rate."
pub.1131467044,Scalable and Reliable Multi-dimensional Sensor Data Aggregation in Data Streaming Architectures,"Ever-increasing amounts of data and requirements to process them in real time lead to more and more analytics platforms and software systems designed according to the concept of stream processing. A common area of application is processing continuous data streams from sensors, for example, IoT devices or performance monitoring tools. In addition to analyzing pure sensor data, analyses of data for entire groups of sensors often need to be performed. Therefore, data streams of the individual sensors have to be continuously aggregated to a data stream for a group. Motivated by a real-world application scenario of analyzing power consumption in Industry 4.0 environments, we propose that such a stream aggregation approach has to allow for aggregating sensors in hierarchical groups, support multiple such hierarchies in parallel, provide reconfiguration at runtime, and preserve the scalability and reliability qualities of stream processing techniques. We propose a stream processing architecture fulfilling these requirements, which can be integrated into existing big data architectures. As all state-of-the-art stream processing frameworks have to handle a trade-off between latency, resource-efficiency, and correctness, our proposed architecture can be configured for low latency and resource-efficient computation or for always ensuring correct results. To assist adopters in choosing appropriate configuration options, we provide an experimental comparison. We present a pilot implementation of our proposed architecture and show how it is used in industry. Furthermore, in experimental evaluations we show that our solution scales linearly with the amount of sensors and provides adequate reliability in the presence of faults."
pub.1124149129,Hardware-Conscious Stream Processing: A Survey,"Data stream processing systems (DSPSs) enable users to express and run stream
applications to continuously process data streams. To achieve real-time data
analytics, recent researches keep focusing on optimizing the system latency and
throughput. Witnessing the recent great achievements in the computer
architecture community, researchers and practitioners have investigated the
potential of adoption hardware-conscious stream processing by better utilizing
modern hardware capacity in DSPSs. In this paper, we conduct a systematic
survey of recent work in the field, particularly along with the following three
directions: 1) computation optimization, 2) stream I/O optimization, and 3)
query deployment. Finally, we advise on potential future research directions."
pub.1120875795,Distributed Classification of Text Streams,"Text stream classification is an important problem that is difficult to solve at scale. Batch processing systems, widely adopted for text classification tasks, cannot provide for low latency. Distributed stream processing systems can offer low latency, but do not support the same level of fault tolerance and determinism as the batch systems. In this work, we demonstrate how the distributed stream processing features can affect the results of a typical text classification data flow. Our analysis shows emerged trade-offs between fault tolerance and reproducibility on the one side, and performance on the other side. We outline potential ways to solve the revealed issues and to handle streaming features."
pub.1121691884,A Survey of Distributed Data Stream Processing Frameworks,"Big data processing systems are evolving to be more stream oriented where each data record is processed as it arrives by distributed and low-latency computational frameworks on a continuous basis. As the stream processing technology matures and more organizations invest in digital transformations, new applications of stream analytics will be identified and implemented across a wide spectrum of industries. One of the challenges in developing a streaming analytics infrastructure is the difficulty in selecting the right stream processing framework for the different use cases. With a view to addressing this issue, in this paper we present a taxonomy, a comparative study of distributed data stream processing and analytics frameworks, and a critical review of representative open source (Storm, Spark Streaming, Flink, Kafka Streams) and commercial (IBM Streams) distributed data stream processing frameworks. The study also reports our ongoing study on a multilevel streaming analytics architecture that can serve as a guide for organizations and individuals planning to implement a real-time data stream processing and analytics framework."
pub.1095836752,Popularity-Aware Differentiated Distributed Stream Processing on Skewed Streams,"Real-world stream data with skewed distribution raises unique challenges to distributed stream processing systems. Existing stream workload partitioning schemes usually use a “one size fits all” design, which leverage either a shuffle grouping or a key grouping strategy for partitioning the stream workloads among multiple processing units, leading to notable problems of unsatisfied system throughput and processing latency. In this paper, we show that the key grouping based schemes result in serious load imbalance and low computation efficiency in the presence of data skewness while the shuffle grouping schemes are not scalable in terms of memory space. We argue that the key to efficient stream scheduling is the popularity of the stream data. We propose and implement a differentiated distributed stream processing system, call DStream, which assigns the popular keys using shuffle grouping while assigns unpopular ones using key grouping. We design a novel efficient and light-weighted probabilistic counting scheme for identifying the current hot keys in dynamic real-time streams. Two factors contribute to the power of this design: 1) the probabilistic counting scheme is extremely computation and memory efficient, so that it can be well integrated in processing instances in the system; 2) the scheme can adapt to the popularity changes in the dynamic stream processing environment. We implement the DStream system on top of Apache Storm. Experiment results using large-scale traces from real-world systems show that DStream achieves a 2.3 × improvement in terms of processing throughput and reduces the processing latency by 64% compared to state-of-the-art designs."
pub.1133105508,Towards a Semantic Edge Processing of Sensor Data. An Incipient Experiment,"This paper addresses a semantic stream processing pipeline, including data collection, semantic annotation, RDF data storage and query processing. We investigate whether the semantic annotation step could be moved on the edge, by designing and evaluating two alternative processing architectures. Experiments show that the edge processing fulfills the low-latency requirement, facilitating the parallel processing of the semantic enrichment for the sensor data."
pub.1094393494,Stream query processing on emerging memory architectures,"Stream query processing is becoming increasingly important as more time-oriented data is produced and analyzed online. Stream processing is typically memory-resident for the fastest processing of ephemeral data. With workload consolidation, processing separate data streams on the same processor may lead to harmful contention between query workloads. This contention may become particularly problematic as new main memory technologies are adopted, such as phase-change memory, that have asymmetric read and write latency. This work presents a preliminary study of performance implications of consolidation and emerging memory on stream query processing. We show that contention in the memory subsystem worsens with a phase-change main memory, suggesting that new stream optimization and hardware approaches will be required to achieve quality of service and quality of data guarantees in future computer servers."
pub.1147192281,KFIML: Kubernetes-Based Fog Computing IoT Platform for Online Machine Learning,"The massive onsite data produced by the Internet of Things (IoT) can bring valuable information and immense potentials, thus empowering a new wave of emerging applications. However, with the rapid increase of onsite IoT data streams, it has become extremely challenging to develop a scalable computing platform and provide a comprehensive workflow for processing IoT data streams with lower latency and more intelligence. To this end, we present a Kubernetes-based scalable fog computing platform (KFIML), integrating big data streaming processing with machine learning (ML)-based applications. We also provide a comprehensive IoT data processing workflow, including data access and transfer, big data processing, online ML, long-term storage, and monitoring. The platform is feasibly validated on a clustered testbed, which comprises a master node, IoT broker servers, worker nodes, and a local database server. By leveraging the lightweight orchestration system, namely Kubernetes, we can readily scale and manage containerized software frameworks on our testbed. The big data processing layer utilizes the advanced data flow frameworks such as Apache Flink, to support both streaming processing and statistical analysis with low latency. In addition, the specified long short-term memory (LSTM)-based ML pipelines are employed on the online ML layer, to enable the real-time predictive analysis of IoT data streams. The experiments on a real-world smart grid use case demonstrate that the container-based KFIML platform can be well-scaled with Kubernetes to efficiently perform big data processing increased onsite IoT data streams with lower latency and conduct ML-based applications."
pub.1136847292,An efficient algorithm for reducing the flow of real-time data stream with least sampling error,"Nature of data stream is determined after complete scanning of whole data sets during real-time data processing. However, it becomes inconvenient to process entire data stream at once in real-time data stream processing. Thus, a sheer sized fixed window of data streams is processed at a particular time. The intensification of sheer sized fixed window at processing node is mitigated by reducing the flowing rate of data stream. Heuristic clustering windowing (HCW) approach and partial blind window (PBW) algorithms are proposed for reducing the flow of data stream with least sampling error. These approaches consist of the combination of systematic sampling and clustering mechanism. A clustering approach is applied on one fraction of data streams whereas systematic sampling handles other portion of streams. These approaches are helpful in reducing flow of data streams in minimum latency."
pub.1022138468,Adaptive Stream Processing using Dynamic Batch Sizing,"The need for real-time processing of ""big data"" has led to the development of frameworks for distributed stream processing in clusters. It is important for such frameworks to be robust against variable operating conditions such as server failures, changes in data ingestion rates, and workload characteristics. To provide fault tolerance and efficient stream processing at scale, recent stream processing frameworks have proposed to treat streaming workloads as a series of batch jobs on small batches of streaming data. However, the robustness of such frameworks against variable operating conditions has not been explored. In this paper, we explore the effects of the batch size on the performance of streaming workloads. The throughput and end-to-end latency of the system can have complicated relationships with batch sizes, data ingestion rates, variations in available resources, workload characteristics, etc. We propose a simple yet robust control algorithm that automatically adapts the batch size as the situation necessitates. We show through extensive experiments that it can ensure system stability and low latency for a wide range of workloads, despite large variations in data rates and operating conditions."
pub.1117052187,Evaluation of distributed stream processing frameworks for IoT applications in Smart Cities,"The widespread growth of Big Data and the evolution of Internet of Things (IoT) technologies enable cities to obtain valuable intelligence from a large amount of real-time produced data. In a Smart City, various IoT devices generate streams of data continuously which need to be analyzed within a short period of time; using some Big Data technique. Distributed stream processing frameworks (DSPFs) have the capacity to handle real-time data processing for Smart Cities. In this paper, we examine the applicability of employing distributed stream processing frameworks at the data processing layer of Smart City and appraising the current state of their adoption and maturity among the IoT applications. Our experiments focus on evaluating the performance of three DSPFs, namely Apache Storm, Apache Spark Streaming, and Apache Flink. According to our obtained results, choosing a proper framework at the data analytics layer of a Smart City requires enough knowledge about the characteristics of target applications. Finally, we conclude each of the frameworks studied here have their advantages and disadvantages. Our experiments show Storm and Flink have very similar performance, and Spark Streaming, has much higher latency, while it provides higher throughput."
pub.1164583244,An efficient architecture for processing real-time traffic data streams using apache flink,"Big Data technologies emerging day by day and are making drastic changes in various real-world applications. Traditional data mining tools adequate to process volumes of data but from past decades the rapid growth in data becomes difficult for processing. Due to continuous flow of data, data streams require additional computational processing than the traditional one. Big data stream processing considers different features of the data streams heterogeneity, scalability, fault tolerance and query optimization. Efficient implementation of these features in real-world applications using big data analytics is a challenging job during data storage, processing, and analysis phases. Therefore, the proposed model FRTSPS is a generic architecture which is influenced by popular big data processing Lambda architecture, based on distributed computing platform. The architecture using open-source platform Apache Flink for doing data processing. Flink is a popular platform for processing historical and stream data flows at once parallelly. Its stateful streaming can obtain more scalability and flexibility along with high throughput and low latency than the remaining stream processing programming models."
pub.1106685091,Geospatial Data Streams,"Geospatial stream processing refers to a class of software systems for processing high volume geospatial data streams with very low latency, that is, in near real time. Owing to the limitation of database management systems (DBMSs), data stream management systems (DSMSs) are oriented toward processing large data streams in near real time. Despite the differences between these two classes of management systems, DSMSs resemble DBMSs: they process data streams using SQL, SQL-like expressions, and operators defined by relational algebra. Geospatial data streams demonstrate at least two Big Data core features, volume and velocity. Increasingly, a dominant approach is to leverage in-memory computing over a cluster of commodity hardware. Existing distributed in-memory query engines and their processing models are predominantly based on relational paradigms and continuous operator models, without explicit support for spatiotemporal queries. Data management systems and technologies have drastically improved the availability of data analysis capabilities. Two major reasons for the widespread use of database systems are data independence, separating physical representation and storage from the actual information, and declarative languages, separating the program specification from its execution environment. In the era of Big Data, it is important to ensure that well-established declarative language concepts make their way into the advanced analytics of geospatial data streams. This chapter gives an insight into geospatial stream processing at a conceptual level, that is, from the user perspective, and presents a framework for efficient real-time analytics of big geospatial streams based on distributed processing on large clusters and a declarative, SQL-based approach. This chapter gives an insight into geospatial data stream processing at the conceptual level, which is, exclusively from the user's perspective, using a declarative, SQL-based approach. It presents a novel, in-memory parallel and distributed prototype that supports real-time processing and analysis of large geospatial data streams. Geospatial stream processing refers to a class of software systems for processing high volume geospatial data streams with very low latency, that is, in near real time. Owing to the limitation of database management systems (DBMSs), data stream management systems (DSMSs) are oriented toward processing large data streams in near real time. Despite the differences between these two classes of management systems, DSMSs resemble DBMSs: they process data streams using SQL, SQL-like expressions, and operators defined by relational algebra. Geospatial data streams demonstrate at least two Big Data core features: volume and velocity."
pub.1105796499,KerA: Scalable Data Ingestion for Stream Processing,"Big Data applications are increasingly moving from batch-oriented execution models to stream-based models that enable them to extract valuable insights close to real-time. To support this model, an essential part of the streaming processing pipeline is data ingestion, i.e., the collection of data from various sources (sensors, NoSQL stores, filesystems, etc.) and their delivery for processing. Data ingestion needs to support high throughput, low latency and must scale to a large number of both data producers and consumers. Since the overall performance of the whole stream processing pipeline is limited by that of the ingestion phase, it is critical to satisfy these performance goals. However, state-of-art data ingestion systems such as Apache Kafka build on static stream partitioning and offset-based record access, trading performance for design simplicity. In this paper we propose KerA, a data ingestion framework that alleviate the limitations of state-of-art thanks to a dynamic partitioning scheme and to lightweight indexing, thereby improving throughput, latency and scalability. Experimental evaluations show that KerA outperforms Kafka up to 4x for ingestion throughput and up to 5x for the overall stream processing throughput. Furthermore, they show that KerA is capable of delivering data fast enough to saturate the big data engine acting as the consumer."
pub.1048110702,Research and Analysis of the Stream Materialized Aggregate List,"The problem of low-latency processing of large amounts of data acquired in continuously changing environment has led to the genesis of Stream Processing Systems (SPS). However, sometimes it is crucial to process both historical (archived) and current data, in order to obtain full knowledge about various phenomena. This is achieved in a Stream Data Warehouse (StrDW), where analytical operations on both historical and current data streams are performed. In this paper we focus on Stream Materialized Aggregate List (StrMAL) – a stream repository tier of StrDW. As a motivating example, the liquefied petrol storage and distribution system, containing continuous telemetric data acquisition, transmission and storage, will be presented as possible application for Stream Materialized Aggregate List."
pub.1155231725,Continuous Distributed Processing of Software Defined Radar,"Software-defined radar has been an active research field for more than ten years. However, the low performance and low scalability of the traditional processing techniques of SDR make it hard to deal with complex radar tasks. The stream processing engine is a software that supports low latency and high scalability distributed data processing. It is widely used in log, image, and video processing scenarios but rarely used in radar applications. This paper reviews processing software-defined radar data by using the stream processing engine, and we illustrate the differences between processing log data and radar data. Furthermore, we discuss some optimizations for stream processing engines to get better performance in the radar data processing."
pub.1094980429,Distributed Low-Latency Out-of-Order Event Processing for High Data Rate Sensor Streams,"Event-based Systems (EBS) are used to detect and analyze meaningful events in surveillance, sports, finances and many other areas. With rising data and event rates and with correlations among these events, sequential event processing becomes infeasible and needs to be distributed. Existing approaches cannot deal with the ubiquity of out-of-order event arrival that is introduced by network delays when distributing EBS. Order-less event processing may result in a system failure. We present a low-latency approach based on K-slack that achieves ordered event processing on high data rate sensor and event streams without a-priori knowledge. Slack buffers are dynamically adjusted to fit the disorder in the streams without using local or global clocks. The middleware transparently reorders the event input streams so that events can still be aggregated and processed to a granularity that satisfies the demands of the application. On a Realtime Locating System (RTLS) our system performs accurate low-latency event detection under the predominance of out-of-order event arrival and with a close to linear performance scale-up when the system is distributed over several threads and machines."
pub.1119617711,Nephele Streaming: Stream Processing Under QoS Constraints At Scale,"The ability to process large numbers of continuous data streams in a
near-real-time fashion has become a crucial prerequisite for many scientific
and industrial use cases in recent years. While the individual data streams are
usually trivial to process, their aggregated data volumes easily exceed the
scalability of traditional stream processing systems. At the same time,
massively-parallel data processing systems like MapReduce or Dryad currently
enjoy a tremendous popularity for data-intensive applications and have proven
to scale to large numbers of nodes. Many of these systems also provide
streaming capabilities. However, unlike traditional stream processors, these
systems have disregarded QoS requirements of prospective stream processing
applications so far. In this paper we address this gap. First, we analyze
common design principles of today's parallel data processing frameworks and
identify those principles that provide degrees of freedom in trading off the
QoS goals latency and throughput. Second, we propose a highly distributed
scheme which allows these frameworks to detect violations of user-defined QoS
constraints and optimize the job execution without manual interaction. As a
proof of concept, we implemented our approach for our massively-parallel data
processing framework Nephele and evaluated its effectiveness through a
comparison with Hadoop Online. For an example streaming application from the
multimedia domain running on a cluster of 200 nodes, our approach improves the
processing latency by a factor of at least 13 while preserving high data
throughput when needed."
pub.1173286666,Real-time Stream Processing in IoT Environments,"New IoT devices generate massive real-time data. Fast, real-time stream processing is needed for IoT data insight extraction. There are pros and downsides to real-time stream processing for the Internet of Things (IoT). This article analyzes present methods and technologies. The inquiry begins with large-scale, real-time IoT data analysis. Real-time analytics and bulk processing limits are important for IoT decision-making. We then investigate IoT stream processing frameworks, methodologies, and systems' complexity. We explore Internet of Things technology selection and tradeoffs between scalability, accuracy, latency, and real-time processing system scalability. It covers real-time stream processing system design and deployment and risk mitigation. Case studies and use cases highlight real-time IoT stream processing. Healthcare, smart city, and industrial IoT operational efficiency and proactive decision-making improve with real-time data. Finally, the report suggests the Internet of Things real-time stream processing research. New IoT data analytics protocols, methods, and technologies are highlighted. We teach real-time stream processing to IoT academics, practitioners, and decision-makers. They could then develop more efficient and scalable IoT data stream optimization solutions."
pub.1143044344,Active replication for latency-sensitive stream processing in Apache Flink,"Stream processing frameworks allow processing massive amounts of data shortly after it is produced, and enable a fast reaction to events in scenarios such as data center monitoring, smart transportation, or telecommunication networks. Many scenarios depend on the fast and reliable processing of incoming data, requiring low end-to-end latencies from the ingest of a new event to the corresponding output. The occurrence of faults jeopardizes these guarantees: Currently-leading high-availability solutions for stream processing such as Spark Streaming or Apache Flink's implement passive replication through snapshotting, requiring a stop-the-world operation to recover from a failure. Active replication, while incurring higher deployment costs, can overcome these limitations and allow to mask the impact of faults and match stringent end-to-end latency requirements. We present the design, implementation, and evaluation of active replication in the popular Apache Flink platform. Our study explores two alternative designs, a leader-based approach leveraging external services (Kafka and ZooKeeper) and a leaderless implementation leveraging a novel deterministic merging algorithm. Our evaluation using a series of microbenchmarks and a SaaS cloud monitoring scenario on a 37-server cluster show that the actively-replicated Flink can fully mask the impact of faults on end-to-end latency."
pub.1024513158,A Graph-Based Cloud Architecture for Big Stream Real-Time Applications in the Internet of Things,"The Internet of Things (IoT) will consist of billions of interconnected heterogeneous devices denoted as “smart objects.” Smart objects are generally sensor/actuator-equipped and have constrained resources in terms of: (i) processing capabilities; (ii) available ROM/RAM; and (iii) communication reliability. To meet low-latency requirements, real-time IoT applications must rely on specific architectures designed in order to handle and process gigantic (in terms of number of sources of information and rate of received data) streams of data coming from smart objects. We refer to this smart object-generated data stream as “Big Stream,” in contrast to traditional “Big Data” scenarios, where real-time constraints are not considered. In this paper, we propose a novel Cloud architecture for Big Stream applications that can efficiently handle data coming from deployed smart objects through a graph-based processing platform and deliver processed data to consumer applications with lowest latency."
pub.1125192565,Hardware-Conscious Stream Processing,"Data stream processing systems (DSPSs) enable users to express and run stream applications to continuously process data streams. To achieve realtime data analytics, recent researches keep focusing on optimizing the system latency and throughput. Witnessing the recent great achievements in the computer architecture community, researchers and practitioners have investigated the potential of adoption hardware-conscious stream processing by better utilizing modern hardware capacity in DSPSs. In this paper, we conduct a systematic survey of recent work in the field, particularly along with the following three directions: 1) computation optimization, 2) stream I/O optimization, and 3) query deployment. Finally, we advise on potential future research directions."
pub.1164387754,Janus: Latency-Aware Traffic Scheduling for IoT Data Streaming in Edge Environments,"This article focuses on a simple, yet fundamental question of distributed edge computing: “how to handle IoT traffic with different levels of sensitivity and criticality by satisfying the application-specific latency constraints?” This question arises in the practical deployment of edge computing, where user data can arrive at a much faster rate than that they can be processed by an edge node. Addressing this question is critical for meeting the latency requirement for latency-sensitive applications, but existing approaches are inadequate to the problem. We present Janus, a multi-level traffic scheduling system for managing multiple data streams with various degrees of latency constraints. At the edge node level, Janus uses multi-level queues to manage data streams with different latency constraints. It then allocates the output bandwidth of the edge node according to the requirements of applications in different priority queues, aiming to reduce the queuing and processing delay of latency-sensitive streams while maximizing the edge-node throughput. At the network level, Janus actively redirects incoming data streams to the less-loaded ones to achieve better network-wide load balance and improve the overall throughput. Experiments show that Janus reduces the latency to only 16.6% of a non-priority based solution and improves the throughput by 1.7x of a state-of-the-art priority-aware data stream scheduling approach."
pub.1169940770,Lc‐Stream: An elastic scheduling strategy with latency constraints in geo‐distributed stream computing environments,"Summary An effective scheduling strategy is critical for achieving better performance in real‐time stream processing systems. How to quickly and efficiently process real‐time data stream is always challenging, especially when clusters are collaborating in a Geo‐Distributed computing environment. To address these challenges, we propose an elastic scheduling strategy with Latency Constraints in Geo‐Distributed stream computing environments called Lc‐Stream. This article discusses our work from the following aspects: (1) An optimized data stream redirection method that is proposed based on queuing network algorithm, along with a computing resource model, a latency constrained scheduling model and a communication energy consumption model. (2) An updated node selection method based on the inter‐layer task correlation, to reduce the communication latency between groups at the executor granularity. (3) A network cluster distribution for Geo‐Distributed computing environment to ensure energy saving under low transmission latency. Experimental results show that compared to R‐Storm, Lc‐Stream reduces total latency by over 19% and increases throughput by over 37% in typical cross‐domain multi‐task topologies. Compared to Ts‐Stream, Lc‐Stream also reduces total latency by over 15% and increases throughput by over 21%. At the same time, it helps to balance the load among the systems and avoid overuse of compute nodes."
pub.1154361871,Micro-batch and data frequency for stream processing on multi-cores,"Latency or throughput is often critical performance metrics in stream processing. Applications’ performance can fluctuate depending on the input stream. This unpredictability is due to the variety in data arrival frequency and size, complexity, and other factors. Researchers are constantly investigating new ways to mitigate the impact of these variations on performance with self-adaptive techniques involving elasticity or micro-batching. However, there is a lack of benchmarks capable of creating test scenarios to further evaluate these techniques. This work extends and improves the SPBench benchmarking framework to support dynamic micro-batching and data stream frequency management. We also propose a set of algorithms that generates the most commonly used frequency patterns for benchmarking stream processing in related work. It allows the creation of a wide variety of test scenarios. To validate our solution, we use SPBench to create custom benchmarks and evaluate the impact of micro-batching and data stream frequency on the performance of Intel TBB and FastFlow. These are two libraries that leverage stream parallelism for multi-core architectures. Our results demonstrated that our test cases did not benefit from micro-batches on multi-cores. For different data stream frequency configurations, TBB ensured the lowest latency, while FastFlow assured higher throughput in shorter pipelines."
pub.1182136807,Real-Time Data Processing Architectures for IoT Applications: A Comprehensive Review,"The advent of the Internet of Things (IoT) has resulted in an exponential surge in data generation, necessitating sophisticated data engineering solutions for real-time processing. This review paper scrutinizes the challenges associated with managing the vast volumes, diversity, and velocity of IoT data. It underscores the significance of low-latency processing and emphasizes the imperative for robust security and privacy measures. Key strategies elucidated encompass edge computing for latency reduction and bandwidth optimization, alongside the utilization of stream processing frameworks such as Apache Kafka and Apache Flink for real-time analytics. Furthermore, the paper delves into the realm of distributed computing, particularly Apache Spark, and investigates the integration of machine learning to augment decision-making and glean insights from IoT data streams."
pub.1094205897,Towards Execution Guarantees for Stream Queries,"The unbounded nature of data streams and the low-latency requirements of stream processing present interesting challenges in Data Stream Management System (DSMS) design. Streaming query operators are typically designed to produce results with low latency, as well as to efficiently manage their state. Stream-progress delimitation techniques, such as punctuation, can help query operators achieve these goals. In this work, we look at deriving execution guarantees with respect to result production and state management for complete queries over punctuated streams. These guarantees are derived before query execution. We formalize notions of successful stream processing at an operator level, and extend these definitions to stream queries as a whole. We introduce a framework, punctuation contracts, for analyzing data processing and punctuation propagation from input to output on individual operators. We then use our framework to analyze complete queries and determine, prior to execution, if every valid input is eventually emitted, and no item remains in operator state indefinitely. Finally, we discuss extensions needed to bound query memory requirements; we describe four stream properties that can be used to help understand and quantify memory and CPU usage."
pub.1031311757,Evaluation of high-frequency financial transaction processing in distributed memory systems,"A financial system requires processing a large-scale data stream. These data are considerably large for realtime processing. Using a distributed memory system is the most efficient means to process and store this data. In addition, using a high-performance interconnection technology such as InfiniBand is necessary because existing Ethernet technology that uses low bandwidth and possesses high latency can have detrimental effects on the financial system. In particular, in a system that processes large-scale financial data, high latency can cause financial loss. Therefore, high-performance interconnection technology is critical for processing large-scale financial data. We propose a scheme that uses high-performance interconnection technology in a distributed memory system for processing a large-scale data stream. In experiments that test the performance of our system, we use Redis, which is an open source data structure server, and messages of the size of the actual financial data in existence. Experimental results show that our distributed memory system using InfiniBand can process financial transactions with high throughput and low latency."
pub.1095518382,Autonomic Parallel Data Stream Processing,"Data Stream Processing (Da SP) is a recent and highly active research field, applied in various real world scenarios. Differently than traditional applications, input data is seen as transient continuous streams that must be processed “on the fly”, with critical requirements on throughput, latency and memory occupancy. A parallel solution is often advocated, but the problem of designing and implementing high throughput and low latency DaSP applications is complex per se and because of the presence of multiple streams characterized by high volume, high velocity and high variability. Moreover, parallel DaSP applications must be able to adapt themselves to data dynamics in order to satisfy desired QoS levels. The aim of our work is to study these problems in an integrated way, providing to the programmers a methodological framework for the parallelization of DaSP applications."
pub.1100847659,Single Window Stream Aggregation Using Reconfigurable Hardware,"High throughput and low latency stream aggregation - and stream processing in general - is critical for many emerging applications that analyze massive volumes of continuously produced data on-the-fly, to make real time decisions. In many cases, high speed stream aggregation can be achieved incrementally by computing partial results for multiple windows. However, for particular problems, storing all incoming raw data to a single window before processing is more efficient or even the only option. This paper presents the first FPGA-based single window stream aggregation design. Using Maxeler's dataflow engines (DFEs), up to 8 million tuples-per-second can be processed (1.1 Gbps) offering 1–2 orders of magnitude higher throughput than a state-of-the-art stream processing software system. DFEs have a direct feed of incoming data from the network as well as direct access to off-chip DRAM processing a tuple in less than $4\mu \mathbf{sec}$, 4 orders of magnitude lower latency than software. The proposed approach is able to support challenging queries required in realistic stream processing problems (e.g. holistic functions). Our design offers aggregation for up to 1 million concurrently active keys and handles large windows storing up to 6144 values (24 KB) per key."
pub.1122094343,Latency-Aware Deployment of IoT Services in a Cloud-Edge Environment,"Abstract
Efficient scheduling of data elements and computation units can help to reduce the latency of processing big IoT stream data. In many cases, moving computation turns out to be more cost-effective than moving data. However, deploying computations from cloud-end to edge devices may face two difficult situations. First, edge devices usually have limited computing power as well as storage capability, and we need to selectively schedule computation tasks. Secondly, the overhead of stream data processing varies over time and makes it necessary to adaptively adjust service deployment at runtime. In this work, we propose a heuristics approach to adaptively deploying services at runtime. The effectiveness of the proposed approach is demonstrated by examining real cases of China’s State Power Grid."
pub.1093251716,Elastic Stream Processing with Latency Guarantees,"Many Big Data applications in science and industry have arisen, that require large amounts of streamed or event data to be analyzed with low latency. This paper presents a reactive strategy to enforce latency guarantees in data flows running on scalable Stream Processing Engines (SPEs), while minimizing resource consumption. We introduce a model for estimating the latency of a data flow, when the degrees of parallelism of the tasks within are changed. We describe how to continuously measure the necessary performance metrics for the model, and how it can be used to enforce latency guarantees, by determining appropriate scaling actions at runtime. Therefore, it leverages the elasticity inherent to common cloud technology and cluster resource management systems. We have implemented our strategy as part of the Nephele SPE. To showcase the effectiveness of our approach, we provide an experimental evaluation on a large commodity cluster, using both a synthetic workload as well as an application performing real-time sentiment analysis on real-world social media data."
pub.1086118766,Performance Modeling of Stream Joins,"Streaming analysis is widely used in a variety of environments, from cloud computing infrastructures up to the network's edge. In these contexts, accurate modeling of streaming operators' performance enables fine-grained prediction of applications' behavior without the need of costly monitoring. This is of utmost importance for computationally-expensive operators like stream joins, that observe throughput and latency very sensitive to rate-varying data streams, especially when deterministic processing is required. In this paper, we present a modeling framework for estimating the throughput and the latency of stream join processing. The model is presented in an incremental step-wise manner, starting from a centralized non-deterministic stream join and expanding up to a deterministic parallel stream join. The model describes how the dynamics of throughput and latency are influenced by the number of physical input streams, as well as by the amount of parallelism in the actual processing and the requirement for determinism. We present an experimental validation of the model with respect to the actual implementation. The proposed model can provide insights that are catalytic for understanding the behavior of stream joins against different system deployments, with special emphasis on the influences of determinism and parallelization."
pub.1094941232,Adding Stream Processing System Flexibility to Exploit Low-overhead Communication Systems,"Previously, we demonstrated that we can build a real-world financial application using a stream processing system running on commodity hardware. In this paper, we propose making stream processing systems more flexible and demonstrate how this flexibility can be used to exploit low-overhead communication systems to speed up streaming applications. With our prototype, we now have an options market data processing system that can achieve less than ${\bf 30}\ \mu\sec$ average latency at 30x the February 2008 OPRA rate on a cluster of blades using InfiniBand. Across shared memory, this system can achieve less than ${\bf 20}\ \mu\sec$ average latency at 25x the February 2008 OPRA rate on a single machine."
pub.1129393916,Leaving stragglers at the window,"Stream Processing Engines (SPEs) are used to process large volumes of application data to emit high velocity output. Under high load, SPEs aim to minimize output latency by leveraging sample processing for many applications that can tolerate approximate results. Sample processing limits input to only a subset of events such that the sample is statistically representative of the input while ensuring output accuracy guarantees. For queries containing window operators, sample processing continuously samples events until all events relevant to the window operator have been ingested. However, events can suffer from large ingestion delays due to long or bursty network latencies. This leads to stragglers that are events generated within the window's timeline but are delayed beyond the window's deadline. Window computations that account for stragglers can add significant latency while providing inconsequential accuracy improvement. We propose Aion, an algorithm that utilizes sampling to provide approximate answers with low latency by minimizing the effect of stragglers. Aion quickly processes the window to minimize output latency while still achieving high accuracy guarantees. We implement Aion in Apache Flink and show using benchmark workloads that Aion reduces stream output latency by up to 85% while providing 95% accuracy guarantees."
pub.1095537271,Operator-scheduling using dynamic chain for continuous-query processing,"Sensor networks are a sort of wireless networks; monitoring and collecting data about the natural phenomena. These sensor data behave very differently from traditional database sources: they are continuous arrival in multiple, rapid, time varying, possibly unpredictable, unbounded streams, and keeping no record of historical information. Continuous-query processing for these data streams must be run using an efficient scheduler; the development of the original Chain algorithm for scheduler has focused only on minimizing the maximum run-time memory usage, ignoring the important aspect of output latency. During bursts in input streams, Chain suffers from tuple starvation, thereby incurring a high latency for these tuples. In this paper the proposed Dynamic Chain algorithm which is used in the scheduler is very powerful in reducing memory requirements for the system and has a very good performance in the latency issue, which is the drawback of the other scheduling algorithms."
pub.1129773954,Evaluating the impact of a coordinated checkpointing in distributed data streams processing systems using discrete event simulation,"Data Streams Processing systems process continuous flows of data under Quality of Service requirements. Data streams often contain critical information which requires real-time processing. To guarantee systems' dependability and avoid information loss, one must use a fault-tolerance strategy. However, there are several strategies available, and the proper evaluation of which mechanism is better for each system architecture is challenging, especially in large-scale distributed systems. In this paper, we propose a discrete simulation model for investigating the impacts of the Coordinated Checkpoint fault tolerance strategy imposes on Data Stream Processing Systems. Results show that this strategy critically affects stream processing in failure-prone situations due to an increase in latency up to 120% and information loss, reaching 95% of the processing window in the worst case."
pub.1036910962,Parallel processing of continuous queries over data streams,"In this paper, we propose parallel processing of continuous queries over data streams to handle the bottleneck of single processor DSMSs. Queries are executed in parallel over the logical machines in a multiprocessing environment. Scheduling parallel execution of operators is performed via finding the shortest path in a weighted graph called Query Mega Graph (QMG), which is a logical view of K machines. By lapse of time, number of tuples waiting in queues of different operators may be very different. When a queue becomes full, re-scheduling is done by updating weight of edges of QMG. In the new computed path, machines with more workload will be used less. The proposed system is formally presented and its correctness is proved. It is also modeled in PetriNets and its performance is evaluated and compared with serial query processing as well as the Min-Latency scheduling algorithm. The presented system is shown to outperform them w.r.t. tuple latency (response time), memory usage, throughput and also tuple loss- critical parameters in any data stream management systems."
pub.1061388438,Data Stream Management Systems for Computational Finance,"Because financial applications rely on a continual stream of time-sensitive data, any data management system must be able to process complex queries on the fly. Although many organizations turn to custom solutions, data stream management systems can offer the same low-latency processing with the flexibility to handle a range of applications."
pub.1127797481,Recovery-Conscious Adaptive Watermark Generation for Time-Order Event Stream Processing,"Achieving low latency in time-order real-time event stream processing of data produced by a large number of IoT devices is difficult due to the buffering time required for sorting event messages arrived. Despite the existence of realtime IoT applications that highly depend on the order of event occurrences, event messages arriving from the devices to the event processing system are not time-ordered in many cases. Therefore, such an event processing system needs to buffer arriving event messages, sort, and then process them. This buffering time tends to be decided as a safe large period to guarantee correctness in terms of time-ordering, and leads to an increase of latency. Furthermore, failure recovery mechanism of a stateful stream processing deployed into distributed processing nodes makes this problem more difficult. It frequently causes large time-order reversal among event messages in the distributed stream processing system, that cannot be handled with a buffering time designed for real-time processing during normal periods. In this paper, we introduce a novel technique to adaptively determine the minimum necessary amount of buffering time to guarantee time-order sorting depending on the event-time progress in arrival event messages. To be more precise, instead of specifying the actual buffering time, the method adaptively controls the minimum time margin to determine the current “watermark” that shows the event time upon which a processing unit in a distributed stream processing system believes to have received all the messages. Further, we developed a complete function to sort messages in event-time order while keeping low latency in our developed distributed event stream processing platform. Through the experimental evaluations, it is shown that the developed method has the capability to determine a proper watermark to achieve both a strong guarantee on time-order correctness and low latency in real-time event stream processing."
pub.1094372353,Data stream generation for concurrent computation in VLSI signal processors,"Signal processing usually requires extremely high computing power. Fortunately, with advance in VLSI technology, the required performance could be achieved with more functional units performing concurrent computations on a chip. Supplying demanding data streams to these computational units soon becomes the system-performance bottleneck because of slow off-chip I/O and memory with less improvement speed. We have proposed a data stream generation (DSG) scheme that explores the data reuse property existing in most signal processing algorithms while supplying appropriate data sequences. This scheme can make the VLSI signal processors much more latency and cost efficient. In the illustrating example, our DSG supplies the specified streams with 4-cycle setup time at the beginning (latency), instead of 48 cycles for each block in conventional FIFO approaches and only requires 1/6-storage elements."
pub.1170173882,PECJ: Stream Window Join on Disorder Data Streams with Proactive Error Compensation,"Stream Window Join (SWJ), a vital operation in stream analytics, struggles with achieving a balance between accuracy and latency due to out-of-order data arrivals. Existing methods predominantly rely on adaptive buffering, but often fall short in performance, thereby constraining practical applications. We introduce PECJ, a solution that proactively incorporates unobserved data to enhance accuracy while reducing latency, thus requiring robust predictive modeling of stream oscillation. At the heart of PECJ lies a mathematical formulation of the posterior distribution approximation (PDA) problem using variational inference (VI). This approach circumvents error propagation while meeting the low-latency demands of SWJ. We detail the implementation of PECJ, striking a balance between complexity and generality, and discuss both analytical and learning-based approaches. Experimental evaluations reveal PECJ's superior performance. The successful integration of PECJ into a multi-threaded SWJ benchmark testbed further establishes its practical value, demonstrating promising advancements in enhancing data stream processing capabilities amidst out-of-order data."
pub.1119983284,VAStream,"Processing high-volume, high-velocity data streams is an important big data problem in many sciences, engineering, and technology domains. There are many open-source distributed stream processing and cloud platforms that offer low-latency stream processing at scale, but the visualization and user-interaction components of these systems are limited to visualizing the outcome of stream processing results. Visual analysis represents a new form of analysis where the user has more control and interactive capabilities either to dynamically change the visualization, analytics or data management processes. VAStream provides an environment for big data stream processing along with interactive visualization capabilities. The system environment consists of hardware and software modules to optimize streaming data workflow (that includes data ingest, pre-processing, analytics, visualization, and collaboration components). The system environment is evaluated for two real-time streaming applications. The real-time event detection using social media streams uses text data arriving from sources such as Twitter to detect emerging events of interest. The real-time river sensor network analysis project uses unsupervised classification methods to classify sensor network streams arriving from the US river network to detect water quality problems. We discuss implementation details and provide performance comparison results of various individual stream processing operations for both stream processing applications."
pub.1170778529,LARA: Latency-Aware Resource Allocator for Stream Processing Applications,"One of the key metrics of interest for stream processing applications is “latency”, which indicates the total time it takes for the application to process and generate insights from streaming input data. For mission-critical video analytics applications like surveillance and monitoring, it is of paramount importance to report an incident as soon as it occurs so that necessary actions can be taken right away. Stream processing applications are typically developed as a chain of microser-vices and are deployed on container orchestration platforms like Kubernetes. Allocation of system resources like “cpu” and “memory” to individual application microservices has direct impact on “latency”. Kubernetes does provide ways to allocate these resources e.g. through fixed resource allocation or through vertical pod autoscaler (VPA), however there is no straight-forward way in Kubernetes to prioritize “latency” for an end-to-end application pipeline. In this paper, we present LARA, which is specifically designed to improve “latency” of stream processing application pipelines. LARA uses a regression-based technique for resource allocation to individual microservices. We implement four real-world video analytics application pipelines i.e. license plate recognition, face recognition, human attributes detection and pose detection, and show that compared to fixed allocation, LARA is able to reduce latency by up to 2.8X and is consistently better than VPA. While reducing latency, LARA is also able to deliver over 2X throughput compared to fixed allocation and is almost always better than VPA."
pub.1022851873,FastFlow: Efficient Scalable Model‐Driven Framework for Processing Massive Mobile Stream Data,"Massive stream data mining and computing require dealing with an infinite sequence of data items with low latency. As far as we know, current Stream Processing Engines (SPEs) cannot handle massive stream data efficiently due to their inability of horizontal computation modeling and lack of interactive query. In this paper, we detail the challenges of stream data processing and introduce FastFlow, a model-driven infrastructure. FastFlow differs from other existing SPEs in terms of its user-friendly interface, support of complex operators, heterogeneous outputs, extensible computing model, and real-time deployment. Further, FastFlow includes optimizers to reorganize the execution topology for batch query to reduce resource cost rather than executing each query independently."
pub.1133259175,Boosting Big Data Streaming Applications in Clouds With BurstFlow,"The rapid growth of stream applications in financial markets, health care, education, social media, and sensor networks represents a remarkable milestone for data processing and analytic in recent years, leading to new challenges to handle Big Data in real-time. Traditionally, a single cloud infrastructure often holds the deployment of Stream Processing applications because it has extensive and adaptative virtual computing resources. Hence, data sources send data from distant and different locations of the cloud infrastructure, increasing the application latency. The cloud infrastructure may be geographically distributed and it requires to run a set of frameworks to handle communication. These frameworks often comprise a Message Queue System and a Stream Processing Framework. The frameworks explore Multi-Cloud deploying each service in a different cloud and communication via high latency network links. This creates challenges to meet real-time application requirements because the data streams have different and unpredictable latencies forcing cloud providers’ communication systems to adjust to the environment changes continually. Previous works explore static micro-batch demonstrating its potential to overcome communication issues. This paper introduces BurstFlow, a tool for enhancing communication across data sources located at the edges of the Internet and Big Data Stream Processing applications located in cloud infrastructures. BurstFlow introduces a strategy for adjusting the micro-batch sizes dynamically according to the time required for communication and computation. BurstFlow also presents an adaptive data partition policy for distributing incoming streams across available machines by considering memory and CPU capacities. The experiments use a real-world multi-cloud deployment showing that BurstFlow can reduce the execution time up to 77% when compared to the state-of-the-art solutions, improving CPU efficiency by up to 49%."
pub.1113978664,A Fog Node Architecture for Real-Time Processing of Urban IoT Data Streams,"Sensors and IoT devices in smart cities generate vast volumes of data that need to be harnessed to help smart city applications make informed decisions on the fly. However, time-sensitive applications cannot tolerate sending data streams to the cloud for processing because of the unacceptable latency and network bandwidth requirements. Cities require the ability to efficiently stream data and process data streams in real-time at the edge. This paper describes a fog node-based architecture that aims to deal with the streaming and processing of data generated by the various devices and equipment of the city to enable the creation of value-added services. The core components of the fog node are a powerful distributed messaging system and a powerful stream processing engine that can adequately scale with the data volumes."
pub.1130468544,Smart Community Edge: Stream Processing Edge Computing Node for Smart Community Services,"A smart community utilizes information technology to interconnect and manage community infrastructures. Smart community networks should support a large number of Internet of Things (IoT) devices in community infrastructures to provide services such as smart grids and health monitoring systems. In comparison to cloud-based solutions, smart community services can be deployed in the edge computing area to reduce service latency and to encapsulate private and local information. Furthermore, smart community services can leverage network virtualization technologies to support IoT network services at the edge. A service-oriented container-based solution that processes data streams from IoT sensors using conventional hardware will improve the compatibility and latency of these virtualized network services at the edge. To this end, a software-based edge computing node, namely, the smart community edge (SCE), was proposed to develop a platform for smart community services. SCE supports data-tapping applications, especially for IoT devices, and has a stream processing feature with a comparatively shorter processing delay. This tapping and processing function was named multi-service authorized stream content analysis. SCE captures network stream data and enables service applications using shared memory buffers for a shorter processing delay. SCE supports services as Docker containers to provide remote deployment, service compatibility, and service isolation. SCE allows IoT services to run at the edge through conventional hardware devices, thus, reducing the service latency for delay-sensitive services, which approximately require to sustain latency less than 10 ms. The proposed SCE achieves 10 Gbps bandwidth with a 16 core server when compared to the f-stack library with a 5 Gbps bandwidth. SCE deployment on conventional hardware devices shows its capability of operating at 1-10 Gbps line rates to support up to eight services at 500 Mbps data bandwidth per service, while keeping the overall latency below 1 ms. Therefore, SCE provides a platform for delay-sensitive IoT services at the network edge."
pub.1099593039,Characterization of Big Data Stream Processing Pipeline,"In recent years there has been a surge in applications focusing on streaming data to generate insights in real-time. Both academia, as well as industry, have tried to address this use case by developing a variety of Stream Processing Engines (SPEs) with a diverse feature set. On the other hand, Big Data applications have started to make use of High-Performance Computing (HPC) which possess superior memory, I/O, and networking resources compared to typical Big Data clusters. Recent studies evaluating the performance of SPEs have focused on commodity clusters. However, exhaustive studies need to be performed to profile individual stages of a stream processing pipeline and how best to optimize each of these stages to best leverage the resources provided by HPC clusters. To address this issue, we profile the performance of a big data streaming pipeline using Apache Flink as the SPE and Apache Kafka as the intermediate message queue. We break the streaming pipeline into two distinct phases and evaluate percentile latencies for two different networks, namely 40GbE and InfiniBand EDR (100Gbps), to determine if a typical streaming application is network intensive enough to benefit from a faster interconnect. Moreover, we explore whether the volume of input data stream has any effect on the latency characteristics of the streaming pipeline, and if so how does it compare for different stages in the streaming pipeline and different network interconnects. Our experiments show an increase of over 10x in 98 percentile latency when input stream volume is increased from 128MB/s to 256MB/s. Moreover, we find the intermediate stages of the stream pipeline to be a significant contributor to the overall latency of the system."
pub.1016078937,Stream Processing on Demand for Lambda Architectures,"Growing amounts of data and the demand to process them within time constraints have led to the development of big data systems. A generic principle to design such systems that allows for low latency results is called the lambda architecture. It defines that data is analyzed twice by combining batch and stream processing techniques in order to provide a real time view. This redundant processing of data makes this architecture very expensive. In cases where process results are not continuously required to be low latency or time constraints lie within several minutes, a clear decision whether both processing layers are inevitable is not possible yet. Therefore, we propose stream processing on demand within the lambda architecture in order to efficiently use resources and reduce hardware investments. We use performance models as an analytical decision-making solution to predict response times of batch processes and to decide when to additionally deploy stream processes. By the example of a smart energy use case we implement and evaluate the accuracy of our proposed solution."
pub.1104600057,Viper: A module for communication-layer determinism and scaling in low-latency stream processing," Stream Processing Engines (SPEs) process continuous streams of data and produce results in a real-time fashion, typically through one-at-a-time tuple analysis. In Fog architectures, the limited resources of the edge devices, enabling close-to-the-source scalable analysis, demand for computationally- and energy-efficient SPEs. When looking into the vital SPE processing properties required from applications, determinism, which ensures consistent results independently of the way the analysis is parallelized, has a strong position besides scalability in throughput and low processing latency. SPEs scale in throughput and latency by relying on shared-nothing parallelism, deploying multiple copies of each operator to which tuples are distributed based on its semantics. The coordination of the asynchronous analysis of parallel operators required to enforce determinism is then carried out by additional dedicated sorting operators. To prevent this costly coordination from becoming a bottleneck, we introduce the Viper communication module, which can be integrated in the SPE communication layer and boost the coordination of the parallel threads analyzing the data. Using Apache Storm and data extracted from the Linear Road benchmark and a real-world smart grid system, we show benefits in the throughput, latency and energy efficiency coming from the utilization of the Viper module."
pub.1029666475,Design principles for developing stream processing applications,"Abstract Stream processing applications are used to ingest, process, and analyze continuous data streams from heterogeneous sources of live and stored data, generating streams of output results. These applications are, in many cases, complex, large‐scale, low‐latency, and distributed in nature. In this paper, we describe the design principles and architectural underpinnings for stream processing applications. These principles are distilled from our experience in building real‐world applications both for internal use as well as with customers from several industrial and academic domains. We provide principles, guidelines, as well as appropriate implementation examples to highlight the different aspects of stream processing application design and development. Copyright © 2010 John Wiley & Sons, Ltd."
pub.1131342922,Reducing Fault-tolerant Overhead for Distributed Stream Processing with Approximate Backup,"The stream processing model continuously processes online data in an on-pass fashion that can be more vulnerable to failures than other offline-data processing schemes. Checkpoint-based fault-tolerant methods have been widely used to enhance the reliability of stream processing systems. To ensure exact data recoveries upon failures, full-backup mechanisms are used to store a complete copy of data, which introduces substantial runtime overhead and increases output latency. In the meantime, a wide range of online processing applications prefer quick-and-dirty results with a slight degradation inaccuracy to delayed exact results. This paper introduces a novel approximate fault-tolerant problem (OAFP) with the objective of reducing the failure-free fault-tolerant overhead and ensuring user-defiled output accuracy requirement upon failure at the same time. We present an approximate fault-tolerant scheme based on sampling backup mechanism and study the trade-off between fault-tolerant overhead and output accuracy in stream processing systems. We proposed two algorithms to compute backup plans for both single-node failure and correlated failure scenarios. Extensive experiments with different types of stream topologies are conducted on our simulator to verify the correctness and effectiveness of our approach. We prove our solution guarantees the output accuracy requirement with minimum FT latency for directed acyclic graph (DAG) stream topologies with single-node failures."
pub.1004868216,Quality-Driven Continuous Query Execution over Out-of-Order Data Streams,"Executing continuous queries over out-of-order data streams, where tuples are not ordered according to timestamps, is challenging; because high result accuracy and low result latency are two conflicting performance metrics. Although many applications allow trading exact query results for lower latency, they still expect the produced results to meet a certain quality requirement. However, none of existing disorder handling approaches have considered minimizing the result latency while meeting user-specified requirements on the quality of query results. In this demonstration, we showcase AQ-K-slack, an adaptive, buffer-based disorder handling approach, which supports executing sliding window aggregate queries over out-of-order data streams in a quality-driven manner. By adapting techniques from the field of sampling-based approximate query processing and control theory, AQ-K-slack dynamically adjusts the input buffer size at query runtime to minimize the result latency, while respecting a user-specified threshold on relative errors in produced query results. We demonstrate a prototype stream processing system, which extends SAP Event Stream Processor with the implementation of AQ-K-slack. Through an interactive interface, the audience will learn the effect of different factors, such as the aggregate function, the window specification, the result error threshold, and stream properties, on the latency and the accuracy of query results. Moreover, they can experience the effectiveness of AQ-K-slack in obtaining user-desired latency vs. result accuracy trade-offs, compared to naive disorder handling approaches that make extreme trade-offs. For instance, by scarifying 1% result accuracy, our system can reduce the result latency by 80% when compared to the state of the art."
pub.1094561429,A Modified Chain Scheduling Algorithm in Data Stream System,"For the applications that require real-time processing of high-volume data streams, the scheduling strategy must be adaptability. The Chain algorithm focuses solely on minimizing the maximum run-time memory usage, ignoring the important aspect of output latency. Our aim is to design a scheduling strategy that minimizes the maximum run-time system memory, while maintaining the output latency within specified bounds."
pub.1105796446,TurboStream: Towards Low-Latency Data Stream Processing,"Data Stream Processing (DSP) applications are often modelled as a directed acyclic graph: operators with data streams among them. Inter-operator communications can have a significant impact on the latency of DSP applications, accounting for 86% of the total latency. Despite their impact, there has been relatively little work on optimizing inter-operator communications, focusing on reducing inter-node traffic but not considering inter-process communication (IPC) inside a node, which often generates high latency due to the multiple memory-copy operations. This paper describes the design and implementation of TurboStream, a new DSP system designed specifically to address the high latency caused by inter-operator communications. To achieve this goal, we introduce (1) an improved IPC framework with OSRBuffer, a DSP-oriented buffer, to reduce memory-copy operations and waiting time of each single message when transmitting messages between the operators inside one node, and (2) a coarse-grained scheduler that consolidates operator instances and assigns them to nodes to diminish the inter-node IPC traffic. Using a prototype implementation, we show that our improved IPC framework reduces the end-to-end latency of intra-node IPC by 45.64% to 99.30%. Moreover, TurboStream reduces the latency of DSP by 83.23% compared to JStorm."
pub.1140367485,The Challenges and Prerequisites of Data Stream Processing in Fog Environment for Digital Twin in Smart Industry,"Smart industry systems are based on integrating historical and current data from sensors with physical and digital systems to control product states. For example, Digital Twin (DT) system predicts the future state of physical assets using live simulation and controls the current state through real-time feedback. These systems rely on the ability to process big data stream to provide real-time responses. For, example it is estimated that one autonomous vehicle (AV) could produce 30 terabytes of data per day. AV will not be on the road before using an effective way to managing its big data and solve latency challenges. Cloud computing failed in the latency challenge, while Fog computing addresses it by moving parts of the computations from the Cloud to the edge of the network near the asset to reduce the latency. This work studies the challenges in data stream processing for DT in a fog environment. The challenges include fog architecture, the necessity of loosely-coupling design, the used virtual machine versus container, the stateful versus stateless operations, the stream processing tools, and live migration between fog nodes. The work also proposes a fog computing architecture and provides a vision of the prerequisites to meet the challenges."
pub.1171899080,Improvement of Data Stream Processing using Adaptive Ingestion,"Adaptive batch sizing algorithm offers several advantages over static sizing in various real-world scenarios and use cases especially Streaming Data Processing. The data ingestion layer in distributed stream processing system plays a crucial role in the overall stream processing system. It is responsible for accessing data from diverse locations and delivering it to the processing layer. In the realm of data ingestion, achieving low latency, high throughput, and scalability across multiple producers and consumers is an essential mandate. To elevate performance and availability during the data ingestion process dynamic batch interval control algorithm has been proposed. This algorithm adjusts the length of batch intervals dynamically based on data ingestion rates, thereby optimizing the data stream ingestion layer. It is implemented and operated within the Apache Flume to facilitating the transmission of data to Spark Streaming, Experimental results demonstrate that the proposed batch interval control scheme yields significantly higher throughput and adjustable processing time during the data stream ingestion phase compared to a static batch interval approach."
pub.1071882266,Enabling RDF Stream Processing for Sensor Data Management in the Environmental Domain,"<p>This paper presents a generic approach to integrate environmental sensor data efficiently, allowing the detection of relevant situations and events in near real-time through continuous querying. Data variety is addressed with the use of the Semantic Sensor Network ontology for observation data modelling, and semantic annotations for environmental phenomena. Data velocity is handled by distributing sensor data messaging and serving observations as RDF graphs on query demand. The stream processing engine presented in the paper, morph-streams++, provides adapters for different data formats and distributed processing of streams in a cluster. An evaluation of different configurations for parallelization and semantic annotation parameters proves that the described approach reduces the average latency of message processing in some cases.</p>"
pub.1118539523,Architectural Impact on Performance of In-memory Data Analytics: Apache Spark Case Study,"While cluster computing frameworks are continuously evolving to provide
real-time data analysis capabilities, Apache Spark has managed to be at the
forefront of big data analytics for being a unified framework for both, batch
and stream data processing. However, recent studies on micro-architectural
characterization of in-memory data analytics are limited to only batch
processing workloads. We compare micro-architectural performance of batch
processing and stream processing workloads in Apache Spark using hardware
performance counters on a dual socket server. In our evaluation experiments, we
have found that batch processing are stream processing workloads have similar
micro-architectural characteristics and are bounded by the latency of frequent
data access to DRAM. For data accesses we have found that simultaneous
multi-threading is effective in hiding the data latencies. We have also
observed that (i) data locality on NUMA nodes can improve the performance by
10% on average and(ii) disabling next-line L1-D prefetchers can reduce the
execution time by up-to 14\% and (iii) multiple small executors can provide
up-to 36\% speedup over single large executor."
pub.1130371931,PStream: A Popularity-Aware Differentiated Distributed Stream Processing System,"Real-world stream data with skewed distributions raises unique challenges to distributed stream processing systems. Existing stream workload partitioning schemes usually use a “one size fits all” design, which leverages either a shuffle grouping or a key grouping strategy for partitioning the stream workloads among multiple processing units, leading to notable problems of unsatisfied system throughput and processing latency. In this article, we show that the key grouping based schemes result in serious load imbalance and low computation efficiency in the presence of data skewness while the shuffle grouping schemes are not scalable in terms of memory space. We argue that the key to efficient stream scheduling is the popularity of the stream data. We propose PStream, a popularity-aware differentiated distributed stream processing system which assigns the hot keys using shuffle grouping while assigns rare ones using key grouping. PStream leverages a novel light-weighted probabilistic counting scheme for identifying the currently hot keys in dynamic real-time streams. The scheme is extremely efficient in computation and memory consumption, so that the predictor based on it can be well integrated into processing instances in the system. We further design an adaptive threshold configuration scheme, which can quickly adapt to the dynamical popularity changes in highly dynamical real-time streams. We implement PStream on top of Apache Storm and conduct comprehensive experiments using large-scale traces from real-world systems to evaluate the performance of this design. Results show that PStream achieves a 2.3× improvement in terms of processing throughput and reduces the processing latency by 64 percent compared to state-of-the-art designs."
pub.1091641335,Strider,"Real-time processing of data streams emanating from sensors is becoming a common task in industrial scenarios. An increasing number of processing jobs executed over such platforms are requiring reasoning mechanisms. The key implementation goal is thus to efficiently handle massive incoming data streams and support reasoning, data analytic services. Moreover, in an on-going industrial project on anomaly detection in large potable water networks, we are facing the effect of dynamically changing data and work characteristics in stream processing. The Strider system addresses these research and implementation challenges by considering scalability, fault-tolerance, high throughput and acceptable latency properties. We will demonstrate the benefits of Strider on an Internet of Things-based real world and industrial setting."
pub.1004611432,Partitioning for Scalable Complex Event Processing on Data Streams,"Many applications processing dynamic data require to filter, aggregate, join as well as to recognize event patterns in streams of data in an online fashion. However, data analysis and complex event processing (CEP) on high volume and/or high rate streams are challenging tasks. Typically, partitioning techniques are leveraged for achieving low latency and scalable processing. Unfortunately, sequence-based operations such as CEP operations as well as long-running continuous queries make partitioning much more difficult than for batch-oriented approaches.In this paper, we address this challenge by presenting partitioning strategies for CEP queries. We discuss two strategies for stream and pattern partitioning and we present a cost-based optimization approach for determining the number of partitions as well as the split points in the queries to achieve better load balancing and avoid congestions of processing nodes in a cluster environment."
pub.1091750682,SLIIC: System-Level Intelligent Intensive Computing,Recently several architectural approaches have been explored that promise to hide memory latency for applications that include data-intensive applications while improving scalability The SLIIC project demonstrated and compared some of the advantages and disadvantages of the PIM (processor-in-memory) and stream processing approaches to hiding memory latency. The SLIIC project built board prototypes for PIM and stream processing architectures and implemented data-intensive applications in simulation and in hardware to measure the performance. Speedups of up to 54 measured in cycles and 16 measured in execution time were obtained over commercial microprocessors.
pub.1094998514,Fulfilling End-to-End Latency Constraints in Large-scale Streaming Environments,"The on-line processing of high volume data streams is a prerequisite for many modern applications relying on real-time data such as global sensor networks or multimedia streaming. In order to achieve efficient data processing and scalability w.r.t. the number of distributed data sources and applications, in-network processing of data streams in an overlay network of data processing operators has been proposed. For such stream processing overlay networks, the placement of operators onto physical hosts plays an important role for the resulting quality of service-in particular, the end-to-end latency-and network load. To this end, we present an enhanced placement algorithm that minimizes the network load put onto the system by a stream processing task under user-defined delay constraints in this paper. Our algorithm finds first the optimal solution in terms of network load and then degrades this solution to find a constrained optimum. In order to reduce the overhead of the placement algorithm, we included mechanisms to reduce the search space in terms of hosts that are considered during operator placement. Our evaluations show that this approach leads to an operator placement of high quality solution while inducing communication overhead proportional only to a small percentage of the total hosts."
pub.1142450272,LMStream: When Distributed Micro-Batch Stream Processing Systems Meet GPU,"This paper presents LMStream, which ensures bounded latency while maximizing
the throughput on the GPU-enabled micro-batch streaming systems. The main ideas
behind LMStream's design can be summarized as two novel mechanisms: (1) dynamic
batching and (2) dynamic operation-level query planning. By controlling the
micro-batch size, LMStream significantly reduces the latency of individual
dataset because it does not perform unconditional buffering only for improving
GPU utilization. LMStream bounds the latency to an optimal value according to
the characteristics of the window operation used in the streaming application.
Dynamic mapping between a query to an execution device based on the data size
and dynamic device preference improves both the throughput and latency as much
as possible. In addition, LMStream proposes a low-overhead online cost model
parameter optimization method without interrupting the real-time stream
processing. We implemented LMStream on Apache Spark, which supports micro-batch
stream processing. Compared to the previous throughput-oriented method,
LMStream showed an average latency improvement up to a maximum of 70.7%, while
improving average throughput up to 1.74x."
pub.1105053106,A Scalable Architecture for Real-Time Stream Processing of Spatiotemporal IoT Stream Data—Performance Analysis on the Example of Map Matching,"Scalable real-time processing of large amounts of data has become a research topic of particular importance due to the continuously rising amount of data that is generated by devices equipped with sensing components. While existing approaches allow for fault-tolerant and scalable stream processing, we present a pipeline architecture that consists of well-known open source tools to specifically integrate spatiotemporal internet of things (IoT) data streams. In a case study, we utilize the architecture to tackle the online map matching problem, a pre-processing step for trajectory mining algorithms. Given the rising amount of vehicle location data that is generated on a daily basis, existing map matching algorithms have to be implemented in a distributed manner to be executable in a stream processing framework that provides scalability. We demonstrate how to implement state-of-the-art map matching algorithms in our distributed stream processing pipeline and analyze measured latencies."
pub.1149659019,"Research Summary: Deterministic, Explainable and Efficient Stream Processing","The vast amounts of data collected and processed by technologies such as Cyber-Physical Systems require new processing paradigms that can keep up with the increasing data volumes. Edge computing and stream processing are two such paradigms that, combined, allow users to process unbounded datasets in an online manner, delivering high-throughput, low-latency insights. Moving stream processing to the edge introduces challenges related to the heterogeneity and resource constraints of the processing infrastructure. In this work, we present state-of-the-art research results that improve the facilities of Stream Processing Engines (SPEs) with data provenance, custom scheduling, and other techniques that can support the usability and performance of streaming applications, spanning through the edge-cloud contexts, as needed."
pub.1165619901,An Elastic Scalable Grouping for Stateful Operators in Stream Computing Systems,"In distributed stream computing systems, dynamic data skew and cluster heterogeneity can lead to major load imbalance among multiple instances of stateful operators. Existing stream grouping schemes mainly focus on data load balancing for stateful operators, but they are not considered to be sufficiently elastic scalable, which directly affects the latency and throughput. We propose an elastic scalable grouping (called Es-Stream) for stateful operators. This paper discusses the following aspects: (1) Investigating the dynamic grouping of real-time data stream, proposing a general data stream graph model and a data stream grouping model, as well as formalizing the problem of load balancing optimization and data stream grouping. (2) Utilizing key splitting to solve the bottleneck problem caused by high-frequency keys in the data streams, and lightweight weight adjustment strategy to dynamically change the data tuple allocation probability of the instance according to the network cost, data stream rate and processing rate. (3) Implementing Es-Stream in Apache Storm platform and evaluating the system using metrics such as latency, throughput and load imbalance. Experimental results showed that Es-Stream reduces latency by up to 72%, increases throughput by up to 44% and reduces load imbalance by up to 75%, compared with existing state-of-the-art grouping schemes."
pub.1022646384,Real-Time Log Analysis Using Hitachi uCosminexus Stream Data Platform,"In this demo, we present real-time log analysis using Hitachi uCosminexus Stream Data Platform, uCSDP for short. Real-time log analysis is one of the key applications that offers preventive measures to detect irregular manipulations and human mistakes in system management, and reduces the risk and loss caused by such operations to the minimum in advance. uCSDP is the stream data processing system featuring its declarative query processing language, flexible time management, RAS support for high-available processing, and eager scheduling for ultra low latency processing. This demo highlights the uCSDP features for realizing real-time log analysis very easily and effectively."
pub.1045798927,Processing high data rate streams in System S,"High-performance stream processing is critical in many sense-and-respond application domains—from environmental monitoring to algorithmic trading. In this paper, we focus on language and runtime support for improving the performance of sense-and-respond applications in processing data from high-rate live streams. The central tenets of this work are the programming model, the workload splitting mechanisms, the code generation framework, and the underlying System S middleware and Spade programming model. We demonstrate considerable scalability behavior coupled with low processing latency in a real-world financial trading application."
pub.1095311929,Minimizing Latency in Fault-Tolerant Distributed Stream Processing Systems,"Event stream processing (ESP) applications target the real-time processing of huge amounts of data. Events traverse a graph of stream processing operators where the information of interest is extracted. As these applications gain popularity, the requirements for scalability, availability, and dependability increase. In terms of dependability and availability, many applications require a precise recovery, i.e., a guarantee that the outputs during and after a recovery would be the same as if the failure that triggered recovery had never occurred. Existing solutions for precise recovery induce prohibitive latency costs, either by requiring continuous checkpoint or logging (in a passive replication approach) or perfect synchronization between replicas executing the same operations (in an active replication approach). We introduce a novel technique to guarantee precise recovery for ESP applications while minimizing the latency costs as compared to traditional approaches. The technique minimizes latencies via speculative execution in a distributed system. In terms of scalability, the key component of our approach is a modified software transactional memory that provides not only the speculation capabilities but also optimistic parallelization for costly operations."
pub.1149876414,Real-Time Big Data Analytics for Data Stream Challenges: An Overview,"The conventional approach of evaluating massive data is inappropriate for real-time analysis; therefore, analysing big data in a data stream remains a critical issue for numerous applications. It is critical in real-time big data analytics to process data at the point where they are arriving at a quick reaction and good decision making, necessitating the development of a novel architecture that allows for real-time processing at high speed and low latency. Processing and anlayzing a data stream in real-time is critical for a variety of applications; however, handling a large amount of data from a variety of sources, such as sensor networks, web traffic, social media, video streams, and other sources, is a considerable difficulty. The main goal of this paper is to give an overview of the current architecture for real time big data analytics, real-time data stream processing methods available, including their system architectures Lambda, kappa, and delta large data stream processing."
pub.1126122958,Stream Processing on Clustered Edge Devices,"The Internet of Things continuously generates avalanches of raw sensor data to be transferred to the Cloud for processing and storage. Due to network latency and limited bandwidth, this vertical offloading model, however, fails to meet requirements of time-critical data-intensive applications which must act upon generated data with minimum time delays. To address such a limitation, this article proposes a novel distributed architecture enabling stream data processing at the edge of the network, broadening the principle of enabling processing closer to data sources adopted by Fog and Edge Computing. Specifically, this architecture extends the Apache NiFi stream processing middleware with support for run-time clustering of heterogeneous edge devices, such that computational tasks can be horizontally offloaded to peer devices and executed in parallel. As opposed to vertical offloading on the Cloud, the proposed solution does not suffer from increased network latency and is thus able to offer 5-25 times faster response time, as demonstrated by the experiments on a run-time license plate recognition system."
pub.1164902548,Edge Cloud Collaborative Stream Computing for Real-Time Structural Health Monitoring,"Structural Health Monitoring (SHM) is crucial for the safety and maintenance
of various infrastructures. Due to the large amount of data generated by
numerous sensors and the high real-time requirements of many applications, SHM
poses significant challenges. Although the cloud-centric stream computing
paradigm opens new opportunities for real-time data processing, it consumes too
much network bandwidth. In this paper, we propose ECStream, an Edge Cloud
collaborative fine-grained stream operator scheduling framework for SHM. We
collectively consider atomic and composite operators together with their
iterative computability to model and formalize the problem of minimizing
bandwidth usage and end-to-end operator processing latency. Preliminary
evaluation results show that ECStream can effectively balance bandwidth usage
and end-to-end operator computation latency, reducing bandwidth usage by 73.01%
and latency by 34.08% on average compared to the cloud-centric approach."
pub.1123211993,SBASH Stack Based Allocation of Sheer Window Architecture for Real Time Stream Data Processing,"The processing of real-time data streams is complex with large number of volume and variety. The volume and variety of data streams enhances a number of processing units to run in real time. The required number of processing units used for processing data streams are lowered by using a windowing mechanism. Therefore, the appropriate size of window selection is vital for stream data processing. The coarse size window will directly affect the overall processing time. On the other hand, a finely sized window has to deal with an increased number of management costs. In order to manage such streams of data, we have proposed a SBASH architecture, which can be helpful for determining a unipartite size of a sheer window. The sheer window reduces the overall latency of data stream processing by a certain extent. The time complexity to process such sheer window is equivalent to w log n w. These windows are allocated and retrieved in a stack-based manner, where stacks ≥ n, which is helpful in reducing the number of comparisons made during retrieval."
pub.1117328384,Reproducible and Reliable Distributed Classification of Text Streams,"Large-scale classification of text streams is an essential problem that is hard to solve. Batch processing systems are scalable and proved their effectiveness for machine learning but do not provide low latency. On the other hand, state-of-the-art distributed stream processing systems are able to achieve low latency but do not support the same level of fault tolerance and determinism. In this work, we discuss how the distributed streaming computational model and fault tolerance mechanisms can affect the correctness of text classification data flow. We also propose solutions that can mitigate the revealed pitfalls."
pub.1149973002,Micro-batch and Data Frequency for Stream Processing on Multi-cores,"Latency or throughput are often critical performance metrics in stream processing. Applications' performance can fluctuate depending on the input stream. This unpredictability is tied to variations in data arrival frequency, data size, complexity, and other factors. Researchers are constantly investigating new ways to mitigate the impact of these variations on performance with self-adaptive techniques involving elasticity or micro-batching. However, there is a lack of benchmarks capable of creating test scenarios to further evaluate these techniques. This work extends and improves the SPBench benchmarking framework to support dynamic micro-batching and data stream frequency management. We also propose a set of algorithms that generate the most commonly used frequency patterns for benchmarking stream processing in related work. It allows the creation of a wide variety of test scenarios. To validate our solution, we use SPBench to create custom benchmarks and evaluate the impact of micro-batching and data stream frequency on the performance of Intel TBB and FastFlow. These are two libraries that leverage stream parallelism for multi-core architectures. Our results demonstrated that most test cases benefited from micro-batches, especially high throughput applications with ordering constraints. For different data stream frequency configurations, TBB ensured the lowest latency, while FastFlow assured higher throughput in shorter pipelines."
pub.1099592994,Low Latency Stream Processing,"Worldwide data production is increasing both in volume and velocity, and with this acceleration, data needs to be processed in streaming settings as opposed to the traditional store and process model. Distributed streaming frameworks are designed to process such data in real time with reasonable time constraints. Apache Heron is a production-ready large-scale distributed stream processing framework. The network is of utmost importance to scale streaming applications to large numbers of nodes with a reasonable latency. High performance computing (HPC) clusters feature interconnects that can perform at higher levels than traditional Ethernet. In this paper the authors present their findings on integrating Apache Heron distributed stream processing system with two high performance interconnects; Infiniband and Intel Omni-Path and show that they can be utilized to improve performance of distributed streaming applications."
pub.1034622075,Potential-driven load distribution for distributed data stream processing,"A large class of applications require real-time processing of continuous stream data resulting in the development of data stream management systems (DSMS). Since many of these applications are distributed, distributed DSMSs are starting to receive attention. In this paper, we focus on an important issue in distributed DSMS operation, namely load distribution to minimize end-to-end latency. We identify the often conflicting requirements of load distribution, and propose a ""potential-driven"" load distribution approach to mimic the movements of objects in the physical world. Our approach also takes into account heterogeneous machines, different network conditions, and resource constraints. We present experimental results that investigate our algorithms from various aspects, and show that they outperform existing techniques in terms of end-to-end latency."
pub.1151540526,Combining stream with data parallelism abstractions for multi-cores,"Stream processing applications have seen an increasing demand with the raised availability of sensors, IoT devices, and user data. Modern systems can generate millions of data items per day that require to be processed timely. To deal with this demand, application programmers must consider parallelism to exploit the maximum performance of the underlying hardware resources. In this work, we introduce improvements to stream processing applications by exploiting fine-grained data parallelism (via Map and MapReduce) inside coarse-grained stream parallelism stages. The improvements are including techniques for identifying data parallelism in sequential codes, a new language, semantic analysis, and a set of definition and transformation rules to perform source-to-source parallel code generation. Moreover, we investigate the feasibility of employing higher-level programming abstractions to support the proposed optimizations. For that, we elect SPar programming model as a use case, and extend it by adding two new attributes to its language and implementing our optimizations as a new algorithm in the SPar compiler. We conduct a set of experiments in representative stream processing and data-parallel applications. The results showed that our new compiler algorithm is efficient and that performance improved by up to 108.4x in data-parallel applications. Furthermore, experiments evaluating stream processing applications towards the composition of stream and data parallelism revealed new insights. The results showed that such composition may improve latencies by up to an order of magnitude. Also, it enables programmers to exploit different degrees of stream and data parallelism to accomplish a balance between throughput and latency according to their necessity."
pub.1084708350,Large-Scale Data Stream Processing Systems,"In our data-centric society, online services, decision making, and other aspects are increasingly becoming heavily dependent on trends and patterns extracted from data. A broad class of societal-scale data management problems requires system support for processing unbounded data with low latency and high throughput. Large-scale data stream processing systems perceive data as infinite streams and are designed to satisfy such requirements. They have further evolved substantially both in terms of expressive programming model support and also efficient and durable runtime execution on commodity clusters. Expressive programming models offer convenient ways to declare continuous data properties and applied computations, while hiding details on how these data streams are physically processed and orchestrated in a distributed environment. Execution engines provide a runtime for such models further allowing for scalable yet durable execution of any declared computation. In this chapter we introduce the major design aspects of large scale data stream processing systems, covering programming model abstraction levels and runtime concerns. We then present a detailed case study on stateful stream processing with Apache Flink, an open-source stream processor that is used for a wide variety of processing tasks. Finally, we address the main challenges of disruptive applications that large-scale data streaming enables from a systemic point of view."
pub.1017075798,Column Access-aware In-stream Data Cache with Stream Processing Framework,"In recent years, researches focus on addressing the query bottleneck issue of big data, e.g. NoSQL databases, MapReduce and big data processing framework. Although NoSQL databases have many advantages on On-Line Analytical Processing (OLAP), it is a big project to migrate Relational Database Management System (RDBMS) to NoSQL. Therefore, the optimization of RDBMS is still important. In this paper, we construct Column Access-aware In-stream Data Cache (CAIDC) for relational databases, which is an integral part of RDBMS and in-memory cache. Furthermore, a live synchronization approach from physical RDBMS to in-memory data cache using stream processing framework is proposed. On one hand, CAIDC provides low latency while supporting log-based trigger in the presence of updates to maintain data consistency because of stream processing framework. On the other hand, CAIDC translates the frequently accessed data to column-oriented in-memory cache by the column access frequency to ensure heavy hitter queries. Finally, experimental results show that this approach is supporting a wide range of applications with big data."
pub.1156523192,TriJoin: A Time-Efficient and Scalable Three-Way Distributed Stream Join System,"<p>Stream join is one of the most fundamental operations in data stream processing applications. Existing distributed stream join systems can support efficient two-way join, which is a join operation between two streams. Based the two-way join, implementing a three-way join require to be split into double two-way joins, where the second two-way join needs to wait for the join result transmitted from the first two-way join. We show through experiments that such a design raises prohibitively high processing latency. To solve this problem, we propose TriJoin, a time-efficient three-way distributed stream join system. We design a symmetric wait-free structure by symmetrically partitioning tuples and reused join. TriJoin utilizes reused join to join each new tuple with the intermediate result of the other two streams and stored tuples locally. For a new tuple, TriJoin only joins it with the intermediate result to generate the final result without waiting, greatly reducing the processing latency. In TriJoin, we design two partitioning and storage schemes according to two different forms of three-way stream join. We implement TriJoin and conduct comprehensive experiments to evaluate the performance using real-world traces. Results show that TriJoin significantly reduces the processing latency by up to 68%, compared to existing designs.</p> <p> </p>"
pub.1122492388,Neptune,"Distributed dataflow systems allow users to express a wide range of computations, including batch, streaming, and machine learning. A recent trend is to unify different computation types as part of a single stream/batch application that combines latency-sensitive (""stream"") and latency-tolerant (""batch"") jobs. This sharing of state and logic across jobs simplifies application development. Examples include machine learning applications that perform batch training and low-latency inference, and data analytics applications that include batch data transformations and low-latency querying. Existing execution engines, however, were not designed for unified stream/batch applications. As we show, they fail to schedule and execute them efficiently while respecting their diverse requirements. We present Neptune, an execution framework for stream/batch applications that dynamically prioritizes tasks to achieve low latency for stream jobs. Neptune employs coroutines as a lightweight mechanism for suspending tasks without losing task progress. It couples this fine-grained control over CPU resources with a locality-and memory-aware (LMA) scheduling policy to determine which tasks to suspend and when, thereby sharing executors among heterogeneous jobs. We evaluate our open-source Spark-based implementation of Neptune on a 75-node Azure cluster. Neptune achieves up to 3x lower end-to-end processing latencies for latency-sensitive jobs of a stream/batch application, while minimally impacting the throughput of batch jobs."
pub.1111164150,DABS-Storm: A Data-Aware Approach for Elastic Stream Processing,"In the last decade, stream processing has become a very active research domain motivated by the growing number of stream-based applications. These applications make use of continuous queries, which are processed by a stream processing engine (SPE) to generate timely results given the ephemeral input data. Variations of input data streams, in terms of both volume and distribution of values, have a large impact on computational resource requirements. Dynamic and Automatic Balanced Scaling for Storm (DABS-Storm) is an original solution for handling dynamic adaptation of continuous queries processing according to evolution of input stream properties, while controlling the system stability. Both fluctuations in data volume and distribution of values within data streams are handled by DABS-Storm to adjust the resources usage that best meets processing needs. To achieve this goal, the DABS-Storm holistic approach combines a proactive auto-parallelization algorithm with a latency-aware load balancing strategy."
pub.1044084568,Dynamic routing of data stream tuples among parallel query plan running on multi-core processors,"In this paper, a method for fast processing of data stream tuples in parallel execution of continuous queries over a multiprocessing environment is proposed. A copy of the query plan is assigned to each of processing units in the multiprocessing environment. Dynamic and continuous routing of input data stream tuples among the graph constructed by these copies (called the Query Mega Graph) for each input tuple determines that, after getting processed by each processing unit (e.g., processor), to which next processor it should be forwarded. Selection of the proper next processor is performed such that the destination processor imposes the minimum tuple latency to the corresponding tuple, among all of the alternative processors. The tuple latency is derived from processing, buffering and communication time delay which varies in different practical parallel systems.Parallel system architectures that would be suitable as the desired multiprocessing environment for employing the proposed Dynamic Tuple Routing (DTR) method are considered and analyzed. Also, practical challenges and issues for the proper parallel underlying system are discussed. Implementation of the desired parallel system on multi-core systems is provided and used for evaluating the proposed DTR method. Evaluation results show that the proposed DTR method outperforms similar method such as the Eddies in terms of tuple latency, throughput and tuple loss."
pub.1117021018,Optimal Placement of Stream Processing Operators in the Fog,"Elastic data stream processing enables applications to query and analyze streams of real time data. This is commonly facilitated by processing the flow of the data streams using a collection of stream processing operators which are placed in the cloud. However, the cloud follows a centralized approach which is prone to high latency delay. For avoiding this delay, we leverage on the fog computing paradigm which extends the cloud to the edge of the network. In order to design a stream processing solution for the fog, we first formulate an optimization problem for the placement of stream processing operators, which is tailored to fog computing environments. Then, we build a plugin (for stream processing frameworks) which solves the optimization problem periodically in order to support the dynamic resources of the fog. We evaluate this approach by performing experiments on an OpenStack testbed. The results show that our plugin reduces the response time and the cost by 31.5% and 8.8% respectively, compared to optimizing the placement of operators only upon initialization."
pub.1170173111,A Latency Guaranteed Scheduling Strategy under Performance Constraints in Big Data Stream Computing Environments,"Efficient utilization of computing resources in a stream computing environment is crucial for system performance. Existing scheduling strategies can hardly guarantee latency under performance constraints, let alone accounting for the communication cost incurred by scheduling itself. To address these issues, we propose Lg-Stream, a latency guaranteed scheduling strategy under performance constraints. This paper discusses the Lg-Stream strategy from the following aspects: (1) We model the topology as a queuing network to evaluate the system latency; (2) For scenarios with limited resources and latency constraints, we ensure that each executor-to-component allocation optimizes the system's processing latency to the maximum, consequently altering the components’ parallelism; (3) We place executors that communicate with each other on the same node as much as possible. Experimental results demonstrated that in comparison to existing state-of-the-art scheduling strategies, it reduces the average system latency up to 30%."
pub.1117920677,Task Scheduling for Streaming Applications in a Cloud-Edge System,"With the increasing popularity of ubiquitous smart devices, more and more IoT (Internet of Things) data processing applications are deployed. Due to the inherent defects of traditional data transmission networks and the low latency requirement of applications, effective use of bandwidth computing resources to support the efficient deployment of applications has become a very important issue. In this paper, we focus on how to deploy multi-source streaming data processing applications in a cloud-edge collaborative computing network and pay attention to make the overall application data processing delay lower. We abstract the application into a form of streaming data processing, formalize it as a Stream Processing Task Scheduling Problem. We present an efficient algorithm to solve the above problem. Simulation experiments show that our approach can significantly reduce the end-to-end latency of applications compared to commonly used greedy algorithms."
pub.1115049134,Reliable stream data processing for elastic distributed stream processing systems,"Distributed stream processing system (DSPS) has proven to be an effective way to process and analyze large-scale data streams in real-time fashions. The reliability problem of DSPS is becoming a popular topic in recent years. Novel elastic DSPSs provide the ability to seamlessly adapt to stream workload changes, which introduce new reliability challenges: (1) operators can be scaled up and down at runtime, requiring fault tolerant methods to maintain data backup consistency under the runtime dynamics. (2) Rollback recovery to the last checkpoint may undo recent auto-scaling adjustments, which will introduce high cost and unacceptable impact to the system. In this paper, we put forward a novel fault-tolerant mechanism to deal with these issues. In particular, we propose a self-adaptive backup unit, elastic data slice (EDS), that can partition and merge data backups according to operator auto-scaling at runtime. The consistency of recovery is guaranteed by new upstream backup protocols, which restart the system from the status after auto-scaling instead of last checkpoint and avoid high recovery latency. Based on them, we implement a prototype system named SPATE. Evaluations on SPATE show that our mechanism supports auto-scaling changes with similar overhead compared to existing approaches, while achieving low recovery latency despite auto-scaling."
pub.1145163878,Resource scheduling and provisioning for processing of dynamic stream workflows under latency constraints,"There has been an increasing demand for large-scale processing of stream workflows. Many streaming data sources, such as video and audio, differ in terms of the type and density of information they contain over time, which require dynamic workflow structures where paths change in response to data variations. Additionally, information is often only inferred from examining discrete chunks and hence, cannot be arbitrarily parallelised like other streaming data sources. Stream processes using these data sources often operate as dynamic workflows, where different paths are selected based on prior processing information, relating to differences in the nature of the data itself. To better enable low latency analysis in such scenarios, this work investigates strategies to schedule and provision streaming dynamic workflow scenarios of various complexities and degrees of unpredictability. It is found that, as scenarios become increasingly uncertain and unpredictable, it is better to use simpler strategies which do not make assumptions about workflow structures or completion times."
pub.1153841522,ShadowSync,"Mission-critical, real-time, continuous stream processing applications that interact with the real world have stringent latency requirements. For example, e-commerce websites like Amazon improve their marketing strategy by performing real-time advertising based on customers' behavior, and latency long tail can cause significant revenue loss. Recent work [39] showed a positive correlation between latency long tail and variance in the execution time of synchronous invocation chains (critical paths) in microservices benchmarks. This paper shows that asynchronous, very short but intense resource demands (called millibottlenecks) outside of critical paths can also cause significant latency long tail. Using a traffic analysis stream processing application benchmark, we evaluated the impact of asynchronous workload bursts generated by a multi-layer data structure called LSM-tree (log-structured merge-tree) for continuous checkpointing. Outside of the critical path, LSM-tree relies on maintenance operations (e.g., flushing/compaction during a checkpoint) to reorganize LSM-tree in memory and on disk to keep data access latency short. Although asynchronous, such recurrent maintenance operations can cause frequent millibottlenecks, particularly when they overlap, a problem we call ShadowSync. For scheduling and statistical reasons, significant latency long tail can arise from ShadowSync caused by asynchronous recurrent operations. Our experimental results show that with typical settings of benchmark components such as RocksDB, ShadowSync can prolong request message latency by up to 2 seconds. We show effective mitigation methods can alleviate both scheduled and statistical ShadowSync reducing the latency long tail to less than 20% of the original at the 99.9th percentile."
pub.1095302715,Zero-Latency Data Warehousing (ZLDWH): the State-of-the-art and experimental implementation approaches,"Increased data volumes and accelerating update speeds are fundamentally changing the role of the data warehouse in modern businesses. More data, coming in faster, and requiring immediate conversion into decisions means that organizations are confronting the need for a Zero-Latency Data Warehousing. The Zero-Latency Data Warehouse (ZLDWH) is an extended stage in the data warehouse development, which will enable a complete business intelligence process to observe, understand, predict, react to, reorganize, monitor, automate and control feedback loops in the minimal latency. This paper reviews the start-of-the-art of the Zero-Latency Data Warehouse and investigates two experimental approaches towards its implementations. (1) The Grid-based Zero-Latency Data Stream Warehouse (GZLDSWH) with the aim to tackle the resource limitation issues in data stream processing without using approximation approaches. (2) Sense & Response Service Architecture (SARESA) towards a complete Business Intelligence process to sense, interpret, predict, automate and respond to business processes."
pub.1152678356,Runtime reconfiguration of data services for dealing with out-of-range stream fluctuation in cloud-edge environments,"The integration of cloud and IoT edge devices is of significance in reducing the latency of IoT stream data processing by moving services closer to the edge-end. In this connection, a key issue is to determine when and where services should be deployed. Common service deployment strategies used to be static based on the rules defined at the design time. However, dynamically changing IoT environments bring about unexpected situations such as out-of-range stream fluctuation, where the static service deployment solutions are not efficient. In this paper, we propose a dynamic service deployment mechanism based on the prediction of upcoming stream data. To effectively predict upcoming workloads, we combine the online machine learning methods with an online optimization algorithm for service deployment. A simulation-based evaluation demonstrates that, compared with those state-of-the art approaches, the approach proposed in this paper has a lower latency of stream processing."
pub.1003412594,Latency-aware elastic scaling for distributed data stream processing systems,"Elastic scaling allows a data stream processing system to react to a dynamically changing query or event workload by automatically scaling in or out. Thereby, both unpredictable load peaks as well as underload situations can be handled. However, each scaling decision comes with a latency penalty due to the required operator movements. Therefore, in practice an elastic system might be able to improve the system utilization, however it is not able to provide latency guarantees defined by a service level agreement (SLA). In this paper we introduce an elastic scaling system, which optimizes the utilization under certain latency constraints defined by a SLA. Specifically, we present a model, which estimates the latency spike created by a set of operator movements. We use this model to built a latency-aware elastic operator placement algorithm, which minimizes the number of latency violations. We show that our solution is able to reduce the 90th percentile of the end to end latency by up to 30% and reduce the number of latency violations by 50%. The achieved system utilization for our approach is comparable to a scaling strategy, which does not use latency as optimization target."
pub.1093198872,Evolutionary Algorithms that use Runtime Migration of Detector Processes to Reduce Latency in Event-Based Systems,"Event-based systems (EBS) are widely used to efficiently process massively parallel data streams. In distributed event processing the allocation of event detectors to machines is crucial for both the latency and efficiency, and a naive allocation may even cause a system failure. But since data streams, network traffic, and event loads cannot be predicted sufficiently well the optimal detector allocation cannot be found a-priori and must instead be determined at runtime. This paper describes how evolutionary algorithms (EA) can be used to minimize both network and processing latency by means of runtime migration of event detectors. The paper qualitatively evaluates the algorithms on synthetical data streams in a distributed event-based system. We show that some EAs work efficiently even with large numbers of event detectors and machines and that a hybrid of Cuckoo Search and Particle Swarm Optimization outperforms others."
pub.1153650006,An adaptive non-migrating load-balanced distributed stream window join system,"Stream processing systems are widely used to process large amounts of data generated by applications in real time due to their advantages in latency and throughput. In most streaming applications, the system requires a comprehensive analysis of data from multiple data sources, so stream joins are the basis of stream processing systems. Similar to other big data problems, stream joins suffer from load imbalance, where a few nodes responsible for handling most of the load can become bottlenecks, thereby increasing latency and reducing throughput. Therefore, how to obtain a good load-balancing effect with low overhead is a critical issue in designing stream join systems. To solve this problem, we propose an adaptive non-migrating load-balancing method, which is mainly oriented to the stream window join problem. Considering that the completeness of the stream join results during the splitting of state to multiple downstream instances can be guaranteed by replicating the input tuples into multiple replicas and sending them to those downstream instances, our method can control the replication and forwarding of input tuples by setting up routing tables, and then when the system becomes unbalanced, our method can change the load distribution of the system by directly changing the partitioning of the tuples arriving later instead of state migration, and thus achieving load balancing with very low overhead. Based on our method, we develop a distributed stream window join system, NM-Join, which is built on Flink. We theoretically analyze the completeness and effectiveness of our method and provide extensive experimental evaluations of NM-Join in terms of load-balancing effect, latency, and throughput. Experimental results show that our method is able to perform load balancing with very low additional overhead, and thus outperforms existing load-balancing methods in terms of latency and throughput."
pub.1144339454,A Machine Learning-Based Elastic Strategy for Operator Parallelism in a Big Data Stream Computing System,"Elastic scaling in/out of operator parallelism degree is needed for processing real time dynamic data streams under low latency and high stability requirements. Usually the operator parallelism degree is set when a streaming application is submitted to a stream computing system and kept intact during runtime. This may substantially affect the performance of the system due to the fluctuation of input streams and availability of system resources. To address the problems brought by the static parallelism setting, we propose and implement a machine learning based elastic strategy for operator parallelism (named Me-Stream) in big data stream computing systems. The architecture of Me-Stream and its key models are introduced, including parallel bottleneck identification, parameter plan generation, parameter migration and conversion, and instances scheduling. Metrics of execution latency and process latency of the proposed scheduling strategy are evaluated on the widely used big data stream computing system Apache Storm. The experimental results demonstrate the efficiency and effectiveness of the proposed strategy."
pub.1003923740,Adaptive input admission and management for parallel stream processing,"In this paper, we propose a framework for adaptive admission control and management of a large number of dynamic input streams in parallel stream processing engines. The framework takes as input any available information about input stream behaviors and the requirements of the query processing layer, and adaptively decides how to adjust the entry points of streams to the system. As the optimization decisions propagate early from input management layer to the query processing layer, the size of the cluster is minimized, the load balance is maintained, and latency bounds of queries are met in a more effective and timely manner. Declarative integration of external meta-data about data sources makes the system more robust and resource-efficient. Additionally, exploiting knowledge about queries moves data partitioning to the input management layer, where better load balance for query processing can be achieved. We implemented these techniques as a part of the Borealis stream processing system and conducted experiments showing the performance benefits of our framework."
pub.1174024365,"NebulaStream - Data Stream Processing in Massively Distributed, Heterogeneous, Volatile Environments","Modern data-driven applications arising in such domains as smart manufacturing, healthcare, and the Internet of Things, pose new challenges to data processing systems. Traditional stream processing systems, such as Flink, Spark, and Kafka Streams are ill-suited to cope with the massive scale of distribution, the heterogeneous computing landscape, and requirements, such as timely processing and actuation. Classical approaches like managed runtimes, interpretation-based query processing, and the optimization of single queries that neglect interactions, greatly limit throughput, latency, energy-efficiency, and the general usability of these systems for emerging applications involving distributed data processing at scale in a sensor-edge-cloud-environment. To overcome these limitations, we are researching and building NebulaStream, a novel data stream processing system for massively distributed, heterogeneous environments. NebulaStream supports (potentially resource-constrained) heterogeneous devices, a hierarchical topology (with the distribution of computation and data flow in a cloud-edge-continuum), and the sharing of computations and data across multiple concurrent queries."
pub.1105914649,Mobile Resource Aware Scheduling for Mobile Edge Environment,"In stream processing applications, a data stream is a continuous stream of data items that are generated from multiple sources distributed at various geographic locations. A common method of streaming processing is to transfer raw data streams to a data center for unified processing. However, the method does not scale well when a huge amount of data for stream processing is generated at the edge of the Internet, with the development of smartphones, Internet of things, 5G and other technologies in recent years. For stream processing applications, processing data at the edge can significantly reduce the response latency of the applications. However, the mobility of edge nodes in a mobile edge environment poses a significant challenge to scheduling stream processing tasks efficiently to achieve high system throughputs. In this paper, we introduce a scheduling algorithm, referred to as Mobile Resource Aware (MRA) stream processing scheduling, for mobile edge environment. Compared with other existing scheduling algorithms, our MRA algorithm can optimally schedule resources for stream processing tasks through adapting to the mobile edge environment with limited node resources. We implement MRA scheduling algorithm in Storm through a custom scheduler and we evaluate the performance of MRA in an emulation mobile edge environment. Our experimental results have demonstrated that our MRA algorithm can achieve significantly higher system performance than the other two existing scheduling algorithms."
pub.1135022450,A Traffic and Resource Aware Online Storm Scheduler,"Streaming applications have become widespread with the advent of big data and IoT. They are latency-sensitive applications that aim to process vast amounts of data in near real time. They are usually modelled as directed graphs and their deployment and orchestration in a cluster of nodes are managed by distributed stream processing systems. These systems are responsible for placing the graph components within the cluster nodes, which determines the application’s communication overhead and ultimately affects performance metrics such as latency and throughput. This work presents an adaptive, heuristic-based, scheduling algorithm for distributed stream processing systems that aims to minimize the latency and maximize the throughput of streaming applications deployed in heterogeneous clusters, while considering the resource constraints of the available nodes. The proposed approach uses a graph partitioning algorithm and real time traffic data monitored from a deployed application to derive a near-optimal operator placement plan that minimizes inter-node communication and balances the overall communication load distribution. We evaluated our approach using three micro-benchmark and two practical applications, and the results demonstrate that our scheduler outperforms the default scheduler of a popular stream processing system and a state-of-the-art algorithm, improving throughput by up to 106% and reducing complete latency by up to 58% for most applications."
pub.1038157497,About saccade generation in reading,"In their model, Findlay & Walker propose that where and when the eyes move is determined by two relatively independent processing streams. Whereas both saccade direction and amplitude result from a low-level visual analysis of the peripheral visual stimulation, saccade latency results mainly from higher-level processes related to processing of the central information. In the present commentary, reading eye movement data are put forward as evidence against a strict autonomy of “Where” and “When” processing streams. First, saccade direction and amplitude might be modified by high-level processes related to word identification. Second, the direction of a saccade directly affects its latency."
pub.1144687244,Multimodal Event Processing: A Neural-Symbolic Paradigm for the Internet of Multimedia Things,"With the Internet of Multimedia Things (IoMT) becoming a reality, new approaches are needed to process real-time multimodal event streams. Existing approaches to event processing have limited consideration for the challenges of multimodal events, including the need for complex content extraction, and increased computational and memory costs. This article explores event processing as a basis for processing real-time IoMT data. This article introduces the multimodal event processing (MEP) paradigm, which provides a formal basis for native approaches to neural multimodal content analysis (i.e., computer vision, linguistics, and audio) with symbolic event processing rules to support real-time queries over multimodal data streams using the multimodal event processing language to express single, primitive multimodal, and complex multimodal event patterns. The content of multimodal streams is represented using multimodal event knowledge graphs to capture the semantic, spatial, and temporal content of the multimodal streams. The approach is implemented and evaluated within a MEP engine using single and multimodal queries achieving near real-time performance with a throughput of ~30 frames processed per second (fps) and subsecond latency of 0.075–0.30 s for video streams of 30 fps input rate. Support for high input stream rates (45 fps) is achieved through content-aware load-shedding techniques with a ~127X latency improvement resulting in only a minor decrease in accuracy."
pub.1111650415,Scheduling Stream Processing Tasks on Geo-Distributed Heterogeneous Resources,"Low-latency processing of data streams from distributed sensors is becoming increasingly important for a growing number of IoT applications. In these environments sensor data collected at the edge of the network is typically transmitted in a number of hops: from devices to intermediate resources to clusters of cloud resources. Scheduling processing tasks of dataflow jobs on all the resources of these environments can significantly reduce application latencies and network congestion. However, for this schedulers need to take the heterogeneity of processing resources and network topologies into account. This paper examines multiple methods for scheduling distributed dataflow tasks on geo-distributed, heterogeneous resources. For this, we developed an optimization function that incorporates the latencies, bandwidths, and computational resources of heterogeneous topologies. We evaluated the different placement methods in a virtual geo-distributed and heterogeneous environment with an IoT application. Our results show that metaheuristic methods that take service quality metrics into account can find significantly better placements than methods that only take topologies into account, with latencies reduced by almost 50%."
pub.1021351624,Data stream processing via code annotations,"Time-to-solution is an important metric when parallelizing existing code. The REPARA approach provides a systematic way to instantiate stream and data parallel patterns by annotating the sequential source code with C$${\mathtt {C}}$$++11$${\mathtt {11}}$$ attributes. Annotations are automatically transformed in a target parallel code that uses existing libraries for parallel programming (e.g., FastFlow). In this paper, we apply this approach for the parallelization of a data stream processing application. The description shows the effectiveness of the approach in easily and quickly prototyping several parallel variants of the sequential code by obtaining good overall performance in terms of both throughput and latency."
pub.1181148504,Intelligent Data Source Emission Rate Control for Optimising the Performance of Streaming Applications,"Streaming applications are expected to process an ever-increasing amount of data with high throughput and stringent latency requirements. Flooding these applications with incoming data may overload the stream processing engine, leading to a system with unstable queues and infinitely growing latencies. Existing stream processing systems are equipped to deal with such overload scenarios reactively, either through back pressure or load shedding mechanisms. These mechanisms, however, have considerable drawbacks as they consume additional system resources, incur in non-negligible performance overheads, and may compromise the quality of application-level results. To address this gap, we propose a strategy based on reinforcement learning to throttle the input rate of data sources in streaming applications. The proposed strategy mitigates overload scenarios by addressing the source of the problem, thus allowing resources to be better utilized by application and system components and mitigating the performance overhead of system-level reactive mechanisms. Through our experiments with two different applications, we demonstrate that our proposed approach reduces end-to-end latencies by up to 82% and increases throughput by up to 10% compared to back pressure mechanisms implemented in state-of-the-art stream processing engines."
pub.1106346130,Towards Low-Latency Batched Stream Processing by Pre-Scheduling,"Many stream processing frameworks have been developed to meet the requirements of real-time processing. Among them, batched stream processing frameworks are widely advocated with the consideration of their fault-tolerance, high throughput and unified runtime with batch processing. In batched stream processing frameworks, straggler, happened due to the uneven task execution time, has been regarded as a major hurdle of latency-sensitive applications. Existing straggler mitigation techniques, operating in either reactive or proactive manner, are all post-scheduling methods, and therefore inevitably result in high resource overhead or long job completion time. We notice that batched stream processing jobs are usually recurring with predictable characteristics. By exploring such a heuristic, we present a pre-scheduling straggler mitigation framework called Lever. Lever first identifies potential stragglers and evaluates nodes capacity by analyzing execution information of historical jobs. Then, Lever carefully pre-schedules job input data to each node before task scheduling so as to mitigate potential stragglers. We implement Lever and contribute it as an extension of Apache Spark Streaming. Our experimental results show that Lever can reduce job completion time by 30.72 to 42.19 percent over Spark Streaming, a widely adopted batched stream processing system and outperforms traditional techniques significantly."
pub.1017386516,Scheduling Strategies for Processing Continuous Queries over Streams,"Stream data processing poses many challenges. Two important characteristics of stream data processing – bursty arrival rates and the need for near real-time performance requirement – challenge the allocation of limited resources in the system. Several scheduling algorithms (e.g., Chain strategy) have been proposed for minimizing the maximal memory requirements in the literature. In this paper, we propose novel scheduling strategies to minimize tuple latency as well as total memory requirement. We first introduce a path capacity strategy (PCS) with the goal of minimizing tuple latency. We then compare the PCS and the Chain strategy to identify their limitations and propose additional scheduling strategies that improve upon them. Specifically, we introduce a segment strategy (SS) with the goal of minimizing the memory requirement, and its simplified version. In addition, we introduce a hybrid strategy, termed the threshold strategy (TS), to addresses the combined optimization of both tuple latency and memory requirement. Finally, we present the results of a wide range of experiments conducted to evaluate the efficiency and the effectiveness of the proposed scheduling strategies."
pub.1129219843,◾ Key Technologies for Big Data Stream Computing,"ABSTRACT As a new trend for data-intensive computing, real-time stream computing is gaining signicant attention in the Big Data era. In theory, stream computing is an eective way to support Big Data by providing extremely low-latency processing tools and massively parallel processing architectures in real-time data analysis. However, in most existing stream computing environments, how to eciently deal with Big Data stream computing and how to build ecient Big Data stream computing systems are posing great challenges to Big Data computing research. First, the data stream graphs and the system architecture for Big Data stream computing, and some related key technologies, such as system structure, data transmission, application interfaces, and high availability, are systemically researched. en, we give a classication of the latest research and depict the development status of some popular Big Data stream computing systems, including Twitter Storm, Yahoo! S4, Microso TimeStream, and Microso Naiad. Finally, the potential challenges and future directions of Big Data stream computing are discussed."
pub.1159999610,A Comprehensive Performance Analysis of Stream Processing with Kafka in Cloud Native Deployments for IoT Use-cases,"The constant growth of the number of Internet of Things devices drives a huge increase in data that needs to be analyzed, at times in real time. Multiple platforms are available for delivering such data to analytics engines that can perform various operations on the data with low processing latency. These platforms can find their home in cloud native environments where high availability and scaling to the actual workload can be easily achieved. While the deployment environment is elastic, clusters still need to be adequately dimensioned to accommodate the components of the platforms even under high load. In this paper, we provide an analysis in this regard: we discuss key performance indicators of the popular Kafka message bus and the related Kafka Streams processing engine. Namely, we analyze latency, throughput, CPU and memory resource footprint aspects of these services under varying load and processing tasks that appear in Internet of Things applications. We find subsecond processing latency and linear but heavily task-dependent scaling behavior in the other performance indicators’ case."
pub.1061523330,"ScaleJoin: A Deterministic, Disjoint-Parallel and Skew-Resilient Stream Join","The inherently large and varying volumes of information generated in large scale systems demand near real-time processing of data streams. In this context, data streaming is imperative for data-intensive processing infrastructures. Stream joins, the streaming counterpart of database joins, compare tuples coming from different streams and constitute one of the most important and expensive data streaming operators. Algorithmic implementations of stream joins have to be capable of efficiently processing bursty and rate-varying data streams in a deterministic and skew-resilient fashion. To leverage the design of modern multicore architectures, scalability and parallelism need to be addressed also in the algorithmic design. In this paper we present ScaleJoin, an algorithmic construction for deterministic and parallel stream joins that guarantees all the above properties, thus filling in a gap in the existing state-of-the-art. Key to the novelty of ScaleJoin is the ScaleGate data structure and its lock-free implementation. ScaleGate facilitates concurrent data exchange and balances independent actions among processing threads; enabling fine-grain parallelism and deterministic processing. It allows ScaleJoin to run on an arbitrary number of processing threads, evenly sharing the overall comparisons run in parallel and achieving disjoint and skew-resilient high processing throughput and low processing latency."
pub.1149415653,Dynamic Data Partitioning Strategy Based on Heterogeneous Flink Cluster,"As a popular big data stream processing engine, Apache Flink has obvious advantages in stream data processing. The data partitioning strategy of Flink stream processing jobs ignores metrics such as heterogeneous cluster, real-time computational resources of nodes and communication delays between nodes, which can easily cause load imbalance problems. In this paper, we propose a dynamic data partitioning strategy for stream processing jobs based on heterogeneous Flink cluster for adjusting the data load among nodes. The strategy dynamically adjusts the ratio of data processed by each node according to the real-time resource information, communication delay and hardware information of each node, so that the data load among nodes is at a relatively balanced level after multiple rounds of feedback adjustment. The experimental results show that the strategy can effectively improve the throughput and reduce latency of the job"
pub.1172276541,Low-Latency Adaptive Distributed Stream Join System Based on a Flexible Join Model,"Stream join is a fundamental operation in stream processing and has attracted extensive research due to its large resource consumption and serious impact on system performance. As the theoretical basis of stream join systems, the stream join model greatly affects system performance. State-of-the-art stream join models either consume too much computing resources or too much storage resources, thus resulting in lower throughput or higher latency. In this paper, we propose a new stream join model for processing arbitrary join predicates, called CoModel, which offers a flexible trade-off between memory and computing resource consumption. More importantly, CoModel can achieve the minimum sum of the number of store operations and join operations among all existing join models, and thus can achieve the lowest latency and highest throughput when the overheads associated with the local stream join for each input tuple are approximately constant. We give a trade-off strategy for CoModel and theoretically prove its performance advantages based on queuing theory. Furthermore, we design and implement an adaptive distributed stream join system, CoStream, based on CoModel. CoStream can adaptively adjust its structure according to resource constraints and statistics of input data. We conduct extensive experiments for CoStream to evaluate its performance and adaptivity, and the results show that CoStream has the lowest latency and highest throughput in various scenarios."
pub.1172604491,Improving data latency in ETL with filtering algorithms for stream processing an experimental setup,"The Extract, Transform and Load (ETL) system is a standard approach of managing and sustaining movement and transaction of data assets. The exponential growth of data necessitates the development of better approaches and ways of surpassing the traditional ETL limits and effectively manage these big data in near real-time, in terms of availability, speed of delivery or latency and scalability in processing, thus enhance the functionality and increase data value. The purpose of this research is to identify approaches and series of combination of implementing ETL for streaming and near real-time data with special focus on data latency. It is important to note that while the approaches might improve latency, it is also possible that other aspects such as availability and scalability could be compromised, as well as other limitations. The research then presents an experimental setup to simulate a near real-time scenario for landslide mapping using Lidar data, introduces 3 sets of algorithms combo: (1) data model transformation, (2) alert threshold (3) query filter techniques in stream processing with the aim of achieving low latency. Furthermore, the research expects to contribute some modified filtering techniques for accelerating queries for prioritized datasets and reduce overall data filtering time. The experiment will be performed on a Hadoop platform."
pub.1047316794,Feature‐based high‐availability mechanism for quantile tasks in real‐time data stream processing,"SUMMARY Under distributed Cloud environment, the real‐time and continuous data stream makes the availability during processing essential but expensive. For aggregation tasks of data stream processing systems, traditional replica‐based high‐availability mechanisms require large overheads at run‐time and long recovery latency at fail‐time, because of specific nature of aggregations. In this paper, we focus on the typical quantile tasks and propose a feature‐based high‐availability mechanism to reduce related overhead and the latency. With the help of monitor module, quantile feature is maintained incrementally through histogram synopsis over time‐based sliding window, and the failed quantile tasks can be recovered precisely with high probability in an efficient way. The effectiveness has been analyzed theoretically, and meanwhile, the acceptable tradeoff between overheads and performance has been demonstrated by comprehensive experiments on both synthetic and real data. Copyright © 2013 John Wiley & Sons, Ltd."
pub.1107190924,High-Performance Stateful Stream Processing on Solid-State Drives,"Stream processing has been widely used in big data analytics because it provides real-time information on continuously incoming data streams with low latency. As the volume of data increases and the processing logic becomes more complicated, the size of internal states in stream processing applications also increases. To deal with large states efficiently, modern stream processing systems support storing internal states on solid state drives (SSDs) by utilizing persistent key-value (KV) stores optimized for SSDs. For example, Apache Flink and Apache Samza store internal states on RocksDB. However, delegating state management to persistent KV stores degrades the performance, because the KV stores cannot optimize their state management strategies according to stream query semantics as they are not aware of the query semantics. In this paper, we investigate the performance limitations of current state management approaches on SSDs and show that query-aware optimizations can significantly improve the performance of stateful query processing on SSDs. Based on our observation, we propose a new stream processing system design with static and runtime query-aware optimizations. We also discuss additional research directions on integrating emerging storage technologies with stateful stream processing."
pub.1106438427,High Performance Distributed In-Memory Architectures For Trade Surveillance System,"With rapid growth of world economy, the user activity in the capital markets is increased. This results into large number of transactional activities in trading systems. Hence, the trade surveillance system with low latency and high throughput is needed to monitor such a large amount of data in order to improve user experience by reducing discrepancies and frauds. In-memory technology reduces this latency by processing as well as caching data in main memory thereby removing the overhead of disk access. Currently, open-source frameworks such as Apache Ignite, Apache Flink and Kafka Streams provides in-memory streaming and caching functionalities along with scalability and fault-tolerant features. The paper talks about Trade Surveillance System (TSS), which includes Complex Event Processing (CEP). Here we discuss design, implementation and tuning of three different high-performance architectures for trade surveillance system using Ignite, Flink and Kafka Streams as in-memory streaming technologies. Paper also compares system throughput, support for fault tolerance and effect of caching on streaming throughput for all three architectures. Based on experiments, it is seen that Ignite outperforms Flink and Kafka Streams in CEP based streaming. Flink is more reliable considering fault-tolerance and event-time processing at streaming layer compared to Ignite. Though Kafka Streams also provides fault-tolerance and event-time processing out of the box, it shows high latency due to disk based processing."
pub.1174018069,Blazing through Hard Drive Telemetry Data Streams with Parallel Processing,"The 2024 DEBS Grand Challenge addresses the topic of hard drive failure predictive maintenance, through analysis of data streams that contain SMART readings, reported by drives located in different groups of storage servers. This paper details the technical implementation of a solution that focuses primarily on parallelizing the data stream processing to obtain vertical scalability. When processing two queries concerning the addressed topic, and setting a threshold of a maximum 16 ms latency for responding, our solution obtained a throughput of about 57% out of the maximum possible when no processing is made on the data stream. We also describe an initial work-in-progress implementation of a distributed extension that relies on Apache Kafka, meant to further scale the throughput of the parallel solution and to address possible failure conditions of retrieving the input stream."
pub.1095236198,Symbiosis: Sharing mobile resources for stream processing,"Nowadays, the overwhelming amount of data generated on Internet have boosted the development of new solutions to process data on time windows closer to real time. Such solutions are known as Distributed Stream Processing Engines (DSPEs). DSPEs were specially designed to process data streams over cluster infrastructure, however the great massification of mobile devices opens new opportunities to process data closer to the source in order to reduce latency and traffic over the network. In this paper we propose Symbiosis, an architecture oriented to process data streams over mobile clients such as tablets and smartphones. Symbiosis model aims to exploit mobile devices resources to pre-process data streams generated on neighbors clients. Implementing data processing on mobile nodes is challenging due to their mobility and the limited battery power. In order to cope with such a requirements, Symbiosis proposes a data processing method based on checkpoints which consider both mobility and available energy in the device."
pub.1002648241,Resolving longitudinal amplitude and phase information of two continuous data streams for high-speed and real-time processing,"Abstract. Although there is an increase of performance in DSPs, due to its nature of execution a DSP could not perform high-speed data processing on a continuous data stream. In this paper we discuss the hardware implementation of the amplitude and phase detector and the validation block on a FPGA. Contrary to the software implementation which can only process data stream as high as 1.5 MHz, the hardware approach is 225 times faster and introduces much less latency."
pub.1093919258,Adaptive Provisioning of Stream Processing Systems in the Cloud,"With the advent of data-intensive applications that generate large volumes of real-time data, distributed stream processing systems (DSPS) become increasingly important in domains such as social networking and web analytics. In prac-tice, DSPSs must handle highly variable workloads caused by unpredictable changes in stream rates. Cloud computing offers an elastic infrastructure that DSPSs can use to obtain resources on-demand, but an open problem is to decide on the correct resource allocation when deploying DSPSs in the cloud. This paper proposes an adaptive approach for provisioning virtual machines (VMs) for the use of a DSPS in the cloud. We initially perform a set of benchmarks across performance metrics such as network latency and jitter to explore the feasibility of cloud-based DSPS deployments. Based on these results, we propose an algorithm for VM provisioning for DSPSs that reacts to changes in the stream workload. Through a prototype implementation on Amazon EC2, we show that our approach can achieve low-latency stream processing when VMs are not overloaded, while adjusting resources dynamically with workload changes."
pub.1061403771,"AdaptWID: An Adaptive, Memory-Efficient Window Aggregation Implementation","Memory efficiency is important for processing high-volume data streams. Previous stream-aggregation methods can exhibit excessive memory overhead in the presence of skewed data distributions. Further, data skew is a common feature of massive data streams. The authors introduce the AdaptWID algorithm, which uses adaptive processing to cope with time-varying data skew. AdaptWID models the memory usage of alternative aggregation algorithms and selects between them at runtime on a group-by-group basis. The authors' experimental study using the NiagaraST stream system verifies that the adaptive algorithm improves memory usage while maintaining execution cost and latency comparable to existing implementations."
pub.1174846335,In-Network Management of Parallel Data Streams over Programmable Data Planes,"Current data centers host an ever increasing number of data analytics applications who are dealing with a growing number of data sources and a continuously increasing volume of data. Parallel stream processing is a powerful paradigm supporting the large-scale deployment of data-analytics applications. However, its performance is limited by its processing capacity of splitting the data streams into parallelizable sub-streams. The splitter that is traditionally executed on general-purpose computational resources can benefit from in-network computing nodes on the communication path. Programmable data planes and corresponding programming models, e.g., Programming Protocol-independent Packet Processors (P4), offer the flexibility of enabling distinct parallelization semantics that can be individually adapted to the dynamic workload. In this paper, we propose Stateful and Scalable Splitter Switch (S4), a network-centric approach leveraging P4 to support parallel stream processing. S4 supports up to 286k concurrent data streams, with a parallelism degree of up to ~ 500k operator instances and a latency overhead of only 2μ$s$."
pub.1175921927,D-Matrix: FPGA-Based Solutions for General Stream Processing in High-Energy Physics Experiments,"The data acquisition system plays an increasingly important role in high-energy physics experiments. The expansion of the experimental scale requires the data acquisition system to have enhanced online data processing capabilities. The D-Matrix system is designed as a general distributed stream processing platform, aiming to make more use of heterogeneous computing units, such as FPGA, for suitable online data processing to optimize both processing speed and latency. In the D-Matrix system, to facilitate the design of various data processing modules in high-energy physics experiments, we abstract the concepts of basic patterns and derived modules. Based on these concepts, the D-Matrix system is designed with a series of generic data stream processing modules to accomplish various complex data processing by cascading these generic modules. This article introduces the existing base patterns and derives stream processing modules in the D-Matrix system based on FPGA-based hardware and also describes the implementation of these modules in the CSR External-target Experiment (CEE)."
pub.1162763975,SASPAR: Shared Adaptive Stream Partitioning,"Data partitioning induces network transfers and dominates the cost of stream data analytics. Moreover, partitioning streaming data for multiple stream queries in the same cluster can easily saturate the network bandwidth and lead to high end-to-end latencies. The goal of this paper is to share the partition operation in streaming workloads and maximize the sharing opportunities for multiple stream queries. However, there are several challenges, such as minimizing data copy, optimizing the partitioning strategy for multiple queries, and minimizing latency. We propose SASPAR, Shared Adaptive Stream Partitioner, which is able to share data partitioning among multiple stream queries. Our contributions are threefold. First, we propose a new technique to optimize the partitioning strategy for multiple stream queries. Second, we present an adaptive query execution framework that performs optimizations at run-time, without stopping the query execution plan. Third, we utilize meta-heuristics and machine learning when solving the underlying optimization problem takes more time than expected. SASPAR is designed as a versatile layer to sit on top of a stream processing engine (SPE). We operate SASPAR on top of three state-of-the-art SPEs with hundreds of stream queries. Our experimental results show that SASPAR improves the performance (throughput and latency) of all underlying SPEs by up to 3x."
pub.1136340833,ESPBench: The Enterprise Stream Processing Benchmark,"Growing data volumes and velocities in fields such as Industry 4.0 or the
Internet of Things have led to the increased popularity of data stream
processing systems. Enterprises can leverage these developments by enriching
their core business data and analyses with up-to-date streaming data. Comparing
streaming architectures for these complex use cases is challenging, as existing
benchmarks do not cover them. ESPBench is a new enterprise stream processing
benchmark that fills this gap. We present its architecture, the benchmarking
process, and the query workload. We employ ESPBench on three state-of-the-art
stream processing systems, Apache Spark, Apache Flink, and Hazelcast Jet, using
provided query implementations developed with Apache Beam. Our results
highlight the need for the provided ESPBench toolkit that supports benchmark
execution, as it enables query result validation and objective latency
measures."
pub.1040757236,A model for continuous query latencies in data streams,"In this paper we propose a formal model for characterizing latencies affecting the computation of a continuous query either in a Data Stream Management System (DSMS) or in a Complex Event Processing (CEP) system. In the model, a query can be thought of as constructed out of basic Event Processing Units (EPUs) interconnected among themselves. EPUs are modeled considering just few parameters, used to define the EPU processing logic. In order to model the continuous query we use an acyclic directed (data-flow) graph whose nodes are the EPUs and edges represent the flow of information (events) processed by the EPUs themselves. The outcome of this model is to associate with each dataflow graph a set of latency metrics, namely reactivity, activity, and output latencies, and a complexity measure - that we call data-flow graph complexity - representing the input dimension required to produce an output event. The proposed model can be used to compare and contrast different data-flow graphs in order to assess their latency metrics. This is a crucial step in selecting one of such graphs that meets at best the latency requirements imposed by the programmer before its actual submission to a DSMS or to a CEP system. Furthermore, the model can be considered an effective mean through which formally comparing dataflow graphs and predicting their behavior before an actual experimental validation phase."
pub.1100416622,Dynamic Algorithm Selection for the Logic of Tasks in IoT Stream Processing Systems,"Various Internet of Things (IoT) and Industry 4.0 use cases, such as city-wide monitoring or machine control, require low-latency distributed processing of continuous data streams. This fact has boosted research on making Stream Processing Frameworks (SPFs) IoT-ready, meaning that their cloud and IoT service management mechanisms (e.g., task placement, load balancing, algorithm selection) need to consider new requirements, e.g., ultra low latency due to physical interactions. The algorithm selection problem refers to selecting dynamically which internal logic a deployed streaming task should use in case of various alternatives, but it is not sufficiently supported in current SPFs. To the best of our knowledge, this work is the first to add this capability to SPFs. Our solution is based on i) architectural extensions of typical SPF middleware, ii) a new schema for characterizing algorithmic performance in the targeted context, and iii) a streaming-specific optimization problem formulation. We implemented our solution as an extension to Apache Storm and demonstrate how it can reduce stream processing latency by up to a factor of 2.9 in the tested scenarios."
pub.1004062772,Big Data Analytics Platforms for Real-Time Applications in IoT,"Big data platforms have predominantly focused on the volume aspects of large-scale data management. The growing pervasiveness of Internet of Things (IoT) applications, along with their associated ability to collect data from physical and virtual sensors continuously, highlights the importance of managing the velocity dimension of big data too. In this chapter, we motivate the analytics requirements of IoT applications using several practical use cases, characterize the trade-offs between processing latency and data volume capacity of contemporary big data platforms, and discuss the critical role that Distributed Stream Processing and Complex Event Processing systems play in addressing the analytics needs of IoT applications."
pub.1160820042,Developing Scalable and Lightweight Data Stream Ingestion Framework for Stream Processing,"According to the development of technology, enormous amount of data are being generated as a continuous basis from Social media, IOT devices, and web etc. This lead to big data era. Many researchers are paying attention on massive amount of data stream processing coming with a rapid rate to gain valuable information in real-time or to make immediate decision. Data Ingestion Stage is an important part in data stream processing system. It is responsible for the data collection from different locations and then deliver this data for processing. The most important requirement of data ingestion is to provide low latency, high throughput, and scalability with many data producers and consumers. It can influence on entire stream processing performance. In big data stream computing, speed at which data being created and explosive growth of data lead to new challenges. One challenge is to accurately ingest different stream data into a processing platform or data storage platform. Current existing data stream ingestion systems use a combination of Apache NiFi and Kafka. Apache Nifi is used for collection and preprocessing of structured and unstructured data feeds. Kafka is used for message distribution. However, processor such as MergeRecord in Nifi can be memory, I/O CPU intensive. As a result, when processing massive data streams creation with high speed can lead to a lot of memory effort, input/output bottleneck or central processing unit (CPU) bottleneck. It leads to impact on the performance of stream processing layer and it is not appropriate for time sensitive applications. In this paper, we propose to use a combination of StreamSets Data Collector and Kafka to collect and transform from various sources of structured and unstructured feeds."
pub.1043179531,The Aurora and Borealis Stream Processing Engines,"Over the last several years, a great deal of progress has been made in the area of stream-processing engines (SPEs). Three basic tenets distinguish SPEs from current data processing engines. First, they must support primitives for streaming applications. Unlike Online Transaction Processing (OLTP), which processes messages in isolation, streaming applications entail time series operations on streams of messages. Second, streaming applications entail a real-time component. If one is content to see an answer later, then one can store incoming messages in a data warehouse and run a historical query on the warehouse to find information of interest. This tactic does not work if the answer must be constructed in real time. The need for real-time answers also dictates a fundamentally different storage architecture. DBMSs universally store and index data records before making them available for query activity. Such outbound processing, where data are stored before being processed, cannot deliver real-time latency, as required by SPEs. To meet more stringent latency requirements, SPEs must adopt an alternate model, which we refer to as “inbound processing”, where query processing is performed directly on incoming messages before (or instead of) storing them. Lastly, an SPE must have capabilities to gracefully deal with spikes in message load. Incoming traffic is usually bursty, and it is desirable to selectively degrade the performance of the applications running on an SPE. The Aurora stream-processing engine, motivated by these three tenets, is currently operational, has been used to build various application systems, and has been transferred to the commercial domain. Borealis is a distributed stream-processing system that inherits core stream-processing functionality from Aurora and enriches it with distribution functionality, in order to provide advanced capabilities that are commonly required by newly emerging stream-processing applications."
pub.1122132949,Lock-free Data Structures for Data Stream Processing,"Processing data in real-time instead of storing and reading from tables has led to a specialization of DBMS into the so-called data stream processing paradigm. While high throughput and low latency are key requirements to keep up with varying stream behavior and to allow fast reaction to incoming events, there are many possibilities how to achieve them. In combination with modern hardware, like server CPUs with tens of cores, the parallelization of stream queries for multithreading and vectorization is a common schema. High degrees of parallelism, however, need efficient synchronization mechanisms to allow good scaling with threads for shared memory access.In this work, we identify the most time-consuming operations for stream processing exemplarily for our own stream processing engine PipeFabric. In addition, we present different design principles of lock-free data structures which are suited to overcome those bottlenecks. We will finally demonstrate how lock-freedom greatly improves performance for join processing and tuple exchange between operators under different workloads. Nevertheless, the efficient usage of lock-free data structures comes with additional efforts and pitfalls, which we also discuss in this paper."
pub.1137038261,ESPBench: The Enterprise Stream Processing Benchmark,"Growing data volumes and velocities in fields such as Industry 4.0 or the Internet of Things have led to the increased popularity of data stream processing systems. Enterprises can leverage these developments by enriching their core business data and analyses with up-to-date streaming data. Comparing streaming architectures for these complex use cases is challenging, as existing benchmarks do not cover them. ESPBench is a new enterprise stream processing benchmark that fills this gap. We present its architecture, the benchmarking process, and the query workload. We employ ESPBench on three state-of-the-art stream processing systems, Apache Spark, Apache Flink, and Hazelcast Jet, using provided query implementations developed with Apache Beam. Our results highlight the need for the provided ESPBench toolkit that supports benchmark execution, as it enables query result validation and objective latency measures."
pub.1094345190,Latency aware decoder for high-order modulations MIMO transmissions with parallel processing architectures,"High order modulations with multi-stream transmissions have been proposed recently as a promising feature to increase the available data rates in the upcoming releases of LTE systems. Increasing the modulation order requires additional processing capabilities in both infrastructure nodes and mobile terminals. However, most of decoding algorithms are based either on turbo processing strategies or successive interference cancellation schemes. For mobile terminals, such strategies introduce significant decoding latency and are not compatible with parallel computing architectures that are implemented in modern devices. This paper proposes a tunable complexity scheme for multi-stream transmissions with high modulation orders (64QAM and higher). The scheme is based on lattice reduction followed by a list based maximum likelihood detection. The proposed strategy allows the same performance as successive interference cancellation schemes and offers a better decoding latency with parallel processing architectures."
pub.1042213700,Integrating fault-tolerance and elasticity in a distributed data stream processing system,"Recently there has been an increasing interest in building distributed platforms for processing of fast data streams. In this demonstration, we highlight the need for elasticity in distributed data stream processing systems and present Enorm, a data stream processing platform with focus on elasticity, i.e. the ability to dynamically scale resource usage according to the runtime workload fluctuations. In order to achieve dynamic scaling with minimal overhead and latency, we use an integrated approach for both fault-tolerance and elasticity. The idea is that both fault-tolerance and elasticity essentially require replicating or migrating computation states among different nodes. Integrating and sharing the state management operations between the two modules can not only provide abundant opportunities to reduce the system's runtime overhead but also simplify the system's architecture."
pub.1061541784,Extending MapReduce across Clouds with BStream,"Today, batch processing frameworks like Hadoop MapReduce are difficult to scale to multiple clouds due to latencies involved in inter-cloud data transfer and synchronization overheads during shuffle-phase. This inhibits the MapReduce framework from guaranteeing performance at variable load surges without over-provisioning in the internal cloud (IC). We propose BStream, a cloud bursting framework for MapReduce that couples stream-processing in the external cloud (EC) with Hadoop in the internal cloud (IC). Stream processing in EC enables pipelined uploading, processing and downloading of data to minimize network latencies. We use this framework to meet job deadlines. BStream uses an analytical model to minimize the usage of EC. We propose different checkpointing strategies that overlap output transfer with input transfer/processing and simultaneously reduce the computation involved in merging the results from EC and IC. Checkpointing further reduces job completion time. We experimentally compare BStream with other related works and illustrate performance benefits due to stream processing and checkpointing strategies in EC. Lastly, we characterize the operational regime of BStream."
pub.1085457810,"A Compressed, Inference-Enabled Encoding Scheme for RDF Stream Processing","The number of sensors producing data streams at a high velocity keeps increasing. This paper describes an attempt to design an inference-enabled, distributed, fault-tolerant framework targeting RDF streams in the context of an industrial project. Our solution gives a special attention to the latency issue, an important feature in the context of providing reasoning services. Low latency is attained by compressing the scheme and data of processed streams with a dedicated semantic-aware encoding solution. After providing an overview of our architecture, we detail our encoding approach which supports a trade-off between two common inference methods, i.e., materialization and query reformulation. The analysis of results of our prototype emphasize the relevance of our design choices."
pub.1094615559,GDSW: A General Framework for Distributed Sliding Window Over Data Streams,"The big data era is characterized by the emergence of live data with high volume and fast arrival rate, it poses a new challenge to stream processing applications: how to process the unbounded live data in real time with high throughput. The sliding window technique is widely used to handle the unbounded live data by storing the most recent history of streams. However, existing centralized solutions cannot satisfy the requirements for high processing capacity and low latency due to the single-node bottleneck. Moreover, existing studies on distributed windows primarily focus on specific operators, while a general framework for processing various window-based operators is wanted. In this paper, we firstly classify the window-based operators to two categories: data-independent operators and data-dependent operators. Then, we propose GDSW, a general framework for distributed count-based sliding window, which can handle both of data-independent and data-dependent operators. Besides, in order to balance system load, we further propose a dynamic load balance algorithm called DAD based on buffer usage. Our framework is implemented on Apache Storm 0.10.0. Extensive evaluation shows that GDSW can achieve sub-second latency, and 10X improvement in throughput compared with centralized processing, when processing rapid data rate or big size window."
pub.1094354845,Bleach: A Distributed Stream Data Cleaning System,"Existing scalable data cleaning approaches have focused on batch data cleaning. However, batch data cleaning is not suitable for streaming big data systems, in which dynamic data is generated continuously. Despite the increasing popularity of stream-processing systems, few stream data cleaning techniques have been proposed so far. In this paper, we bridge this gap by addressing the problem of rule-based stream data cleaning, which sets stringent requirements on latency, rule dynamics and ability to cope with the continuous nature of data streams. We design a system, called Bleach, which achieves real-time violation detection and data repair on a dirty data stream. Bleach relies on efficient, compact and distributed data structures to maintain the necessary state to repair data. Additionally, it supports rule dynamics and uses a “cumulative” sliding window operation to improve cleaning accuracy. We evaluate a prototype of Bleach using both synthetic and real data streams and experimentally validate its high throughput, low latency and high cleaning accuracy, which are preserved even with rule dynamics."
pub.1037041521,Concurrent data structures for efficient streaming aggregation,"We briefly describe our study on the problem of streaming multiway aggregation, where large data volumes are received from multiple input streams. Multiway aggregation is a fundamental computational component in data stream management systems, requiring low-latency and high throughput solutions.We focus on the problem of designing concurrent data structures enabling for low-latency and high-throughput multiway aggregation; an issue that has been overlooked in the literature. We propose two new concurrent data structures and their lock-free linearizable implementations, supporting both order-sensitive and order-insensitive aggregate functions.Results from an extensive evaluation show significant improvement in the aggregation performance,in terms of both processing throughput and latency over the commonly-used techniques based on queues."
pub.1151585256,Applied Research on Construction of Real-Time Data Platform,"In order to construct a real-time computing platform for processing stream data and batch data simultaneously; Using experimental verification and comprehensive analysis methods; Experiments were conducted to compare Flink with spark structured streaming, deploy it on Yan, combine Flink with spring, and Flink asynchronously does not support the use of keyedstate and Flink CEP; Get the result that Flink can organically integrate stream processing and batch processing; It is concluded that Flink meets the requirements of low latency, exactly once guarantee, high throughput and efficient processing."
pub.1137199453,Enabling Internet of Media Things With Edge-Based Virtual Multimedia Sensors,"Multimedia sensors have recently become a significant data source in the Internet of Things (IoT), giving rise to the Internet of Media Things (IoMT). Since multimedia applications are usually latency-sensitive, data processing in the cloud is not always suitable. A strategy to minimize delay is processing multimedia streams closer to data sources, exploiting resources at the network edge. Moreover, virtualization is widely used to reduce complexity arising from heterogeneity in IoT environments. In this paper, we propose an edge-based architecture and platform to manage virtual multimedia sensors (VMS), enabling IoMT applications to be easily deployed. Our proposal encompasses V-PRISM, a software architecture tailored for IoMT, and ALFA, a distributed implementation of the architecture. V-PRISM components were designed to be deployed and executed in multiple edge nodes. VMSs are in charge of processing multimedia streams and provide an abstraction layer between IoMT applications and physical devices that produce those streams. This paper describes the proposal and the results of experiments showing that the proposed approach can successfully perform multimedia stream processing for applications that requires low-latency."
pub.1068094236,PrefixSummary: A Directory Structure for Selective Probing on Wireless Stream of Heterogeneous XML Data,"Wireless broadcasting of heterogeneous XML data has become popular in many applications, where energy-efficient processing of user queries at the mobile client is a critical issue. This paper proposes a new index structure for wireless stream of heterogeneous XML data to enhance tuning time performance in processing path queries on the stream. The index called PrefixSummary stores for each location path in the XML data the address of a bucket in the stream which contains an XML node satisfying the location path and appearing first in the stream. We present algorithms to generate broadcast stream with the proposed index and to process a path query on the stream efficiently by exploiting the index. We also suggest a replication scheme of PrefixSummary within a broadcast cycle to reduce latency in query processing. By analysis and experiment we show the proposed PrefixSummary approach can reduce tuning time for processing path queries significantly while it can also achieve reasonable access time performance by means of replication of the index over the broadcast stream."
pub.1048184589,Operator scheduling in data stream systems,"Abstract.In many applications involving continuous data streams, data arrival is bursty and data rate fluctuates over time. Systems that seek to give rapid or real-time query responses in such an environment must be prepared to deal gracefully with bursts in data arrival without compromising system performance. We discuss one strategy for processing bursty streams - adaptive, load-aware scheduling of query operators to minimize resource consumption during times of peak load. We show that the choice of an operator scheduling strategy can have significant impact on the runtime system memory usage as well as output latency. Our aim is to design a scheduling strategy that minimizes the maximum runtime system memory while maintaining the output latency within prespecified bounds. We first present  Chain scheduling, an operator scheduling strategy for data stream systems that is near-optimal in minimizing runtime memory usage for any collection of single-stream queries involving selections, projections, and foreign-key joins with stored relations. Chain scheduling also performs well for queries with sliding-window joins over multiple streams and multiple queries of the above types. However, during bursts in input streams, when there is a buildup of unprocessed tuples, Chain scheduling may lead to high output latency. We study the online problem of minimizing maximum runtime memory, subject to a constraint on maximum latency. We present preliminary observations, negative results, and heuristics for this problem. A thorough experimental evaluation is provided where we demonstrate the potential benefits of Chain scheduling and its different variants, compare it with competing scheduling strategies, and validate our analytical conclusions."
pub.1154180247,Shepherd: Seamless Stream Processing on the Edge,"Next generation applications such as augmented/vir-tual reality, autonomous driving, and Industry 4.0, have tight latency constraints and produce large amounts of data. To address the real-time nature and high bandwidth usage of new applications, edge computing provides an extension to the cloud infrastructure through a hierarchy of datacenters located between the edge devices and the cloud. Outside of the cloud and closer to the edge, the network becomes more dynamic requiring stream processing frameworks to adapt more frequently. Cloud based frameworks adapt very slowly because they employ a stop-the-world approach and it can take several minutes to reconfigure jobs resulting in downtime. In this paper, we propose Shepherd, a new stream processing framework for edge computing. Shepherd minimizes downtime during application reconfiguration, with almost no impact on data processing latency. Our experiments show that, compared to Apache Storm, Shepherd reduces application downtime from several minutes to a few tens of milliseconds."
pub.1061662091,Energy- and Latency-Efficient Processing of Full-Text Searches on a Wireless Broadcast Stream,"In wireless mobile computing environments, broadcasting is an effective and scalable technique to disseminate information to a massive number of clients, wherein the energy usage and latency are considered major concerns. This paper presents an indexing scheme for the energy- and latency-efficient processing of full-text searches over the wireless broadcast data stream. Although a lot of access methods and index structures have been proposed in the past for full-text searches, all of them are targeted for data in disk storage, not wireless broadcast channels. For full-text searches on a wireless broadcast stream, we firstly introduce a naive, inverted list-style indexing method, where inverted lists are placed in front of the data on the wireless channel. In order to reduce the latency overhead, we propose a two-level indexing method which adds another level of index structure to the basic inverted list-style index. In addition, we propose a replication strategy of the index list and index tree to further improve the latency performance. We analyze the performance of the proposed indexing scheme with respect to the latency and energy usage measures, and show the optimality of index replication. The correctness of the analysis is demonstrated through simulation experiments, and the effectiveness of the proposed scheme is shown by implementing a real wireless information delivery system."
pub.1094134328,A Performance Comparison of Open-Source Stream Processing Platforms,"Distributed stream processing platforms are a new class of real-time monitoring systems that analyze and extract knowledge from large continuous streams of data. These type of systems are crucial for providing high throughput and low latency required by Big Data or Internet of Things monitoring applications. This paper describes and analyzes three main open-source distributed stream-processing platforms: Storm, Flink, and Spark Streaming. We analyze the system architectures and we compare their main features. We carry out two experiments concerning threats detection on network traffic to evaluate the throughput efficiency and the resilience to node failures. Results show that the performance of native stream processing systems, Storm and Flink, is up to 15 times higher than the micro-batch processing system, Spark Streaming. However, Spark Streaming is robust to node failures and provides recovery without losses."
pub.1095494120,Stream As You Go: The Case for Incremental Data Access and Processing in the Cloud,"Cloud infrastructures promise to provide highperformance and cost-effective solutions to large-scale data processing problems. In this paper, we identify a common class of data-intensive applications for which data transfer latency for uploading data into the cloud in advance of its processing may hinder the linear scalability advantage of the cloud. For such applications, we propose a “stream-as-you-go” approach for incrementally accessing and processing data based on a stream data management architecture. We describe our approach in the context of a DNA sequence analysis use case and compare it against the state of the art in MapReduce-based DNA sequence analysis and incremental MapReduce frameworks. We provide experimental results over an implementation of our approach based on the IBM InfoSphere Streams computing platform deployed on Amazon EC2, showing an order of magnitude improvement in total processing time over the state of the art."
pub.1141496923,Micro-Workflows Data Stream Processing Model for Industrial Internet of Things,"The fog computing paradigm has become prominent in stream processing for IoT systems where cloud computing struggles from high latency challenges. It enables the deployment of computational resources between the edge and cloud layers and helps to resolve constraints, primarily due to the need to react in real-time to state changes, improve the locality of data storage, and overcome external communication channels’ limitations. There is an urgent need for tools and platforms to model, implement, manage, and monitor complex fog computing workflows. Traditional scientific workflow management systems (SWMSs) provide modularity and flexibility to design, execute, and monitor complex computational workflows used in smart industry applications. However, they are mainly focused on batch execution of jobs consisting of tightly coupled tasks. Integrating data streams into SWMSs of IoT systems is challenging. We proposed a microworkflow model to redesign the monolith architecture of workflow systems into a set of smaller and independent workflows that support stream processing. Micro-workflow is an independent data stream processing service that can be deployed on different layers of the fog computing environment. To validate the feasibility and practicability of the micro-workflow refactoring, we provide intensive experimental analysis evaluating the interval between sensor messages, the time interval required to create a message, between sending sensor message and receiving the message in SWMS, including data serialization, network latency, etc. We show that the proposed decoupling support of the independence of implementation, execution, development, maintenance, and cross-platform deployment, where each micro-workflow becomes a standalone computational unit, is a suitable mechanism for IoT stream processing."
pub.1127756216,Flink‐ER: An Elastic Resource‐Scheduling Strategy for Processing Fluctuating Mobile Stream Data on Flink,"As real-time and immediate feedback becomes increasingly important in tasks related to mobile information, big data stream processing systems are increasingly applied to process massive amounts of mobile data. However, when processing a drastically fluctuating mobile data stream, the lack of an elastic resource-scheduling strategy limits the elasticity and scalability of data stream processing systems. To address this problem, this paper builds a flow-network model, a resource allocation model, and a data redistribution model as the foundation for proposing Flink with an elastic resource-scheduling strategy (Flink-ER), which consists of a capacity detection algorithm, an elastic resource reallocation algorithm, and a data redistribution algorithm. The strategy improves the performance of the platform by dynamically rescaling the cluster and increasing the parallelism of operators based on the processing load. The experimental results show that the throughput of a cluster was promoted under the premise of meeting latency constraints, which verifies the efficiency of the strategy."
pub.1131566412,FoT-Stream: A Fog platform for data stream analytics in IoT,"The Internet of Things (IoT) has developed infrastructures and applications that generate large amounts of data. These data are usually yielded as streams, characterized for being continuous and infinite, and presenting the peculiarity of modifying their behavior over time. Due to the large capacity of storage, data processing, and provisioning of resources, such data are generally processed/analyzed in Cloud Computing environments. Although those environments provide IoT infrastructure with adequate scalability and resource-centric features, the distance between devices and the cloud can impose limitations to achieve low latency in data traffic. In order to maintain scalability, achieve low latency, and reduce data traffic between the IoT devices and the Cloud, the Fog Computing was proposed, providing resource availability at the edge of the network. Although Fog Computing establishes resource availability at the edge of the network, the technologies/techniques currently used for IoT data processing and analysis may not be sufficient to support the continuous and unlimited streams that IoT platforms and applications produce. In addition, data stream applications at Fog must be supported by the computationally limited devices. Therefore, aiming at taking advantage of Fog Computing and resolve the gap of the data stream in IoT environments, this work presents a new platform called FoT-Stream designed to process and analyze, in real-time, data stream from the Internet of Things in Fog. The main advantage of using our approach is the possibility of reducing the amount of data transmitted on the network infrastructure, which allows, as a consequence, to perform an online data modeling, by detecting changes in data behavior, and a reduction of the Internet usage. Our results in a real-world scenario emphasize FoT-Stream considerably reduces the latency and amount of data traffic in IoT environments without affecting the system throughput."
pub.1103852898,Characterizing the Impact of Topology on IoT Stream Processing,"The Internet of Things (IoT) extends traditional cyber-physical systems by linking sensor based edge devices to network accessible services and resources. In most current IoT deployments, sensor data is streamed from edge devices to servers for storage. Analytical pipelines are then used to translate this raw sensor data into actionable information in real-time. As additional IoT devices are deployed, the volume and rate of data received on the server side can increase dramatically. This has a possibility of offsetting the response latencies beyond acceptable limits for IoT analytical systems. In this paper, we compare the impact of alternative server-side stream processing topologies for ingesting and analyzing IoT sensor data in real-time. We use real building sensor data with our real-time IoT platform called Namatad. We have characterized and analyzed the latency and QoS impact due to the different levels of granularity of the ingestion and routing process by which we transmit data into the analytical pipelines. Our results show that as IoT systems continue to scale in density, server-side topology management for IoT data streams is critical for latency-sensitive control and analysis applications."
pub.1154887106,Recovery-Robust Inquiry to External Services from Stream Processing Application,"In the commercial development of a stream processing application, it’s frequently required to connect to external existing services or applications from it in order to, for example, retrieve data from a data store server. If we connect them through message queues (MQs) adopting exactly-once (EO) message delivery semantics to avoid duplicate processing of the same messages caused by a failure recovery of the upstream backup technique that involves rereading of messages stored in MQs, it causes serious performance degradation on the inquiries. This becomes a major obstacle in realizing real-time applications. This paper investigates the kind of problems that practically happen in failure recovery of a stream processing application that inquires to external services or applications through MQs adopting at-least-once (ALO) message delivery semantics instead of EO to avoid the latency overhead. The efforts revealed three typical consistency problems on message sequencing that happen in failure recovery. By solving each of the three problems, our developed R3CM technique to inquire to external services/applications from a stream processing application turns the adoption of ALO semantics in MQs into an attractive option to avoid latency overhead."
pub.1133450925,SR3,"Modern stream processing applications need to store and update state along with their processing, and process live data streams in a timely fashion from massive and geo-distributed data sets. Since they run in a dynamic distributed environment and their workloads may change in unexpected ways, multiple stream operators can fail at the same time, causing severe state loss. However, the state-of-the-art stream processing systems are mainly designed for low-latency intra-datacenter settings and do not scale well for running stream applications that contain large distributed states, suffering a significantly centralized bottleneck and high latency to recover state. They offer failure recovery mainly through three approaches: replication recovery, checkpointing recovery, and DStream-based lineage recovery, which are either slow, resource-expensive or fail to handle multiple simultaneous failures. We present SR3, a customizable state recovery framework that provides fast and scalable state recovery mechanisms for protecting large distributed states in stream processing systems. SR3 offers three recovery mechanisms --- the star-structured recovery, the line-structured recovery, and the tree-structured recovery --- to cater to the needs of different stream processing computation models, state sizes, and network settings. Our design adopts a decentralized architecture that partitions and replicates states by using consistent ring overlays that leverage distributed hash tables (DHTs). We show that this approach can significantly improve the scalability and flexibility of state recovery. We realize the SR3 design on a prototype integrated with the widely adopted Apache Storm framework. Large-scale experiments using real-world datasets demonstrate SR3's scalability, fast recovery, and flexibility properties."
pub.1019029669,Energy and Latency Efficient Access of Wireless XML Stream,"<p>In this article, we address the problem of delayed query processing raised by tree-based index structures in wireless broadcast environments, which increases the access time of mobile clients. We propose a novel distributed index structure and a clustering strategy for streaming XML data that enables energy and latencyefficient broadcasting of XML data. We first define the DIX node structure to implement a fully distributed index structure which contains the tag name, attributes, and text content of an element, as well as its corresponding indices. By exploiting the index information in the DIX node stream, a mobile client can access the stream with shorter latency. We also suggest a method of clustering DIX nodes in the stream, which can further enhance the performance of query processing in the mobile clients. Through extensive experiments, we demonstrate that our approach is effective for wireless broadcasting of XML data and outperforms the previous methods.</p>"
pub.1124131388,Stateful Stream Processing for Digital Twins: Microservice-Based Kafka Stream DSL,"Digital Twin is a virtual representation of a technological process or a piece of equipment, that supports monitoring, control and state prediction based on the data, gathered from the sensor networks. To parallelize event processing and produce near-real-time insights over data streams, Digital Twin should be implemented based on an Event-Driven architecture. The Event-Driven architecture is loosely-coupled by its nature. One of the recent possible solutions for loose coupling system is a Microservice approach, a cohesive and independent process that interacts using messages. Stateless behavior is the nature of the microservice, but on the other hand, the vast majority of stream processing in Digital Twin imply stateful operations. Thus, in this paper, we propose a case-study of the possibility to use Apache Kafka Stream API (Kafka stream DSL) to build stateful microservice for real-time manufacturing data analysis. Also, in the presented work we discuss the fulfillment of such requirements as fault tolerance, processing latency, and scalability to support the stateful stream processing in Digital Twins implementation."
pub.1131823668,Event-Driven Data Pipeline for Network Management Systems,"Developing an event-driven applications for network management systems(NMS) requires data-wrangling operations to be performed in a data pipeline with low latency and high throughput. Data stream processing engines(SPEs) enables the pipeline to feature extract complex data in near real time. We present a low latency and high throughput data pipeline that performs operations such as data cleansing, filtering, aggregations and join of streams on pipeline stages for event-driven machine learning applications. When the data stream has imbalance in arrival of records, a standalone system handle it ineffectively that reduces the performance of data pipeline. In this work, the system powered by SPEs handle the imbalance by scaling out resources and degree of parallelism such that performance is not degraded."
pub.1132552353,A Reactive Batching Strategy of Apache Kafka for Reliable Stream Processing in Real-time,"Modern stream processing systems need to process large volumes of data in real-time. Various stream processing frameworks have been developed and messaging systems are widely applied to transfer streaming data among different applications. As a distributed messaging system with growing popularity, Apache Kafka processes streaming data in small batches for efficiency. However, the robustness of Kafka’s batching method against variable operating conditions is not known. In this paper we study the impact of the batch size on the performance of Kafka. Both configuration parameters, the spatial and temporal batch size, are considered. We build a Kafka testbed using Docker containers to analyze the distribution of Kafka’s end-to-end latency. The experimental results indicate that evaluating the mean latency only is unreliable in the context of real-time systems. In the experiments where network faults are injected, we find that the batch size affects the message loss rate in the presence of an unstable network connection. However, allocating resources for message processing and delivery that will violate the reliability requirements implemented as latency constraints of a real-time system is inefficient To address these challenges we propose a reactive batching strategy. We evaluate our batching strategy in both good and poor network conditions. The results show that the strategy is powerful enough to meet both latency and throughput constraints even when network conditions are variable."
pub.1094747669,A High-Throughput and Low-Latency Parallelization of Window-based Stream Joins on Multicores,"Data Stream Processing (DaSP) is a paradigm characterized by on-line (often real-time) applications working on unlimited data streams whose elements must be processed efficiently “on the fly”. DaSP computations are characterized by data-flow graphs of operators connected via streams and working on the received elements according to high throughput and low latency requirements. To achieve these constraints, high-performance DaSP operators requires advanced parallelism models, as well related design and implementation techniques targeting multi-core architectures. In this paper we focus on the parallelization of the window-based stream join, an important operator that raises challenging issues in terms of parallel windows management. We review the state-of-the-art solutions about the stream join parallelization and we propose our novel parallel strategy and its implementation on multicores. As demonstrated by experimental results, our parallel solution introduces two important advantages with respect to the existing solutions: (i) it features an high-degree of configurability in order to address the symmetricity/asymmetricity of input streams (in terms of their arrival rate and window length); (ii) our parallelization provides a high throughput and it is definitely better than the compared solutions in terms of latency, providing an efficient way to perform stream joins on latency-sensible applications."
pub.1121933471,Linear Scheduling of Big Data Streams on Multiprocessor Sets in the Cloud,"Nowadays, there is an accelerating need to efficiently and timely handle large amounts of data that arrives continuously. Streams of big data led to the emergence of Distributed Stream Processing Systems (DSPS) that assign processing tasks to the available resources (dynamically or not) and route streaming data between them. Efficient scheduling of processing tasks of data flows can reduce application latencies and eliminate network congestion. In this work, we propose a linear complexity scheme for the task allocation and scheduling problem to improve system’s performance, load balancing and memory efficiency, in applications where there is need for heavy communication (all-to-all) between the tasks assigned to pairs of components."
pub.1094998452,Event Stream Processing with Out-of-Order Data Arrival,"Complex event processing has become increasingly important in modern applications, ranging from supply chain management for RFID tracking to real-time intrusion detection. The goal is to extract patterns from such event streams in order to make informed decisions in real-time. However, networking latencies and even machine failure may cause events to arrive out-of-order at the event stream processing engine. In this work, we address the problem of processing event pattern queries specified over event streams that may contain out-of-order data. First, we analyze the problems state-of-the-art event stream processing technology would experience when faced with out-of-order data arrival. We then propose a new solution of physical implementation strategies for the core stream algebra operators such as sequence scan and pattern construction, including stack-based data structures and associated purge algorithms. Optimizations for sequence scan and construction as well as state purging to minimize CPU cost and memory consumption are also introduced. Lastly, we conduct an experimental study demonstrating the effectiveness of our approach."
pub.1182125259,Real-Time AI Analytics with Apache Flink: Powering Immediate Insights with Stream Processing,"Real-time AI analytics is the latest favorite of Apache Flink, and businesses love what it offers, as the framework has everything to help analyze data as it streams in. With the widespread need for swift, data-driven decision-making, Flink's speed of low latency processing, event timing, and ability to leverage AI models reactively so you have instant insights make it a solid"
pub.1094874230,High Frequency Trading with Complex Event Processing,"High frequency trading is steadily taking over the equity trading world. High frequency trading involves very high speed systems placing trades at sub millisecond speeds across multiple stock exchanges. HFT is a good example for Big Data analytics - especially the velocity aspect of big data. For HFT strategies to be profitable, real time processing of big data is essential. In this paper we discuss the challenges faced by HFT systems and the opportunity for big data processing with low latency in the field. Most HFT systems are designed using real time stream processing, which have certain drawbacks. We present a theoretical framework for building high frequency trading systems using the complex event processing paradigm which could overcome the drawbacks of stream processing. Complex event processing enables detecting patterns of events from disparate events streams and responds to the detected pattern. The applicability of the framework for HFT applications is discussed."
pub.1092091908,Strider: A Hybrid Adaptive Distributed RDF Stream Processing Engine,"Real-time processing of data streams emanating from sensors is becoming a common task in Internet of Things scenarios. The key implementation goal consists in efficiently handling massive incoming data streams and supporting advanced data analytics services like anomaly detection. In an on-going, industrial project, a 24 / 7 available stream processing engine usually faces dynamically changing data and workload characteristics. These changes impact the engine’s performance and reliability. We propose Strider, a hybrid adaptive distributed RDF Stream Processing engine that optimizes logical query plan according to the state of data streams. Strider has been designed to guarantee important industrial properties such as scalability, high availability, fault tolerance, high throughput and acceptable latency. These guarantees are obtained by designing the engine’s architecture with state-of-the-art Apache components such as Spark and Kafka. We highlight the efficiency (e.g., on a single machine machine, up to 60x gain on throughput compared to state-of-the-art systems, a throughput of 3.1 million triples/second on a 9 machines cluster, a major breakthrough in this system’s category) of Strider on real-world and synthetic data sets."
pub.1127631512,Deciding Backup Location Methods for Distributed Stream Processing System,"A large amount of stream data are generated from some devices such as sensors and cameras. These stream data should be timely processed for real-time applications to satisfy the data latency requirements. To process a large amount of data in a short time, utilizing stream processing on edge/fog computing is a promising technology. In the stream processing system, a snapshot of processes and replications of the stream data are stored on another server, and when server fault or load spike of server occurs, the process is continued by using the stored snapshots and replicated data. Therefore, with edge computing environment, which has low bandwidth resource, process recovery takes a long time due to the transferring of restored data. In this paper, we propose a stream processing system architecture to decide servers to store snapshots and replication data and redeploy processes by considering the load of each server and the network bandwidth. We also propose a semi-optimal algorithm that reduces the computational cost by appropriately sorting servers and tasks according to the network bandwidth and server load. The algorithm can find a solution over 1000 times faster than the Coin or Branch and Cut (CBC) solver."
pub.1045829981,Advances and Challenges for Scalable Provenance in Stream Processing Systems,"While data provenance is a well-studied topic in both database and workflow systems, its support within stream processing systems presents a new set of challenges. Part of the challenge is the high stream event rate and the low processing latency requirements imposed by many streaming applications. For example, emerging streaming applications in healthcare or finance call for data provenance, as illustrated in the Century stream processing infrastructure that we are building for supporting online healthcare analytics. At anytime, given an output data element (e.g., a medical alert) generated by Century, the system must be able to retrieve the input and intermediate data elements that led to its generation. In this paper, we describe the requirements behind our initial implementation of Century’s provenance subsystem. We then analyze its strengths and limitations and propose a new provenance architecture to address some of these limitations. The paper also includes a discussion on the open challenges in this area."
pub.1148092532,Evaluating model serving strategies over streaming data,"We present the first performance evaluation study of model serving integration tools in stream processing frameworks. Using Apache Flink as a representative stream processing system, we evaluate alternative Deep Learning serving pipelines for image classification. Our performance evaluation considers both the case of embedded use of Machine Learning libraries within stream tasks and that of external serving via Remote Procedure Calls. The results indicate superior throughput and scalability for pipelines that make use of embedded libraries to serve pre-trained models. Whereas, latency can vary across strategies, with external serving even achieving lower latency when network conditions are optimal due to better specialized use of underlying hardware. We discuss our findings and provide further motivating arguments towards research in the area of ML-native data streaming engines in the future."
pub.1021516935,Adaptive Scheduling Strategy for Data Stream Management System,"More and more applications involve processing continuous data streams, and the data stream management system (DSMS) is designed to deal with such data streams. Due to features of large volume and stochastic arrival, DSMS must process data stream efficiently in order to avoid system memory exhaustion and reduce the data access latency, to satisfy requirements of the application requirement. One of the key factors, which significantly impact the system performance significantly, is the scheduling strategy adopted by the DSMS. Chain scheduling is an operator-based scheduling strategy for DSMS, which has near-optimal in terms of run-time memory usage. FIFO strategy achieves optimal performance in terms of data access latency. Inspired by the two important scheduling strategies, Chain and FIFO, we propose two novel adaptive strategies for DSMS, ASCF and CSS, which efficiently deal with the varying input load in terms of both memory usage and data access latency. To give a fair comparison performance with other competing strategies, we design thorough simulation experiment and run different strategies under the same system environment. The outcomes of simulation experiment demonstrate the potential benefits and advantages of ASCF and CSC."
pub.1029611281,Estimating online vacancies in real-time road traffic monitoring with traffic sensor data stream,"Real-time road traffic monitoring is widely considered to be a promising traffic management approach in urban environments. In the smart cities scenario, traffic trajectory sensor data streams are constantly produced in real time from probe vehicles, which include taxis and buses. By exploiting the mass sensor data streams, we can effectively predict and prevent traffic jams in a timely manner. However, there are two urgent challenges to processing the massive amounts of continuously generated trajectory sensor data: (1) the inhomogeneous sparseness in both spatial and temporal dimensions that is introduced by probe vehicles moving at their own will, and (2) processing stream data in real time manner with low latency. In this study, we aim to ameliorate the aforementioned two issues. We propose an online approach to addresses the major defect of inhomogeneous sparseness, which focuses on employing only real-time data rather than mining historical data. Furthermore, we set up a real-time system to process trajectory data with low latency. Our tests are performed using field test data sets derived from taxis in an urban environment; the results show that our proposed method lends validity and efficiency advantages for tackling the sparseness, and our real-time system is viable for low latency applications such as trafficmonitoring."
pub.1142646655,Service Deployment with Predictive Ability for Data Stream Processing in a Cloud-Edge Environment,"Runtime IoT data fluctuation brings challenges for optimizing the resource allocation for a data stream processing (DSP) flow in a cloud-edge environment. It can result in extra high latency for a flow. Optimized strategy of dynamic resource allocation is still hard to design to timely dealing with the IoT data fluctuation. In this paper, the above challenge is abstracted and redefined as the service deployment problem. An improved GA optimization algorithm, integrating with the IoT data fluctuation prediction ability, is proposed to handle IoT data fluctuations during the running of a DSP flow. Effectiveness of the proposed approach is evaluated based on the real datasets from a real application."
pub.1105096488,Adaptive correlated prefetch with large-scale hybrid memory system for stream processing,"Owing to the exponential growth of real-time data generation, the importance of stream processing is ever increasing. However, the data processing paradigm of stream processing is quite different, so it is difficult to expect high performance from memory systems applied to existing data centers. To solve this problem, two main solutions are suggested in this paper. First, a hybrid main memory and small buffer architecture are designed to reflect the execution characteristics of stream processing. Second, a hardware-based prefetch module supports correlation prefetching. Stream processing tends to accept incoming data in the main memory, so the prefetch module is used to divert data from the main memory layer to the buffer layer based on an intelligent clustering algorithm. This clustering algorithm affects the rapidly changing data access pattern of stream processing applications. By using heterogeneous main memories, not only can one enjoy the fast access latency of DRAM but also its nonvolatility, scalability, and low power consumption. The proposed hybrid memory architecture with our prefetch buffer structure can improve the buffer hit rate by 9–14% over other prefetch methods, reduce energy consumption by 26% over the conventional DRAM-only model, and achieve similar execution time over the 1/8-size DRAM space of the DRAM-only model."
pub.1154202045,Towards Low-Latency Big Data Infrastructure at Sangfor,"As a top cybersecurity vendor, Sangfor needs collects log streams from thousands of endpoint detection devices such as NTA, STA, EDR and identifies security threats in real-time way everyday. The discovery and disposal of network security incidents are highly real-time in nature with seconds or even milliseconds response time to prevent possible cyber attacks and data leaks. In order to extract more valuable information, the log streams are analyzed using stream processing with pattern matching like CEP (Complex Event Processing) in memory, and then stored in a persistent storage systems such as a data warehouse system or a search engine system for data scientists and network security engineers to do OLAP (Online Analytical Processing). Sangfor needs to build a low-latency big data platform to meet the challenges of massive logs.More and more open source systems are proposed to solve the problem of data processing in a certain aspect. Many decisions must be made to balance the benefits when designing a real-time big data infrastructure. What’s more, how to architecture these systems and construct a one-stack unified big data platform have been the key obstacles for big data analytics. In this paper, we present the overall architecture of our low-latency big data infrastructure and identify four important design decisions i.e. message queue, stream processing, OLAP, and data lake. We analyze the advantages and disadvantages of existing open source system and clarify the reason behind our choices. We also describe the improvements and optimizations to make the open-source stacks fit in Sangfor’s environments, including designing a real-time development platform based on Flink and re-architecting Apache Kylin, Clickhouse and Presto as a HOLAP system. Then we highlight two important use cases to verify the rationality of our infrastructure."
pub.1119409778,Towards Concurrent Stateful Stream Processing on Multicore Processors (Technical Report),"Recent data stream processing systems (DSPSs) can achieve excellent
performance when processing large volumes of data under tight latency
constraints. However, they sacrifice support for concurrent state access that
eases the burden of developing stateful stream applications. Recently, some
have proposed managing concurrent state access during stream processing by
modeling state accesses as transactions. However, these are realized with locks
involving serious contention overhead. Their coarse-grained processing paradigm
further magnifies contention issues and tends to poorly utilize modern
multicore architectures. This paper introduces TStream , a novel DSPS
supporting efficient concurrent state access on multicore processors.
Transactional semantics is employed like previous work, but scalability is
greatly improved due to two novel designs: 1) dual-mode scheduling, which
exposes more parallelism opportunities, 2) dynamic restructuring execution,
which aggressively exploits the parallelism opportunities from dual-mode
scheduling without centralized lock contentions. To validate our proposal, we
evaluate TStream with a benchmark of four applications on a modern multicore
machine. The experimental results show that 1) TStream achieves up to 4.8 times
higher throughput with similar processing latency compared to the
state-of-the-art and 2) unlike prior solutions, TStream is highly tolerant of
varying application workloads such as key skewness and multi-partition state
accesses."
pub.1147547338,Targeting a light-weight and multi-channel approach for distributed stream processing,"Processing high-throughput data-streams has become a major challenge in areas such as real-time event monitoring, complex dataflow processing, and big data analytics. While there has been tremendous progress in distributed stream processing systems in the past few years, the high-throughput and low-latency (a.k.a. high sustainable-throughput) requirement of modern applications is pushing the limits of traditional data processing infrastructures. This paper introduces a new distributed stream processing engine (DSPE), called Asynchronous Iterative Routing (or simply “AIR”), which implements a light-weight, dynamic sharding protocol. AIR expedites direct and asynchronous communication among all the worker nodes via a channel-like communication protocol on top of the Message Passing Interface (MPI), thereby completely avoiding the need for a dedicated driver node. The system adopts a new progress-tracking protocol, called hew-meld, which has been experimentally observed to show a low processing latency on our asynchronous master-less architecture when compared to the conventional low-watermark technique. The current version of AIR is also equipped with two fault tolerance and recovery strategies namely checkpointing & rollback and replication. With its unique design, AIR scales out particularly well to multi-core HPC architectures; specifically, we deployed it on clusters with up to 16 nodes and 448 cores (thus reaching a peak of 435.3 million events and 55.14 GB of data processed per second), which we found to significantly outperform existing DSPEs."
pub.1163656162,StreamSwitch: Fulfilling Latency Service-Layer Agreement for Stateful Streaming,"Distributed stream systems provide low latency by processing data as it arrives. However, existing systems do not provide latency guarantee, a critical requirement of real-time analytics, especially for stateful operators under burst and skewed workload. We present StreamSwitch, a control plane for stream systems to bound operator latency while optimizing resource usage. Based on a novel stream switch abstraction that unifies dynamic scaling and load balancing into a holistic control framework, our design incorporates reactive and predictive metrics to deduce the healthiness of executors and prescribes practically optimal scaling and load balancing decisions in time. We implement a prototype of StreamSwitch and integrate it with Apache Flink and Samza. Experimental evaluations on real-world applications and benchmarks show that StreamSwitch provides cost-effective solutions for bounding latency and outperforms the state-of-the-art alternative solutions."
pub.1169921206,Challenges in Implementing Real-Time Stream Data in Bioinformatics,"Creating efficient and adaptable methods for data analysis is crucial in bioinformatics as the amount of real-time stream data continues to grow. We describe an integrated approach that combines complex event processing (CEP), online machine learning (ML), and data stream clustering to solve the problems associated with real-time stream data processing in bioinformatics. This method brings together online machine learning, complex event processing (CEP), and clustering in data streams. Flexible, all-encompassing, and real-time research are just a few of the ways in which this innovative approach excels above more conventional ones. We use several key performance indicators to show how our approach improves upon the status quo. In this study, we looked at ten different strategies and compared their performance over a range of metrics. One of these features was the flexibility to grow or shrink without sacrificing data security, performance, latency, accuracy, or any combination thereof. According to the findings, the novel approach performs better than the conventional methods in almost all respects. Better scalability, resource usage, data integrity, throughput, latency, accuracy, and scalability were all benefits. The proposed method was proven to be a more efficient choice for real-time data processing in bioinformatics due to its increased flexibility and comprehensiveness. This allows us to surmount the obstacles given by the volatile character of biological data. Bio informaticists and other medical professionals may use its flexibility and real-time analytical abilities in a variety of contexts, from analyzing genetic data to monitoring patients' health."
pub.1127951034,Towards Concurrent Stateful Stream Processing on Multicore Processors,"Recent data stream processing systems (DSPSs) can achieve excellent performance when processing large volumes of data under tight latency constraints. However, they sacrifice support for concurrent state access that eases the burden of developing stateful stream applications. Recently, some have proposed managing concurrent state access during stream processing by modeling state accesses as transactions. However, these are realized with locks involving serious contention overhead. The coarse-grained processing paradigm adopted in these proposals magnify contention issues and does not exploit modern multicore architectures to their full potential. This paper introduces TStream, a novel DSPS supporting efficient concurrent state access on multicore processors. Transactional semantics is employed like previous work, but scalability is greatly improved due to two novel designs: 1) dual-mode scheduling, which exposes more parallelism opportunities, 2) dynamic restructuring execution, which aggressively exploits the parallelism opportunities from dual-mode scheduling without centralized lock contentions. To validate our proposal, we evaluate TStream with a benchmark of four applications on a modern multicore machine. Experimental results show that 1) TStream achieves up to 4.8 times higher throughput with similar processing latency compared to the state-of-the-art and 2) unlike prior solutions, TStream is highly tolerant of varying application workloads such as key skewness and multi-partition state accesses."
pub.1169117630,Apache Kafka - Real-time Data Processing,"Apache Kafka is creating a lot of buzz these days. While LinkedIn, where Kafka was founded, is the most well known user, there are many companies that use this technology successfully. Kafka has several features that make it a good t for companies' requirements: scalability, data partitioning, low latency, and the ability to handle large number of diverse consumers. It works with Apache Storm and Apache Spark for real-time analysis and rendering of streaming data. The combination of messaging and processing technologies enables stream processing at linear scale. Common use cases include: Messaging, Website activity tracking, Log aggregation, Stream Processing, Commit log."
pub.1138456469,Evaluative Review of Streaming Analytics: Tools and Technologies in Real-Time Data Processing,"Nowadays, big data processing systems are evolving to be more stream-oriented; where each data record is processed as it arrives by distributed and low latency computational frameworks [18]. Data streams have been extensively used in several fields of computational analytics such as data mining, business intelligence etc. [17]. In every field, the data stream can be considered as an ordered sequence of data items, as they continuously arrive over the period. Due to this characteristic, streaming data analytics is a challenging area of research [5, 11]. This paper aims to present data stream processing as a growing research field , along with streaming analytics frameworks as a rich focus area. The paper also contributes to evaluate the efficacy of available stream analytics frameworks. One of the Industry 4.0 use case - predictive maintenance rail transportation - has been illustrated here as a case study design mapped with streaming analytics framework."
pub.1152402926,"Blue Danube: A Large-Scale, End-to-End Synchronous, Distributed Data Stream Processing Architecture for Time-Sensitive Applications","An extensive list of time-sensitive applications requiring ultra-low latency ranging from a few microseconds to a few milliseconds are presented in recent publications and IEEE standards. Time-sensitive applications, include industrial, critical healthcare and transportation applications as also applications for Smart Grids and the Internet of Vehicles – one of the most active research fields of Intelligent Transportation Systems of Smart Cities. In this work, we mainly set our focus on the suite of safety applications which attracts strong interest from the research community, as it aims to avoid road accidents and save lives. The IEEE Time-Sensitive Networking (TSN) set of standards specifies fundamental real-time characteristics. Nevertheless, as TSN works on Data Link layer (Layer 2 of the OSI model) the benefits of these characteristics fade away when other layers are crossed from the Application layer (Layer 7). Indicatively, recent research works report latencies on the order of tens of seconds when benchmarking Data Stream Processing and IoT platforms, and thus they are not suited for time-critical applications. Such platforms mainly use loosely coupled components with asynchronous communication. On Application layer, we propose a novel End-to-End Synchronous, Distributed Architecture for Large-Scale, High-Bandwidth, Ultra-Low Latency Data Stream Processing. Through our Big Data Stream analysis experiments (4.7 Gbit/s total average aggregated throughput, 1 Terabyte in-memory distributed database, 4 milliseconds average query latency) we have demonstrated the suitability of our architecture for time-sensitive applications such as accident avoidance for the Internet of Vehicles."
pub.1122094364,Joint Operator Scaling and Placement for Distributed Stream Processing Applications in Edge Computing,"Distributed Stream Processing (DSP) systems are well acknowledged to be potent in processing huge volume of real-time stream data with low latency and high throughput. Recently, the edge computing paradigm shows great potentials in supporting and boosting the DSP applications, especially the time-critical and latency-sensitive ones, over the Internet of Things (IoT) or mobile devices by means of offloading the computation from remote cloud to edge servers for further reduced communication latencies. Nevertheless, various challenges, especially the joint operator scaling and placement, are yet to be properly explored and addressed. Traditional efforts in this direction usually assume that the data-flow graph of a DSP application is pre-given and static. The resulting models and methods can thus be ineffective and show bad user-perceived quality-of-service (QoS) when dealing with real-world scenarios with reconfigurable data-flow graphs and scalable operator placement. In contrast, in this paper, we consider that the data-flow graphs are configurable and hence propose the joint operator scaling and placement problem. To address this problem, we first build a queuing-network-based QoS estimation model, then formulate the problem into an integer-programming one, and finally propose a two-stage approach for finding the near-optimal solution. Experiments based on real-world DSP test cases show that our method achieves higher cost effectiveness than traditional ones while meeting the user-defined QoS constraints."
pub.1094055180,An event-processing system alerting analytically to networked vehicles,"There are demands for alerts that use the results of intensive data analysis, such as relationships among events generated by many vehicles over a large area. These alerts require data analysis on servers. However, when a server receives massive amounts of data from vehicles, the processing latency increases because of the communication delays between the vehicles and the servers and the increased workload for data analysis on the server. Therefore we need to develop latency-tolerant alert-generating systems with scalable performance. In this paper, we report on a high-speed event-processing system architecture that integrates event processing in the in-vehicle systems with event processing in the servers. The in-vehicle system analyzes the vehicle's sensor data, detects events, and sends packets of the event information to the servers. The server has a stream-processing system, a pre-aggregation system, and a full-data-accumulation system. The stream-processing system receives the packets from the in-vehicle system. The pre-aggregation system creates and updates decision tables for the alerts repeatedly. The alerter of the stream-processing system generates alerts from the table. We implemented a prototype system to generate alerts about obstacles. We tested input data from actual vehicles and a traffic simulator and estimated the vehicles can receive the alerts within 1.2 sec even when the server receives massive data from 120,000 vehicles, which meets the performance requirement for our alert scenarios."
pub.1029265134,Large-Scale DNA Sequence Analysis in the Cloud: A Stream-Based Approach,"Cloud computing technologies have made it possible to analyze big data sets in scalable and cost-effective ways. DNA sequence analysis, where very large data sets are now generated at reduced cost using the Next-Generation Sequencing (NGS) methods, is an area which can greatly benefit from cloud-based infrastructures. Although existing solutions show nearly linear scalability, they pose significant limitations in terms of data transfer latencies and cloud storage costs. In this paper, we propose to tackle the performance problems that arise from having to transfer large amounts of data between clients and the cloud based on a streaming data management architecture. Our approach provides an incremental data processing model which can hide data transfer latencies while maintaining linear scalability. We present an initial implementation and evaluation of this approach for SHRiMP, a well-known software package for NGS read alignment, based on the IBM InfoSphere Streams computing platform deployed on Amazon EC2."
pub.1011754334,Fast Heuristics for Near-Optimal Task Allocation in Data Stream Processing over Clusters,"We study provisioning and job reconfiguration techniques for adapting to execution environment changes when processing data streams on cluster-based deployments. By monitoring the performance of an executing job, we identify computation and communication bottlenecks. In such cases we reconfigure the job by reallocating its tasks to minimize the communication cost. Our work targets data-intensive applications where the inter-node transfer latency is significant. We aim to minimize the transfer latency while keeping the nodes below some computational load threshold. We propose a scalable centralized scheme that employs fast allocation heuristics. Our techniques are based on a general group-based job representation that is commonly found in many distributed data stream processing frameworks. Using this representation we devise linear-time task allocation algorithms that improve existing quadratic-time solutions in practical cases. We have implemented and evaluated our proposals using both synthetic and real-world scenarios. Our results show that our algorithms: (a) exhibit significant allocation throughput while producing near-optimal allocations, and (b) significantly improve existing task-level approaches."
pub.1123561891,Resource- and Message Size-Aware Scheduling of Stream Processing at the Edge with application to Realtime Microscopy,"Whilst computational resources at the cloud edge can be leveraged to improve
latency and reduce the costs of cloud services for a wide variety mobile, web,
and IoT applications; such resources are naturally constrained. For distributed
stream processing applications, there are clear advantages to offloading some
processing work to the cloud edge. Many state of the art stream processing
applications such as Flink and Spark Streaming, being designed to run
exclusively in the cloud, are a poor fit for such hybrid edge/cloud deployment
settings, not least because their schedulers take limited consideration of the
heterogeneous hardware in such deployments. In particular, their schedulers
broadly assume a homogeneous network topology (aside from data locality
consideration in, e.g., HDFS/Spark). Specialized stream processing frameworks
intended for such hybrid deployment scenarios, especially IoT applications,
allow developers to manually allocate specific operators in the pipeline to
nodes at the cloud edge. In this paper, we investigate scheduling stream
processing in hybrid cloud/edge deployment settings with sensitivity to CPU
costs and message size, with the aim of maximizing throughput with respect to
limited edge resources. We demonstrate real-time edge processing of a stream of
electron microscopy images, and measure a consistent reduction in end-to-end
latency under our approach versus a resource-agnostic baseline scheduler, under
benchmarking."
pub.1144339477,A Data Stream Prediction Strategy for Elastic Stream Computing Systems,"In a distributed stream processing system, elastic resource provisioning/scheduling is the main factor that affects system performance and limits system applications. However, in the data stream computing platform, resource allocation is often suboptimal due to the large fluctuations of the data stream rate, which creates a performance bottleneck for the cluster. In this paper, we propose a data stream prediction strategy (Dp-Stream) for elastic computing system to mitigate the resource allocation issue. First, we establish a back propagation (BP) neural network prediction model based on genetic simulated annealing algorithm to predict the trend of the data stream rate in the next time window of the cluster; second, according to the time latency, the estimation model adjusts the resources allocated to the critical operations of the critical path in the Directed Acyclic Graph (DAG) and finally, the resource communication cost is optimized. We evaluate the prediction accuracy and system latency of the proposed scheduling strategy in Storm. The experimental results prove the feasibility and effectiveness of the proposed strategy."
pub.1003390012,The PipeFlow approach,"In this paper, we present a description of our solution for solving the DEBS Grand Challenge 2015 that targets the analysis of taxi trips. Our implementation of this challenge is based on a general-purpose stream processing system called PipeFlow, which is designed and implemented to efficiently process continuous queries over high volume/speed data streams with low latency. Moreover, we present an experimental evaluation to show the effectiveness of the proposed solution with respect to query throughput and latency."
pub.1107852098,Benchmarking Distributed Stream Data Processing Systems,"The need for scalable and efficient stream analysis has led to the development of many open-source streaming data processing systems (SDPSs) with highly diverging capabilities and performance characteristics. While first initiatives try to compare the systems for simple workloads, there is a clear gap of detailed analyses of the systems' performance characteristics. In this paper, we propose a framework for benchmarking distributed stream processing engines. We use our suite to evaluate the performance of three widely used SDPSs in detail, namely Apache Storm, Apache Spark, and Apache Flink. Our evaluation focuses in particular on measuring the throughput and latency of windowed operations, which are the basic type of operations in stream analytics. For this benchmark, we design workloads based on real-life, industrial use-cases inspired by the online gaming industry. The contribution of our work is threefold. First, we give a definition of latency and throughput for stateful operators. Second, we carefully separate the system under test and driver, in order to correctly represent the open world model of typical stream processing deployments and can, therefore, measure system performance under realistic conditions. Third, we build the first benchmarking framework to define and test the sustainable performance of streaming systems. Our detailed evaluation highlights the individual characteristics and use-cases of each system."
pub.1105914886,A Data Stream Processing Optimisation Framework for Edge Computing Applications,"Data Stream Processing (DSP) is a widely used programming paradigm to process an unbounded event stream. Often, DSP frameworks are deployed on the cloud with a scalable resource model. One of the key requirements of DSP is to produce results with low latency. With the emergence of IoT, many event sources have been located outside the cloud which can result in higher end-to-end latency due to communication overhead. However, due to the abundance of resources at the IoT layer, Edge computing has emerged as a viable computational paradigm. In this paper, we devise an optimisation framework, consisting of a constraint satisfaction formulation and a system model, that aims to minimise end-to-end latency through appropriate placement of DSP operators either on cloud nodes or edge devices, i.e. deployed in an edge-cloud integrated environment. We test our optimisation framework using OMNeT++, with realistic topologies and power consumption data, and show that it is capable of achieving ≈ 1.65 times reduction of latency compared to edge-only and cloud-only placements, which in turn also reduces the energy consumption per event by up to ≈ 4% at the edge layer. To the best of our knowledge our optimisation framework is the first of its kind to integrate power, bandwidth and CPU constraints with latency minimisation."
pub.1118999984,Benchmarking Distributed Stream Data Processing Systems,"The need for scalable and efficient stream analysis has led to the
development of many open-source streaming data processing systems (SDPSs) with
highly diverging capabilities and performance characteristics. While first
initiatives try to compare the systems for simple workloads, there is a clear
gap of detailed analyses of the systems' performance characteristics. In this
paper, we propose a framework for benchmarking distributed stream processing
engines. We use our suite to evaluate the performance of three widely used
SDPSs in detail, namely Apache Storm, Apache Spark, and Apache Flink. Our
evaluation focuses in particular on measuring the throughput and latency of
windowed operations, which are the basic type of operations in stream
analytics. For this benchmark, we design workloads based on real-life,
industrial use-cases inspired by the online gaming industry. The contribution
of our work is threefold. First, we give a definition of latency and throughput
for stateful operators. Second, we carefully separate the system under test and
driver, in order to correctly represent the open world model of typical stream
processing deployments and can, therefore, measure system performance under
realistic conditions. Third, we build the first benchmarking framework to
define and test the sustainable performance of streaming systems.
  Our detailed evaluation highlights the individual characteristics and
use-cases of each system."
pub.1008332611,Real-Time Urban Monitoring in Dublin Using Semantic and Stream Technologies,"Several sources of information, from people, systems, things, are already available in most modern cities. Processing these continuous flows of information and capturing insight poses unique technical challenges that span from response time constraints to data heterogeneity, in terms of format and throughput. To tackle these problems, we focus on a novel prototype to ease real-time monitoring and decision-making processes for the City of Dublin with three main original technical aspects: (i) an extension to SPARQL to support efficient querying of heterogeneous streams; (ii) a query execution framework and runtime environment based on IBM InfoSphere Streams, a high-performance, industrial strength, stream processing engine; (iii) a hybrid RDFS reasoner, optimized for our stream processing execution framework. Our approach has been validated with real data collected on the field, as shown in our Dublin City video demonstration. Results indicate that real-time processing of city information streams based on semantic technologies is indeed not only possible, but also efficient, scalable and low-latency."
pub.1011141592,Real-time stream processing for active fire monitoring on Landsat 8 direct reception data,"Abstract. Some remote sensing applications are relatively time insensitive, for others, near-real-time processing (results 30-180 minutes after data reception) offer a viable solution. There are, however, a few applications, such as active wildfire monitoring or ship and airplane detection, where real-time processing and image interpretation offers a distinct advantage. The objective of real-time processing is to provide notifications before the complete satellite pass has been received. This paper presents an automated system for real-time, stream–based processing of data acquired from direct broadcast push-broom sensors for applications that require a high degree of timeliness. Based on this system, a processing chain for active fire monitoring using Landsat 8 live data streams was implemented and evaluated. The real-time processing system, called the FarEarth Observer, is connected to a ground station’s demodulator and uses its live data stream as input. Processing is done on variable size image segments assembled from detector lines of the push broom sensor as they are streamed from the satellite, enabling detection of active fires and sending of notifications within seconds of the satellite passing over the affected area, long before the actual acquisition completes. This approach requires performance optimized techniques for radiometric and geometric correction of the sensor data. Throughput of the processing system is kept well above the 400Mbit/s downlink speed of Landsat 8. A latency of below 10 seconds from sensor line acquisition to anomaly detection and notification is achieved. Analyses of geometric and radiometric accuracy and comparisons in latency to traditional near-real-time systems are also presented."
pub.1003721849,Tick Scheduling: A Deadline Based Optimal Task Scheduling Approach for Real-Time Data Stream Systems,"Most of the current research work on timely streaming data processing focuses on minimizing average tuple latency instead of strict individual tuple latency upper-bound, that is, deadline. In this paper, we propose a novel deadline-scheduling strategy, namely tick scheduling (TS), dealing with applications with specified deadline constraints over high volume, possibly bursting, and continuous data streams. We demonstrate that TS policy, which combines precise batch scheduling plan construction and adaptive batch maintenance mechanism can significantly improve system performance by greatly reducing system overheads and adapting gracefully to the time-varying data arrival-rate. Experimental results show the significant improvements provided by our proposed policy."
pub.1052287726,Borealis-R,"Borealis-R is a replication-based system for both fast and highly-available processing of data streams over wide-area networks. In Borealis-R, multiple operator replicas send outputs to downstream replicas, allowing each replica to use whichever data arrives first. To further reduce latency, replicas run without coordination, possibly processing data in different orders. Despite this flexibility, Borealis-R guarantees that applications always receive the same results as in the non-replicated, failure-free case. In addition, Borealis-R deploys replicas at select network locations to effectively improve performance as well as availability. We demonstrate the strengths of Borealis-R using a live wide-area monitoring application. We show that Borealis-R outperforms previous solutions in terms of latency and that it uses system resources efficiently by carefully deploying and discarding replicas."
pub.1128585887,The Architectural Optimizations of a Low-Complexity and Low-Latency FFT Processor for MIMO-OFDM Communication Systems,"Fast Fourier Transform (FFT) processor is a paramount signal processing component in MIMO-OFDM wireless communication systems. Furthermore, novel applications introduced in 5G incur much more rigorous requirements for FFT designs including complexity, latency, and re-configurability. This paper presents the VLSI architecture design and circuit implementation of a FFT processor that is jointly optimized for low-latency, low-complexity, and configurability. Specifically, the proposed architecture processes two data streams concurrently and supports power-of-two FFT sizes from 64 to 2048 symbols. Moreover, a novel data-processing sequence is presented so that data streams are processed in a time-multiplexing manner and an efficient hardware-sharing architecture can be designed. In addition, a highly efficient I/O reorder mechanism is proposed so that the memory elements are shared between processing stages and the efficiency for utilizing memory components is enhanced. Based on the architectural optimizations, two FFT processors are realized. The ultra-low latency design achieves a latency of 4 μs with 41% area reduction compared to the comparable designs. On the other hand, the ultra-low complexity structure achieves a latency of 25 μs with 24% area reduction compared to the state-of-the-art implementations."
pub.1125460848,Evaluation of Stream Processing Frameworks,"The increasing need for real-time insights in data sparked the development of multiple stream processing frameworks. Several benchmarking studies were conducted in an effort to form guidelines for identifying the most appropriate framework for a use case. In this article, we extend this research and present the results gathered. In addition to Spark Streaming and Flink, we also include the emerging frameworks Structured Streaming and Kafka Streams. We define four workloads with custom parameter tuning. Each of these is optimized for a certain metric or for measuring performance under specific scenarios such as bursty workloads. We analyze the relationship between latency, throughput, and resource consumption and we measure the performance impact of adding different common operations to the pipeline. To ensure correct latency measurements, we use a single Kafka broker. Our results show that the latency disadvantages of using a micro-batch system are most apparent for stateless operations. With more complex pipelines, customized implementations can give event-driven frameworks a large latency advantage. Due to its micro-batch architecture, Structured Streaming can handle very high throughput at the cost of high latency. Under tight latency SLAs, Flink sustains the highest throughput. Additionally, Flink shows the least performance degradation when confronted with periodic bursts of data. When a burst of data needs to be processed right after startup, however, micro-batch systems catch up faster while event-driven systems output the first events sooner."
pub.1044011997,How to Win a Hot Dog Eating Contest,"In the quest for valuable information, modern big data applications continuously monitor streams of data. These applications demand low latency stream processing even when faced with high volume and velocity of incoming changes and the user's desire to ask complex queries. In this paper, we study low-latency incremental computation of complex SQL queries in both local and distributed streaming environments. We develop a technique for the efficient incrementalization of queries with nested aggregates for batch updates. We identify the cases in which batch processing can boost the performance of incremental view maintenance but also demonstrate that tuple-at-a-time processing often can achieve better performance in local mode. Batch updates are essential for enabling distributed incremental view maintenance and amortizing the cost of network communication and synchronization. We show how to derive incremental programs optimized for running on large-scale processing platforms. Our implementation of distributed incremental view maintenance can process tens of million of tuples with few-second latency using hundreds of nodes."
pub.1150640915,SWAN,"Wide-area stream analytics is commonly being used to extract operational or business insights from the data issued from multiple distant datacenters. However, timely processing of such data streams is challenging because wide-area network (WAN) bandwidth is scarce and varies widely across both different geo-locations (i.e., spatially) and points of time (i.e., temporally). Stream analytics desirable under a WAN setup requires the consideration of path diversity and the associated bandwidth from data source to sink when performing operator task placement for the query execution plan. It also has to enable fast adaptation to dynamic resource conditions, e.g., changes in network bandwidth, to keep the query execution stable. We present SWAN, a WAN stream analytics engine that incorporates two key techniques to meet the aforementioned requirements. First, SWAN provides a fast heuristic model that captures WAN characteristics at runtime and evenly distributes tasks to nodes while maximizing the network bandwidth for intermediate data. Second, SWAN exploits a stream relaying operator (or RO) to extend a query plan for better facilitating path diversity. This is driven by our observation that oftentimes, a longer path with more communication hops provides higher bandwidth to reach the data sink than a shorter path, allowing us to trade-off query latency for higher query throughput. SWAN stretches a given query plan by adding ROs at compile time to opportunistically place it over such a longer path. In practice, throughput gains do not necessarily lead to significant latency increases, due to higher network bandwidth providing more in-flight data transfers. Our prototype improves the latency and the throughput of stream analytics performances by 77.6% and 5.64X, respectively, compared to existing approaches, and performs query adaptations within seconds."
pub.1092910784,Lever,"With the vast involvement of streaming big data in many applications (e.g., stock market data, sensor data, social network data, etc.), quickly mining and analyzing such data is becoming more and more important. To provide fault tolerance and efficient stream processing at scale, recent stream processing frameworks have proposed to adapt batch processing systems, such as MapReduce and Spark, to handle streaming data by putting the streams into micro-batches and treating the workloads as a continuous series of small jobs [1]. The fundamental challenge of building a batched stream processing system is to minimize the processing latency of each micro-batch. In this paper, we focus on the straggler problem, where a subset of workers are straggling behind and significantly affecting the job completion time. The straggler problem is a well-known critical problem in parallel processing systems. In comparing to large batch processing, the straggler problems in micro-batch processing are more severe and harder to tackle. We argue that the problem of using the existing straggler mitigation solutions for micro-batch processing is that they detect (or predict) stragglers and re-schedule stragglers too late in the data handling pipeline. The re-scheduling actions are carried out during the task execution period, hence it would inevitably increase the processing time of the micro-batches. Furthermore, as the data have already been dispatched, re-scheduling would inherently incur expensive data relocation. Such overhead would become significant in micro-batch processing due to the short processing time of each micro-batch. We refer to this type of methods as post-scheduling techniques. To address the problem, we propose a new pre-scheduling framework, called Lever, which predicts stragglers and makes timely scheduling decisions to minimize the processing latency. As shown in Figure 1, Lever periodically collects and analyzes the historical job profiles of the recurring micro-batch jobs. Based on such information, Lever pre-schedules the data through three main steps, i.e. identify potential stragglers, evaluate node capacity and choose suitable helpers. More importantly, Lever makes the re-scheduling decisions before the batching module dispatches the data. As the scheduling is done while the data are being batched, it would not increase the processing time of the micro-batch. We implemented Lever in Spark Streaming, which has been contributed to the open source community as an extension of Apache Spark Streaming. To the best of our knowledge, this is the first work specifically addressing the straggler problem in continuous micro-batch processing. We conduct various experiments to validate the effectiveness of Lever. The experimental results demonstrate that Lever reduces job completion time by 30.72% to 42.19% and outperforms traditional techniques significantly."
pub.1093406474,Capital Market Surveillance using Stream Processing,"Exchanges and regulators need effective tools for surveillance and monitoring for timely detection and prevention of fraudulent activities such as market manipulation, price rigging and insider trading. In this paper we describe how Complex Event Processing (CEP) technology can be used in real time detection of potential fraudulent activity. We introduce the general concepts of CEP and how it can be used in Banking and Financial Markets for real time fraud detection. Finally we demonstrate how event stream processing using a data stream management system (DSMS) is superior to a traditional solution designed using a DBMS. We present results obtained after from using a commercial event stream processing system (IBM's InfoSphere Streams platform) for certain typical low latency fraud detection scenarios."
pub.1093402515,RTA: Real Time Actionable Events Detection as a Service,"Nowadays vast amounts of data are being produced in continuous ways. They may come from sensors, smart meters, application logs, monitoring software etc. The data need to be processed in realtime to gain actionable insights. Services like smart grid load balancing, cloud platform maintenance, can be carried out in an efficient way. Stream processing is the programming paradigm that answers such demand. When talking about stream processing, we can easily recall several famous open-source software frameworks such as Spark Streaming, Samza, Flink and Storm. Although they provide distributed, robust, low-latency stream processing engines, it's still difficult for an end user to set up a usable stream processing application from scratch. Firstly, users are required to write code to define their business related stream processing logic. Secondly, the submission and update of the stream processing logic require service restart, therefore it may lead to service unavailability for minutes. Thirdly, extra operation effort are required for handling scaling and failover issues. In this paper, we present RTA, a released research service on realtime data processing. The RTA service fills the gap between the stream processing requester and the existing software stacks. It offers a SQL-like stream query language for defining stream processing logic definition over streaming data. It allows users easily define their stream processing logic without programming. In RTA service, stream processing logic is also treated as a type of input, which enables online logic update without service downtime. The RTA service also provides scalability, high availability and resource isolation for serving multiple tenants. In this paper, we also provide a comprehensive evaluation of our service through a case study."
pub.1131901677,Rethinking Operators Placement of Stream Data Application in the Edge,"Maximum Sustainable Throughput (MST) refers to the amount of data that a Data Stream Processing (DSP) system can ingest while keeping stable performance. It has been acknowledged as an accurate metric to evaluate the performance of stream data processing. Yet, existing operators placements continue to focus on latency and throughput, not MST, as main performance objective when deploying stream data applications in the Edge. In this paper, we argue that MST should be used as an optimization objective when placing operators. This is specially important in the Edge, where network bandwidth and data streams are highly dynamic. We demonstrate that through the design and evaluation of a MST-driven operators placement (based on constraint programming) for stream data applications. Through simulations, we show how existing placement strategies that target overall communications reduction often fail to keep up with the rate of data streams. Importantly, the constraint programming-based operators placement is able to sustain up to 5x increased data ingestion compared to baseline strategies."
pub.1001641215,Scalable stateful stream processing for smart grids,"We describe a solution to the ACM DEBS Grand Challenge 2014, which evaluates event-based systems for smart grid analytics. Our solution follows the paradigm of stateful data stream processing and is implemented on top of the SEEP stream processing platform. It achieves high scalability by massive data-parallel processing and the option of performing semantic load-shedding. In addition, our solution is fault-tolerant, ensuring that the large processing state of stream operators is not lost after failure. Our experimental results show that our solution processes 1 month worth of data for 40 houses in 4 hours. When we scale out the system, the time reduces linearly to 30 minutes before the system bottlenecks at the data source. We then apply semantic load-shedding, maintaining a low median prediction error and reducing the time further to 17 minutes. The system achieves these results with median latencies below 30 ms and a 90th percentile below 50 ms."
pub.1127978260,Duality-Based Locality-Aware Stream Partitioning in Distributed Stream Processing Engines,"In this paper, we propose duality-based locality-aware stream partitioning (LSP) in distributed stream processing engines (DSPEs). In general, LSP directly uses the locality concept of distributed batch processing engines (DBPEs). This concept does not fully take into account the characteristics of DSPEs and therefore does not maximize cluster resource utilization. To solve this problem, we first explain the limitations of existing LSP, and we then propose a duality relationship between DBPEs and DSPEs. We finally propose a simple but efficient ping-based mechanism to maximize the locality of DSPEs based on the duality. The insights uncovered in this paper can maximize the throughput and minimize the latency in stream partitioning."
pub.1154383384,Scalable Containerized Pipeline for Real-time Big Data Analytics,"With the widespread usage of IoT, processing data streams in real-time have become very important. The traditional data-stream processing systems are inefficient in processing big data for detecting anomalies, classifications, clustering, and prediction in real-time using minimal resources. In this paper, we address this limitation by proposing a scalable pipeline for real-time processing of big data streams. Our proposed solution is capable of dynamically managing resources for different components of the pipeline using automatic scaling. The pipeline is containerized and deployed on a Kubernetes cluster. The proposed scalable pipeline is evaluated using a case study of anomaly detection in IoT data. The proposed solution yields a $\times 1.31$ to $\times 2.4$ increase in throughput, and $\times 32$ to $\times 80$ decreased latency compared to the commonly used static resource allocation strategy for data pipelines."
pub.1131999483,An Optimal Model for Optimizing the Placement and Parallelism of Data Stream Processing Applications on Cloud-Edge Computing,"The Internet of Things has enabled many application scenarios where a large number of connected devices generate unbounded streams of data, often processed by data stream processing frameworks deployed in the cloud. Edge computing enables offloading processing from the cloud and placing it close to where the data is generated, thereby reducing the time to process data events and deployment costs. However, edge resources are more computationally constrained than their cloud counterparts, raising two interrelated issues, namely deciding on the parallelism of processing tasks (a.k.a. operators) and their mapping onto available resources. In this work, we formulate the scenario of operator placement and parallelism as an optimal mixed-integer linear programming problem. The proposed model is termed as Cloud-Edge data Stream Placement (CESP). Experimental results using discrete-event simulation demonstrate that CESP can achieve an end-to-end latency at least ≃ 80% and monetary costs at least ≃ 30% better than traditional cloud deployment."
pub.1107852147,Complex Event Processing under Constrained Resources by State-Based Load Shedding,"Complex event processing (CEP) systems evaluate queries over event streams for low-latency detection of user-specified event patterns. They need to process streams of growing volume and velocity, while the heterogeneity of event sources yields unpredictable input rates. Against this background, models and algorithms for the optimisation of CEP systems have been proposed in the literature. However, when input rates grow by orders of magnitude during short peak times, exhaustive realtime processing of event streams becomes infeasible. CEP systems shall therefore resort to best-effort query evaluation, which maximises the accuracy of pattern detection while staying within a predefined latency bound. For traditional data stream processing, this is achieved by load shedding that drops some input data without processing it, guided by the estimated importance of particular data entities for the processing accuracy. In this work, we argue that such input-based load shedding is not suited for CEP queries in all situations. Unlike for relational stream processing, where the impact of shedding is assessed based on the operator selectivity, the importance of an event for a CEP query is highly dynamic and largely depends on the state of query processing. Depending on the presence of particular partial matches, the impact of dropping a single event can vary drastically. Hence, this PhD project is devoted to state-based load shedding that, instead of dropping input events, discards partial matches to realise best-effort processing under constrained resources. In this paper, we describe the addressed problem in detail, sketch our envisioned solution for state-based load shedding, and present preliminary experimental results that indicate the general feasibility of our approach."
pub.1107278404,Large Scale Stream Analytics using a Resource-constrained Edge,"A key challenge for smart city analytics is fast extraction, accumulation and processing of sensor data collected from a large number of loT devices. Edge computing has enabled processing of simple analytics, such as aggregation, geographically closer to the IoT devices to improve latency. However, the throughput of processing in the edge depends on the type of resources available, the number of IoT devices connected and the type of stream analytics performed in the edge. We introduce a framework called Seagull for building efficient, large scale IoT-based applications. Our framework distributes the stream analytics processing tasks to the nodes based on their proximity to the sensor data source as well as the amount of processing the nodes can handle. Our evaluation shows the effect of various stream analytics parameters on the maximum sustainable throughput for a resource-constrained edge device."
pub.1148180912,Software Defined Ultra-low Latency Video-Over-IP System with Compression,"Traditional Video-over-IP implementations in software either result in higher latency due to the processing time required for compression, or in high bandwidth required when transmitted as an uncompressed stream. With UHD-1 and UHD-2 video, this is even more of a challenge, as the uncompressed stream requires high-performance Ethernet networks or dedicated hardware implementing the compression. In some cases however, a software implementation and standard COTS equipment are beneficial to allow higher flexibility. With JPEG XS, a mezzanine compression codec was developed that can also be implemented as an ultra-low latency system in software. However, special attention must be paid how the data for processing is distributed across multiple threads, how large the number of threads is, in order to achieve optimal latency. Using a case study for UHD-1 Video-over-IP, this paper explains how such a system can be implemented with COTS components, which software architecture is necessary and how far the latency can be reduced."
pub.1118544730,Fault Tolerance for Stream Processing Engines,"Distributed Stream Processing Engines (DSPEs) target applications related to
continuous computation, online machine learning and real-time query processing.
DSPEs operate on high volume of data by applying lightweight operations on
real-time and continuous streams. Such systems require clusters of hundreds of
machine for their deployment. Streaming applications come with various
requirements, i.e., low-latency, high throughput, scalability and high
availability. In this survey, we study the fault tolerance problem for DSPEs.
We discuss fault tolerance techniques that are used in modern stream processing
engines that are Storm, S4, Samza, SparkStreaming and MillWheel. Further, we
give insight on fault tolerance approaches that we categorize as active
replication, passive replication and upstream backup. Finally, we discuss
implications of the fault tolerance techniques for different streaming
application requirements."
pub.1093737023,Adaptive Scheduling of Parallel Jobs in Spark Streaming,"Streaming data analytics has become increasingly vital in many applications such as dynamic content delivery (e.g., advertisements), Twitter sentiment analysis, and security event processing (e.g., intrusion detection systems, and spam filters). Emerging stream processing systems, such as Spark Streaming, treat the continuous stream as a series of micro-batches of data and continuously process these micro-batch jobs. Such micro-batch based stream processing provides several advantages over traditional stream processing systems, which process streaming data one record at a time, including fast recovery from failures, better load balancing and scalability. However, efficient scheduling of micro-batch jobs to achieve high throughput and low latency is very challenging due to the complex data dependency and dynamism inherent in streaming workloads. In this paper, we propose A-scheduler, an adaptive scheduling approach that dynamically schedules parallel micro-batch jobs in Spark Streaming and automatically adjusts scheduling parameters to improve performance and resource efficiency. Specifically, A-scheduler dynamically schedules multiple jobs concurrently using different policies based on their data dependencies and automatically adjusts the level of job parallelism and resource shares among jobs based on workload properties. We implemented A-scheduler and evaluated it with a real-time security event processing workload. Our experimental results show that A-scheduler can reduce end-to-end latency by 42% and improve workload throughput and energy efficiency by 21% and 13%, respectively, compared to the default Spark Streaming scheduler."
pub.1124430387,Enhancing the Processing of Healthcare Data Streams using Fog Computing,"Healthcare data streams originate from various sensors and Internet of Things (IoT) devices deployed in medical equipment and healthcare facilities as well as worn by patients. These vast volumes of data need to be leveraged to improve patient care, optimize processes, and help health sector stakeholders and applications make faster and more informed decisions. Many healthcare applications use the power of the cloud for data processing. However, time-sensitive healthcare applications cannot tolerate sending data streams to the cloud for processing due to unacceptable high latency and network bandwidth requirements. Healthcare facilities and caregivers need the ability to efficiently stream data and process data streams in real-time at the edge. This paper describes a five-tier architecture that aims to deal with the streaming and processing of data generated by the various devices and equipment of healthcare facilities and systems to enable the creation of smart healthcare applications. The architecture is based on emerging and established technologies, including IoT, edge/fog computing, data integration techniques, cloud computing, and data analytics. The proposed architecture will facilitate the creation of healthcare applications for real-time event detection, notification of alerts, and building monitoring dashboards. Fog node components include an advanced and widely recognized distributed messaging system, Apache Kafka, and the popular stream processing engine, Apache Storm, capable of processing large amounts of data."
pub.1017839617,A holistic approach to build real-time stream processing system with GPU,"Stream processing needs to process huge volume of data with strict deadline requirements. These applications generally consume large amount of network bandwidth and involve compute-intensive operations. Accelerating such operations with general purpose GPU has drawn a lot of attention from both academia and industry. However, GPU has not been applied to real-time stream processing due to its programming paradigm and unpredictable latency.In this paper, we study the problem of applying GPU to real-time processing and propose a holistic approach for building real-time stream processing system with GPU. Based on the proposed techniques, we build a GPU-accelerated SRTP reverse proxy that achieves more than 10Gbps overall throughput and guarantees low latency. Our work demonstrates that using GPU in high-speed real-time stream processing is both feasible and attractive."
pub.1094769573,Adaptive Disorder Control in Continuous Data Streams,"A disorder control is the key factor regarding accuracy and latency of query results when processing sliding window aggregates over continuous data streams. Many stream systems maintain buffers or leverage punctuations for the control. However, current systems suffer from the lack of adaptivity in the measure for estimating buffer sizes or punctuations, which may lead to inaccurate or delayed query results. To address this problem, we propose a probabilistic approach to using an adaptive measure derived from the distributions of tuple generation intervals and network latencies. In our approach, the measure estimates buffer sizes or punctuations according to a drop ratio, which denotes a percentage of tuple drops permissible in run-time processing. The drop ratio can be defined declaratively in a query specification and it provides a way for users to control the tuple drops as their intention. The experimental results show that our adaptive measure estimates more appropriate buffer sizes than ad hoc measures, which means that the proposed measure provides a lower latency, while retaining accuracy by satisfying the given drop ratio."
pub.1151935147,RMLStreamer-SISO: An RDF Stream Generator from Streaming Heterogeneous Data,"Stream-reasoning query languages such as CQELS and C-SPARQL enable query answering over RDF streams. Unfortunately, there currently is a lack of efficient RDF stream generators to feed RDF stream reasoners. State-of-the-art RDF stream generators are limited with regard to the velocity and volume of streaming data they can handle. To efficiently generate RDF streams in a scalable way, we extended the RMLStreamer to also generate RDF streams from dynamic heterogeneous data streams. This paper introduces a scalable solution that relies on a dynamic window approach to generate RDF streams with low latency and high throughput from multiple heterogeneous data streams. Our evaluation shows that our solution outperforms the state-of-the-art by achieving millisecond latency (compared to seconds that state-of-the-art solutions need), constant memory usage for all workloads, and sustainable throughput of around 70,000 records/s (compared to 10,000 records/s that state-of-the-art solutions take). This opens up the access to numerous data streams for integration with the semantic web.Resource type: SoftwareLicense: MIT LicenseURL: https://github.com/RMLio/RMLStreamer/releases/tag/v2.3.0"
pub.1093761072,Investigating Metrics to Build a Benchmark Tool for Complex Event Processing Systems,"Despite companies' demand for data streams processing systems to handle large volumes of flowing data, we did not find many software to assess these sort of systems. In fact, up to date, there are few papers proposing metrics to evaluate these systems or describing software for benchmarks. Most of the papers focus on metrics such as throughput, latency and memory consumption. However, there are other metrics, which system administrators and users should consider, such as information latency, the correctness of results, adaptability on different workloads and others. Therefore, in this paper, we summarized some key metrics used to assess systems for processing online data streams. In addition, we discuss three benchmark tools found in the literature to assess this type of system. At the end of this paper, we propose a new benchmark tool for complex event processing distributed systems called B2-4CEP, which incorporate themetrics described in this paper."
pub.1095472229,A self-recovery technique for highly-available stream processing over local area networks,"We present a replication-based and self-recovery-based approach, replica backup, that realizes both continuous and highly-available data stream processing over local area networks. In our approach, we use process-pairs mechanism in which peer operators run in parallel and independently so that each downstream operator can use whichever data arrives first. To further realize continuously stable communication among operators and improve the robustness of system, we devise automatical recovery mechanism that overcomes the limitation of one-off recovery mechanism. In this paper, we first outline the basic design and framework associating with our self-recovery technique. Next, we develop central leader election algorithm (CLEA) that can choose a new operator according with the placement of candidates. This operator placement algorithm that directly measures the latency among operators aims to balance the cost of data stream processing and latency guarantee. Finally, we compare our replica backup method with previous high-available technique through experiments on network simulator ns-3 to demonstrate the utility of our work."
pub.1152122918,Scalable Deployments for Real-Time AI Video Stream Processing,"Real-time stream processing is becoming more prevalent today due to huge chunks of data needing to be processed upon arrival. In video streaming the need for real-time management is both important and challenging because video frames come at high frequency. AI advances have made it possible to understand video feeds at a high level in real-time, making them a valuable source of information in regards to human behavior, trends, surveillance and much more. As a consequence of these reasons, there is a need for a highly performant and deployable system in terms of latency, scalability, accuracy of results and computing power, which leverages the cloud as a service. A design for a low-latency and highly scalable system is much needed as video stream processing or video analytics has uses in areas such as surveillance, real-time video analytics, criminality and autonomous vehicles, which require fast and accurate analysis of data. Such a system should be able to employ not only streaming but also different processing actions including machine learning models which can be applied to frames resulting in data analysis or further data pipelines. We developed a scalable real-time video processing system which consumes video frames, processes them using deep learning models, renders them and stores the resulting semantic information in a database for further downstream processing. We show that our proposed processing architectures are a suitable solution for modern video analytics systems, which can be scaled both vertically and horizontally, and achieves real-time latency within maximum of 1 second for frame rates ranging from 10 to 60 fps."
pub.1000513979,Feasibility analysis of AsterixDB and Spark streaming with Cassandra for stream-based processing,"For getting up-to-date insight into online services, extracted data has to be processed in near real time. For example, major big data companies (Facebook, LinkedIn, Twitter) analyse streaming data for development of new services. Several technologies have been developed, which could be selected for implementation of stream processing functionalities. The contribution of this paper is feasibility analysis of technologies for stream-based processing of semi-structured data. Particularly, feasibility of a Big Data management system for semi-structured data (AsterixDB) will be compared to Spark streaming, which has been integrated with Cassandra NoSQL database for persistence. The study focuses on stream processing in a simulated social media use case (tweet analysis), which has been implemented to Eucalyptus cloud computing environment on a distributed shared memory multiprocessor platform. The results indicate that AsterixDB is able to provide significantly better performance both in terms of throughput and latency, when data feed functionality of AsterixDB is used, and stream processing has been implemented with Java. AsterixDB also scaled on the same level or better, when the amount of nodes on the cloud platform was increased. However, stream processing in AsterixDB was delayed by batching of data, when tweets were streamed into the database with data feeds."
pub.1067367718,Complex event detection at wire speed with FPGAs,"
                    Complex event detection is an advanced form of data stream processing where the stream(s) are scrutinized to identify given event patterns. The challenge for many
                    complex event processing
                    (CEP) systems is to be able to evaluate event patterns on high-volume data streams while adhering to real-time constraints. To solve this problem, in this paper we present a hardware-based complex event detection system implemented on
                    field-programmable gate arrays
                    (FPGAs). By inserting the FPGA directly into the data path between the network interface and the CPU, our solution can detect complex events at gigabit wire speed with constant and fully predictable latency, independently of network load, packet size, or data distribution. This is a significant improvement over CPU-based systems and an architectural approach that opens up interesting opportunities for hybrid stream engines that combine the flexibility of the CPU with the parallelism and processing power of FPGAs.
                  "
pub.1118826231,Strider: A Hybrid Adaptive Distributed RDF Stream Processing Engine,"Real-time processing of data streams emanating from sensors is becoming a
common task in Internet of Things scenarios. The key implementation goal
consists in efficiently handling massive incoming data streams and supporting
advanced data analytics services like anomaly detection. In an on-going,
industrial project, we found out that a 24/7 available stream processing engine
usually faces dynamically changing data and workload characteristics. These
changes impact the engine's performance and reliability. We propose Strider, a
hybrid adaptive distributed RDF Stream Processing engine that optimizes logical
query plan according to the state of data streams. Strider has been designed to
guarantee important industrial properties such as scalability, high
availability, fault-tolerant, high throughput and acceptable latency. These
guarantees are obtained by designing the engine's architecture with
state-of-the-art Apache components such as Spark and Kafka. We highlight the
efficiency (e.g., on a single machine machine, up to 60x gain on throughput
compared to state-of-the-art systems, a throughput of 3.1 million
triples/second on a 9 machines cluster, a major breakthrough in this system's
category) of Strider on real-world and synthetic data sets."
pub.1148913575,Enabling Stateful Functions for Stream Processing in the Programmable Data Plane,"Sensor-rich environments are crucial components of the Internet of Things ecosystem and benefit from real-time applications. Many applications perform real-time analytics on these IoT workloads by performing continuous stream processing for a window of sequence data elements. However, executing light-weight stateful functions on server CPUs adds to the communication latency of each small message in a high data rate environment, primarily due to messages traveling through a complex network stack to reach the CPU. Thus, we present an in-network function deployment architecture with low latency and low resource footprint by introducing a new compute layer. We propose an FPGA-based Switch/NIC prototype with a compute layer utilizing RISC-V soft cores and High-Level Synthesis modules. We evaluate the design for two microbenchmarks on a Zynq 7000 FPGA each, achieving less than 10 μs in latency and consuming less than 6 % of resources."
pub.1034367502,Grand challenge,"MapReduce is a popular scalable processing framework for large-scale data. In this paper, we first briefly present our efforts on rectifying the traditional batch-oriented MapReduce framework for low-latency data stream processing. We investigated how to utilize such a MapReduce-style platform for fast sensor data processing by taking the DEBS Grand Challenge 2013 as an example. Both the analysis and experiments verify that our approach can obtain highly scalable solutions."
pub.1151650156,Resource Configuration Tuning for Stream Data Processing Systems via Bayesian Optimization,"Stream data processing systems are becoming increasingly popular in the big data era. Systems such as Apache Flink typically provide a number (e.g., 30) of configuration parameters to flexibly specify the amount of resources (e.g., CPU cores and memory) allocated for tasks. These parameters significantly affect task performance. However, it is hard to manually tune them for optimal performance for an unknown program running on a given cluster. An automatic as well as fast resource configuration tuning approach is therefore desired. To this end, we propose to leverage Bayesian optimization to automatically tune the resource configurations for stream data processing systems. We first select a machine learning model—Random Forest—to construct accurate performance models for a stream data processing program. We subsequently take the Bayesian optimization (BO) algorithm, along with the performance models, to iteratively search the optimal configurations for a stream data processing program. Experimental results show that our approach improves the 99th-percentile tail latency by a factor of 2.62× on average and up to 5.26× overall. Furthermore, our approach improves throughput by a factor of 1.05× on average and up to 1.21× overall."
pub.1119370056,A quality model for evaluating and choosing a stream processing framework architecture,"Today, we have to deal with many data (Big data) and we need to make
decisions by choosing an architectural framework to analyze these data coming
from different area. Due to this, it become problematic when we want to process
these data, and even more, when it is continuous data. When you want to process
some data, you have to first receive it, store it, and then query it. This is
what we call Batch Processing. It works well when you process big amount of
data, but it finds its limits when you want to get fast (or real-time)
processing results, such as financial trades, sensors, user session activity,
etc. The solution to this problem is stream processing. Stream processing
approach consists of data arriving record by record and rather than storing it,
the processing should be done directly. Therefore, direct results are needed
with a latency that may vary in real-time.
  In this paper, we propose an assessment quality model to evaluate and choose
stream processing frameworks. We describe briefly different architectural
frameworks such as Kafka, Spark Streaming and Flink that address the stream
processing. Using our quality model, we present a decision tree to support
engineers to choose a framework following the quality aspects. Finally, we
evaluate our model doing a case study to Twitter and Netflix streaming."
pub.1044052837,RPV-II: A Stream-Based Real-Time Parallel Vision System and Its Application to Real-Time Volume Reconstruction,"In this paper, we present RPV-II, a stream-based real-time parallel image processing environment on distributed parallel computers, or PC-cluster, and its performance evaluation using a realistic application. The system is based on our previous PC-cluster system for real-time image processing and computer vision, and is designed to overcome the problems of our previous system, one of which is long latency when we use pipelined structures. This becomes a serious problem when we apply the system to interactive applications. To make the latency shorter, we have introduced stream data transfer, or fine grained data transfer, to RPV-II. One frame data is divided into small elements such as pixels, lines and voxels, and we have developed efficient real-time data transfer mechanism of those. Using RPV-II we have developed a real-time volume reconstruction system by visual volume intersection method, and we have measured the system performance. Experimental results show better performance than that of our previous system, RPV."
pub.1106923111,Providing streaming joins as a service at Facebook," Stream processing applications reduce the latency of batch data pipelines and enable engineers to quickly identify production issues. Many times, a service can log data to distinct streams, even if they relate to the same real-world event (e.g., a search on Facebook's search bar). Furthermore, the logging of related events can appear on the server side with different delay, causing one stream to be significantly behind the other in terms of logged event times for a given log entry. To be able to stitch this information together with low latency , we need to be able to join two different streams where each stream may have its own characteristics regarding the degree in which its data is out-of-order . Doing so in a streaming fashion is challenging as a join operator consumes lots of memory, especially with significant data volumes. This paper describes an end-to-end streaming join service that addresses the challenges above through a streaming join operator that uses an adaptive stream synchronization algorithm that is able to handle the different distributions we observe in real-world streams regarding their event times. This synchronization scheme paces the parsing of new data and reduces overall operator memory footprint while still providing high accuracy. We have integrated this into a streaming SQL system and have successfully reduced the latency of several batch pipelines using this approach. "
pub.1083916320,Benchmarking Distributed Stream Processing Platforms for IoT Applications,"Internet of Things (IoT) is a technology paradigm where millions of sensors monitor, and help inform or manage, physical, environmental and human systems in real-time. The inherent closed-loop responsiveness and decision making of IoT applications makes them ideal candidates for using low latency and scalable stream processing platforms. Distributed Stream Processing Systems (DSPS) are becoming essential components of any IoT stack, but the efficacy and performance of contemporary DSPS have not been rigorously studied for IoT data streams and applications. Here, we develop a benchmark suite and performance metrics to evaluate DSPS for streaming IoT applications. The benchmark includes 13 common IoT tasks classified across functional categories and forming micro-benchmarks, and two IoT applications for statistical summarization and predictive analytics that leverage various dataflow patterns of DSPS. These are coupled with stream workloads from real IoT observations on smart cities. We validate the benchmark for the popular Apache Storm DSPS, and present the results."
pub.1095736235,Fast and Highly-Available Stream Processing over Wide Area Networks,"We present a replication-based approach that realizes both fast and highly-available stream processing over wide area networks. In our approach, multiple operator replicas send outputs to each downstream replica so that it can use whichever data arrives first. To further expedite the data flow, replicas run independently, possibly processing data in different orders. Despite this complication, our approach always delivers what non-replicated processing would produce without failures. We call this guarantee replication transparency. In this paper, we first discuss semantic issues for replication transparency and extend stream-processing primitivesaccordingly. Next, we develop an algorithm that manages replicas at geographically dispersed servers. This algorithm strives to achieve the best latency guarantee, relative to the cost of replication. Finally, we substantiate the utility of our work through experiments on PlanetLab servers as well as simulations based on real network traces."
pub.1042492150,Runtime‐aware adaptive scheduling in stream processing,"Summary  Long‐running stream applications usually share the same fundamental computational infrastructure. To improve the efficiency of data processing in stream processing systems, a data analysis operator could be partitioned into n parallel tasks. The partitioned tasks are usually deployed on m nodes coexisting with other application operators. Because the node performance can vary in unpredictable ways (i.e., (1) stream input rates may fluctuate and (2) computational resource availability varies as other applications are affected), the nodes have different processing steps, and the slow node determines the operator performance. Hence, the tasks should be redistributed at runtime for stream applications to meet their strict latency requirements. Our key idea is to redistribute the tasks to the best node dynamically adaptive to resource or load fluctuations. In this paper, we present a runtime‐aware adaptive schedule mechanism that aims at minimizing the operator processing latency and minimizing the latency difference between different nodes' tasks. We propose a new abstraction called performance cost ratio (PCR) that evaluates the node performance. The higher the node's PCR is, the less cost the node will pay for processing one tuple, and the more tasks should be deployed on it. In a scheduling, we first sort tasks descendingly by their loads and sort nodes by their PCR. Then we reassign the amount of computation according to the node's PCR to keep the node's PCR and its input rate the same or in similar proportion in all PCRs. The PCR‐based quantitative algorithm applies itself to make tasks loads quantized to the processing capacity of nodes, move the minimum amount of operator's tasks, and keep the tasks local at the same time. We have implemented a runtime‐aware adaptive scheduler as an extension to Storm and evaluated this strategy. We achieve the optimization goal using less computational resources. Copyright © 2015 John Wiley & Sons, Ltd. "
pub.1119009399,Real-time Text Analytics Pipeline Using Open-source Big Data Tools,"Real-time text processing systems are required in many domains to quickly
identify patterns, trends, sentiments, and insights. Nowadays, social networks,
e-commerce stores, blogs, scientific experiments, and server logs are main
sources generating huge text data. However, to process huge text data in real
time requires building a data processing pipeline. The main challenge in
building such pipeline is to minimize latency to process high-throughput data.
In this paper, we explain and evaluate our proposed real-time text processing
pipeline using open-source big data tools which minimize the latency to process
data streams. Our proposed data processing pipeline is based on Apache Kafka
for data ingestion, Apache Spark for in-memory data processing, Apache
Cassandra for storing processed results, and D3 JavaScript library for
visualization. We evaluate the effectiveness of the proposed pipeline under
varying deployment scenarios to perform sentiment analysis using Twitter
dataset. Our experimental evaluations show less than a minute latency to
process $466,700$ Tweets in $10.7$ minutes when three virtual machines
allocated to the proposed pipeline."
pub.1122234891,Simois: A Scalable Distributed Stream Join System with Skewed Workloads,"Many BigData applications require to perform quick join operations on different large-scale real-time data streams. The key challenge to design an efficient stream join system is how to reasonably partition the streaming data among distributed processing nodes to avoid high density of join computation. However, the skewed distribution of real world streams raises great challenges for streaming data partitioning in distributed stream join systems. Existing hash based partitioning schemes incur significant load imbalance which leads to low system throughput and long processing latency, while shuffling based strategies incur redundant join computation and much more communication. To address this issue, in this paper, we propose and implement a scalable distributed stream join system, Simois, which shuffles the potential top heavy-load keys while hashing the others. However, how to identify the keys which lead to the heavy workload imbalance is challenging, because the heavy workload is determined by the current joint status of two streams, and the distribution of the two streams may change with time. To solve this problem, we design a novel efficient exponential counting scheme for identifying the keys with the heaviest workload in the two dynamic streams. The proposed exponential counting scheme needs extremely low computation and space cost, so that it can be well implemented in a stream processing system. Moreover, we design a popularity decline algorithm to make our design adaptive to the highly dynamic changes of streams. We implement Simois on top of Apache Storm and conduct comprehensive experiments using large-scale real world traces. Experiment results show that Simois improves the system throughput significantly by 52% and reduces the average latency by 37%, compared to existing state-of-the-art designs."
pub.1035807598,Scheduling Strategies and Their Evaluation in a Data Stream Management System,"MavStream, a Data Stream Management System (DSMS), has been developed for processing stream data from applications such as network monitoring, sensor monitoring and traffic management systems that require near-real time results and have to process unbounded streams of data. In order to be useful, a result produced by MavStream has to meet certain Quality of Service (QoS) requirements on tuple latency, memory usage, and throughput. Strategies used for scheduling the operators of continuous query (CQ) significantly affect the QoS metrics and hence are of interest. This paper discusses scheduling strategies used in MavStream, their design, implementation, and evaluation. Scheduling is done in MavStream at the operator level. The scheduler maintains a ready queue of operators and decides on the operators to be scheduled based on the scheduling strategy. We first introduce the path capacity scheduling strategy with the goal of minimizing tuple latency by scheduling operator paths with maximum processing capacity. Later we discuss segment-scheduling strategy that aims at minimization of total memory requirement by scheduling operator segments with maximum memory release capacity. We then discuss simplified segment strategy, which splits operator path into just two segments providing better tuple latency performance than segment scheduling strategy and lower memory utilization than path capacity scheduling strategy. Extensive set of experiments have been designed and performed to evaluate the proposed scheduling strategies by simulating real time streams. The performance metrics of average tuple latency, memory utilization and throughput are compared with each other for different strategies and with round robin strategy to validate the analytical conclusions."
pub.1148777931,A NOVEL TRUE REAL-TIME SPATIOTEMPORAL DATA STREAM PROCESSING FRAMEWORK,"The ability to interpret spatiotemporal data streams in real-time is critical for a range of systems. However, processing vast amounts of spatiotemporal data out of several sources, such as online traffic, social platforms, sensor networks, and other sources, is a considerable challenge. The major goal of this study is to create a framework for processing and analyzing spatiotemporal data from multiple sources with irregular shapes so that researchers can focus on data analysis instead of worrying about the data sources' structure. We introduced a novel spatiotemporal data paradigm for true-real-time stream processing, which enables high-speed and low-latency real-time data processing, with these considerations in mind. A comparison of two state-of-the-art real-time process architectures was offered, as well as a full review of the various open-source technologies for real-time data stream processing, and their system topologies were also presented. Hence, this study proposed a brand-new framework that integrates Apache Kafka for spatiotemporal data ingestion, Apache flink for true real-time processing of spatiotemporal stream data, as well as machine learning for real-time predictions, and Apache Cassandra at the storage layer for distributed storage in real-time. The proposed framework was compared with others from the literature using the following features: Scalability (Sc), prediction tools (PT), data analytics (DA), multiple event types (MET), data storage (DS), Real-time (Rt), and performance evaluation (PE) stream processing (SP), and our proposed framework provided the ability to handle all of this task."
pub.1136429210,The changing face of data analytics from Batch Analytics to Analytics-on-the-fly,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Data Analytics started with Hadoop in 2005 when it made possible to mine large data sets to extract intelligence from it. These were batch jobs that could run for hours. Then a natural evolution happened around 2010 when Apache Spark and Kafka enabled stream processing of data at scale. The stream processing movement reduced the time from data-to-insights to tens of minutes. But some industries demanded lowering query latencies along with lowering data latencies. For example, the Facebook Newsfeed and the Linkedin FollowFeed applications required super-low data latency. It is 2018, enter Analytics-on-the-fly! In this talk, Dhruba walks you through the history and evolution of the original Hadoop architecture that separates compute from storage. He inspects the Lambda Architecture and the reasons why it is popular for Stream Processing systems. Then he goes on to describe the Aggregator-Leaf-Tailer (ALT) architecture that provides Analytics-on-the-fly by allowing fast queries on semi structured data. He peels apart the disaggregated and cloud-friendly nature of ALT that allows one to scale compute, storage, data rates and query volumes independently. He describes how replacing the map-reduce framework in earlier generation Hadoop with an RocksDB-based indexing framework in the ALT architecture reduces query latency. He talks about the CQRS pattern in this architecture that isolates query latencies from bursty steams. He elaborates why these new applications demand higher concurrent queries and how the ATL architecture provides it. The talk concludes with a description of how the ALT architecture is already in production in a set of applications at Facebook, Linkedin and Rockset."
pub.1139498198,Online Staypoint Detection in High-Frequency Location Update Streams,"The increasing availability of indoor and outdoor location sensors raises the interest for understanding the mobility in different places, e.g., in museums, train stations, or cities. One of the highly used approaches for understanding the mobility of individuals is to detect staypoints from trajectories. Although offline detection of staypoints fits best for long-term planning, some applications require online staypoint detection from trajectories, like online recommendation systems. Latency plays an important role here. In this paper, we propose a stream-based approach for staypoint detection, which can be realized by applying a Data Stream Management System. We claim that the proposed approach can detect staypoints with low latency from the high-frequency location update streams. To evaluate our approach, we compare it with a batch-based approach on real data from an indoor tracking system and Geolife dataset. The results demonstrate that the online approach detects staypoints with much lower latency compared to traditional approaches like offline and batch processing. Moreover, we prove that the accuracy of the stream-based approach is similar to the batch-based approach."
pub.1103659698,C-Stream," Stream processing is a computational paradigm for on-the-fly processing of live data. This paradigm lends itself to implementations that can provide high throughput and low latency by taking advantage of various forms of parallelism that are naturally captured by the stream processing model of computation, such as pipeline, task, and data parallelism. In this article, we describe the design and implementation of C-Stream , which is an elastic stream processing engine. C-Stream encompasses three unique properties. First, in contrast to the widely adopted event-based interface for developing streaming operators, C-Stream provides an interface wherein each operator has its own driver loop and relies on data availability application programming interfaces (APIs) to decide when to perform its computations. This self-control-based model significantly simplifies the development of operators that require multiport synchronization. Second, C-Stream contains a dynamic scheduler that manages the multithreaded execution of the operators. The scheduler, which is customizable via plug-ins, enables the execution of the operators as co-routines, using any number of threads. The base scheduler implements back-pressure, provides data availability APIs, and manages preemption and termination handling. Last, C-Stream varies the degree of parallelism to resolve bottlenecks by both dynamically changing the number of threads used to execute an application and adjusting the number of replicas of data-parallel operators. We provide an experimental evaluation of C-Stream. The results show that C-Stream is scalable, highly customizable, and can resolve bottlenecks by dynamically adjusting the level of data parallelism used. "
pub.1165779473,Analyzing C++ Stream Parallelism in Shared-Memory when Porting to Flink and Storm,"Stream processing plays a crucial role in various information-oriented digital systems. Two popular frameworks for real-time data processing, Flink and Storm, provide solutions for effective parallel stream processing in Java. An option to leverage Java's mature ecosystem for distributed stream processing involves porting legacy C++ applications to Java. However, this raises considerations on the adequacy of the equivalent Java mechanisms and potential degradation in throughput. Therefore, our objective is to evaluate programmability and performance when converting stream processing applications from C++ to Java while also exploring the parallelization capabilities offered by Flink and Storm. Furthermore, we aim to assess the throughput of Flink and Storm on shared-memory manycore machines, a hardware architecture commonly found in cloud environments. To achieve this, we conduct experiments involving four different stream processing applications. We highlight challenges encountered when porting C++ to Java and working with Flink and Storm. Furthermore, we discuss throughput, latency, CPU, and memory usage results."
pub.1122315455,"Linearize, predict and place","Many IoT applications found in cyber-physical systems, such as smart grids, must take control actions in response to critical events, such as supply-demand mismatch, which requires low-latency processing of streaming data for rapid event detection and anomaly remediation. These streaming applications generally take the form of directed acyclic graphs (DAGs), where vertices represent operators and edges represent the flow of data between these operators. Edge computing has recently attracted significant attention as a means to readily meet the requirements of latency-critical IoT applications due to its ability to provide low-latency processing near the source of data. To accrue the benefits of edge computing, the constituent operators of these applications must be placed in a manner that intelligently trades-off inter-operator communication costs with the cost of interference incurred due to co-location of operators on the same resource-constrained edge devices. To address these challenges and to substantially simplify the placement problem for DAGs of arbitrary sizes and topologies, we present an algorithm that first transforms any arbitrary stream processing DAG into an approximate set of linear chains. Subsequently, a data-driven latency prediction model for co-located linear chains is used to inform the placement of operators such that the makespan, defined as the maximum latency of all paths in the DAG, is minimized. We empirically evaluate our algorithm using a variety of DAG placement scenarios on a Beagle Bone cluster, which is representative of an edge computing environment."
pub.1141391198,Optimizing checkpoint‐based fault‐tolerance in distributed stream processing systems: Theory to practice,"Abstract Fault‐tolerance is an essential part of a stream processing system that guarantees data analysis could continue even after failures. State‐of‐the‐art distributed stream processing systems use checkpointing to support fault‐tolerance for stateful computations where the state of the computations is periodically persisted. However, the frequency of performing checkpoints impacts the performance (utilization, latency, and throughput) of the system as the checkpointing process consumes resources and time that can be used for actual computations. In practice, systems are often configured to perform checkpoints based on crude values ignoring factors such as checkpoint and restart costs, leading to suboptimal performance. In our previous work, we proposed a theoretical optimal checkpoint interval that maximizes the system utilization for stream processing systems to minimize the impact of checkpointing on system performance. In this article, we investigate the practical benefits of our proposed theoretical optimal by conducting experiments in a real‐world cloud setting using different streaming applications; we use Apache Flink, a well‐known stream processing system for our experiments. The experiment results demonstrate that an optimal interval can achieve better utilization, confirming the practicality of the theoretical model when applied to real‐world applications. We observed utilization improvements from 10% to 200% for a range of failure rates from 0.3 failures per hour to 0.075 failures per minute. Moreover, we explore how performance measures: latency and throughput are affected by the optimal interval. Our observations demonstrate that significant improvements can be achieved using the optimal interval for both latency and throughput."
pub.1093273261,A Comparison of Stream Processing Frameworks,"This study compares the performance of Big Data Stream Processing frameworks including Apache Spark, Flink, and Storm. Also, it measures the resource usage and performance scalability of the frameworks against a varying number of cluster sizes. It has been observed that, Flink outperforms both Spark and Storm under equal constraints. However, Spark can be optimized to provide the higher throughput than Flink with the cost of higher latency."
pub.1143538967,Research and Application of Complex Event Processing Method Based on RDF Stream,"The development of Semantic Web technology has produced massive resource description framework (RDF) data, most of which are collected from the Internet of Things in the form of streams and analyzed in real time. How to reason and query RDF streams, and to mine valuable information from the streams for decision-making has become a hot research topic. Complex event processing (CEP) can perform real-time analysis of event streams, but the processed data usually lacks semantic information, is not suitable for semantic interoperability between multi-source heterogeneous data, and cannot combine data streams with domain ontology. In order to solve the above problems, CEPR, a complex event processing method based on RDF flow, is proposed. CEPR is based on answer set programming ASP, using LARS that extends ASP, combining Datalog and relational algebra, to implement RDF flow inference and query on the Flink platform. Use the PHM 2010 tool wear data set to evaluate the performance of CEPR, and compare CEPR with C-SPARQL and Strider. Experiments show that CEPR has a greater advantage in query latency."
pub.1123750047,Evaluation of IoT stream processing at edge computing layer for semantic data enrichment," The fast development of Internet of Things (IoT) computing and technologies has prompted a decentralization of Cloud-based systems. Indeed, sending all the information from IoT devices directly to the Cloud is not a feasible option for many applications with demanding requirements on real-time response, low latency, energy-aware processing and security. Such decentralization has led in a few years to the proliferation of new computing layers between Cloud and IoT, known as Edge computing layer, which comprises of small computing devices (e.g. Raspberry Pi) to larger computing nodes such as Gateways, Road Side Units, Mini Clouds, MEC Servers, Fog nodes, etc. In this paper, we study the challenges of processing an IoT data stream in an Edge computing layer. By using a real life data stream set arising from a car data stream as well as a real infrastructure using Raspberry Pi and Node-Red server, we highlight the complexities of achieving real time requirements of applications based on IoT stream processing."
pub.1112315033,Scheduling Data in Neural Network Applications,"Neuromorphic computing is becoming common in diverse systems, including time-sensitive, real-time systems. This requires that large data sets be processed quickly with low latency. A stream processing architecture helps achieve this goal by beginning processing as soon as samples are received. In this paper, to achieve a low-latency implementation, neurons are parsed into fine-grain operations and scheduled onto stages of the pipeline. Wide datapaths that process multiple samples in each stage of the pipeline greatly improve throughput and latency. Also, the sparsity of partially connected neural networks allows proposed schedulers to further decrease latency. These schedulers use 'greedy' heuristics based on data dependencies to develop a reduced latency schedule. The schedulers were testing against sets of 1000 partially-connected neural networks, with sets ranging in number of layers from three to fifteen. The set of heuristics developed showed reductions of up to 67% over a sequential schedule."
pub.1123437617,Spatial Query Processing on AIS Data Streams in Data Stream Management Systems,"Spatio-temporal data streams from moving objects have become ubiquitous in the recent years, not only, but also in the maritime domain. The Automatic Identification System (AIS) is an important technology in the maritime domain that creates huge amounts of streaming moving object data and enables new use cases. The data streams can improve the situation awareness, help Vessel Traffic Services (VTSs) to get an overview of certain situations and detect upcoming critical situations automatically. For these use cases, queries have to be processed on data streams with continuous results and little delay.To reach this goal, Data Stream Management Systems (DSMSs) lay a foundation to process data streams, but lack the capabilities for spatio-temporal query processing. We tackle this research gap with techniques known from moving object databases and integrate those into the stream processing. We present a system that integrates the moving object algebra from moving object databases into the interval approach from data stream processing to run queries on AIS data. This new approach allows us to define very diverse spatio-temporal queries on AIS data streams, such as radius queries, k-nearest neighbors (kNN) queries as well as queries with moving polygons. Additionally, the approach allows us to use short-time prediction to detect situations before they occur, e. g., to avoid collisions. Our results show that the system is very flexible, offers a clear semantics and produces results on AIS streams with many vessels with low latency."
pub.1149507816,Knowledge graph stream processing at the edge,"We present a knowledge graph management system designed to run on Edge computing devices that handles high-frequency data streams. During the design phase, we took into account the inherent limitations of the devices, i.e., limited computing power and storage space, as well as the expectations of applications, e.g., low latency, high throughput, and intelligent data management. This results in a compact, decompression-free, in-memory, streaming-enabled RDF store that supports continuous querying and some forms of reasoning. The system addresses efficient query processing of data continuously arriving at a fast pace and is well-adapted to event-driven applications such as anomaly and risk detection. We empirically emphasize its accuracy, robustness, latency, and throughput properties on a real-world IoT setting originating from the energy management domain."
pub.1130897913,IPC: Resource and network cost-aware distributed stream scheduling on skewed streams,"The performance of distributed stream processing engines is significantly compromised when processing stream data with skewed distribution. Current stream partitioning schemes are not able to meet the rigorous requirements of distributed stream processing. We show that network cost is an essential factor for partitioning data, and this factor should be considered when designing a stream partitioning scheme. Additionally, we should efficiently utilize resources in the data partitioning process. Current stream partitioning schemes either use a shuffle grouping approach that efficiently manages workload but faces scalability issues in terms of memory or uses hash-based key grouping schemes that suffer from load balancing issues. We argue that network cost and resource utilization are two crucial factors for stream partitioning schemes. We propose and implement a distributed stream partitioning scheme call IPC that minimizes the network cost and efficiently utilizes resources by leveraging two techniques: process near source and process at local. It also utilizes key splitting and local load estimation techniques to achieve load balancing. We implement the IPC on top of Apache Storm. Experiment results using large scale real-time datasets show that IPC achieves an up to 4.2x improvement in throughput and reduces processing latency by 97% compared to state-of-the-art designs."
pub.1128045903,Prompt: Dynamic Data-Partitioning for Distributed Micro-batch Stream Processing Systems,"Advances in real-world applications require high-throughput processing over large data streams. Micro-batching has been proposed to support the needs of these applications. In micro-batching, the processing and batching of the data are interleaved, where the incoming data tuples are first buffered as data blocks, and then are processed collectively using parallel function constructs (e.g., Map-Reduce). The size of a micro-batch is set to guarantee a certain response-time latency that is to conform to the application's service-level agreement. In contrast to tuple-at-a-time data stream processing, micro-batching has the potential to sustain higher data rates. However, existing micro-batch stream processing systems use basic data-partitioning techniques that do not account for data skew and variable data rates. Load-awareness is necessary to maintain performance and to enhance resource utilization. A new data partitioning scheme termed Prompt is presented that leverages the characteristics of the micro-batch processing model. In the batching phase, a frequency-aware buffering mechanism is introduced that progressively maintains run-time statistics, and provides online key-based sorting as data tuples arrive. Because achieving optimal data partitioning is NP-Hard in this context, a workload-aware greedy algorithm is introduced that partitions the buffered data tuples efficiently for the Map stage. In the processing phase, a load-aware distribution mechanism is presented that balances the size of the input to the Reduce stage without incurring inter-task communication overhead. Moreover, Prompt elastically adapts resource consumption according to workload changes. Experimental results using real and synthetic data sets demonstrate that Prompt is robust against fluctuations in data distribution and arrival rates. Furthermore, Prompt achieves up to 200% improvement in system throughput over state-of-the-art techniques without degradation in latency."
pub.1112757271,SLA-Based Adaptation Schemes in Distributed Stream Processing Engines †,"With the upswing in the volume of data, information online, and magnanimous cloud applications, big data analytics becomes mainstream in the research communities in the industry as well as in the scholarly world. This prompted the emergence and development of real-time distributed stream processing frameworks, such as Flink, Storm, Spark, and Samza. These frameworks endorse complex queries on streaming data to be distributed across multiple worker nodes in a cluster. Few of these stream processing frameworks provides fundamental support for controlling the latency and throughput of the system as well as the correctness of the results. However, none has the ability to handle them on the fly at runtime. We present a well-informed and efficient adaptive watermarking and dynamic buffering timeout mechanism for the distributed streaming frameworks. It is designed to increase the overall throughput of the system by making the watermarks adaptive towards the stream of incoming workload, and scale the buffering timeout dynamically for each task tracker on the fly while maintaining the Service Level Agreement (SLA)-based end-to-end latency of the system. This work focuses on tuning the parameters of the system (such as window correctness, buffering timeout, and so on) based on the prediction of incoming workloads and assesses whether a given workload will breach an SLA using output metrics including latency, throughput, and correctness of both intermediate and final results. We used Apache Flink as our testbed distributed processing engine for this work. However, the proposed mechanism can be applied to other streaming frameworks as well. Our results on the testbed model indicate that the proposed system outperforms the status quo of stream processing. With the inclusion of learning models like naïve Bayes, multilayer perceptron (MLP), and sequential minimal optimization (SMO)., the system shows more progress in terms of keeping the SLA intact as well as quality of service (QoS)."
pub.1023769214,Database support for processing complex aggregate queries over data streams,"Over the last few years, the increasing demand on processing streaming data with high throughput and low latency has led to the development of specialized stream processing engines (SPE). Although existing SPEs show high performance in evaluating stateless operations and stateful operations with small windows, their performance degrades significantly when calculating exact answers for complex aggregate queries with huge windows. Examples include correlated aggregations, quantile and ordering statistic computation. Meanwhile, modern database systems have demonstrated the ability of processing complex analytical tasks efficiently over very large datasets, using technologies such as vertical storage, vectorized query execution, etc. This suggests the feasibility of leveraging database systems to assist SPEs to process complex aggregate queries to reduce their evaluation latency. The goal of this thesis is to investigate the potential of combining database systems with SPEs in the context of stream processing so as to improve the overall query evaluation performance. To this end, the following two major topics will be addressed in this thesis: (1) dynamic migration of complex aggregate operations between the SPE and the database in response to varying system load and (2) efficient evaluation of continuous queries over streaming data that is migrated to the database."
pub.1093747188,"Scalejoin: A deterministic, disjoint-parallel and skew-resilient stream join","The inherently large and varying volumes of data generated to facilitate autonomous functionality in large scale cyber-physical systems demand near real-time processing of data streams, often as close to the sensing devices as possible. In this context, data streaming is imperative for dataintensive processing infrastructures. Stream joins, the streaming counterpart of database joins, compare tuples coming from different streams and constitute one of the most important and expensive data streaming operators. Dictated by the needs of big data streaming analytics, algorithmic implementations of stream joins have to be capable of efficiently processing bursty and rate-varying data streams in a deterministic and skew-resilient fashion. To leverage the design of modern multicore architectures, scalability and parallelism need to be addressed also in the algorithmic design. In this paper we present Scalejoin, an algorithmic construction for deterministic and parallel stream joins that guarantees all the above properties, thus filling in a gap in the existing state-of-the art. Key to the novelty of Scalejoin is a new data structure, Scalegate, and its lock-free implementation. ScaleGate facilitates concurrent data exchange and balances independent actions among processing threads; it also enables fine-grain parallelism while providing the necessary synchronization for deterministic processing. As a result, it allows Scalejoin to run on an arbitrary number of processing threads that can evenly share the overall comparisons run in parallel and achieve high processing throughput and low processing latency. As we show, Scalejoin not only guarantees deterministic, disjoint and skew-resilient parallelism, but also achieves higher throughput than state-of-the-art parallel stream joins."
pub.1170726188,Evaluation of Adaptive Micro-batching Techniques for GPU-Accelerated Stream Processing,"Stream processing plays a vital role in applications that require continuous, low-latency data processing. Thanks to their extensive parallel processing capabilities and relatively low cost, GPUs are well-suited to scenarios where such applications require substantial computational resources. However, micro-batching becomes essential for efficient GPU computation within stream processing systems. However, finding appropriate batch sizes to maintain an adequate level of service is often challenging, particularly in cases where applications experience fluctuations in input rate and workload. Addressing this challenge requires adjusting the optimal batch size at runtime. This study proposes a methodology for evaluating different self-adaptive micro-batching strategies in a real-world complex streaming application used as a benchmark."
pub.1090296213,A Scalable Platform for Low-Latency Real-Time Analytics of Streaming Data,"The ability to process high-volume high-speed streaming data from different data sources is critical for modern organizations to gain insights for business decisions. In this research, we present the streaming analytics platform (SDAP), which provides a set of operators to specify the process of stream data transformations and analytics. SDAP adopts a declarative approach to model and design, delivering analytics capabilities through the combination of a set of primitive operators in a simple manner. The model includes a topology to design streaming analytics specifications using a set of atomic data manipulation operators. Our evaluation demonstrates that SDAP is capable of maintaining low-latency while scaling to a cloud of distributed computing nodes, and providing easier process design and execution of streaming analytics."
pub.1148976313,SLedge: Scheduling and Load Balancing for a Stream Processing EDGE Architecture,"Natural disasters have a significant impact on human welfare. In recent years, disasters are more violent and frequent due to climate change, so their impact may be higher if no preemptive measures are taken. In this context, real-time data processing and analysis have shown great potential to support decision-making, rescue, and recovery after a disaster. However, disaster scenarios are challenging due to their highly dynamic nature. In particular, we focus on data traffic and available processing resources. In this work, we propose SLedge—an edge-based processing model that enables mobile devices to support stream processing systems’ tasks under post-disaster scenarios. SLedge relies on a two-level control loop that automatically schedules SPS’s tasks over mobile devices to increase the system’s resilience, reduce latency, and provide accurate outputs. Our results show that SLedge can outperform a cloud-based infrastructure in terms of latency while keeping a low overhead. SLedge processes data up to five times faster than a cloud-based architecture while improving load balancing among processing resources, dealing better with traffic spikes, and reducing data loss and battery drain."
pub.1134030326,More on Pipelined Dynamic Scheduling of Big Data Streams,"An important as well as challenging task in modern applications is the management and processing with very short delays of large data volumes. It is quite often, that the capabilities of individual machines are exceeded when trying to manage such large data volumes. In this regard, it is important to develop efficient task scheduling algorithms, which reduce the stream processing costs. What makes the situation more difficult is the fact that the applications as well as the processing systems are prone to changes during runtime: processing nodes may be down, temporarily or permanently, more resources may be needed by an application, and so on. Therefore, it is necessary to develop dynamic schedulers, which can effectively deal with these changes during runtime. In this work, we provide a fast and fair task migration policy while maintaining load balancing and low latency times. The experimental results have shown that our scheme offers better load balancing and reduces the overall latency compared to the state of the art strategies, due to the stepwise communication and the pipeline based processing it employs."
pub.1040137064,A high throughput processing engine for taxi-generated data streams,"The ACM DEBS Grand Challenge 2015 focuses on real-time analytics over a high volume geospatial data stream composed of taxi trip reports from New York City. The goal of the challenge is to provide a solution which continuously identifies the most frequent routes (query 1) and most profitable areas (query 2) for taxis in New York City. The solution needs to process the incoming data stream in near real-time to provide valid information about taxi positions to end-users in a real-world deployment. We propose a modular processing engine design which is configured to offer efficient performance with a high data throughput and low processing latency. It consists of three main components: an input processor which pre-processes data objects to detect outliers, and two independent query processors tailored to the requirements of challenge queries. To efficiently compute query results, query processors use algorithms customized to the distribution of the taxi-generated data stream. Our experimental evaluation shows that the system can on average process 350,000 input events per second in a distributed mode, while achieving an average latency of less than 1 ms for both queries. Due to their excellent performance, the proposed algorithms are well suited for efficient tracking of a large number of vehicles that are present in modern urban areas."
pub.1039141540,Processing data streams with hard real-time constraints on heterogeneous systems,"Data stream processing applications such as stock exchange data analysis, VoIP streaming, and sensor data processing pose two conflicting challenges: short per-stream latency -- to satisfy the milliseconds-long, hard real-time constraints of each stream, and high throughput -- to enable efficient processing of as many streams as possible. High-throughput programmable accelerators such as modern GPUs hold high potential to speed up the computations. However, their use for hard real-time stream processing is complicated by slow communications with CPUs, variable throughput changing non-linearly with the input size, and weak consistency of their local memory with respect to CPU accesses. Furthermore, their coarse grain hardware scheduler renders them unsuitable for unbalanced multi-stream workloads. We present a general, efficient and practical algorithm for hard real-time stream scheduling in heterogeneous systems. The algorithm assigns incoming streams of different rates and deadlines to CPUs and accelerators. By employing novel stream schedulability criteria for accelerators, the algorithm finds the assignment which simultaneously satisfies the aggregate throughput requirements of all the streams and the deadline constraint of each stream alone. Using the AES-CBC encryption kernel, we experimented extensively on thousands of streams with realistic rate and deadline distributions. Our framework outperformed the alternative methods by allowing 50% more streams to be processed with provably deadline-compliant execution even for deadlines as short as tens milliseconds. Overall, the combined GPU-CPU execution allows for up to 4-fold throughput increase over highly-optimized multi-threaded CPU-only implementations."
pub.1118190628,Towards a decentralized algorithm for mapping network and computational resources for distributed data-flow computations,"Several high-throughput distributed data-processing applications require
multi-hop processing of streams of data. These applications include continual
processing on data streams originating from a network of sensors, composing a
multimedia stream through embedding several component streams originating from
different locations, etc. These data-flow computing applications require
multiple processing nodes interconnected according to the data-flow topology of
the application, for on-stream processing of the data. Since the applications
usually sustain for a long period, it is important to optimally map the
component computations and communications on the nodes and links in the
network, fulfilling the capacity constraints and optimizing some quality metric
such as end-to-end latency. The mapping problem is unfortunately NP-complete
and heuristics have been previously proposed to compute the approximate
solution in a centralized way. However, because of the dynamicity of the
network, it is practically impossible to aggregate the correct state of the
whole network in a single node. In this paper, we present a distributed
algorithm for optimal mapping of the components of the data flow applications.
We propose several heuristics to minimize the message complexity of the
algorithm while maintaining the quality of the solution."
pub.1095531396,Towards a Decentralized Algorithm for Mapping Network and Computational Resources for Distributed Data-Flow Computations,"Several high-throughput distributed data-processing applications require multi-hop processing of streams of data. These applications include continual processing on data streams originating from a network of sensors, composing a multimedia stream through embedding several component streams originating from different locations, etc. These data-flow computing applications require multiple processing nodes interconnected according to the data-flow topology of the application, for on-stream processing of the data. Since the applications usually sustain for a long period, it is important to optimally map the component computations and communications on the nodes and links in the network, fulfilling the capacity constraints and optimizing some quality metric such as end-to-end latency. The mapping problem is unfortunately NP-complete and heuristics have been previously proposed to compute the approximate solution in a centralized way. However, because of the dynamicity of the network, it is practically impossible to aggregate the correct state of the whole network in a single node. In this paper, we present a distributed algorithm for optimal mapping of the components of the data flow applications. We propose several heuristics to minimize the message complexity of the algorithm while maintaining the quality of the solution."
pub.1032591497,Parallel Patterns for Window-Based Stateful Operators on Data Streams: An Algorithmic Skeleton Approach,"The topic of Data Stream Processing is a recent and highly active research area dealing with the in-memory, tuple-by-tuple analysis of streaming data. Continuous queries typically consume huge volumes of data received at a great velocity. Solutions that persistently store all the input tuples and then perform off-line computation are impractical. Rather, queries must be executed continuously as data cross the streams. The goal of this paper is to present parallel patterns for window-based stateful operators, which are the most representative class of stateful data stream operators. Parallel patterns are presented “à la” Algorithmic Skeleton, by explaining the rationale of each pattern, the preconditions to safely apply it, and the outcome in terms of throughput, latency and memory consumption. The patterns have been implemented in the FastFlow$$\mathtt {FastFlow}$$ framework targeting off-the-shelf multicores. To the best of our knowledge this is the first time that a similar effort to merge the Data Stream Processing domain and the field of Structured Parallelism has been made."
pub.1112608855,Parallel Graph Processing,"Real-time processing of user data streams in online services inadvertently creates tension between the users and analysts: users are looking for stronger privacy, while analysts desire for higher utility data analytics in real time. To resolve this tension, this paper describes the design, implementation, and evaluation of PrivApprox, a data analytics system for privacy-preserving stream processing. PrivApprox provides three important properties: (i) privacy, zero-knowledge privacy guarantee for users, a privacy bound tighter than the state-of-the-art differential privacy; (ii) utility, an interface for data analysts to systematically explore the trade-offs between the output accuracy (with error estimation) and the query execution budget; and (iii) latency, near real-time stream processing based on a scalable “synchronization-free” distributed architecture. The key idea behind PrivApprox is to combine two techniques together, namely, sampling (used for approximate computation) and randomized response (used for privacy-preserving analytics). The resulting combination is complementary – it achieves stronger privacy guarantees and also improves the performance for stream analytics."
pub.1007372870,Quality-driven disorder handling for concurrent windowed stream queries with shared operators,"Handling timestamp-disorder among stream tuples is a basic requirement for data stream processing, and involves an inevitable tradeoff between the latency and the quality of stream query results. To meet the tradeoff requirements of diverse streaming applications, the approach of buffer-based, quality-driven disorder handling (QDDH) was proposed recently, which aims to minimize sizes of stream-sorting buffers, thus the result latency, while honoring user-specified result-quality requirements. Previous work on QDDH focuses only on individual stream queries. However, streaming systems often run multiple queries concurrently, and may exploit sharing opportunities across the concurrent queries. Under such shared query execution, stream-sorting buffers can be shared across queries as well, which can potentially reduce the overall memory cost incurred by the sorting buffers. In this paper, focusing on windowed stream queries, we propose a solution for doing QDDH for concurrent queries, across which common source and stream-filtering operators are shared. Experimental results show that our solution can determine the optimal way of sharing sorting buffers across the concurrent queries, such that the goal of quality-driven result-latency minimization is achieved for each query at a minimum memory cost."
pub.1124666790,Evaluation of Real-Time Stream Processing for Internet of Things Applications,"Nowadays, Internet of Things (IoT) application need information on demand and in real-time. With this, big data became vital part of IoT applications as the data generated by the application is very large. In most of the application IoT applications generate continuous streams of data which need to be analyzed in real-time to produce the information about the event. Apache kafka has the ability to handle continuous stream of data in distributed manner. In this work, we evaluate the performance of IoT applications by applying the apache kafka at middleware layer (processing layer). We evaluate the latency and throughput for educational, smart home, and smart building applications."
pub.1145845930,An Edge-Cloud Collaborative Object Detection System,"Edge computing system usually consists of the lightweight neural network to preprocess the video stream, and then transmits the intermediate data to the cloud for video analysis, which not only ensures the real-time performance of video processing but also greatly reduces the WAN bandwidth consumption. However, many existing edge processing systems sacrifice video processing accuracy to reduce intermediate transmission volume or reduce processing delay. Therefore, the leveraging of accuracy and latency places a challenge on how to deploy the network on the edge device and set the pre-processing parameters. This paper builds a real-time video stream processing system, then tries to achieve the balance between the cost and benefit of edge preprocessing by designing a dynamic configuration algorithm for optimal preprocessing deployment to achieve low latency, low transmission, and high precision real-time video processing."
pub.1019245976,An Optimization of the Delay Scheduling Algorithm for Real-Time Video Stream Processing,"We used Spark as a platform for large-scale, real-time intelligent video stream analysis. We observed that the default task scheduling algorithm of Spark was not efficient for scheduling image frame data processing tasks, incurring problems such as poor data locality, high network traffic, low utilization of computing resources, etc. This paper investigates why Spark’s default task scheduling algorithm is not suitable for real-time video stream processing. Further, we present a new real-time task scheduling algorithm that leverages the notion of data locality. This algorithm schedules tasks based on data locality and information collected at runtime, including task execution time and workload of each node. Experiments show that our proposed algorithm increases data locality and CPU utilization while reducing network traffic and latency."
pub.1098987144,Continuous analytics,"Stream query processing has been one of the more popular topics in database research so far this century. The basic idea is to provide database-style query processing over data on-the-fly as they arrive at the system,. Compared to the store-first, query-later approach followed by traditional database systems, stream query processing holds the promise for dramatically improved efficiency and reduced latency. Work in this area was originally motivated by ""real-time"" data-intensive scenarios such as sensor networks, financial trading applications, and network security. Lately, stream processing has been moving from the research lab into the real world through efforts at start-up companies, traditional database vendors, and open source projects. Not surprisingly, the practical uses and advantages of the technology are turning out to be different than many had originally expected. In this talk, I'll survey the state of the art in stream query processing and related technologies such as event processing, discuss some of the implications for data-intensive system architectures, and provide my views on the future role of this technology from both a research and a commercial perspective. In particular, I'll describe the notion of Continuous Analytics, which leverages Stream Query Processing techniques to solve some of the inherent bottlenecks that have existed in database systems since their inception. I will also discuss several implementation issues that arose through experience with specific application deployments including the need to handle out-of-order data and high-cardinality dimensions."
pub.1041949755,Discretized streams,"Many ""big data"" applications must act on data in real time. Running these applications at ever-larger scales requires parallel platforms that automatically handle faults and stragglers. Unfortunately, current distributed stream processing models provide fault recovery in an expensive manner, requiring hot replication or long recovery times, and do not handle stragglers. We propose a new processing model, discretized streams (D-Streams), that overcomes these challenges. D-Streams enable a parallel recovery mechanism that improves efficiency over traditional replication and backup schemes, and tolerates stragglers. We show that they support a rich set of operators while attaining high per-node throughput similar to single-node systems, linear scaling to 100 nodes, sub-second latency, and sub-second fault recovery. Finally, D-Streams can easily be composed with batch and interactive query models like MapReduce, enabling rich applications that combine these modes. We implement D-Streams in a system called Spark Streaming."
pub.1013038010,Continuous analytics over discontinuous streams,"Continuous analytics systems that enable query processing over steams of data have emerged as key solutions for dealing with massive data volumes and demands for low latency. These systems have been heavily influenced by an assumption that data streams can be viewed as sequences of data that arrived more or less in order. The reality, however, is that streams are not often so well behaved and disruptions of various sorts are endemic. We argue, therefore, that stream processing needs a fundamental rethink and advocate a unified approach toward continuous analytics over discontinuous streaming data. Our approach is based on a simple insight - using techniques inspired by data parallel query processing, queries can be performed over independent sub-streams with arbitrary time ranges in parallel, generating partial results. The consolidation of the partial results over each sub-stream can then be deferred to the time at which the results are actually used on an on-demand basis. In this paper, we describe how the Truviso Continuous Analytics system implements this type of order-independent processing. Not only does the approach provide the first real solution to the problem of processing streaming data that arrives arbitrarily late, it also serves as a critical building block for solutions to a host of hard problems such as parallelism, recovery, transactional consistency, high availability, failover, and replication."
pub.1153128655,Comparative Analysis of GPU Stream Processing between Persistent and Non-persistent Kernels,"Employing GPU to accelerate stream data processing has shown to be a considerable success in recent research. The stream processing engines need to process continuous data. Persistent Thread (PT), where GPU threads remain in a loop throughout executions, rather than non-Persistent Thread (nonPT) kernels can give several advantages. PT provides high-performance low-latency data processing by reducing kernel launch overhead and hiding memory copy overhead to the GPU. In this paper, we comparatively analyze the performances of PT and nonPT in various aspects. We also figure out the cause of advantages and what needs to be considered in order to obtain actual performance gain. The evaluation was done with four distinct application scenarios and the result shows that PT yields at most x4.4 higher performance over nonPT under desirable conditions."
pub.1034770766,SQL Streaming Process in Query Engine Net,"The massively growing data volume and the pressing need for low latency are pushing the traditional store-first-query-later data warehousing technologies beyond their limits. Many enterprise applications are now based on continuous analytics of data streams. While integrating stream processing with query processing takes advantage of SQL’s expressive power and DBMS’s data management capability, it raises serious challenges in dealing with complex dataflow, applying queries to unbounded stream data, and providing highly scalable, dynamically configurable, elastic infrastructure.To solve these problems, we model the general graph-structured, continuous dataflow analytics as a SQL Streaming Process with multiple connected and stationed continuous queries; then we extend the query engine to support cyclebased query execution for processing unbounded stream data chunk-wise with sound semantics; and finally, we develop the Query Engine Net (QE-Net) over the Distributed Caching Platforms (DCP) as a dynamically configurable elastic infrastructure for parallel and distributed execution of SQL Streaming Processes.We extended the PostgreSQL engines for building the QE-Net infrastructure. Our experience shows its merit in leveraging SQL and query processing to analyze real-time, graph-structured and unbounded streams. Integrating it with a commercial and proprietary MPP based database cluster is being investigated."
pub.1149821078,Alps: An Adaptive Load Partitioning Scaling Solution for Stream Processing System on Skewed Stream,"The distributed stream processing system suffers from the rate variation and skewed distribution of input stream. The scaling policy is used to reduce the impact of rate variation, but cannot maintain high performance with a low overhead when input stream is skewed. To solve this issue, we propose Alps, an Adaptive Load Partitioning Scaling system. Alps exploits adaptive partitioning scaling algorithm based on the willingness function to determine whether to use a partitioning policy. To our knowledge, this is the first approach integrates scaling policy and partitioning policy in an adaptive manner. In addition, Alps achieves the outstanding performance of distributed stream processing system with the least overhead. Compared with state-of-the-art scaling approach DS2, Alps reduces the end-to-end latency by 2 orders of magnitude on high-speed skewed stream and avoids the waste of resources on low-speed or balanced stream."
pub.1095661102,Wide-Area Spark Streaming: Automated Routing and Batch Sizing,"Modern stream processing frameworks, such as Spark Streaming, are designed to support a wide variety of stream processing applications, such as real-time data analytics in social networks. As the volume of data to be processed increases rapidly, there is a pressing need for processing them across multiple geo-distributed datacenters. However, these frameworks are not designed to take limited and varying inter-datacenter bandwidth into account, leading to longer query latencies. In this paper, we focus on reducing latencies for spark streaming queries in wide-area networks, by automatically selecting data flow routes and determining micro-batch sizes across geo- distributed datacenters. Specifically, we formulate a nonconvex optimization problem, and solve it with an efficient heuristic algorithm based on readily measurable operating traces. We conducted experiments on Amazon EC2 with emulated bandwidth constraints. Our experimental results have demonstrated the effectiveness of our proposed algorithm, as compared to the existing Spark Streaming."
pub.1091641301,Samza," Distributed stream processing systems need to support stateful processing, recover quickly from failures to resume such processing, and reprocess an entire data stream quickly. We present Apache Samza, a distributed system for stateful and fault-tolerant stream processing. Samza utilizes a partitioned local state along with a low-overhead background changelog mechanism, allowing it to scale to massive state sizes (hundreds of TB) per application. Recovery from failures is sped up by re-scheduling based on Host Affinity. In addition to processing infinite streams of events, Samza supports processing a finite dataset as a stream, from either a streaming source (e.g., Kafka), a database snapshot (e.g., Databus), or a file system (e.g. HDFS), without having to change the application code (unlike the popular Lambda-based architectures which necessitate maintenance of separate code bases for batch and stream path processing).  Samza is currently in use at LinkedIn by hundreds of production applications with more than 10, 000 containers. Samza is an open-source Apache project adopted by many top-tier companies (e.g., LinkedIn, Uber, Netflix, TripAdvisor, etc.). Our experiments show that Samza: a) handles state efficiently, improving latency and throughput by more than 100X compared to using a remote storage; b) provides recovery time independent of state size; c) scales performance linearly with number of containers; and d) supports reprocessing of the data stream quickly and with minimal interference on real-time traffic."
pub.1140776303,Shared-Memory Parallel Hash-Based Stream Join in Continuous Data Streams,"Stream join is known as one of the most important and computationally expensive stream operations in data stream management systems (DSMSs). Parallelization techniques that leverage modern multi-core processor have been proposed for stream join in literature. Equi-join is the most frequent type of join in query workloads, and symmetric hash join (SHJ) is the most effective algorithm to process that in data streams. In this paper, as the first research work, we propose a shared-memory parallel symmetric hash join algorithm on multi-core processors for equi-based stream join. Also, we introduce a novel parallel algorithm called chunk-based pairing hash join that significantly elevates throughput and scalability. We have performed extensive experimental evaluation that demonstrates high scalability and low latency for our proposed algorithms."
pub.1004410350,"Design, implementation, and evaluation of the linear road bnchmark on the stream processing core","Stream processing applications have recently gained significant attention in the networking and database community. At the core of these applications is a stream processing engine that performs resource allocation and management to support continuous tracking of queries over collections of physically-distributed and rapidly-updating data streams. While numerous stream processing systems exist, there has been little work on understanding the performance characteristics of these applications in a distributed setup. In this paper, we examine the performance bottlenecks of streaming data applications, in particular the Linear Road stream data management benchmark, in achieving good performance in large-scale distributed environments, using the Stream Processing Core (SPC), a stream processing middleware we have developed. First, we present the design and implementation of the Linear Road benchmark on the SPC middleware. SPC has been designed to scale to tens of thousands of processing nodes, while supporting concurrent applications and multiple simultaneous queries. Second, we identify the main performance bottlenecks in the Linear Road application in achieving scalability and low query response latency. Our results show that data locality, buffer capacity, physical allocation of processing elements to infrastructure nodes, and packaging for transporting streamed data are important factors in achieving good application performance. Though we evaluate our system primarily for the Linear Road application, we believe it also provides useful insights into the overall system behavior for supporting other distributed and large-scale continuous streaming data applications. Finally, we examine how SPC can be used and tuned to enable a very efficient implementation of the Linear Road application in a distributed environment."
pub.1094612671,GeeLytics: Geo-Distributed Edge Analytics for Large Scale IoT Systems Based on Dynamic Topology,"High data rate sensors such as video cameras, audio sensors, and motion sensors are becoming ubiquitous in the Internet of Things (IoT). In large scale IoT systems like smart cities, a large number of sensors are now widely deployed at different locations, generating a huge amount of stream data. Although the generated data provide us great potential to sense our live environments, it still remains a big challenge to efficiently extract real-time results from sensor data to make fast decisions. Existing stream processing platforms, such as Storm, Spark Streaming, and S4, are well designed to process stream data within a cluster in the Cloud, but they are not suitable for highly distributed IoT systems in which data are naturally geo-distributed and low latency analytics results are expected to be shared across users and applications. To tackle this problem, we design an edge analytics platform called GeeLytics, which can perform real-time stream processing both at the network edges and in the Cloud in a dynamic and transparent manner. In this position paper we discuss its use cases, motivation, and preliminary architecture design. As compared with the start of the art, GeeLytics is designed to support dynamic stream processing topologies by taking into account the system characteristics of heterogeneous edge/Cloud nodes and also the current system workload. This shall achieve low latency analytics results while minimizing the edge-to-Cloud bandwidth consumption. In addition, using docker application containers for packaging up deployable tasks and a distributed pub/sub mechanism for inter-task stream data routing, GeeLytics shall provide better resource isolation and system efficiency to support multi-tenancy."
pub.1004391931,C-MR,"The widespread appeal of MapReduce is due, in part, to its simple programming model. Programmers provide only application logic while the MapReduce framework handles the logistics of data distribution and parallel task management. We present the Continuous-MapReduce (C-MR) framework which implements a modified MapReduce processing model to continuously execute workflows of MapReduce jobs on unbounded data streams. In keeping with the philosophy of MapReduce, C-MR abstracts away the complexities of parallel stream processing and workflow scheduling while providing the simple and familiar MapReduce programming interface with the addition of stream window semantics. Modifying the MapReduce processing model allowed us to: (1) maintain correct stream order and execution semantics in the presence of parallel and asynchronous processing elements; (2) implement an operator scheduler framework to facilitate latency-oriented scheduling policies for executing complex workflows of MapReduce jobs; and (3) leverage much of the work that has gone into the last decade of stream processing research including: pipelined parallelism, incremental processing for both Map and Reduce operations, minimizing redundant computations, sharing of sub-queries, and adaptive query processing. C-MR was developed for use on a multiprocessor architecture, where we demonstrate its effectiveness at supporting high-performance stream processing even in the presence of load spikes and external workloads."
pub.1093841181,Runtime Migration of Stateful Event Detectors with Low-Latency Ordering Constraints,"Runtime migration has been widely adopted to achieve several tasks such as load balancing, performance optimization, and fault-tolerance. However, existing migration techniques do not work for event detectors in distributed publish/subscribe systems that are used to analyze sensor data. Since low-latency time-constraints are no longer valid they reorder streams incorrectly and cause erroneous event detector states. This paper presents a safe runtime migration of stateful event detectors that respects low-latency time-constraints and seamlessly orders input events correctly on the migrated host. Event streams are only forwarded until timing delays are properly calibrated, the migrated event detector immediately stops processing after its state is transferred, and the processing overhead is negligible. On a Realtime Locating System (RTLS) we show that we can efficiently migrate event detectors at runtime between servers where other techniques would fail."
pub.1117788622,Hydrological stream data pipeline framework based on IoTDB,"With the increasing amount of hydrological data in Chuhe river basin, the traditional relational database has been unable to meet the needs of users, which not only makes it difficult to achieve low latency and high throughput in the real-time transmission of hydrological data, but also causes the phenomenon of long time or even system crash when querying large amount of annual water-level data. To solve this problem, this paper proposes a stream data pipeline framework based on timeseries databases IoTDB and Kafka, which can provide services for hydrological early warning and anomaly detection researchers. Based on the hydrological sensor data of Chuhe river, the processing scenarios of sensor stream data are set and compared with other NoSQL (HBase, MongoDB, RiakTS and Redis) in different scenarios. The performance and workload of different NoSQL in this data pipeline are tested. Finally, it is docked with Flink real-time stream data processing platform and compared with other data pipelines. The experimental results show that the stream data pipeline composed of IoTDB, Kafka and Flink is outstanding in data acquisition, transmission, incremental query and data analysis."
pub.1095097842,Data-Intensive Workflow Optimization Based on Application Task Graph Partitioning in Heterogeneous Computing Systems,"Stream based data processing model is proven to be an established method to optimize data-intensive applications. Data-intensive applications involve movement of huge amount of data between execution nodes that incurs large costs. Data-streaming model improves the execution performance of such applications. In the stream-based data processing model, performance is usually measured by throughput and latency. Optimization of these performance metrics in heterogeneous computing environment becomes more challenging due to the difference in the computing capacity of execution nodes and variations in the data transfer capability of communication links between these nodes. This paper presents a dual objective Partitioning based Data-intensive Workflow optimization Algorithm (PDWA) for heterogeneous computing systems. The proposed PDWA provides significantly reduced latency with increase in the throughput. In the proposed algorithm, the application task graph is partitioned such that the inter-partition data movement is minimal. Such optimized partitioning enhances the throughput. Each partition is mapped to the execution node that gives minimum execution time for that particular partition. PDWA also exploits partial task duplication to reduce the latency. We evaluated the proposed algorithm with synthesized benchmarks and workflows from the real-world workloads, and the proposed algorithm shows 60% reduced latency with 47% improvement in the throughput as compared to the approach when workflows are not partitioned."
pub.1043733168,Auto-scaling techniques for elastic data stream processing,"Typical use cases like financial trading or monitoring of manufacturing equipment pose huge challenges regarding end to end latency as well as throughput towards existing data stream processing systems. Established solutions like Apache S4 or Storm need to scale out to a large set of hosts to meet these challenges. An ideal system can react to workload changes by on demand acquisition or release of hosts. Thereby, it can handle unexpected peak loads as well as improve the average utilization of the system. This property is called elasticity. The major challenge for an elastic scaling system is to find the right point in time to scale in or out. To determine this right point is difficult, because it depends on constantly changing system and workload characteristics. In this demonstration, we apply three alternative auto-scaling techniques known from other domains on top of an existing elastic data stream processing system. A user of the demonstration can experience the influence of the chosen auto-scaling technique on the latency and the system utilization using a real-world use case based on different workloads from the Frankfurt stock exchange."
pub.1094430238,Video Processing on GPU: Analysis of Data Transfer Overhead,"In this work, we study one of the major problems in exploring the power of GPUs to accelerate video processing applications: countless frames have to be transferred back and forth between the CPU and GPU. We evaluate four different data transfer approaches currently available on modern GPUs: Standard Allocation, Pinned Memory, Data Stream, and Zero-Copy. Our results show that Data Stream is the most efficient strategy, but requires more programming effort. Zero-Copy, on the other hand, demonstrates inferior performance due to the significant latency incurred by the PCIe bus transfers for every memory access."
pub.1159915103,A two-tier coordinated load balancing strategy over skewed data streams,"Load imbalance severely affects cluster performance, and the polarization of resources due to load skewing leads to further worsening of system throughput and latency problems. The proliferation of tasks to be processed in the big data era leads to more severe load skewing. How to cope with the surge of skewed data stream in the context of big data is a new challenge now. In this paper, we propose a coordinated load balancing strategy on skewed data streams (referred to as St-Stream), which is a two-tier hierarchical system for handling data streams. The proposed strategy is characterized by performing a migration pairing strategy for resources at the task allocation stage by cutting and moving out the tasks of high-load nodes in a hierarchical manner, and the moved-out operators are placed in the routing table, and the routing table operators are moved out to these nodes sequentially according to the tasks required by low-load nodes. We further design a two-tier coordination scheme for the resource allocation problem, which can adjust the skewed load from within the nodes and then dynamically restore the balance between the nodes. We implemented St-Stream on Apache Storm, which achieves a 21% coordination in processing CPU utilization, a 17.6% reduction in latency, and a 0.3 improvement in load balance recovery compared to the baseline design. Our experimental results demonstrate that the proposed load balancing strategy better balances the cluster load and improves the performance of the stream processing system."
pub.1035102681,Analysis of Memory Constrained Live Provenance,"We conjecture that meaningful analysis of large-scale provenance can be preserved by analyzing provenance data in limited memory while the data is still in motion; that the provenance needs not be fully resident before analysis can occur. As a proof of concept, this paper defines a stream model for reasoning about provenance data in motion for Big Data provenance. We propose a novel streaming algorithm for the backward provenance query, and apply it to the live provenance captured from agent-based simulations. The performance test demonstrates high throughput, low latency and good scalability, in a distributed stream processing framework built on Apache Kafka and Spark Streaming."
pub.1053520827,Grand challenge,"In this paper, we present a real-time capable event-based system, which is tailored towards analytical query processing in the context of soccer games. The main challenge is to meet the application's strict real-time and low-latency requirements in face of streams of high-velocity sensor data. We describe a workflow-like architecture for query processing based on a publish/subscribe model. Queries are structured into computational tasks that are arranged sequentially and/or in parallel. Tasks are connected by preallocated ring buffers providing total event ordering and fast as well as decoupled event access. Our evaluation results show the effectiveness of the proposed system in terms of low-latency processing under real-time conditions. Speeding up the system by a factor of 50 compared to real-time introduces almost no latency overhead."
pub.1132774006,Load-Aware Shedding in Stream Processing Systems,"Distributed stream processing systems are today gaining momentum as a tool to perform analytics on continuous data streams. Load shedding is a technique used to handle unpredictable spikes in the input load whenever available computing resources are not adequately provisioned. In this paper, we propose Load-Aware Shedding (LAS), a novel load shedding solution that, unlike previous works, does not rely neither on a pre-defined cost model nor on any assumption on the tuple execution duration. Leveraging sketches, LAS efficiently estimates the execution duration of each tuple with small error bounds and uses this knowledge to proactively shed input streams at any operator to limiting queuing latencies while dropping as few tuples as possible. We provide a theoretical analysis proving that LAS is an (ε,δ)$$({\varepsilon }, \delta )$$-approximation of the optimal online load shedder. Furthermore, through an extensive practical evaluation based on simulations and a prototype, we evaluate its impact on stream processing applications."
pub.1093790223,Effective Multi-Stream Joining in Apache Samza Framework,"Increasing adoption of Big Data in business environments have driven the needs of stream joining in realtime fashion. Multi-stream joining is an important stream processing type in todays Internet companies, and it has been used to generate higher-quality data in business pipelines. Multi-stream joining can be performed in two models: (1) All-In-One (AIO) Joining and (2) Step-By-Step (SBS) Joining. Both models have advantages and disadvantages with regard to memory footprint, joining latency, deployment complexity, etc. In this work, we analyze the performance tradeoffs associated with these two models using Apache Samza."
pub.1094268471,100 Gbit/s End-to-End Communication: Designing Scalable Protocols with Soft Real-Time Stream Processing,"With the recent roll-out of 100 Gbit Ethernet technology for high-performance computing applications and the technology for 100 Gbit wireless communication emerging on the horizon, it is just a matter of time until non-high performance computing applications will have to utilize these data rates. Since 10 Gbit/s protocol processing is already challenging for current server machines and simply upscaling the computing resources is no solution, new approaches are needed. In this paper, we present a stream processing based design approach for scalable communication protocols. The stream processing paradigm enables us to adapt the communication protocol processing for a certain hardware configuration without touching the protocol's implementation. We use this design technique to develop a prototype communication protocol for ultra-high throughput applications and we demonstrate how to adapt the protocol processing for a Stable Throughput as well as for a Low Latency scenario. Last but not least, we present the evaluation results of the experiments, which show that the measured throughput respectively latency of the adapted protocol, scales nearly linear with the number of provided interfaces."
pub.1005889119,Adaptive query processing in data stream management systems under limited memory resources,"Many data stream sources are prone to periods of spikes in volume as well as periods of delays and silence. Because peak load during a spike can be orders of magnitude higher than a typical load, fully provisioning data stream monitoring system with all needed resources is generally difficult to achieve. Furthermore, data stream sources are subject to network delays and congestions as they connect to a data stream monitoring system over shared communication channels. Careless management of delays and periods of silence will eventually drop system performance drastically. Our research contribution investigates system performance during periods of peak load and periods of delays while supporting data stream applications, e.g., as in monitoring online stocks. We propose an algorithm, termed EM-SWJoin, that utilizes external memory data structures to keep up with the variable data arrival rates while keeping disk access latency at minimum. We also propose ADEDAS; an algorithm that guarantees an ordered release of output results while controlling the impact of delays over stream processing. Finally, we investigate how to deploy column-stores in data stream environments where column-oriented physical design approaches replace row-by-row data representations."
pub.1174082559,PP-Stream: Toward High-Performance Privacy-Preserving Neural Network Inference via Distributed Stream Processing,"Privacy preservation is critical for neural network inference, which often involves collaborative execution of different parties to make predictions on sensitive data based on sensitive neural network models. However, the expensive cryptographic operations of privacy preservation also pose performance chal-lenges to neural network inference. We address this performance-security tension by designing PP-Stream, a distributed stream processing system for high-performance privacy-preserving neural network inference. PP-Stream adopts hybrid privacy-preserving mechanisms for linear and non-linear operations of neural network inference. It treats inference data as real-time data streams, and parallelizes the inference operations across multiple pipelined stages that are executed by multiple servers and threads. It also solves the load-balanced resource allocation across servers and threads as an optimization problem. We prototype PP-Stream and show via testbed experiments that it achieves low inference latencies on various neural network models."
pub.1050461113,Flexible time management in data stream systems,"Continuous queries in a Data Stream Management System (DSMS) rely on time as a basis for windows on streams and for defining a consistent semantics for multiple streams and updatable relations. The system clock in a centralized DSMS provides a convenient and well-behaved notion of time, but often it is more appropriate for a DSMS application to define its own notion of time---its own clock(s), sequence numbers, or other forms of ordering and times-tamping. Flexible application-defined time poses challenges to the DSMS, since streams may be out of order and uncoordinated with each other, they may incur latency reaching the DSMS, and they may pause or stop. We formalize these challenges and specify how to generate heartbeats so that queries can be evaluated correctly and continuously in an application-defined time domain. Our heartbeat generation algorithm is based on parameters capturing skew between streams, unordering within streams, and latency in streams reaching the DSMS. We also describe how to estimate these parameters at run-time, and we discuss how heartbeats can be used for processing continuous queries."
pub.1138989639,EIRES: Efficient Integration of Remote Data in Event Stream Processing,"To support reactive and predictive applications, complex event processing (CEP) systems detect patterns in event streams based on predefined queries. To determine the events that constitute a query match, their payload data may need to be assessed together with data from remote sources. Such dependencies are problematic, since waiting for remote data to be fetched interrupts the processing of the stream. Yet, without event selection based on remote data, the query state to maintain may grow exponentially. In either case, the performance of the CEP system degrades drastically. To tackle these issues, we present EIRES, a framework for efficient integration of static data from remote sources in CEP. It employs a cost-model to determine when to fetch certain remote data elements and how long to keep them in a cache for future use. EIRES combines strategies for (i) prefetching that queries remote data based on anticipated use and (ii) lazy evaluation that postpones the event selection based on remote data without interrupting the stream processing. Our experiments indicate that the combination of these strategies improves the latency of query evaluation by up to 3,725x for synthetic data and 47x for real-world data."
pub.1093951529,An Enforcement of Real Time Scheduling in Spark Streaming,"With the exponential growth in continuous data streams, real time streaming processing has been gaining a lot of popularity. Spark Streaming is one of the open source frameworks for reliable, high-throughput and low latency stream processing. Though it is a near real time stream processing framework running on commodity hardware, real time event processing is not guaranteed in its scheduling system. Profiling results indicate that the total delay time of events with unstable inputs is more volatile and presents big fluctuations. In this paper, we propose a simple, yet effective scheduling strategy to reduce the worst case event processing time by dynamic adjusting the time window of batch intervals. It is a real time enhancement to Spark Streaming based on Spark's framework. The proposed strategy is evaluated using two streaming benchmarks and our preliminary results demonstrate the feasibility of our approach with unstable event streams."
pub.1136452911,SenseRT: A Streaming Architecture for Smart Building Sensors,"Building Management Systems (BMSs) have evolved in recent years, in ways that
require changes to existing network architectures that follow the
store-then-analyse approach. The primary cause is the increasing deployment of
a diverse range of cost-effective sensors and actuators in smart buildings that
generate real-time streaming data. Any in-building system with a large number
of sensors needs a framework for real-time data collection and concurrent
stream processing from sensors connected using a range of networks.
  We present SenseRT, a system for managing and analysing in-building real-time
streams of sensor data. SenseRT collects streams of real-time data from sensors
connected using a range of network protocols. It supports concurrent modules
simultaneously performing stream processing over real-time data, asynchronously
and non-blocking, with results made available with minimal latency. We describe
a prototype implementation deployed in two University department buildings,
demonstrating its effectiveness."
pub.1095192464,A User Behavior Anomaly Detection Approach Based on Sequence Mining Over Data Streams,"How to design a low-latency and accurate approach for user behavior anomaly detection over data streams has become a great challenge. However, existing studies cannot meet low-latency and accurate requirements, due to a large number of subsequences and sequential relationship in behaviors. This paper presents BADSM, a user behavior anomaly detection approach based on sequence mining over data streams that seeks to address such challenge. BADSM uses self-adaptive behavior pruning algorithm to adaptively divide data stream into behaviors and decrease the number of subsequences to improve the efficiency of sequence mining. Meanwhile, the top-k abnormal scoring algorithm is used to reduce the complexity of traversal and obtain quantitative detection result to improve accuracy. We design and implement a streaming anomaly detection system based on BADSM to perform online detection. Extensive experiments confirm that BADSM significantly reduces processing delay by at least 36.8% and false positive rate by 6.4% compared with the classic sequence mining approach PrefixSpan."
pub.1071882181,A Scalable Big Stream Cloud Architecture for the Internet of Things,"<p>The Internet of Things (IoT) will consist of billions (50 billions by 2020) of interconnected heterogeneous devices denoted as “Smart Objects:” tiny, constrained devices which are going to be pervasively deployed in several contexts. To meet low-latency requirements, IoT applications must rely on specific architectures designed to handle the gigantic stream of data coming from Smart Objects. This paper propose a novel Cloud architecture for Big Stream applications that can efficiently handle data coming from Smart Objects through a Graph-based processing platform and deliver processed data to consumer applications with low latency. The authors reverse the traditional “Big Data” paradigm, where real-time constraints are not considered, and introduce the new “Big Stream” paradigm, which better fits IoT scenarios. The paper provides a performance evaluation of a practical open-source implementation of the proposed architecture. Other practical aspects, such as security considerations, and possible business oriented exploitation plans are presented.</p>"
pub.1091111538,IoT Stream Processing and Analytics in the Fog,"The emerging fog paradigm has been attracting increasing interest from both academia and industry, due to the low-latency, resilient, and cost-effective services it can provide. Many fog applications, such as video mining and event monitoring, rely on data stream processing and analytics, which are very popular in the cloud, but have not been comprehensively investigated in the context of fog architecture. In this article, we present the general models and architecture of fog data streaming, by analyzing the common properties of several typical applications. We also analyze the design space of fog streaming with the consideration of four essential dimensions (system, data, human, and optimization), where both new design challenges and the issues that arise from leveraging existing techniques are investigated, such as cloud stream processing, computer networks, and mobile computing."
pub.1132062169,Big Data Velocity Management–From Stream to Warehouse via High Performance Memory Optimized Index Join,"Efficient resource optimization is critical to manage the velocity and volume of real-time streaming data in near-real-time data warehousing and business intelligence. This article presents a memory optimisation algorithm for rapidly joining streaming data with persistent master data in order to reduce data latency. Typically during the transformation phase of ETL (Extraction, Transformation, and Loading) a stream of transactional data needs to be joined with master data stored on disk. To implement this process, a semi-stream join operator is commonly used. Most semi-stream join operators cache frequent parts of the master data to improve their performance, this process requires careful distribution of allocated memory among the components of the join operator. This article presents a cache inequality approach to optimise cache size and memory. To test this approach, we present a novel Memory Optimal Index-based Join (MOIJ) algorithm. MOIJ supports many-to-many types of joins and adapts to dynamic streaming data. We also present a cost model for MOIJ and compare the performance with existing algorithms empirically as well as analytically. We envisage the enhanced ability of processing near-real-time streaming data using minimal memory will reduce latency in processing big data and will contribute to the development of high-performance real-time business intelligence systems."
pub.1118827191,IoT Stream Processing and Analytics in The Fog,"The emerging Fog paradigm has been attracting increasing interests from both
academia and industry, due to the low-latency, resilient, and cost-effective
services it can provide. Many Fog applications such as video mining and event
monitoring, rely on data stream processing and analytics, which are very
popular in the Cloud, but have not been comprehensively investigated in the
context of Fog architecture. In this article, we present the general models and
architecture of Fog data streaming, by analyzing the common properties of
several typical applications. We also analyze the design space of Fog streaming
with the consideration of four essential dimensions (system, data, human, and
optimization), where both new design challenges and the issues arise from
leveraging existing techniques are investigated, such as Cloud stream
processing, computer networks, and mobile computing."
pub.1093879021,Real-Time Analytics for Fast Evolving Social Graphs,"Existing Big Data streams coming from social and other connected sensor networks exhibit intrinsic inter-dependency enabling unique challenges to scalable graph analytics. Data from these graphs is usually collected in different geographically located data servers making it suitable for distributed processing on clouds. While numerous solutions for large scale static graph analysis have been proposed, addressing in real-time the dynamics of social interactions requires novel approaches that leverage incremental stream processing and graph analytics on elastic clouds. We propose a scalable solution based on our stream processing engine, Floe, on top of which we perform real-time data processing and graph updates to enable low latency graph analytics on large evolving social networks. We demonstrate the platform on a large Twitter data set by performing several fast graph and non-graph analytics to extract in real-time the top k influential nodes, with different metrics, during key events such as the US NFL playoffs. This information allows advertisers to maximize their exposure to the public by always targeting the continuously changing set of most influential nodes. Its applicability spans multiple domains including surveillance, counter-terrorism, or disease spread monitoring. The evaluation will be performed on a combination our local cluster of 16 eight-core nodes running Eucalyptus fabric and 100s of virtual machines on the Amazon AWS public cloud. We will showcase the low latency in detecting changes in the graph under variable data streams, and also the efficiency of the platform to utilize resources and to elastically scale to meet demand."
pub.1109814853,Ares: a High Performance and Fault-tolerant Distributed Stream Processing System,"Distributed Stream Processing Systems (DSPSs) have been widely deployed to process infinite data streams. Short processing latency and short recovery time are both vital for many DSPS applications. Existing DSPS designs commonly leverage elaborated task allocation strategies to achieve short processing latency. Such designs, however, ignore the requirement of system fault tolerance. Indeed, providing fault tolerant capability in a DSPS can cause significant degradation of system performance. Especially, the intrinsic dependency between upstream and downstream tasks can incur cascaded waiting during recovery, leading to prohibitively long recovery time. In this paper, we propose Ares, a high performance and fault tolerant DSPS. Ares considers both system performance and fault tolerant capability during task allocation. In the design of Ares, we formalize the problem of Fault Tolerant Scheduler (FTS) for finding an optimal task allocation which maximizes the system utility. We use a game-theoretic approach to solve the FTS problem and propose a novel Nirvana algorithm based on best-response dynamics. We mathematically prove the existence of Nash equilibrium in the FTS game. We implement Ares atop Apache Storm and conduct comprehensive experiments to evaluate this design. The results show that, compared to existing designs Ares achieves a 3.6 x improvement of throughput, as well as reducing the processing latency and the recovery time by 50.2 % and 52.5 %, respectively."
pub.1121646033,Ubiq: A Scalable and Fault-Tolerant Log Processing Infrastructure,"Most of today’s Internet applications generate vast amounts of data (typically, in the form of event logs) that needs to be processed and analyzed for detailed reporting, enhancing user experience and increasing monetization. In this paper, we describe the architecture of Ubiq, a geographically distributed framework for processing continuously growing log files in real time with high scalability, high availability and low latency. The Ubiq framework fully tolerates infrastructure degradation and data center-level outages without any manual intervention. It also guarantees exactly-once semantics for application pipelines to process logs as a collection of multiple events. Ubiq has been in production for Google’s advertising system for many years and has served as a critical log processing framework for several dozen pipelines. Our production deployment demonstrates linear scalability with machine resources, extremely high availability even with underlying infrastructure failures, and an end-to-end latency of under a minute."
pub.1138726266,Retracted: Container selection processing implementing extensive neural learning in cloud services,"The container selection processing performance analyses huge data with minimal resources and the lowest latency. The elastic management applications execute the containers with the specific hierarchy of virtual processing and machine management. The container that has performance degradation implicit provision of least expensive containers with minimal resources helps to increase the performance of containers. The specific data and stream processing prompts scrutinizing of data through the container selection process through different methodologies. To exterminate the bottleneck problem that selects efficient and required size, processing speed, and its reliability of guiding the batch processing of containers. The extensive neural learning handles container optimality involves a dynamic selection of appropriate containers in cloud service providers. The cloud service providers along with container selection contain batch processing and stream processing allocates efficient container-specific selection appropriately. In the huge data segregation of data processed data that emphasizes multiple data scrutinizing."
pub.1021332177,SABER,"Modern servers have become heterogeneous, often combining multi-core CPUs with many-core GPGPUs. Such heterogeneous architectures have the potential to improve the performance of data-intensive stream processing applications, but they are not supported by current relational stream processing engines. For an engine to exploit a heterogeneous architecture, it must execute streaming SQL queries with sufficient data-parallelism to fully utilise all available heterogeneous processors, and decide how to use each in the most effective way. It must do this while respecting the semantics of streaming SQL queries, in particular with regard to window handling. We describe Saber, a hybrid high-performance relational stream processing engine for CPUs and GPGPUs. Saber executes window-based streaming SQL queries in a data-parallel fashion using all available CPU and GPGPU cores. Instead of statically assigning query operators to heterogeneous processors, Saber employs a new adaptive heterogeneous lookahead scheduling strategy, which increases the share of queries executing on the processor that yields the highest performance. To hide data movement costs, Saber pipelines the transfer of stream data between CPU and GPGPU memory. Our experimental comparison against state-of-the-art engines shows that Saber increases processing throughput while maintaining low latency for a wide range of streaming SQL queries with both small and large window sizes."
pub.1038885046,"A high-throughput, scalable solution for calculating frequent routes and profitability of New York taxis","Processing complex queries on unbounded event streams in real-time, is a challenge for many data processing systems. These systems are expected to process data with reduced latency to generate real-time events, and at high throughput to minimize the required hardware. In this regard, Grand Challenge 2015 [6] focuses on evaluating two queries (frequent routes and profitable cells) in real-time with low latency and high throughput. These queries involve processing windows of thousands of records. Firstly, such processing demands efficient data structures and algorithms to minimize the processing overhead. Secondly, the system should partition data to evaluate them in parallel to make it scalable. In this paper, we present a set of data structures that we designed to evaluate the aforementioned queries with O(log n) time complexity and a data partitioning technique to evaluate them in parallel. We then evaluate our solution on a single machine as well as in a distributed setting in a commodity cluster of machines over a 1Gbps LAN. We were able to process the frequent routes query with the 173 million trips dataset within 5 minutes with less than 4 millisecond latency and the profitable cells query with same dataset within 11 minutes with less than 5 millisecond latency."
pub.1112608758,Achieving Low Latency Transactions for Geo-replicated Storage with Blotter,"Approximate computing has become a promising mechanism to trade off accuracy for efficiency. The idea behind approximate computing is to compute over a representative sample instead of the entire input dataset. Thus, approximate computing – based on the chosen sample size – can make a systematic trade-off between the output accuracy and computation efficiency. Unfortunately, the state-of-the-art systems for approximate computing primarily target batch analytics, where the input data remains unchanged during the course of computation. Thus, they are not well-suited for stream analytics. This motivated the design of StreamApprox– a stream analytics system for approximate computing. To realize this idea, an online stratified reservoir sampling algorithm is designed to produce approximate output with rigorous error bounds. Importantly, the proposed algorithm is generic and can be applied to two prominent types of stream processing systems: (1) batched stream processing such as Apache Spark Streaming, and (2) pipelined stream processing such as Apache Flink."
pub.1094737331,Dynamic Resource Management in a MapReduce-Style Platform for Fast Data Processing,"There is a recent interest in building MapReduce-style platforms for fast data processing, such as MapReduce online [2] and Muppet [5]. In this paper, we highlight the need for dynamic load management in a distributed data stream processing system and present Enorm, a MapReduce-style data stream processing platform with the focus on techniques to achieve dynamic resource management, i.e. the ability to dynamically balance the workload among the running instances and scale the resource usage according to the runtime workload fluctuations. The original MapReduce framework is designed for batched processing and dynamic scaling can only be achieved between batches. To address this problem, we propose a MapReduce-style computation framework and a set of corresponding adaptation strategies that can perform dynamic scaling on the fly with low processing latency."
pub.1101834439,Privacy-Preserving Data Analytics,"Real-time processing of user data streams in online services inadvertently creates tension between the users and analysts: users are looking for stronger privacy, while analysts desire for higher utility data analytics in real time. To resolve this tension, this paper describes the design, implementation, and evaluation of PrivApprox, a data analytics system for privacy-preserving stream processing. PrivApprox provides three important properties: (i) privacy, zero-knowledge privacy guarantee for users, a privacy bound tighter than the state-of-the-art differential privacy; (ii) utility, an interface for data analysts to systematically explore the trade-offs between the output accuracy (with error estimation) and the query execution budget; and (iii) latency, near real-time stream processing based on a scalable “synchronization-free” distributed architecture. The key idea behind PrivApprox is to combine two techniques together, namely, sampling (used for approximate computation) and randomized response (used for privacy-preserving analytics). The resulting combination is complementary – it achieves stronger privacy guarantees and also improves the performance for stream analytics."
pub.1163567450,"Multi-stream Adaptive Offloading of Joint Compressed Video Streams, Feature Streams, and Semantic Streams in Edge Computing Systems","Edge computing (EC) is a promising paradigm for serving latency-sensitive video applications. However, massive compressed video transmission and analysis require considerable bandwidth and computing resources, posing enormous challenges for current multimedia frameworks. Novel multi-stream frameworks that incorporate feature streams are more practical. The reason is that feature streams containing compact video frame feature data have a lower bitrate and better serve machine vision tasks. Nevertheless, feature extraction by devices increases the latency and energy consumption of local computing. Therefore, how to offload suitable streams according to video task requirements and system resources is a challenging issue. This paper studies EC-based multi-stream adaptive offloading. We model the multi-stream offloading and computation problem to maximize system utility by jointly optimizing offloading decisions, computation resource allocation, and video frame sampling rates. Frame sampling rates, processing latency, and energy consumption are considered in system utility modeling. The formulated optimization problem is a mixed-integer programming (MIP) problem. We propose an efficient algorithm to address this MIP problem. The proposed algorithm relies on the Hungarian algorithm and improved greedy Markov approximation. The simulation results validate our proposed algorithm’s superior performance."
pub.1159429570,An Optimization Method of Large-Scale Video Stream Concurrent Transmission for Edge Computing,"Concurrent access to large-scale video data streams in edge computing is an important application scenario that currently faces a high cost of network access equipment and high data packet loss rate. To solve this problem, a low-cost link aggregation video stream data concurrent transmission method is proposed. Data Plane Development Kit (DPDK) technology supports the concurrent receiving and forwarding function of multiple Network Interface Cards (NICs). The Q-learning data stream scheduling model is proposed to solve the load scheduling of multiple queues of multiple NICs. The Central Processing Unit (CPU) transmission processing unit was dynamically selected by data stream classification, as well as a reward function, to achieve the dynamic load balancing of data stream transmission. The experiments conducted demonstrate that this method expands the bandwidth by 3.6 times over the benchmark scheme for a single network port, and reduces the average CPU load ratio by 18%. Compared to the UDP and DPDK schemes, it lowers the average system latency by 21%, reduces the data transmission packet loss rate by 0.48%, and improves the overall system transmission throughput. This transmission optimization scheme can be applied in data centers and edge computing clusters to improve the communication performance of big data processing."
pub.1112608752,Privacy-Preserving Data Analytics,"Abstract
            Real-time processing of user data streams in online services inadvertently creates tension between the users and analysts: users are looking for stronger privacy, while analysts desire for higher utility data analytics in real time. To resolve this tension, this paper describes the design, implementation, and evaluation of PrivApprox, a data analytics system for privacy-preserving stream processing. PrivApprox provides three important properties: (i) privacy, zero-knowledge privacy guarantee for users, a privacy bound tighter than the state-of-the-art differential privacy; (ii) utility, an interface for data analysts to systematically explore the trade-offs between the output accuracy (with error estimation) and the query execution budget; and (iii) latency, near real-time stream processing based on a scalable “synchronization-free” distributed architecture. The key idea behind PrivApprox is to combine two techniques together, namely, sampling (used for approximate computation) and randomized response (used for privacy-preserving analytics). The resulting combination is complementary – it achieves stronger privacy guarantees and also improves the performance for stream analytics."
pub.1110420290,Network-Aware Grouping in Distributed Stream Processing Systems,"Distributed Stream Processing (DSP) systems have recently attracted much attention because of their ability to process huge volumes of real-time stream data with very low latency on clusters of commodity hardware. Existing workload grouping strategies in a DSP system can be classified into four categories (i.e. raw and blind, data skewness, cluster heterogeneity, and dynamic load-aware). However, these traditional stream grouping strategies do not consider network distance between two communicating operators. In fact, the traffic from different network channels makes a significant impact on performance. How to grouping tuples according to network distances to improve performance has been a critical problem.In this paper, we propose a network-aware grouping framework called Squirrel to improve the performance under different network distances. Identifying the network location of two communicating operators, Squirrel sets a weight and priority for each network channel. It introduces Weight Grouping to assign different numbers of tuples to each network channel according to channel’s weight and priority. In order to adapt to changes in network conditions, input load, resources and other factors, Squirrel uses Dynamic Weight Control to adjust network channel’s weight and priority online by analyzing runtime information. Experimental results prove Squirrel’s effectiveness and show that Squirrel can achieve 1.67x improvement in terms of throughput and reduce the latency by 47%."
pub.1035198500,Dispatching stream operators in parallel execution of continuous queries,"Data stream is a continuous, rapid, time-varying sequence of data elements which should be processed in an online manner. These matters are under research in Data Stream Management Systems (DSMSs). Single processor DSMSs cannot satisfy data stream applications’ requirements properly. Main shortcomings are tuple latency, tuple loss, and throughput. In our previous publications, we introduced parallel execution of continuous queries to overcome these problems via performance improvement, especially in terms of tuple latency. We scheduled operators in an event-driven manner which caused system performance reduction in periods between consecutive scheduling instances.In this paper, a continuous scheduling method (dispatching) is presented to be more compatible with the continuous nature of data streams as well as queries to improve system adaptivity and performance. In a multiprocessing environment, the dispatching method forces processing nodes (logical machines) to send partially-processed tuples to next machines with minimum workload to execute the next operator on them. So, operator scheduling is done continuously and dynamically for each tuple processed by each operator. The dispatching method is described, formally presented, and its correctness is proved. Also, it is modeled in PetriNets and is evaluated via simulation. Results show that the dispatching method significantly improves system performance in terms of tuple latency, throughput, and tuple loss. Furthermore, the fluctuation of system performance parameters (against variation of system and stream characteristics) diminishes considerably and leads to high adaptivity with the underlying system."
pub.1107030362,Performance Analysis of Large-scale Distributed Stream Processing Systems on the Cloud,"Real-time data processing is often a necessity as it can provide insights that have less value if discovered off-line or after the fact. However, large-scale stream processing systems are non-trivial to build and deploy. While there are many frameworks that allow users to create large-scale distributed systems, there remains many challenges in understanding the performance, cost of deployment and considerations and impact of potential (partial) outages on real-time systems performance. Our work considers the performance of Cloud-based stream processing systems in terms of back-pressure and expected utilization. The performance of an exemplar stream application is explored using different Cloud-based virtual machine resources and where the scale of deployment and cost benefits are taken into consideration in relation to the overall performance. To achieve this, we develop an algorithm based on queueing theory to predict the throughput and latency of stream data processing while supporting system stability. Our methodology for making fundamental measurements is applicable to mainstream stream processing frameworks such as Apache Storm and Heron. The method is especially suitable for large-scale distributed stream processing where jobs can run for extended time periods. We benchmark the performance of the system on the national research cloud of Australia (Nectar), and present a performance analysis based on estimating the overall effective utilization."
pub.1124553298,Time-SWAD: A Dataflow Engine for Time-based Single Window Stream Aggregation,"High throughput and low latency streaming aggregation is essential for many applications that analyze massive volumes of data in real-time. Incoming data need to be stored in a single sliding window before processing, in cases where incremental aggregations are wasteful or not possible at all; this puts tremendous pressure to the memory bandwidth. In addition, particular problems call for time-based windows, defined by a time-interval, where the amount of data per window may vary and as a consequence are more challenging to handle. This paper describes Time-SWAD, the first accelerator for time-based single-window stream aggregation. Time-SWAD is a dataflow engine (DFE), implemented on a Maxeler machine, offering high processing throughput, up to 150 Mtuples/sec, similar to related GPU systems, which however do not support both time-based and single windows. It uses a direct feed of incoming data from the network and has direct access to off-chip DRAM, enabling ultra-low processing latency of $1-10 \ \mu \text{sec}$, at least 4 orders of magnitude lower than software-based solutions."
pub.1163617858,Data Stream Clustering Using Embedding Dimension at Edge Gateway,"Data generated by Internet of Things (IoT) devices is typically transmitted to cloud data centers for processing. The huge volume of data generated by many IoT devices prompts exploring the possibility of processing the data at the edge gateway and transmitting summarized information to the cloud data center for more complex processing. Given the limited CPU and memory available at edge gateways, a promising and emerging paradigm is the use of stream based clustering and transmission of the summarized information in the form of cluster centers to the core. Motivated by these considerations, we present in this paper the results of our efforts in constructing a system that processes IoT data based on this promising paradigm. Constructing the system requires choosing a run-time for stream based processing. We find that Apache Storm allows for low latency processing with low memory requirements. We then propose a new algorithm, DenStreamED, that uses the embedding dimension to represent the incoming data into an embedded space in which stream based clustering is performed. Experiments on real data sets, carried out by executing DenStreamED in Apache Storm, show the efficacy of our proposed algorithm and the feasibility of the proposed paradigm in providing superior results while minimizing network traffic and improving scalability."
pub.1093391424,AEDSMS: Automotive Embedded Data Stream Management System,"Data stream management systems (DSMSs) are useful for the management and processing of continuous data at a high input rate with low latency. In the automotive domain, embedded systems use a variety of sensor data and communications from outside the vehicle to promote autonomous and safe driving. Thus, the software developed for these systems must be capable of handling large volumes of data and complex processing. At present, we are developing a platform for the integration and management of data in an automotive embedded system using a DSMS. However, compared with conventional DSMS fields, we have encountered new challenges such as precompiling queries when designing automotive systems (which demands time predictability), distributed stream processing in in-vehicle networks, and real-time scheduling and sensor data fusion by stream processing. Therefore, we developed an automotive embedded DSMS (AEDSMS) to address these challenges. The main contributions of the present study are: (1) a clear understanding of the challenges faced when introducing DSMSs into the automotive field; (2) the development of AEDSMS to tackle these challenges; and (3) an evaluation of AEDSMS during runtime using a driving assistance application."
pub.1149507822,Real-time analysis of market data leveraging Apache Flink,"In this paper, we present a solution to the DEBS 2022 Grand Challenge (GC). According to the GC requirements, the proposed software continuously observes notifications about financial instruments being traded, aiming to timely detect breakout patterns. Our solution leverages Apache Flink, an open-source, scalable stream processing platform, which allows us to process incoming data streams with low latency and exploit the parallelism offered by the underlying computing infrastructure."
pub.1152714909,Colocating Real-time Storage and Processing: An Analysis of Pull-based versus Push-based Streaming,"Real-time Big Data architectures evolved into specialized layers for handling
data streams' ingestion, storage, and processing over the past decade. Layered
streaming architectures integrate pull-based read and push-based write RPC
mechanisms implemented by stream ingestion/storage systems. In addition, stream
processing engines expose source/sink interfaces, allowing them to decouple
these systems easily. However, open-source streaming engines leverage workflow
sources implemented through a pull-based approach, continuously issuing read
RPCs towards the stream ingestion/storage, effectively competing with write
RPCs. This paper proposes a unified streaming architecture that leverages
push-based and/or pull-based source implementations for integrating
ingestion/storage and processing engines that can reduce processing latency and
increase system read and write throughput while making room for higher
ingestion. We implement a novel push-based streaming source by replacing
continuous pull-based RPCs with one single RPC and shared memory (storage and
processing handle streaming data through pointers to shared objects). To this
end, we conduct an experimental analysis of pull-based versus push-based design
alternatives of the streaming source reader while considering a set of stream
benchmarks and microbenchmarks and discuss the advantages of both approaches."
pub.1117526466,Event Stream Processing on Heterogeneous System Architecture,"Due to the widespread availability of general purpose GPUs, an integration of their processing capabilities into an event stream pipeline presents an exciting opportunity riddled with challenging requirements: Even though the single instruction multiple data (SIMD) model is a natural fit to answer long running event queries on high volume streams, those queries are usually associated with latency requirements that make transferring data to GPUs unfeasible. Traditionally, this challenge is solved through software by scheduling some tasks to the GPU and some to the CPU. However, the assumptions about transfer do not hold for widely adopted integrated GPUs (iGPUs), which directly share memory with the CPU. We develop a prototypical event processing framework based on the Heterogeneous System Architecture (HSA) and show that a variety of new HSA features enable iGPUs to be an affordable accelerator for a wide variety of event processing queries."
pub.1181907894,Mastering Real-Time Data Processing Applications : Optimization Strategies for Peak Performance,"This comprehensive article explores strategies for optimizing real-time data processing applications in the era of big data and distributed systems. It examines the growing demand for real-time analytics across industries and organizations' challenges in maintaining low latency and scalability. The article discusses key optimization techniques, including leveraging the Akka framework's actor model and supervision strategies, implementing efficient data ingestion through stream processing and partitioning, utilizing caching strategies, employing robust monitoring and auto-scaling mechanisms, ensuring fault tolerance through checkpointing and graceful degradation, and adopting asynchronous communication patterns. Throughout the article, real-world case studies and performance metrics demonstrate the significant improvements these strategies can bring to system throughput, latency, and resilience in various sectors such as finance, e-commerce, telecommunications, and manufacturing."
pub.1126031399,An experiment-driven performance model of stream processing operators in fog computing environments,"Data stream processing (DSP) is an interesting computation paradigm in geo-distributed infrastructures such as Fog computing because it allows one to decentralize the processing operations and move them close to the sources of data. However, any decomposition of DSP operators onto a geo-distributed environment with large and heterogeneous network latencies among its nodes can have significant impact on DSP performance. In this paper, we present a mathematical performance model for geo-distributed stream processing applications derived and validated by extensive experimental measurements. Using this model, we systematically investigate how different topological changes affect the performance of DSP applications running in a geo-distributed environment. In our experiments, the performance predictions derived from this model are correct within 2% even in complex scenarios with heterogeneous network delays between every pair of nodes."
pub.1024209568,"Scheduling parity checks for increased throughput in early-termination, layered decoding of QC-LDPC codes on a stream processor","A stream processor is a power-efficient, high-level-language programmable option for embedded applications that are computation intensive and admit high levels of data parallelism. Many signal-processing algorithms for communications are well matched to stream-processor architectures, including partially parallel implementations of layered decoding algorithms such as the turbo-decoding message-passing (TDMP) algorithm. Communication among clusters of functional units in the stream processor impose a latency cost during both the message-passing phase and the parity-check phase of the TDMP algorithm with early termination; the inter-cluster communications latency is a significant factor in limiting the throughput of the decoder. We consider two modifications of the schedule for the TDMP algorithm with early termination; each halves the communication required between functional-unit clusters of the stream processor in each iteration. We show that these can provide a substantial increase in the information throughput of the decoder without increasing the probability of error."
pub.1149968894,NETREACT: Distributed Event Detection in Sensor Data Streams with Disaggregated Packet Processing Pipelines,"A new phenomenon called in-network computing has recently emerged with the aim of offloading calculations beyond the traditional task of packet forwarding to network switches. One of the most studied in-network computing applications is processing of sensor data streams. Existing works such as FastReact focus on solving this problem using flexible SmartNICs. In this paper, we propose NETREACT: an improved ASIC-oriented design for distributed event detection in sensor data streams to achieve a disaggregated processing pipeline. In contrast to existing approaches, NETREACT distributes the event detection task among a set of switches while leveraging the capabilities of the Intel Tofino platform in terms of boosting throughput and reducing latency. The proposed event-rule disaggregation method has the advantage of overcoming the hardware resource constraints and improving the overall network performance."
pub.1093780866,The Best of Two Worlds: Integrating IBM InfoSphere Streams with Apache YARN,"The seamless confluence of data in motion and data at rest has the potential to redefine the Big Data analytics landscape in a diverse range of domains. To make this happen, existing data intensive computing frameworks need to be repurposed and integrated at control, data, and management levels. Towards this end, we present the system level integration of IBM InfoSphere Streams with Apache YARN. Our design leverages the key differentiating features of the two frameworks to blend high throughput batch-processing with near line-rate, low latency stream-processing. In addition, both frameworks are able to share resources and offer the same interfaces that their users are accustomed to. Using two real-world examples, we illustrate how such a system can be used in production."
pub.1116853465,Automatic Deadline-Oriented Sampling Method for Coarse-Grained Stream Processing,"Towards IoT-enabled world, the response time starting from data occurrence at the source until processed data delivery to the actuator is another QoS metric to be concerned. We call this requirement deadline. In coarse-grained stream processing, we could partially drop data in the streams with a specific drop rate to meet the deadline. This paper proposes an autonomic sampling method to decide the drop rate aiming at response time reduction oriented by the user-specified deadline. With consideration of processing and communicating time sharing among distributed worker nodes, we calculate a sampling number to satisfy the deadline requirement while preserving the maximum drop rate. The device will set a goal to maintain this sampling number for the next operating window. To evaluate the performance, we have implemented the proposed method on top of our previously-proposed stream processing engine called EdgeCEP. The results present that our proposed method can reduce almost 2-times latency and preserve a higher amount of request outputs compared to the fixed rate approach."
pub.1094116397,Big Data Analytics on High Velocity Streams: A Case Study,"Big data management is often characterized by three Vs: Volume, Velocity and Variety. While traditional batch-oriented systems such as Map Reduce are able to scale-out and process very large volumes of data in parallel, they also introduce some significant latency. In this paper, we focus on the second V (Velocity) of the Big Data triad; We present a case-study where we use a popular open-source stream processing engine (Storm) to perform real-time integration and trend detection on Twitter and Bitly streams. We describe our trend detection solution below and experimentally demonstrate that our architecture can effectively process data in real-time-even for high-velocity streams."
pub.1091752700,Discretized Streams: A Fault-Tolerant Model for Scalable Stream Processing,"Many big data applications need to act on data arriving in real time. However, current programming models for distributed stream processing are relatively low-level often leaving the user to worry about consistency of state across the system and fault recovery. Furthermore, the models that provide fault recovery do so in an expensive manner, requiring either hot replication or long recovery times. We propose a new programming model discretized streams (D-Streams), that offers a high-level functional API, strong consistency, and efficient fault recovery. D-Streams support a new recovery mechanism that improves efficiency over the traditional replication and upstream backup schemes in streaming databases-parallel recovery of lost state-and unlike previous systems also mitigate stragglers. We implement D-Streams as an extension to the Spark cluster computing engine that lets users seamlessly intermix streaming, batch and interactive queries. Our system can process over 60 million records/second at sub-second latency on 100 nodes."
pub.1129393925,Incremental stream query analytics,"Applications in the Internet of Things (IoT) create many data processing challenges because they have to deal with massive amounts of data and low latency constraints. The DEBS Grand Challenge 2020 specifies an IoT problem whose objective is to identify special type of events in a stream of electricity smart meters data. In this work, we present the Sequential Incremental DBSCAN-based Event Detection Algorithm (SINBAD), a solution based on an incremental version of the clustering algorithm DBSCAN and scenario specific data processing optimizations. SINBAD manages to calculate solutions up to 7 times faster and up to 26% more accurate than the baseline provided by the DEBS Grand Challenge."
pub.1000939159,Towards Scalable Architectures for Clickstream Data Warehousing,"Click-stream data warehousing has emerged as a monumental information management and processing challenge for commercial enterprises. Traditional solutions based on commercial DBMS technology often suffer from poor scalability and large processing latencies. One of the main problems is that click-stream data is inherently collected in a distributed manner, but in general these distributed click-stream logs are collated and pushed upstream in a centralized database storage repository, creating storage bottlenecks. In this paper, we propose a design of an ad-hoc retrieval system suitable for click-stream data warehouses, in which the data remains distributed and database queries are rewritten to be executed against the distributed data. The query rewrite does not involve any centralized control and is therefore highly scalable. The elimination of centralized control is achieved by supporting a restricted subset of SQL, which is sufficient for most click-stream data analysis. Evaluations conducted using both synthetic and real data establish the viability of this approach."
pub.1095518348,"Achieving Low Latency, Reduced Memory Footprint and Low Power Consumption with Data Streaming","In addition to its patient friendly properties, Ultrasound Imaging has become attractive because of its ability to provide images in real time. This low latency implementations allows for fast scanning and a quick time to establish a precise diagnostic using medical imaging. This study presents a framework aimed at the stream line processing of images, the ultimate goal being twofold. The first goal is to keep the latency as low as possible by processing the data as soon as there are enough samples available. The second goal is to reduce the required processing power per image. To achieve these goals, the framework allows several images to be processed simultaneously albeit in sequence. This allows taking advantage of periods where the processor is not fully loaded. This study shows how the latency is kept at the strict minimum while the required processing power is reduced when compared to a traditional image based implementation. The application runs a temporal adaptive filter on a hardware platform based on a Digital Signal Processors (DSP)."
pub.1131295525,Fuzzy-logic using Unary Bit-Stream Processing,"There is a growing attention to the theory of fuzzy-logic and its applications. Efficient hardware design of the fuzzy-inference engine has become necessary for high-performance applications. Considering the facts that fuzzy-logic variables have truth values in the [0, 1] interval and fuzzy controllers include minimum and maximum operations, this work proposes to apply the concept of unary processing to the platform of fuzzy-logic. In unary processing, data in the [0, 1] interval is encoded as bit-stream with the value defined by the frequency of 1s. Operations such as minimum and maximum functions can be implemented using simple logic gates. Latency, however, has been an important issue in the unary designs. To mitigate the latency, the proposed design processes right-aligned bit-streams. A one-hot decoder is used for fast detection of the bit-stream with maximum value. Implementing a fuzzy-inference engine with 81 fuzzy-inference rules, the proposed architecture provides 82%, 46%, and 67% saving in the hardware area, power and energy consumption, respectively, and 94% reduction in the number of used LUTs compared to conventional binary implementation."
pub.1137312092,WindFlow: High-Speed Continuous Stream Processing With Parallel Building Blocks,"Nowadays, we are witnessing the diffusion of Stream Processing Systems (SPSs) able to analyze data streams in near realtime. Traditional SPSs like Storm and Flink target distributed clusters and adopt the continuous streaming model, where inputs are processed as soon as they are available while outputs are continuously emitted. Recently, there has been a great focus on SPSs for scale-up machines. Some of them (e.g., BriskStream) still use the continuous model to achieve low latency. Others optimize throughput with batching approaches that are, however, often inadequate to minimize latency for live-streaming applications. Our contribution is to show a novel software engineering approach to design the runtime system of SPSs targeting multicores, with the aim of providing a uniform solution able to optimize throughput and latency. The approach has a formal nature based on the assembly of components called building blocks, whose composition allows optimizations to be easily expressed in a compositional manner. We use this methodology to build a new SPS called WindFlow. Our evaluation showcases the benefits of WindFlow: it provides lower latency than SPSs for continuous streaming, and can be configured to optimize throughput, to perform similarly and even better than batch-based scale-up SPSs."
pub.1043825376,Nova,"This paper describes a workflow manager developed and deployed at Yahoo called Nova, which pushes continually-arriving data through graphs of Pig programs executing on Hadoop clusters. (Pig is a structured dataflow language and runtime for the Hadoop map-reduce system.) Nova is like data stream managers in its support for stateful incremental processing, but unlike them in that it deals with data in large batches using disk-based processing. Batched incremental processing is a good fit for a large fraction of Yahoo's data processing use-cases, which deal with continually-arriving data and benefit from incremental algorithms, but do not require ultra-low-latency processing."
pub.1093836285,Governor: Smoother Stream Processing Through Smarter Backpressure,"Distributed micro-batch streaming systems, such as Spark Streaming, employ backpressure mechanisms to maintain a stable, high throughput stream of results that is robust to runtime dynamics. Checkpointing in stream processing systems is a process that creates periodic snapshots of the data flow for fault tolerance. These checkpoints can be expensive to produce and add significant delay to the data processing. The checkpointing latencies are also variable at runtime, which in turn compounds the challenges for the backpressure mechanism to maintain stable performance. Consequently, the interferences caused by the checkpointing may degrade system performance significantly, even leading to exhaustion of resources or system crash. This paper describes Governor, a controller that factors the checkpointing costs into the backpressure mechanism. It not only guarantees a smooth execution of the stream processing but also reduces the throughput loss caused by interferences of the checkpointing. Our experimental results on four stateful streaming operators with real-world data sources demonstrate that Governor implemented in Spark Streaming can achieve 26% throughput improvement, and lower the risk of system crash, with negligible overhead."
pub.1094771410,Spark-Based Anomaly Detection Over Multi-Source VMware Performance Data in Real-Time,"Anomaly detection refers to identifying the patterns in data that deviate from expected behavior. These nonconforming patterns are often termed as outliers, malwares, anomalies or exceptions in different application domains. This paper presents a novel, generic real-time distributed anomaly detection framework for multi-source stream data. As a case study, we have decided to detect anomaly for multi-source VMware-based cloud data center. The framework monitors VMware performance stream data (e.g., CPU load, memory usage, etc.) continuously. It collects these data simultaneously from all the VM wares connected to the network. It notifies the resource manager to reschedule its resources dynamically when it identifies any abnormal behavior of its collected data. We have used Apache Spark, a distributed framework for processing performance stream data and making prediction without any delay. Spark is chosen over a traditional distributed framework (e.g., Hadoop and MapReduce, Mahout, etc.) that is not ideal for stream data processing. We have implemented a flat incremental clustering algorithm to model the benign characteristics in our distributed Spark based framework. We have compared the average processing latency of a tuple during clustering and prediction in Spark with Storm, another distributed framework for stream data processing. We experimentally find that Spark processes a tuple much quicker than Storm on average."
pub.1090855694,Scalable Online Analytics on Cloud Infrastructures,"The need for low latency analysis of high velocity real time continuous data streams has led to the emergence of Stream Processing Systems (SPSs). Contemporary SPSs allow a stream processing application to be hosted on Cloud infrastructures and dynamically scaled so as to adapt to the fluctuating data rates. However, the run time scalability incorporated in these SPSs are in their early adaptations and are based on simple local/global threshold based controls. This work studies the issues with the local and global auto scaling techniques that may lead to performance inefficiencies in real time traffic analysis on Cloud platforms and presents an efficient hybrid auto scaling strategy StreamScale which addresses the identified issues. The proposed StreamScale auto-scaling algorithm accounts for the gaps in the local/global scaling approaches and effectively identifies (de)parallelization opportunities in stream processing applications for maintaining QoS at reduced costs. Simulation based experimental evaluation on representative stream application topologies indicate that the proposed StreamScale auto-scaling algorithm exhibits better performance in comparison to both local and global auto-scaling approaches."
pub.1002601151,Scale Out Parallel and Distributed CDR Stream Analytics,"In the era of information explosion, huge amount of data are generated from various sensing devices continuously, which are often too low level for analytics purpose, and too massive to load to data-warehouses for filtering and summarizing with the reasonable latency. Distributed stream analytics for multilevel abstraction is the key to solve this problem.We advocate a distributed infrastructure for CDR (Call Detail Record) stream analytics in the telecommunication network where the stream processing is integrated into the database engine, and carried out in terms of continuous querying; the computation model is based on network-distributed (rather than clustered) Map-Reduce scheme. We propose the window based cooperation mechanism for having multiple engines synchronized and cooperating on the data falling in a common window boundary, based on time, cardinality, etc. This mechanism allows the engines to cooperate window by window without centralized coordination. We further propose the quantization mechanism for integrating the discretization and abstraction of continuous-valued data, for efficient and incremental data reduction, and in turn, network data movement reduction. These mechanisms provide the key roles in scaling out CDR stream analytics.The proposed approach has been integrated into the PostgreSQL engine.Our preliminary experiments reveal its merit for large-scale distributed stream processing."
pub.1125630752,A Holistic Stream Partitioning Algorithm for Distributed Stream Processing Systems,"The performances of modern distributed stream processing systems are critically affected by the distribution of the load across workers. Skewed data streams in real world are very common and pose a great challenge to these systems, especially for stateful applications. Key splitting, which allows a single key to be routed to multiple workers, is a great idea to achieve good balance of load in the cluster. However, it comes with the cost of increased memory consumption and computation overhead as well as network communication. In this paper, we present a new definition of metric to model the cost of key splitting for intra-operator parallelism in stream processing systems and provide a novel perspective to reduce replication factor while keeping both overall load imbalance and processing latency low. Similar to previous work, our approach treats the head and the tail of the distribution differently in order to reduce memory requirements. For the head, it uses our proposed notion of regional load imbalance to decide dynamically whether to make one more worker responsible for the heavy hitter or not. For the tail, it simply uses hash partitioning to keep the size of the routing table for the head as small as possible. Extensive experimental evaluation demonstrates that our approach provides superior performance compared to the state-of-the-art partitioning algorithms in terms of load imbalance, replication factor and latency over different levels of skewed stream distributions."
pub.1120196472,Applying Security to a Big Stream Cloud Architecture for the Internet of Things,"The Internet of Things (IoT) is expected to interconnect billions (around 50 by 2020) of heterogeneous sensor/actuator-equipped devices denoted as “Smart Objects” (SOs), characterized by constrained resources in terms of memory, processing, and communication reliability. Several IoT applications have real-time and low-latency requirements and must rely on architectures specifically designed to manage gigantic streams of information (in terms of number of data sources and transmission data rate). We refer to “Big Stream” as the paradigm which best fits the selected IoT scenario, in contrast to the traditional “Big Data” concept, which does not consider real-time constraints. Moreover, there are many security concerns related to IoT devices and to the Cloud. In this paper, we analyze security aspects in a novel Cloud architecture for Big Stream applications, which efficiently handles Big Stream data through a Graph-based platform and delivers processed data to consumers, with low latency. The authors detail each module defined in the system architecture, describing all refinements required to make the platform able to secure large data streams. An experimentation is also conducted in order to evaluate the performance of the proposed architecture when integrating security mechanisms."
pub.1019694104,Applying Security to a Big Stream Cloud Architecture for the Internet of Things,"<p>The Internet of Things (IoT) is expected to interconnect billions (around 50 by 2020) of heterogeneous sensor/actuator-equipped devices denoted as “Smart Objects” (SOs), characterized by constrained resources in terms of memory, processing, and communication reliability. Several IoT applications have real-time and low-latency requirements and must rely on architectures specifically designed to manage gigantic streams of information (in terms of number of data sources and transmission data rate). We refer to “Big Stream” as the paradigm which best fits the selected IoT scenario, in contrast to the traditional “Big Data” concept, which does not consider real-time constraints. Moreover, there are many security concerns related to IoT devices and to the Cloud. In this paper, we analyze security aspects in a novel Cloud architecture for Big Stream applications, which efficiently handles Big Stream data through a Graph-based platform and delivers processed data to consumers, with low latency. The authors detail each module defined in the system architecture, describing all refinements required to make the platform able to secure large data streams. An experimentation is also conducted in order to evaluate the performance of the proposed architecture when integrating security mechanisms.</p>"
pub.1094933960,Media Streams Planning with Transcoding,"High-quality video, both uncompressed and compressed with relatively low compression ratio, is used in interactive environments, where minimum end-to-end latency is required. The media streams planning solves multi-point data routing for such applications even when bit rate of streams is comparable to capacity of network links. Recent advances in acceleration of high-throughput low-latency video processing enable on-the-fly video transcoding even for 4K and 8K video streams, thus allowing individual adaptation of data stream for each recipient without affecting the others. Stimulated by these advances, we present a novel media streams planning with transcoding on network nodes, and a mixed integer programming approach to solve the problem optimally. Our approach also enhances traditional end-to-end network model of the whole system with known topology of certain network regions. Our solution was implemented in the Co Universe, which is middleware for orchestration of collaborative environments. Measurements show that performance of the new solver is sufficient for interactive solving of most practical use-cases, thus enabling self-organization of high-performance collaborative environments."
pub.1105021378,Mitigating Network Side Channel Leakage for Stream Processing Systems in Trusted Execution Environments,"A crucial concern regarding cloud computing is the confidentiality of sensitive data being processed in the cloud. Trusted Execution Environments (TEEs), such as Intel Software Guard extensions (SGX), allow applications to run securely on an untrusted platform. However, using TEEs alone for stream processing is not enough to ensure privacy as network communication patterns may leak information about the data. This paper introduces two techniques -- anycast and multicast --for mitigating leakage at inter-stage communications in streaming applications according to a user-selected mitigation level. These techniques aim to achieve network data obliviousness, i.e., communication patterns should not depend on the data. We implement these techniques in an SGX-based stream processing system. We evaluate the latency and throughput overheads, and the data obliviousness using three benchmark applications. The results show that anycast scales better with input load and mitigation level, and provides better data obliviousness than multicast."
pub.1099864758,Fixed-Latency Gigabit Serial Links in a Xilinx FPGA for the Upgrade of the Muon Spectrometer at the ATLAS Experiment,"We present an implementation of fixed-latency gigabit serial links in a low-cost Xilinx field-programmable gate array. The implementation is targeted for a data packet router in the upgrade of the ATLAS muon spectrometer. The router serves as a packet switch. It handles up to 12 serial inputs at 4.8 Gbps from on-detector electronics and four 4.8-Gbps outputs to the trigger processing circuits. The input serial streams are deserialized and aligned to a common clock domain for NULL suppression and data packet forwarding. Gigabit transceivers are used in the processing, and a scheme is developed to maintain low and fixed-latency packet multiplexing through the router. We analyze the latency of the scheme and demonstrate its performance in a setup similar to that of the final detector arrangement."
pub.1112141659,Efficient deadline-aware scheduling for the analysis of Big Data streams in public Cloud,"The emergence of Big Data has had a profound impact on how data are analyzed. Open source distributed stream processing platforms have gained popularity for analyzing streaming Big Data as they provide low latency required for streaming Big Data applications using Cloud resources. However, existing resource schedulers are still lacking the efficiency and deadline meeting that Big Data analytical applications require. Recent works have already considered streaming Big Data characteristics to improve the efficiency and the likelihood of deadline meeting for scheduling in the platforms. Nevertheless, they have not taken into account the specific attributes of analytical application, public Cloud utilization cost and delays caused by performance degradation of leasing public Cloud resources. This study, therefore, presents BCframework, an efficient deadline-aware scheduling framework used by streaming Big Data analysis applications based on public Cloud resources. BCframework proposes a scheduling model which considers public Cloud utilization cost, performance variation, deadline meeting and latency reduction requirements of streaming Big Data analytical applications. Furthermore, it introduces two operator scheduling algorithms based on both a novel partitioning algorithm and an operator replication method. BCframework is highly adaptable to the fluctuation of streaming Big Data and the performance degradation of public Cloud resources. Experiments with the benchmark and real-world queries show that BCframework can significantly reduce the latency and utilization cost and also minimize deadline violations and provisioned virtual machine instances."
pub.1162914770,Complex Event Recognition with Allen Relations,"Contemporary applications require the processing of large, high-velocity streams of symbolic events derived from sensor data. A complex event recognition (CER) system processes these symbolic events online and reports the satisfaction of complex event patterns with minimal latency. We extend an Event Calculus dialect optimised for online CER with Allen’s interval algebra, in order to provide more accurate event patterns. We demonstrate the effectiveness of our system on real data streams from maritime situational awareness."
pub.1100410077,RASP: Real-Time Network Analytics with Distributed NoSQL Stream Processing,"In this paper we present RASP, a system that combines latest distributed stream processing and NoSQL engines to enable the real-time low latency storage and joining of incoming data streams with external datasets of arbitrary sizes through an extensible, SQL compliant manner. We achieve low latency, real time execution by employing the Kafka and Storm frameworks to join incoming tuples as they arrive, while the denormalized result is being stored in HBase, a distributed NoSQL engine with the use of Phoenix, a framework that fully supports SQL. We fine-tune the topology execution to achieve maximum performance and we also apply a set of optimizations both in the HBase storage and the Phoenix SQL execution framework. We use RASP to solve a network analytics problem using real data. RASP performs its computations utilizing an extensible pipeline of Storm bolts that incrementally augment incoming tuples with the execution of different algorithms. We deploy our system over an IaaS cloud and we evaluate its performance for various workloads, cluster sizes and configurations, where we show that in some cases RASP achieves a throughput increase of more than 140% and a latency drop of more than 65 % compared to a vanilla setting."
pub.1093907351,Proactive Plan-Based Continuous Query Processing over Diverse SPARQL Endpoints,"Although the emergence of SPARQL endpoints that allow end-users and applications to query the RDF data they want, continuous processing of building a very large query over diverse SPARQL endpoints requires a sophisticated method. However, current RDF Stream Processing (RSP) applications are limited in terms of scalability and administrative autonomy, due to their tight-coupled data sources (e.g., RDF streams) and being unable to coordinate with existing SPARQL engines. In this paper, we propose a novel continous query processing that is equipped with a proactive adaptation for enhancing a plan-based policy, pulling RDF data periodically from remote sources. Our proactive adaptation forecasts the future update pattern of a source, and decides the best action that guarantees the improved data freshness and efficient system workload. We verify the proposed approach in terms of data adaptability, detection latency, and transmission cost in distributed settings."
pub.1018969488,CEPSim: Modelling and simulation of Complex Event Processing systems in cloud environments," The emergence of Big Data has had profound impacts on how data are stored and processed. As technologies created to process continuous streams of data with low latency, Complex Event Processing (CEP) and Stream Processing (SP) have often been related to the Big Data velocity dimension and used in this context. Many modern CEP and SP systems leverage cloud environments to provide the low latency and scalability required by Big Data applications, yet validating these systems at the required scale is a research problem per se. Cloud computing simulators have been used as a tool to facilitate reproducible and repeatable experiments in clouds. Nevertheless, existing simulators are mostly based on simple application and simulation models that are not appropriate for CEP or for SP. This article presents CEPSim, a simulator for CEP and SP systems in cloud environments. CEPSim proposes a query model based on Directed Acyclic Graphs (DAGs) and introduces a simulation algorithm based on a novel abstraction called event sets. CEPSim is highly customizable and can be used to analyse the performance and scalability of user-defined queries and to evaluate the effects of various query processing strategies. Experimental results show that CEPSim can simulate existing systems in large Big Data scenarios with accuracy and precision."
pub.1093600579,Geelytics: Enabling on-demand Edge Analytics Over Scoped Data Sources,"Large-scale Internet of Things (IoT) systems typically consist of a large number of sensors and actuators distributed geographically in a physical environment. To react fast on real time situations, it is often required to bridge sensors and actuators via real-time stream processing close to IoT devices. Existing stream processing platforms like Apache Storm and S4 are designed for intensive stream processing in a cluster or in the Cloud, but they are unsuitable for large scale IoT systems in which processing tasks are expected to be triggered by actuators on-demand and then be allocated and performed in a Cloud-Edge environment. To fill this gap, we designed and implemented a new system called Geelytics, which can enable on-demand edge analytics over scoped data sources via IoT-friendly interfaces to sensors and actuators. This paper presents its design, implementation, interfaces, and core algorithms. Three example applications have been built to showcase the potential of Geelytics in enabling advanced IoT edge analytics. Our preliminary evaluation results demonstrate that we can reduce the bandwidth cost by 99 % in a face detection example, achieve less than 10 milliseconds reacting latency and about 1.5 seconds startup latency in an outlier detection example, and also save 65 % duplicated computation cost via sharing intermediate results in a data aggregation example."
pub.1094552141,A tightly coupled hybrid SIMD/SISD system,"This paper presents a proposed processing system based on a single instruction stream, multiple data stream (SIMD) architecture that is tightly coupled with a host processor. The system is designed with the objective of maintaining a low-latency SIMD instruction issue by reducing the overhead introduced by the path between the host processor and SIMD machine. The target applications are those that need a mixture of single instruction, single data (SISD) and SIMD instructions."
pub.1169451418,Daedalus: Self-Adaptive Horizontal Autoscaling for Resource Efficiency of Distributed Stream Processing Systems,"Distributed Stream Processing (DSP) systems are capable of processing large
streams of unbounded data, offering high throughput and low latencies. To
maintain a stable Quality of Service (QoS), these systems require a sufficient
allocation of resources. At the same time, over-provisioning can result in
wasted energy and high operating costs. Therefore, to maximize resource
utilization, autoscaling methods have been proposed that aim to efficiently
match the resource allocation with the incoming workload. However, determining
when and by how much to scale remains a significant challenge. Given the
long-running nature of DSP jobs, scaling actions need to be executed at
runtime, and to maintain a good QoS, they should be both accurate and
infrequent. To address the challenges of autoscaling, the concept of
self-adaptive systems is particularly fitting. These systems monitor themselves
and their environment, adapting to changes with minimal need for expert
involvement.
  This paper introduces Daedalus, a self-adaptive manager for autoscaling in
DSP systems, which draws on the principles of self-adaption to address the
challenge of efficient autoscaling. Daedalus monitors a running DSP job and
builds performance models, aiming to predict the maximum processing capacity at
different scale-outs. When combined with time series forecasting to predict
future workloads, Daedalus proactively scales DSP jobs, optimizing for maximum
throughput and minimizing both latencies and resource usage. We conducted
experiments using Apache Flink and Kafka Streams to evaluate the performance of
Daedalus against two state-of-the-art approaches. Daedalus was able to achieve
comparable latencies while reducing resource usage by up to 71%."
pub.1152650110,SDN-enabled Resource Provisioning Framework for Geo-Distributed Streaming Analytics,"
                    Geographically distributed (geo-distributed) datacenters for stream data processing typically comprise multiple edges and core datacenters connected through
                    Wide-Area Network (WAN)
                    with a master node responsible for allocating tasks to worker nodes. Since WAN links significantly impact the performance of distributed task execution, the existing task assignment approach is unsuitable for distributed stream data processing with low latency and high throughput demand. In this paper, we propose SAFA, a resource provisioning framework using the
                    Software-Defined Networking (SDN)
                    concept with an SDN controller responsible for monitoring the WAN, selecting an appropriate subset of worker nodes, and assigning tasks to the designated worker nodes. We implemented the data plane of the framework in P4 and the control plane components in Python. We tested the performance of the proposed system on Apache Spark, Apache Storm, and Apache Flink using the Yahoo! streaming benchmark on a set of custom topologies. The results of the experiments validate that the proposed approach is viable for distributed stream processing and confirm that it can improve at least 1.64× the processing time of incoming events of the current stream processing systems.
                  "
pub.1047717752,Dynamic Resource Management In a Massively Parallel Stream Processing Engine,"The emerging interest in Massively Parallel Stream Processing Engines (MPSPEs), which are able to process long-standing computations over data streams with ever-growing velocity at a large-scale cluster, calls for efficient dynamic resource management techniques to avoid any waste of resources and/or excessive processing latency. In this paper, we propose an approach to integrate dynamic resource management with passive fault-tolerance mechanisms in a MPSPE so that we can harvest the checkpoints prepared for failure recovery to enhance the efficiency of dynamic load migrations. To maximize the opportunity of reusing checkpoints for fast load migration, we formally define a checkpoint allocation problem and provide a pragmatic algorithm to solve it. We implement all the proposed techniques on top of Apache Storm, an open-source MPSPE, and conduct extensive experiments using a real dataset to examine various aspects of our techniques. The results show that our techniques can greatly improve the efficiency of dynamic resource reconfiguration without imposing significant overhead or latency to the normal job execution."
pub.1182030475,SwiftFrame: Developing Low-latency Near Real-time Response Framework,"In today’s era, handling of big data poses significant challenges due to its massive volume and diverse formats. A rapid processing of such diverse data formats, spanning vast volumes, is essential for delivering swift and effective real-time responses. This paper proposes a near real-time response framework named as SwiftFrame for low-latency, high-speed applications. Leveraging Docker, the proposed architecture seamlessly deploys and manages Apache Kafka, Apache Zookeeper, and PostgreSQL. Apache Flink efficiently processes data streams from Kafka, orchestrates tasks, and stores results in PostgreSQL. Grafana enables real-time visualization, offering a comprehensive monitoring interface. The proposed architecture, SwiftFrame, is designed to handle modern data processing requirements, ensures low latency and high throughput, suitable for diverse high-speed applications. In the experimental setup, four different scenarios have been considered and each scenario has repeated 5 times. The experimental results show that the proposed architecture outperforms in terms of latency and throughput achieving an average latency of 0.13 sec and an average throughput of 8.7 records per second. The future research directions of this paper are also highlighted."
pub.1100899517,Scalable Linux Container Provisioning in Fog and Edge Computing Platforms,"The tremendous increase in the number of mobile devices and the proliferation of all kinds of new types of sensors is creating new value opportunities by analyzing, developing insights from, and actuating upon large volumes of data streams generated at the edge of the network. While general purpose processing required to unleash this value is abundant in Cloud datacenters, bringing raw IoT data streams to the Cloud poses critical challenges, including: (i) regulatory constraints related to data sensitivity, (ii) significant bandwidth costs and (iii) latency barriers inhibiting near-real-time applications. Edge Computing aspires to extend the traditional cloud model to the “edge of the network”, to deliver low-latency, bandwidth-efficiencies and controlled privacy. For all the commonalities between the two models, transitioning the provisioning and orchestration of a distributed analytics platform from Cloud to Edge is not trivial. The two models present totally different cost structures such as price of bandwidth, data communication latency, power density and availability. In this paper, we address the challenge associated with transitioning scalable provisioning from Cloud to distributed Edge platforms. We identify current scalability challenges in Linux container provisioning at the Edge; we propose a novel peer-to-peer model taking on them; we present a prototype of this model designed for and tested on real Edge testbeds, and we report a scalability evaluation on a scale-out virtualized platform. Our results demonstrate significant savings in terms of provisioning latency and bandwidth utilization."
pub.1123036282,Real-Time Stream Processing in Social Networks with RAM3S,"The avalanche of (both user- and device-generated) multimedia data published in online social networks poses serious challenges to researchers seeking to analyze such data for many different tasks, like recommendation, event recognition, and so on. For some such tasks, the classical “batch” approach of big data analysis is not suitable, due to constraints of real-time or near-real-time processing. This led to the rise of stream processing big data platforms, like Storm and Flink, that are able to process data with a very low latency. However, this complicates the task of data analysis since any implementation has to deal with the technicalities of such platforms, like distributed processing, synchronization, node faults, etc. In this paper, we show how the RAM 3 S framework could be profitably used to easily implement a variety of applications (such as clothing recommendations, job suggestions, and alert generation for dangerous events), being independent of the particular stream processing big data platforms used. Indeed, by using RAM 3 S, researchers can concentrate on the development of their data analysis application, completely ignoring the details of the underlying platform."
pub.1170562540,Streamlining trajectory map-matching: a framework leveraging spark and GPU-based stream processing,"Real-time online trajectory map-matching has emerged as a critical component in the era of location-based services (LBS) and intelligent transportation systems (ITS). It refers to the process of aligning a user’s GPS trajectory data with the corresponding road network in real-time. This technology has significant implications for various industries and applications. As our reliance on LBS and ITS continues to grow, the demand for faster, more accurate, and more reliable trajectory map-matching methods becomes increasingly important. Contemporary online map-matching predominantly employs stream processing techniques. Based on stream processing frameworks, we propose a heterogeneous hybrid architecture for map-matching. The architecture integrates Spark Streaming and graphics processing unit (GPU) heterogeneous computing for the first time. The hidden Markov model is employed as the map-matching algorithm, and Spark Streaming serves as the distributed processing platform. We conduct map-matching experiments using a GPS taxi trajectory dataset in Beijing’s Haidian District. The results demonstrate that in comparison to other analogous research, our framework’s performance has increased by over ten times, possessing a superior data processing capability and lower latency. This research provides a novel approach of stream-based heterogeneous computation for processing large-scale geographic data."
pub.1141370779,Resource-Efficient Visual Multiobject Tracking on Embedded Device,"Multiobject tracking (MOT) is a crucial technology for security surveillance, which is computationally intensive due to the requirement of processing a large number of video streams within low latency in practice. The input video streams of MOT are processed on a cloud computing center with abundant computational capability, posing heavy pressures on delivering video streams to the cloud. Recent advances in the Internet-of-Things (IoT) technology provide edge-computing-based solutions for video analytics at scale. However, the gap between MOT’s high computational capability demand and IoT devices’ resource-constrained nature remains significant. In this article, a resource-efficient MOT (REMOT) method is proposed for real-time surveillance on IoT embedded devices, including an affinity measurement based on an appearance model with angular triplet loss and a motion association that substitutes the time-consuming graph-based data association stage. Considering the tradeoff between latency and accuracy, we design an optimization strategy on the parallel processing of deep learning models’ layers to accelerate the inference speed with less accuracy loss. Besides, we employ a model compression strategy for model size reduction. Experiments on MOT16 and MOT17 benchmarks demonstrate that REMOT reduces 2.4$\times $ latency compared with the original implementation and achieves a running speed of 81 frames per second (fps) on an embedded device with only a marginal accuracy loss (6%), which meets the requirements of real-time processing and low-latency response for surveillance."
pub.1032303003,Enhanced stream processing in a DBMS kernel,"Continuous query processing has emerged as a promising query processing paradigm with numerous applications. A recent development is the need to handle both streaming queries and typical one-time queries in the same application. For example, data warehousing can greatly benefit from the integration of stream semantics, i.e., online analysis of incoming data and combination with existing data. This is especially useful to provide low latency in data-intensive analysis in big data warehouses that are augmented with new data on a daily basis. However, state-of-the-art database technology cannot handle streams efficiently due to their ""continuous"" nature. At the same time, state-of-the-art stream technology is purely focused on stream applications. The research efforts are mostly geared towards the creation of specialized stream management systems built with a different philosophy than a DBMS. The drawback of this approach is the limited opportunities to exploit successful past data processing technology, e.g., query optimization techniques. For this new problem we need to combine the best of both worlds. Here we take a completely different route by designing a stream engine on top of an existing relational database kernel. This includes reuse of both its storage/execution engine and its optimizer infrastructure. The major challenge then becomes the efficient support for specialized stream features. This paper focuses on incremental window-based processing, arguably the most crucial streamspecific requirement. In order to maintain and reuse the generic storage and execution model of the DBMS, we elevate the problem at the query plan level. Proper optimizer rules, scheduling and intermediate result caching and reuse, allow us to modify the DBMS query plans for efficient incremental processing. We describe in detail the new approach and we demonstrate efficient performance even against specialized stream engines, especially when scalability becomes a crucial factor."
pub.1095439652,Clustering Events on Streams using Complex Context Information,"Monitoring applications play an increasingly important role in many domains. They detect events in monitored systems and take actions such as invoke a program or notify an administrator. Often administrators must then manually investigate events to figure out the source of a problem. Stream processing engines (SPEs) are general purpose data management systems for monitoring applications. They provide low-latency stream processing but have limited or no support for manual event investigation. In this paper, we propose a new technique for an SPE to support event investigation by automatically classifying events on streams. Unlike previous stream clustering algorithms, our approach takes into account complex user-defined contexts for events. Our approach comprises three key components: an event context data model, a distance measure for event contexts, and an online clustering algorithm for event contexts. We evaluate our approach using synthetic data and show that complex context information can improve online event classification."
pub.1171379831,A High-Speed Asynchronous Data I/O Method for HEPS,"The High Energy Photon Source (HEPS) is expected to produce a substantial volume of data, lead to immense data I/O pressure during computing. Inefficient data I/O can significantly impact computing performance. To address this challenge, firstly, we have developed a data I/O framework for HEPS. This framework consists of three layers: data channel layer, distributed memory management layer, and I/O interface layer. It mask the underlying data differences in formats and sources, while implementing efficient I/O methods. Additionally, it supports both stream computing and batch computing. Secondly, we have designed a data processing pipeline scheme aimed at reducing I/O latency and optimizing I/O bandwidth utilization during the processing of high-throughput data. This involves breaking down the computing task into several stages, including data loading, data pre-processing, data processing, and data writing, which are executed asynchronously and in parallel. Finally, we introduce the design of stream data I/O process. The primary objective of stream data I/O is to enable real-time online processing of raw data, avoiding I/O bottlenecks caused by disk storage. This approach ensures the stability of data transmission and integrates distributed memory management to guarantee data integrity in memory."
pub.1025351576,Measuring Performance of Complex Event Processing Systems,"Complex Event Processing (CEP) or stream data processing are becoming increasingly popular as the platform underlying event-driven solutions and applications in industries such as financial services, oil & gas, smart grids, health care, and IT monitoring. Satisfactory performance is crucial for any solution across these industries. Typically, performance of CEP engines is measured as (1) data rate, i.e., number of input events processed per second, and (2) latency, which denotes the time it takes for the result (output events) to emerge from the system after the business event (input event) happened. While data rates are typically easy to measure by capturing the numbers of input events over time, latency is less well defined. As it turns out, a definition becomes particularly challenging in the presence of data arriving out of order. That means that the order in which events arrive at the system is different from the order of their timestamps. Many important distributed scenarios need to deal with out-of-order arrival because communication delays easily introduce disorder.With out-of-order arrival, a CEP system cannot produce final answers as events arrive. Instead, time first needs to progress enough in the overall system before correct results can be produced. This introduces additional latency beyond the time it takes the system to perform the processing of the events. We denote the former as information latency and the latter as system latency. This paper discusses both types of latency in detail and defines them formally without depending on particular semantics of the CEP query plans. In addition, the paper suggests incorporating these definitions as metrics into the benchmarks that are being used to assess and compare CEP systems."
pub.1138114795,A 26-32 GHz Dual-Polarization Receiver with Autonomous Polarization Alignment for Fast-Response Mm-Wave MIMO Links in Highly Dynamic Mobile Environments,"Next-generation wireless communication systems mandate extreme data-rates (multi-Gb/s), ultra-low latency, and fast response, necessitating the use of mm-Wave frequency bands. To maximize the channel capacity, polarization MIMOs are gaining increasing attentions, which simultaneously transmit two independent data streams using the orthogonal polarization modes on the same frequency channel [1–8]. On the other hand, polarization misalignment between the transmitting and receiving antennas will result in cross-polarization contamination between both signal streams as well polarization mismatch signal loss. Traditionally, this is corrected using backend digital processing, which is sufficient for applications in static or slowly varying environments and fixed wireless accesses, such as, backhaul, customer premise equipment (CPE), and data center. However, for future latency-sensitive mobile applications especially in highly dynamic and rapidly changing channel environments, real-time recurrent polarization realignments are needed, and the backend processing time will cause major penalty on link latency and response time. To address this challenge, we present a polarization MIMO receiver (RX) architecture that utilizes frontend mixed-domain feedback loops on signal polarizations; without any backend DSP computation, the RX achieves rapid autonomous polarization alignment in micro-seconds $(\mu s)$ and enables future ultra-low latency and extreme-data-rate polarization MIMOs for highly dynamic and mobile applications."
pub.1134563970,Hone: Mitigating Stragglers in Distributed Stream Processing With Tuple Scheduling,"Low latency stream processing on large clusters consisting of hundreds to thousands of servers is an increasingly important challenge. A crucial barrier to tackling this challenge is stragglers, i.e., tasks that are significantly straggling behind others in processing the stream data. However, prior straggler mitigation solutions have significant limitations. They balance streaming workloads among tasks but may incur imbalanced backlogs when the workloads exhibit variance, causing stragglers as well. Fortunately, we observe that carefully scheduling the outgoing tuples of different tasks can yield benefits for balancing backlogs, and thus avoids stragglers. To this end, we present Hone, a tuple scheduler that aims to minimize the maximum queue backlog of all tasks over time. Hone leverages an online Largest-Backlog-First (LBF) algorithm with a provable good competitive ratio to perform efficient tuple scheduling. We have implemented Hone based on Apache Storm and evaluated it extensively via both simulations and testbed experiments. Our results show that under the same workload balancing strategy–shuffle grouping, Hone outperforms the original Storm significantly, with the end-to-end tuple processing latency reduced by 78.7 percent on average."
pub.1136045291,Heterogeneity-aware elastic scaling of streaming applications on cloud platforms,"Rise of Big Data techniques has led to the requirement for low latency analysis of high-velocity continuous data streams in real time. Several solutions, including Stream Processing Systems (SPSs), have been developed to enable real-time distributed stream processing. However, emerging application scenarios such as smart cities and wearable assistance that involve highly variable data rates keep on posing new challenges to the established stream processing engines for maintaining cost-effective executions. To cater to such scenarios, many modern SPSs have been proposed that leverage Cloud environment. The run-time scalability incorporated in these SPSs is in their early adaptations and are based on fixed scale sizes. Moreover, these scaling approaches do not adequately consider both the structure of the hosted streaming applications and the characteristic features of the underlying Cloud environment. Achieving true cost benefits of orchestrating streaming applications on Cloud-based pay-as-you-go model while maintaining the desired QoS, necessitates that both these issues are accounted in making the scaling decisions. This work presents a heterogeneity-aware, efficient auto-scaling strategy StreamScale-H which addresses both these issues. Simulation experiments, on representative stream applications, indicate that the proposed StreamScale-H auto-scaling algorithm exhibits much better performance in comparison with the state-of-the-art algorithms."
pub.1094324263,Event-driven Scheduling for Parallel Stream Processing,"To optimize real-time stream-processing applications for chip-level multi processors, several challenges have to be met. Poor scalability and poor internal data pressure may result from serial dependencies within or between the algorithms. Load imbalances introduced by the parallel-processing hardware and execution environment may also limit performance. To maximize the throughput and minimize the latency of parallel stream-processing applications, we propose an approach that complements run-time dynamic load balancing with static pre-compile partitioning. In our solution, the dynamic features are based on event-driven scheduling, while the static features benefit from profile-guided automatic optimizations. In this paper, we present some recent enhancements of DSPE, an open-source development environment, featuring model and source code generators for prototyping, refining and customizing realtime stream-processing applications. By using our approach on micro-benchmarks and sample applications, we also show that it is possible to reduce the impact of the different speed-up constrainers."
pub.1181715359,Best Practices for Implementing Continuous Streaming with Azure Databricks,"Continuous data streaming is essential for modern applications that require real-time processing of large data sets. Azure Databricks, a scalable data analytics platform, is widely used to implement such streaming systems. This paper presents best practices for implementing continuous streaming with Azure Databricks, focusing on key aspects such as architecture design, data ingestion, and stream processing optimization. The integration of Apache Spark within Databricks enables efficient, fault-tolerant stream processing at scale, making it ideal for handling high-throughput data streams.
 Key considerations discussed include selecting appropriate data sources, leveraging Delta Lake for reliable data storage, and ensuring efficient stream processing through resource allocation and checkpointing. The paper emphasizes the importance of partitioning data to optimize processing performance and reduce latency, alongside monitoring and alerting strategies to maintain system health. Best practices for handling common challenges such as late data arrival, scaling out the infrastructure, and managing backpressure are also explored.
 Furthermore, the use of Azure Databricks in conjunction with other Azure services, like Event Hubs and Azure Data Lake Storage, is highlighted to ensure seamless data flow across the streaming pipeline. Finally, security and compliance aspects are discussed, focusing on the secure handling of sensitive data during real-time processing.
 This paper aims to provide a comprehensive guide for organizations looking to implement robust, scalable, and efficient continuous streaming solutions using Azure Databricks in various real-world scenarios"
pub.1028664730,Adaptive Processing for Continuous Query over Data Stream,"Stream applications such as sensor data processing, financial tickers and Internet traffic analysis require that information, naturally, occur as a stream of data values. Due to a late and out-of-order arrival of infinite, unbound and multiple input streams, processing continuous queries over them may lead to producing an incorrect answer or delaying query execution. Hence to minimize this waiting time, previous works have used timeout technique without considering the frequency of timeouts. It results in decreasing the accuracy of query execution results, since the more the frequency of timeouts, the more the loss of data. We propose an AP-STO method using StB that stores operator’s state and a window time-out method based on the waiting time for the next tuple by resetting the size of a window according to the frequency of timeouts. It reduces a data lost rate and increases the tuples output-rate. We compare AP-STO method with an existing method and use output-rate and response time as criteria for performance evaluation. Our proposed method shows a substantial improvement in system performance in terms of the accuracy of query execution and the increment of tuples output-rate per a query due to the reduction in loss rate of data."
pub.1140671267,Eunomia: Efficiently Eliminating Abnormal Results in Distributed Stream Join Systems,"With the emergence of big data applications, stream join systems are widely used in extracting valuable information among multi-source streams. However, providing completeness of processing results in a large-scale distributed stream join system is challenging because it is hard to guarantee the consistency among all instances. We show through experiments that the abnormal result can make the quality of achieved data unacceptable in practice. In this paper, we propose Eunomia, a novel distributed stream join system which leverages an ordered propagation model for efficiently eliminating abnormal results. We design a light-weighted self-adaptive strategy to adjust the structure in the model according to the dynamic stream input rates and workloads. It can improve the scalability and performance significantly. We implement Eunomia and conduct comprehensive experiments to evaluate its performance. The results show that Eunomia eliminates abnormal results to guarantee the completeness, improves the system throughput by 25% and reduces the processing latency by 74% compared to state-of-the-art designs."
pub.1162960124,Anomaly detection with a container-based stream processing framework for Industrial Internet of Things,"Online anomaly detection is a key challenge for industrial internet of things (IIoT) applications, as anomalies may occur in data streams from sensors and cause losses or damages. However, most existing methods for online anomaly detection have limitations in efficiency, effectiveness and timeliness, especially with the massive and distributed data streams from IIoT devices. Therefore, developing a data stream processing framework to discover anomalies in time and ensure the proper operation of the system is an urgent issue for IIoT. In this paper, we propose a flexible stream processing framework that enables online anomaly detection for IIoT applications. The framework exploits a distributed computing architecture based on docker containers to improve flexibility, migration capability and customization. The framework also uses a central mediator to coordinate data stream processing tasks running on different docker nodes. Moreover, we develop a prediction-based online anomaly detection model that consists of batch model training and data stream anomaly detection processes. The model uses long short-term memory (LSTM) neural networks to predict data stream values and a dynamic sliding window method to model prediction errors and detect anomalies. We implement a case study to detect abnormal heating temperatures from an industrial heating plant and evaluate the performance of the proposed framework and anomaly detection model. The results show that our framework and model can achieve high accuracy and low latency in detecting anomalies, and they outperform existing methods in terms of scalability, efficiency and adaptability for IIoT applications."
pub.1136723936,Scotty,"
                    Window aggregation is a core operation in data stream processing. Existing aggregation techniques focus on reducing latency, eliminating redundant computations, or minimizing memory usage. However, each technique operates under different assumptions with respect to workload characteristics, such as properties of aggregation functions (e.g., invertible, associative), window types (e.g., sliding, sessions), windowing measures (e.g., time- or count-based), and stream (dis)order. In this article, we present
                    Scotty
                    , an efficient and general open-source operator for sliding-window aggregation in stream processing systems, such as Apache Flink, Apache Beam, Apache Samza, Apache Kafka, Apache Spark, and Apache Storm. One can easily extend Scotty with user-defined aggregation functions and window types. Scotty implements the concept of general stream slicing and derives workload characteristics from aggregation queries to improve performance without sacrificing its general applicability. We provide an in-depth view on the algorithms of the general stream slicing approach. Our experiments show that Scotty outperforms alternative solutions.
                  "
pub.1067367335,Out-of-order processing,"Many stream-processing systems enforce an order on data streams during query evaluation to help unblock blocking operators and purge state from stateful operators. Such in-order processing (IOP) systems not only must enforce order on input streams, but also require that query operators preserve order. This order-preserving requirement constrains the implementation of stream systems and incurs significant performance penalties, particularly for memory consumption. Especially for high-performance, potentially distributed stream systems, the cost of enforcing order can be prohibitive. We introduce a new architecture for stream systems, out-of-order processing (OOP), that avoids ordering constraints. The OOP architecture frees stream systems from the burden of order maintenance by using explicit stream progress indicators, such as punctuation or heartbeats, to unblock and purge operators. We describe the implementation of OOP stream systems and discuss the benefits of this architecture in depth. For example, the OOP approach has proven useful for smoothing workload bursts caused by expensive end-of-window operations, which can overwhelm internal communication paths in IOP approaches. We have implemented OOP in two stream systems, Gigascope and NiagaraST. Our experimental study shows that the OOP approach can significantly outperform IOP in a number of aspects, including memory, throughput and latency."
pub.1129363125,BDSP in the cloud: Scheduling and Load Balancing utlizing SDN and CEP,"With many applications generating a large streaming data set, there is a critical need for scalable computing frameworks for processing it with low latency. Although there are various big data stream processing frameworks, they lack adequate virtualization and optimization mechanisms aimed at enhancing the process of big data stream processing. To address this problem, we propose a cloud-based framework that couples virtual machines and software-defined networks to enhance Cloud resource allocations to the applications with streaming data processing requirements. A novel Cloud resource allocation algorithm based on the proposed framework is also proposed. We validated the proposed resource allocation algorithm using CloudSIM and compared the proposed algorithm with baseline algorithms to determine its effectiveness. The results indicate that by using a virtual machine and an SDN controller, the virtual machine is able to handle 2000 requests at a maximum of 136 seconds."
pub.1093753610,Sequence Pattern Query Processing over Out-of-Order Event Streams,"Complex event processing has become increasingly important in modern applications, ranging from RFID tracking for supply chain management to real-time intrusion detection. A key aspect of complex event processing is to extract patterns from event streams to make informed decisions in real-time. However, network latencies and machine failures may cause events to arrive out-of-order at the event processing engine. State-of-the-art event stream processing technology experiences significant challenges when faced with out-of-order data arrival including output blocking, huge system latencies, memory resource overflow, and incorrect result generation. To address these problems, we propose two alternate solutions: aggressive and conservative strategies respectively to process sequence pattern queries on out-of-order event streams. The aggressive strategy produces maximal output under the optimistic assumption that out-of-order event arrival is rare. In contrast, to tackle the unexpected occurrence of an out-of-order event and with it any premature erroneous result generation, appropriate error compensation methods are designed for the aggressive strategy. The conservative method works under the assumption that out-of-order data may be common, and thus produces output only when its correctness can be guaranteed. A partial order guarantee (POG) model is proposed under which such correctness can be guaranteed. For robustness under spiky workloads, both strategies are supplemented with persistent storage support and customized access policies. Our experimental study evaluates the robustness of each method, and compares their respective scope of applicability with state-of-art methods."
pub.1046533700,Design and Implementation of a Distributed Audio/Video Stream Service Framework based on CORBA,"This paper present a design and implementation of a distributed audio, Video stream service framework based on CORBA for efficient processing and control of audio/video stream. We design software components which support processing, control and transmission of audio/video streams as distributed objects. For optimization of stream transmission performance, we separate the transmission path of control data and media data. Distributed objects are defined by IDL and implemented using JAVA. And device dependent facilities like media capturing, playing and communication channels are implemented using JMF (Java Media Framework) components. We show a connection establishment and control procedure of streams communication. And for evaluation, we implement a test system and experiment a system performance. Our experiments show that test system has somewhat longer connection latency time compared to TCP connection establishment, but has optimized media transmission time compared to CORBA IIOP. Also test system show acceptable service quality of media transmission."
pub.1008743653,Tools and strategies for debugging distributed stream processing applications,"Abstract  Distributed data stream processing applications are often characterized by data flow graphs consisting of a large number of built‐in and user‐defined operators connected via streams. These flow graphs are typically deployed on a large set of nodes. The data processing is carried out on‐the‐fly, as tuples arrive at possibly very high rates, with minimum latency. It is well known that developing and debugging distributed, multi‐threaded, and asynchronous applications, such as stream processing applications, can be challenging. Thus, without domain‐specific debugging support, developers struggle when debugging distributed applications. In this paper, we describe tools and language support to support debugging distributed stream processing applications. Our key insight is to view debugging of stream processing applications from four different, but related, perspectives. First, debugging the semantics of the application involves verifying the operator‐level composition and inspecting the flows at the logical level. Second, debugging the user‐defined operators involves traditional source‐code debugging, but strongly tied to the stream‐level interactions. Third, debugging the deployment details of the application require understanding the runtime physical layout and configuration of the application. Fourth, debugging the performance of the application requires inspecting various performance metrics (such as communication rates, CPU utilization, etc.) associated with streams, operators, and nodes in the system. In light of this characterization, we developed several tools such as a debugger‐aware compiler and an associated stream debugger , composition and deployment visualizers , and performance visualizers , as well as language support, such as configuration knobs for logging and tracing, deployment configurations such as operator‐to‐process and process‐to‐node mappings, monitoring directives to inspect streams, and special sink adapters to intercept and dump streaming data to files and sockets, to name a few. We describe these tools in the context of Spade —a language for creating distributed stream processing applications, and System S —a distributed stream processing middleware under development at the IBM Watson Research Center. Published in 2009 by John Wiley & Sons, Ltd. "
pub.1136648814,"Stochastic distributed data stream partitioning using task locality: design, implementation, and optimization","Distributed stream processing engines (DSPEs) provide stream partitioning methods for distributing messages to tasks deployed in the distributed environment for real-time stream processing. Among these methods, the original locality-aware stream partitioning (LSP) is a binary LSP that sends messages only to downstreams on the same node as upstreams. The binary LSP degrades performance at general configurations because it focuses only on task locality and does not consider downstream status like distributed batch processing engines. In this paper, we propose a Stochastic LSP (SLSP) method that considers not only task locality but also downstream status by computing stream partitioning probability based on the round-trip time to downstreams. We also present coarse-grained and fine-grained methods for probing downstreams at node-level and process-level, respectively. Then, we optimize our SLSP using a weighted closeness to prioritize the partitioning probabilities and a parallel thread model to process each stage of the SLSP in parallel. Finally, we implement the SLSP in Apache Storm, a representative DSPE, and empirically evaluate it with the binary LSP. Experimental results show that our SLSP greatly reduces latency by up to 208% while maintaining a similar throughput compared to the binary LSP at general configurations. These results indicate that our SLSP performs the optimized stream partitioning by reflecting downstream status as well as task locality."
pub.1143223836,Whale,"To process large-scale real-time data streams, existing distributed stream processing systems (DSPSs) leverage different stream partitioning strategies. The one-to-many data partitioning strategy plays an important role in various applications. With one-to-many data partitioning, an upstream processing instance sends a generated tuple to a potentially large number of downstream processing instances. Existing DSPSs leverage an instance-oriented communication mechanism, where an upstream instance transmits a tuple to different downstream instances separately. However, in one-to-many data partitioning, multiple downstream instances typically run on the same machine to exploit multi-core resources. Therefore, a DSPS actually sends a data item to a machine multiple times, raising significant unnecessary costs for serialization and communication. We show that such a mechanism can lead to serious performance bottleneck due to CPU overload. To address the problem, we design and implement Whale, an efficient RDMA (Remote Direct Memory Access) assisted distributed stream processing system. Two factors contribute to the efficiency of this design. First, we propose a novel RDMA-assisted stream multicast scheme with a self-adjusting non-blocking tree structure to alleviate the CPU workloads of an upstream instance during one-to-many data partitioning. Second, we re-design the communication mechanism in existing DSPSs by replacing the instance-oriented communication with a new worker-oriented communication scheme, which saves significant costs for redundant serialization and communication. We implement Whale on top of Apache Storm and conduct comprehensive experiments to evaluate its performance with large-scale real world datasets. The results show that Whale achieves 56.6 improvement of system throughput and 97% reduction of processing latency compared to existing designs."
pub.1022956844,Enabling Time Sensitive Information Retrieval on the Web through Real Time Search Engines Using Streams,"Real time search engines constantly index web content originated by data streams also. This is because, the web sources like social networking sites, news, and tweets provide up to date information through streams. As new content is arrived constantly from those sources, it is very challenging job for search engines to have efficient indexing mechanisms to ensure index freshness and coverage of the index. Such updated index supports faster search whose results also include the latest content available. Latencies such as retrieval latency and indexing latency play an important role in index freshness. The former is the time taken to fetch the content after its publication while the latter is the time taken to make index on the newly fetched content. This paper presents a framework which optimizes indexing latency and also indexing coverage. The empirical results revealed that the proposed framework is capable of achieving index freshness and coverage in order to support faster processing of search queries."
pub.1094812880,An Efficient Rule Balancing for Scalable Complex Event Processing,"Owing to Big Data popularity, the metrics Variety, Volume and Velocity (V3), are gaining importance in large scale data intensive applications. Complex Event Processing (CEP) is an efficient solution for identifying events of interest on data streams arriving from geographically distributed heterogeneous sources in near-real time. CEP is capable of handling large variety of data with high computational velocity and delivers better solution compared to other existing techniques. However, handling large volume data streams still poses challenge in CEP systems. Hence, a middleware is required to manage huge streams of data on scalable distributed environment. This paper proposes Scalable Complex Event Processing (SCALACEP) framework for managing voluminous data streams. This paper addresses challenges such as state management and efficient rule distribution for design of distributed framework for CEP using the proposed SCALACEP. This paper also proposes a novel way of indexing CEP rules based on Geometric series. The proposed indexing is used for CEP Rule allocation and CEP rule pruning in the SCALACEP system. SCALACEP is evaluated and compared with other existing systems in the constrained virtualized environment and found to give better results in terms of throughput and reduction in Latency and Multicast."
pub.1174739857,Emma: Elastic Multi-Resource Management for Realtime Stream Processing,"In stream processing applications, an operator is often instantiated into multiple parallel execution instances, referred to as executors, to facilitate large-scale data processing. Due to unpredictable changes in executor workloads, data tuples processed by different executors may exhibit varying latency. In particular, within the same operator, the executor with the maximum latency significantly impacts the end-to-end (E2E) latency of the application. Existing solutions, such as load balancing and horizontal scaling, which involve workload migration, often incur substantial time overhead induced by state migration and synchronization. In contrast, elastically scaling up/down resources of executors rather than moving workloads can not only effectively handle workload fluctuations but also offer rapid adjustments; however, prior works only considered CPU scaling with the assumption of sufficient memory. In this paper, we propose Emma, an elastic multi-resource manager. Emma leverages the resource elasticity of lightweight virtualization containers, e.g., Linux containers, to resize the resource of executors at runtime. The core of Emma is a multi-resource provisioning plan that conducts performance analysis and resource adjustment in real-time. We explore the relationship between resources and performance experimentally and theoretically, guiding the plan to adaptively allocate the appropriate combination of resources to each executor to 1) accommodate the dynamic workload; 2) efficiently utilize resources to enhance the performance of as many executors as possible. Additionally, we propose an online learning method that makes the manager seamlessly adapt to diverse stream applications. We integrate Emma with Apache Samza, and our experiments show that compared to existing solutions, Emma can significantly reduce latency by orders of magnitude in real-world applications."
pub.1153377694,Balancing Performance and Energy Consumption of Bagging Ensembles for the Classification of Data Streams in Edge Computing,"In recent years, the Edge Computing (EC) paradigm has emerged as an enabling factor for developing technologies like the Internet of Things (IoT) and 5G networks, bridging the gap between Cloud Computing services and end-users, supporting low latency, mobility, and location awareness to delay-sensitive applications. An increasing number of solutions in EC have employed machine learning (ML) methods to perform data classification and other information processing tasks on continuous and evolving data streams. Usually, such solutions have to cope with vast amounts of data that come as data streams while balancing energy consumption, latency, and the predictive performance of the algorithms. Ensemble methods achieve remarkable predictive performance when applied to evolving data streams due to several models and the possibility of selective resets. This work investigates a strategy that introduces short intervals to defer the processing of mini-batches. Well balanced, our strategy can improve the performance (i.e., delay, throughput) and reduce the energy consumption of bagging ensembles to classify data streams. The experimental evaluation involved six state-of-art ensemble algorithms (OzaBag, OzaBag Adaptive Size Hoeffding Tree, Online Bagging ADWIN, Leveraging Bagging, Adaptive RandomForest, and Streaming Random Patches) applying five widely used machine learning benchmark datasets with varied characteristics on three computer platforms. As a result, our strategy can significantly reduce energy consumption in 96% of the experimental scenarios evaluated. Despite the trade-offs, it is possible to balance them to avoid significant loss in predictive performance."
pub.1067368697,Supporting scalable analytics with latency constraints,"Recently there has been a significant interest in building big data analytics systems that can handle both ""big data"" and ""fast data"". Our work is strongly motivated by recent real-world use cases that point to the need for a general, unified data processing framework to support analytical queries with different latency requirements. Toward this goal, we start with an analysis of existing big data systems to understand the causes of high latency. We then propose an extended architecture with mini-batches as granularity for computation and shuffling, and augment it with new model-driven resource allocation and runtime scheduling techniques to meet user latency requirements while maximizing throughput. Results from real-world workloads show that our techniques, implemented in Incremental Hadoop, reduce its latency from tens of seconds to sub-second, with 2x-5x increase in throughput. Our system also outperforms state-of-the-art distributed stream systems, Storm and Spark Streaming, by 1-2 orders of magnitude when combining latency and throughput."
pub.1160157697,STAR: A Cache-based Stream Warehouse System for Spatial Data," The proliferation of mobile phones and location-based services has given rise to an explosive growth in spatial data. To enable spatial data analytics, spatial data needs to be streamed into a data stream warehouse system that can provide real-time analytical results over the most recent and historical spatial data in the warehouse. Existing data stream warehouse systems are not tailored for spatial data. In this article, we introduce the STAR system. STAR is a distributed in-memory data stream warehouse system that provides low-latency and up-to-date analytical results over a fast-arriving spatial data stream. STAR supports both snapshot and continuous queries that are composed of aggregate functions and ad hoc query constraints over spatial, textual, and temporal data attributes. STAR implements a cache-based mechanism to facilitate the processing of snapshot queries that collectively utilizes the techniques of query-based caching (i.e., view materialization) and object-based caching. Moreover, to speed up processing continuous queries, STAR proposes a novel index structure that achieves high efficiency in both object checking and result updating. Extensive experiments over real datasets demonstrate the superior performance of STAR over existing systems. "
pub.1095106866,An On-the-fly Provenance Tracking Mechanism for Stream Processing Systems,"Applications that operate over streaming data with high-volume and real-time processing requirements are becoming increasingly important. These applications process streaming data in real-time and deliver instantaneous responses to support precise and on-time decisions. In such systems, traceability - the ability to verify and investigate the source of a particular output - in real-time is extremely important. This ability allows raw streaming data to be checked and processing steps to be verified and validated in timely manner. Therefore, it is crucial that stream systems have a mechanism for dynamically tracking provenance - the process that produced result data - at execution time, which we refer to as on-the-fty stream provenance tracking. In this paper, we propose a novel on-the-fty provenance tracking mechanism that enables provenance queries to be performed dynamically without requiring provenance assertions to be stored persistently. We demonstrate how our provenance mechanism works by means of an on-the-fty provenance tracking algorithm. The experimental evaluation shows that our provenance solution does not have a significant effect on the normal processing of stream systems given a 7% overhead. Moreover, our provenance solution offers low-latency processing (0.3 ms per additional component) with reasonable memory consumption."
pub.1034292755,A physical operator algebra for prioritized elements in data streams,"Data stream management systems are a natural choice to efficiently process continuous queries
  over high volume data streams, e.g., to monitor sensor data or transaction streams. An immediate reaction
  on detected critical or security relevant situations is essential for a secure and economic operation,
  as in our scenario of monitoring decentralized energy systems, which realize geographically distributed
  energy generation processes. Without further provisions existing processing approaches may lead to a delay
  of critical or security relevant messages in high load situations, e.g., caused by bursts.
  One way to allow an adequate processing in such situations is to prioritize queries that handle critical
  situations. Unfortunately, problems are not always solely identifiable by a query. Sometimes certain
  – e.g., out of range – data values or error messages indicate situations, which urge a faster
  processing of all queries processing these data. Traditional approaches on continuous query execution assume
  a stream order, typically based on timestamps, and a processing following this order. In this
  article we consider the prioritization of those elements and propose an out-of-order execution in the data
  stream.
  We provide a comprehensive and formally founded approach for prioritizing data stream elements.
  Prioritized elements benefit twice from our approach. On the one hand, they are able to “overtake”
  lower prioritized elements, e.g., in queues. On the other hand, prioritized results can be produced earlier
  in stateful operators than this would be possible in other approaches. Still, the semantics of the queries
  remains unchanged. We implemented our approach and show with measurements that a very low latency
  of prioritized elements can be achieved – even under high load. As a result, all queries that
  process prioritized elements can benefit from our approach."
pub.1049515516,Data Stream Analytics as Cloud Service for Mobile Applications,"Many mobile applications are based on cloud services such as location service, messaging service, etc. Currently most cloud services are based on statically prepared information rather than the real-time analytics results of dynamically captured events. A paradigm shift is to take Continuous Stream Analytics (CSA) as a cloud service, which, however, poses several specific challenges in scalability, latency, time-window semantics and transaction control.In this work we extend the SQL query engine to unify the processing of static relations and dynamic streams for providing the platform support of CSA service. This platform is significantly differentiated from the current generation of stream processing systems which are in general built separately from the database engine thus unable to take advantage of the functionalities already offered by the existing data management technology, and suffer from the overhead of inter-platform data access and movement.To capture the window semantics in CSA, we introduce the cycle-based query model and support it in terms of the cut-and-rewind query execution mechanism. This mechanism allows a SQL query to run cycle by cycle for processing the unbounded stream data chunk by chunk, but without shutting the query instance down between chunks for continuously maintaining the application state across the execution cycles, as required by sliding-window oriented operations. We also propose the cycle-based transaction model with cycle-based isolation and visibility. To scale-up analytics computation, we introduce the parallel infrastructure with multi-engines cooperated and synchronized based the common data chunking criteria without centralized coordination. To scale-up service provisioning, we investigate how to stage the continuously generated analytics results efficiently through metadata manipulation without physical data moving and copying.We have prototyped our approach by extending the PostgreSQL, resulting in a new kind of tightly integrated, highly efficient platform for providing CSA service. We tested the throughput and latency of this service using a well-known stream processing benchmark and with WebOS based Palm phones. The test results show that the proposed approach is highly competitive. Providing CSA cloud service using HP Neoview parallel database engine is currently explored."
pub.1012875206,A new operator for efficient stream-relation join processing in data streaming engines,"In the last decade, Stream Processing Engines (SPEs) have emerged as a new processing paradigm that can process huge amounts of data while retaining low latency and high-throughputs. Yet, it is often necessary to join streaming data with traditional databases to provide more contextual information for the end-users and applications. The major problem that we confront is to join the fast arriving stream tuples with the static relation tuples that are on a slow database. This is what we call the Stream-Relation Join (SRJ) problem. Currently, SPEs use a naive tuple-by-tuple approach for SRJ processing where the SPE accesses the database for every incoming tuple. Some SPEs use cache to avoid accessing the database for every incoming tuple, while others do not because of the stochastic nature of streaming data. In this paper, we propose a new SRJ operator to facilitate SRJ processing regardless of the cache performance using two techniques: batching and out-of-order processing. The proposed operator provides an effective generic solution to the SRJ problem and the cost of incorporating our operator into different SPEs is minimal. Our experiments use a variety of synthetic and real data sets demonstrating that our operator outperforms the state-of-the-art tuple-by-tuple approach in terms of maximizing the throughput under ordering and memory constraints."
pub.1121692650,Performance Comparison of Multiples and Target Detection with Imager-driven Processing Mode for Ultrafast-Imager,"Latest vision tasks trend to be the real-time processing with high throughput frame rate and low latency. High spatiotemporal resolution imagers continue to spring up but only a few of them can be used in real applications owing to the excessive computational burden and lacking of suitable architecture. This paper presents a solution for target detection task in imager-driven processing mode (IMP), which takes shorter time in processing than the time gap between frames, even if the ulreafast imager run at full frame rate. High throughput pixel stream outputted from imager is analyzed base on multi features in a fully pipelined and bufferless architecture in FPGA. A pyramid shape model consisting of 2-D Processing Element (PE) array is proposed to search the connected regions of target candidates distributed at different time slices, and extract corresponding features when the stream pass through. A Label based 1-D PE Array collects the feature flow generated by the pyramid according to their labels, and output the feature vector of each target candidate in real time. The proposed model has been tested in simulation and experiments for target detection with 0.8Gpixel/sec (2320×1726 with 192FPS) data stream input, and the latency is less than 1 microsecond."
pub.1167691911,"Optimizing Data Processing: A Comparative Study of Big Data Platforms in Edge, Fog, and Cloud Layers","Intelligent applications in several areas increasingly rely on big data solutions to improve their efficiency, but the processing and management of big data incur high costs. Although cloud-computing-based big data management and processing offer a promising solution to provide scalable and abundant resources, the current cloud-based big data management platforms do not properly address the high latency, privacy, and bandwidth consumption challenges that arise when sending large volumes of user data to the cloud. Computing in the edge and fog layers is quickly emerging as an extension of cloud computing used to reduce latency and bandwidth consumption, resulting in some of the processing tasks being performed in edge/fog-layer devices. Although these devices are resource-constrained, recent increases in resource capacity provide the potential for collaborative big data processing. We investigated the deployment of data processing platforms based on three different computing paradigms, namely batch processing, stream processing, and function processing, by aggregating the processing power from a diverse set of nodes in the local area. Herein, we demonstrate the efficacy and viability of edge-/fog-layer big data processing across a variety of real-world applications and in comparison to the cloud-native approach in terms of performance."
pub.1152723444,BS-Join: A novel and efficient mixed batch-stream join method for spatiotemporal data management in Flink,"The new computing model, mixed batch-stream data processing, plays a crucial role in big spatiotemporal data managements. As the core of the above computing method, mixed batch-stream data join has high requirements on the throughput and latency due to the coexistence of two types of data sources. Apache Flink is the most suitable distributed system for mixed batch-stream data join, with lower latency than the join calculation model based on Hadoop and Spark, and it simulates remote real-time reading of batch data sources and completes join calculation with the DataStream API. However, as the degree of parallelism increases, frequent remote data reads will cause huge disk and communication pressure, thereby reducing the job efficiency and scalability. To make things trickier, the above effects are further amplified when simulating complex operations such as range joins. Aiming at the above difficulties and the characteristics of mixed batch-stream data join, a cache-based framework supporting mixed batch-stream join computing natively is proposed, which increases the search speed in the process of data join by building indexes in batch data sources. Meanwhile, for equijoin and range join, an optimization mechanism based on hotspot awareness and an optimization mechanism based on skip list are proposed respectively to further improve the job efficiency. In , the advantages of our work are highlighted as follows: (1) The proposed framework enables Flink to natively support mixed batch-stream data join, and can improve throughput by 5 times and speedup by 4 times; (2) The optimization mechanism based on hotspot awareness can further improve the efficiency of equijoin; (3) Compared with range queries by traditional Operators in Flink, the throughput can be increased by 6 times while the latency is reduced by 45%."
pub.1130733686,A Throughput Model for Data Stream Processing on Fog Computing,"Today’s society faces an unprecedented deluge of data that requires processing and analysis. Data Stream Processing (DSP) applications are often employed to extract valuable information in a timely manner as they can handle data as it is generated. The typical approach for deploying these applications explores the Cloud computing paradigm, which has limitations when data sources are geographically distributed, hence introducing high latency and achieving low processing throughput. To address these problems, current work attempts to take the computation closer to the edges of the Internet, exploring Fog computing. The effective adoption of this approach is achieved with proper throughput modeling that accounts for characteristics of the DSP application and Fog infrastructure, including the location of devices, processing and bandwidth requirements of the application, as well as selectivity and parallelism level of operators. In this work, we propose a throughput model for DSP applications embracing these characteristics. Results show that the model estimates the application throughput with less than 1% error."
pub.1124940675,Dimensionality Reduction for Low-latency High-throughput Fraud Detection on Datastreams,"Given the exponential data growth and the recent focus on understanding high-dimensional ""in-motion"" data, fundamental machine learning tools, such as Principal Component Analysis (PCA), require computation-efficient streaming algorithms that operate near-real-time. Despite the different streaming PCA flavors, there is no algorithm that provably recovers the principal components in the same precision regime as the batch PCA algorithm does, while maintaining low-latency and highthroughput processing. This work, introduces a novel temporal accumulate / retract learning framework for streaming PCA. We consider the accumulate / retract framework implementation of several competitive PCA algorithms with proven theoretical advantages. We benchmark the improved PCA algorithms on real-world streams (i.e. bank transactions fraud detection) and prove their low-latency (millisecond level) and high-throughput (thousands events/second) processing guarantees."
pub.1067367601,Predictable performance for unpredictable workloads," This paper introduces Crescando: a scalable, distributed relational table implementation designed to perform large numbers of queries and updates with guaranteed access latency and data freshness. To this end, Crescando leverages a number of modern query processing techniques and hardware trends. Specifically, Crescando is based on parallel, collaborative scans in main memory and so-called ""query-data"" joins known from data-stream processing. While the proposed approach is not always optimal for a given workload, it provides latency and freshness guarantees for all workloads. Thus, Crescando is particularly attractive if the workload is unknown, changing, or involves many different queries. This paper describes the design, algorithms, and implementation of a Crescando storage node, and assesses its performance on modern multi-core hardware. "
pub.1022693793,Alignment data stream for the ATLAS inner detector,"The ATLAS experiment uses a complex trigger strategy to be able to achieve the necessary Event Filter rate output, making possible to optimize the storage and processing needs of these data. These needs are described in the ATLAS Computing Model, which embraces Grid concepts. The output coming from the Event Filter will consist of three main streams: a primary stream, the express stream and the calibration stream. The calibration stream will be transferred to the Tier-0 facilities which will allow the prompt reconstruction of this stream with an admissible latency of 8 hours, producing calibration constants of sufficient quality to permit a first-pass processing. An independent calibration stream is developed and tested, which selects tracks at the level-2 trigger (LVL2) after the reconstruction. The stream is composed of raw data, in byte-stream format, and contains only information of the relevant parts of the detector, in particular the hit information of the selected tracks. This leads to a significantly improved bandwidth usage and storage capability. The stream will be used to derive and update the calibration and alignment constants if necessary every 24h. Processing is done using specialized algorithms running in Athena framework in dedicated Tier-0 resources, and the alignment constants will be stored and distributed using the COOL conditions database infrastructure. The work is addressing in particular the alignment requirements, the needs for track and hit selection, timing and bandwidth issues."
pub.1053093331,Processing generalized k-nearest neighbor queries on a wireless broadcast stream,"In this paper, we investigate the problem of processing generalized k-nearest neighbor (GkNN) queries, which involve both spatial and non-spatial specifications for data objects, in a wireless broadcasting system. We present a method for processing GkNN queries on the broadcast stream. In particular, we propose a novel R-tree variant index structure, called the bit-vector R-tree (bR-tree), which stores additional bit-vector information to describe non-spatial attribute values of the data objects. In addition, each node in the bR-tree stores only one pointer to its children, which makes the bR-tree compact. We generate the broadcast stream by multiplexing the bR-tree and the data objects in the broadcasting channel. The corresponding search algorithm for the broadcast stream is also described. Through a series of comprehensive simulation experiments, we prove the efficiency of the proposed method with regard to energy consumption, latency, and memory requirement, which are the major performance concerns in a wireless broadcasting system. Furthermore, we test the practicality of the proposed method in a real prototype system."
pub.1094661577,Scaling the Real-Time Traffic Sensing with GPS Equipped Probe Vehicles,"In Intelligent Transportation System, GPS has become a major source of floating car data. GPS measurement from vehicles can be collected and be further analyzed for real-time urban traffic sensing or monitoring. However, the main challenge coming to the real-time traffic estimation system is how to scale up the system easily when the number of GPS vehicle probes grows dramatically. In this paper, a scalable real-time traffic estimation system based on distributed stream processing is developed. Differing from the traditional batch-style distributed computing techniques, e.g. MapReduce, the distributed stream processing focus on not only distributed computing but also realtime and in-memory computing such that latency is reduced. We show the system architecture, data flow and some implementation experiences for estimating urban traffic using Twitter Storm, which is the open source distributed stream processing framework. The experiment results illustrate that our system can scale well and scale up easily as the input GPS data increase. It is effective and efficient for applying stream computing in real-time traffic estimation system."
pub.1029230064,The 8 requirements of real-time stream processing,"Applications that require real-time processing of high-volume data steams are pushing the limits of traditional data processing infrastructures. These stream-based applications include market feed processing and electronic trading on Wall Street, network and infrastructure monitoring, fraud detection, and command and control in military environments. Furthermore, as the ""sea change"" caused by cheap micro-sensor technology takes hold, we expect to see everything of material significance on the planet get ""sensor-tagged"" and report its state or location in real time. This sensorization of the real world will lead to a ""green field"" of novel monitoring and control applications with high-volume and low-latency processing requirements.Recently, several technologies have emerged---including off-the-shelf stream processing engines---specifically to address the challenges of processing high-volume, real-time data without requiring the use of custom code. At the same time, some existing software technologies, such as main memory DBMSs and rule engines, are also being ""repurposed"" by marketing departments to address these applications.In this paper, we outline eight requirements that a system software should meet to excel at a variety of real-time stream processing applications. Our goal is to provide high-level guidance to information technologists so that they will know what to look for when evaluation alternative stream processing solutions. As such, this paper serves a purpose comparable to the requirements papers in relational DBMSs and on-line analytical processing. We also briefly review alternative system software technologies in the context of our requirements.The paper attempts to be vendor neutral, so no specific commercial products are mentioned."
pub.1121370802,BGElasor: Elastic-Scaling Framework for Distributed Streaming Processing with Deep Neural Network,"In face of constant fluctuations and sudden bursts of data stream, elasticity of distributed stream processing system has become increasingly important. The proactive policy offers a powerful means to realize the effective elastic scaling. The existing methods lack the latent features of data stream, it leads the poor prediction. Furthermore, the poor prediction results in the high cost of adaptation and the instability. To address these issues, we propose the framework named BGElasor, which is a proactive and low-cost elastic-scaling framework based on the accurate prediction using deep neural networks. It can capture the potentially-complicated pattern to enhance the accuracy of prediction, reduce the cost of adaptation and avoid adaptation bumps. The experimental results show that BGElasor not only improves the prediction accuracy with three kinds of typical loads, but also ensure the end-to-end latency on QoS with low cost."
pub.1094135663,IO Latency Hiding in Pipelined Architectures,"This paper reports upon development of a novel mathematical formalism for analyzing data pipelines. The method accounts for IO and CPU latencies in the stages of the data pipeline. An experimental pipeline was constructed using a video encoder, frame processing, and transport of the frames over an IP (Internet Protocol) network. The pipelined architecture provides a method to overlap processing with DMA, encoding and network transport latency so that streams can be processed with optimal scalability. The model expectations were compared with experimental test results and found to be consistent. The model is therefore expected to provide a good estimate for the scalabilitv of streaming video-on-demand systems. Video-on-demand is a rapidly growing service segment for entertainment, advertising, on-line education, and a myriad of emergent applications."
pub.1003317695,Alignment data streams for the ATLAS inner detector,"The ATLAS experiment uses a complex trigger strategy to be able to reduce the Event Filter rate output, down to a level that allows the storage and processing of these data. These concepts are described in the ATLAS Computing Model which embraces Grid paradigm. The output coming from the Event Filter consists of four main streams: physical stream, express stream, calibration stream, and diagnostic stream. The calibration stream will be transferred to the Tier-0 facilities that will provide the prompt reconstruction of this stream with a minimum latency of 8 hours, producing calibration constants of sufficient quality to allow a first-pass processing. The Inner Detector community is developing and testing an independent common calibration stream selected at the Event Filter after track reconstruction. It is composed of raw data, in byte-stream format, contained in Readout Buffers (ROBs) with hit information of the selected tracks, and it will be used to derive and update a set of calibration and alignment constants. This option was selected because it makes use of the Byte Stream Converter infrastructure and possibly gives better bandwidth usage and storage optimization. Processing is done using specialized algorithms running in the Athena framework in dedicated Tier-0 resources, and the alignment constants will be stored and distributed using the COOL conditions database infrastructure. This work is addressing in particular the alignment requirements, the needs for track and hit selection, and the performance issues."
pub.1133338643,Scalable Joint Optimization of Placement and Parallelism of Data Stream Processing Applications on Cloud-Edge Infrastructure,"Abstract
The Internet of Things has enabled many application scenarios where a large number of connected devices generate unbounded streams of data, often processed by data stream processing frameworks deployed in the cloud. Edge computing enables offloading processing from the cloud and placing it close to where the data is generated, whereby reducing both the time to process data events and deployment costs. However, edge resources are more computationally constrained than their cloud counterparts. This gives rise to two interrelated issues, namely deciding on the parallelism of processing tasks (a.k.a. operators) and their mapping onto available resources. In this work, we formulate the scenario of operator placement and parallelism as an optimal mixed integer linear programming problem. To overcome the issue of scalability with the optimal model, we devise a resource selection technique that reduces the number of resources evaluated during placement and parallelization decisions. Experimental results using discrete-event simulation demonstrate that the proposed model coupled with the resource selection technique is 94% faster than solving the optimal model alone, and it produces solutions that are only 12% worse than the optimal, yet it performs better than state-of-the-art approaches."
pub.1111636494,Radar: Reducing Tail Latencies for Batched Stream Processing with Blank Scheduling,"Real time processing of stream data has become increasingly vital. Batched stream systems which discretize stream data into micro-batches and leverage batch system to process these micro-batch stream jobs have attracted wide attention from academia and industry. Such batched stream system always works on heterogeneous environments which have heterogeneous resources and heterogeneous tasks. Unfortunately, current batched stream system implementations designed and optimized for homogeneous environments perform poorly on heterogeneous environments. We attribute suboptimal performance in heterogeneous environments to schedule tasks according to data locality and free slots. On the one hand, data locality creates a barrier between large tasks of slow node and powerful capacity of fast node because slow nodes prefer local large tasks rather than remote small tasks. On another hand, due to scheduler's blind eye to task size, there is a very high probability that large tasks are scheduled in the last few waves. These two aspects hinder perfect load balancing, causing tail latencies of large tasks. To address these issues, we propose a blank scheduling framework called Radar. Being aware of node capacity and task size, Radar pre-steals large tasks from slow nodes and schedules tasks according to the principle of large task first. Then Radar fills the small free slots by choosing small tasks corresponding to node's capacity. We implement Radar in Spark-2.1.1. Experimental results with benchmark show that Radar can reduce job completion time by 27.78% to 42.79 % over Spark Streaming. Experimental results with real Tencent production application show that Radar can reduce response time by 28.57 %."
pub.1008654229,Generic windowing support for extensible stream processing systems,"Stream processing applications process high volume, continuous feeds from live data sources, employ data‐in‐motion analytics to analyze these feeds, and produce near real‐time insights with low latency. One of the fundamental characteristics of such applications is the on‐the‐fly nature of the computation, which does not require access to disk resident data. Stream processing applications store the most recent history of streams in memory and use it to perform the necessary modeling and analysis tasks. This recent history is often managed using windows. All data stream management systems provide some form of windowing functionality. Windowing makes it possible to implement streaming versions of the traditionally blocking relational operators, such as streaming aggregations, joins, and sorts, as well as any other analytic operator that requires keeping the most recent tuples as state, such as time series analysis operators and signal processing operators. In this paper, we provide a categorization of different window types and policies employed in stream processing applications and give detailed operational semantics for various window configurations. We describe an extensibility mechanism that makes it possible to integrate windowing support into user‐defined operators, enabling consistent syntax and semantics across system‐provided and third‐party toolkits of streaming operators. We describe the design and implementation of a runtime windowing library that significantly simplifies the construction of window‐based operators by decoupling the handling of window policies and operator logic from each other. We present our experience using the windowing library to implement a relational operators toolkit and compare the efficacy of the solution to an earlier implementation that did not employ a common windowing library. Copyright © 2013 John Wiley & Sons, Ltd."
pub.1113524488,GASSER: An Auto-Tunable System for General Sliding-Window Streaming Operators on GPUs,"Today’s stream processing systems handle high-volume data streams in an efficient manner. To achieve this goal, they are designed to scale out on large clusters of commodity machines. However, despite the efficient use of distributed architectures, they lack support to co-processors like graphical processing units (GPUs) ready to accelerate data-parallel tasks. The main reason for this lack of integration is that GPU processing and the streaming paradigm have different processing models, with GPUs needing a bulk of data present at once while the streaming paradigm advocates a tuple-at-a-time processing model. This paper contributes to fill this gap by proposing Gasser, a system for offloading the execution of sliding-window operators on GPUs. The system focuses on completely general functions by targeting the parallel processing of non-incremental queries that are not supported by the few existing GPU-based streaming prototypes. Furthermore, Gasser provides an auto-tuning approach able to automatically find the optimal value of the configuration parameters (i.e., batch length and the degree of parallelism) needed to optimize throughput and latency with the given query and data stream. The experimental part assesses the performance efficiency of Gasser by comparing its peak throughput and latency against Apache Flink, a popular and scalable streaming system. Furthermore, we evaluate the penalty induced by supporting completely general queries against the performance achieved by the state-of-the-art solution specifically optimized for incremental queries. Finally, we show the speed and accuracy of the auto-tuning approach adopted by Gasser, which is able to self-configure the system by finding the right configuration parameters without manual tuning by the users."
pub.1094679175,Dynamic Block Sizing for Data Stream Processing Systems,"Real-time processing of big data is becoming one of the core operations in various areas, such as social networks and anomaly detection. Thanks to the rich information of the data, multiple queries can be executed to analyse the data and discover a variety of business values. It is very typical that a cluster infrastructure running for example a Spark Streaming data stream processing system would execute multiple queries simultaneously. To enable multiple queries being answered from the same data concurrently, it is important to effectively allocate the CPU-cores of the underlying infrastructure to the queries, meanwhile adhering to the latency constraints of the individual queries. In this paper, we consider the problem of allocating CPU-cores in a Spark Streaming infrastructure in the context of two types of queries, namely primary and optional, that are associated with high-and low-priority analysis, respectively. We develop a controller, iBLOC, that adjusts the block sizes of streaming jobs on the fly and the parallelism level of jobs, according to the input data rates and the query priorities. Our evaluation shows that we can achieve significant CPU-core savings from the primary query type such that multiple queries can run together without impairing their latency constraints, in comparison to a static block-sizing scheme."
pub.1094268411,Language Level Checkpointing Support for Stream Processing Applications,"Many streaming applications demand continuous processing of live data with little or no downtime, therefore, making high-availability a crucial operational requirement. Fault tolerance techniques are generally expensive and when directly applied to streaming systems with stringent throughput and latency requirements, they might incur a prohibitive performance overhead. This paper describes a flexible, light-weight fault tolerance solution in the context of the Spade language and the System S distributed stream processing engine. We devised language extensions so users can define and parameterize checkpoint policies easily. This configurable fault tolerance solution is implemented through code generation in Spade, which reduces the overall application fault tolerance costs by incurring them only for the parts of the application that require it. In this paper we focus on the overall design of our checkpoint mechanism and we also describe an incremental checkpointing algorithm that is suitable for on-the-fly processing of high-rate data streams."
pub.1093879835,Efficient Parallel Execution of Streaming Applications on Multi-Core Processors,"We propose a method for the parallel execution of applications that process continuous streams of data. Unlike pipeline-based approaches, which are frequently employed to parallelize software for multi-core processors, our method supports nonlinear structures that may contain conditionals. Nonlinear structures reduce the latency for processing an element from a stream, which is particularly important for embedded systems that are subject to real-time constraints."
pub.1148519304,Detecting rumours with latency guarantees using massive streaming data,"Today’s social networks continuously generate massive streams of data, which provide a valuable starting point for the detection of rumours as soon as they start to propagate. However, rumour detection faces tight latency bounds, which cannot be met by contemporary algorithms, given the sheer volume of high-velocity streaming data emitted by social networks. Hence, in this paper, we argue for best-effort rumour detection that detects most rumours quickly rather than all rumours with a high delay. To this end, we combine techniques for efficient, graph-based matching of rumour patterns with effective load shedding that discards some of the input data while minimising the loss in accuracy. Experiments with large-scale real-world datasets illustrate the robustness of our approach in terms of runtime performance and detection accuracy under diverse streaming conditions."
pub.1007798974,High performance stream queries in scala,"Traffic monitoring is an important stream processing application, which is highly dynamic and requires aggregation of spatially collocated data. Inspired by this, the DEBS 2015 Grand Challenge uses publicly available taxi transportation information to compute online the most frequent routes and most profitable areas. We describe our solution to the DEBS 2015 Grand Challenge, which can process events at a 10 ms latency and at a throughput of 114,000 events per second."
pub.1085072890,Defining the execution semantics of stream processing engines,"The ability to process large volumes of data on the fly, as soon as they become available, is a fundamental requirement in today’s information systems. Modern distributed stream processing engines (SPEs) address this requirement and provide low-latency and high-throughput data stream processing in cluster platforms, offering high-level programming interfaces that abstract from low-level details such as data distribution and hardware failures. The last decade saw a rapid increase in the number of available SPEs. However, each SPE defines its own processing model and standardized execution semantics have not emerged yet. This paper tackles this problem and analyzes the execution semantics of some widely adopted modern SPEs, namely Flink, Storm, Spark Streaming, Google Dataflow, and Azure Stream Analytics. We specifically target the notions of windowing and time, traditionally considered the key distinguishing factors that characterize the behavior of SPEs. We rely on the SECRET model, introduced in 2010 to analyze the windowing semantics for the SPEs available at that time. We show that SECRET models well some aspects of the behavior of modern SPEs, and we shed light on the evolution of SPEs after the introduction of SECRET by analyzing the elements that SECRET cannot fully capture. In this way, the paper contributes to the research in the area of stream processing by: (1) contrasting and comparing some widely used modern SPEs based on a formal model of their execution semantics; (2) discussing the evolution of SPEs since the introduction of the SECRET model; (3) suggesting promising research directions to direct further modeling efforts."
pub.1094550436,Programming Abstraction for Resource Aware Stream Processing for Scientific Workflows,"As the volume of real time data available for use in scientific discovery explodes, the limiting factor is increasingly the amount of time and attention a scientist can give to a problem. Processing event streams from heterogeneous sources in real time adds a dimension to e-Science workflow systems that is less well understood. Considering the types of computations that scientific workflows focus on and the latencies associated with them, it is not immediately evident that scientific workflows can directly apply to high throughput real time event processing. In this paper we propose a model for extending an established scientific workflow system to incorporate event processing without losing the richness of the programming abstraction."
pub.1129393927,Anomaly detection for NILM task with Apache Flink,"The topic of the 2020 DEBS Grand Challenge is to develop a solution for Non Intrusive Load Monitoring (NILM). Sensors continuously send voltage and current data into a stream processing application that would detect the pattern of power data based on the data characteristics. NILM is important in signal processing especially in those advancing areas such as 5G and IoT products, which generate massive amounts of data from the edge of the network. Our solution focuses on how to divide and parallelize jobs as small as possible while keeping some reasonable Service Level Agreement (SLA) including job sizes and latency so that it would be practical for edge or fog deployment. This paper describes our solution based on Apache Flink, a stream processing framework, and the DBSCAN density based clustering algorithm for anomaly detection through the context of data provided by DEBS Grand Challenge."
pub.1158769993,Stateful Adaptive Streams with Approximate Computing and Elastic Scaling,"The model of approximate computing can be used to increase performance or optimize resource usage in stream and graph processing. It can be used to satisfy performance requirements (e.g., throughput, lag) in stream processing by reducing the effort that applications need to process datasets. There are currently multiple stream processing platforms, and most of them do not natively support approximate results. A recent one, Stateful Functions, is an API that uses Flink to enable developers to easily build stream and graph processing applications. It also retains Flink's features like stateful computations, fault-tolerance, scalability, control events and its graph processing library Gelly. Herein we present Approxate, an extension over this platform to support approximate results. It can also support more efficient stream and graph processing by allocating available resources adaptively, driven by user-defined requirements on throughput, lag, and latency. This extension enables flexibility in computational trade-offs such as trading accuracy for performance. The user can choose which metrics should be guaranteed at the cost of others, and/or the accuracy. Approxate incorporates approximate computing (using load shedding) with adaptive accuracy and resource manegement in state-of-the-art stream processing platforms, which are not targeted in other relevant related work. It does not require significant modifications to application code, and minimizes imbalance in data source representation when dropping events."
pub.1145663237,Approximate Fault-Tolerant Data Stream Aggregation for Edge Computing,"With the development of IoT, edge computing has been attracting attention in recent years. In edge computing, simple data processing, such as aggregation and filtering, can be performed at network edges to reduce the amount of data communication and distribute the processing load. In edge computing applications, it is important to guarantee low latency, high reliability, and fault tolerance. We are working on the solution of this problem in the context of environmental sensing applications. In this paper, we outline our approach. In the proposed method, the aggregate value of each device is calculated approximately and the fault tolerance is also guaranteed approximately even when the input data is missing due to sensor device failure or communication failure. In addition, the proposed method reduces the delay by outputting the processing result when the error guarantee satisfies the user’s requirement."
pub.1170557862,A Comprehensive Benchmarking Analysis of Fault Recovery in Stream Processing Frameworks,"Nowadays, several software systems rely on stream processing architectures to
deliver scalable performance and handle large volumes of data in near
real-time. Stream processing frameworks facilitate scalable computing by
distributing the application's execution across multiple machines. Despite
performance being extensively studied, the measurement of fault tolerance-a key
feature offered by stream processing frameworks-has still not been measured
properly with updated and comprehensive testbeds. Moreover, the impact that
fault recovery can have on performance is mostly ignored. This paper provides a
comprehensive analysis of fault recovery performance, stability, and recovery
time in a cloud-native environment with modern open-source frameworks, namely
Flink, Kafka Streams, and Spark Structured Streaming. Our benchmarking analysis
is inspired by chaos engineering to inject failures. Generally, our results
indicate that much has changed compared to previous studies on fault recovery
in distributed stream processing. In particular, the results indicate that
Flink is the most stable and has one of the best fault recovery. Moreover,
Kafka Streams shows performance instabilities after failures, which is due to
its current rebalancing strategy that can be suboptimal in terms of load
balancing. Spark Structured Streaming shows suitable fault recovery performance
and stability, but with higher event latency. Our study intends to (i) help
industry practitioners in choosing the most suitable stream processing
framework for efficient and reliable executions of data-intensive applications;
(ii) support researchers in applying and extending our research method as well
as our benchmark; (iii) identify, prevent, and assist in solving potential
issues in production deployments."
pub.1171846589,Event-Driven AI Workflows in Serverless Computing: Enabling Real-Time Data Processing and Decision-Making,"Real-time data processing and decision-making are increasingly crucial in various applications, driven by the continuous influx of data streams. Event-driven AI workflows within serverless computing environments offer a promising approach to handle these real-time demands efficiently. This paper presents a framework for simulating and analyzing the performance characteristics of such workflows. Our proposed approach utilizes simulated data with varying event rates and durations to investigate the impact on key performance metrics like latency, throughput, and resource utilization. This enables a comprehensive evaluation of the inherent trade-offs within event-driven AI systems. The key findings reveal a trade-off between latency and throughput. As the event rate increases, average processing latency generally increases while average throughput increases. Resource utilization remains relatively stable across different event rates in the simulated scenarios (e.g., 75.55% at 2 events/second, 74.51% at 10 events/second). This framework provides a valuable tool for understanding the performance characteristics of event-driven AI workflows and optimizing resource allocation strategies."
pub.1118601694,Benchmarking Distributed Stream Processing Platforms for IoT Applications,"Internet of Things (IoT) is a technology paradigm where millions of sensors
monitor, and help inform or manage, physical, envi- ronmental and human systems
in real-time. The inherent closed-loop re- sponsiveness and decision making of
IoT applications makes them ideal candidates for using low latency and scalable
stream processing plat- forms. Distributed Stream Processing Systems (DSPS) are
becoming es- sential components of any IoT stack, but the efficacy and
performance of contemporary DSPS have not been rigorously studied for IoT data
streams and applications. Here, we develop a benchmark suite and per- formance
metrics to evaluate DSPS for streaming IoT applications. The benchmark includes
13 common IoT tasks classified across various func- tional categories and
forming micro-benchmarks, and two IoT applica- tions for statistical
summarization and predictive analytics that leverage various dataflow
compositional features of DSPS. These are coupled with stream workloads sourced
from real IoT observations from smart cities. We validate the IoT benchmark for
the popular Apache Storm DSPS, and present empirical results."
pub.1121653454,Towards Dynamic Data Placement for Polystore Ingestion,"Integrating low-latency data streaming into data warehouse architectures has become an important enhancement to support modern data warehousing applications. In these architectures, heterogeneous workloads with data ingestion and analytical queries must be executed with strict performance guarantees. Furthermore, the data warehouse may consists of multiple different types of storage engines (a.k.a., polystores or multi-stores). A paramount problem is data placement; different workload scenarios call for different data placement designs. Moreover, workload conditions change frequently. In this paper, we provide evidence that a dynamic, workload-driven approach is needed for data placement in polystores with low-latency data ingestion support. We study the problem based on the characteristics of the TPC-DI benchmark in the context of an abbreviated polystore that consists of S-Store and Postgres."
pub.1101062623,Model-based Scheduling for Stream Processing Systems,"Stream processing is emerging to react to the changing business situations of real-time processing. The main aim of this paradigm is to deal with the huge volume of data in the format of information flows originating from distributed devices. This consequently poses challenges to the scheduling problem in cloud data centers regarding the time-varying velocity of data ingesting and processing. In response to the uncertainties and complexities of streaming data, we propose a model-based scheduling scheme for stream processing systems, capturing the system behavior and providing an optimal allocation strategy to adapt to the changing work conditions. The proposed scheduling policy is implemented in Apache Storm, and micro-benchmarks with various shapes (e.g line, star, and diamond) were used in the evaluation. A topology that tracks trending topics on Twitter is also used, where the input is feeding with tweets in realtime. Experimental results show that the proposed solution can perform estimations that are well aligned with the system performance. The proposed scheduling policy achieves an improved performance with regards throughput and latency under varying ingesting rates."
pub.1134528989,[Retracted] Probabilistic Hesitant Fuzzy Methods for Prioritizing Distributed Stream Processing Frameworks for IoT Applications,"Distributed stream processing frameworks (DSPFs) are the vital engine, which can handle real-time data processing and analytics for IoT applications. How to prioritize DSPFs and select the most suitable one for special IoT applications is an open issue. To help developers of IoT applications to solve this complex issue, a novel probabilistic hesitant fuzzy multicriteria decision making (MCDM) model is put forward in this paper. To characterize the requirements for large-scale IoT data stream processing, a novel evaluation criteria system including qualitative and quantitative criteria is established. To accurately model the collective opinions from skilled developers and consider their psychological distance, the definition of probabilistic hesitant fuzzy sets (PHFSs) is used. To derive the importance degrees of criteria, a novel probabilistic hesitant fuzzy best-worst (PHFBW) method is proposed based on the score value. To prioritize the DSPFs and choose the most suitable one, a novel probabilistic hesitant fuzzy MULTIMOORA method is put forward. Finally, a practical case composed of four Apache stream processing frameworks, namely, Storm, Flink, Spark, and Samza, is studied. The obtained results indicate that throughput, latency, and reliability are considered to be the three most important criteria, and Flink is the most suitable stream framework."
pub.1174020532,A Comprehensive Benchmarking Analysis of Fault Recovery in Stream Processing Frameworks,"Nowadays, several software systems rely on stream processing architectures to deliver scalable performance and handle large volumes of data in near real-time. Stream processing frameworks facilitate scalable computing by distributing the application's execution across multiple machines. Despite performance being extensively studied, the measurement of fault tolerance---a key feature offered by stream processing frameworks---has still not been measured properly with updated and comprehensive testbeds. Moreover, the impact that fault recovery can have on performance is mostly ignored. This paper provides a comprehensive analysis of fault recovery performance, stability, and recovery time in a cloud-native environment with modern open-source frameworks, namely Flink, Kafka Streams, and Spark Structured Streaming. Our benchmarking analysis is inspired by chaos engineering to inject failures. Generally, our results indicate that much has changed compared to previous studies on fault recovery in distributed stream processing. In particular, the results indicate that Flink is the most stable and has one of the best fault recovery. Moreover, Kafka Streams shows performance instabilities after failures, which is due to its current rebalancing strategy that can be suboptimal in terms of load balancing. Spark Structured Streaming shows suitable fault recovery performance and stability, but with higher event latency. Our study intends to (i) help industry practitioners in choosing the most suitable stream processing framework for efficient and reliable executions of data-intensive applications; (ii) support researchers in applying and extending our research method as well as our benchmark; (iii) identify, prevent, and assist in solving potential issues in production deployments."
pub.1144948739,Model-based Reinforcement Learning for Elastic Stream Processing in Edge Computing,"Low-latency data processing is critical for enabling next generation Internet-of-Things(IoT) applications. Edge computing-based stream processing techniques that optimize for low latency and high throughput provide a promising solution to ensure a rich user experience by meeting strict application requirements. However, manual performance tuning of stream processing applications in heterogeneous and dynamic edge computing environments is not only time consuming but also not scalable. Our work presented in this paper achieves elasticity for stream processing applications deployed at the edge by automatically tuning the applications to meet the performance requirements. The proposed approach adopts a learning model to configure the parallelism of the operators in the stream processing application using a reinforcement learning(RL) method. We model the elastic control problem as a Markov Decision Process(MDP) and solve it by reducing it to a contextual Multi-Armed Bandit(MAB) problem. The techniques proposed in our work uses Upper Confidence Bound(UCB)-based methods to improve the sample efficiency in comparison to traditional random exploration methods such as the e-greedy method. It achieves a significantly improved rate of convergence compared to other RL methods through its innovative use of MAB methods to deal with the tradeoff between exploration and exploitation. In addition, the use of model-based pre-training results in sub-stantially improved performance by initializing the model with appropriate and well-tuned parameters. The proposed techniques are evaluated using realistic and synthetic workloads through both simulation and real testbed experiments. The experiment results demonstrate the effectiveness of the proposed approach compared to standard methods in terms of cumulative reward and convergence speed."
pub.1035636655,Deterministic real-time analytics of geospatial data streams through ScaleGate objects,"This paper presents our solution to the DEBS 2015 Grand Challenge. The analysis of the Grand Challenge is partitioned among an arbitrary number of processing units by leveraging ScaleGate, a recently proposed abstract data type with its concurrent implementation which articulates data access in parallel data streaming. ScaleGate aims not only at supporting high throughput and low latency parallel streaming analysis, but also at guaranteeing deterministic processing, which is one of the biggest challenges in parallelizing computation while maintaining consistency."
pub.1118785723,StreamLearner: Distributed Incremental Machine Learning on Event Streams: Grand Challenge,"Today, massive amounts of streaming data from smart devices need to be
analyzed automatically to realize the Internet of Things. The Complex Event
Processing (CEP) paradigm promises low-latency pattern detection on event
streams. However, CEP systems need to be extended with Machine Learning (ML)
capabilities such as online training and inference in order to be able to
detect fuzzy patterns (e.g., outliers) and to improve pattern recognition
accuracy during runtime using incremental model training. In this paper, we
propose a distributed CEP system denoted as StreamLearner for ML-enabled
complex event detection. The proposed programming model and data-parallel
system architecture enable a wide range of real-world applications and allow
for dynamically scaling up and out system resources for low-latency,
high-throughput event processing. We show that the DEBS Grand Challenge 2017
case study (i.e., anomaly detection in smart factories) integrates seamlessly
into the StreamLearner API. Our experiments verify scalability and high event
throughput of StreamLearner."
pub.1086122355,StreamLearner,"Today, massive amounts of streaming data from smart devices need to be analyzed automatically to realize the Internet of Things. The Complex Event Processing (CEP) paradigm promises low-latency pattern detection on event streams. However, CEP systems need to be extended with Machine Learning (ML) capabilities such as online training and inference in order to be able to detect fuzzy patterns (e.g. outliers) and to improve pattern recognition accuracy during runtime using incremental model training. In this paper, we propose a distributed CEP system denoted as StreamLearner for ML-enabled complex event detection. The proposed programming model and data-parallel system architecture enable a wide range of real-world applications and allow for dynamically scaling up and out system resources for low-latency, high-throughput event processing. We show that the DEBS Grand Challenge 2017 case study (i.e., anomaly detection in smart factories) integrates seamlessly into the StreamLearner API. Our experiments verify scalability and high event throughput of StreamLearner."
pub.1165570753,An adaptive load balancing strategy for stateful join operator in skewed data stream environments,"As one of the most computationally intensive operations in stream processing applications, join operation can cause severe load imbalance problem when dealing with skewed data. Most of the popular solutions focused on monitoring-based dynamic balancing strategies, making it difficult to quickly adapt to the changing frequency of data stream, and sometimes failing the balancing strategies that try to address the skewed load in the cluster. To address these issues, we propose to use the prediction results of a deep reinforcement learning model and adjust the grouping strategy in advance before the frequency change of data stream. It will enable the system to quickly adapt to data stream fluctuation, while managing the resources for effective resource utilization. The following contributions are made in this paper: 1) Explore the main factors that trigger the load skewness problem in distributed stream join systems and carefully model the load balancing problem at the application level. 2) Develop a Gated Recurrent Unit Sequence to Sequence model to predict key frequency distribution of streams, and propose a dynamic grouping algorithm and a feedback-based resource elasticity scaling algorithm to solve the load imbalance problem caused by hot keys in real time. 3) Design and implement an adaptive stream join system Aj-Stream based on the prediction model and the proposed algorithm on Apache Storm. 4) Evaluate the system performance through extensive experiments on a large scale real-world dataset and multiple synthetic datasets. The experimental results demonstrate that the Aj-Stream proposed in this paper exhibits stable throughput and latency performance with both static data streams of varying skewnesses and dynamic data streams. In comparison to existing stream-connected systems, Aj-Stream demonstrated a 22.1% increase in system throughput and a 45.5% decrease in system latency when dealing with frequently fluctuating data streams."
pub.1032722758,Keep calm and react with foresight,"This paper addresses the problem of designing scaling strategies for elastic data stream processing. Elasticity allows applications to rapidly change their configuration on-the-fly (e.g., the amount of used resources) in response to dynamic workload fluctuations. In this work we face this problem by adopting the Model Predictive Control technique, a control-theoretic method aimed at finding the optimal application configuration along a limited prediction horizon in the future by solving an online optimization problem. Our control strategies are designed to address latency constraints, using Queueing Theory models, and energy consumption by changing the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) support available in the modern multicore CPUs. The proactive capabilities, in addition to the latency- and energy-awareness, represent the novel features of our approach. To validate our methodology, we develop a thorough set of experiments on a high-frequency trading application. The results demonstrate the high-degree of flexibility and configurability of our approach, and show the effectiveness of our elastic scaling strategies compared with existing state-of-the-art techniques used in similar scenarios."
pub.1174770466,Data-Priority Aware Fair Task Scheduling for Stream Processing at the Edge,"Real-time data stream processing at the edge is crucial for time-sensitive tasks within large-scale IoT systems. Task scheduling plays a key role in managing the Quality of Service (QoS), necessitating a prioritization system to distinguish between high and low-priority tasks, thus ensuring efficient data processing on edge nodes. Existing scheduling algorithms rigidly prioritize tasks deemed as high-priority, often at the expense of fairness and overall system efficiency. In this paper, we propose a Priority-aware Fair Task Scheduling (FTS-Hybrid) algorithm that addresses these challenges by managing priority-based task execution in a controlled manner. Our task scheduling algorithm streamlines resource utilization and enhances system responsiveness, contributing to low latency and high throughput, outperforming competing techniques including First-Come-First-Serve (FCFS), Round Robin (RR), and Priority Scheduling (PS). We implemented FTS-Hybrid on Apache Storm and evaluated its performance using an open-source real-time IoT benchmark (RIoTBench). Experimental results show that the FTS-Hybrid algorithm reduces task execution latency by 24 %, 31 %, and 26 % compared with FCFS, RR, and PS, respectively, by strategically mitigating queuing delays under dynamic workload conditions."
pub.1171595733,Event-Driven AI Workflows in Serverless Computing: Enabling Real-Time Data Processing and Decision-Making,"Real-time data processing and decision-making are increasingly crucial in various applications, driven by the continuous influx of data streams. Event-driven AI workflows within serverless computing environments offer a promising approach to handle these real-time demands efficiently. This paper presents a framework for simulating and analyzing the performance characteristics of such workflows. Our proposed approach utilizes simulated data with varying event rates and durations to investigate the impact on key performance metrics like latency, throughput, and resource utilization. This enables a comprehensive evaluation of the inherent trade-offs within event-driven AI systems. The key findings reveal a trade-off between latency and throughput. As the event rate increases, average processing latency generally increases while average throughput increases. Resource utilization remains relatively stable across different event rates in the simulated scenarios (e.g., 75.55% at 2 events/second, 74.51% at 10 events/second). This framework provides a valuable tool for understanding the performance characteristics of event-driven AI workflows and optimizing resource allocation strategies."
pub.1168377796,JVAP: A Joint Video Acceleration Processing Architecture for Online Edge Systems,"In visual intelligent scenarios, large amounts of real-time video data are generated at the end. During the optimization process for different video tasks, frequent data copying between devices and hosts can be limited by data bandwidth, resulting in high system latency. We investigate computing bottlenecks in online video processing to reduce processing latency and improve efficiency. In this paper, we propose a joint video acceleration processing (JVAP) architecture for online edge systems. First, video-compressed streams are transmitted to the GPU for decoding and conversion of data content. Second, we design data pre-processing and post-processing modules to achieve specific functional operators and separately complete operator combinations and stitching. Different computing tasks can reuse the implemented operator library. Third, we modify the data interface of the inference task model to maintain the consistent flow of data in the GPU. We conduct experiments using videos of different qualities and model frameworks of varying scales. The results indicate that the proposed method enhances the average processing efficiency over 11% with respect to existing representative acceleration frameworks and extends the potential application of online intelligent inference algorithms."
pub.1127948479,Load Shedding for Complex Event Processing: Input-based and State-based Techniques,"Complex event processing (CEP) systems that evaluate queries over streams of events may face unpredictable input rates and query selectivities. During short peak times, exhaustive processing is then no longer reasonable, or even infeasible, and systems shall resort to best-effort query evaluation and strive for optimal result quality while staying within a latency bound. In traditional data stream processing, this is achieved by load shedding that discards some stream elements without processing them based on their estimated utility for the query result. We argue that such input-based load shedding is not always suitable for CEP queries. It assumes that the utility of each individual element of a stream can be assessed in isolation. For CEP queries, however, this utility may be highly dynamic: Depending on the presence of partial matches, the impact of discarding a single event can vary drastically. In this work, we therefore complement input-based load shedding with a statebased technique that discards partial matches. We introduce a hybrid model that combines both input-based and statebased shedding to achieve high result quality under constrained resources. Our experiments indicate that such hybrid shedding improves the recall by up to 14× for synthetic data and 11.4× for real-world data, compared to baseline approaches."
pub.1123202508,Monte-Carlo Tree Search and Reinforcement Learning for Reconfiguring Data Stream Processing on Edge Computing,"Distributed Stream Processing (DSP)applications are increasingly used in new pervasive services that process enormous amounts of data in a seamless and near real-time fashion. Edge computing has emerged as a means to minimise the time to handle events by enabling processing (i.e., operators)to be offloaded from the Cloud to the edges of the Internet, where the data is often generated. Deciding where to execute such operations (i.e., edge or cloud)during application deployment or at runtime is not a trivial problem. In this work, we employ Reinforcement Learning (RL)and Monte-Carlo Tree Search (MCTS)to reassign operators during application runtime. Experimental results show that RL and MCTS algorithms perform better than traditional placement techniques. We also introduce an optimisation to a MCTS algorithm, called MCTS-Best-UCT, that achieves similar latency with fewer operator migrations and faster execution time. In certain scenarios, the time needed by MCTS-Best-UCT to find the best end-to-end latency is at least 33 % smaller than the time required by the other algorithms."
pub.1149507812,Zero-shot cost models for distributed stream processing,"This paper proposes a learned cost estimation model for Distributed Stream Processing Systems (DSPS) with an aim to provide accurate cost predictions of executing queries. A major premise of this work is that the proposed learned model can generalize to the dynamics of streaming workloads out-of-the-box. This means a model once trained can accurately predict performance metrics such as latency and throughput even if the characteristics of the data and workload or the deployment of operators to hardware changes at runtime. That way the model can be used to solve tasks such as optimizing the placement of operators to minimize the end-to-end latency of a streaming query or maximize its throughput even under varying conditions. Our evaluation on a well-known DSPS, Apache Storm, shows that the model can predict accurately for unseen workloads and queries while generalizing across real-world benchmarks."
pub.1122493799,An adaptive and real-time based architecture for financial data integration,"In this paper we are proposing an adaptive and real-time approach to resolve real-time financial data integration latency problems and semantic heterogeneity. Due to constraints that we have faced in some projects that requires real-time massive financial data integration and analysis, we decided to follow a new approach by combining a hybrid financial ontology, resilient distributed datasets and real-time discretized stream. We create a real-time data integration pipeline to avoid all problems of classic Extract-Transform-Load tools, which are data processing latency, functional miscomprehensions and metadata heterogeneity. This approach is considered as contribution to enhance reporting quality and availability in short time frames, the reason of the use of Apache Spark. We studied Extract-Transform-Load (ETL) concepts, data warehousing fundamentals, big data processing technics and oriented containers clustering architecture, in order to replace the classic data integration and analysis process by our new concept resilient distributed DataStream for online analytical process (RDD4OLAP) cubes which are consumed by using Spark SQL or Spark Core basics."
pub.1000931377,Keep calm and react with foresight," This paper addresses the problem of designing scaling strategies for elastic data stream processing. Elasticity allows applications to rapidly change their configuration on-the-fly (e.g., the amount of used resources) in response to dynamic workload fluctuations. In this work we face this problem by adopting the Model Predictive Control technique, a control-theoretic method aimed at finding the optimal application configuration along a limited prediction horizon in the future by solving an online optimization problem. Our control strategies are designed to address latency constraints, using Queueing Theory models, and energy consumption by changing the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) support available in the modern multicore CPUs. The proactive capabilities, in addition to the latency- and energy-awareness, represent the novel features of our approach. To validate our methodology, we develop a thorough set of experiments on a high-frequency trading application. The results demonstrate the high-degree of flexibility and configurability of our approach, and show the effectiveness of our elastic scaling strategies compared with existing state-of-the-art techniques used in similar scenarios. "
pub.1128542718,EVPS: An Automotive Video Acquisition and Processing Platform,"This paper describes a versatile and flexible video acquisition and processing platform for automotive. It is designed to meet aggressive requirements in terms of bandwidth and latency when implementing ADAS functions. Based on a Xilinx Ultrascale+ FPGA device, a vision processing pipeline mixing software and hardware tasks is implemented on this platform. This setup is able to collect four automotive camera streams (MIPI CSI2) and process them in the loop before transmitting a more intelligible pre-processed/enhanced data."
pub.1005548254,Optimal Use of Mixed Task and Data Parallelism for Pipelined Computations,"This paper addresses optimal mapping of parallel programs composed of a chain of data parallel tasks onto the processors of a parallel system. The input to the programs is a stream of data sets, each of which is processed in order by the chain of tasks. This computation structure, also referred to as a data parallel pipeline, is common in several application domains, including digital signal processing, image processing, and computer vision. The parameters of the performance for such stream processing are latency (the time to process an individual data set) and throughput (the aggregate rate at which data sets are processed). These two criteria are distinct since multiple data sets can be pipelined or processed in parallel. The central contribution of this research is a new algorithm to determine a processor mapping for a chain of tasks that optimizes latency in the presence of a throughput constraint. We also discuss how this algorithm can be applied to solve the converse problem of optimizing throughput with a latency constraint. The problem formulation uses a general and realistic model of intertask communication and addresses the entire problem of mapping, which includes clustering tasks into modules, assigning of processors to modules, and possible replicating of modules. The main algorithms are based on dynamic programming and their execution time complexity is polynomial in the number of processors and tasks. The entire framework is implemented as an automatic mapping tool in the Fx parallelizing compiler for a dialect of High Performance Fortran."
pub.1125950640,SPICE: Streaming PCA Fault Identification and Classification Engine in Predictive Maintenance,"Data-driven predictive maintenance needs to understand high-dimensional “in-motion” data, for which fundamental machine learning tools, such as Principal Component Analysis (PCA), require computation-efficient algorithms that operate near-real-time. Despite the different streaming PCA flavors, there is no algorithm that precisely recovers the principal components as the batch PCA algorithm does, while maintaining low-latency and high-throughput processing. This work introduces a novel processing framework, employing temporal accumulate/retract learning for streaming PCA. The framework is instantiated with several competitive PCA algorithms with proven theoretical advantages. We benchmark the framework in a real-world predictive maintenance scenario (i.e. fault classification in a coal coke production line) and prove its low-latency (millisecond level) and high-throughput (thousands events/second) processing guarantees."
pub.1094647826,Dynamic Urban Surveillance Video Stream Processing Using Fog Computing,"The recent rapid development of urbanization and Internet of things (IoT) encourages more and more research on Smart City in which computing devices are widely distributed and huge amount of dynamic real-time data are collected and processed. Although vast volume of dynamic data are available for extracting new living patterns and making urban plans, efficient data processing and instant decision making are still key issues, especially in emergency situations requesting quick response with low latency. Fog Computing, as the extension of Cloud Computing, enables the computing tasks accomplished directly at the edge of the network and is characterized as low latency and real time computing. However, it is non-trivial to coordinate highly heterogeneous Fog Computing nodes to function as a homogeneous platform. In this paper, taking urban traffic surveillance as a case study, a dynamic video stream processing scheme is proposed to meet the requirements of realtime information processing and decision making. Furthermore, we have explored the potential to enable multi-target tracking function using a simpler single target tracking algorithm. A prototype is built and the performance is evaluated. The experimental results show that our scheme is a promising solution for smart urban surveillance applications."
pub.1013117316,Load Distribution for Distributed Stream Processing,"Distributed steam processing is necessary for a large class of stream-based applications. To exploit the full power of distributed computation, effective load distribution techniques must be developed to optimize the system performance and cope with time-varying loads. When traditional load balancing or load sharing strategies are applied to such systems, we find that they either fall short in achieving good load distribution or fail to maintain good task partition in the long run.In this paper, we study two important issues of dynamic load distribution in the context of data-intensive stream processing. The first one is how to allocate processing resources for push-based tasks such that the average end-to-end data processing latency can be minimized. The second issue is how to maintain a good load distribution dynamically for long running continuous queries. We propose a new hybrid load distribution strategy that addresses the above concerns by load clustering. To achieve scalability, our algorithm is completely decentralized and asynchronous."
pub.1118720579,RIoTBench: A Real-time IoT Benchmark for Distributed Stream Processing Platforms,"The Internet of Things (IoT) is an emerging technology paradigm where
millions of sensors and actuators help monitor and manage, physical,
environmental and human systems in real-time. The inherent closedloop
responsiveness and decision making of IoT applications make them ideal
candidates for using low latency and scalable stream processing platforms.
Distributed Stream Processing Systems (DSPS) hosted on Cloud data-centers are
becoming the vital engine for real-time data processing and analytics in any
IoT software architecture. But the efficacy and performance of contemporary
DSPS have not been rigorously studied for IoT applications and data streams.
Here, we develop RIoTBench, a Realtime IoT Benchmark suite, along with
performance metrics, to evaluate DSPS for streaming IoT applications. The
benchmark includes 27 common IoT tasks classified across various functional
categories and implemented as reusable micro-benchmarks. Further, we propose
four IoT application benchmarks composed from these tasks, and that leverage
various dataflow semantics of DSPS. The applications are based on common IoT
patterns for data pre-processing, statistical summarization and predictive
analytics. These are coupled with four stream workloads sourced from real IoT
observations on smart cities and fitness, with peak streams rates that range
from 500 to 10000 messages/sec and diverse frequency distributions. We validate
the RIoTBench suite for the popular Apache Storm DSPS on the Microsoft Azure
public Cloud, and present empirical observations. This suite can be used by
DSPS researchers for performance analysis and resource scheduling, and by IoT
practitioners to evaluate DSPS platforms."
pub.1129363116,NAMB: A Quick and Flexible Stream Processing Application Prototype Generator,"The importance of Big Data is nowadays established, both in industry and research fields, especially stream processing for its capability to analyze continuous data streams and provide statistics in real-time. Several data stream processing (DSP) platforms exist like the Storm, Flink, Spark Streaming and Heron Apache projects, or industrial products such as Google MillWheel. Usually, each platform is tested and analyzed using either specifically crafted benchmarks or realistic applications. Unfortunately, these applications are only briefly described and their source code is generally not available. Hence, making quick evaluations often involves rewriting complete applications on different platforms. The lack of a generic prototype application also makes it difficult for a developer to quickly evaluate the impact of some design choices. To address these issues, we present NAMB (Not only A Micro-Benchmark), a generic application prototype generator for DSP platforms. Given a high-level description of a stream processing application and its workload, NAMB automatically generates the code for different platforms. It features a flexible architecture which makes it easy to support new platforms. We demonstrate the benefits of our proposal to quickly generate application prototypes as well as benchmarks used in published papers. Overall, our approach provides easily replicable, comparable and customizable prototypes for data stream platforms. Moreover, NAMB provides similar performance in terms of latency and throughput to existing benchmarks, while only requiring a simple high-level description."
pub.1149378104,Zero-Shot Cost Models for Distributed Stream Processing,"This paper proposes a learned cost estimation model for Distributed Stream
Processing Systems (DSPS) with an aim to provide accurate cost predictions of
executing queries. A major premise of this work is that the proposed learned
model can generalize to the dynamics of streaming workloads out-of-the-box.
This means a model once trained can accurately predict performance metrics such
as latency and throughput even if the characteristics of the data and workload
or the deployment of operators to hardware changes at runtime. That way, the
model can be used to solve tasks such as optimizing the placement of operators
to minimize the end-to-end latency of a streaming query or maximize its
throughput even under varying conditions. Our evaluation on a well-known DSPS,
Apache Storm, shows that the model can predict accurately for unseen workloads
and queries while generalizing across real-world benchmarks."
pub.1148283793,Dynamic Control of Data-Intensive Services over Edge Computing Networks,"Next-generation distributed computing networks (e.g., edge and fog computing)
enable the efficient delivery of delay-sensitive, compute-intensive
applications by facilitating access to computation resources in close proximity
to end users. Many of these applications (e.g., augmented/virtual reality) are
also data-intensive: in addition to user-specific (live) data streams, they
require access to (static) digital objects (e.g., image database) to complete
the required processing tasks. When required objects are not available at the
servers hosting the associated service functions, they must be fetched from
other edge locations, incurring additional communication cost and latency. In
such settings, overall service delivery performance shall benefit from jointly
optimized decisions around (i) routing paths and processing locations for live
data streams, together with (ii) cache selection and distribution paths for
associated digital objects. In this paper, we address the problem of dynamic
control of data-intensive services over edge cloud networks. We characterize
the network stability region and design the first throughput-optimal control
policy that coordinates processing and routing decisions for both live and
static data-streams. Numerical results demonstrate the superior performance
(e.g., throughput, delay, and resource consumption) obtained via the novel
multi-pipeline flow control mechanism of the proposed policy, compared with
state-of-the-art algorithms that lack integrated stream processing and data
distribution control."
pub.1148813495,NMMF-Stream: A Fast and Accurate Stream-Processing Scheme for Network Monitoring Data Recovery,"Recovery of missing network monitoring data is of great significance for network operation and maintenance tasks such as anomaly detection and traffic prediction. To exploit historical data for more accurate missing data recovery, some recent studies combine the data together as a tensor to learn more features. However, the need of performing high cost data decomposition compromises their speed and accuracy, which makes them difficult to track dynamic features from streaming monitoring data. To ensure fast and accurate recovery of network monitoring data, this paper proposes NMMF-Stream, a stream-processing scheme with a context extraction module and a generation module. To achieve fast feature extraction and missing data filling with a low sampling rate, we propose several novel techniques, including the context extraction based on both positive and negative monitoring data, context validation via measuring the Pointwise Mutual Information, GRU-based temporal feature learning and memorization, and a new composite loss function to guide the fast and accurate data filling. We have done extensive experiments using two real network traffic monitoring data sets and one network latency data set. The experimental results demonstrate that, compared with three baselines, NMMF-Stream can fill the newly arrived monitoring data very quickly with much higher accuracy."
pub.1154433659,Dynamic Control of Data-Intensive Services Over Edge Computing Networks,"Next-generation distributed computing networks (e.g., edge and fog computing) enable the efficient delivery of delay-sensitive, compute-intensive applications by facilitating access to computation resources in close proximity to end users. Many of these applications (e.g., augmented/virtual reality) are also data-intensive: in addition to user-specific (live) data streams, they require access to shared (static) digital objects (e.g., image database) to complete the required processing tasks. When required objects are not available at the servers hosting the associated service functions, they must be fetched from other edge locations, incurring additional communication cost and latency. In such settings, overall service delivery performance shall benefit from jointly optimized decisions around (i) routing paths and processing locations for live data streams, together with (ii) cache selection and distribution paths for associated digital objects. In this paper, we address the problem of dynamic control of data-intensive services over edge cloud networks. We characterize the network stability region and design the first throughput-optimal control policy that coordinates processing and routing decisions for both live and static data-streams. Numerical results demonstrate the superior performance (e.g., throughput, delay, and resource consumption) obtained via the novel multi-pipeline flow control mechanism of the proposed policy, compared with state-of-the-art algorithms that lack integrated stream processing and data distribution control."
pub.1064199397,A Stream Processor for Extracting Usage Intelligence From High-Momentum Internet Data,"The data streams of the Internet are quite large and present significant challenges to those wishing to analyze these streams on a continuous basis. Opportunities for analysis for a Network Service Provider include understanding subscriber usage patterns for developing new services, network demand flows for network operations and capacity planning functions, and early detection of network security breaches. The conventional analysis paradigm of store first, then analyze later has significant cost and latency issues when analyzing these high-momentum streams. This article presents a deployed architecture for a general purpose stream processor that includes dynamically configurable Capture Models that can be tailored for compact collection of statistics of the stream in real time. The highly configurable flow processing model is presented with numerous examples of how multiple streams can be merged and split based on the requirements at hand."
pub.1168163737,Uncertainty-Aware Optimisation for Sustainable Multimedia Event Processing in Big Data Streams,"Multimedia Event Processing (MEP) systems play a critical role in various Internet of Things (IoT) applications, including Smart Cities and Health and Safety, by processing large amounts of multimedia data streams. These systems often leverage state-of-the-art Deep Neural Network (DNN) models to enhance their capabilities. However, the growth of Cloud and Edge Big Data applications has imposed a significant environ-mental burden, further intensified by the substantial energy consumption associated with certain DNN model operations. This study addresses the environmental impact of growing Big Data stream applications and focuses on optimising MEP systems to mitigate this issue. We tackle uncertainties arising from user-defined Quality of Service (QoS) interpretations and service worker measurement imprecisions by using uncertainty-aware solutions for the service selection problem in order to improve the QoS within MEP systems. Our results reveal substantial advantages in employing uncertainty-aware strategies. These approaches consistently enhance QoS metrics, outperforming their uncertainty-oblivious counterparts. Specifically, we report improvements in more than 67%, 69%, and 20% of the scenarios, on average, for energy consumption, latency, and accuracy, respectively. These enhancements become evident within just three hours of processing, resulting in energy savings of up to 1.2 kilowatt-hours and latency reductions of 213 seconds, with a 0.29% average loss in query accuracy. These strategies improve system efficiency and ecological sustainability while incurring a small accuracy trade-off. When extrapolated over a year, the environmental benefits become even more noticeable, surpassing the energy requirements for a 1000 Km electric vehicle round-trip from Amsterdam to Paris and back."
pub.1106072382,AWStream,"The emerging class of wide-area streaming analytics faces the challenge of scarce and variable WAN bandwidth. Non-adaptive applications built with TCP or UDP suffer from increased latency or degraded accuracy. State-of-the-art approaches that adapt to network changes require developer writing sub-optimal manual policies or are limited to application-specific optimizations. We present AWStream, a stream processing system that simultaneously achieves low latency and high accuracy in the wide area, requiring minimal developer efforts. To realize this, AWStream uses three ideas: (i) it integrates application adaptation as a first-class programming abstraction in the stream processing model; (ii) with a combination of offline and online profiling, it automatically learns an accurate profile that models accuracy and bandwidth trade-off; and (iii) at runtime, it carefully adjusts the application data rate to match the available bandwidth while maximizing the achievable accuracy. We evaluate AWStream with three real-world applications: augmented reality, pedestrian detection, and monitoring log analysis. Our experiments show that AWStream achieves sub-second latency with only nominal accuracy drop (2-6%)."
pub.1144782513,Balancing Performance and Energy Consumption of Bagging Ensembles for the Classification of Data Streams in Edge Computing,"In recent years, the Edge Computing (EC) paradigm has emerged as an enabling
factor for developing technologies like the Internet of Things (IoT) and 5G
networks, bridging the gap between Cloud Computing services and end-users,
supporting low latency, mobility, and location awareness to delay-sensitive
applications. Most solutions in EC employ machine learning (ML) methods to
perform data classification and other information processing tasks on
continuous and evolving data streams. Usually, such solutions have to cope with
vast amounts of data that come as data streams while balancing energy
consumption, latency, and the predictive performance of the algorithms.
Ensemble methods achieve remarkable predictive performance when applied to
evolving data streams due to the combination of several models and the
possibility of selective resets. This work investigates strategies for
optimizing the performance (i.e., delay, throughput) and energy consumption of
bagging ensembles to classify data streams. The experimental evaluation
involved six state-of-art ensemble algorithms (OzaBag, OzaBag Adaptive Size
Hoeffding Tree, Online Bagging ADWIN, Leveraging Bagging, Adaptive
RandomForest, and Streaming Random Patches) applying five widely used machine
learning benchmark datasets with varied characteristics on three computer
platforms. Such strategies can significantly reduce energy consumption in 96%
of the experimental scenarios evaluated. Despite the trade-offs, it is possible
to balance them to avoid significant loss in predictive performance."
pub.1118879932,Scalable real-time processing with Spark Streaming: implementation and design of a Car Information System,"Streaming data processing is a hot topic in big data these days, because it
made it possible to process a huge amount of events within a low latency. One
of the most common used open-source stream processing platforms is Spark
Streaming, which is demonstrated and discussed based on a real-world use-case
in this paper. The use-case is about a Car Information System, which is an
example for a classic stream processing system. First the System is de- signed
and engineered, whereby the application architecture is created carefully,
because it should be adaptable for similar use-cases. At the end of this paper
the CIS and Spark Streaming is evaluated by the use of the Goal Question Metric
model. The evaluation proves that Spark Streaming is capable to create stream
processing in a scalable and fault tolerant manner. But it also shows that
Spark is a very fast moving project, which could cause problems during the
development and maintenance of a software project."
pub.1094888033,A Disk Based Stream Oriented Approach For Storing Big Data,"This paper proposes an extension to the generally accepted definition of Big Data and from this extended definition proposes a specialized database design for storing high throughput data from low-latency sources. It discusses the challenges a financial company faces with regards to processing and storing data and how existing database technologies are unsuitable for this niche task. A prototype database called CakeDB is built using a stream oriented, disk based storage design and insert throughput tests are conducted to demonstrate how effectively such a design would handle high throughput data as per the use case."
pub.1018348064,Hierarchical organization and neuronal response latencies in the primate visual system,"While the anatomical connectivity of the macaque visual system is organized hierarchically, many dorsal cortical visual areas respond nearly simultaneously to flashed stimuli. Does this contradict hierarchical processing? To address this issue, we constructed an integrate-and-fire network, consisting of M and P layers of LGN, and of several visual cortical areas for which latency data were available. The neural network implemented the areas’ known laminar connectivity. The model displayed simultaneous onset of activity in several dorsal stream areas, closely matching experimental observations. Thus, anatomical organization and neurophysiological data are not contradictory, and hierarchical processing does not imply sequential response timing."
pub.1095035591,Data Triage: An Adaptive Architecture for Load Shedding in TeiegraphCQ,"Many of the data sources used in stream query processing are known to exhibit bursty behavior. Data in a burst often has different characteristics than steady-state data, and therefore may be of particular interest. In this paper, we describe the Data Triage architecture that we are adding to TelegraphCQ to provide low latency results with good accuracy under such bursts."
pub.1094791566,Simultaneous Equation Systems for Query Processing on Continuous-Time Data Streams,"We introduce Pulse, a framework for processing continuous queries over models of continuous-time data, which can compactly and accurately represent many real-world activities and processes. Pulse implements several query operators, including filters, aggregates and joins, that work by solving simultaneous equation systems, which in many cases is significantly cheaper than processing a stream of tuples. As such, Pulse translates regular queries to work on continuous-time inputs, to reduce computational overhead and latency while meeting user-specified error bounds on query results. For error bound checking, Pulse uses an approximate query inversion technique that ensures the solver executes infrequently and only in the presence of errors, or no previously known results. We first discuss the high-level design of Pulse, which we fully implemented in a stream processing system. We then characterise Pulse's behavior through experiments with real data, including financial data from the New York Stock Exchange, and spatial data from the Automatic Identification System for tracking naval vessels. Our results verify that Pulse is practical and demonstrates significant performance gains for a variety of workload and query types."
pub.1143903259,DSM: Data Sharing Management system for in-vehicle communication,"The automotive industry is going through a tremendous evolution of its Electrical/Electronic architectures, based on heterogeneous computing units on the same centralized fail-safe hardware. To access the different sensors/actuators dispatched all over the vehicle, these architectures use various communication protocols, each offering a certain degree of determinism and real-time guarantees. However, when multiple data streams pass through these protocols to reach the interoperable computing units, it becomes hard to maintain a consistent data set in the distributed processing environment. To ensure these end-to-end guarantees, we define a Data Sharing Management (DSM) system that acts as a multi-protocol gateway between different components of the processing environment. Evaluation results of the FPGA-based implementation show that the additional latency induced by this DSM is less than 2% (4,276us) of the end-to-end Ethernet latency and less than 6% (370ns) of the end-to-end PCIe latency."
pub.1174082561,A Predictive Profiling and Performance Modeling Approach for Distributed Stream Processing in Edge,"The advent of edge computing has allowed the continuously generated data to be processed closer to their sources instead of being sent to the cloud for processing. Given the heterogeneous and limited computational resources and dynamic nature of edge computing, stream processing systems need an accurate and easily accessible performance modeling/measurement to perform efficiently in edge environments. This paper proposes a predictive profiling model to enable measuring the performance of a system by predicting the operators' processing time on heterogeneous devices without having to carry out the testing on individual devices. This profiling model comprises a quadratic function to generate CPU clock speed/processing time curves for each operator. By using these curves, the model predicts the processing times of operators without requiring any extra profiling runs. Moreover, a performance model is proposed to deal with (performance) degradation of stream processing applications by modeling their topologies as systems comprising M/M/1 queues. The model uses the performance expectations of queueing models to define the data transfer rates inside topologies and uses Integer Linear Programming to specify the maximum input rate and an operator placement plan that can process that input rate. Experimental results showed that the profiling approach predicts the processing times of 17 operators with an average error rate of 5%. The performance model finds the maximum input rate accurately, while the operator placement plan achieves up to 84% higher throughput and 70% less latency in AWS EC2 instances and 257% higher throughput and 66% less latency in real hardware compared to the default resource-aware scheduler of Apache Storm."
pub.1093255426,Accelerating Complex Event Processing through GPUs,"Complex Event Processing (CEP) is a well-known technology in real-time Big Data processing systems. Performance of CEP engines is expected to scale with ever-increasing data rates and complex use cases. CEP operators like stream join and event patterns involve high computational complexity; hence, have a considerable impact on the overall query processing performance. Distributed event processing and CPU-level parallel event processing algorithms are common approaches for improving the performance. We explore how commodity massively parallel architectures like modern Graphics Processing Units (GPUs) can be utilized to improve the performance of frequently used CEP operators. We demonstrate how CEP operators such as event filter, event window, and stream join can be redesigned and implemented on GPUs to gain an order of magnitude improvement in throughput compared to a CPU-based implementation. This work is demonstrated using NVIDIA CUDA based implementation of CEP operators for Siddhi CEP engine on low-end GPUs. Moreover, this approach reduces event queuing at the incoming event queue, even with a large number of event streams, high arrival rates, and several complex queries. Consequently, the average latency experienced by incoming events is also reduced."
pub.1143073465,STAR,"The proliferation of mobile phones and location-based services has given rise to an explosive growth in spatial data. In order to enable spatial data analytics, spatial data needs to be streamed into a data stream warehouse system that can provide real-time analytical results over the most recent and historical spatial data in the warehouse. Existing data stream warehouse systems are not tailored for spatial data. In this paper, we introduce the STAR (Spatial Data Stream Warehouse) system. STAR is a distributed in-memory data stream warehouse system that provides low-latency and up-to-date analytical results over a fast-arriving spatial data stream. STAR supports queries that are composed of aggregate functions and ad hoc query constraints over spatial, textual, and temporal data attributes. STAR implements a cache-based mechanism to facilitate the processing of queries that collectively utilizes the techniques of query-based caching (i.e., view materialization) and object-based caching. Extensive experiments over real data sets demonstrate the superior performance of STAR over existing systems."
pub.1151746640,An Effective Pruning Scheme for Top-k Dominating Query Processing on Uncertain Data Streams,"In the modern age of information explosion, everyone can easily obtain all kinds of data, so how to find the most valuable information in massive data has become an important issue. In general, most data collected from the applications of Internet of Things (IoT) become uncertain since there is a probability or part of the data is missing. However, the calculation of the uncertain data will be much more complicated than certain (or deterministic) data. As a result, the performance of uncertain data handling becomes a significant challenge in meeting low latency requirements. In this work, we propose a distributed computing algorithm and apply it to an edge computing to calculate the probabilistic top-k dominating (PTKD) objects of uncertain data. The overall latency of PTKD query processing is significantly reduced. The main idea of this method is to reduce the cost of time without unnecessary calculations of objects. Experiments show that the proposed algorithm can improve 58% latency on average. With a high pruning rate, performance can be reduced by up to 92%."
pub.1120059569,Model-based Operator Placement for Data Processing in IoT Environments,"The advances of the Internet of Things (IoT) lead to further challenges for data processing. Besides deriving meaningful information from a high amount of raw data, processing data in a timely manner is required as well, in order to enable the development of reactive IoT applications. Usually, the processing of IoT data is done in cloud-based infrastructures, which provide on-demand resources to process the data as needed. However, this affects timely processing, since sending data to off-premise cloud infrastructures increases latency and network traffic. In this paper, we propose a method to process data streams primarily on-premise in IoT environments, i. e., data is processed near to their data sources and the processing power already provided by IoT devices in the environment is explored."
pub.1119872050,Multi-Objective Reinforcement Learning for Reconfiguring Data Stream Analytics on Edge Computing,"There is increasing demand for handling massive amounts of data in a timely manner via Distributed Stream Processing (DSP). A DSP application is often structured as a directed graph whose vertices are operators that perform transformations over the incoming data and edges representing the data streams between operators. DSP applications are traditionally deployed on the Cloud in order to explore the virtually unlimited number of resources. Edge computing has emerged as a suitable paradigm for executing parts of DSP applications by offloading certain operators from the Cloud and placing them close to where the data is generated, hence minimising the overall time required to process data events (i.e., the end-to-end latency). The operator reconfiguration consists of changing the initial placement by reassigning operators to different devices given target performance metrics. In this work, we model the operator reconfiguration as a Reinforcement Learning (RL) problem and define a multi-objective reward considering metrics regarding operator reconfiguration, and infrastructure and application improvement. Experimental results show that reconfiguration algorithms that minimise only end-to-end processing latency can have a substantial impact on WAN traffic and communication cost. The results also demonstrate that when reconfiguring operators, RL algorithms improve by over 50% the performance of the initial placement provided by state-of-the-art approaches."
pub.1124381166,Stream Processing Architecture,"This chapter discusses the architectural aspects of stream processing. The principles of the stream architecture are based on the key properties of many performance-demanding applications and the characteristics of modern VLSI technology. Stream processors are designed for the strengths of modern VLSI technology with minimum execution overheads, providing efficiency on par with application-specific solutions such as Application Specific Integrated Circuits (ASIC). The streaming architecture focuses on effective management of locality (state), bandwidth (communication) and throughput optimization rather than latency. Many of performance-demanding applications come from the signal processing, image processing, graphics, and scientific computing domains. Applications in all of these domains exhibit high degrees of data parallelism, typically have structured control, and can exploit locality. Stream processors embrace the principles of explicit locality and parallelism found in multicore architectures to achieve high performance with high efficiency for applications that use the stream programming model, rather than the traditional parallel extensions to the von Neumann execution model. Parallel accelerators, such as the Nvidia general-purpose graphics processing unit (GPGPU) and the Intel Xeon, are increasingly being proposed to speed up program execution. A GPGPU is a type of many-core architecture processor that is vastly different from the central processing unit (CPU) both in programming interface and performance characteristics. The CPU can synchronize with the graphics processing unit via the driver, but this operation suffers from high overhead."
pub.1132369270,Query processing optimization in broadcasting XML data in mobile communications,"Todays, XML as a de facto standard is used to broadcast data over mobile wireless networks. In these networks, mobile clients send their XML queries over a wireless broadcast channel and recieve their desired XML data from the channel. However, downloading the whole XML data by a mobile device is a challenge since the mobile devices used by clients are small battery powered devices with limited resources.
 To meet this challenge, the XML data should be indexed in such a way that the desired XML data can be found easily and only such data can be downloaded instead of the whole XML data by the mobile clients. Several indexing methods are proposed to selectively access the XML data over an XML stream. However, the existing indexing methods cause an increase in the size of XML stream by including some extra information over the XML stream. In this paper, a new XML stream structure is proposed to disseminate the XML data over a broadcast channel by grouping and summarizing the structural information of XML nodes. By summarizing such information, the size of XML stream can be reduced and therefore, the latency of retrieving the desired XML data over a wirless broadcast channel can be reduced. The proposed XML stream structure also contains indexes in order to skip from the irrelevant parts over the XML stream. It therefore can reduce the energy consumption of mobile devices in downloading the results of XML queries. In addition, our proposed XML stream structure can process different types of XML queries and experimental results showed that it improves the performace of XML query processing over the XML data stream compared to the existing research works in terms of access and tuning times."
pub.1174396135,Design and Implementation of a Data-Flowing Oriented Zero-Trust Security Situational Awareness Framework,"Amid growing concern for safeguarding data assets, conventional security architectures, such as static authentication and privilege control mechanisms, fall short in ensuring data security. This paper aims to reduce data loss utilizing zero-trust architecture while minimising disruption to the original system. A zero-trust security situation awareness framework is developed, focusing on an abstract data-flowing process. The system is operated on a generalized distributed big data stream processing engine, and its performance was evaluated using a test dataset that included scenarios involving malicious insiders and account burglaries. The results indicate that the system performs exceptionally well in identifying and blocking malicious initiators, preventing risky access, and minimising data loss. The system guarantees a latency overhead of 46 ms on average, demonstrating its real-time processing capabilities."
pub.1104571548,MASES,"Dataflow and task graph descriptions are widely used for mapping and scheduling of real-time streaming applications onto heterogeneous processing platforms. Such applications are often characterized by the need to process large-volume data streams with not only high throughput, but also low latency. Mapping such application descriptions into tightly constrained implementations requires optimization of pipelined scheduling of tasks on different processing elements. This poses the problem of finding an optimal solution across a latency-throughput objective space. In this paper, we present a novel list-scheduling based heuristic called MASES for pipelined dataflow scheduling to minimize latency under throughput and heterogeneous resource constraints. MASES explores the flexibility provided by mobility and slack of actors in a partial schedule. It can find a valid schedule if one exists even under tight throughput and resource constraints. Furthermore, MASES can improve runtime by up to 4x while achieving similar results as other latency-oriented heuristics for problems they can solve."
pub.1175798901,Canalis: A Throughput-Optimized Framework for Real-Time Stream Processing of Wireless Communication,"Stream processing, which involves real-time computation of data as it is created or received, is vital for various applications, specifically wireless communication. The evolving protocols, the requirement for high-throughput, and the challenges of handling diverse processing patterns make it demanding. Traditional platforms grapple with meeting real-time throughput and latency requirements due to large data volume, sequential and indeterministic data arrival, and variable data rates, leading to inefficiencies in memory access and parallel processing. We present Canalis, a throughput-optimized framework designed to address these challenges, ensuring high-performance while achieving low energy consumption. Canalis is a hardware-software co-designed system. It includes a programmable spatial architecture, FluxSPU (Flux Stream Processing Unit), proposed by this work to enhance data throughput and energy efficiency. FluxSPU is accompanied by a software stack that eases the programming process. We evaluated Canalis with eight distinct benchmarks. When compared to CPU and GPU in mobile SoC to demonstrate the effectiveness of domain specialization, Canalis achieves an average speedup of 13.4× and 6.6×, and energy savings of 189.8× and 283.9×, respectively. In contrast to equivalent ASICs of the benchmarks, the average energy overhead of Canalis is within 2.4×, successfully maintaining generalizations without incurring significant overhead."
pub.1125766529,SPARCS: Stream-Processing Architecture applied in Real-time Cyber-physical Security,"In this paper, we showcase a complete, end-to-end, fault tolerant, bandwidth and latency optimized architecture for real time utilization of data from multiple sources that allows the collection, transport, storage, processing, and display of both raw data and analytics. This architecture can be applied for a wide variety of applications ranging from automation/control to monitoring and security. We propose a practical, hierarchical design that allows easy addition and reconfiguration of software and hardware components, while utilizing local processing of data at sensor or field site (""fog computing"") level to reduce latency and upstream bandwidth requirements. The system supports multiple fail-safe mechanisms to guarantee the delivery of sensor data. We describe the application of this architecture to cyber-physical security (CPS) by supporting security monitoring of an electric distribution grid, through the collection and analysis of distribution-grid level phasor measurement unit (PMU) data, as well as Supervisory Control And Data Acquisition (SCADA) communication in the control area network."
pub.1095489993,Application of PCI Express Interface in High-Performance Video Systems,"Rapid evolution of the high-performance digital cameras entailed a development of new gigabit interfaces for video streaming and further image processing. A 25 megapixel camera, shooting with 100 frames per second, can easily produce a stream of data reaching tens of gigabits per second. The situation is more complex in case of image processing systems that acquire video stream from many cameras. In this case the video data throughput could reach 100 Gbps. Image acquisition systems require a flexible interface that allows sending data with high throughput. The image stream is usually transmitted to a high performance CPU that collects video data and performs further image processing. The PCI Express (PCIe) bus is widely used in modern computers. The PCIe standard provides a scalable, low latency connection between a CPU and peripheral devices. The paper discuses the application of a PCIe interface in an image acquisition system based on MTCA.4 standard. The system uses an external CPU connected to a MTCA.4 chassis. The initial results of performance evaluation are presented and discussed."
pub.1145284530,Scabbard,"Single-node multi-core stream processing engines (SPEs) can process hundreds of millions of tuples per second. Yet making them fault-tolerant with exactly-once semantics while retaining this performance is an open challenge: due to the limited I/O bandwidth of a single-node, it becomes infeasible to persist all stream data and operator state during execution. Instead, single-node SPEs rely on upstream distributed systems, such as Apache Kafka, to recover stream data after failure, necessitating complex cluster-based deployments. This lack of built-in fault-tolerance features has hindered the adoption of single-node SPEs.
                  We describe Scabbard, the first single-node SPE that supports exactly-once fault-tolerance semantics despite limited local I/O bandwidth. Scabbard achieves this by integrating persistence operations with the query workload. Within the operator graph, Scabbard determines when to persist streams based on the selectivity of operators: by persisting streams after operators that discard data, it can substantially reduce the required I/O bandwidth. As part of the operator graph, Scabbard supports parallel persistence operations and uses markers to decide when to discard persisted data. The persisted data volume is further reduced using workload-specific compression: Scabbard monitors stream statistics and dynamically generates computationally efficient compression operators. Our experiments show that Scabbard can execute stream queries that process over 200 million tuples per second while recovering from failures with sub-second latencies."
pub.1127552880,Performance Modeling and Vertical Autoscaling of Stream Joins,"Streaming analysis is widely used in cloud as well as edge infrastructures.
In these contexts, fine-grained application performance can be based on
accurate modeling of streaming operators. This is especially beneficial for
computationally expensive operators like adaptive stream joins that, being very
sensitive to rate-varying data streams, would otherwise require costly frequent
monitoring.
  We propose a dynamic model for the processing throughput and latency of
adaptive stream joins that run with different parallelism degrees. The model is
presented with progressive complexity, from a centralized non-deterministic up
to a deterministic parallel stream join, describing how throughput and latency
dynamics are influenced by various configuration parameters. The model is
catalytic for understanding the behavior of stream joins against different
system deployments, as we show with our model-based autoscaling methodology to
change the parallelism degree of stream joins during the execution. Our
thorough evaluation, for a broad spectrum of parameter, confirms the model can
reliably predict throughput and latency metrics with a fairly high accuracy,
with the median error in estimation ranging from approximately 0.1% to 6.5%,
even for an overloaded system. Furthermore, we show that our model allows to
efficiently control adaptive stream joins by estimating the needed resources
solely based on the observed input load. In particular, we show it can be
employed to enable efficient autoscaling, even when big changes in the input
load happen frequently (in the realm of seconds)."
pub.1018686286,Towards an extensible efficient event processing kernel,"The efficient processing of large collections of patterns (Boolean expressions, XPath queries, or continuous SQL queries) over data streams plays a central role in major data intensive applications ranging from user-centric processing and personalization to real-time data analysis. On the one hand, emerging user-centric applications, including computational advertising and selective information dissemination, demand determining and presenting to an end-user only the most relevant content that is both user-consumable and suitable for limited screen real estate of target (mobile) devices. We achieve these user-centric requirements through novel high-dimensional indexing structures and (parallel) algorithms. On the other hand, applications in real-time data analysis, including computational finance and intrusion detection, demand meeting stringent subsecond processing requirements and providing high-frequency and low-latency event processing over data streams. We achieve real-time data analysis requirements by leveraging reconfigurable hardware -- FPGAs -- to sustain line-rate processing by exploiting unprecedented degrees of parallelism and potential for pipelining, only available through custom-built, application-specific, and low-level logic design. Finally, we conduct a comprehensive evaluation to demonstrate the superiority of our proposed techniques in comparison with state-of-the-art algorithms designed for event processing."
pub.1104336607,Strome: Energy-Aware Data-Stream Processing,"Handling workloads generated by a large number of users, data-stream–processing systems also require large amounts of energy. To reduce their energy footprint, such systems typically rely on the operating systems of their servers to adjust processor speeds depending on the current workload by performing dynamic voltage and frequency scaling (DVFS). In this paper, we show that, although effective, this approach still leaves room for significant energy savings due to DVFS making conservative assumptions regarding its impact on application performance. To leverage the unused potential we present Strome, an energy-aware technique to minimize energy demand in data-stream–processing systems by dynamically adapting upper limits for the power demand of hardware components. In contrast to DVFS, Strome exploits information on application performance and is therefore able to achieve energy savings while minimizing its effects on throughput and latency. Our evaluation shows that Strome is particularly effective in the face of varying workloads, reducing power demand by up to 25 % compared with the state-of-the-art data-stream–processing system Heron relying on DVFS."
pub.1019629555,FPGA Based Low-Latency Market Data Feed Handler,"Financial market data refers to price and trading data transmitted between financial exchange instruments and traders. Delivery of financial market feeds requires massive data processing with ultra-low latency. FAST protocol is a financial technology standard for compressing data stream during network transmission. This paper presents the design and implementation of a hardware accelerator for financial market data in FAST protocol. We propose a parallel data decoding architecture for field analysis process, which is the key feature in our design. The decoder of this work is able to parse and filter FAST format messages, and with an additional parallel structure compared with typical handlers, achieving a 40% speedup on decoding time compared to previous attempts. The filter function is reconfigurable for various user preferences and further protocol updates. Test under massive source data indicated an average latency of 1.6μs per message."
pub.1092900509,Reducing tail latencies in micro-batch streaming workloads,"Spark Streaming discretizes streams of data into micro-batches, each of which is further sub-divided into tasks and processed in parallel to improve job throughput. Previous work [2, 3] has lowered end-to-end latency in Spark Streaming. However, two causes of high tail latencies remain unaddressed: 1) data is not load-balanced across tasks, and 2) straggler tasks can increase end-to-end latency by 8 times more than the median task on a production cluster [1]. We propose a feedback-control mechanism that allows frameworks to adaptively load-balance workloads across tasks according to their processing speeds. The task runtimes are thus equalized, lowering end-to-end tail latency. Further, this reduces load on machines that have transient resource bottlenecks, thus resolving the bottlenecks and preventing them from having an enduring impact on task runtimes."
pub.1125634136,Using Embeddings for Dynamic Diverse Summarisation in Heterogeneous Graph Streams,"A high-volume of data generated nowadays by the rise of Smart Cities and Internet of Things can be represented as graph streams. While many graph processing algorithms could analyse small graphs when challenging real-world graphs occur in distributed settings like sensor-based ones, a more suitable analysis is needed. Specifically, challenges like dynamism, heterogeneity, continuity and high-volume of these graph streams could benefit from real-time analysis. This analysis should happen with reduced network traffic and latency while maintaining high data expressibility and usability. Therefore, our key question is: Can we define a dynamic graph stream summarisation system that provides expressive graphs while ensuring high usability and limited resource usage? In this paper, we explore this question and propose a multi-source system with windowing, data fusion, conceptual clustering and top-k scoring that can result in expressive, dynamic graph summaries with limited resources at no expense of usability. Our results show that sending top-k fused diverse summarisation, results in 34% to 90% reduction of forwarded messages and redundancy-awareness with an F-score ranging from 0.57 to 0.88 depending on the k compared to sending all the available information. Also, the summaries’ quality follows the agreement of ideal summaries determined by human judges. Nevertheless, these results occur at the expense of higher latency ranging from similar latency to the baseline up to 4 times more depending on the approach; therefore, there is some trade-off between latency, the number of forwarded messages, and expressiveness."
pub.1125113059,Generalizing Streaming Pipeline Design for Big Data,"Streaming data refers to the data that is sent to a cloud or a processing centre in real time. Even though we have limited exposure to such applications that can process data streams on a live basis and generate useful insights, we are still in infancy when it comes to complicated stream processing. Current streaming data analytics tools represents the third-/fourth-generation data processing capability in the big data hierarchy which includes the Hadoop ecosystem, Apache Storm™ and Apache Kafka™ and its likes, Apache Spark™ framework, and now, Apache Flink™ with its non-batch stateful streaming core. Each of these individually cannot handle all the aspects of a data processing pipeline, alone. It is essential to have good synergy between these technologies to cater to the management, streaming, processing and fault-tolerant requirements of various data-driven applications. Companies tailor their pipelines exclusively for their requirements, since making a general framework entails mammoth interfacing and configuration efforts that are not cost-effective for them. In this paper, we envision and implement such a generalized minimal stream processing pipeline and measure its performance, on some data sets, in the form of delays and latencies of data arrival at pivotal checkpoints in the pipeline. We virtualize this using a Docker™ container without much loss in performance."
pub.1168377403,QueryEdge: Real-Time Muti-Video Query in Edge-Cloud Collaborative System,"The real-time query of surveillance video plays a significant role in many fields such as public safety, smart city, and abnormality monitoring. However, with the exponential growth of surveillance video data, traditional cloud-based intelligent video processing faces significant challenges in terms of latency and bandwidth, while the pure edge computing approach is deficient in query accuracy due to its lack of computational power. Existing edge cloud collaboration approaches, such as SurveilEdge, focus on real-time target queries within a single video stream and do not show promising results during target queries across multiple video streams. For this reason, this paper proposes Query Edge, an edge-cloud collaborative real-time query system for multiple video streams. Specifically, we design a real-time query system based on an edge-cloud collaboration framework to achieve highly accurate and low-latency target query services in multiple video streams. In addition, we introduce a prioritization mechanism and a load-balancing strategy in the query task scheduling process to further improve query efficiency. The evaluation proves that QueryEdge has a significant improvement in query latency and bandwidth consumption compared with pure cloud computing, pure edge computing, and SurveilEdge."
pub.1132388677,Locality/Fairness-Aware Job Scheduling in Distributed Stream Processing Engines,"Distributed stream processing engines (DSPEs) deploy multiple tasks on distributed servers to process data streams in real time. Many DSPEs have provided locality-aware stream partitioning (LSP) methods to reduce network communication costs. However, an even job scheduler provided by DSPEs deploys tasks far away from each other on the distributed servers, which cannot use the LSP properly. In this paper, we propose a Locality/Fairness-aware job scheduler (L/F job scheduler) that considers locality together to solve problems of the even job scheduler that only considers fairness. First, the L/F job scheduler increases cohesion of contiguous tasks that require message transmissions for the locality. At the same time, it reduces coupling of parallel tasks that do not require message transmissions for the fairness. Next, we connect the contiguous tasks into a stream pipeline and evenly deploy stream pipelines to the distributed servers so that the L/F job scheduler achieves high cohesion and low coupling. Finally, we implement the proposed L/F job scheduler in Apache Storm, a representative DSPE, and evaluate it in both synthetic and real-world workloads. Experimental results show that the L/F job scheduler is similar in throughput compared to the even job scheduler, but latency is significantly improved by up to 139.2% for the LSP applications and by up to 140.7% even for the non-LSP applications. The L/F job scheduler also improves latency by 19.58% and 12.13%, respectively, in two real-world workloads. These results indicate that our L/F job scheduler provides superior processing performance for the DSPE applications."
pub.1022976575,Influence of Parallelism Property of Streaming Engines on Their Performance,"Recent developments in Big Data are increasingly focusing on supporting computations in higher data velocity environments, including processing of continuous data streams in support of the discovery of valuable insights in real-time. In this work we investigate performance of streaming engines, specifically we address a problem of identifying optimal parameters that may affect the throughput (messages processed/second) and the latency (time to process a message). These parameters are also function of the parallelism property, i.e. a number of additional parallel tasks (threads) available to support parallel computation. In experimental evaluation we identify optimal cluster performance by balancing the degree of parallelism with number of nodes, which yield maximum throughput with minimum latency."
pub.1181398917,Pango FPGA Implementation of a Real-Time Display System for Multi-Channel Video Processing,"A Pango FPGA-based solution for merging and processing multiple independent video streams and reconstructing a real-time display system is presented. The system handles each video stream individually, then produces a combined video output in HDMI format. To ensure high efficiency and low latency, a parallel processing scheme and a hardware-oriented architecture are employed. The Pango PG50K FPGA board acts as the central hardware for managing inputs, reconstructing video, and producing outputs. Rapid-access DDR3-SDRAM ensures efficient data caching and signal integrity. This system is ideal for applications requiring rapid response, such as traffic monitoring and industrial automation, and supports quick algorithm iteration and optimization, enhancing performance and adaptability."
pub.1095217767,Introducing visual latencies into spin-lattice models for image segmentation: a neuromorphic approach to a computer vision problem,In this study we show how an algorithmic principle which might play a role in information processing in the brain of higher vertebrates - the so called visual latencies - can be transferred with high efficiency to a model system which is better suited for implementation on conventional computer hardware. To this end we assign luminance dependent temporal delays (latencies) to the individual pixels of an image. This temporal structure of the input data stream then accelerates and improves the relaxation of a spin-lattice labeling algorithm for scene segmentation.
pub.1125509612,Aion: Better Late than Never in Event-Time Streams,"Processing data streams in near real-time is an increasingly important task.
In the case of event-timestamped data, the stream processing system must
promptly handle late events that arrive after the corresponding window has been
processed. To enable this late processing, the window state must be maintained
for a long period of time. However, current systems maintain this state in
memory, which either imposes a maximum period of tolerated lateness, or causes
the system to degrade performance or even crash when the system memory runs
out.
  In this paper, we propose AION, a comprehensive solution for handling late
events in an efficient manner, implemented on top of Flink. In designing AION,
we go beyond a naive solution that transfers state between memory and
persistent storage on demand. In particular, we introduce a proactive caching
scheme, where we leverage the semantics of stream processing to anticipate the
need for bringing data to memory. Furthermore, we propose a predictive cleanup
scheme to permanently discard window state based on the likelihood of receiving
more late events, to prevent storage consumption from growing without bounds.
  Our evaluation shows that AION is capable of maintaining sustainable levels
of memory utilization while still preserving high throughput, low latency, and
low staleness."
pub.1139811089,Distributed Latent Dirichlet Allocation on Streams," Latent Dirichlet Allocation (LDA) has been widely used for topic modeling, with applications spanning various areas such as natural language processing and information retrieval. While LDA on small and static datasets has been extensively studied, several real-world challenges are posed in practical scenarios where datasets are often huge and are gathered in a streaming fashion. As the state-of-the-art LDA algorithm on streams, Streaming Variational Bayes (SVB) introduced Bayesian updating to provide a streaming procedure. However, the utility of SVB is limited in applications since it ignored three challenges of processing real-world streams: topic evolution , data turbulence , and real-time inference . In this article, we propose a novel distributed LDA algorithm—referred to as StreamFed-LDA— to deal with challenges on streams. For topic modeling of streaming data, the ability to capture evolving topics is essential for practical online inference. To achieve this goal, StreamFed-LDA is based on a specialized framework that supports lifelong (continual) learning of evolving topics. On the other hand, data turbulence is commonly present in streams due to real-life events. In that case, the design of StreamFed-LDA allows the model to learn new characteristics from the most recent data while maintaining the historical information. On massive streaming data, it is difficult and crucial to provide real-time inference results. To increase the throughput and reduce the latency, StreamFed-LDA introduces additional techniques that substantially reduce both computation and communication costs in distributed systems. Experiments on four real-world datasets show that the proposed framework achieves significantly better performance of online inference compared with the baselines. At the same time, StreamFed-LDA also reduces the latency by orders of magnitudes in real-world datasets. "
pub.1095364349,A methodical approach for stream-oriented configurable signal processing,"The characteristics of the signal processing tasks associated with an advanced wireless receiver are well matched to the capabilities offered by CCM (custom computing machine) technology. Collectively, digital receiver algorithms seem to share the following properties: (a) repetitive operations are performed on huge data sets, (b) the dominant computations are conducive to very deep computational pipelines, (c) a moderate amount of latency can be tolerated, and (d) different environmental conditions require different signal processing methods, which in turn require distinct computational structures (time-varying computation). In addition, when coupled with run-time reconfiguration, the amalgamation of signal processing tasks may be compactly combined in a power-efficient computing module. This paper focuses on the design methodology for implementing large and intricate stream-oriented signal processing tasks."
pub.1093673356,Fault-Tolerant and Elastic Streaming MapReduce with Decentralized Coordination,"The MapReduce programming model, due to its simplicity and scalability, has become an essential tool for processing large data volumes in distributed environments. Recent Stream Processing Systems (SPS) extend this model to provide low-latency analysis of high-velocity continuous data streams. However, integrating MapReduce with streaming poses challenges: first, the runtime variations in data characteristics such as data-rates and key-distribution cause resource overload, that inturn leads to fluctuations in the Quality of the Service (QoS); and second, the stateful reducers, whose state depends on the complete tuple history, necessitates efficient fault-recovery mechanisms to maintain the desired QoS in the presence of resource failures. We propose an integrated streaming MapReduce architecture leveraging the concept of consistent hashing to support runtime elasticity along with locality-aware data and state replication to provide efficient load-balancing with low-overhead fault-tolerance and parallel fault-recovery from multiple simultaneous failures. Our evaluation on a private cloud shows up to $2.8\times$ improvement in peak throughput compared to Apache Storm SPS, and a low recovery latency of 700 — 1500 ms from multiple failures."
pub.1148907500,An Analytical Approach Towards Data Stream Processing on Smart Society for Sustainable Development,"In real-time processing, stream has to be processed as soon as it’s generated. Data streams generated from IOT sensors are processed into a finite-size window. A sheer window is considered for processing of data streams in a particular time stamp. In these sheer windows, compare reduce aggregate (CRA) algorithm is applied for determining linear relation for multiple feature vectors. A real-time inference pattern is determine using hash-based classification. In this chapter, sheer window hash-based classification using binarised window analytic (SHCUBA) approach is proposed. This approach is beneficial for calculating linear relationship between sheer windows. SHCUBA approach is comprised of transformation and virtualisation. Here, time and space complexity for this approach is O(n)$$O(n)$$ and O1$$O\left(1\right)$$, respectively. This reduces latency and space requirements for various real-time use cases such as smart applications, sentiment analysis, IOT-based solutions, fraud detection and prevention, stock market prediction, etc. Data generated in smart societies can be correlated using SHCUBA approach for inferring useful decisions."
pub.1093593236,Low Latency Analytics for Streaming Traffic Data with Apache Spark,"Demand for new efficient methods for processing large-scale heterogeneous data in real-time is growing. Currently, one key challenge in Big Data is performing low-latency analysis with real-time data. In vehicle traffic, continuous high speed data streams generate large data volumes. Harnessing new technologies is required to benefit from all the potential this data withholds. This work studies the state-of-the-art in distributed and parallel computing, storage, query and ingestion methods, and evaluates tools for periodical and real-time analysis of heterogeneous data. We also introduce a Big Data cloud platform with ingestion, analysis, storage and data query APIs to provide programmable environment for analytics system development and evaluation."
pub.1112078041,Planner: Cost-efficient Execution Plans Placement for Uniform Stream Analytics on Edge and Cloud,"Stream processing applications handle unbounded and continuous flows of data items which are generated from multiple geographically distributed sources. Two approaches are commonly used for processing: Cloud-based analytics and Edge analytics. The first one routes the whole data set to the Cloud, incurring significant costs and late results from the high latency networks that are traversed. The latter can give timely results but forces users to manually define which part of the computation should be executed on Edge and to interconnect it with the remaining part executed in the Cloud, leading to sub-optimal placements. In this paper, we introduce Planner, a middleware for uniform and transparent stream processing across Edge and Cloud. Planner automatically selects which parts of the execution graph will be executed at the Edge in order to minimize the network cost. Real-world micro-benchmarks show that Planner reduces the network usage by 40% and the makespan (end-to-end processing time) by 15% compared to state-of-the-art."
pub.1061252346,Real-time Public Mood Tracking of Chinese Microblog Streams with Complex Event Processing,"There are not many real-time public mood tracking frameworks over social media streams at present. Real-time public mood tracking over microblogs becomes necessary for further studies with low-latency requirements. To address this issue, we propose a hierarchical framework for real-time public mood time series tracking over Chinese microblog streams using complex event processing. Complex event processing is able to handle high-speed and high-volume data streams. First, we transform microblogs into emotional microblog events through the text sentiment analysis. Then, we apply an online batch window technique to summarize the public mood in different periods. For the public mood time series, we use smoothing and trend following methods to find the rising or falling trends of the public mood. Finally, we apply the method to 6606 microblogs to verify its feasibility. The result demonstrates that the proposed model is not only feasible but also effective."
pub.1148628450,iGPU-Accelerated Pattern Matching on Event Streams,"Pattern matching, also known as Match-Recognize in SQL, is an expensive operator of particular relevance in many event stream applications. However, because of its sequential nature and challenging latency requirements, current stream processing engines do not provide any parallel processing support for pattern matching. In addition, hardware accelerators based on dedicated GPUs also offer limited support due to the overhead of transferring data between their local and main memory. In contrast, however, integrated GPUs (iGPUs), with their ability to access main memory directly, offer great potential to accelerate pattern matching. This paper presents the first full-fledged implementation of pattern matching cooperatively using iGPUs and CPUs. Our results obtained from a preliminary experimental performance comparison confirm the potential of our iGPU-based approaches for accelerating pattern matching."
pub.1141395453,A comprehensive study on fault tolerance in stream processing systems,"Stream processing has emerged as a useful technology for applications which require continuous and low latency computation on infinite streaming data. Since stream processing systems (SPSs) usually require distributed deployment on clusters of servers in face of large-scale of data, it is especially common to meet with failures of processing nodes or communication networks, but should be handled seriously considering service quality. A failed system may produce wrong results or become unavailable, resulting in a decline in user experience or even significant financial loss. Hence, a large amount of fault tolerance approaches have been proposed for SPSs. These approaches often have their own priorities on specific performance concerns, e.g., runtime overhead and recovery efficiency. Nevertheless, there is a lack of a systematic overview and classification of the state-of-the-art fault tolerance approaches in SPSs, which will become an obstacle for the development of SPSs. Therefore, we investigate the existing achievements and develop a taxonomy of the fault tolerance in SPSs. Furthermore, we propose an evaluation framework tailored for fault tolerance, demonstrate the experimental results on two representative open-sourced SPSs and exposit the possible disadvantages in current designs. Finally, we specify future research directions in this domain."
pub.1130307001,Performance-sensitive Data Distribution Method for Distributed Stream Processing Systems,"In the distributed stream processing, data skew and dynamics can result in imbalanced load distribution at downstream tasks and affect the throughput of the systems. Efficient data distribution method is urgently needed to solve the problem of imbalanced load distribution to improve system throughput. Existing researches cannot better balance the semantic correctness and the throughput of distributed stream processing systems, and it is hard to apply to distributed clusters with different node performance. In this paper, we propose a performance-sensitive data distribution method. Firstly, we propose a load balancing framework that considers the performance of nodes. Then we present a data redistribution algorithm, which aims to balance the execution delay between parallel nodes and follows the principle of ""Less Migration key, Less Partition key""(LMLP). The proposed algorithm can reduce the extra cost caused by the data redistribution process and it can be applied to distributed clusters with different performance. Experimental results show that our method is better than the similar methods in terms of load imbalance degree, complete latency, routing table size, etc. And we also use real data to prove that our method can obtain higher throughput and resource utilization."
pub.1156565142,TVSR‐OR: Tile‐based 360‐degree video streaming over real time streaming protocol with optimized read,"Abstract Among video application use cases and scenarios, 360‐degree video applications gradually become significant. The state‐of‐the‐art solution for 360‐degree video streaming is field of view (FOV) transmission, whose quality of experience highly depends on system processing speed and network latency. The extant 360‐degree video FOV transmission profiles transmit multiple video tile streams to the client over either Hypertext Transfer Protocol (HTTP) or Real Time Streaming Protocol (RTSP). The client needs to merge all different tile streams after receiving them, which may cause a data synchronization problem of waiting for each tile to sync in the presents of packet losses. Therefore, we propose a system to select and merge tile streams on the Content Delivery Network (CDN) server and transmit the merged stream to the client. During the streaming service, the client can send a viewport switch request through RTSP signaling, and the server will subsequently deliver the merged video stream of the new viewport to the client after receiving the request. Also, to reduce disk overhead from parallel reads, we optimize the system with a file prefetch strategy to reduce unorganized, random, and parallel reads into serialized reads of large data trunks. We evaluate its performance through user request simulation experiments. Compared with the extant transmission solution, our system provides slightly lower video transmission latency, lower central processing unit (CPU) usage, and better disk performance under heavy service load."
pub.1143613113,Lachesis,"Data streaming applications in Cyber-Physical Systems enable high-throughput, low-latency transformations of raw data into value. The performance of such applications, run by Stream Processing Engines (SPEs), can be boosted through custom CPU scheduling. Previous schedulers in the literature require alterations to SPEs to control the scheduling through user-level threads. While such alterations allow for fine-grained control, they hinder the adoption of such schedulers due to the high implementation cost and potential limitations in application semantics (e.g., blocking I/O). Motivated by the above, we explore the feasibility and benefits of custom scheduling without alterations to SPEs but, instead, by orchestrating the OS scheduler (e.g., using nice and cgroup) to enforce the scheduling goals. We propose Lachesis, a standalone scheduling middleware, decoupled from any specific SPE, that can schedule multiple streaming applications, run in one or many nodes, and possibly multiple SPEs. Our evaluation with real-world and synthetic workloads, several SPEs and hardware setups, shows its benefits over default OS scheduling and other state-of-the-art schedulers: up to 75% higher throughput, and 1130x lower average latency once such SPEs reach their peak processing capacity."
pub.1105972808,Real Time Processing in Mobile Clouds,"With rapid advances in mobile device and cloud computing technologies, a new computing paradigm in which large amounts of data are stored and processed on mobile devices is emerging. Despite the powerful hardware available, mobile devices have limited capacities as they are powered by battery and connected by unstable, low bandwidth, wireless networks. Apache Storm is a scalable platform that provides distributed real-time stream processing paradigm and fault tolerant capability. This paper studies the existing problems of applying Storm to mobile environment, and then proposes a new framework to address these problems with the goal that it would outperform Storm in performance in mobile environment. More specifically, we hope that our framework would reduce processing latency, energy consumption and provide guarantee that processing latency is under certain predefined threshold. Concretely, we formulate the resource allocation and task scheduling optimization problem and propose a heuristic solution to approximate the optimal solution. In our heuristic solution, we generate task scheduling and resource (worker node) allocation strategies according to collected inter-task traffics and latency information of running topologies. Extensive evaluations are performed through proof-of-concept real hardware implementation. Results show that our proposed framework effectively reduces processing latency by up to 50% compared with Storm, also it controls processing latency under certain predefined threshold in abnormal situation."
pub.1118651669,On measuring performances of C-SPARQL and CQELS,"To cope with the massive growth of semantic data streams, several RDF Stream
Processing (RSP) engines have been implemented. The efficiency of their
throughput, latency and memory consumption can be evaluated using available
benchmarks such as LSBench and City- Bench. Nevertheless, these benchmarks lack
an in-depth performance evaluation as some measurement metrics have not been
considered. The main goal of this paper is to analyze the performance of two
popular RSP engines, namely C-SPARQL and CQELS, when varying a set of
performance metrics. More precisely, we evaluate the impact of stream rate,
number of streams and window size on execution time as well as on memory
consumption."
pub.1037451441,Benchmarking Fast-Data Platforms for the Aadhaar Biometric Database,"Aadhaar is the world’s largest biometric database with a billion records, being compiled as an identity platform to deliver social services to residents of India. Aadhaar processes streams of biometric data as residents are enrolled and updated. Besides ∼$$\sim $$1 million enrollments and updates per day, up to 100 million daily biometric authentications are expected during delivery of various public services. These form critical Big Data applications, with large volumes and high velocity of data. Here, we propose a stream processing workload, based on the Aadhaar enrollment and Authentication applications, as a Big Data benchmark for distributed stream processing systems. We describe the application composition, and characterize their task latencies and selectivity, and data rate and size distributions, based on real observations. We also validate this benchmark on Apache Storm using synthetic streams and simulated application logic. This paper offers a unique glimpse into an operational national identity infrastructure, and proposes a benchmark for “fast data” platforms to support such eGovernance applications."
pub.1135085154,A Distributed Stream Data Processing Platform Design and Implementation in Smart Cities,"The existence of bounded data and unbounded data gives a great challenge for data processing in smart cities. The wide application of the internet of things (IoT) makes the data amount rapidly increase. This leads to further raise the requirement for data processing in smart cities, especially the demand for low latency and abundant data in real-time video services. To solve this problem, a Flink based framework with smart city adaption is proposed. A mathematical model for data processing in smart cities is formulated. Through this model's solution, the path with the minimum resource occupancy ratio (ROR) is obtained. The superiority and feasibility of our work are validated via numerical simulation and prototype implementation, respectively."
pub.1135478214,Graceful Performance Degradation in Apache Storm,"The concept of stream data processing is becoming challenging in most business sectors where try to improve their operational efficiency by deriving valuable information from unstructured, yet, contentiously generated high volume raw data in an expected time spans. A modern streamlined data processing platform is required to execute analytical pipelines over a continues flow of data-items that might arrive in a high rate. In most cases, the platform is also expected to dynamically adapt to dynamic characteristics of the incoming traffic rates and the ever-changing condition of underlying computational resources while fulfill the tight latency constraints imposed by the end-users. Apache Storm has emerged as an important open source technology for performing stream processing with very tight latency constraints over a cluster of computing nodes. To increase the overall resource utilization, however, the service provider might be tempted to use a consolidation strategy to pack as many applications as possible in a (cloud-centric) cluster with limited number of working nodes. However, collocated applications can negatively compete with each other, for obtaining the resource capacity in a shared platform that, in turn, the result may lead to a severe performance degradation among all running applications.The main objective of this work is to develop an elastic solution in a modern stream processing ecosystem, for addressing the shared resource contention problem among collocated applications. We propose a mechanism, based on design principles of Model Predictive Control theory, for coping with the extreme conditions in which the collocated analytical applications have different quality of service (QoS) levels while the shared-resource interference is considered as a key performance limiting parameter. Experimental results confirm that the proposed controller can successfully enhance the p$$p$$-99 latency of high priority applications by 67%, compared to the default round robin resource allocation strategy in Storm, during the high traffic load, while maintaining the requested quality of service levels."
pub.1111349206,Pec: Proactive Elastic Collaborative Resource Scheduling in Data Stream Processing,"In the Distributed Parallel Stream Processing Systems (DPSPS), elastic resource allocation allows applications to dynamically response to workload fluctuations. However, resource provisioning can be particularly challenging, due to the unpredictability of the workload. In addition, unlike CPU resources, bandwidth resources are often ignored in resource allocation. Moreover, resource allocation and resource placement are considered separately. In this paper, we investigate the proactive elastic resource scheduling problem for computation-intensive and communication-intensive applications, which aims at meeting the latency requirement with the minimal energy cost, and propose a dynamic collaborative strategy from the systemic perspective. Specifically, we first model a collaborative workload prediction pattern to accurately predict the upcoming workload, and construct a latency estimation model to estimate the latency of the application. Then, we design an energy-efficient resource pre-allocation method, in which the CPU frequency adjustment and the stability of resource reconfigurations are both considered. Finally, we present a communication-aware resource placement approach. Simulation results show that, compared with the reactive strategies, our strategy achieves an obviously better latency performance, and effectively avoids unnecessary resource adjustments. Meanwhile, the energy consumption is about saved by 50 percent on average, and the communication cost is maintained at a very low level of 4 percent."
pub.1026049360,Real-time stream processing for Big Data,"Abstract
                  With the rise of the web 2.0 and the Internet of things, it has become feasible to track all kinds of information over
time, in particular fine-grained user activities and sensor data on their environment and even their biometrics. However,
while efficiency remains mandatory for any application trying to cope with huge amounts of data, only part of the
potential of today's Big Data repositories can be exploited using traditional batch-oriented approaches as the value of
data often decays quickly and high latency becomes unacceptable in some applications. In the last couple of years, several
distributed data processing systems have emerged that deviate from the batch-oriented approach and tackle data items as
they arrive, thus acknowledging the growing importance of timeliness and velocity in Big Data analytics.
                  In this article, we give an overview over the state of the art of stream processors for low-latency Big Data analytics and
conduct a qualitative comparison of the most popular contenders, namely Storm and its abstraction layer Trident, Samza and
Spark Streaming. We describe their respective underlying rationales, the guarantees they provide and discuss the
trade-offs that come with selecting one of them for a particular task."
pub.1039080811,Fault-tolerance in the borealis distributed stream processing system,"Over the past few years, Stream Processing Engines (SPEs) have emerged as a new class of software systems, enabling low latency processing of streams of data arriving at high rates. As SPEs mature and get used in monitoring applications that must continuously run (e.g., in network security monitoring), a significant challenge arises: SPEs must be able to handle various software and hardware faults that occur, masking them to provide high availability (HA). In this article, we develop, implement, and evaluate DPC (Delay, Process, and Correct), a protocol to handle crash failures of processing nodes and network failures in a distributed SPE.
                  Like previous approaches to HA, DPC uses replication and masks many types of node and network failures. In the presence of network partitions, the designer of any replication system faces a choice between providing availability or data consistency across the replicas. In DPC, this choice is made explicit: the user specifies an availability bound (no result should be delayed by more than a specified delay threshold even under failure if the corresponding input is available), and DPC attempts to minimize the resulting inconsistency between replicas (not all of which might have seen the input data) while meeting the given delay threshold. Although conceptually simple, the DPC protocol tolerates the occurrence of multiple simultaneous failures as well as any further failures that occur during recovery.
                  This article describes DPC and its implementation in the Borealis SPE. We show that DPC enables a distributed SPE to maintain low-latency processing at all times, while also achieving eventual consistency, where applications eventually receive the complete and correct output streams. Furthermore, we show that, independent of system size and failure location, it is possible to handle failures almost up-to the user-specified bound in a manner that meets the required availability without introducing any inconsistency."
pub.1118948185,Elasticutor: Rapid Elasticity for Realtime Stateful Stream Processing,"Elasticity is highly desirable for stream processing systems to guarantee low
latency against workload dynamics, such as surges in data arrival rate and
fluctuations in data distribution. Existing systems achieve elasticity
following a resource-centric approach that uses dynamic key partitioning across
the parallel instances, i.e. executors, to balance the workload and scale
operators. However, such operator-level key repartitioning needs global
synchronization and prohibits rapid elasticity. To address this problem, we
propose an executor-centric approach, whose core idea is to avoid
operator-level key repartitioning while implementing each executor as the
building block of elasticity. Following this new approach, we design the
Elasticutor framework with two level of optimizations: i) a novel
implementation of executors, i.e., elastic executors, that perform elastic
multi-core execution via efficient intra-executor load balancing and executor
scaling and ii) a global model-based scheduler that dynamically allocates CPU
cores to executors based on the instantaneous workloads. We implemented a
prototype of Elasticutor and conducted extensive experiments. Our results show
that Elasticutor doubles the throughput and achieves an average processing
latency up to 2 orders of magnitude lower than previous methods, for a dynamic
workload of real-world applications."
pub.1095844754,Power and Area Efficient Sorting Networks Using Unary Processing,"Sorting is a common task in a wide range of applications from signal and image processing to switching systems. For applications that require high performance, sorting is often performed in hardware. Hardware cost and power consumption are the dominant concerns. The usual approach is to wire up a network of compare-and-swap units in a configuration called a Batcher (or Bitonic) network. This paper proposes a novel area-and power-efficient approach to sorting networks based on “unary processing.” Data is encoded as serial bit-streams, with values represented by the fraction of 1's in a stream of 0's and 1's. (This is an evolution of prior work on stochastic logic. Unlike stochastic logic, the unary approach is deterministic and completely accurate.) Synthesis results of complete sorting networks show up to 87% area and power saving compared to the conventional binary implementations. However, the latency increases. To mitigate the increased latency, the paper uses a novel time-encoding of data. The approach is validated with implementation of an important application of sorting: median filtering. The result is a low-cost, energy-efficient implementation of median filtering with only a slight accuracy loss."
pub.1018598520,Proactive elasticity and energy awareness in data stream processing,"Data stream processing applications have a long running nature (24 hr/7 d) with workload conditions that may exhibit wide variations at run-time. Elasticity is the term coined to describe the capability of applications to change dynamically their resource usage in response to workload fluctuations. This paper focuses on strategies for elastic data stream processing targeting multicore systems. The key idea is to exploit Model Predictive Control, a control-theoretic method that takes into account the system behavior over a future time horizon in order to decide the best reconfiguration to execute. We design a set of energy-aware proactive strategies, optimized for throughput and latency QoS requirements, which regulate the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) support offered by modern multicore CPUs. We evaluate our strategies in a high-frequency trading application fed by synthetic and real-world workload traces. We introduce specific properties to effectively compare different elastic approaches, and the results show that our strategies are able to achieve the best outcome."
pub.1175437810,Integrating Serverless and DRL for Infrastructure Management in Streaming Data Processing across Edge-Cloud Continuum,"Advancements in stream data processing engines (SDPE) increasingly require the use of modern Deep Reinforcement Learning (DRL)-based management strategies and distributed computing paradigms like serverless computing to enhance efficiency and scalability across the edge-cloud continuum. This study explores the challenges associated with this integration, especially with Apache Spark. Despite their advanced capabilities, modern SDPEs still lack full maturity in terms of efficiently managing dynamic resource demands and seamlessly integrating with other technologies. To fill this gap, we propose the architecture of ISIM-SDP, acronym for Integrating Serverless and DRL for Infrastructure Management in Streaming Data Processing across edge-cloud continuum. By implementing a DRL-based approach, the system dynamically adjusts resource allocation in real-time, enhancing the flexibility and scalability of computational resources. ISIM-SDP leverages serverless frameworks to reduce operational overhead and improve system responsiveness. Experimental results demonstrate the effectiveness of ISIM-SDP in optimizing resource usage and improving the throughput and latency of stream processing tasks."
pub.1136505472,RDMA-Based Apache Storm for High-Performance Stream Data Processing,"Apache Storm is a scalable fault-tolerant distributed real time stream-processing framework widely used in big data applications. For distributed data-sensitive applications, low-latency, high-throughput communication modules have a critical impact on overall system performance. Apache Storm currently uses Netty as its communication component, an asynchronous server/client framework based on TCP/IP protocol stack. The TCP/IP protocol stack has inherent performance flaws due to frequent memory copying and context switching. The Netty component not only limits the performance of the Storm but also increases the CPU load in the IPoIB (IP over InfiniBand) communication mode. In this paper, we introduce two new implementations for Apache Storm communication components with the help of RDMA technology. The performance evaluation on Mellanox QDR Cards (40 Gbps) shows that our implementations can achieve speedup up to 5×$$\times$$ compared with IPoIB and 10×$$\times$$ with Gigabit Ethernet. Our implementations also significantly reduce the CPU load and increase the throughput of the system."
pub.1092084545,RIoTBench: An IoT benchmark for distributed stream processing systems,"Summary  The Internet of Things (IoT) is an emerging technology paradigm where millions of sensors and actuators help monitor and manage physical, environmental, and human systems in real time. The inherent closed‐loop responsiveness and decision making of IoT applications make them ideal candidates for using low latency and scalable stream processing platforms. Distributed stream processing systems (DSPS) hosted in cloud data centers are becoming the vital engine for real‐time data processing and analytics in any IoT software architecture. But the efficacy and performance of contemporary DSPS have not been rigorously studied for IoT applications and data streams. Here, we propose RIoTBench , a real‐time IoT benchmark suite, along with performance metrics, to evaluate DSPS for streaming IoT applications. The benchmark includes 27 common IoT tasks classified across various functional categories and implemented as modular microbenchmarks. Further, we define four IoT application benchmarks composed from these tasks based on common patterns of data preprocessing, statistical summarization, and predictive analytics that are intrinsic to the closed‐loop IoT decision‐making life cycle. These are coupled with four stream workloads sourced from real IoT observations on smart cities and smart health, with peak streams rates that range from 500 to 10 000 messages/second from up to 3 million sensors. We validate the RIoTBench suite for the popular Apache Storm DSPS on the Microsoft Azure public cloud and present empirical observations. This suite can be used by DSPS researchers for performance analysis and resource scheduling, by IoT practitioners to evaluate DSPS platforms, and even reused within IoT solutions. "
pub.1093690600,Concentus: Applying Stream Processing to Online Collective Interaction,"The collective experience is the experience of unity, belonging, and purpose that occurs when large numbers of people come together and perceive themselves and others as part of a single social entity; and interact with each another accordingly. We are exploring how the collective experience can be supported in a fully computer-mediated environment through activities where a virtual crowd performs synchronous collective action over a shared focal state (e.g. collectively controlling a character in a game; pulsing text-based messages in time to form collective chants). Supporting collective interaction requires a system architecture that is able to process large numbers of input actions into an aggregated collective representation at low latency. We have created a scalable distributed system called Concentus that applies approaches found in distributed stream processing to online collective interaction. Concentus allows for different implementations of aggregation engine, the primary component of the system, to be measured in-situ with other core components (e.g. client connection handlers). We have evaluated the performance of two aggregation approaches: one based on Spark Streaming, a general purpose distributed stream processing engine, and another that performs aggregation on a single thread on one machine; and have measured their performance against the key metric of interaction latency (time from input submission to perceiving the effect on the shared state) as the crowd size scales. The evaluation revealed that both approaches are capable of supporting 50,000 participants with latencies under 1 second; with the single threaded approach performing better on smaller data sizes, and Spark Streaming on larger data sets. We discuss the implications on collective application design."
pub.1160374326,Utility-Aware Load Shedding for Real-time Video Analytics at the Edge,"Real-time video analytics typically require video frames to be processed by a
query to identify objects or activities of interest while adhering to an
end-to-end frame processing latency constraint. Such applications impose a
continuous and heavy load on backend compute and network infrastructure because
of the need to stream and process all video frames. Video data has inherent
redundancy and does not always contain an object of interest for a given query.
We leverage this property of video streams to propose a lightweight Load
Shedder that can be deployed on edge servers or on inexpensive edge devices
co-located with cameras and drop uninteresting video frames. The proposed Load
Shedder uses pixel-level color-based features to calculate a utility score for
each ingress video frame, which represents the frame's utility toward the query
at hand. The Load Shedder uses a minimum utility threshold to select
interesting frames to send for query processing. Dropping unnecessary frames
enables the video analytics query in the backend to meet the end-to-end latency
constraint with fewer compute and network resources. To guarantee a bounded
end-to-end latency at runtime, we introduce a control loop that monitors the
backend load for the given query and dynamically adjusts the utility threshold.
Performance evaluations show that the proposed Load Shedder selects a large
portion of frames containing each object of interest while meeting the
end-to-end frame processing latency constraint. Furthermore, the Load Shedder
does not impose a significant latency overhead when running on edge devices
with modest compute resources."
pub.1024915677,Cut-and-Rewind: Extending Query Engine for Continuous Stream Analytics,"Combining data warehousing and stream processing technologies has great potential in offering low-latency data-intensive analytics. Unfortunately, such convergence has not been properly addressed so far. The current generation of stream processing systems is in general built separately from the data warehouse and query engine, which can cause significant overhead in data access and data movement, and is unable to take advantage of the functionalities already offered by the existing data warehouse systems.In this work we tackle some hard problems in integrating stream analytics capability into the existing query engine. We define an extended SQL query model that unifies queries over both static relations and dynamic streaming data, and develop techniques to extend query engines to support the unified model. We propose the cut-and-rewind query execution model to allow a query with full SQL expressive power to be applied to stream data by converting the latter into a sequence of “chunks”, and executing the query over each chunk sequentially, but without shutting the query instance down between chunks for continuously maintaining the application context across the execution cycles as required by sliding-window operators. We also propose the cycle-based transaction model to support Continuous Querying with Continuous Persisting (CQCP) with cycle-based isolation and visibility.We have prototyped our approach by extending the PostgreSQL. This work has resulted in a new kind of tightly integrated, highly efficient system with the advanced stream processing capability as well as the full DBMS functionality. We demonstrate the system with the popular Linear Road benchmark, and report the performance. By leveraging the matured code base of a query engine to the maximal extent, we can significantly reduce the engineering investment needed for developing the streaming technology. Providing this capability on proprietary parallel analytics engine is work in progress."
pub.1091566755,Monitoring and Mining Data Streams,"This work is preliminary since funding for this piece was only received 6 months ago. We have so far managed to complete a revision of the Borealis code base making it much more usable for general applications. We have worked closely with personnel from USARIEM to identify special processing needs for PAN's in a WPSM setting and have redesigned major portions of the Borealis code base to reflect this. It takes a novel position with respect to dealing with failure in a sensor network. The Aurora stream processing engine was a very valuable exercise to gain understanding about the basic technical questions that must be addressed by any stream processing engine. These issues included memory management, tuple scheduling, and load control. Aurora was designed to run on a single server, and its primary optimization goal was low-latency processing. This set of assumptions was chosen because it gave us the best opportunity to study the fundamentals, and it was useful for a large class of monitoring applications."
pub.1128825409,An analysis of technological frameworks for data streams,"Real-time data analysis is becoming increasingly important in Big Data environments for addressing data stream issues. To this end, several technological frameworks have been developed, both open-source and proprietary, for the analysis of streaming data. This paper analyzes some open-source technological frameworks available for data streams, detailing their main characteristics. The objective is to facilitate decisions on which framework to use, meeting the needs of data mining methods for data streams. In this sense, there are important factors affecting the choice about which framework is most suitable for this purpose. Some of these factors are the existence of data mining libraries, the available documentation, the maturity of the platform, fault tolerance and processing guarantees, among others. Another decisive factor when choosing a data stream framework is its performance. For this reason, two comparisons have been made: a performance and latency comparison between Spark Streaming, Spark Structured Streaming, Storm, Flink and Samza following the Yahoo Streaming Benchmark methodology, and a comparison between Spark Streaming and Flink with a clustering algorithm for data streaming called streaming K-means."
pub.1148867820,Phoebe: QoS-Aware Distributed Stream Processing through Anticipating Dynamic Workloads,"Distributed Stream Processing systems have become an essential part of big
data processing platforms. They are characterized by the high-throughput
processing of near to real-time event streams with the goal of delivering
low-latency results and thus enabling time-sensitive decision making. At the
same time, results are expected to be consistent even in the presence of
partial failures where exactly-once processing guarantees are required for
correctness. Stream processing workloads are oftentimes dynamic in nature which
makes static configurations highly inefficient as time goes by. Static resource
allocations will almost certainly either negatively impact upon the Quality of
Service and/or result in higher operational costs.
  In this paper we present Phoebe, a proactive approach to system auto-tuning
for Distributed Stream Processing jobs executing on dynamic workloads. Our
approach makes use of parallel profiling runs, QoS modeling, and runtime
optimization to provide a general solution whereby configuration parameters are
automatically tuned to ensure a stable service as well as alignment with
recovery time Quality of Service targets. Phoebe makes use of Time Series
Forecasting to gain an insight into future workload requirements thereby
delivering scaling decisions which are accurate, long-lived, and reliable. Our
experiments demonstrate that Phoebe is able to deliver a stable service while
at the same time reducing resource over-provisioning."
pub.1139022325,BAN-Storm: a Bandwidth-Aware Scheduling Mechanism for Stream Jobs,"The essential component of the Big Data system is the processing frameworks and engines responsible for crunching the data. To cope with the growing computing demands of real-time Big Data applications, researchers have proposed several computing frameworks. The core of the computing frameworks i.e., the scheduling mechanisms for real-time stream processing need to accommodate several important aspects such as incorporating resource awareness, heterogeneity of the computing resources, load balancing, etc. These aspects contribute significantly to the attained performance of the computing frameworks. Therefore, ignoring one of these aspects may lead to degraded performance. Most of the present stream processing frameworks do not consider the communication patterns and heterogeneity of the computing resources. This causes the highly communicating tasks mapped on different and costly remote nodes resulting in the increased communication overheads and latencies. In this work, we propose BAN-Storm, a stream scheduler that considers inter-task communication along the other important scheduling aspects such as heterogeneity, etc. to schedule stream jobs. The core objective of the proposed scheduler is to gain performance (i.e., higher throughput and reduced latency) using a resource-aware mapping mechanism. The proposed BAN-Storm schedules stream jobs considering Inter-task communication and machine’s computing power. The BAN-Storm employs a two-phase mapping mechanism i.e., in the first phase, the tasks are grouped so that the inter-group communication becomes low. In the second phase, for the resource-aware mapping, the computing power of each node is calculated using FLOPS, Memory (i.e., RAM), and Bandwidth followed by the task-group assignment to nodes (mapping on more capable nodes first). Apache Storm is used for the implementation of the proposed BAN-Storm scheduling mechanism. Experimental evaluation is done using the two real application topologies. The attained results are benchmarked using the three state-of-the-art stream schedulers. The thorough experimental results show up to 30% higher attained throughput as compared to the Apache Storm scheduler. Moreover, the attained results show that the proposed BAN-Storm provisions up to 33–66% fewer resources as compared to the default Storm."
pub.1037342817,WebLogic event server,"This paper describes WebLogic Event Server (WL EvS), an application server designed for hosting event-driven applications that require low latency and deterministic behavior. WL EvS is based on a modular architecture in which both server components and applications are represented as modules. The application programming model supports applications that are a mixture of reusable Java components and EPL (Event Processing Language), a query language that extends SQL with stream processing capabilities. WL EvS applications are meta-data driven, in that application behavior can be changed without recompilation or redeploying an application. The paper also presents the results of a benchmark performance study. The results show that the approach used by WL EvS can handle extremely high volumes of events while providing deterministic latency."
pub.1181641026,Apache Kafka on Big Data Event Streaming for Enhanced Data Flows,"Apache Kafka's distributed architecture and message queuing capabilities offer significant improvements in real-time and batch data processing efficiency and reliability. This research aims to optimize Kafka setups, data partitioning, and Kafka Connect integration to create a robust and scalable data streaming infrastructure. The study focuses on enhancing data input, processing, and dissemination across systems and applications. By optimizing Kafka configurations and leveraging its capabilities, the research seeks to achieve significant improvements in data processing speed, real-time analytics, and scalable data pipelines. The evaluation of Kafka Event Stream Throughput Over Time and Latency Distribution Across Brokers demonstrates the system's performance and efficiency. The results indicate a throughput of 750-1340 events per hour and a latency distribution of 6-15 milliseconds. Additionally, Consumer Lag Over Time analysis reveals consistent performance with values ranging from 70-140. This research contributes to the advancement of big data processing by demonstrating the effectiveness of Apache Kafka in creating a robust and efficient data streaming infrastructure. The findings provide valuable insights for organizations seeking to optimize their data pipelines and leverage the power of real-time analytics."
pub.1051996004,Supporting a spectrum of out-of-order event processing technologies,"This demonstration presents a complex event processing system which focuses on out-of-order handling. State-of-the-art event stream processing technology experiences significant challenges when faced with out-of-order data arrival including huge system latencies, missing results, and incorrect result generation. We propose two out-of-order handling techniques, conservative and aggressive strategies. We will show the efficiency of our techniques and how they can satisfy various QoS requirements of different applications."
pub.1169058331,SDADS: Stream Data Anomaly Detection System,"In a cloud-native architecture, the operational data of various system components experiences a significant increase. From the distributed complex system, obtaining the operation status data and realizing real-time monitoring and abnormal alarm play an important role in guaranteeing the smooth production. However, handling a large volume of stream data in real-time poses challenges such as high computational demands, low latency, and high concurrency. Therefore, this study presents the design and implementation of a system for anomaly detection in stream data called Stream Data Anomaly Detection System (SDADS). SDADS leverages message queues for processing business data and further processes the data using a distributed computing framework. It also provides functions for persistent data storage and anomaly detection alerts. The use of SDADS enables the management of complex business processes such as server provisioning and application deployment, simplifies business development logic, reduces operational time cost, and allows users to focus solely on the anomaly detection algorithms themselves."
pub.1151140708,Modeling the estimation of end-to-end packet latency for a chain of NFV nodes in 5G networks,"It is expected that future communication networks will provide configurable delay-sensitive types of services (for example, streaming video, machine interaction). To support a variety of applications and use cases of servers providing various functions, you can use network function virtualization (NFV), which will be able to provide flexible implementation and placement of configuration of the necessary network functions. This article analyzes the end-to-end packet latency (E2E) for multiple traffic flows passing through the chain of embedded virtual network functions (VNF) in fifth-generation communication networks (5G). The Dominant of Generalized Resource Processing (DR-GPS) is used to distribute computing resources and transfer data between threads in each node of Network Function Virtualization (NFV) to achieve equitable distribution and utilization of available resources. The tandem queuing model is designed for incoming packets combined in several streams passing through the NFV node and its outgoing transmission channel. To analyze manageability, we separate packet processing (and transmission) of various streams in the simulation and determine the average packet processing and transmission rates of each stream as approximate service speeds."
pub.1004322524,"Incremental, iterative data processing with timely dataflow","We describe the timely dataflow model for distributed computation and its implementation in the Naiad system. The model supports stateful iterative and incremental computations. It enables both low-latency stream processing and high-throughput batch processing, using a new approach to coordination that combines asynchronous and fine-grained synchronous execution. We describe two of the programming frameworks built on Naiad: GraphLINQ for parallel graph processing, and differential dataflow for nested iterative and incremental computations. We show that a general-purpose system can achieve performance that matches, and sometimes exceeds, that of specialized systems."
pub.1124224498,Extended Kalman Filter for Large Scale Vessels Trajectory Tracking in Distributed Stream Processing Systems,"The growing number of vessel data being constantly reported by a variety of remote sensors, such as the Automatic Identification System (AIS), requires new data analytics that can operate at high data rates and are highly scalable. Based on a real-world dataset from maritime transport, we propose a large scale vessel trajectory tracking application implemented in the distributed stream processing system Apache Flink. By implementing a state-space model (SSM) - the Extended Kalman Filter (EKF) - we firstly demonstrate that an implementation of SSMs is feasible in modern distributed data flow systems and secondly we show that we can reach a high performance by leveraging the inherent parallelization of the distributed system. In our experiments we show that the distributed tracking system is able to handle a throughput of several hundred vessels per ms. Moreover, we show that the latency to predict the position of a vessel is well below 500 ms on average, allowing for real-time applications."
pub.1176111935,Enabling Adaptive Sampling for Intra-Window Join: Simultaneously Optimizing Quantity and Quality,">Sampling is one of the most widely employed approximations in big data processing. Among various challenges in sampling design, sampling for join is particularly intriguing yet complex. This perplexing problem starts with a classical case where the join of two Bernoulli samples shrinks its output size quadratically and exhibits a strong dependency on the input data, presenting a unique challenge that necessitates adaptive sampling to guarantee both the quantity and quality of the sampled data. The community has made strides in achieving this goal by constructing offline samples and integrating support from indexes or key frequencies. However, when dealing with stream data, due to the need for real-time processing and high-quality analysis, methods developed for processing static data become unavailable. Consequently, a fundamental question arises: Is it possible to achieve adaptive sampling in stream data without relying on offline techniques? To address this problem, we propose FreeSam, which couples hybrid sampling with intra-window join, a key stream join operator. Our focus lies on two widely used metrics: output size, ensuring quantity, and variance, ensuring quality. FreeSam enables adaptability in both the desired quantity and quality of data sampling by offering control on the two-dimensional space spanned by these metrics. Meanwhile, adjustable trade-offs between quality and performance make FreeSam practical for use. Our experiments show that, for every 1% increase in latency limitation, FreeSam can yield a 3.83% increase in the output size while maintaining the level of the estimator's variance. Additionally, we give FreeSam a multi-core implementation and ensure predictability of its latency through both an analytic model and a neural network model. The accuracy of these models is 88.05% and 96.75% respectively."
pub.1129697904,Simultaneous Transmitting and Air Computing for High-Speed Point-to-Point Wireless Communication,"The development of the fifth-generation (5G) cellular technologies promotes the rapid deployment of the Internet of Things (IoT), where low latency is highly demanded for wireless devices with small size and limited resources. For the intensive computation tasks, how to reduce the latency is still a challenging issue along with the boosting of IoT devices. To reduce the latency, in this paper we consider high-speed point-to-point wireless communications, e.g., orbital-angular-momentum (OAM) based wireless communications, and develop a practical scheme with the computation executed during transmissions. Specifically, we design a new weighted function for the data-stream transmitted on each OAM-mode to achieve the simultaneous transmitting and air computing (STAC). Then, we propose the STAC signal detection scheme to directly obtain the computation result without individually processing each transmitted data-stream. Theoretical analyses show that the computation latency can be efficiently decreased. Also, the symbol error rate (SER) is significantly reduced with our proposed scheme, which helps decrease the number of re-transmissions and the corresponding time used for communications. Simulation and numerical results verify the performance improvements with the proposed scheme."
pub.1151082430,Phoebe: QoS-Aware Distributed Stream Processing through Anticipating Dynamic Workloads,"Distributed Stream Processing systems have become an essential part of big data processing platforms. They are characterized by the high-throughput processing of near to real-time event streams with the goal of delivering low-latency results and thus enabling time-sensitive decision making. At the same time, results are expected to be consistent even in the presence of partial failures where exactly-once processing guarantees are required for correctness. Stream processing workloads are oftentimes dynamic in nature which makes static configurations highly inefficient as time goes by. Static resource allocations will almost certainly either negatively impact upon the Quality of Service and/or result in higher operational costs. In this paper we present Phoebe, a proactive approach to system auto-tuning for Distributed Stream Processing jobs executing on dynamic workloads. Our approach makes use of parallel profiling runs, QoS modeling, and runtime optimization to provide a general solution whereby configuration parameters are automatically tuned to ensure a stable service as well as alignment with recovery time Quality of Service targets. Phoebe makes use of Time Series Forecasting to gain an insight into future workload requirements thereby delivering scaling decisions which are accurate, long-lived, and reliable. Our experiments demonstrate that Phoebe is able to deliver a stable service while at the same time reducing resource over-provisioning."
pub.1094933105,Optimal Processing Node Discovery Algorithm for Distributed Computing in IoT,"The number of Internet-connected sensing and control devices is growing. Some anticipate them to number in excess of 212 billion by 2020. Inherently, these devices generate continuous data streams, many of which need to be stored and processed. Traditional approaches, whereby all data are shipped to the cloud, may not continue to be effective as cloud infrastructure may not be able to handle myriads of data streams and their associated storage and processing needs. Using cloud infrastructure alone for data processing significantly increases latency, and contributes to unnecessary energy inefficiencies, including potentially unnecessary data transmission in constrained wireless networks, and on cloud computing facilities increasingly known to be significant consumers of energy. In this paper we present a distributed platform for wireless sensor networks which allows computation to be shifted from the cloud into the network. This reduces the traffic in the sensor network, intermediate networks, and cloud infrastructure. The platform is fully distributed, allowing every node in a homogeneous network to accept continuous queries from a user, find all nodes satisfying the user's query, find an optimal node (Fermat-Weber point) in the network upon which to process the query, and provide the result to the user. Our results show that the number of required messages can be decreased up to 49% and processing latency by 42% in comparison with state-of-the-art approaches, including Innet."
pub.1171915811,Vortex: A Stream-oriented Storage Engine For Big Data Analytics,"Organizations are increasingly looking for ways to simplify collection and transformation of vast amounts of data collected from a highly connected internet. Data analytics over continuous streams of data enables interactive applications and reduces time to insights. Traditionally, streaming data collection and analysis has been either achieved by building systems, or using data warehouses built for batch processing. In this paper, we present Vortex, a storage engine that we built inside Google BigQuery to support real-time analytics. Vortex is a streaming-first storage system that supports both streaming and batch data analytics. Today, BigQuery uses Vortex to support petabyte scale data ingestion with sub-second data freshness and query latency."
pub.1181148509,High-throughput Real-time Edge Stream Processing with Topology-Aware Resource Matching,"With the proliferation of Internet of Things (IoT) devices, real-time stream processing at the edge of the network has gained significant attention. However, edge stream processing systems face substantial challenges due to the heterogeneity and constraints of computational and network resources and the intricacies of multi-tenant application hosting. An optimized placement strategy for edge application topology becomes crucial to leverage the advantages offered by Edge computing and enhance the throughput and end-to-end latency of data streams. This paper presents Beaver, a resource scheduling framework designed to deploy stream processing topologies across distributed edge nodes efficiently. Its core is a novel scheduler that employs a synergistic integration of graph partitioning within application topologies and a two-sided matching technique to optimize the strategic placement of stream operators. Beaver aims to achieve optimal performance by minimizing bottlenecks in the network, memory, and CPU resources at the edge. We implemented a prototype of Beaver using Apache Storm and Kubernetes orchestration engine and evaluated its performance using an open-source real-time IoT benchmark (RIoTBench). Compared to state-of-the-art techniques, experimental evaluations demonstrate at least 1.6× improvement in the number of tuples processed within a one-second deadline under varying network delay and bandwidth scenarios."
pub.1175679247,Exploring Real-Time Data Processing Using Big Data Frameworks,"Big data frameworks that weaken the throughput of data processing, allowing for real-time data processing like Apache Spark, Kafka, and Flink are other developments. Regarding quick decisions by each measurement, the scalability, fault tolerance, and latency of three architectures Here each stream processing, lambda, and Kappa have been further studied and measured to approach a conclusion. Based on a methodical survey of literature, performance laws, and case studies, all three frameworks and architectures pros and cons measure us, which can then be used for separate operations use situations. For example, our studies have shown that the smudge feels natural for micro-batch situations, and for high-throughput, Kafka, and actual stream processing, Flink feels natural due to complicated event time handling. However, the findings from this research show a relevant effect on the system architecture for companies that want to implement real-time data. Future research needs to study the possibility of edge computing to reduce reaction time and data processing bottlenecks, as well as the possibility of connecting that with machine knowledge opportunities to better understand predictive analysis. This illustrates the research that practitioners and researchers have to do to make real-time data processing part of their operations"
pub.1040678858,Low communication overhead dynamic mapping of multiple HEVC video stream decoding on NoCs,"The High Efficiency Video Coding (HEVC) standard offers several parallelisation tools such as wave-front parallel processing (WPP) and Tiles (independent frame regions) to better manage the computationally expensive workloads on modern multicore/many-core platforms. However, poor allocation of tile-level HEVC decoding tasks to processing elements may result in increased latency and energy consumption due to data-communication overhead between dependent tiles. In this work, we discuss the difficulties in decoding multiple HEVC bitstreams with highly varying resolutions and data-dependency characteristics as seen in HEVC coded video streams with random-access, adaptive group of pictures (GoP) structures. Secondly, in order to address the above challenges, we introduce a runtime tile allocation scheme that help to reduce the energy usage during HEVC decoding. Evaluations against a bin-packing algorithm, show that the proposed workload mapping technique is able to maintain reasonably acceptable latency results, whilst reducing communication overhead (8-10%) and increasing the mean processor idle periods (~30%) to support dynamic power management."
pub.1152877417,VEI,"The large scale use of real-time computer vision for IoT applications faces challenges of big data streams, complex processing, low latency requirements, and data privacy concerns. Edge computing allows data to be processed close to the source, vastly reducing the data that needs to be sent to the cloud, thus reducing network bandwidth requirements, and lowering application latency. Additionally, sensitive video streams can be confined to the privacy perimeter of the end-user. However, current IoT edge middleware are designed for low data rate sensor applications, and do not satisfy the demanding needs of computer vision-based IoT. In this paper, we present the design and implementation of a novel edge gateway targeted specifically at emerging IoT computer vision applications. The proposed edge gateway enables realization of multiple vision algorithms at the edge from a single camera stream. Furthermore, unlike existing edge gateways available from public cloud service providers, the proposed gateway is vendor-neutral, and capable of connecting to multiple cloud providers. This allows for increased application resilience, lower costs, and avoids cloud vendor lock-in. We experimentally evaluate the performance of the proposed edge gateway for multiple computer vision applications, and multiple public clouds."
pub.1095135581,Evaluation of Scheduling Heuristics for Jitter Reduction of Real-Time Streaming Applications on Multi-core General Purpose Hardware,"The real-time system research community has paid a lot of attention to the design of safety critical hard real-time systems for which the use of non-standard hardware and operating systems can be justified. However, stream processing applications like medical imaging systems are often not considered safety critical enough to justify the use of hard real-time techniques that would increase the cost of these systems significantly. Instead commercial off the shelf (COTS) hardware and OS are used, and techniques at the application level are employed to reduce the variation in the end-to-end latency of these imaging processing systems. In this paper, we study the effectiveness of a number of scheduling heuristics that are intended to reduce the latency and the jitter of stream processing applications that are executed on COTS multiprocessor systems. The proposed scheduling heuristics take the execution times of tasks into account as well as dependencies between the tasks, the data structures accessed by the tasks, and the memory hierarchy. Experiments were carried out on a quad core symmetric multiprocessing (SMP) Intel processor. These experiments show that the proposed heuristics can reduce the end-to-end latency with almost 60%, and reduce the variation in the latency with more than 90% when compared with a naive scheduling heuristic that does not consider execution times, dependencies and the memory hierarchy."
pub.1111250460,Integrating workload balancing and fault tolerance in distributed stream processing system,"Distributed Stream Processing Engine (DSPE) is designed for processing continuous streams so as to achieve the real-time performance with low latency guaranteed. To satisfy such requirement, the availability and efficiency are the main concern of the DSPE system, which can be achieved by a proper design of the fault tolerance module and the workload balancing module, respectively. However, the inherent characteristics of data streams, including persistence, dynamic and unpredictability, pose great challenges in satisfying both properties. As far as we know, most of the state-of-the-art DSPE systems take either fault tolerance or workload balancing as its single optimization goal, which in turn receives a higher resource overhead or longer recovery time. In this paper, we combine the fault tolerance and workload balancing mechanisms in the DSPE to reduce the overall resource consumption while keeping the system interactive, high-throughput, scalable and highly available. Based on our data-level replication strategy, our method can handle the dynamic data skewness and node failure scenario: during the distribution fluctuation of the incoming stream, we rebalance the workload by selectively inactivate the data in high-load nodes and activate their replicas on low-load nodes to minimize the migration overhead within the stateful operator; when a fault occurs in the process, the system activates the replicas of the data affected to ensure the correctness while keeping the workload balanced. Extensive experiments on various join workloads on both benchmark data and real data show our superior performance compared with baseline systems."
pub.1157650959,Optimizing Data Stream Throughput for Real-Time Applications,"Many problems, like recommendation services, website log activities, commit logs, and event sourcing services, are often very high volume and high velocity as many activity messages are generated. Various cloud vendors offer services for building message transport and processing pipelines. We present the Apache Kafka optimization process for using Kafka as a messaging system in high throughput and low latency systems. Experiments are conducted with various configurations for observing their effects on performance metrics. The performance, Durability, and reliability trade-offs of Apache Kafka are presented for various data rates, resource utilization, and configurations on how to utilize resources more economically and choose the best Kafka configuration for use cases. This research presents by tuning configuration parameters leveraged throughput by 19%."
pub.1163025076,Accelerating Stream Processing Queries with Congestion-aware Scheduling and Real-time Linux Threads,"Stream Processing Engines (SPEs) have been used by companies and industries to develop queries able to extract insights from data streams. The Edge/IoT context poses additional challenges, since streaming queries need to run closer to data producers to save latency, i.e., on resource-constrained devices. Lachesis is a middleware helping Linux to schedule more efficiently threads of the SPE, which revealed useful especially for devices with limited CPU resources. Lachesis does not require any architectural change to the SPE implementation. It collects metrics from the SPE, and computes high-level priorities that are converted into hints to the Operating System to affect its actual scheduling of threads. This paper extends the initial contribution of Lachesis in two main directions: i) we optimize the policy assigning to threads a priority proportional to their actual load by accurately studying the implementation of Storm and Flink, two popular SPEs; ii) instead of restricting the OS scheduling to traditional SCHED_OTHER threads as done previously by Lachesis, we leverage the real-time capability of the modern Linux kernel. Our experimental evaluation shows that both enhancements provide important benefits compared with the previous version of Lachesis: we get +9.75% (average) throughput (+19% peak) with --27% latency on average (--40% peak)."
pub.1149940430,Automatic Performance Tuning for Distributed Data Stream Processing Systems,"Distributed data stream processing systems (DSPSs) such as Storm, Flink, and Spark Streaming are now routinely used to process continuous data streams in (near) real-time. However, achieving the low latency and high throughput demanded by today's streaming applications can be a daunting task, especially since the performance of DSPSs highly depends on a large number of system parameters that control load balancing, degree of parallelism, buffer sizes, and various other aspects of system execution. This tutorial offers a comprehensive review of the state-of-the-art automatic performance tuning approaches that have been proposed in recent years. The approaches are organized into five main categories based on their methodologies and features: cost modeling, simulation-based, experiment-driven, machine learning, and adaptive tuning. The categories of approaches will be analyzed in depth and compared to each other, exposing their various strengths and weaknesses. Finally, we will identify several open research problems and challenges related to automatic performance tuning for DSPSs."
pub.1113066080,Multi-Level Elasticity for Data Stream Processing,"This paper investigates reactive elasticity in stream processing environments where the performance goal is to analyze large amounts of data with low latency and minimum resources. Working in the context of Apache Storm, we propose an elastic management strategy which modulates the parallelism degree of applications’ components while explicitly addressing the hierarchy of execution containers (virtual machines, processes and threads). We show that provisioning the wrong kind of container may lead to performance degradation and propose a solution that provisions the least expensive container (with minimum resources) to increase performance. We describe our monitoring metrics and show how we take into account the specifics of an execution environment. We provide an experimental evaluation with real-world applications which validates the applicability of our approach."
pub.1095171770,Interactive Querying and Data Visualization for Abuse Detection in Social Network Sites,"Big Data technologies have traditionally operated in an offline setting, collecting large batches of information on clusters of commodity machines and performing complex and time-consuming computations over it. While frameworks following this approach served well for most applications involving big data analysis during the last decade, other use cases have recently emerged posing challenging requirements on latency and demanding real-time data processing, querying and visualization. That is the case for applications aiming at detecting threatening behaviors in social network platforms, where timely action is required to avoid adverse consequences. In this sense, more and more attention has been drawn towards online data processing systems claiming to address the limitations of batch-oriented frameworks. This paper reports a work in progress on distributed data processing for enabling low-latency querying over big data sets. Two software architectures are discussed for addressing the problem and an experimental evaluation is performed on a proof of concept implementation showing how an approach based on query pre-processing and stateful distributed stream computation can meet the requirements for supporting interactive querying on large and continuously generated data."
pub.1092571458,Sub-millisecond Stateful Stream Querying over Fast-evolving Linked Data,"Applications like social networking, urban monitoring and market feed processing require stateful stream query: a query consults not only streaming data but also stored data to extract timely information; useful information from streaming data also needs to be continuously and consistently integrated into stored data to serve inflight and future queries. However, prior streaming systems either focus on stream computation, or are not stateful, or cannot provide low latency and high throughput to handle the fast-evolving linked data and increasing concurrency of queries. This paper presents Wukong+S, a distributed stream querying engine that provides sub-millisecond stateful query at millions of queries per-second over fast-evolving linked data. Wukong+S uses an integrated design that combines the stream processor and the persistent store with efficient state sharing, which avoids the cross-system cost and sub-optimal query plan in conventional composite designs (e.g., Storm/Heron+Wukong). Wukong+S uses a hybrid store to differentially manage timeless data and timing data accordingly and provides an efficient stream index with locality-aware partitioning to facilitate fast access to streaming data. Wukong+S further provides decentralized vector timestamps with bounded snapshot scalarization to scale with nodes and massive queries at efficient memory usage. We have designed Wukong+S conforming to the RDF data model and Continuous SPARQL (C-SPARQL) query interface and have implemented Wukong+S by extending a state-of-the-art static RDF store (namely Wukong). Evaluation on an 8-node RDMA-capable cluster using LSBench and CityBench shows that Wukong+S significantly outperforms existing system designs (e.g., CSPARQL-engine, Storm/Heron+Wukong, and Spark Streaming/Structured Streaming) for both latency and throughput, usually at the scale of orders of magnitude."
pub.1119218951,P4CEP: Towards In-Network Complex Event Processing,"In-network computing using programmable networking hardware is a strong trend
in networking that promises to reduce latency and consumption of server
resources through offloading to network elements (programmable switches and
smart NICs). In particular, the data plane programming language P4 together
with powerful P4 networking hardware has spawned projects offloading services
into the network, e.g., consensus services or caching services. In this paper,
we present a novel case for in-network computing, namely, Complex Event
Processing (CEP). CEP processes streams of basic events, e.g., stemming from
networked sensors, into meaningful complex events. Traditionally, CEP
processing has been performed on servers or overlay networks. However, we argue
in this paper that CEP is a good candidate for in-network computing along the
communication path avoiding detouring streams to distant servers to minimize
communication latency while also exploiting processing capabilities of novel
networking hardware. We show that it is feasible to express CEP operations in
P4 and also present a tool to compile CEP operations, formulated in our P4CEP
rule specification language, to P4 code. Moreover, we identify challenges and
problems that we have encountered to show future research directions for
implementing full-fledged in-network CEP systems."
pub.1105955329,P4CEP,"In-network computing using programmable networking hardware is a strong trend in networking that promises to reduce latency and consumption of server resources through offloading to network elements (programmable switches and smart NICs). In particular, the data plane programming language P4 together with powerful P4 networking hardware has spawned projects offloading services into the network, e.g., consensus services or caching services. In this paper, we present a novel case for in-network computing, namely, Complex Event Processing (CEP). CEP processes streams of basic events, e.g., stemming from networked sensors, into meaningful complex events. Traditionally, CEP processing has been performed on servers or overlay networks. However, we argue in this paper that CEP is a good candidate for in-network computing along the communication path avoiding detouring streams to distant servers to minimize communication latency while also exploiting processing capabilities of novel networking hardware. We show that it is feasible to express CEP operations in P4 and also present a tool to compile CEP operations, formulated in our P4CEP rule specification language, to P4 code. Moreover, we identify challenges and problems that we have encountered to show future research directions for implementing full-fledged in-network CEP systems."
pub.1095700865,"The First International Workshop on Distributed Event Processing, Systems and Applications (DEPSA'07)","An increasing number of applications today are based on consuming, processing, and producing data streams that take the form of time ordered series of events. Many of these applications, such as e-business process management, systems and network monitoring, financial analysis, and security surveillance, need to handle large volumes of events and produce results with low-latency. This new paradigm of distributed computing and data processing, which is based on event flows and event processing, has also created a vast number of new research problems in a wide area of topics, ranging from systems performance and scalability to programming models and language support to applications in real-time data mining."
pub.1000769044,Synopsis based load shedding in XML streams,"Stream systems are susceptible to variations in data arrival rate. At times, data arrival rate may spike up to cause unacceptable output latencies and unpredictable system behavior. Recently, load shedding systems have been proposed to deal with this situation. But almost all these systems are for relational data streams and, to the best of our knowledge, none has been proposed for XML data streams so far except [15]. Dropping data randomly may have been an effective method for load shedding in the relational context, due to the uniformity of relational data. But in the XML context, the same method will lead to much invasive negative effect on processing of XML queries due to the recursive and nested structure of XML data. We propose a load shedding framework for XML data streams. We explore the effectiveness of various load shedding techniques based on a general load shedding strategy that takes into account QoS parameters and relative accuracy of the query results. We implement various load shedding strategies and present their result."
pub.1127952463,DLEEL: Multi-Predicate Spatial Queries on User-generated Streaming Data,"This paper demonstrates DLEEL; a research system that supports scalable spatial queries with multiple predicates on user-generated data streams, such as social media streams. Supported queries include spatial-social queries and spatial-keyword queries, which are popular in different applications but have never been addressed in the challenging environment of streaming data, where data arrives with excessively high rates. DLEEL distinguishes itself with three novel contributions: (1) Indexing spatial-social data in for personalized real-time search: DLEEL is the first to address personalized queries on streaming spatial- social data through novel low-overhead indexing that scales for large amounts of data and users. The novel indexing has a hybrid storage architecture that trades off indexing overhead, memory consumption, and query latency. (2) Indexing spatial-keyword data for real-time search: DLEEL is the first to enrich existing spatial-keyword indexes with novel streaming data components. The new components reveal performance losses and gains from a system perspective, trading off the system overhead with flexibility to support a variety of queries. (3) Scalable query processing: DLEEL exploits the indexes content to smartly prune the search space on multiple dimensions and support efficient query latency for its different queries on excessive number of data records. DLEEL is demonstrated using a stream of 5 billions real tweets collected from Twitter APIs and real query locations obtained from a popular web search engine. DLEEL has shown superior performance with serving incoming queries with an average latency of few milliseconds while digesting hundreds of thousands of data records every second."
pub.1112321597,Efficient resource scheduling for the analysis of Big Data streams,"The emergence of Big Data has had a profound impact on how data are analyzed. Open source distributed stream processing platforms have gained popularity for analyzing streaming Big Data as they provide low latency required for streaming Big Data applications using cluster resources. However, existing resource schedulers are still lacking the efficiency that Big Data analytical applications require. Recent works have already considered streaming Big Data characteristics to improve the efficiency of scheduling in the platforms. Nevertheless, they have not taken into account the specific attributes of analytical applications. This study, therefore, presents Bframework, an efficient resource scheduling framework used by streaming Big Data analysis applications based on cluster resources. Bframework proposes a query model using Directed Graphs (DGs) and introduces operator assignment and operator scheduling algorithms based on a novel partitioning algorithm. Bframework is highly adaptable to the fluctuation of streaming Big Data and the availability of cluster resources. Experiments with the benchmark and well-known real-world queries show that Bframework can significantly reduce the latency of streaming Big Data analysis queries up to about 65%."
pub.1143225296,A Survey of IoT Stream Query Execution Latency Optimization within Edge and Cloud,"IoT (Internet of Things) streaming data has increased dramatically over the recent years and continues to grow rapidly due to the exponential growth of connected IoT devices. For many IoT applications, fast stream query processing is crucial for correct operations. To achieve better query performance and quality, researchers and practitioners have developed various types of query execution models—purely cloud‐based, geo‐distributed, edge‐based, and edge‐cloud‐based models. Each execution model presents unique challenges and limitations of query processing optimizations. In this work, we provide a comprehensive review and analysis of query execution models within the context of the query execution latency optimization. We also present a detailed overview of various query execution styles regarding different query execution models and highlight their contributions. Finally, the paper concludes by proposing promising future directions towards advancing the query executions in the edge and cloud environment."
pub.1126658198,An Architecture for QoS-Aware Fog Service Provisioning," The proliferation of IoT technologies and the broad deployment of sensors and IoT devices are changing the way and the speed of delivery of many services. IoT data streams are typically transmitted to cloud services for processing. However, time-sensitive IoT applications cannot tolerate high latency they may experience when IoT data streams are sent to the cloud. Fog computing-based solutions for this kind of applications are becoming more and more attractive due to the low latency they can provide and guarantee. Given the growing deployments of fog nodes, we propose in this paper an architecture for quality of service (QoS) aware fog service provisioning, which allows to schedule the execution of IoT applications’ tasks on a cluster of fog nodes. A fog broker component can implement various scheduling policies to help IoT applications fulfill their QoS requirements. The results of the simulations we performed show that using some simple strategies, it is possible to keep low the latency of applications and distribute the load among the fog nodes of the cluster."
pub.1101866488,Dynamically Improving Resiliency to Timing Errors for Stream Processing Workloads,"Large-scale data processing paradigms, such as stream processing, are widespread in academic and corporate workloads. These environments are commonly subject to real-time requirements, such as latency and throughput, and resiliency requirements to node or network failures. These requirements have generally been approached as separate problems. Intermittent timing delays due to factors such as garbage collection can further complicate the management of the stream processing workload. Insufficient resource allocations can also lead to poor performance. Currently, tuning these applications is done manually. We show that improper configuration can greatly affect performance. It is reported that even 100ms of increased latency in online sales platforms can potentially result in lower sales. In this paper we propose Dynamo, a framework and monitor that implements a methodology for addressing both the performance and timing error problems by increasing the resiliency of stream processing frameworks to timing delays. Dynamo autonomously adjusts the resource allocation by using a performance profile that is generated through application profiling. Dynamo partitions an application's allocated resources into active and passive partitions that are dynamically adjusted to match an application's multi-modal behavior. The distribution of resources determines the amount of computation that Dynamo can duplicate and process redundantly, thereby reducing the probability of timing errors that affect a tuple's total execution time. In our experiments, we observed improvements in the number of tuples with missed deadlines. Our results show that Dynamo is able to consistently improve the resiliency to timing errors over a number of differing occurrence rates. Furthermore, we show that the improvement in the number of missed deadlines increases with the amount of spare resources, with a 71.40% reduction in the best case."
pub.1158125873,On combining system and machine learning performance tuning for distributed data stream applications,"The growing need to identify patterns in data and automate decisions based on them in near-real time, has stimulated the development of new machine learning (ML) applications processing continuous data streams. However, the deployment of ML applications over distributed stream processing engines (DSPEs) such as Apache Spark Streaming is a complex procedure that requires extensive tuning along two dimensions. First, DSPEs have a plethora of system configuration parameters, like degree of parallelism, memory buffer sizes, etc., that have a direct impact on application throughput and/or latency, and need to be optimized. Second, ML models have their own set of hyperparameters that require tuning as they can affect the overall prediction accuracy of the trained model significantly. These two forms of tuning have been studied extensively in the literature but only in isolation from each other. This manuscript presents a comprehensive experimental study that combines system configuration and hyperparameter tuning of ML applications over DSPEs. The experimental results reveal unexpected and complex interactions between the choices of system configurations and hyperparameters, and their impact on both application and model performance. These insights motivate the need for new combined system and ML model tuning approaches, and open up new research directions in the field of self-managing distributed stream processing systems."
pub.1120111317,Distributed Streaming Analytics on Large-scale Oceanographic Data using Apache Spark,"Real-world data from diverse domains require real-time scalable analysis.
Large-scale data processing frameworks or engines such as Hadoop fall short
when results are needed on-the-fly. Apache Spark's streaming library is
increasingly becoming a popular choice as it can stream and analyze a
significant amount of data. In this paper, we analyze large-scale geo-temporal
data collected from the USGODAE (United States Global Ocean Data Assimilation
Experiment) data catalog, and showcase and assess the ability of Spark stream
processing. We measure the latency of streaming and monitor scalability by
adding and removing nodes in the middle of a streaming job. We also verify the
fault tolerance by stopping nodes in the middle of a job and making sure that
the job is rescheduled and completed on other nodes. We design a full-stack
application that automates data collection, data processing and visualizing the
results. We also use Google Maps API to visualize results by color coding the
world map with values from various analytics."
pub.1156715013,STREAM: Toward READ-Based In-Memory Computing for Streaming-Based Processing for Data-Intensive Applications,"With the rise of data-intensive applications, traditional computing paradigms have hit the memory-wall. In-memory computing using emerging nonvolatile memory (NVM) technology is a promising solution strategy to overcome the limitations of the von-Neumann architecture. In-memory computing using NVM devices has been explored in both analog and digital domains. Analog in-memory computing can perform matrix–vector multiplication (MVM) in an extremely energy-efficient manner. However, analog in-memory computing is prone to errors and resulting precision is therefore low. On the contrary, digital in-memory computing is a viable option for accelerating scientific computations that require deterministic precision. In recent years, several digital in-memory computing styles have been proposed. Unfortunately, state-of-the-art digital in-memory computing styles rely on repeated WRITE operations which involve switching of NVM devices. WRITE operations in NVM cells are expensive in terms of energy, latency, and device endurance. In this article, we propose a READ-based in-memory computing framework called STREAM. The framework performs streaming-based data processing for data-intensive applications. The STREAM framework consists of a synthesis tool that decomposes an arbitrary Boolean function into in-memory compute kernels. Two synthesis approaches are proposed to generate READ-based in-memory compute kernels using data structures from logic synthesis. A hardware/software co-design technique is developed to minimize the intercrossbar data communication. The STREAM framework is evaluated using circuits from the ISCAS85 benchmark suite, and Suite-Sparse applications to scientific computing. Compared with state-of-the-art in-memory computing framework, the proposed framework improves latency and energy performance with up to $200 \times $ and $20\times $ , respectively."
pub.1095702678,Hardware Accelerator for Minimum Mean Square Error Interference Alignment,"Ahstract-A dedicated hardware architecture for the digital baseband processing of minimum mean square error interference alignment is presented. The computationally intensive task of calculating the precoding and decoding matrices has been implemented and the underlying algorithm has been optimized for real-time capability, efficiency and flexibility. The required number of iterations has been optimized and appropriate low-latency algorithms for the computation of basic operations have been identified to meet a real-time constraint of 1 ms processing latency. The architecture has been verified and synthesized for a Xilinx Virtex-6 LX550T FPGA. The maximum number of antennas, users and data streams is configurable at synthesis time. The actual parameters are configurable at runtime. Different degrees of parallelism allow a tradeoff between resource requirements, latency and throughput. The target FPGA resources are sufficient for real-time system configurations up to 5 users with 3 antennas."
pub.1084900353,"Implementing a Volunteer Notification System into a Scalable, Analytical Realtime Data Processing Environment","The pace at which next-generation Internet of Things networks, consisting of wirelessly distributed sensors and devices, are being developed is speeding up. More and more devices produce data in automated manners and the demand of smartphones and wearable devices is continuously increasing. With respect to volunteer notification systems (VNS), the resulting vast amounts of data can be utilized for profiling and predicting the whereabouts of people that, combined with machine learning algorithms, complement artificial intelligence (AI)-based decision systems. Hence, VNS benefit from keeping pace with the current developments by using the corresponding data streams in order to improve decision making during the volunteer selection process. In emergency scenarios, the velocity, low latency and reaction times of the system are essential, which results in the need of online stream-processing and real-time computational solutions. This paper will focus on a basic concept for implementing a VNS approach into a scalable, fault-tolerant environment that uses state-of-the-art analytical tools to process information streams in real-time as well as on demand, and applies machine learning algorithms for an AI-based volunteer selection. This work concentrates on leveraging open source Big Data technologies with the aim to deliver a robust, secure and highly available enterprise-class Big Data platform. Within the given context, this work will furthermore give an insight on state-of-the-art proprietary solutions for Big Data processing that are currently available."
pub.1148518919,Rafiki: Task-Level Capacity Planning in Distributed Stream Processing Systems,"Distributed Stream Processing is a valuable paradigm for reliably processing vast amounts of data at high throughput rates with low end-to-end latencies. Most systems of this type offer a fine-grained level of control to parallelize the computation of individual tasks within a streaming job. Adjusting the parallelism of tasks has a direct impact on the overall level of throughput a job can provide as well as the amount of resources required to provide an adequate level of service. However, finding optimal parallelism configurations that fall within the expected Quality of Service requirements is no small feat to accomplish.In this paper we present Rafiki, an approach to automatically determine optimal parallelism configurations for Distributed Stream Processing jobs. Here we conduct a number of proactive profiling runs to gather information about the processing capacities of individual tasks, thereby making the selection of specific utilization targets possible. Understanding the capacity information enables users to adequately provision resources so that streaming jobs can deliver the desired level of service at a reduced operational cost with predictable recovery times. We implemented Rafiki prototypically together with Apache Flink where we demonstrate its usefulness experimentally."
pub.1012412979,Continuous analytics on geospatial data streams with WSO2 complex event processor,"DEBS Grand Challenge is a yearly, real-life data based event processing challenge posted by Distributed Event Based Systems conference. The 2015 challenge uses a taxi trip data set from New York city that includes 173 million events collected over the year 2013. This paper presents how we used WSO2 CEP, an open source, commercially available Complex Event Processing Engine, to solve the problem. With a 8-core commodity physical machine, our solution did about 350,000 events/second throughput with mean latency less than one millisecond for both queries. With a VirtualBox VM, our solution did about 50,000 events/second throughput with mean latency of 1 and 6 milliseconds for the first and second queries respectively. The paper will outline the solution, present results, and discuss how we optimized the solution for maximum performance."
pub.1160131742,INDIANA—In-Network Distributed Infrastructure for Advanced Network Applications,"Data volumes are exploding as sensors proliferate and become more capable. Edge computing is envisioned as a path to distribute processing and reduce latency. Many models of Edge computing consider small devices running conventional software. Our model includes a more lightweight execution engine for network microservices and a network scheduling framework to configure network processing elements to process streams and direct the appropriate traffic to them. In this article, we describe INDIANA, a complete framework for in-network microservices. We will describe how the two components-the INDIANA network Processing Element (InPE) and the Flange Network Operating System (NOS)-work together to achieve effective in-network processing to improve performance in edge to cloud environments. Our processing elements provide lightweight compute units optimized for efficient stream processing. These elements are customizable and vary in sophistication and resource consumption. The Flange NOS provides first-class flow based reasoning to drive function placement, network configuration, and load balancing that can respond dynamically to network conditions. We describe design considerations and discuss our approach and implementations. We evaluate the performance of stream processing and examine the performance of several exemplar applications on networks of increasing scale and complexity."
pub.1095692574,FPGA accelerated low-latency market data feed processing,"Modern financial exchanges provide updates to their members on the changing status of the market place, by providing streams of messages about events, called a market data feed. Markets are growing busier, and the data-rates of these feeds are already in the gigabit range, from which customers must extract and process messages with sub-millisecond latency. This paper presents an FPGA accelerated approach to market data feed processing, using an FPGA connected directly to the network to parse, optionally decompress, and filter the feed, and then to push the decoded messages directly into the memory of a general purpose processor. Such a solution offers flexibility, as the FPGA can be reconfigured for new data feed formats, and high throughput with low latency by eliminating the operating system's network stack. This approach is demonstrated using the Celoxica AMDC board, which accepts a pair of redundant data feeds over two gigabit Ethernet ports, parses and filters the data, then pushes relevant messages directly into system memory over the PCIe bus. Tests with an ORPA FAST data feed redistribution system show that the AMDC is able to process up to 3.5M messages per second, 12 times the current real-world rate, while the complete system rebroadcasts at least 99% of packets with a latency of less than 26us. The hardware portion of the design has a constant latency, irrespective of throughput, of 4us."
pub.1125165158,Maximum Sustainable Throughput Evaluation Using an Adaptive Method for Stream Processing Platforms,"The volume and type of streaming data is increasing rapidly, thus, real-time processing scenarios for streaming data have continued to increase. The inherent volatility of streaming data causes large changes in the throughput volatility of stream processing platforms, making evaluations of the maximum sustainable throughput (MST) for such platforms a challenge. To address these problems and improve the low efficiency of manual evaluations, a naïve method is typically used to evaluate the MST of platforms by periodically measuring throughput. The throughput volatility is detected using a skip sliding window that is defined as a bounded sequence in which a limited number of throughput values stored to measure volatility in a skipping fashion in each data growth cycle, which is a fixed time interval with respect to the continuous data rate increases. A skipping sliding window that is used to describe the procedure with which the MST is calculated in an approximate way is based on a sliding window, which is used usually in the context of stateful streaming operators. Then, according to the throughput volatility, the method judges whether the MST has been reached using a latency guarantee. However, because this method has low efficiency and a large error rate, we propose an adaptive MST evaluation method that adds a data-growth factor function to the naïve method cycle that dynamically and adaptively tunes the data rate for each data growth cycle. The experimental results from four open-source benchmarks running on three mainstream stream processing platforms show that the adaptive MST evaluation method has a lower error rate and executes faster compared with the naïve evaluation method. Moreover, the proposed method is insensitive to parameter variations and is suitable for mainstream stream processing platforms."
pub.1110109734,StreamChain,"Processing at block granularity and blockchains seem inseparable. The original role of blocks is to amortize the cost of cryptography (e.g., solving proof-of-work) and to make data transfers more efficient in a geo-distributed setting. While blocks are a simple and powerful tool for amortizing these costs, today in permissioned distributed ledgers, that are often neither geo-distributed, nor require proof-of-work, the benefits of operating on blocks are overshadowed by the large latencies they introduce. Our proposal is to switch the distributed ledger processing paradigm from block processing to stream transaction processing and rely on batching (i.e., block formation) only for amortizing the cost of disk accesses for commit operations. This paradigm shift enables shaving off end-to-end latencies by more than an order of magnitude and opens up new use-cases for permissioned ledgers. We demonstrate a proof-of-concept of our idea using Hyperledger Fabric, achieving end-to-end latencies of less than 10ms while maintaining relatively high throughput, namely close to 1500 tps."
pub.1095506523,Scalable and Reliable Monitoring for Power Systems,"There is a growing interest in monitoring wide-area power grids, where dedicated devices, so-called phasor measurement units (PMUs), located in substations within the grid, steadily provide measurement data to control centers, thereby enabling the continuous supervision of the system state. The trend towards increasing numbers of PMUs to provide a more accurate and finer-grained system view gives rise to scalability and availability challenges that current monitoring systems cannot handle. Strict latency requirements for the delivery of measurement data further exacerbates the measurement data acquisition and processing problem. We propose a new distributed system using an integrated cloud-based approach for PMU data acquisition, processing, and storage, capable of scaling to large numbers of PMUs while exhibiting robustness in the face of failures. Unlike other wide-area monitoring systems, our proposed system runs on a distributed stream processing platform, thereby making it eligible for cloud-based environments. Our evaluation, which focuses on large-scale data acquisition, shows that the system is able to meet stringent latency requirements and bounds on data loss at a large scale while scaling out the underlying processing infrastructure."
pub.1000611958,Experience in Extending Query Engine for Continuous Analytics,"Combining data warehousing and stream processing technologies has great potential in offering low-latency data-intensive analytics. Unfortunately, such convergence has not been properly addressed so far. The current generation of stream processing systems isin general built separately from the data warehouse and query engine, which can causesignificant overhead in data access and data movement, and is unable to take advantage of the functionalities already offered by the existing data warehouse systems.In this work we tackle some hard problems not properly addressed previously in integrating stream analytics capability into the existing query engine. We define an extended SQL query model that unifies queries over both static relations and dynamic streaming data, and develop techniques to extend query engines to support the unified model. We propose the cut-and-rewind query execution model to allow a query with full SQL expressive power to be applied to stream data by converting the latter into a sequence of “chunks”, and executing the query over each chunk sequentially, but without shutting the query instance down between chunks for continuously maintaining the application context across the execution cycles as required by sliding-window operators. We also propose the cycle-based transaction model to support Continuous Querying with Continuous Persisting (CQCP) with cycle-based isolation and visibility.We have prototyped our approach by extending the PostgreSQL. This work has resulted in a new kind of tightly integrated, highly efficient system with the advanced stream processing capability as well as the full DBMS functionality. We demonstrate it with the popular Linear Road benchmark, and report the performance. By leveraging the matured code base of a query engine to the maximal extent, we can significantly reduce the engineering investment needed for developing the streaming technology. Providing this capability on proprietary parallel analytics engine is work in progress."
pub.1093404964,"Low Complexity, Low Latency Resampling of Asynchronously Sampled Signals","In Signal processing, activity driven data acquisition is of interest as it offers power savings and data compression when dealing with sparse real world signals. However, such schemes naturally result in asynchronous samples which cannot be handled by traditional signal processing. Hence, there is a need for efficient resampling schemes that convert the asynchronous samples to a synchronous stream. This work proposes two novel low latency non-iterative resampling schemes that allow for pipelined hardware and real-time software implementations. The first scheme caters to low complexity applications and is a modification to the Akima algorithm [1]. The second scheme addresses high performance applications and is based on windowed sinc interpolation. Simulation results are presented to demonstrate the performance. Complexity comparison with existing methods is also provided."
pub.1095301719,AccStream: Accuracy-aware Overload Management for Stream Processing Systems,"With the rapid growth of social media and Internet-of-Things, real-time processing of big data has become a core operation in various business areas. It is of paramount importance that big-data analyses are executed timely with specified accuracy guarantees. However, workloads in the wild are highly bursty with skewed contents and often present the conundrum of meeting latency and accuracy requirements simultaneously. In this paper we propose AccStream, which selectively samples and processes data tuples and blocks on emerging batch streaming platforms with a special focus on analysis of aggregation, e.g., counts, and top-k. AccStream dynamically learns the latency model of analysis jobs via on-line probing technique and employs sample theory to determine the lower limit of data so as to fulfill given accuracy targets. A unique feature of AccStream ensuring strong latency-accuracy fulfillment even under conflicts is the hybrid windowing that trades off data freshness via a combination of tumbling and rolling windows. We evaluate the prototype of AccStream on Spark Streaming, analyzing Twitter data. Our extensive results confirm that AccStream is able to achieve the latency and accuracy target against a wide range of conditions, i.e., slow and fast dynamic load intensities and content skewnesses, even when facing conflicting latency and accuracy targets. All in all, the effectiveness of AccStream in delivering timely, accurate, and (partial) fresh streaming analytics lies in shedding the adequate amount of input data at the right time and place."
pub.1100893786,Stateful Load Balancing for Parallel Stream Processing,"Timely processing of streams in parallel requires dynamic load balancing to diminish skewness of data. In this paper we study this problem for stateful operators with key grouping for which the process of load balancing involves a lot of state movements. Consequently, load balancing is a bi-objective optimization problem, namely Minimum-Cost-Load-Balance (MCLB). We address MCLB with two approximate algorithms by a certain relaxation of the objectives: (1) a greedy algorithm ELB performs load balancing eagerly but relaxes the objective of load imbalance to a range; and (2) a periodic algorithm CLB aims at reducing load imbalance via a greedy procedure of minimizing the covariance of substreams but ignores the objective of state movement by amortizing the overhead of it over a relative long period. We evaluate our approaches with both synthetic and real data. The results show that they can adapt effectively to load variations and improve latency efficiently comparing to the existing solutions whom ignored the overhead of state movement in stateful load balancing."
pub.1094802108,A Memory-based Continuous Query Index for Stream Processing,"Most of the “Big Data” applications, such as decision support and emergency response, must provide users with fresh, low latency results, especially for aggregation results on key performance metrics. However, disk-oriented approaches to online storage are becoming increasingly problematic. They do not scale grace-fully to meet the needs of large-scale Web applications, and improvements in disk capacity have far out-stripped improvements in access latency and bandwidth. To this end, the paper proposes a memory-based continuous query index to implement scalable and efficient aggregation query."
pub.1175880369,Decision-change Informed Rejection Improves Robustness in Pattern Recognition-based Myoelectric Control,"Post-processing techniques have been shown to improve the quality of the
decision stream generated by classifiers used in pattern-recognition-based
myoelectric control. However, these techniques have largely been tested
individually and on well-behaved, stationary data, failing to fully evaluate
their trade-offs between smoothing and latency during dynamic use.
Correspondingly, in this work, we survey and compare 8 different
post-processing and decision stream improvement schemes in the context of
continuous and dynamic class transitions: majority vote, Bayesian fusion, onset
locking, outlier detection, confidence-based rejection, confidence scaling,
prior adjustment, and adaptive windowing. We then propose two new temporally
aware post-processing schemes that use changes in the decision and confidence
streams to better reject uncertain decisions. Our decision-change informed
rejection (DCIR) approach outperforms existing schemes during both steady-state
and transitions based on error rates and decision stream volatility whether
using conventional or deep classifiers. These results suggest that added
robustness can be gained by appropriately leveraging temporal context in
myoelectric control."
pub.1150377505,"Morphological, Object Detection Framework for Embedded, Event-based Sensing","This paper presents a high-speed, object detection algorithm that leverages data from an event-based camera and a spike-based, cellular neural network framework for morphological processing. Event-based data flows into the algorithm in a time-serial, asynchronous fashion, but the algorithm and subsequent architecture description lends itself towards a parallel approach. A cellular neural-network (CNN) is composed of multimodal processing elements that provide the means to spatiotemporally filter event data, but also are used to apply a cascade of piece-wise linear functions in a synchronous fashion. When applied in succession, these morphological operations form object “blobs”, produce shape skeletons, and place centroids. Over an event stream, this rapid centroid placement provides a means to perform low-latency object detection in an embedded framework. Using processing intervals of 25ms and assuming a clock of 100 MHz, a computational latency of around 5.5μs is incurred and an estimated 161uW is consumed by the morphological algorithm thus providing a promising solution for event-based, embedded processing."
pub.1117021016,Towards Context-Aware and Dynamic Management of Stream Processing Pipelines for Fog Computing,"Newly arising IoT-driven use cases often require low-latency anaiytics to derive time-sensitive actions, where a centralized cloud approach is not applicable. An emerging computing paradigm, referred to as fog computing, shifts the focus away from the central cloud by offloading specific computational parts of analytical stream processing pipelines (SPP) towards the edge of the network, thus leveraging existing resources close to where data is generated. However, in scenarios of mobile edge nodes, the inherent context changes need to be incorporated in the underlying fog cluster management, thus accounting for the dynamics by relocating certain processing elements of these SPP. This paper presents our initial work on a conceptual architecture for context-aware and dynamic management of SPP in the fog. We provide preliminary results, showing the general feasibility of relocating processing elements according to changes in the geolocation."
pub.1036323776,Stream Processors,"Stream processors, like other multi core architectures partition their functional units and storage into multiple processing elements. In contrast to typical architectures, which contain symmetric general-purpose cores and a cache hierarchy, stream processors have a significantly leaner design. Stream processors are specifically designed for the stream execution model, in which applications have large amounts of explicit parallel computation, structured and predictable control, and memory accesses that can be performed at a coarse granularity. Applications in the streaming model are expressed in a gather–compute–scatter form, yielding programs with explicit control over transferring data to and from on-chip memory. Relying on these characteristics, which are common to many media processing and scientific computing applications, stream architectures redefine the boundary between software and hardware responsibilities with software bearing much of the complexity required to manage concurrency, locality, and latency tolerance. Thus, stream processors have minimal control consisting of fetching medium- and coarse-grained instructions and executing them directly on the many ALUs. Moreover, the on-chip storage hierarchy of stream processors is under explicit software control, as is all communication, eliminating the need for complex reactive hardware mechanisms."
pub.1132682463,Throughput prediction based on ExtraTree for stream processing tasks,"In the era of big data, as the amount of streaming data continues to increase, stream processing tasks (SPTs) face serious challenges in real-time processing scenarios with low latency and high throughput. However, much of the current literature on the performance of SPTs pays attention to the reactive approach, which cannot well avoid the problem of system crashes due to the inherent performance volatility. In this paper, a novel throughput prediction method based on ExtraTree for SPTs is presented to address these challenges. A volatility detection algorithm was proposed to obtain the reasonable metric values after the performance volatility of SPTs was studied. Moreover, a selection algorithm of regression function was proposed to output the performance values of SPTs under a relative stead state. Furthermore, a ExtraTree-based algorithm was proposed to predict the throughput of SPTs. The experimental results from two open-source benchmarks running on Apache Flink, a popular stream processing system (SPS), indicated that the average of the accuracy and efficiency of the proposed method could achieve 90.535% and 0.835 s/10,000 samples, which proved the effectiveness of the proposed method on the task of predicting the throughput of SPTs."
pub.1156039312,Real-time data processing for ultrafast X-ray computed tomography using modular CUDA based pipelines,"In this article, a new version of the Real-time Image Stream Algorithms (RISA) data processing suite is introduced. It now features online detector data acquisition, high-throughput data dumping and enhanced real-time data processing capabilities. The achieved low-latency real-time data processing extends the application of ultrafast electron beam X-ray computed tomography (UFXCT) scanners to real-time scanner control and process control. We implemented high performance data packet reception based on data plane development kit (DPDK) and high-throughput data storing using both hierarchical data format version 5 (HDF5) as well as the adaptable input/output system version 2 (ADIOS2). Furthermore, we extended RISA's underlying pipelining framework to support the fork-join paradigm. This allows for more complex workflows as it is necessary, e.g. for online data processing. Also, the pipeline configuration is moved from compile-time to runtime, i.e. processing stages and their interconnections can now be configured using a configuration file. In several benchmarks, RISA is profiled regarding data acquisition performance, data storage throughput and overall processing latency. We found that using direct IO mode significantly improves data writing performance on the local data storage. We could further prove that RISA is now capable of concurrently receiving, processing and storing data from up to 768 detector channels (3072 MB/s) at 8000 fps on a single-GPU computer in real-time. Program Program Title: GLADOS/RISA CPC Library link to program files: https://doi.org/10.17632/65sx747rvm.2 Developer's repository link: https://codebase.helmholtz.cloud/risa Licensing provisions: Apache-2.0 Programming language: C++ Journal reference of previous version: Comput. Phys. Commun. 219 (2017) 353-360 [1] Does the new version supersede the previous version?: Yes. Reasons for the new version: Extended capabilities for real-time operation with latest UFXCT hardware. Summary of revisions: (i) Add forking and joining of processing pipeline branches (ii) Add runtime (re-)configuration of pipeline stages and connections (iii) Add UDP receiver stage to acquire detector data in real-time (iv) Add high-throughput data dumping Nature of problem: Ultrafast electron beam X-ray computed tomography scanners stream multiple Gigabytes of raw data per second via Ethernet to a control computer. Receiving the data with low latency, real-time image-based control would become possible. For this, data need to be captured from the network, stored on disk, reconstructed and post-processed concurrently. The current total data rate of up to 3072 MB/s requires high-throughput solutions for each of these tasks. Solution method: Using a pipeline scheme, RISA processes incoming raw data in distinct stages (sources, processors, sinks). These are implemented in GPU kernels and are executed concurrently to exploit data parallelism as well as task parallelism. To capture detector data, we implemente"
pub.1095062743,A STREAMING APPROACH TO RADIO ASTRONOMY IMAGING,"The emergence of large-field radio telescopes has generated the need to process huge amounts of data “on the fly” in order to avoid substantial costs in storage and signal processing. In this paper, we describe a streaming software approach to radio astronomy, and in particular, show how the IBM-developed stream computing architecture System $S$ enables seamless “on the fly” radio astronomy imaging. This approach overcomes pertinent issues of memory limitation and latencies related to data processing, and it allows autonomous processor allocation to tasks as the need arises."
pub.1174983200,ViEdge: An Edge-based Platform for Video Analytics Applications in Smart Estates,"Video analytics plays a crucial role in the development of smart estates and cities. Applications such as garbage dumping detection, lift monitoring, safety surveillance, etc. rely on video analytics, and require fast response time. Traditional cloud-based systems are ill-suited for these applications due to their limitations in handling large volume of video data with low latency. In contrast, the IoT-Edge-Cloud paradigm is better suited for such applications, but it presents challenges such as system heterogeneity, and resource allocation and orchestration. There is a need for an efficient platform for distributed video stream processing where resource orchestration and scalability aspects are tailored to smart estate applications. In this paper, we present ViEdge, an edge-based platform for video analytics applications in smart estates. It is highly adaptable and scalable, making it ideal for various deployments in such environments. Our implementation of ViEdge utilizes Kubernetes (K8s) for resource management and orchestration, and Apache Storm for distributed video stream processing. To study ViEdge’s customization capabilities, we evaluated its performance on a heterogeneous edge testbed. We observed increased latency in Apache Storm when integrated with Kubernetes, affecting overall application performance. However, by developing a heuristic-based scheduler, we demonstrate that ViEdge effectively reduces end-to-end latency and enhances frame processing rates."
pub.1101063532,Real-Time Cloud Robotics in Practical Smart City Applications,"Smart mobile devices are one of the main factors of realizing the smart city of tomorrow. To this aim, in this paper, we investigate utilizing cloud robotics as a practical processing platform for making the existing inexpensive mobile devices such as robots smarter and collaborative. Smart cities with the scattered massive number of sensors would need complicated infrastructure to connect and process large quantities of data. Cloud robotics has emerged as an efficient computing means in intensive data processing applications. Hence, we introduce C2RO cloud robotics platform that uses real-time stream processing technology to virtually connect the energy-efficient and low-cost city mobile devices or sensors. Moreover, to mitigate the effects of latency, we propose the hybrid cloud robotics computation model used in C2RO cloud robotics platform as a processing model employing both edge and cloud computing technologies in robotics. We demonstrate the superiority of C2RO platform through practical experiments and show that our proposed computational model provides significant optimization in the latency and processing rate, and achieves the real-time target of a high-profile urban project."
pub.1093994556,"SpaceFibre Based On-board Networks for Real-Time Video Data Streams SpaceFibre, Long Paper","High-speed onboard networks for the space industry with a lot of tasks that could be solved only by transmitting large data streams in a short time, with minimum overheads and accepted latencies. Particular tasks for data transmission require various types of traffic and onboard network topologies. Video data in many applications generate high throughput real-time data streams, from most demanding onboard traffic. The SpaceFibre protocol, which gives an ability to transmit data with high speeds and different quality of services (QoS), could be prospective technology for the spacecraft tasks and missions. Implementation of SpaceFibre and considering its application for Russian space missions is going on. The paper presents use cases for SpaceFibre based onboard networks for real-time video data streams in prospective missions. We consider features and characteristics of raw, non-compressed video data streams for processing and real-time control (e.g. to support docking), data streams of compressed motion imagery to record video, science experiment high quality video, robotics, high definition television frames to monitors, etc. The paper considers requirements and restrictions for building SpaceFibre onboard networks for real-time video data streams. Streaming Data Transport Protocol is mapped on a SpaceFibre network for transmission of streaming data from onboard cameras (video stream), to onboard monitors and to a high rate downlink."
pub.1121230150,An Exploratory Study of How Specialists Deal with Testing in Data Stream Processing Applications,"[Background] Nowadays, there is a massive growth of data volume and speed in
many types of systems. It introduces new needs for infrastructure and
applications that have to handle streams of data with low latency and high
throughput. Testing applications that process such data streams has become a
significant challenge for engineers. Companies are adopting different
approaches to dealing with this issue. Some have developed their own solutions
for testing, while others have adopted a combination of existing testing
techniques. There is no consensus about how or in which contexts such solutions
can be implemented. [Aims] To the best of our knowledge, there is no
consolidated literature on that topic. The present paper is an attempt to fill
this gap by conducting an exploratory study with practitioners. [Method] We
used qualitative methods in this research, in particular interviews and survey.
We interviewed 12 professionals who work in projects related to data streams,
and also administered a questionnaire with other 105 professionals. The
interviews went through a transcription and coding process, and the
questionnaires were analysed to reinforce findings. [Results] This study
presents current practices around software testing in data stream processing
applications. These practices involve methodologies, techniques, and tools.
[Conclusions] Our main contribution is a compendium of alternatives for many of
the challenges that arise when testing streaming applications from a
state-of-the-practice perspective."
pub.1006223100,A Retransmission Control Algorithm for Low-Latency UDP Stream on StreamCode-Base Active Networks,"We propose an algorithm for real-time data stream that achieves low latency as well as low packet loss rate. In our algorithm a sequence number is included in each packet, and it is renumbered as if there is no packet loss in the upper stream when a packet loss and corresponding packet retransmission occurs. This conceals the packet loss and its recovery from succeeding nodes in the downstream, thus simplifies packet processing in succeeding nodes. This algorithm can easy be implemented on capsule-type active networks, and we evaluated this algorithm on StreamCode based active networks. It is implemented with 1.0k line in-packet programs plus node initialization programs, and the evaluation results show that 27.1% end-to-end packet loss rate is suppressed to around 5.6%."
pub.1105217193,eChIDNA: Continuous Data Validation in Advanced Metering Infrastructures,"New laws and regulations increase the demands for a more data-intense metering infrastructure towards more adaptive electricity networks (aka smart grids). The automatic measuring, often involving wireless communication, introduces errors both in software and during data transmission. These demands, as well as the large data volumes that need to be validated, present new challenges to utilities. First, measurement errors cannot be allowed to propagate to the data stored by utilities. Second, manual fixing of errors after storing is not a feasible option with increasing data volumes and decreasing lead times for new services and analysis. Third, validation is not only to be applied to current readings but also to past readings when new types of errors are discovered. This paper addresses these issues by proposing a hybrid system, eChIDNA, utilizing both the store-then-process and the data streaming processing paradigms, enabling for high throughput, low latency distributed and parallel analysis. Validation rules are built upon this paradigm and then implemented on the state of the art Apache Storm Stream Processing Engine to assess performance. Furthermore, patterns of common errors are matched, triggering alerts as a first step towards automatic correction of errors. The system is evaluated with production data from hundreds of thousands of smart meters. The results show a performance in the thousands messages per second realm, showing that stream processing can be used to validate large volumes of meter data online with low processing latency, identifying common errors as they appear. The results from the pattern matching are cross-validated with system experts and show that pattern matching is a viable way to minimize time required from human operators."
pub.1048248819,An Efficient Cache Mechanism for Improving Response Times in Integrated RFID Middleware,"This paper proposes an efficient caching mechanism appropriate for the integrated RFID middleware which can integrate wireless sensor networks (WSNs) and RFID (radio frequency identification) systems. The operating environment of the integrated RFID middleware is expected to face the situations of a significant amount of data reading from RFID readers, constant stream data input from large numbers of autonomous sensor nodes, and queries from various applications to history data sensed before and stored in distributed storages. Consequently, an efficient middleware layer equipping with caching mechanism is inevitably necessary for low latency of request-response while processing both data stream from sensor networks and history data from distributed database. For this purpose, the proposed caching mechanism includes two optimization methods to reduce the overhead of data processing in RFID middleware based on the classical cache implementation polices. One is data stream cache (DSC) and the other is history data cache (HDC), according to the structure of data request. We conduct a number of simulation experiments under different parameters and the results show that the proposed caching mechanism contributes considerably to fast request-response times."
pub.1134291867,FATM: A failure‐aware adaptive fault tolerance model for distributed stream processing systems,"Summary  Distributed Stream Processing Systems (DSPS) are very popular to process unbounded data streams in real‐time. Low processing latency is a fundamental requirement for DSPS applications to maintain the real‐time response. This requirement of low processing latency for DSPS is badly affected due to inevitable failures in computing systems. Generally, DSPS grapple with these inevitable failures by triggering periodic checkpoints. The periodic checkpoints pessimistically persist the application state so that the execution may be resumed after the failure. These periodic checkpoints incur high overheads due to the high frequency of checkpoints triggering, which increases the overall execution time. On the other hand, failure occurrences in real‐world systems are not periodic. This sharp contrast between the periodic checkpoints and failure distributions in the real‐world systems makes the periodic checkpoints inefficient. We propose a failure‐aware adaptive fault tolerance model called FATM which triggers the checkpoints inline with the underlying failure rate. Further, we design a model for utility factor and checkpoint overheads to evaluate the performance of fault tolerance models for DSPS. We implement the FATM atop Apache Flink and perform a series of experiments. To validate the effectiveness of FATM, experiment results are compared with the existing checkpoint‐based models of DSPS. The results show that the FATM significantly reduces the checkpoint frequency, increases the utility factor, and reduces the checkpoint overheads by 28%. "
pub.1112118411,SpinStreams,"The ubiquity of data streams in different fields of computing has led to the emergence of Stream Processing Systems (SPSs) used to program applications that extract insights from unbounded sequences of data items. Streaming applications demand various kinds of optimizations. Most of them are aimed at increasing throughput and reducing processing latency, and need cost models used to analyze the steady-state performance by capturing complex aspects like backpressure and bottleneck detection. In those systems, the tendency is to support dynamic optimizations of running applications which, although with a substantial run-time overhead, are unavoidable in case of unpredictable workloads. As an orthogonal direction, this paper proposes SpinStreams, a static optimization tool able to leverage cost models that programmers can use to detect and understand the inefficiencies of an initial application design. SpinStreams suggests optimizations for restructuring applications by generating code to be run on the SPS. We present the theory behind our optimizations, which cover more general classes of application structures than the ones studied in the literature so far. Then, we assess the accuracy of our models in Akka, an actor-based streaming framework providing a Java and Scala API."
pub.1095151547,An Adaptive Replica Mechanism for Real-Time Stream Processing,"In Internet of Things, many applications are modeled as the continuous stream processing of the senor data, the replica mechanism is required to guarantee availability. However, the replicas' backup, placement bring the latency at run-time due to the resources consumption from memory, bandwidth. In this paper, a mechanism is proposed as greedy fashion by the resources cost to place replicas, which could tradeoff between the availability, overheads in the system. The extensive experiments show that the availability of the proposed mechanism can be provided in a more stable manner than the traditional random placement under the same conditions."
pub.1068092834,Design and Evaluation of a Configurable Query Processing Hardware for Data Streams,"In this paper, we propose Configurable Query Processing Hardware (CQPH), an FPGA-based accelerator for continuous query processing over data streams. CQPH is a highly optimized and minimal-overhead execution engine designed to deliver real-time response for high-volume data streams. Unlike most of the other FPGA-based approaches, CQPH provides on-the-fly configurability for multiple queries with its own dynamic configuration mechanism. With a dedicated query compiler, SQL-like queries can be easily configured into CQPH at run time. CQPH supports continuous queries including selection, group-by operation and sliding-window aggregation with a large number of overlapping sliding windows. As a proof of concept, a prototype of CQPH is implemented on an FPGA platform for a case study. Evaluation results indicate that a given query can be configured within just a few microseconds, and the prototype implementation of CQPH can process over 150 million tuples per second with a latency of less than a microsecond. Results also indicate that CQPH provides linear scalability to increase its flexibility (i.e., on-the-fly configurability) without sacrificing performance (i.e., maximum allowable clock speed)."
pub.1047569861,Data-Driven Stream Mining Systems for Computer Vision,"In this chapter, we discuss the state of the art and future challenges in adaptive stream mining systems for computer vision. Adaptive stream mining in this context involves the extraction of knowledge from image and video streams in real-time, and from sources that are possibly distributed and heterogeneous. With advances in sensor and digital processing technologies, we are able to deploy networks involving large numbers of cameras that acquire increasing volumes of image data for diverse applications in monitoring and surveillance. However, to exploit the potential of such extensive networks for image acquisition, important challenges must be addressed in efficient communication and analysis of such data under constraints on power consumption, communication bandwidth, and end-to-end latency. We discuss these challenges in this chapter, and we also discuss important directions for research in addressing such challenges using dynamic, data-driven methodologies."
pub.1108063267,Hardware-accelerated data acquisition and authentication for high-speed video streams on future heterogeneous automotive processing platforms,"With the increasing use of Ethernet-based communication backbones in safety-critical real-time domains, both efficient and predictable interfacing and cryptographically secure authentication of high-speed data streams are becoming very important. Although the increasing data rates of in-vehicle networks allow the integration of more demanding (e.g., camera-based) applications, processing speeds and, in particular, memory bandwidths are no longer scaling accordingly. The need for authentication, on the other hand, stems from the ongoing convergence of traditionally separated functional domains and the extended connectivity both in- (e.g., smart-phones) and outside (e.g., telemetry, cloud-based services and vehicle-to-X technologies) current vehicles. The inclusion of cryptographic measures thus requires careful interface design to meet throughput, latency, safety, security and power constraints given by the particular application domain. Over the last decades, this has forced system designers to not only optimize their software stacks accordingly, but also incrementally move interface functionalities from software to hardware. This paper discusses existing and emerging methods for dealing with high-speed data streams ranging from software-only via mixed-hardware/software approaches to fully hardware-based solutions. In particular, we introduce two approaches to acquire and authenticate GigE Vision Video Streams at full line rate of Gigabit Ethernet on Programmable SoCs suitable for future heterogeneous automotive processing platforms."
pub.1111015871,Scalable and Real-Time Time Series Analytics: Telemedicine as Use Case,"Real-time processing and data analytics of big data has become a main operation in different business, such as extracting manufacturing, healthcare, smart Cities, social and media network data, … etc. Also another concept has been appear that significant interest in building new system refers to high-speed real-time and near real-time data streams. Big data workloads in the wild show a strong temporal variability that not only poses the risk of slow responsiveness in data analysis, but also leads to a high risk of service outage. The recent development of batch streaming systems based on the MapReduce framework is shown effective on non-overloaded systems. However, little is known on how to enhance the performance of the batch streaming systems for bursty workloads. In this paper, we propose a latency-driven data controller, which aims to process as much data as possible, while processing these as fast as the application target latency and system capacity allow. In particular by implementing Spark Streaming as emerging and complex batch streaming system which include features that allow placing data in an augmented distributed memory, shedding out-of-date data, (improving the processing locality of Map tasks, and delaying data processing in transient overloads"
pub.1139063999,RDMA-Based Apache Storm for High-Performance Stream Data Processing,"Apache Storm is a scalable fault-tolerant distributed real-time stream-processing framework widely used in big data applications. For distributed data-sensitive applications, low-latency, high-throughput communication modules have a critical impact on overall system performance. Apache Storm currently uses Netty as its communication component, an asynchronous server/client framework based on TCP/IP protocol stack. The TCP/IP protocol stack has inherent performance flaws due to frequent memory copying and context switching. The Netty component not only limits the performance of the Storm but also increases the CPU load in the IPoIB (IP over InfiniBand) communication mode. In this paper, we introduce two new implementations for Apache Storm communication components with the help of RDMA technology. The performance evaluation on Mellanox QDR Cards (40 Gbps) shows that our implementations can achieve speedup up to 5×$$\times $$ compared with IPoIB and 10×$$\times $$ with 1 Gigabit Ethernet. Our implementations also significantly reduce the CPU load and increase the throughput of the system."
pub.1094946927,Secured Fast Prediction of Cloud Data Stream with Balanced Load Factor Using Ensemble Tree Classification,"Cloud infrastructures are used for predicted the data stream with high latency rate on varying load factors with different ensemble models. some of existing stream applications analyze only the temporal relation between data but the Spatio-temporal data information is not processed. For the fast prediction of Spatio-temporal data stream from the cloud infrastructure data distribution, an effective load balancing query processing approach is not spread widely. To achieve load balance statistics on cloud data stream, Ensemble Tree Metric Space Indexing (E-tree MSI) technique is employed and performed with three processes such as scheduling, classification and mapping of cloud data stream for fast effective load balancing. Initially, Fast Predictive Look-ahead Scheduling (FPLS) approach is used to continuously schedule the Spatio-temporal data stream files. The workload of the infrastructure is scheduled in E-tree MSI technique and helps to easily balance the load factor. Secondly, Parallel Ensemble Tree Classification (PETC) in E-tree MSI technique executes the classification operations on cloud data stream. The classification of data stream in E-tree MSI technique reduces the overload factor. Finally, bilinear quadrilateral mapping process in E-tree MSI technique linearly predicts the result from cloud data stream storage, with minimal execution time. Experiment is conducted on factors such as linear load balance factor measure, execution time for mapping, and classification accuracy rate."
pub.1103609024,KAFKA: The Modern Platform for Data Management and Analysis in Big Data Domain,"In todays 21st century as technology is getting so much advanced, Apache Kafka emanate as one of the finest technology in the present world. Its fast, scalable, distributed stream processing platform and fault tolerant messaging system has made this technology to roar in the field of data processing and analysis. Apache Kafka is a distributed streaming platform mainly designed for low latency and high throughput. It is publish-subscribe messaging reassess as a constituent to each of number of legatee of commit log. The key notion of Apache Kafka is that it is used as a cluster on any number of servers. Server of Kafka stores record streams in classes known as topics. Every record contains a key, a value and a time stamp. It has two classes of application. Firstly, for building pipelines of real time data streams which is reliable to get the data between the systems or between the applications. Secondly, build applications streaming for real time that reacts to the record streams. A single Kafka mediator can handle hundreds of megabytes of reads and writes per second from thousands of clients. Scalable Kafka is designed to allow a single cluster to serve the central data backbone for a large organization."
pub.1092818788,Toward Fog-Based Event-Driven Services for Internet of Vehicles: Design and Evaluation,"Abstract
Internet of Vehicles (IoV) is an emerging technology for smart city. Connected vehicles can publish event data expressing their driving records so that remote vehicular cloud services can subscribe these event data and correlate them with sensed data collected from environment to provide driving services. The distributed event streams generated from heterogeneous sources can be computed for the correlation between them for on-demand traffic situation detection in IoV environment. The characteristic of event-driven service is to react to real-time service functions dependent on event trigger mechanism so that it is suitable to build situation-aware traffic applications in IoV. Meanwhile, Complex Event Processing (CEP) technology is an event stream processing technology used to compute event correlation between distributed event streams from heterogeneous sources and react to the matched specified actions immediately. CEP can be adopted to realize event-driven services in IoV. Due to the mobility of vehicles and limited bandwidth in wireless communication, the events generated from vehicles sent to remote vehicular cloud may suffer delay or get lost. Therefore, event data generated from vehicles is uncertain event due to response latency or incorrect execution of event-driven services on the cloud. Fog computing is an emerging computing paradigm that moves computation tasks from cloud to network edges and promises to reduce response latency and save bandwidth usage in wireless network for IoV. Therefore, in this paper, we propose a fog-based event-driven service mechanism for IoV and examine a case study to evaluate it."
pub.1145440053,Runtime Adaptation of Data Stream Processing Systems: The State of the Art,"Data stream processing (DSP) has emerged over the years as the reference paradigm for the analysis of continuous and fast information flows, which often have to be processed with low-latency requirements to extract insights and knowledge from raw data. Dealing with unbounded dataflows, DSP applications are typically long running and thus, likely experience varying workloads and working conditions over time. To keep a consistent service level in face of such variability, a lot of effort has been spent studying strategies for runtime adaptation of DSP systems and applications. In this survey, we review the most relevant approaches from the literature, presenting a taxonomy to characterize the state of the art along several key dimensions. Our analysis allows us to identify current research trends as well as open challenges that will motivate further investigations in this field."
pub.1050972300,A Stream Processing System for Multisource Heterogeneous Sensor Data,"With the rapid development of the Internet of Things (IoT), a variety of sensor data are generated around everyone’s life. New research perspective regarding the streaming sensor data processing of the IoT has been raised as a hot research topic that is precisely the theme of this paper. Our study serves to provide guidance regarding the practical aspects of the IoT. Such guidance is rarely mentioned in the current research in which the focus has been more on theory and less on issues describing how to set up a practical system. In our study, we employ numerous open source projects to establish a distributed real time system to process streaming data of the IoT. Two urgent issues have been solved in our study that are (1) multisource heterogeneous sensor data integration and (2) processing streaming sensor data in real time manner with low latency. Furthermore, we set up a real time system to process streaming heterogeneous sensor data from multiple sources with low latency. Our tests are performed using field test data derived from environmental monitoring sensor data collected from indoor environment for system validation. The results show that our proposed system is valid and efficient for multisource heterogeneous sensor data integration and streaming data processing in real time manner."
pub.1105889148,Chi,"Stream-processing workloads and modern shared cluster environments exhibit high variability and unpredictability. Combined with the large parameter space and the diverse set of user SLOs, this makes modern streaming systems very challenging to statically configure and tune. To address these issues, in this paper we investigate a novel control-plane design, Chi, which supports continuous monitoring and feedback, and enables dynamic re-configuration. Chi leverages the key insight of embedding control-plane messages in the data-plane channels to achieve a low-latency and flexible control plane for stream-processing systems.
                  Chi introduces a new reactive programming model and design mechanisms to asynchronously execute control policies, thus avoiding global synchronization. We show how this allows us to easily implement a wide spectrum of control policies targeting different use cases observed in production. Large-scale experiments using production workloads from a popular cloud provider demonstrate the flexibility and efficiency of our approach."
pub.1092476827,Ewya: An Interoperable Fog Computing Infrastructure with RDF Stream Processing,"Fog computing is an emerging technology for the Internet of Things (IoT) that aims to support processing on resource-constrained distributed nodes in between the sensors and actuators on the ground and compute clusters in the cloud. Fog Computing benefits from low latency, location awareness, mobility, wide-spread deployment and geographical distribution at the edge of the network. However, there is a need to investigate, optimise for and measure the performance, scalability and interoperability of resource-constrained Fog nodes running real-time applications and queries on streaming IoT data before we can realise these benefits. With Eywa, a novel Fog Computing infrastructure, we (1) formally define and implement a means of distribution and control of query workload with an inverse publish-subscribe and push mechanism, (2) show how data can be integrated and made interoperable through organising data as Linked Data in the Resource Description Format (RDF), (3) test if we can improve RDF Stream Processing query performance and scalability over state-of-the-art engines with our approach to query translation and distribution for a published IoT benchmark on resource-constrained nodes and (4) position Fog Computing within the Internet of the Future."
pub.1158022757,Evolving the Digital Industrial Infrastructure for Production: Steps Taken and the Road Ahead,"The Internet of Production (IoP) leverages concepts such as digital shadows, data lakes, and a World Wide Lab (WWL) to advance today’s production. Consequently, it requires a technical infrastructure that can support the agile deployment of these concepts and corresponding high-level applications, which, e.g., demand the processing of massive data in motion and at rest. As such, key research aspects are the support for low-latency control loops, concepts on scalable data stream processing, deployable information security, and semantically rich and efficient long-term storage. In particular, such an infrastructure cannot continue to be limited to machines and sensors, but additionally needs to encompass networked environments: production cells, edge computing, and location-independent cloud infrastructures. Finally, in light of the envisioned WWL, i.e., the interconnection of production sites, the technical infrastructure must be advanced to support secure and privacy-preserving industrial collaboration. To evolve today’s production sites and lay the infrastructural foundation for the IoP, we identify five broad streams of research: (1) adapting data and stream processing to heterogeneous data from distributed sources, (2) ensuring data interoperability between systems and production sites, (3) exchanging and sharing data with different stakeholders, (4) network security approaches addressing the risks of increasing interconnectivity, and (5) security architectures to enable secure and privacy-preserving industrial collaboration. With our research, we evolve the underlying infrastructure from isolated, sparsely networked production sites toward an architecture that supports high-level applications and sophisticated digital shadows while facilitating the transition toward a WWL."
pub.1129390436,VidCEP: Complex Event Processing Framework to Detect Spatiotemporal Patterns in Video Streams,"Video data is highly expressive and has traditionally been very difficult for
a machine to interpret. Querying event patterns from video streams is
challenging due to its unstructured representation. Middleware systems such as
Complex Event Processing (CEP) mine patterns from data streams and send
notifications to users in a timely fashion. Current CEP systems have inherent
limitations to query video streams due to their unstructured data model and
lack of expressive query language. In this work, we focus on a CEP framework
where users can define high-level expressive queries over videos to detect a
range of spatiotemporal event patterns. In this context, we propose: i) VidCEP,
an in-memory, on the fly, near real-time complex event matching framework for
video streams. The system uses a graph-based event representation for video
streams which enables the detection of high-level semantic concepts from video
using cascades of Deep Neural Network models, ii) a Video Event Query language
(VEQL) to express high-level user queries for video streams in CEP, iii) a
complex event matcher to detect spatiotemporal video event patterns by matching
expressive user queries over video data. The proposed approach detects
spatiotemporal video event patterns with an F-score ranging from 0.66 to 0.89.
VidCEP maintains near real-time performance with an average throughput of 70
frames per second for 5 parallel videos with sub-second matching latency."
pub.1125160540,VidCEP: Complex Event Processing Framework to Detect Spatiotemporal Patterns in Video Streams,"Video data is highly expressive and has traditionally been very difficult for a machine to interpret. Querying event patterns from video streams is challenging due to its unstructured representation. Middleware systems such as Complex Event Processing (CEP) mine patterns from data streams and send notifications to users in a timely fashion. Current CEP systems have inherent limitations to query video streams due to their unstructured data model and lack of expressive query language. In this work, we focus on a CEP framework where users can define high-level expressive queries over videos to detect a range of spatiotemporal event patterns. In this context, we propose- i) VidCEP, an in-memory, on the fly, near real-time complex event matching framework for video streams. The system uses a graph-based event representation for video streams which enables the detection of high-level semantic concepts from video using cascades of Deep Neural Network models, ii) a Video Event Query language (VEQL) to express high-level user queries for video streams in CEP, iii) a complex event matcher to detect spatiotemporal video event patterns by matching expressive user queries over video data. The proposed approach detects spatiotemporal video event patterns with an F-score ranging from 0.66 to 0. S9. VidCEP maintains near real-time performance with an average throughput of 70 frames per second for 5 parallel videos with sub-second matching latency."
pub.1129363060,Q-Flink: A QoS-Aware Controller for Apache Flink,"Modern stream-data processing platforms are required to execute processing pipelines over high-volume, yet high-velocity, datasets under tight latency constraints. Apache Flink has emerged as an important new technology of large-scale platform that can distribute processing over a large number of computing nodes in a cluster (i.e., scale-out processing). Flink allows application developers to design and execute queries over continuous raw-inputs to analyze a large amount of streaming data in a parallel and distributed fashion. To increase the throughput of computing resources in stream processing platforms, a service provider might be tempted to use a consolidation strategy to pack as many processing applications as possible on the working nodes, with the hope of increasing the total revenue by improving the overall resource utilization. However, there is a hidden trap for achieving such a higher throughput solely by relying on an interference-oblivious consolidation strategy. In practice, collocated applications in a shared platform can fiercely compete with each others for obtaining the capacity of shared resources (e.g., cache and memory bandwidth) which in turn can lead to a severe performance degradation for all consolidated workloads. This paper addresses the shared resource contention problem associated with the auto-resource controlling mechanism of Apache Flink engine running across a distributed cluster. A controlling strategy is proposed to handle scenarios in which stream processing applications may have different quality of service (QoS) requirements while the resource interference is considered as the key performance-limiting parameter. The performance evaluation is carried out by comparing the proposed controller with the default Flink resource allocation strategy in a testbed cluster with total 32 Intel Xeon cores under different workload traffic with up to 4000 streaming applications chosen from various benchmarking tools. Experimental results demonstrate that the proposed controller can successfully decrease the average latency of high priority applications by 223% during the burst traffic while maintaining the requested QoS enforcement levels."
pub.1117328355,Haren,"In modern Stream Processing Engines (SPEs), numerous diverse applications, which can differ in aspects such as cost, criticality or latency sensitivity, can co-exist in the same computing node. When these differences need to be considered to control the performance of each application, custom scheduling of operators to threads is of key importance (e.g., when a smart vehicle needs to ensure that safety-critical applications always have access to computational power, while other applications are given lower, variable priorities). Many solutions have been proposed regarding schedulers that allocate threads to operators to optimize specific metrics (e.g., latency) but there is still lack of a tool that allows arbitrarily complex scheduling strategies to be seamlessly plugged on top of an SPE. We propose Haren to fill this gap. More specifically, we (1) formalize the thread scheduling problem in stream processing in a general way, allowing to define ad-hoc scheduling policies, (2) identify the bottlenecks and the opportunities of scheduling in stream processing, (3) distill a compact interface to connect Haren with SPEs, enabling rapid testing of various scheduling policies, (4) illustrate the usability of the framework by integrating it into an actual SPE and (5) provide a thorough evaluation. As we show, Haren makes it is possible to adapt the use of computational resources over time to meet the goals of a variety of scheduling policies."
pub.1167503022,Evolving the Digital Industrial Infrastructure for Production: Steps Taken and the Road Ahead,"The Internet of Production (IoP) leverages concepts such as digital shadows, data lakes, and a World Wide Lab (WWL) to advance today’s production. Consequently, it requires a technical infrastructure that can support the agile deployment of these concepts and corresponding high-level applications, which, e.g., demand the processing of massive data in motion and at rest. As such, key research aspects are the support for low-latency control loops, concepts on scalable data stream processing, deployable information security, and semantically rich and efficient long-term storage. In particular, such an infrastructure cannot continue to be limited to machines and sensors, but additionally needs to encompass networked environments: production cells, edge computing, and location-independent cloud infrastructures. Finally, in light of the envisioned WWL, i.e., the interconnection of production sites, the technical infrastructure must be advanced to support secure and privacy-preserving industrial collaboration. To evolve today’s production sites and lay the infrastructural foundation for the IoP, we identify five broad streams of research: (1) adapting data and stream processing to heterogeneous data from distributed sources, (2) ensuring data interoperability between systems and production sites, (3) exchanging and sharing data with different stakeholders, (4) network security approaches addressing the risks of increasing interconnectivity, and (5) security architectures to enable secure and privacy-preserving industrial collaboration. With our research, we evolve the underlying infrastructure from isolated, sparsely networked production sites toward an architecture that supports high-level applications and sophisticated digital shadows while facilitating the transition toward a WWL."
pub.1145130981,A Multi-Metric Adaptive Stream Processing System,"Stream processing systems (SPS) have to deal with highly dynamic scenarios where its adaptation is mandatory in order to accomplish realistic applications requirements. In this work, we propose a new adaptive SPS for real-time processing that, based on input data rate variation, dynamically adapts the number of active operator replicas. Our SPS extends Storm by pre-allocating, for each operator, a set of inactive replicas which are activated (or deactivated) when necessary without the Storm reconfiguration cost. We exploit the MAPE model and define a new metric that aggregates the value of multiple metrics to dynamically changes the number of replicas of an operator. We deploy our SPS over Google Cloud Platform and results confirm that our metric can tolerate highly dynamic conditions, improving resource usage while preserving high throughput and low latency."
pub.1094642093,"ObsCon: Integrated Monitoring and Control for Parallel, Real-Time Applications","A large class of emerging compute-intensive applications demand real-time or near real-time processing guarantees on streaming data. Sensor processing in particular, has stringent latency requirements for carrying out its digital processing for rapidly incoming radar data streams. The consequent demands on the cluster middleware used to run such codes include (i) efficient online observation of current application performance, coupled with (ii) highly responsive controllers able to dynamically adjust the application's input- and data-dependent runtime behavior. We present the Obs(erver)Con(troller) software for online monitoring and control, which based on specifications of acceptable application states and tunable knobs within the execution environment, ensures that application performance falls within acceptable limits. ObsCon topologies are dynamic, making possible the runtime association of ObsCon methods with arbitrary DAG-structured, distributed/parallel stream processing applications running on high end cluster machines. This paper describes the ObsCon software and its ‘grey box’ use with a high performance cluster code that exports to ObsCon select 'hooks' for online monitoring and control - Adaptive Digital Beamforming for a phase-array radar system."
pub.1093298232,"Edge-Computing-Aware Deployment of Stream Processing Tasks Based on Topology-external Information: Model, Algorithms, and a Storm-Based Prototype","Stream Processing Frameworks (SPF, e.g., Apache Storm) are solutions that facilitate and manage the execution of processing topologies that consist of multiple parallelizable steps (or tasks) and involve continuous data exchange among these tasks. Stemming from the world of Cloud-centric Big Data processing, SPFs often fail to address certain requirements of Internet-of-Things systems. For example, existing deployment solutions ignore the fact that topology tasks can also be involved in other interactions and data-intensive communication flows, which are not taking place between the tasks, but between a task and another Internet-of-things entity, such as an actuator, a database, or a user. This paper describes SPF extensions for taking these interactions into account. The extensions are described both generically and as extensions of Apache Storm. In a simple evaluation upon a topology which involves topology-external interactions, we demonstrate how our solution can eliminate latency requirements violations and reduce Cloud-to-edge bandwidth consumption to 1/3 compared to Apache Storm."
pub.1026095030,Image Registration by a Regularized Gradient Flow. A Streaming Implementation in DX9 Graphics Hardware,"Abstract.The presented image registration method uses a regularized gradient flow to correlate the intensities in two images. Thereby, an energy functional is successively minimized by descending along its regularized gradient. The gradient flow formulation makes use of a robust multi-scale regularization, an efficient multi-grid solver and an effective time-step control. The data processing is arranged in streams and mapped onto the functionality of a stream processor. This arrangement automatically exploits the high data parallelism of the problem, and local data access helps to maximize throughput and hide memory latency. Although dedicated stream processors exist, we use a DX9 compatible graphics card as a stream architecture because of its ideal price-performance ratio. The new floating point number formats guarantee a sufficient accuracy of the algorithm and eliminate previously present concerns about the use of graphics hardware for medical computing. Therefore, the implementation achieves reliable results at very high performance, registering two 2572 images in approximately 3sec, such that it could be used as an interactive tool in medical image analysis."
pub.1118176288,"Lawn: an Unbound Low Latency Timer Data Structure for Large Scale, High Throughput Systems","As demand for Real-Time applications rises among the general public, the
importance of enabling large-scale, unbound algorithms to solve conventional
problems with low to no latency is critical for product viability. Timer
algorithms are prevalent in the core mechanisms behind operating systems,
network protocol implementation, stream processing, and several database
capabilities. This paper presents a field-tested algorithm for low latency,
unbound range timer structure, based upon the well excepted Timing Wheel
algorithm. Using a set of queues hashed by TTL, the algorithm allows for a
simpler implementation, minimal overhead no overflow and no performance
degradation in comparison to the current state of the algorithms under typical
use cases."
pub.1107479468,Latency Aware and Service Delay with Task Scheduling in Mobile Edge Computing,"In a traditional Mobile Cloud Computing (MCC), a stream of data produced by mobile users (MUs) is uploaded to the remote cloud for additional processing throughout the Internet. Though, due to long WAN distance it causes high End to End latency. With the intention of minimize the average response time and key constrained Service Delay (network and cloudlet Delay) for mobile users (MUs), offload their workloads to the geographically distributed cloudlets network, we propose the Multi-layer Latency Aware Workload Assignment Strategy (MLAWAS) to allocate MUs workloads into optimal cloudlets, Simulation results demonstrate that MLAWAS earns the minimum average response time as compared with two other existing strategies."
pub.1094253781,Data Transfer Matters for GPU Computing,"Graphics processing units (GPUs) embrace manycore compute devices where massively parallel compute threads are offloaded from CPUs. This heterogeneous nature of GPU computing raises non-trivial data transfer problems especially against latency-critical real-time systems. However even the basic characteristics of data transfers associated with GPU computing are not well studied in the literature. In this paper, we investigate and characterize currently-achievable data transfer methods of cutting-edge GPU technology. We implement these methods using open-source software to compare their performance and latency for real-world systems. Our experimental results show that the hardware-assisted direct memory access (DMA) and the I/O read-and-write access methods are usually the most effective, while on-chip microcontrollers inside the GPU are useful in terms of reducing the data transfer latency for concurrent multiple data streams. We also disclose that CPU priorities can protect the performance of GPU data transfers."
pub.1117328359,STRETCH,"Despite the established scientific knowledge on efficient parallel and elastic data stream processing, it is challenging to combine generality and high level of abstraction (targeting ease of use) with fine-grained processing aspects (targeting efficiency) in stream processing frameworks. Towards this goal, we propose STRETCH, a framework that aims at guaranteeing (i) high efficiency in throughput and latency of stateful analysis and (ii) fast elastic reconfigurations (without requiring state transfer) for intra-node streaming applications. To achieve these, we introduce virtual shared-nothing parallelization and propose a scheme to implement it in STRETCH, enabling users to leverage parallelization techniques while also taking advantage of shared-memory synchronization, which has been proven to boost the scaling-up of streaming applications while supporting determinism. We provide a fully-implemented prototype and, together with a thorough evaluation, correctness proofs for its underlying claims supporting determinism and a model (also validated empirically) of virtual shared-nothing and pure shared-nothing scalability behavior. As we show, STRETCH can match the throughput and latency figures of the front of state-of-the-art solutions, while also achieving fast elastic reconfigurations (taking only a few milliseconds)."
pub.1136298158,A Performance Benchmark for Stream Data Storage Systems,"Modern business intelligence relies on efficient processing on very large amount of stream data, such as various event logging and data collected by sensors. To meet the great demand for stream processing, many stream data storage systems have been implemented and widely deployed, such as Kafka, Pulsar and DistributedLog. These systems differ in many aspects including design objectives, target application scenarios, access semantics, user API, and implementation technologies. Each system use a dedicated tool to evaluate its performance. And different systems measure different performance metrics using different loads. For infrastructure architects, it is important to compare the performances of different systems under diverse loads using the same benchmark. Moreover, for system designers and developers, it is critical to study how different implementation technologies affect their performance. However, there is no such a benchmark tool yet which can evaluate the performances of different systems. Due to the wide diversities of different systems, it is challenging to design such a benchmark tool. In this paper, we present SSBench, a benchmark tool designed for stream data storage systems. SSBench abstracts the data and operations in different systems as “data streams” and “reads/writes” to data streams. By translating stream read/write operations into the specific operations of each system using its own APIs, SSBench can evaluate different systems using the same loads. In addition to measure simple read/write performance, SSBench also provides several specific performance measurements for stream data, including end-to-end read latency, performance under imbalanced loads and performance of transactional loads. This paper also presents the performance evaluation of four typical systems, Kafka, Pulsar, DistributedLog and ZStream, using SSBench, and discussion of the causes for their performance differences from the perspective of their implementation techniques."
pub.1136009634,A Dynamic Pyramid Tilling Method for Traffic Data Stream Based on Flink,"Traffic guidance, traffic management and emergency vehicle traffic all require keeping abreast of traffic status. Intelligent Transportation Systems (ITS) is highly expected to provide real-time traffic condition information service. To achieve this, the capability of handling dynamic data stream collected from multi traffic monitoring sources and serving the public with information timely is essential for ITS. With the wide spread of Internet of Things technology, not only the amount, but also the spatial and temporal resolutions of real-time traffic data have explosive growth, thereby enhancing the difficulty of real-time traffic data processing in ITS. Web pyramid map tiles is wide accepted for massive spatial data service, and the latency of tile generation significantly reduces the timeliness of information transmission and the reliability of services. A Flink-based method for dynamic pyramid tile generation and updating is proposed here. Take advantages of combining grid indexes, employing data partition and window selection mechanisms, and applying iterative computational characteristics for resampling, the distributed dynamic pyramid map tile generation algorithm (DPTG), can quickly visualize real-time spatial traffic data with digital map tiles. Taking the national highway road data from China as an example, the experimental results show that the Flink-based DPTG method has high efficiency and scalability in both batch processing and stream processing mode, which highlights the capability of the proposed method to support real-time traffic monitoring data processing for timely large-scale public service in ITS."
pub.1120701556,Anomaly Detection over Streaming Data: Indy500 Case Study,"Sports racing is attracting billions of audiences each year. It is powered and transformed by the latest data analysis technologies, from race car design, driving skill improvements to audience engagement on social media. However, most of the data processing are off-line and retrospective analysis. The emerging real-time data analysis from the Internet of Things (IoT) result in fast data streams generated from distributed sensors. Applying advanced Machine Learning/Artificial Intelligence over such data streams to discover new information, predict future insights and make control decision is a crucial process. In this paper, we start by articulating racing car big data characteristics and present time-critical anomaly detection of the racing cars with the realtime sensors of cars and the tracks from actual racing events. We build a scalable system infrastructure based on neuro-morphic Hierarchical Temporal Memory Algorithm (HTM) algorithm and Storm stream processing engine. By courtesy of historical Indy500 racing logs, evaluation experiments on this prototype system demonstrate good performance in terms of anomaly detection accuracy and service level objective (SLO) of latency for a real-world streaming application."
pub.1164443305,Evolving the Digital Industrial Infrastructure for Production: Steps Taken and the Road Ahead,"The Internet of Production (IoP) leverages concepts such as digital shadows, data lakes, and a World Wide Lab (WWL) to advance today’s production. Consequently, it requires a technical infrastructure that can support the agile deployment of these concepts and corresponding high-level applications, which, e.g., demand the processing of massive data in motion and at rest. As such, key research aspects are the support for low-latency control loops, concepts on scalable data stream processing, deployable information security, and semantically rich and efficient long-term storage. In particular, such an infrastructure cannot continue to be limited to machines and sensors, but additionally needs to encompass networked environments: production cells, edge computing, and location-independent cloud infrastructures. Finally, in light of the envisioned WWL, i.e., the interconnection of production sites, the technical infrastructure must be advanced to support secure and privacy-preserving industrial collaboration. To evolve today’s production sites and lay the infrastructural foundation for the IoP, we identify five broad streams of research: (1) adapting data and stream processing to heterogeneous data from distributed sources, (2) ensuring data interoperability between systems and production sites, (3) exchanging and sharing data with different stakeholders, (4) network security approaches addressing the risks of increasing interconnectivity, and (5) security architectures to enable secure and privacy-preserving industrial collaboration. With our research, we evolve the underlying infrastructure from isolated, sparsely networked production sites toward an architecture that supports high-level applications and sophisticated digital shadows while facilitating the transition toward a WWL."
pub.1118480225,Integrative Dynamic Reconfiguration in a Parallel Stream Processing Engine,"Load balancing, operator instance collocations and horizontal scaling are
critical issues in Parallel Stream Processing Engines to achieve low data
processing latency, optimized cluster utilization and minimized communication
cost respectively. In previous work, these issues are typically tackled
separately and independently. We argue that these problems are tightly coupled
in the sense that they all need to determine the allocations of workloads and
migrate computational states at runtime. Optimizing them independently would
result in suboptimal solutions. Therefore, in this paper, we investigate how
these three issues can be modeled as one integrated optimization problem. In
particular, we first consider jobs where workload allocations have little
effect on the communication cost, and model the problem of load balance as a
Mixed-Integer Linear Program. Afterwards, we present an extended solution
called ALBIC, which support general jobs. We implement the proposed techniques
on top of Apache Storm, an open-source Parallel Stream Processing Engine. The
extensive experimental results over both synthetic and real datasets show that
our techniques clearly outperform existing approaches."
pub.1123459712,Big Data Processing-Beyond Batch Processing,"This paper mainly focus on analysis of large sets of students data with one of the batch processing analysis techniques Beyond batch process, analysis of data streaming is done based on program of word counting program which executes data with HDFS along with dynamic created data. To compute similar coherent strategies one can implement a schema named batch and streaming process which dynamically creates data. The architecture is reduced to serve as X-Platform which uses ample number of tools for batch and stream analysis on this proposed frame work. Here we use spark-sql, a query language which acts as interface for interactive process to have iterative processes. Real time streaming data processing involves spark streaming works. Here we focus on preliminary evaluation of results and analysis report which compares data sets performance and also achieve low latency rate with usage of RDD."
pub.1133538537,On the Scheduling of Industrial IoT Tasks in a Fog Computing Environment,"In the industrial sector, a growing number of companies have an ongoing smart factory initiative. In such initiative, previously disparate systems and equipment become connected so the data streams they generate can be turned into actionable insights. Industrial IoT (IIoT) data originate from various sensors and Internet of Things devices deployed in industrial equipment and facilities. The vast volume of generated data need to be leveraged to improve robots’ operation, optimize processes, and help industry stakeholders and applications make faster and more informed decisions. Many existing industrial applications use the power of the cloud for data processing. However, time-sensitive industrial applications cannot tolerate sending IIoT data to the cloud for processing due to unacceptable network bandwidth requirements and high latency. The operation and maintenance staff of industrial facilities need the ability to efficiently stream data and process data in real-time at the edge. Smart factory operations are typically executed as workflows of dependent tasks. This paper investigates the performance of some scheduling stategies for the exection of workflow tasks in a smarty factory fog environment. Simulation results show that the MinMin, GA, and PSO scheduling algorithms offered the best results in terms of execution time."
pub.1132425193,Sensor Data Stream on-line Compression with Linearity-based Methods,The escalation of the Internet of Things applications has put on display the different sensor data processing methods. The sensor data compression is one of the fundamental methods to reduce the amount of data needed to transmit from the sensor node which is often battery powered and operates wirelessly. Reducing the amount of data in wireless transmission is an effective way to reduce overall energy consumption in wireless sensor nodes. The methods presented and tested are suitable for constrained sensor nodes with limited computational power and limited energy resources. The methods presented are compared with each other using compression ratio and inherent latency. Latency is an important parameter in on-line applications. The improved variation of the linear regression-based method called RT-LRbTC is tested and it has proved to be a potential method to be used in a wireless sensor node with a fixed and predictable latency. The compression efficiency of the compression algorithms is tested with real measurement data sets.
pub.1164765454,Keyed Watermarks: A Fine-grained Watermark Generation for Apache Flink,"<p>Big Data Stream processing engines, exemplified by tools like Apache Flink, employ windowing techniques to manage unbounded streams of events. Aggregating relevant data within Windows is important for event-time windowing due to its impact on result accuracy. A pivotal role in this process is attributed to watermarks, unique timestamps signifying event progression in time. Nonetheless, the existing watermark generation method within Apache Flink, operating at the input stream level, exhibits a bias towards faster sub-streams, causing the omission of events from slower counterparts. Our analysis determined that Apache Flink's standard watermark generation approach results in an approximate 33% data loss when 50% of median-proximate keys experience delays. Furthermore, this loss exceeds 37% in cases where 50% of randomly selected keys encounter delays. </p> <p>In this paper, we introduce a pioneering approach termed keyed watermarks to address data loss concerns and enhance data processing precision to a minimum of 99% in most scenarios. Our strategy facilitates distinct progress monitoring by creating individualized watermarks for each logical sub-stream (key). Within our investigation, we delineate the essential architectural and API modifications requisite for integrating keyed watermarks while also highlighting our experience in navigating the expansion of Apache Flink's extensive codebase. Moreover, we conduct a comparative evaluation between the efficacy of our approach and the conventional watermark generation technique concerning the accuracy of event-time tracking, the latency of watermark processing, and the growth of Flink's maintained state.</p>"
pub.1131599341,Plumb: Efficient stream processing of multi‐user pipelines,"Abstract  Operational services run 24 × 7 and require analytics pipelines to evaluate performance. In mature services such as domain name system (DNS), these pipelines often grow to many stages developed by multiple, loosely coupled teams. Such pipelines pose two problems: first, computation and data storage may be duplicated across components developed by different groups, wasting resources. Second, processing can be skewed , with structural skew occurring when different pipeline stages need different amounts of resources, and computational skew occurring when a block of input data requires increased resources. Duplication and structural skew both decrease efficiency, increasing cost, latency, or both. Computational skew can cause pipeline failure or deadlock when resource consumption balloons; we have seen cases where pessimal traffic increases CPU requirements 6‐fold. Detecting duplication is challenging when components from multiple teams evolve independently and require fault isolation. Skew management is hard due to dynamic workloads coupled with the conflicting goals of both minimizing latency and maximizing utilization. We propose Plumb , a framework to abstract stream processing as large‐block streaming (LBS) for a multi‐stage, multi‐user workflow. Plumb users express analytics as a DAG of processing modules, allowing Plumb to integrate and optimize workflows from multiple users. Many real‐world applications map to the LBS abstraction. Plumb detects and eliminates duplicate computation and storage, and it detects and addresses both structural and computational skew by tracking computation across the pipeline. We exercise Plumb using the analytics pipeline for B‐Root DNS. We compare Plumb to a hand‐tuned system, cutting latency to one‐third the original, and requiring  39 %  fewer container hours, while supporting more flexible, multi‐user analytics and providing greater robustness to DDoS‐driven demands. "
pub.1095789043,Integrative Dynamic Reconfiguration in a Parallel Stream Processing Engine,"Load balancing, operator instance collocations and horizontal scaling are critical issues in Parallel Stream Processing Engines to achieve low data processing latency, optimized cluster utilization and minimized communication cost respectively. In previous work, these issues are typically tackled separately and independently. We argue that these problems are tightly coupled in the sense that they all need to determine the allocations of workloads and migrate computational states at runtime. Optimizing them independently would result in suboptimal solutions. Therefore, in this paper, we investigate how these three issues can be modeled as one integrated optimization problem. In particular, we first consider jobs, where workload allocations have little effect on the communication cost, and model the problem of load balance as a Mixed-Integer Linear Program. Afterwards, we present an extended solution called ALBIC, which supports general jobs. We implement the proposed techniques on top of Apache Storm, an open-source Parallel Stream Processing Engine. The extensive experimental results over both synthetic and real datasets show that our techniques clearly outperform existing approaches."
pub.1164451866,A Privacy Enforcing Framework for Data Streams on the Edge,"Recent developments in machine learning (ML) allow for efficient data stream processing and also help in meeting various privacy requirements. Traditionally, predefined privacy policies are enforced in resource-rich and homogeneous environments such as in the cloud to protect sensitive information from being exposed. However, large amounts of data streams generated from heterogeneous IoT devices often result in high computational costs, cause network latency, and increase the chance of data interruption as data travels away from the source. Therefore, this article proposes a novel privacy-enforcing framework for transforming data streams by executing various privacy policies close to the data source. To achieve our proposed framework, we enable domain experts to specify high-level privacy policies in a human-readable form. Then, the edge-based runtime system analyzes data streams (i.e., generated from nearby IoT devices), interprets privacy policies (i.e., deployed on edge devices), and transforms data streams if privacy violations occur. Our proposed runtime mechanism uses a Deep Neural Networks (DNN) technique to detect privacy violations within the streamed data. Furthermore, we discuss the framework, processes of the approach, and the experiments carried out on a real-world testbed to validate its feasibility and applicability."
pub.1033135768,Realtime Data Processing at Facebook,"Realtime data processing powers many use cases at Facebook, including realtime reporting of the aggregated, anonymized voice of Facebook users, analytics for mobile applications, and insights for Facebook page administrators. Many companies have developed their own systems; we have a realtime data processing ecosystem at Facebook that handles hundreds of Gigabytes per second across hundreds of data pipelines. Many decisions must be made while designing a realtime stream processing system. In this paper, we identify five important design decisions that affect their ease of use, performance, fault tolerance, scalability, and correctness. We compare the alternative choices for each decision and contrast what we built at Facebook to other published systems. Our main decision was targeting seconds of latency, not milliseconds. Seconds is fast enough for all of the use cases we support and it allows us to use a persistent message bus for data transport. This data transport mechanism then paved the way for fault tolerance, scalability, and multiple options for correctness in our stream processing systems Puma, Swift, and Stylus. We then illustrate how our decisions and systems satisfy our requirements for multiple use cases at Facebook. Finally, we reflect on the lessons we learned as we built and operated these systems."
pub.1110359719,Multi-Agent Big-Data Lambda Architecture Model for E-Commerce Analytics,"We study big-data hybrid-data-processing lambda architecture, which consolidates low-latency real-time frameworks with high-throughput Hadoop-batch frameworks over a massively distributed setup. In particular, real-time and batch-processing engines act as autonomous multi-agent systems in collaboration. We propose a Multi-Agent Lambda Architecture (MALA) for e-commerce data analytics. We address the high-latency problem of Hadoop MapReduce jobs by simultaneous processing at the speed layer to the requests which require a quick turnaround time. At the same time, the batch layer in parallel provides comprehensive coverage of data by intelligent blending of stream and historical data through the weighted voting method. The cold-start problem of streaming services is addressed through the initial offset from historical batch data. Challenges of high-velocity data ingestion is resolved with distributed message queues. A proposed multi-agent decision-maker component is placed at the MALA stack as the gateway of the data pipeline. We prove efficiency of our batch model by implementing an array of features for an e-commerce site. The novelty of the model and its key significance is a scheme for multi-agent interaction between batch and real-time agents to produce deeper insights at low latency and at significantly lower costs. Hence, the proposed system is highly appealing for applications involving big data and caters to high-velocity streaming ingestion and a massive data pool."
pub.1125945313,A Unified Storage System for Whole-time-range Data Analytics over Unbounded Data,"Nowadays, enterprises rely on acquiring and analysis over huge amounts of data to earn profits on the markets. The data are produced from many sources in forms of event streams and unbounded by nature. In order to efficiently analyze these unbounded data, unified analytic frameworks such as Flink and Spark are introduced recently, which provide both real-time stream processing as well as batch processing. However, the unbounded data is still stored in isolated systems: one for real-time data and another for historical data. Such isolation brings additional complications for both application developers and platform operators. Moreover, multiple storage systems also introduce data migration overhead and increase total system cost. In this paper, we present CStream, a unified storage system for unbounded data streams. CStream uses row format for newly arrived data and columnar format for historical data. In addition, CStream transforms row format data into columnar format data asynchronously and efficiently. As a result, CStream can provide both low-latency real-time data access and high-bandwidth historical data access. We conduct extensive experiments, and the results show that CStream outperforms Kafka by up to 50% in write performance and outperforms HDFS by up to 30% in columnar read performance."
pub.1105021389,FogStore,"We design Fogstore, a key-value store for event-based systems, that exploits the concept of relevance to guarantee low-latency access to relevant data with strong consistency guarantees, while providing tolerance from geographically correlated failures. Distributed event-based processing pipelines are envisioned to utilize the resources of densely geo-distributed infrastructures for low-latency responses - enabling real-time applications. Increasing complexity of such applications results in higher dependence on state, which has driven the incorporation of state-management as a core functionality of contemporary stream processing engines a la Apache Flink and Samza. Processing components executing under the same context (like location) often produce information that may be relevant to others, thereby necessitating shared state and an out-of-band globally-accessible data-store. Efficient access to application state is critical for overall performance, thus centralized data-stores are not a viable option due to the high-latency of network traversals. On the other hand, a highly geo-distributed datastore with low-latency implemented with current key-value stores would necessitate degrading client expectation of consistency as per the PACELC theorem. In this paper we exploit the notion of contextual relevance of events (data) in situation-awareness applications - and offer differential consistency guarantees for clients based on their context. We highlight important systems concerns that may arise with a highly geo-distributed system and show how Fogstore's design tackles them. We present, in detail, a prototype implementation of Fogstore's mechanisms on Apache Cassandra and a performance evaluation. Our evaluations show that Fogstore is able to achieve the throughput of eventually consistent configurations while serving data with strong consistency to the contextually relevant clients."
pub.1096929981,Implementation of Complex Event Processing for Intelligent-Field,"Abstract One of the main challenges for Intelligent Field engineers is handling large amount of data stream in real-time. Finding meaningful patterns among this huge amount of data is just like trying to find a needle in a haystack. Every day reservoir and production engineers are being bombarded with streams of massive amount of real-time data coming from all kinds of Intelligent Field equipment’s including but not limited to Permanent Downhole Monitoring Systems (PDHMS), Multiphase Flowmeters (MPFM), MicroMotion Meters, Wellhead Gauges, Smart Well Completion (SWC), and Electrical Submersible Pumps (ESP). They spend significant amount of time and effort looking at the trends and analyzing the data to find anomalies. Moreover, the existing systems for data cleansing and summarization are based on batch processing, hence, engineers cannot make the right decision on time and they do not have the mechanism to instantly detect interesting patterns as data coming in streams. The objective of this paper is to share Saudi ARAMCO experience with Complex Event Processing (CEP) as an emerging technology that is designed for low-latency and high-throughput event processing for data stream. This paper addresses the architecture, the implementation, and the benefits of CEP as a solution for Intelligent Field. The implementation will cover three common applications of CEP namely real-time data cleansing, pattern detection, and Event-Driven computation. Data cleansing covers handling out-of-bound, negative, and frozen values. Patten detection enables the detection of anomalous behavior in the data stream, such as unusual buildup or drawdown in well pressure in real-time. Event-Driven computation is trigged by events in the field, such as change in downhole pressure to perform advanced calculation logic, such as average reservoir pressure. Implementing CEP will help reservoir and production engineers obtain clean data in real time and receive notification in case of any significant event detected."
pub.1124150859,Edge-Centric Queries’ Stream Management Based on an Ensemble Model,"The Internet of things (IoT) involves numerous devices that can interact with each other or with their environment to collect and process data. The collected data streams are guided to the cloud for further processing and the production of analytics. However, any processing in the cloud, even if it is supported by improved computational resources, suffers from an increased latency. The data should travel to the cloud infrastructure as well as the provided analytics back to end users or devices. For minimizing the latency, we can perform data processing at the edge of the network, i.e., at the edge nodes. The aim is to deliver analytics and build knowledge close to end users and devices minimizing the required time for realizing responses. Edge nodes are transformed into distributed processing points where analytics queries can be served. In this paper, we deal with the problem of allocating queries, defined for producing knowledge, to a number of edge nodes. The aim is to further reduce the latency by allocating queries to nodes that exhibit low load (the current and the estimated); thus, they can provide the final response in the minimum time. However, before the allocation, we should decide the computational burden that a query will cause. The allocation is concluded by the assistance of an ensemble similarity scheme responsible to deliver the complexity class for each query. The complexity class, thus, can be matched against the current load of every edge node. We discuss our scheme, and through a large set of simulations and the adoption of benchmarking queries, we reveal the potentials of the proposed model supported by numerical results."
pub.1117328370,Self-Adaptive Data Stream Processing in Geo-Distributed Computing Environments,"An ever increasing number of services requires real-time analysis of collected data streams. Emerging Fog/Edge computing platforms are appealing for such latency-sensitive applications, encouraging the deployment of Data Stream Processing (DSP) systems in geo-distributed environments. However, the highly dynamic nature of these infrastructures poses challenges on how to satisfy the Quality of Service requirements of both the application and the infrastructure providers. In this doctoral work we investigate how DSP systems can face the dynamicity of workloads and computing environments by self-adapting their deployment and behavior at run-time. Targeting geo-distributed infrastructures, we specifically search for decentralized solutions, and propose a framework for organizing adaptation using a hierarchical control approach. Focusing on application elasticity, we equip the framework with decentralized policies based on reinforcement learning. We extend our solution to consider multi-level elasticity, and heterogeneous computing resources. In the ongoing research work, we aim to face the challenges associated with mobility of users and computing resources, exploring complementary adaptation mechanisms."
pub.1013875873,Online anomaly detection for multi‐source VMware using a distributed streaming framework,"Summary Anomaly detection refers to the identification of patterns in a dataset that do not conform to expected patterns. Such non‐conformant patterns typically correspond to samples of interest and are assigned to different labels in different domains, such as outliers, anomalies, exceptions, and malware. A daunting challenge is to detect anomalies in rapid voluminous streams of data. This paper presents a novel, generic real‐time distributed anomaly detection framework for multi‐source stream data. As a case study, we investigate anomaly detection for a multi‐source VMware‐based cloud data center, which maintains a large number of virtual machines (VMs). This framework continuously monitors VMware performance stream data related to CPU statistics (e.g., load and usage). It collects data simultaneously from all of the VMs connected to the network and notifies the resource manager to reschedule its CPU resources dynamically when it identifies any abnormal behavior from its collected data. A semi‐supervised clustering technique is used to build a model from benign training data only. During testing, if a data instance deviates significantly from the model, then it is flagged as an anomaly. Effective anomaly detection in this case demands a distributed framework with high throughput and low latency. Distributed streaming frameworks like Apache Storm, Apache Spark, S4, and others are designed for a lower data processing time and a higher throughput than standard centralized frameworks. We have experimentally compared the average processing latency of a tuple during clustering and prediction in both Spark and Storm and demonstrated that Spark processes a tuple much quicker than storm on average. Copyright © 2016 John Wiley & Sons, Ltd."
pub.1108006517,Aten: A Dispatcher for Big Data Applications in Heterogeneous Systems,"Stream Processing Engines (SPEs) have to support high data ingestion to ensure the quality and efficiency for the end-user or a system administrator. The data flow processed by SPE fluctuates over time, and requires real-time or near realtime resource pool adjustments (network, memory, CPU and other). This scenario leads to the problem known as skewed data production caused by the non-uniform incoming flow at specific points on the environment, resulting in slow down of applications caused by network bottlenecks and inefficient load balance. This work proposes Aten as a solution to overcome unbalanced data flows processed by Big Data Stream applications in heterogeneous systems. Aten manages data aggregation and data streams within message queues, assuming different algorithms as strategies to partition data flow over all the available computational resources. The paper presents preliminary results indicating that is possible to maximize the throughput and also provide low latency levels for SPEs."
pub.1016981617,An Integration Framework for Sensor Networks and Data Stream Management Systems,"Sensor networks serve as a natural data source to data stream management systems (DSMSs), and in return DSMSs are capable of executing much more complex operations on the data than nodes in the network, allowing a wider variety of queries to be performed on sensor produced data. Integration of these systems to create a unified query plan that is executed across DSMS/sensor boundaries is not a trivial task because of the different architectures and assumptions of these systems. This chapter presents an integrated query processing environment where users can seamlessly query both a data stream management system and a sensor network with one query expression. By integrating the two query processing systems, the optimization goals of the sensor network (primarily power) and server network (primarily latency and quality) can be unified into one quality of service metric. The chapter shows various steps of the unified optimization process for a sample query where the effects of each step that the optimizer takes can be directly viewed using a quality of service monitor. The chapter includes sensors deployed in the demo area in a tiny mockup of a factory application."
pub.1145475933,Sorting in Memristive Memory," Sorting data is needed in many application domains. Traditionally, the data is read from memory and sent to a general-purpose processor or application-specific hardware for sorting. The sorted data is then written back to the memory. Reading/writing data from/to memory and transferring data between memory and processing unit incur significant latency and energy overhead. In this work, we develop the first architectures for in-memory sorting of data to the best of our knowledge. We propose two architectures. The first architecture is applicable to the conventional format of representing data, i.e., weighted binary radix. The second architecture is proposed for developing unary processing systems, where data is encoded as uniform unary bit-streams. As we present, each of the two architectures has different advantages and disadvantages, making one or the other more suitable for a specific application. However, the common property of both is a significant reduction in the processing time compared to prior sorting designs. Our evaluations show on average 37 × and 138× energy reduction for binary and unary designs, respectively, compared to conventional CMOS off-memory sorting systems in a 45 nm technology. We designed a 3×3 and a 5×5 Median filter using the proposed sorting solutions, which we used for processing 64×64 pixel images. Our results show a reduction of 14× and 634× in energy and latency, respectively, with the proposed binary, and 5.6× and 152×10 3 in energy and latency with the proposed unary approach compared to those of the off-memory binary and unary designs for the 3 × 3 Median filtering system. "
pub.1120635680,Adaptive Partitioning and Order-Preserved Merging of Data Streams,"Partitioning is a key concept for utilizing modern hardware, especially to exploit parallelism opportunities from many-core CPUs. In data streaming scenarios where parameters like tuple arrival rates can vary, adaptive strategies for partitioning solve the problem of overestimating or underestimating query workloads. While there are many possibilities to partition the data flow, threads running partitions independently from each other lead to unordered output inevitably. This is a considerable difficulty for applications where tuple order matters, like in stream reasoning or complex event processing scenarios.In this paper, we address this problem by combining an adaptive partitioning approach with an order-preserving merge algorithm. Since reordering output tuples can only worsen latency, we mainly focus on the throughput of queries while keeping the delay on individual tuples minimal. We run micro-benchmarks as well as the Linear Road benchmark, demonstrating correctness and effectiveness of our approach while scaling out on a single Xeon Phi many-core CPU up to 256 partitions."
pub.1122306188,Assessing the Dependability of Apache Spark System: Streaming Analytics on Large-Scale Ocean Data,"Real-world data from diverse domains require real-time scalable analysis. Large-scale data processing frameworks or engines such as Hadoop fall short when results are needed on-the-fly. Apache Spark’s streaming library is increasingly becoming a popular choice as it can stream and analyze a significant amount of data. In this paper, we analyze large-scale geo-temporal data collected from the USGODAE (United States Global Ocean Data Assimilation Experiment) data catalog, and showcase and assess the dependability of Spark stream processing. We measure the latency of streaming and monitor scalability by adding and removing nodes in the middle of a streaming job. We also verify the fault tolerance by stopping nodes in the middle of a job and making sure that the job is rescheduled and completed on other nodes. We design a full-stack application that automates data collection, data processing and visualizing the results. We also use Google Maps API to visualize results by color coding the world map with values from various analytics."
pub.1165811944,Development and implementation of a MATLAB-based phasor data concentrator for synchrophasor applications,"This work presents the development of MatPDC, an IEEE Std. C37.118.2-2011 compliant Phasor Data Concentrator (PDC) implemented in the Matlab environment. MatPDC enables the integration of wide-area monitoring, protection and control system synchrophasor data into the Matlab platform, offering real-time access to synchronized data streams from multiple Phasor Measurement Units (PMUs). The key features of MatPDC include real-time data access, data integrity verification, data aggregation and alignment, compliant data communication, and latency calculation. These features facilitate efficient processing and analysis of synchronized data, reducing complexity and improving latency between data sources and applications. The development of MatPDC addresses the need for accessing and analyzing real-time synchrophasor data within the Matlab environment, providing researchers and engineers with a powerful tool for power system analysis and experimentation. By developing the PDC and synchrophasor applications on the same platform, MatPDC reduces complexity and latency between data sources and applications. It offers real-time access to synchronized data streams, ensuring data integrity through verification mechanisms and aggregating the data into a unified dataset based on time tags. The implementation of MatPDC opens up opportunities for researchers to work with real-time synchrophasor data. It facilitates the development of advanced algorithms, real-time simulations, and the verification of control strategies. The integration and evaluation of MatPDC demonstrate its effectiveness and potential in power system analysis, providing researchers and engineers with a valuable tool for their research and development activities."
pub.1142933781,Trisk,"Due to the long-run and unpredictable nature of stream processing, any statically configuredexecution of stream jobs fails to process data in a timely and efficient manner. To achieve performance requirements, stream jobs need to be reconfigured dynamically. In this paper, we present Trisk, a control plane that support versatile reconfigurations while keeping high efficiency with easy-to-use programming APIs. Trisk enables versatile reconfigurations with usability based on a task-centric abstraction, and encapsulates primitive operations such that reconfigurations can be described by compositing the primitive operations on the abstraction. Trisk adopts a partial pause-and-resume design for efficiency, through which synchronization mechanisms in the native stream systems can further be leveraged. We implement Trisk on Apache Flink and demonstrate its usage and performance under realistic application scenarios. We show that Trisk executes reconfigurations with shorter completion time and comparable latency compared to a state-of-the-art fluid mechanism for state management."
pub.1137105630,An Online Approach to Determine Correlation between Data Streams,"Real time stream processing demands processed outcomes in minimal latency. Massive streams are generated in real time where linear relationship is determined using correlation. Existing approaches are used for correlating static data sets such as, Kandell, Pearson, Spearman etc. These approaches are insufficient to solve noise free online correlation. In this paper, we propose an online ordinal correlation approach having functionalities such as single pass, avoiding recalculation from scratch, removing outliers, and low memory requirements. In this approach, Compare Reduce Aggregate (CRA) algorithm is used for determining association between two feature vectors in real time using single scanning technique. Time and space complexities in CRA algorithm are measured as O(n) and O(1), respectively. This algorithm is used for reducing noise or error in a stream and used as a replacement of rank based correlation. It is recommended to have distinct elements and less variability in the streams for gaining maximum performance of this algorithm."
pub.1093872400,Adaptive Analytic Service for Real-Time Internet of Things Applications,"Emerging Internet of Things(IoT) applications are moving from silo and small scale sensor data sharing to composite and large scale ones. With the rapid growth of application scale, IoT applications is going to leverage cloud infrastructure for scalable solutions and real-time services. Thus large volumes of heterogeneous and high frequency sensor data are fed into IoT cloud services for real-time actionable insight, which raises great challenges of performance and adaptability on cloud solutions. In this paper, we propose a streaming based processing infrastructure for high throughput and low latency IoT real-time analytics services. A data adaptive mechanism is also introduced for heterogeneous data stream integration, interpreting and processing with application logics, as well as context stream. We implemented the proposed mechanisms with spark streaming, and deployed real time IoT analytics service in cloud. Experiment results show that the service has a good scalability and high throughput for IoT data analytics."
pub.1143050452,Dependable IoT Data Stream Processing for Monitoring and Control of Urban Infrastructures,"The Internet of Things describes a network of physical devices interacting and producing vast streams of sensor data. At present there are a number of general challenges which exist while developing solutions for use cases involving the monitoring and control of urban infrastructures. These include the need for a dependable method for extracting value from these high volume streams of time sensitive data which is adaptive to changing workloads. Low-latency access to the current state for live monitoring is a necessity as well as the ability to perform queries on historical data. At the same time, many design choices need to be made and the number of possible technology options available further adds to the complexity. In this paper we present a dependable IoT data processing platform for the monitoring and control of urban infrastructures. We define requirements in terms of dependability and then select a number of mature open-source technologies to match these requirements. We examine the disparate parts necessary for delivering a holistic overall architecture and describe the dataflows between each of these components. We likewise present generalizable methods for the enrichment and analysis of sensor data applicable across various application areas. We demonstrate the usefulness of this approach by providing an exemplary prototype platform executing on top of Kubernetes and evaluate the effectiveness of jobs processing sensor data in this environment."
pub.1140636011,Dependable IoT Data Stream Processing for Monitoring and Control of Urban Infrastructures,"The Internet of Things describes a network of physical devices interacting
and producing vast streams of sensor data. At present there are a number of
general challenges which exist while developing solutions for use cases
involving the monitoring and control of urban infrastructures. These include
the need for a dependable method for extracting value from these high volume
streams of time sensitive data which is adaptive to changing workloads.
Low-latency access to the current state for live monitoring is a necessity as
well as the ability to perform queries on historical data. At the same time,
many design choices need to be made and the number of possible technology
options available further adds to the complexity.
  In this paper we present a dependable IoT data processing platform for the
monitoring and control of urban infrastructures. We define requirements in
terms of dependability and then select a number of mature open-source
technologies to match these requirements. We examine the disparate parts
necessary for delivering a holistic overall architecture and describe the
dataflows between each of these components. We likewise present generalizable
methods for the enrichment and analysis of sensor data applicable across
various application areas. We demonstrate the usefulness of this approach by
providing an exemplary prototype platform executing on top of Kubernetes and
evaluate the effectiveness of jobs processing sensor data in this environment."
pub.1157282633,Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline,"Edge computing solutions that enable the extraction of high-level information
from a variety of sensors is in increasingly high demand. This is due to the
increasing number of smart devices that require sensory processing for their
application on the edge. To tackle this problem, we present a smart vision
sensor System on Chip (SoC), featuring an event-based camera and a low-power
asynchronous spiking Convolutional Neural Network (sCNN) computing architecture
embedded on a single chip. By combining both sensor and processing on a single
die, we can lower unit production costs significantly. Moreover, the simple
end-to-end nature of the SoC facilitates small stand-alone applications as well
as functioning as an edge node in larger systems. The event-driven nature of
the vision sensor delivers high-speed signals in a sparse data stream. This is
reflected in the processing pipeline, which focuses on optimising highly sparse
computation and minimising latency for 9 sCNN layers to 3.36{\mu}s for an
incoming event. Overall, this results in an extremely low-latency visual
processing pipeline deployed on a small form factor with a low energy budget
and sensor cost. We present the asynchronous architecture, the individual
blocks, and the sCNN processing principle and benchmark against other sCNN
capable processors."
pub.1119982262,An Edge-Based Framework for Enabling Data-Driven Pipelines for IoT Systems,"Due to the proliferation of the Internet of Things (IoT) paradigm, the number of devices connected to the Internet is growing. These devices are generating unprecedented amounts of data at the edges of the infrastructure. Although the generated data provides great potential, identifying and processing relevant data points hidden in streams of unimportant data, and doing this in near real time, remains a significant challenge. Existing stream processing platforms require the data to be transported to the cloud for processing, resulting in latencies that can prevent timely decision making or may reduce the amount of data processed. To tackle this problem, we designed an IoT Edge Framework, called R-Pulsar, that extends cloud capabilities to local devices and provides a programming model for deciding what, when, and where data get collected and processed. In this paper, we discuss motivating use cases and the architectural design of R-Pulsar. We have deployed and tested R-Pulsar on embedded devices (Raspberry Pi and Android phone) and present an experimental evaluation that demonstrates that R-Pulsar can enable timely data analytics by effectively leveraging edge and cloud resources."
pub.1130843791,IoT-enabled directed acyclic graph in spark cluster,"Real-time data streaming fetches live sensory segments of the dataset in the heterogeneous distributed computing environment. This process assembles data chunks at a rapid encapsulation rate through a streaming technique that bundles sensor segments into multiple micro-batches and extracts into a repository, respectively. Recently, the acquisition process is enhanced with an additional feature of exchanging IoT devices’ dataset comprised of two components: (i) sensory data and (ii) metadata. The body of sensory data includes record information, and the metadata part consists of logs, heterogeneous events, and routing path tables to transmit micro-batch streams into the repository. Real-time acquisition procedure uses the Directed Acyclic Graph (DAG) to extract live query outcomes from in-place micro-batches through MapReduce stages and returns a result set. However, few bottlenecks affect the performance during the execution process, such as (i) homogeneous micro-batches formation only, (ii) complexity of dataset diversification, (iii) heterogeneous data tuples processing, and (iv) linear DAG workflow only. As a result, it produces huge processing latency and the additional cost of extracting event-enabled IoT datasets. Thus, the Spark cluster that processes Resilient Distributed Dataset (RDD) in a fast-pace using Random access memory (RAM) defies expected robustness in processing IoT streams in the distributed computing environment. This paper presents an IoT-enabled Directed Acyclic Graph (I-DAG) technique that labels micro-batches at the stage of building a stream event and arranges stream elements with event labels. In the next step, heterogeneous stream events are processed through the I-DAG workflow, which has non-linear DAG operation for extracting queries’ results in a Spark cluster. The performance evaluation shows that I-DAG resolves homogeneous IoT-enabled stream event issues and provides an effective stream event heterogeneous solution for IoT-enabled datasets in spark clusters."
pub.1008938941,A Pipelining Implementation for High Resolution Seismic Hazard Maps Production,"Seismic hazard maps are a significant input into emergency hazard management that play an important role in saving human lives and reducing the economic effects after earthquakes. Despite the fact that a number of software tools have been developed (McGuire, 1976, 1978; Bender & Perkins, 1982, 1987; Robinson et al., 2005, 2006; Field et al., 2003), map resolution is generally low, potentially leading to uncertainty in calculations of ground motion level and underestimation of the seismic hazard in a region. In order to generate higher resolution maps, the biggest challenge is to handle the significantly increased data processing workload.In this study, a method for improving seismic hazard map resolution is presented that employs a pipelining implementation of the existing EqHaz program suite (Assatourians & Atkinson, 2013) based on IBM InfoSphere Streams–an advanced stream computing platform. Its architecture is specifically configured for continuous analysis of massive volumes of data at high speeds and low latency. Specifically, it treats processing workload as data streams. Processing procedures are implemented as operators that are connected to form processing pipelines. To handle large processing workload, these pipelines are flexible and scalable to be deployed and run in parallel on large-scale HPC clusters to meet application performance requirements. As a result, mean hazard calculations are possible for maps with resolution up to 2,500,000 points with near-real-time processing time of approximately 5-6minutes."
pub.1149940388,Horae: A Graph Stream Summarization Structure for Efficient Temporal Range Query,"Graph stream, referred to as an evolving graph with a timing sequence of updated edges through a continuous stream, is an emerging data format widely used in big data applications. Coping with a graph stream is challenging because: 1) fully storing the continuously produced and extremely large-scale datasets is difficult if not impossible; 2) supporting queries relevant to both graph topology and temporal information is nontrivial. Recently, graph stream summarization techniques have attracted much attention in providing approximate storage and query processing for a graph stream. Existing designs largely utilize hash functions to reduce the graph scale and leverage a compressive matrix to represent the graph stream. However, such designs are unable to store the time dimension information of graph streams, and thus fail to support temporal queries. In this paper, we propose Horae, a novel graph stream summarization structure for efficient temporal range query, which presents a time prefix embedded multi-layer summarization structure. Our design is based on the insight that an arbitrary temporal range of length $L$ can be decomposed to at most $2\log L$ sub-ranges, where all the time points in each sub-range have the same binary code prefix. We further design an efficient Binary Range Decomposition (BRD) algorithm, which achieves a logarithmic scale query processing time. Experimental results show that Horae significantly reduces the latency of various temporal range queries by two to three orders of magnitude compared to the state-of-the-art designs."
pub.1139074434,DROAllocator: A Dynamic Resource-Aware Operator Allocation Framework in Distributed Streaming Processing,"With the rapid development of Internet services and the Internet of Things (IoT), many studies focus on operator allocation to enhance the DSPAs’ (data stream processing applications) performance and resource utilization. However, the existing approaches ignore the dynamic changes of the node resources to allocate the operator instances to guarantee the performance, which increasing the number of migration leads to the waste of resources and the instability. To address these issues, we propose a framework named DROAllocator to select the appropriate nodes to allocate the operator instances. By capturing the change tendency of the node resources and the operator performance, our allocation mechanism decreases the number of migration to enhance the performance. The experimental results show the DROAllocator not only decrease the number of migrations to allocate the operator instances to ensure the end-to-end throughput and the latency, but also enhance the resource utilization."
pub.1095599042,Providing QoS Guarantees in Large-Scale Operator Networks,"Application areas like global sensor networks and data stream processing involve the on-line processing of large amounts of data in an overlay network of operators on top of the Internet infrastructure. Trying to fulfill QoS guarantees in such networks is a challenging task that should be realized under the requirement for optimal usage of common resources in the network. Therefore in this paper, we formalize a constrained optimization problem for the placement of operators in an overlay network which strives for satisfying user QoS constraints subject to latency, while minimizing the network load induced by the deployment of the operators in the network. Since the initial problem is NP-hard, we solve at a first step the problem in an intermediate continuous latency space and then we map the continuous solution to its discrete variant. Our evaluations provide an analysis about the inherent interdepedence between the two metrics, network usage and latency, subject to this paper and furthermore show that our algorithm achieves a good balance between the user requirements and the usage of the network resources."
pub.1130833289,Parallel Discovery of Trajectory Companion Pattern and System Evaluation,"Trajectories consist of spatial information of moving objects. Over contious time spans, trajectory data form data streams constantly generated from diverse and geographically distributed sources. Discovery of traveling patterns on trajectory streams such as gathering and companies enables value domain applications. Such a discovery needs to process arrival records in various sources and correlate across records near real-time. Thus techniques for handling trajectory streams should scale on distributed cluster computing. The challenge is at three aspects, namely a data model to represent the continuous trajectory data, the parallelism of the discovery algorithm, and an end-to-end parallel framework. In this paper, we propose a parallel discovery method that consists of 1) a model of partitioning trajectory samples on various time intervals; 2) definition on distance measurements of trajectories; and 3) a parallel discovery algorithm. We build a stream processing workflow and investigate experiments on a public dataset to evaluate the system's performance, scalability, stability, and data intensity. Our method discovers trajectory gathering patterns with low latency and scales as the size of trajectory data grows."
pub.1171350436,Efficient Placement of Decomposable Aggregation Functions for Stream Processing over Large Geo-Distributed Topologies,"A recent trend in stream processing is offloading the computation of decomposable aggregation functions (DAF) from cloud nodes to geo-distributed fog/edge devices to decrease latency and improve energy efficiency. However, deploying DAFs on low-end devices is challenging due to their volatility and limited resources. Additionally, in geo-distributed fog/edge environments, creating new operator instances on demand and replicating operators ubiquitously is restricted, posing challenges for achieving load balancing without overloading devices. Existing work predominantly focuses on cloud environments, overlooking DAF operator placement in resource-constrained and unreliable geo-distributed settings. This paper presents NEMO, a resource-aware optimization approach that determines the replication factor and placement of DAF operators in resource-constrained geo-distributed topologies. Leveraging Euclidean embeddings of network topologies and a set of heuristics, NEMO scales to millions of nodes and handles topo-logical changes through adaptive re-placement and re-replication decisions. Compared to existing solutions, NEMO achieves up to 6× lower latency and up to 15× reduction in communication cost, while preventing overloaded nodes. Moreover, NEMO re-optimizes placements in constant time, regardless of the topology size. As a result, it lays the foundation to efficiently process continuous data streams on large, heterogeneous, and geo-distributed topologies."
pub.1095447062,A New Parallelization Model for Detecting Temporal Bursts in Large-Scale Document Streams on a Multi-Core CPU,"Burstiness is the simplest but the most robust criterion for detecting topics and events in online documents. Online documents are referred to as document streams because they have a temporal order. Kleinberg's temporal burst detection algorithm is the most successful algorithm for detecting bursty periods related to a topic-or event-related keyword. Kleinberg's temporal burst detection algorithm aims to find certain time periods in which a keyword occurs at a high frequency. In recent times, large-scale online documents are increasingly common on social media. Therefore, speed-up of burst-detection processing is one of the most important issues in this era of big data. In this paper, we propose a novel parallelization model, called the hybrid parallelization model with a hidden I/O thread, to enable the parallel processing of Kleinberg's temporal burst detection algorithm on a multi-core CPU. In a multi-core CPU environment, I/O latency is a critical issue for improving the performance of a parallelization model. To automatically hide the I/O latency, the proposed parallelization model utilizes speculative I/Os. The results of experiments using actual large-scale document streams show that the proposed parallelization model performs well compared with a conventional parallelization model."
pub.1117767424,On the Cost of Acking in Data Stream Processing Systems,"The widespread use of social networks and applications such as IoT networks generates a continuous stream of data that companies and researchers want to process, ideally in real-time. Data stream processing systems (DSP) enable such continuous data analysis by implementing the set of operations to be performed on the stream as directed acyclic graph (DAG) of tasks. While these DSP systems embed mechanisms to ensure fault tolerance and message reliability, only few studies focus on the impact of these mechanisms on the performance of applications at runtime. In this paper, we demonstrate the impact of the message reliability mechanism on the performance of the application. We use an experimental approach, using the Storm middleware, to study an acknowledgment-based framework. We compare the two standard schedulers available in Storm with applications of various degrees of parallelism, over single and multi cluster scenarios. We show that the acking layer may create an unforeseen bottleneck due to the acking tasks placement; a problem which, to the best of our knowledge, has been overlooked in the scientific and technical literature. We propose two strategies for improving the acking tasks placement and demonstrate their benefit in terms of throughput and latency."
pub.1095413832,Real-Time Integration of Building Energy Data,"An Energy Management System (EMS) is a monitoring tool that tracks buildings energy consumption with the purpose of enhancing energy efficiency, by identifying savings opportunities and misuse situations. To achieve this, EMSs collect data flows-data streams-from a network of energy meters and sensors, which are then combined into useful information. Data must be processed in real-time, to support a timely decision making process. Traditionally EMSs use Database Management Systems (DBMSs) to process data, introducing a persistence step that leads to an unacceptable latency on data evaluation and do not properly support many types of time-series queries. This work explores the feasibility of Data Stream Management Systems (DSMSs) to support Energy Management applications, pointing out how to implement an EMS capable of real-time data processing."
pub.1105430227,Fast SPARQL Join Processing between Distributed Streams and Stored RDF Graphs Using Bloom Filters,"The growth of real-time data generation and stored data leads us to be constantly in thinking about the three V's big data challenges: volume, velocity and variety. Existing RDF Stream Processing (RSP) systems have solved the variety lock by defining a common model for producing, transmitting and continuously querying data in RDF model. On the volume and velocity side, the performances of RSP systems need to be improved particularly in terms of joins process between stored and streaming RDF graphs. Stored RDF data are very important in streaming context (related ontologies, summarized RDF data, non-evolutive RDF data or evolve very slowly over time, etc.) but existing RSP systems such as C-SPARQL, CQELS, SPARQLstream, EP-SPARQL, Sparkwave, etc. use non-optimized and non-scalable approaches for performing join operations between stored and dynamic RDF data. Indeed, these systems need to read the entire local or remote stored RDF data sets while RDF data streams continuously arrived and need to be processed in near real-time. This latency may negatively affect performances in terms of continuous processing and often causes multiple bottlenecks within the network in a distributed environment. That also makes impractical to refresh data or update the stored contents. This paper proposes an approach for distributed real-time joins between stored and streaming RDF graphs using Bloom filters. The join procedure consists of adding fast processing by greatly reducing intermediate results, in-memory indices storage and precomputing query partitions according to the picked SPARQL query variable(s) between the two natures of RDF data. Experimental and evaluations results confirm the performances gained with our approach which significantly speeds up the query processing compared to the actual RSP's techniques."
pub.1125840681,High Performance Sequence-to-Sequence Model for Streaming Speech Recognition,"Recently sequence-to-sequence models have started to achieve state-of-the-art
performance on standard speech recognition tasks when processing audio data in
batch mode, i.e., the complete audio data is available when starting
processing. However, when it comes to performing run-on recognition on an input
stream of audio data while producing recognition results in real-time and with
low word-based latency, these models face several challenges. For many
techniques, the whole audio sequence to be decoded needs to be available at the
start of the processing, e.g., for the attention mechanism or the bidirectional
LSTM (BLSTM). In this paper, we propose several techniques to mitigate these
problems. We introduce an additional loss function controlling the uncertainty
of the attention mechanism, a modified beam search identifying partial, stable
hypotheses, ways of working with BLSTM in the encoder, and the use of chunked
BLSTM. Our experiments show that with the right combination of these
techniques, it is possible to perform run-on speech recognition with low
word-based latency without sacrificing in word error rate performance."
pub.1154179511,Recommendation System Towards Residential Energy Saving Based on Anomaly Detection,"This paper presents a recommender system to promote energy consumption reduction behaviors in residential buildings. The system exploits data stream processing methods jointly with machine learning algorithms on real-time residential data. Specifically, the data stream includes disaggregated power consumption, context, and weather conditions data. Internally the system converts time-series data streams into discrete ordered data points, which serve as inputs for training ML models. This method is used to predict power consumption anomalies. Gradually, the system helps to shape its users’ activities into more energy-efficient ones. The experimental evaluation on real and simulated datasets demonstrates the promising performance of the proposed method, primarily when the K-neighbors neighbors’ algorithm is used to classify the features extracted with interleaving current with the previous data points. The performance assessment of the machine learning algorithms shows the suitability of our implementation for Edge and Fog platforms in terms of accuracy, latency, and model size."
pub.1119250411,Piecewise Linear Approximation in Data Streaming: Algorithmic Implementations and Experimental Analysis,"Piecewise Linear Approximation (PLA) is a well-established tool to reduce the
size of the representation of time series by approximating the series by a
sequence of line segments while keeping the error introduced by the
approximation within some predetermined threshold. With the recent rise of edge
computing, PLA algorithms find a complete new set of applications with the
emphasis on reducing the volume of streamed data. In this study, we identify
two scenarios set in a data-stream processing context: data reduction in sensor
transmissions and datacenter storage. In connection to those scenarios, we
identify several streaming metrics and propose streaming protocols as
algorithmic implementations of several state of the art PLA techniques. In an
experimental evaluation, we measure the quality of the reviewed methods and
protocols and evaluate their performance against those streaming statistics.
All known methods have deficiencies when it comes to handling streaming-like
data, e.g. inflation of the input stream, high latency or poor average error.
Our experimental results highlight the challenges raised when transferring
those classical methods into the stream processing world and present
alternative techniques to overcome them and balance the related trade-offs."
pub.1113472763,DG2CEP: a near real-time on-line algorithm for detecting spatial clusters large data streams through complex event processing,"Spatial concentrations (or spatial clusters) of moving objects, such as vehicles and humans, is a mobility pattern that is relevant to many applications. Fast detection of this pattern and its evolution, e.g., if the cluster is shrinking or growing, is useful in numerous scenarios, such as detecting the formation of traffic jams or detecting a fast dispersion of people in a music concert. On-Line detection of this pattern is a challenging task because it requires algorithms that are capable of continuously and efficiently processing the high volume of position updates in a timely manner. Currently, the majority of approaches for spatial cluster detection operate in batch mode, where moving objects location updates are recorded during time periods of a certain length and then batch-processed by an external routine, thus delaying the result of the cluster detection until the end of the time period. Further, they extensively use spatial data structures and operators, which can be troublesome to maintain or parallelize in on-line scenarios. To address these issues, in this paper we propose DG2CEP, a parallel algorithm that combines the well-known density-based clustering algorithm DBSCAN with the data stream processing paradigm Complex Event Processing (CEP) to achieve continuous and timely detection of spatial clusters. Our experiments with real-world data streams indicate that DG2CEP is able to detect the formation and dispersion of clusters with small latency while having higher similarity to DBSCAN than batch-based approaches."
pub.1150865519,Vector Quantized Bayesian Neural Network Inference for Data Streams,"Bayesian neural networks (BNN) can estimate the uncertainty in predictions, as opposed to non-Bayesian neural networks (NNs). However, BNNs have been far less widely used than non-Bayesian NNs in practice since they need iterative NN executions to predict a result for one data, and it gives rise to prohibitive computational cost. This computational burden is a critical problem when processing data streams with low-latency. To address this problem, we propose a novel model VQ-BNN, which approximates BNN inference for data streams. In order to reduce the computational burden, VQ-BNN inference predicts NN only once and compensates the result with previously memorized predictions. To be specific, VQ-BNN inference for data streams is given by temporal exponential smoothing of recent predictions. The computational cost of this model is almost the same as that of non-Bayesian NNs. Experiments including semantic segmentation on real-world data show that this model performs significantly faster than BNNs while estimating predictive results comparable to or superior to the results of BNNs."
pub.1107415889,LIFE: A FLEXIBLE TESTBED FOR LIGHT FIELD EVALUATION,"Recording and imaging the 3D world has led to the use of light fields. Capturing, distributing and presenting light field data is challenging, and requires an evaluation platform. We define a framework for real-time processing, and present the design and implementation of a light field evaluation system. In order to serve as a testbed, the system is designed to be flexible, scalable, and able to model various end-to-end light field systems. This flexibility is achieved by encapsulating processes and devices in discrete framework systems. The modular capture system supports multiple camera types, general-purpose data processing, and streaming to network interfaces. The cloud system allows for parallel transcoding and distribution of streams. The presentation system encapsulates rendering and display specifics. The real-time ability was tested in a latency measurement; the capture and presentation systems process and stream frames within a 40 ms limit."
pub.1129187647,H,"Computer telephony applications must access network interface and media processing resource boards plugged into a computer. CT boards must access telephony signals and sometimes must work in concert with their fellow boards. Voice, video and real-time fax have low-latency requirements and are event-driven with isochronous data streams, but PCs use asynchronous buses and interrupts lacking any guaranteed real-time processing. Normally the host CPU would handle this kind of processing, but with the proliferation of boards in high density CT systems, it became evident that the CPU and system bus would be overburdened and much of this time-critical processing of isochronous telephony traffic should be offloaded somehow."
pub.1118176166,Vector Quantized Bayesian Neural Network Inference for Data Streams,"Bayesian neural networks (BNN) can estimate the uncertainty in predictions,
as opposed to non-Bayesian neural networks (NNs). However, BNNs have been far
less widely used than non-Bayesian NNs in practice since they need iterative NN
executions to predict a result for one data, and it gives rise to prohibitive
computational cost. This computational burden is a critical problem when
processing data streams with low-latency. To address this problem, we propose a
novel model VQ-BNN, which approximates BNN inference for data streams. In order
to reduce the computational burden, VQ-BNN inference predicts NN only once and
compensates the result with previously memorized predictions. To be specific,
VQ-BNN inference for data streams is given by temporal exponential smoothing of
recent predictions. The computational cost of this model is almost the same as
that of non-Bayesian NNs. Experiments including semantic segmentation on
real-world data show that this model performs significantly faster than BNNs
while estimating predictive results comparable to or superior to the results of
BNNs."
pub.1121899057,An Exploratory Study of How Specialists Deal with Testing in Data Stream Processing Applications,"[Background] Nowadays, there is a massive growth of data volume and speed in many types of systems. It introduces new needs for infrastructure and applications that have to handle streams of data with low latency and high throughput. Testing applications that process such data streams has become a significant challenge for engineers. Companies are adopting different approaches to dealing with this issue. Some have developed their own solutions for testing, while others have adopted a combination of existing testing techniques. There is no consensus about how or in which contexts such solutions can be implemented. [Aims] To the best of our knowledge, there is no consolidated literature on that topic. The present paper is an attempt to fill this gap by conducting an exploratory study with practitioners. [Method] We used qualitative methods in this research, in particular interviews and survey. We interviewed 12 professionals who work in projects related to data streams, and also administered a questionnaire with other 105 professionals. The interviews went through a transcription and coding process, and the questionnaires were analysed to reinforce findings. [Results] This study presents current practices around software testing in data stream processing applications. These practices involve methodologies, techniques, and tools. [Conclusions] Our main contribution is a compendium of alternatives for many of the challenges that arise when testing streaming applications from a state-of-the-practice perspective."
pub.1094783393,Data-Driven Stream Processing at the Edge,"The popularity and proliferation of the Internet of Things (IoT) paradigm is resulting in a growing number of devices connected to the Internet. These devices are generating and consuming unprecedented amounts of data at the edges of the infrastructure, and are enabling new classes of data-driven applications, however, current approaches typically rely on cloud platforms located at the core of the infrastructure to process data. As the number of devices and the amount of data they generate and consume increases, such core-centric approaches are becoming increasingly inefficient as they need to transfer data back and forth between the edge and the core. Furthermore, the latencies associated with such data transfer may not be able to support applications involving time-critical data-driven decision making. In this paper, we propose an edge-based programming framework that allows users to define how data streams are processed based on the content and the location of the data. This enables the definition of data-driven reactive behaviors that can effectively exploit data patterns to dynamically drive stream processing, leveraging resources located at the edges of the infrastructure. We have implemented a prototype of the proposed approach and performed several experiments to evaluate its scalability and efficiency against a more typical single-cloud approach. Using a smart-city application usecase, we illustrate that the presented programming system can support data-driven stream processing using edge resources. In terms of scalability, our experiments show that the system can scale to hundreds of nodes while keeping overheads low. Our experiments also show that our approach can perform up to 56% better than a single cloud approach that does not consider data and user locality."
pub.1160788459,Harnessing Scalable Transactional Stream Processing for Managing Large Language Models [Vision],"Large Language Models (LLMs) have demonstrated extraordinary performance
across a broad array of applications, from traditional language processing
tasks to interpreting structured sequences like time-series data. Yet, their
effectiveness in fast-paced, online decision-making environments requiring
swift, accurate, and concurrent responses poses a significant challenge. This
paper introduces TStreamLLM, a revolutionary framework integrating
Transactional Stream Processing (TSP) with LLM management to achieve remarkable
scalability and low latency. By harnessing the scalability, consistency, and
fault tolerance inherent in TSP, TStreamLLM aims to manage continuous &
concurrent LLM updates and usages efficiently. We showcase its potential
through practical use cases like real-time patient monitoring and intelligent
traffic management. The exploration of synergies between TSP and LLM management
can stimulate groundbreaking developments in AI and database research. This
paper provides a comprehensive overview of challenges and opportunities in this
emerging field, setting forth a roadmap for future exploration and development."
pub.1149437453,A state lossless scheduling strategy in distributed stream computing systems,"Stateful scheduling is of critical importance for the performance of a distributed stream computing system. In such a system, inappropriate task deployment lowers the resource utilization of cluster and introduces more communication between compute nodes. Also an online adjustment to task deployment scheme suffers slow state recovery during task restart. To address these issues, we propose a state lossless scheduling strategy (Sl-Stream) to optimize the task deployment and state recovery process. This paper discusses this strategy from the following aspects: (1) A stream application model and a resource model are constructed, together with the formalization of problems including subgraph partitioning, task deployment and stateful scheduling. (2) A multi-factor topology partitioning method is proposed using a quantum particle swarm algorithm. The assignment between tasks and nodes is optimized using a bipartite graph minimum matching algorithm. (3) A hierarchical local topology migration is performed when an online scheduling is triggered, which ensures the processing sustainability of data streams. (4) A fragment loss-tolerant jerasure tool is used to divide the state data into fragments and periodically save them in upstream vertex instances, which ensures the available fragments be able to reconstruct the whole state in parallel. (5) Metrics including latency, throughput and state recovery time are evaluated in a real distributed stream computing environment. With a comprehensive evaluation of variable-rate input scenarios, the proposed Sl-Stream system provides promising improvements on throughput, latency and state recovery time compared to the existing Storm’s scheduling strategies."
pub.1139463836,Towards autoscaling of Apache Flink jobs,"Abstract Data stream processing has been gaining attention in the past decade. Apache Flink is an open-source distributed stream processing engine that is able to process a large amount of data in real time with low latency. Computations are distributed among a cluster of nodes. Currently, provisioning the appropriate amount of cloud resources must be done manually ahead of time. A dynamically varying workload may exceed the capacity of the cluster, or leave resources underutilized. In our paper, we describe an architecture that enables the automatic scaling of Flink jobs on Kubernetes based on custom metrics, and describe a simple scaling policy. We also measure the e ects of state size and target parallelism on the duration of the scaling operation, which must be considered when designing an autoscaling policy, so that the Flink job respects a Service Level Agreement."
pub.1042603016,JetStream: Enabling high throughput live event streaming on multi-site clouds,"Scientific and commercial applications operate nowadays on tens of cloud datacenters around the globe, following similar patterns: they aggregate monitoring or sensor data, assess the QoS or run global data mining queries based on inter-site event stream processing. Enabling fast data transfers across geographically distributed sites allows such applications to manage the continuous streams of events in real time and quickly react to changes. However, traditional event processing engines often consider data resources as second-class citizens and support access to data only as a side-effect of computation (i.e. they are not concerned by the transfer of events from their source to the processing site). This is an efficient approach as long as the processing is executed in a single cluster where nodes are interconnected by low latency networks. In a distributed environment, consisting of multiple datacenters, with orders of magnitude differences in capabilities and connected by a WAN, this will undoubtedly lead to significant latency and performance variations. This is namely the challenge we address in this paper, by proposing JetStream, a high performance batch-based streaming middleware for efficient transfers of events between cloud datacenters. JetStream is able to self-adapt to the streaming conditions by modeling and monitoring a set of context parameters. It further aggregates the available bandwidth by enabling multi-route streaming across cloud sites, while at the same time optimizing resource utilization and increasing cost efficiency. The prototype was validated on tens of nodes from US and Europe datacenters of the Windows Azure cloud with synthetic benchmarks and a real-life application monitoring the ALICE experiment at CERN. The results show a 3× increase of the transfer rate using the adaptive multi-route streaming, compared to state of the art solutions."
pub.1122597561,REACT: Scalable and High-Performance Regular Expression Pattern Matching Accelerator for In-Storage Processing,"This article proposes REACT, a regular expression matching accelerator, which can be embedded in a modern Solid-State Drive (SSD) and a novel data access scheduling algorithm for high matching throughput. Specifically, REACT, including our data access scheduling algorithm, increases the utilization of SSD and the degree of internal memory parallelism for pattern matching processes. While the low-level flash exhibits long latency, modern SSDs in practice achieve high I/O performance by utilizing the massive internal parallelism at the system-level. However, exploiting the parallelism is limited for pattern matching since the sub-blocks, which constitute an input data and can be placed in multiple flash pages, should be tested in a sequence to process the input correctly. This limitation can induce low utilization of the accelerator. To address this challenge, the proposed REACT simultaneously processes multiple input streams with a parallel processing architecture to maximize matching throughput by hiding the long and irregular latency. The scheduling algorithm finds a data stream which requires a sub-block in closest time and prioritizes the access request to reduce the data stall of REACT. REACT achieves maximum 22.6 percent of matching throughput improvement on a 16-channel high-performance SSD compared to the accelerator without the proposed scheduling algorithm."
pub.1002174513,Continuous analytics on graph data streams using WSO2 complex event processor,"The ACM DEBS Grand Challenge 2016 focuses on analysing the properties of a time evolving social-network graph generated using LDBC (Linked Data Benchmark Council) Social Network Benchmark. In this paper we present how we used WSO2 CEP, an open source, commercially available Complex Event Processing Engine, to solve the problem. On a 4-core/8 GB virtual machine, our solution processed 90,000 events per second with a mean latency of 6 ms for query 1. For query 2 it processed 210,000 events per second with a mean latency of only 0.3 ms. The paper describes the solution we propose, the experiments' results, and presents how we optimized the performance of our solution."
pub.1163826064,Edge Computing in 5G for Mobile AR/VR Data Prediction and Slicing Model,"To reduce computational connectivity issues, AR/VR data necessitates vast computational capabilities, tremendous transmission bandwidth, and ultra-low latency. AR/VR data can process data at the Mobile Edge computation (MEC) reducing the latencies in crucial decisions. Data slicing and edge computing are envisioned as critical enabler technologies for prioritizing data download and upload. Edge computing provides storage and processing resources at the network’s edge. The devices mimic a framework for data prediction as well as a slicing model to slice AR/VR data streaming. AR/VR slicing model requires uploading and downloading streams speed limit, connectivity time, bandwidth, and user pattern as its parameters to predict data slicing model in edge computing to improvise the network utilization. MEC uses ML in 3 ways. (1) ML-based task offloading techniques; (2) ML-based task scheduling methods; and (3) ML-based joint resource allocation methods."
pub.1091763210,Rapid C4I High Performance Computing for Hyperspectral Imaging Exploitation,"A multidisciplinary effort has been initiated which spans the C4I and signal/image processing computational technology areas to integrate diverse capabilities of existing hyperspectral image exploitation systems. The primary objective of the project is to develop and demonstrate support for the rapid, low latency exploitation of hyperspectral information. A flexible and open framework will service exploitation requests from C4I users by tapping into large, dynamic databases of previously processed and raw data to deliver products to the requester with minimal latency. A web-based interface is being developed so that any authorized user with a web browser can input requests. The user can select data sources, exploitation time intervals, and a parallelized exploitation method for execution. To maximize productivity and minimize the decision making cycle of the requestors, algorithm parallelization efforts seek to achieve a minimal latency before initial results begin to stream back to the requestor in typical web prioritized fashion."
pub.1142229575,Functionality-Based Processing-in-Memory Accelerator for Deep Convolutional Neural Networks,"Processing-in-memory (PIM) architectures show the advantage of handling applications that generate complicated memory request patterns; usually, those kinds of memory streams degrade the application’s performance in conventional memory hierarchy systems. In particular, deep convolutional neural networks (DCNNs) processing that consists of several functionalities could be highly optimized if PIM cores can extend the processing capability and data accessibility. In this work, we propose a functionality-based PIM accelerator for DCNNs. We design several modules in addition to the conventional PIM system based on a hybrid memory cube (HMC). First, we compose a new buffer module, namely, a shared cache, in which PIM cores are provided DCNN functionalities and pre-trained weights. The PIM cores subsequently enhance computational utilization and data accessibility. Second, an efficient replacement method complements the shared cache to optimize the data miss rate of DCNN processing. Third, we compose dual prefetchers that can deal with DCNN’s memory access patterns, thereby reducing the system’s overall latency. Fourth, we compose a PIM scheduler for PIM core-level autonomous request control. The PIM scheduler relieves the host processor of significant computational loads, achieving the overall latency of the system and reducing the energy consumption. By the performance evaluation based on the trace-driven HMC simulator, our proposed model improves average latency and bandwidth by 38.9 and 27.9 % with only 18.7 % more energy consumption compared with conventional HMC-based PIM systems. Our system also achieves scalable processing performance because when the DCNN becomes deeper, it processes faster than conventional PIM systems."
pub.1170652047,A Simulation of Energy Optimized Distributed Video Processing on 28 GHz Network,"Transmitting a high data rate video to the cloud for real-time processing purpose requires minimizing the latency, maximizing the application requirements, and optimizing power consumption for the entire system. In this study, we employed a distributed video processing model for an object detection task, assuming that video streams are captured by robots operating in the licensed 28 GHz Milliwave network, en-suring the stability of video uploads. Through the optimization of power consumption, the system efficiently allocated video analysis frames to appropriate devices, resulting in an 18 % decrease in overall power usage."
pub.1036200613,Toward Optimizing Latency Under Throughput Constraints for Application Workflows on Clusters,"In many application domains, it is desirable to meet some user-defined performance requirement while minimizing resource usage and optimizing additional performance parameters. For example, application workflows with real-time constraints may have strict throughput requirements and desire a low latency or response-time. The structure of these workflows can be represented as directed acyclic graphs of coarse-grained application tasks with data dependences. In this paper, we develop a novel mapping and scheduling algorithm that minimizes the latency of workflows that act on a stream of input data, while satisfying throughput requirements. The algorithm employs pipelined parallelism and intelligent clustering and replication of tasks to meet throughput requirements. Latency is minimized by exploiting task parallelism and reducing communication overheads. Evaluation using synthetic benchmarks and application task graphs shows that our algorithm 1) consistently meets throughput requirements even when other existing schemes fail, 2) produces lower-latency schedules, and 3) results in lesser resource usage."
pub.1025149957,Streamflex,"The stream programming paradigm aims to expose coarse-grained parallelism inapplications that must process continuous sequences of events. The appeal ofstream programming comes from its conceptual simplicity. A program is acollection of independent filters which communicate by the means ofuni-directional data channels. This model lends itself naturally toconcurrent and efficient implementations on modern multiprocessors. As theoutput behavior of filters is determined by the state of their inputchannels, stream programs have fewer opportunities for the errors (such asdata races and deadlocks) that plague shared memory concurrent programming. This paper introduces StreamFlex, an extension to Java which marries streams with objects and thus enables to combine, in the same Java virtual machine, stream processing code with traditional object-oriented components. StreamFlex targets high-throughput low-latency applications with stringent quality-of-service requirements. To achieve these goals, it must, at the same time, extend and restrict Java. To allow for program optimization and provide latency guarantees, the StreamFlex compiler restricts Java by imposing a stricter typing discipline on filters. On the other hand, StreamFlex extends the Java virtual machine with real-time capabilities, transactional memory and type-safe region-based allocation. The result is a rich and expressive language that can be implemented efficiently."
pub.1117877467,Parallel Discovery of Trajectory Companions from Heterogeneous Streaming Data,"Trajectory streams consist of large volumes of time- stamped spatial data that are constantly generated from diverse and geographically distributed sources. Discovery of traveling patterns on trajectory streams such as gathering and companies needs to process each record when it arrives and correlates across multiple records near real-time. Thus techniques for handling high-speed trajectory streams should scale on distributed cluster computing. The main issues encapsulate three aspects, namely a data model to represent the continuous trajectory data, the parallelism of a discovery algorithm, and end-to-end performance improvement. In this paper, we propose a parallel discovery method that consists of 1) a model of partitioning trajectories sampled on different time intervals; 2) definition on distance measurements of trajectories; and 3) a parallel discovery algorithm. We develop this method in a stream processing workflow. From parallelization point of view, we investigate system performance, scalability, stability. Our method discovers trajectory gathering patterns with low latency and scales as the size of trajectory data grows."
pub.1128105268,Resource Management and Scheduling in Distributed Stream Processing Systems,"Stream processing is an emerging paradigm to handle data streams upon arrival, powering latency-critical application such as fraud detection, algorithmic trading, and health surveillance. Though there are a variety of Distributed Stream Processing Systems (DSPSs) that facilitate the development of streaming applications, resource management and task scheduling is not automatically handled by the DSPS middleware and requires a laborious process to tune toward specific deployment targets. As the advent of cloud computing has supported renting resources on-demand, it is of great interest to review the research progress of hosting streaming systems in clouds under certain Service Level Agreements (SLA) and cost constraints. In this article, we introduce the hierarchical structure of streaming systems, define the scope of the resource management problem, and present a comprehensive taxonomy in this context covering critical research topics such as resource provisioning, operator parallelisation, and task scheduling. The literature is then reviewed following the taxonomy structure, facilitating a deeper understanding of the research landscape through classification and comparison of existing works. Finally, we discuss the open issues and future research directions toward realising an automatic, SLA-aware resource management framework."
pub.1093478131,Memory arbitration and cache management in stream-based systems,"With the ongoing advancements in VLSI technology the performance of an embedded system is determined to a large extent by the communication of data and instructions. This results in new methods for on- and off-chip communication and caching schemes. In this paper we use an arbitration scheme that exploits the characteristics of continuous 'media' streams while minimizing the latency for random (e.g. CPU) memory accesses to background memory. We also introduce a novel caching scheme for a stream-based multiprocessor architecture, to limit as much as possible the amount of on-chip buffering required to guarantee the throughput of the continuous streams. With these two schemes we can build an architecture for media processing with optimal flexibility at run-time while performance guarantees can be determined at compile-time."
pub.1146314407,Toward optimal operator parallelism for stream processing topology with limited buffers,"Stream processing is an emerging in-memory computing paradigm to handle massive amounts of real-time data. It is vital to have a mechanism to propose proper parallelism for the operators to handle streaming data efficiently. Previous research has mostly focused on parallelism optimization with infinite buffers; however, the topology’s quality of service is severely affected by network buffers. Thus, in this paper, we introduce an extended queueing network to model the relationship between the parallelism and tuple’s average sojourn time with limited buffers. Based on this model, we also propose greedy algorithms to calculate the optimal parallelism for both the minimum latency and maximum throughput with resource constraints. To fairly evaluate the performance of different models, a random parameter generator for the streaming topology is presented. Experiments show that the extended queuing model may properly forecast performance. Compared to the state-of-the-art method, the proposed algorithms reduce the median total sojourn time by 3.74 times and increase the average maximum sustainable throughput by 1.69 times."
pub.1104576381,Adaptive Scheduling Parallel Jobs with Dynamic Batching in Spark Streaming,"Today enterprises have massive stream data that require to be processed in real time due to data explosion in recent years. Spark Streaming as an emerging system is developed to process real time stream data analytics by using micro-batch approach. The unified programming model of Spark Steaming leads to some unique benefits over other traditional streaming systems, such as fast recovery from failures, better load balancing and resource usage. It treats the continuous stream as a series of micro-batches of data and continuously process these micro-batch jobs. However, efficient scheduling of micro-batch jobs to achieve high throughput and low latency is very challenging due to the complex data dependency and dynamism inherent in streaming workloads. In this paper, we propose A-scheduler, an adaptive scheduling approach that dynamically schedules parallel micro-batch jobs in Spark Streaming and automatically adjusts scheduling parameters to improve performance and resource efficiency. Specifically, A-scheduler dynamically schedules multiple jobs concurrently using different policies based on their data dependencies and automatically adjusts the level of job parallelism and resource shares among jobs based on workload properties. Furthermore, we integrate dynamic batching technique with A-Scheduler to further improve the overall performance of the customized Spark Streaming system. It relies on an expert fuzzy control mechanism to dynamically adjust the length of each batch interval in response to time-varying streaming workload and system processing rate. We implemented A-scheduler and evaluated it with a real-time security event processing workload. Our experimental results show that A-scheduler with dynamic batching can reduce end-to-end latency by 38 percent and meanwhile improve workload throughput and energy efficiency by 23 and 15 percent, respectively, compared to the default Spark Streaming scheduler."
pub.1086341554,A 289 MFLOPS single-chip supercomputer,"Reports on a single-chip supercomputer vector processing unit (VPU) which achieves peak performance of 149 MFLOPS for double-precision operation and 289 MFLOPS for single-precision operation with 560 MB/s bus bandwidth at 70 MHz. The VPU chip, fabricated using 0.5- mu m CMOS triple-metal-layer technology, contains about 1.5 million transistors on a 15.75*16.00 mm/sup 2/ die. The VPU uses a single instruction-stream multiple data-stream (SIMD) architecture on a single CMOS chip. The VPU implementation includes multiple vector pipelines operating concurrently, minimum pipeline latency, vectorized conditional branches, and an optimized instruction set for vector operations.<>"
pub.1109817672,On SDN-Enabled Online and Dynamic Bandwidth Allocation for Stream Analytics,"Data communication in cloud-based distributed stream data analytics often involves a collection of parallel and pipelined TCP flows. As the standard TCP congestion control mechanism is designed for achieving “fairness” among competing flows and is agnostic to the application layer contexts, the bandwidth allocation among a set of TCP flows traversing bottleneck links often leads to sub-optimal application-layer performance measures, e.g., stream processing throughput or average tuple complete latency. Motivated by this and enabled by the rapid development of the Software-Defined Networking (SDN) techniques, in this paper, we re-investigate the design space of the bandwidth allocation problem and propose a cross-layer framework which utilizes the additional information obtained from the application layer and provides on-the-fly and dynamic bandwidth adjustment algorithms for helping the stream analytics applications achieving better performance during the runtime. We implement a prototype cross-layer bandwidth allocation framework based on a popular open-source distributed stream processing platform, Apache Storm, together with the OpenDaylight controller, and carry out extensive experiments with real-world analytical workloads on top of a local cluster consisting of 10 workstations interconnected by a SDN-enabled switch. The experiment results clearly validate the effectiveness and efficiency of our proposed framework and algorithms."
pub.1015991543,TimeStream,"TimeStream is a distributed system designed specifically for low-latency continuous processing of big streaming data on a large cluster of commodity machines. The unique characteristics of this emerging application domain have led to a significantly different design from the popular MapReduce-style batch data processing. In particular, we advocate a powerful new abstraction called resilient substitution that caters to the specific needs in this new computation model to handle failure recovery and dynamic reconfiguration in response to load changes. Several real-world applications running on our prototype have been shown to scale robustly with low latency while at the same time maintaining the simple and concise declarative programming model. TimeStream handles an on-line advertising aggregation pipeline at a rate of 700,000 URLs per second with a 2-second delay, while performing sentiment analysis of Twitter data at a peak rate close to 10,000 tweets per second, with approximately 2-second delay."
pub.1174023456,Color-based Lightweight Utility-aware Load Shedding for Real-Time Video Analytics at the Edge,"Real-time video analytics typically require video frames to be processed by a query to identify objects or activities of interest while adhering to an end-to-end frame processing latency constraint. This imposes a continuous and heavy load on backend compute and network infrastructure. Video data has inherent redundancy and does not always contain an object of interest for a given query. We leverage this property of video streams to propose a lightweight Load Shedder that can be deployed on edge servers or on inexpensive edge devices co-located with cameras. The proposed Load Shedder uses pixel-level color-based features to calculate a utility score for each ingress video frame and a minimum utility threshold to select interesting frames to send for query processing. Dropping unnecessary frames enables the video analytics query in the backend to meet the end-to-end latency constraint with fewer compute and network resources. To guarantee a bounded end-to-end latency at runtime, we introduce a control loop that monitors the backend load and dynamically adjusts the utility threshold. Performance evaluations show that the proposed Load Shedder selects a large portion of frames containing each object of interest while meeting the end-to-end frame processing latency constraint. Furthermore, it does not impose a significant latency overhead when running on edge devices with modest compute resources."
pub.1104352180,Efficient Temporal Reasoning on Streams of Events with DOTR,"Many ICT applications need to make sense of large volumes of streaming data to detect situations of interest and enable timely reactions. Stream Reasoning (SR) aims to combine the performance of stream/event processing and the reasoning expressiveness of knowledge representation systems by adopting Semantic Web standards to encode streaming elements. We argue that the mainstream SR model is not flexible enough to properly express the temporal relations common in many applications. We show that the model can miss relevant information and lead to inconsistent derivations. Moving from these premises, we introduce a novel SR model that provides expressive ontological and temporal reasoning by neatly decoupling their scope to avoid losses and inconsistencies. We implement the model in the DOTR system that defines ontological reasoning using Datalog rules and temporal reasoning using a Complex Event Processing language that builds on metric temporal logic. We demonstrate the expressiveness of our model through examples and benchmarks, and we show that DOTR outperforms state-of-the-art SR tools, processing data with millisecond latency."
pub.1061662521,Optimizing Multi-Top-k Queries over Uncertain Data Streams,"Query processing over uncertain data streams, in particular top-κ query processing, has become increasingly important due to its wide application in many fields such as sensor network monitoring and internet traffic control. In many real applications, multiple top-κ queries are registered in the system. Sharing the results of these queries is a key factor in saving the computation cost and providing real-time response. However, due to the complex semantics of uncertain top-κ query processing, it is nontrivial to implement sharing among different top-κ queries and few works have addressed the sharing issue. In this paper, we formulate various types of sharing among multiple top-κ queries over uncertain data streams based on the frequency upper bound of each top-κ query. We present an optimal dynamic programming solution as well as a more efficient (in terms of time and space complexity) greedy algorithm to compute the execution plan of executing queries for saving the computation cost between them. Experiments have demonstrated that the greedy algorithm can find the optimal solution in most cases, and it can almost achieve the same performance (in terms of latency and throughput) as the dynamic programming approach."
pub.1119393674,On SDN-Enabled Online and Dynamic Bandwidth Allocation for Stream Analytics,"Data communication in cloud-based distributed stream data analytics often
involves a collection of parallel and pipelined TCP flows. As the standard TCP
congestion control mechanism is designed for achieving ""fairness"" among
competing flows and is agnostic to the application layer contexts, the
bandwidth allocation among a set of TCP flows traversing bottleneck links often
leads to sub-optimal application-layer performance measures, e.g., stream
processing throughput or average tuple complete latency.
  Motivated by this and enabled by the rapid development of the
Software-Defined Networking (SDN) techniques, in this paper, we re-investigate
the design space of the bandwidth allocation problem and propose a cross-layer
framework which utilizes the additional information obtained from the
application layer and provides on-the-fly and dynamic bandwidth adjustment
algorithms for helping the stream analytics applications achieving better
performance during the runtime.
  We implement a prototype cross-layer bandwidth allocation framework based on
a popular open-source distributed stream processing platform, Apache Storm,
together with the OpenDaylight controller, and carry out extensive experiments
with real-world analytical workloads on top of a local cluster consisting of 10
workstations interconnected by a SDN-enabled switch. The experiment results
clearly validate the effectiveness and efficiency of our proposed framework and
algorithms."
pub.1036866916,Data access performance through parallelization and vectored access. Some results.,"High Energy Physics data processing and analysis applications typically deal with the problem of accessing and processing data at high speed. Recent studies, development and test work have shown that the latencies due to data access can often be hidden by parallelizing them with the data processing, thus giving the ability to have applications which process remote data with a high level of efficiency. Techniques and algorithms able to reach this result have been implemented in the client side of the Scalla/xrootd system, and in this contribution we describe the results of some tests done in order to compare their performance and characteristics. These techniques, if used together with multiple streams data access, can also be effective in allowing to efficiently and transparently deal with data repositories accessible via a Wide Area Network."
pub.1155074969,Object Detection and Classification in FWMAVs for Smart Pollination,"Object Detection and Classification algorithms are necessary for Smart Pollination by FWMAVs(Flapping-Wing Micro Aerial Vehicles). The FWMAVs would need to recognise flowers and navigate towards them. Due to the extremely low limitations on size, power consumption and weight, heavy machine learning models could not be developed to run on small microcontroller chips due to heavy resource consumption and low speed. In this research, we try to overcome this problem by having the FWMAV stream the video in real-time to a server and have the object detection algorithms run online. We implement this in two ways and compare them. In the first one, we directly upload the stream without processing, and in the second, we compress the data before uploading. We compare both methods on the basis of accuracy and performance. We conclude that the former is ideal for crispier data and an environment with lower latency. The latter is advisable for those areas where network has high latency and less data transfer is feasible."
pub.1043017104,Arbitrary streaming permutations with minimum memory and latency,"Streaming architectures are a popular choice for data intensive application due to their high throughput requirements. When assembling components for a streaming application, it is often necessary to build translation blocks between them to match the ordering of the data elements required for the subsequent processing. This paper addresses this need by developing a technique that realizes arbitrary permutations in a streaming architecture. It is parametrized to accommodate any size data sequence and streaming width. This technique is applied to an architecture that receives continuous input at a rate of k elements per clock cycle, and after an initial start-up latency, outputs continuously at the same rate. In addition, the memory usage and latency through the memory array is minimized. This design is evaluated for permutations parametrized by size and stream width in terms of the memory elements and depths required. The class of stride permutation is considered for specific experimental evaluation. On average, this technique and architecture has only half the latency and requires half the memory of other techniques."
pub.1145112693,The delay and window size problems in rule-based stream reasoning,"In recent years, there has been an increasing interest in extending stream processing engines with rule-based temporal reasoning capabilities. To ensure correctness, such systems must be able to output results over the partial data received so far as if the entire (infinite) stream had been available; furthermore, these results must be streamed out as soon as the relevant data is received, thus incurring the minimum possible delay; finally, due to memory limitations, systems can only keep a limited history of previous facts in memory to perform further computations. These requirements pose significant theoretical and practical challenges since temporal rules can derive new information and propagate it both towards past and future time points; as a result, streamed answers can depend on data that has not yet been received, as well as on data that arrived far in the past. Towards developing a solid foundation for practical rule-based stream reasoning, we propose and study in this paper a suite of decision problems that can be exploited by stream reasoning algorithms to tackle the aforementioned challenges, and provide tight complexity bounds for a core temporal extension of Datalog. All of the problems we consider can be solved at design time (under reasonable assumptions), prior to the processing of any data. Solving these problems enables the use of reasoning algorithms that process the input streams incrementally using a sliding window, while at the same time supporting an expressive rule-based knowledge representation language and minimising both latency and memory consumption."
pub.1122707099,Data-Driven Windows to Accelerate Video Stream Content Extraction in Complex Event Processing,"This work presents a data-driven adaptive windowing approach to accelerate video content extraction in DNN-based Complex Event Processing (CEP) systems. The CEP windows continuously monitor low-level content of incoming video frames and exploit interframe correlations to accelerate the overall DNN content extraction process. The two main contributions are: 1) technique to create micro-batches of similar frames within the window by measuring dissimilarities among them, and 2) optimal frame resolution within micro-batches under specified accuracy thresholds for fast model processing. The initial experimental results show that our adaptive micro-batching approach improves 3.75X model throughput execution while maintaining application-level latency bounds under required accuracy constraints."
pub.1020773833,The CHAMP Atmospheric Processing System for Radio Occultation Measurements,"In this paper a description of the CHAMP atmospheric processing system for radio occultation data at GFZ Potsdam is given. The generation of radio occultation products, as e.g. atmospheric excess phases, vertical profiles of refractivity, temperature or water vapour is a complex process. Besides the scientific challenge the design and installation of an automatic data processing system is also of great importance. This system must be able to process the different input data from external data sources, coordinates the different data streams and scientific software modules, and feeds the results into the data centre automatically. Caused by different user demands the CHAMP Atmospheric Processor is divided into two parts: A rapid processing mode makes radio occultation analysis results available on average five hours after measurements. In the standard processing mode quality checked profiles of atmospheric parameters are available with a latency of about two days."
pub.1144950556,A Study on Migration Scheduling in Distributed Stream Processing Engines,"The cost of migrating stateful operators in distributed stream processing has attracted research attention. Reactive migration in response to context changes is the common approach. Other migration scheduling strategies, like proactive migration based on prediction, and delayed migration are nearly neglected. This paper investigates four algorithms that explore these alternative scheduling strategies. The algorithms are implemented in a prototype stream processing overlay and run over an emulated network. Experiments with synthetic workload reveal that (1) proactive migration can reduce average event delivery latency, and (2) that it is important to handle noise in the data to avoid wrong adaptations. Experiments with real workload demonstrate that (1) pro-activity is not always beneficial, and (2) careful timing of migration depending on operator state, has a large potential to limit overhead. The experiments demonstrate a reduction in state size of 38 %, resulting in a 30 % reduction in freeze time. Consideration of operator state size is especially important. The state transfer can lead to contention that further harms event delivery and/or causes network timeouts for cases with limited network resources."
pub.1094082496,The Real-Time Scheduling Strategy Based on Traffic and Load Balancing in Storm,"The era of big data has led to the emergence of new systems for real-time distributed stream processing. Apache Storm is one of the most popular stream processing systems today. However, Storm, as many other stream processing systems, lacks an intelligent scheduling mechanism. The default round-robin scheduling which disregards inter-node traffic and worker nodes load balancing may be inefficient sometimes. This paper proposed a real-time scheduling algorithm based on inter-node traffic and worker nodes load balancing within Storm. Algorithm is divided into two steps: In the first step according to the topology structure and inter-node traffic, executors are assigned to slots to ensure the minimum interaction traffic. The second step, we consider the worker nodes load, to choose the lowest load node for slots assignment. Experiments demonstrate that this scheduling algorithm compared to the default scheduling algorithm, performance of average latency and average inter-node traffic in the system improved above 50%, and compared the traffic-based scheduling algorithm, improved about 10%."
pub.1163882060,Mathematical modeling for further improving task scheduling on Big Data systems,"In the big data era which we have entered, the development of smart scheduler has become a necessity. A Distributed Stream Processing System (DSPS) has the role of assigning processing tasks to the available resources (dynamically or not) and route streaming data between them. Smart and efficient task scheduling can reduce latencies and eliminate network congestions. The most commonly used scheduler is the default Storm scheduler, which has proven to have certain disadvantages, like the inability to handle system changes in a dynamic environment. In such cases, rescheduling is necessary. This paper is an extension of a previous work on dynamic task scheduling. In such a scenario, some type of rescheduling is necessary to have the system working in the most efficient way. In this paper, we extend our previous works Souravlas and Anastasiadou (Appl Sci 10(14):4796, 2020); Souravlas et al. (Appl Sci 11(1):61, 2021) and present a mathematical model that offers better balance and produces fewer communication steps. The scheduler is based on the idea of generating larger sets of communication steps among the system nodes, which we call superclasses. Our experiments have shown that this scheme achieves better balancing and reduces the overall latency."
pub.1095032548,Processing Smart Meter Data Streams in the Cloud,"The ongoing integration of renewable energy sources is likely to increase the fluctuations in the ratio of produced and consumed power. Several types of Demand Response (DR) programs have been proposed to deal with the increasing volatility of power production and consumption. Many of these, such as Real Time Pricing (RTP), require intensive monitoring of the consumers' power consumption. This is one of the reasons why smart meters are currently being deployed by many utilities. Smart meters offer a two-way communication channel between the consumer and the utility, thus extending the power grid by a complex, large scale communication infrastructure. With the growing deployment of smart meters, power utilities face the problem of processing and storing the incoming data to support latency-sensitive applications such as Real-Time Pricing. In this paper we present a set of requirements for a utility-side IT infrastructure to process incoming smart meter data streams. We propose the use of Infrastructure-as-a-Service clouds and frameworks for parallel stream processing in clouds to address these requirements. Based on the Nephele cloud computing framework we demonstrate the practicality of this approach based on experiments with one million simulated smart meters and a prototypical Real-Time Pricing application deployed in our own private cloud."
pub.1061247252,The effectiveness of affinity-based scheduling in multiprocessor network protocol processing (extended version),"Techniques for avoiding the high memory overheads found on many modern shared-memory multiprocessors are of increasing importance in the development of high-performance multiprocessor protocol implementations. One such technique is processor-cache affinity scheduling, which can significantly lower packet latency and substantially increase protocol processing throughput. We evaluate several aspects of the effectiveness of affinity-based scheduling in multiprocessor network protocol processing, under packet-level and connection-level parallelization approaches. Specifically, we evaluate the performance of the scheduling technique (1) when a large number of streams are concurrently supported, (2) when processing includes copying of uncached packet data, (3) as applied to send-side protocol processing, and (4) in the presence of stream burstiness and source locality, two well-known properties of network traffic. We find that affinity-based scheduling performs well under these conditions, emphasizing its robustness and general effectiveness in multiprocessor network processing. In addition, we explore a technique which improves the caching behavior and available packet-level concurrency under connection-level parallelism, and find performance improves dramatically."
pub.1123599449,Performance Assay of Big IoT Data Analytics Framework,"Evaluation of Internet of Things (IoT) technologies in real life has scaled the enumeration of data in huge volumes and that too with high velocity, and thus a new issue has come into picture that is of management & analytics of this BIG IOT STREAM data. In order to optimize the performance of the IoT Machines and services provided by the vendors, industry is giving high priority to analyze this big IoT Stream Data for surviving in the competitive global environment. Thses analysis are done through number of applications using various Data Analytics Framework, which require obtaining the valuable information intelligently from a large amount of real-time produced data. This paper, discusses the challenges and issues faced by distributed stream analytics frameworks at the data processing level and tries to recommend a possible a Scalable Framework to adapt with the volume and velocity of Big IoT Stream Data. Experiments focus on evaluating the performance of three Distributed Stream Analytics Here Analytics frameworks, namely Apache Spark, Splunk and Apache Storm are being evaluated over large steam IoT data on latency & throughput as parameters in respect to concurrency. The outcome of the paper is to find the best possible existing framework and recommend a possible scalable framework."
pub.1120883296,SMPTE ST 2110 Compliant Scalable Architecture on FPGA for End to End Uncompressed Professional Video Transport over IP Networks,"SMTPE ST 2110 is a professional video over IP standard which enables uncompressed video to be transported as separate video, audio and ancillary data streams, allowing true flexibility in media workflows. Uncompressed video transmission allows professional video environments to achieve high quality video distribution while meeting strict latency requirements. Although this standard can be implemented on software, hardware solutions can achieve superior performance, lower latency and lower power consumption while efficiently handling jitter and packet bursts. This paper presents an FPGA based hardware implementation of a SMPTE ST 2110 compliant end to end solution supporting uncompressed video resolutions of up to 4K at 30fps. Due to its scalability, the architecture can be easily extended to support higher video resolutions and/or multiple video streams. Additionally, the interface modularity of the architecture enables integration with any video interface type. In this work, a Serial Digital Interface (SDI) interface is used. The proposed architecture consists of a replicable video processing pipeline for each stream, a Session Description Protocol (SDP) compliant network configuration unit and a SMPTE ST 2059-1/2 compliant time synchronization unit common to all streams, with network access via a 10G UDP/IPv4/Ethernet stack. All modules other than the Ethernet core and SDI interface modules can operate at 200MHz even though 4K 30fps video processing can be achieved with a minimum operating frequency of 100MHz."
pub.1151864027,Multi-View Scheduling of Onboard Live Video Analytics to Minimize Frame Processing Latency,"This paper presents a real-time multi-view scheduling framework for DNN-based live video analytics at the edge to minimize frame processing latency. The work is motivated by applications where a higher frame rate is important, not to miss actions of interest. Examples include defense, border security, and intruder detection applications where sensors (in this paper, cameras) are deployed to monitor key roads, chokepoints, or passageways to identify events of interest (and intervene in real-time). Supporting a higher frame rate entails lowering frame processing latency. We assume that multiple cameras are deployed with partially overlapping views. Each camera has access to limited onboard computing capacity. Many targets cross the field of view of these cameras (but the great majority do not require action). We take advantage of the spatial-temporal correlations among multi-camera video streams to perform target-to-camera assignment such that the maximum frame processing time across cameras is minimized. Specifically, we use a data-driven approach to identify objects seen by multiple cameras, and propose a batch-aware latency-balanced (BALB) scheduling algorithm to drive the object-to-camera assignment. We empirically evaluate the proposed system with a real-world surveillance dataset on a testbed consisting of multiple NVIDIA Jetson boards. The results show that our system substantially improves the video processing speed, attaining multiplicative speedups of 2.45× to 6.85×, and consistently outperforms the competitive static region partitioning strategy."
pub.1093423490,Supporting Distributed Processing of Time-based Media Streams,"There are many challenges in devising solutions for online content processing of live networked multimedia sessions. These include content analysis under uncertainty (evidence of content are missed or hallucinated), the computational complexity of feature extraction and object recognition, and the massive amount of data to be analyzed under real-time requirements. In this paper we focus on middleware supporting on-line media content analysis. Our middleware supports processing, logically organized as a hierarchy of refined events extracted in real time from a set of potentially related time-based media streams. The processing can physically be distributed and redistributed during run time, as a set of interacting components, each performing some content analysis algorithm. The middleware is designed with reuse-ability, scalability, performance, resource management, and fault tolerance in mind by providing support for mechanisms such as, adaptation, reconfiguration, migration, and replication. The goal is to support applications in trading off the reliability and latency of the content analysis against the available computing resources."
pub.1116869799,"Research and Implementation of an Aquaculture Monitoring System Based on Flink, MongoDB and Kafka","With the rapid advancement of intelligent agriculture technology, the application of IoT and sensors in aquaculture domain is becoming more and more widespread. Traditional relational database management systems cannot store the large scale and diversified sensor data flexibly and expansively. Moreover, the sensor stream data usually requires a processing operation with high throughput and low latency. Based on Flink, MongoDB and Kafka, we propose and implement an aquaculture monitoring system. Among them, Flink provides a high throughput, low latency processing platform for sensor data. Kafka, as a distributed publish-subscribe message system, acquires different sensor data and builds reliable pipelines for transmitting real-time data between application programs. MongoDB is suitable for storing diversified sensor data. As a highly reliable and high-performance column database, HBase is often used in sensor data storage schemes. Therefore, using real aquaculture dataset, the execution efficiency of some common operations between HBase and our solution are tested and compared. The experimental results show that the efficiency of our solution is much higher than that of HBase, which provided a feasible solution for the sensor data storage and processing of aquaculture."
pub.1145284544,DARLING,"Complex event processing (CEP) is widely employed to detect user-defined combinations, or patterns, of events in massive streams of incoming data. Numerous applications such as healthcare, fraud detection, and more, use CEP technologies to capture critical alerts, threats, or vital notifications. This requires that the technology meet real-time detection constraints. Multiple optimization techniques have been developed to minimize the processing time for CEP, including parallelization techniques, pattern rewriting, and more. However, these techniques may not suffice or may not be applicable when an unpredictable peak in the input event stream exceeds the system capacity. In such cases, one immediate possible solution is to drop some of the load in a technique known as load shedding. We present a novel load shedding mechanism for real-time complex event processing. Our approach uses statistics that are gathered to detect overload. The solution makes data-driven load shedding decisions to drop the less important events such that we preserve a given latency bound while minimizing the degradation in the quality of results. An extensive experimental evaluation on a broad set of real-life patterns and datasets demonstrates the superiority of our approach over the state-of-the-art techniques."
pub.1100771648,Pushing Intelligence to the Edge with a Stream Processing Architecture,"The cloud computing paradigm underpins the Internet of Things (IoT) by offering a seemingly infinite pool of resources for processing/storing extreme amounts of data generated by complex IoT systems. The cloud has established a convenient and widely adopted approach, where raw data are vertically offloaded to cloud servers from resource-constrained edge devices, which are only seen as simple data generators, not capable of performing more sophisticated processing activities. However, there are more and more emerging scenarios, where the amount of data to be transferred over the network to the cloud is associated with increased network latency, making the results of the computation obsolete. As various categories of edge devices are becoming more and more powerful in terms of hardware resources — specifically, CPU and memory — the established way of off-loading computation to the cloud is not always seen as the most convenient approach. Accordingly, this paper presents a Stream Processing architecture for spreading workload among a local cluster of edge devices to process data in parallel, thus achieving faster execution and response times. The experimental results suggest that such a distributed in-memory approach to data processing at the very edge of a computational network has a potential to address a wide range of IoT-related scenarios."
pub.1120704637,Increased Fault-Tolerance and Real-time Performance Resiliency for Stream Processing Workloads through Redundancy,"Data analytics and telemetry have become paramount to monitoring and maintaining quality-of-service in addition to business analytics. Stream processing—a model where a network of operators receives and processes continuously arriving discrete elements—is well-suited for these needs. Current and previous studies and frameworks have focused on continuity of operations and aggregate performance metrics. However, real-time performance and tail latency are also important. Timing errors caused by either performance or failed communication faults also affect real-time performance more drastically than aggregate metrics. In this paper, we introduce redundancy in the stream data to improve the real-time performance and resiliency to timing errors caused by either performance or failed communication faults. We also address limitations in previous solutions using a fine-grained acknowledgment tracking scheme to both increase the effectiveness for resiliency to performance faults and enable effectiveness for failed communication faults. Our results show that fine-grained acknowledgment schemes can improve the tail and mean latencies by approximately 30%. We also show that these schemes can improve resiliency to performance faults compared to existing work. Our improvements result in 47.4% to 92.9% fewer missed deadlines compared to 17.3% to 50.6% for comparable topologies and redundancy levels in the state of the art. Finally, we show that redundancies of 25% to 100% can reduce the number of data elements that miss their deadline constraints by 0.76% to 14.04% for applications with high fan-out and by 7.45% up to 50% for applications with no fan-out."
pub.1173419315,Mini-batching with Fused Training and Testing for Data Streams Processing on the Edge,"Edge Computing (EC) has emerged as a solution to reduce energy demand and greenhouse gas emissions from digital technologies. EC supports low latency, mobility, and location awareness for delay-sensitive applications by bridging the gap between cloud computing services and end-users. Machine learning (ML) methods have been applied in EC for data classification and information processing. Ensemble learners have often proven to yield high predictive performance on data stream classification problems. Mini-batching is a technique proposed for improving cache reuse in multi-core architectures of bagging ensembles for the classification of online data streams, which benefits application speedup and reduces energy consumption. However, the original mini-batching presents limited benefits in terms of cache reuse and it hinders the accuracy of the ensembles (i.e., their capacity to detect behavior changes in data streams). In this paper, we improve mini-batching by fusing continuous training and test loops for the classification of data streams. We evaluated the new strategy by comparing its performance and energy efficiency with the original mini-batching for data stream classification using six ensemble algorithms and four benchmark datasets. We also compare mini-batching strategies with two hardware-based strategies supported by commodity multi-core processors commonly used in EC. Results show that mini-batching strategies can significantly reduce energy consumption in 95% of the experiments. Mini-batching improved energy efficiency by 96% on average and 169% in the best case. Likewise, our new mini-batching strategy improved energy efficiency by 136% on average and 456% in the best case. These strategies also support better control of the balance between performance, energy efficiency, and accuracy."
pub.1103465772,Low-Cost Sorting Network Circuits Using Unary Processing,"Sorting is a common task in a wide range of applications from signal and image processing to switching systems. For applications that require high performance, sorting is often performed in hardware with application-specified integrated circuits or field-programmable gate arrays. Hardware cost and power consumption are the dominant concerns. The usual approach is to wire up a network of compare-and-swap units in a configuration called the Batcher (or bitonic) network. Such networks can readily be pipelined. This paper proposes a novel area-efficient and power-efficient approach to sorting networks, based on “unary processing.” In unary processing, numbers are encoded uniformly by a sequence of one value (say 1) followed by a sequence of the other value (say 0) in a stream of 0’s and 1’s with the value defined by the fraction of 1’s in the stream. Synthesis results of complete sorting networks show up to 92% area and power saving compared to the conventional binary implementations. However, the latency increases. To mitigate the increased latency, this paper uses a novel time-encoding of data. The approach is validated with two implementations of an important application of sorting: median filtering. The result is a low cost, energy-efficient implementation of median filtering with only a slight accuracy loss, compared to conventional implementations."
pub.1023310920,BFSiena,"StreamMine is a scalable middleware for massive real-time data streaming. In this paper we present the BF Siena: a communication substrate for the StreamMine. BFSiena is a content-based, publish/subscribe communication system which provides support for arbitrary predicate based messaging in the acyclic peer to peer networks. BF Siena is a low latency, high throughput communication system which is well suited for the processing frameworks, like Stream-Mine."
pub.1094818616,Multi-Representation Based Data Processing Architecture for IoT Applications,"Internet of Things (IoT) applications like smart cars, smart cities and wearables are becoming widespread and are the future of the Internet. One of the major challenges for IoT applications is efficiently processing, storing and analyzing the continuous stream of incoming data from a large number of connected sensors. We propose a multi-representation based data processing architecture for IoT applications. The data is stored in multiple representations, like rows, columns, graphs which provides support for diverse application demands. A unifying update mechanism based on deterministic scheduling is used to update the data representations, which completely removes the need for data transfer pipelines like ETL (Extract, Transform and Load). The combination of multiple representations, and the deterministic update mechanism, provides the ability to support real-time analytics and caters to IoT applications by minimizing the latency of operations like computing pre-defined aggregates."
pub.1017285410,ReStream,"Real-time predictive applications can demand continuous and agile development, with new models constantly being trained, tested, and then deployed. Training and testing are done by replaying stored event logs, running new models in the context of historical data in a form of backtesting or ""what if?"" analysis. To replay weeks or months of logs while developers wait, we need systems that can stream event logs through prediction logic many times faster than the real-time rate. A challenge with high-speed replay is preserving sequential semantics while harnessing parallel processing power. The crux of the problem lies with causal dependencies inherent in the sequential semantics of log replay. We introduce an execution engine that produces serial-equivalent output while accelerating throughput with pipelining and distributed parallelism. This is made possible by optimizing for high throughput rather than the traditional stream processing goal of low latency, and by aggressive sharing of versioned state, a technique we term Multi-Versioned Parallel Streaming (MVPS). In experiments we see that this engine, which we call ReStream, performs as well as batch processing and more than an order of magnitude better than a single-threaded implementation."
pub.1135691273,Edge-Stream: a Stream Processing Approach for Distributed Applications on a Hierarchical Edge-computing System,"With the rapid growth of IoT devices, the traditional cloud computing scheme is inefficient for many IoT based applications, mainly due to network data flood, long latency, and privacy issues. To this end, the edge computing scheme is proposed to mitigate these problems. However, in an edge computing system, the application development becomes more complicated as it involves increasing levels of edge nodes. Although some efforts have been introduced, existing edge computing frameworks still have some limitations in various application scenarios. To overcome these limitations, we propose a new programming model called Edge-Stream. It is a simple and programmer-friendly model, which can cover typical scenarios in edge-computing. Besides, we address several new issues, such as data sharing and area awareness, in this model. We also implement a prototype of edge-computing framework based on the Edge-Stream model. A comprehensive evaluation is provided based on the prototype. Experimental results demonstrate the effectiveness of the model."
pub.1013817282,Event stream processing for improved situational awareness in the smart grid,"Deployment of Phasor Measurement Units (PMU) in the United States transmission grid has brought a new data stream to be processed and an opportunity to improve situational awareness on the grid. This new data stream offers opportunity for a faster detection and response algorithm to minimize wide spread outages. High rate of data collection of PMU systems has also brought a challenge on how to extract information from fast moving PMU data stream in real time to improve situational awareness inside a control room. Despite the fact that mathematical and probabilistic methods are the most accurate methods of stability analysis, online decision making algorithms cannot afford the latency brought by those methods. Traditional batch processing Artificial Intelligence (AI) techniques have been extensively studied as potential replacements for these approaches, however conventional AI techniques do not deal with continuous streams of fast moving phasor data. This paper presented a novel application of the stream mining algorithms for synchrophasor data to meet quick decision making requirement of future situational awareness applications in power systems. To prove that the proposed methods are efficient and capable of handling huge amounts of data with reasonable accuracy and within limited resources of memory and computational power, four different experiments with different conditions (changing/unchanging the load conditions of Real Power and Reactive Power, fixing the size of memory, and comparing the performance of non-adaptive Hoeffding tree with traditional decision tree algorithms) were conducted. The algorithms discussed in this paper support decisions inside the control rooms helping stakeholders make informed decisions to improve reliability of the future smart grid."
pub.1130372697,Towards on-node Machine Learning for Ultra-low-power Sensors Using Asynchronous Σ Δ Streams," We propose a novel architecture to enable low-power, complex on-node data processing, for the next generation of sensors for the internet of things (IoT), smartdust, or edge intelligence. Our architecture combines near-analog-memory-computing (NAM) and asynchronous-computing-with-streams (ACS), eliminating the need for ADCs. ACS enables ultra-low power, massive computational resources required to execute on-node complex Machine Learning (ML) algorithms; while NAM addresses the memory-wall that represents a common bottleneck for ML and other complex functions. In ACS an analog value is mapped to an asynchronous stream that can take one of two logic levels ( v  h  , v  l  ). This stream-based data representation enables area/power-efficient computing units such as a multiplier implemented as an AND gate yielding savings in power of ∼90% compared to digital approaches. The generation of streams for NAM and ACS in a brute force manner, using analog-to-digital-converters (ADCs) and digital-to-streams-converters, would sky-rocket the power-latency-energy cost making the approach impractical. Our NAM-ACS architecture eliminates expensive conversions, enabling an end-to-end processing on asynchronous streams data-path. We tailor the NAM-ACS architecture for random forest (RaF), an ML algorithm, chosen for its ability to classify using a reduced number of features. Simulations show that our NAM-ACS architecture enables 75% of savings in power compared with a single ADC, obtaining a classification accuracy of 85% using an RaF-inspired algorithm. "
pub.1053505022,The Operational Processing System for GPS Radio Occultation Data from CHAMP and GRACE,"In this chapter, a description of the CHAMP and GRACE atmospheric processing system for radio occultation data at GFZ Potsdam is given. The generation of radio occultation products, as e.g. atmospheric excess phases and vertical profiles of refractivity or temperature is a complex process. Besides the scientific challenge the design and installation of an automatic data processing system is also of great importance. This system must be able to process the different input data from external data sources, coordinate the different data streams and scientific software modules, and feeds the results into the data center automatically. Caused by different user demands the CHAMP and GRACE Atmospheric Processor is divided into two parts: A near-real time processing mode makes radio occultation analysis results available on average 2 h after measurements. In the standard processing mode quality checked profiles of atmospheric parameters are available with a latency of about 2 days."
pub.1135875695,GstLAL: A software framework for gravitational wave discovery,"The GstLAL library, derived from Gstreamer and the LIGO Algorithm Library, supports a stream-based approach to gravitational-wave data processing. Although GstLAL was primarily designed to search for gravitational-wave signatures of merging black holes and neutron stars, it has also contributed to other gravitational-wave searches, data calibration, and detector-characterization efforts. GstLAL has played an integral role in all of the LIGO-Virgo collaboration detections, and its low-latency configuration has enabled rapid electromagnetic follow-up for dozens of compact binary candidates."
pub.1162252874,Fog assisted Visitor Identification Framework with improved Latency and Network Usage,"Visitor identification is one of the vital problems of society. The visitor identification process consists of an intelligent security camera that monitors the entrance of the door of a residence, taking pictures of the human when they enter, automated techniques extract the human face from the image and identify them with the help of a database in real-time. However, there is no such existing research that can deal with visitor identification using fog computing. In this paper, a visitor identification framework is proposed that is entirely deployed in the fog computing environment. The proposed prototype is deployed in the Java-based iFogSim simulator. Each simulation uses a different number of cameras for video stream generations. The human object part is trimmed and selected from the raw data for processing. The trimmed data is transferred to the fog node instead of the cloud for processing. This paper deals with how the generated data pass through each module and takes how much amount of time to process the data in distinct modules. Generally, the fog-based framework reduces latency, network usage, and energy consumption. The research finds an improvement in overall latency and network usage of fog computing environments over the cloud environment."
pub.1029899339,SenQ: An Embedded Query System for Streaming Data in Heterogeneous Interactive Wireless Sensor Networks,"Interactive wireless sensor networks (IWSNs) manifest diverse application architectures, hardware capabilities, and user interactions that challenge existing centralized [1], or VM-based [2] query system designs. To support in-network processing of streaming sensor data in such heterogeneous environments, we created SenQ, a multi-layer embedded query system. SenQ enables user-driven and peer-to-peer in-network query issue by wearable interfaces and other resource-constrained devices. Complex virtual sensors and user-created streams can be dynamically discovered and shared, and SenQ is extensible to new sensors and processing algorithms. We evaluated SenQ’s efficiency and performance in a testbed for assisted-living, and show that on-demand buffering, query caching, efficient restart and other optimizations reduce network overhead and minimize data latency."
pub.1111323415,Live Traffic Data Analysis Using Stream Processing,"The increasing digitalization in the traffic infrastructure offers a great potential to optimize traffic flows, to save costs, and to improve the CO2 balance. Achieving this requires the use of scalable, high-performance software environments that process live traffic data with minimal latency. However, there are no standard solutions that work out of the box. Instead, technology stacks of complex components must be assembled, configured, and deployed from a large and heterogeneous set of available building blocks. Since there are no guidelines on which particular software to use in which configuration for specific use cases, it is extremely difficult to build such complex architectures from scratch. Nevertheless, many application areas, including traffic data analysis, have domain-specific requirements, which makes it possible to close these gaps on the basis of further research. Following this idea, we analyze how typical applications of traffic data analysis can be implemented using stream processing technologies in order to find reusable solutions that can be used as blueprints for the design of applications with similar requirements. Therefore, a number of typical use cases will be analyzed, implemented and benchmarked on the basis of various stream processing architectures. This way, specific levers are to be found to systematically increase the performance. Our first results show significant performance differences between different software solutions and architectures."
pub.1131672920,GstLAL: A software framework for gravitational wave discovery,"The GstLAL library, derived from Gstreamer and the LIGO Algorithm Library,
supports a stream-based approach to gravitational-wave data processing.
Although GstLAL was primarily designed to search for gravitational-wave
signatures of merging black holes and neutron stars, it has also contributed to
other gravitational-wave searches, data calibration, and
detector-characterization efforts. GstLAL has played an integral role in all of
the LIGO-Virgo collaboration detections, and its low-latency configuration has
enabled rapid electromagnetic follow-up for dozens of compact binary
candidates."
pub.1096950365,Complex Event Processing Implementation for Intelligent-Field,"Abstract One of the main challenges for Intelligent Field engineers is handling large amount of data stream in real-time. Finding meaningful patterns among this huge amount of data is just like trying to find a needle in a haystack. Every day reservoir and production engineers are being bombarded with streams of massive amounts of real-time data coming from all kinds of Intelligent Field equipment’s including but not limited to Permanent Downhole Monitoring Systems (PDHMS), Multiphase Flowmeters (MPFM), MicroMotion Meters, Wellhead Gauges, Smart Well Completion (SWC), and Electrical Submersible Pumps (ESP). They spend significant amount of time and effort looking at the trends and analyzing the data to find anomalies. Moreover, the existing systems for data cleansing and summarization are based on batch processing, hence, engineers cannot make the right decision on time and they do not have the mechanism to instantly detect interesting patterns in incoming data. The objective of this paper is to share Saudi ARAMCO’s experience with Complex Event Processing (CEP) as an emerging technology that is design for low-latency and high-throughput event processing for data stream. This paper addresses the architecture, the implementation, and the benefits of CEP as a solution for the Intelligent Field. The implementation will covers three common applications of CEP namely real-time data cleansing, pattern detection, and event driven computation. Data cleansing covers handling out-of-bound, negative, and frozen values. Patten detection enables the detection of anomalous behavior in the data stream, such as unusual buildup or drawdown in well pressure in real-time. Event driven computation is triggered by an event in the field, such as a change in downhole pressure to perform advance calculation logic such as average reservoir pressure. Implementing CEP will help reservoir and production engineers obtain clean data in real time and receive notification in case of any significant event detected."
pub.1093764203,Meteor Shower: A Reliable Stream Processing System for Commodity Data Centers,"Large-scale failures are commonplace in commodity data centers, the major platforms for Distributed Stream Processing Systems (DSPSs). Yet, most DSPSs can only handle single-node failures. Here, we propose Meteor Shower, a new fault-tolerant DSPS that overcomes large-scale burst failures while improving overall performance. Meteor Shower is based on checkpoints. Unlike previous schemes, Meteor Shower orchestrates operators' checkpointing activities through tokens. The tokens originate from source operators, trickle down the stream graph, triggering each operator that receives these tokens to checkpoint its own state1The sprinkling of tokens resembles a meteor shower.. Meteor Shower is a suite of three new techniques: 1) source preservation, 2) parallel, asynchronous checkpointing, and 3) application-aware checkpointing. Source preservation allows Meteor Shower to avoid the overhead of redundant tuple saving in prior schemes; parallel, asynchronous checkpointing enables Meter Shower operators to continue processing streams during a checkpoint; while application-aware checkpointing lets Meteor Shower learn the changing pattern of operators' state size and initiate checkpoints only when the state size is minimal. All three techniques together enable Meteor Shower to improve throughput by 226% and lower latency by 57% vs prior state-of-the-art. Our results were measured on a prototype implementation running three real world applications in the Amazon EC2 Cloud. The sprinkling of tokens resembles a meteor shower."
pub.1132270324,Towards Streaming Perception,"Embodied perception refers to the ability of an autonomous agent to perceive its environment so that it can (re)act. The responsiveness of the agent is largely governed by latency of its processing pipeline. While past work has studied the algorithmic trade-off between latency and accuracy, there has not been a clear metric to compare different methods along the Pareto optimal latency-accuracy curve. We point out a discrepancy between standard offline evaluation and real-time applications: by the time an algorithm finishes processing a particular image frame, the surrounding world has changed. To these ends, we present an approach that coherently integrates latency and accuracy into a single metric for real-time online perception, which we refer to as “streaming accuracy”. The key insight behind this metric is to jointly evaluate the output of the entire perception stack at every time instant, forcing the stack to consider the amount of streaming data that should be ignored while computation is occurring. More broadly, building upon this metric, we introduce a meta-benchmark that systematically converts any image understanding task into a streaming perception task. We focus on the illustrative tasks of object detection and instance segmentation in urban video streams, and contribute a novel dataset with high-quality and temporally-dense annotations. Our proposed solutions and their empirical analysis demonstrate a number of surprising conclusions: (1) there exists an optimal “sweet spot” that maximizes streaming accuracy along the Pareto optimal latency-accuracy curve, (2) asynchronous tracking and future forecasting naturally emerge as internal representations that enable streaming image understanding, and (3) dynamic scheduling can be used to overcome temporal aliasing, yielding the paradoxical result that latency is sometimes minimized by sitting idle and “doing nothing”."
pub.1147466200,Optimizing Camera Stream Transport in Cloud-Based Industrial Robotic Systems,"Combining visual-guided robotics with cloud networking brought a new era into industrial robotic research and development. New challenges have to be tackled with a focus on providing proper communication and data processing setup: sensor data processing as well as the control software should be decoupled from the local robot hardware and should move into the cloud. In the emerging field of cloud robotics, there are trade-offs that have to be handled. More and more sensors such as cameras are being integrated but it comes with a cost. All sensory data have to be sent through often limited networking resources, while latency must be kept as low as possible. In this paper we propose a general solution for efficient camera stream transportation in cloud robotic systems. After introducing our test scenario with the used hardware and software elements, a detailed overview of the architecture is presented with describing each task of the components. The goal of this paper is to examine the current stream transportation implementations in ROS environment and implement a more efficient method. The performance of the proposed method is investigated and compared with other solutions evidenced by measurements."
pub.1168307433,Truly Scalable Real-time Coincidence Processor based on Serial and Parallel Hybrid Architecture,"Many PET systems use coincidence processors based on a serial architecture for coincidence detection, but this single-threaded coincidence detection has high latency and may even lose events in the case of a high event rate. The parallel coincidence processor has high event processing efficiency. However, the increase in detectors leads to a rapid expansion in the usage of hardware resources. In this study, we investigate a coincidence processor based on a serial and parallel hybrid architecture. The proposed processor first groups and merges data streams to reduce the number of them then uses serial processing to detect coincidence events within the merged data streams. After that, the processor uses a homogeneously distributed coincidence detection network to achieve parallel coincidence detection between different streams. The hybrid architecture of the coincidence processor saves hardware resource usage when the number of input data streams is high. When the coincidence processor has 10 input channels in the second stage of coincidence detection, it can achieve a maximum data transfer rate of 400M events/s and has a coincidence detection efficiency larger than 99.97%. The hybrid architecture of the coincidence processor presented in this paper enables real-time coincidence detection for systems with high event rates such as the long-axis PET while saving lots of hardware resources."
pub.1007745088,CellJoin: a parallel stream join operator for the cell processor,"Low-latency and high-throughput processing are key requirements of data stream management systems (DSMSs). Hence, multi-core processors that provide high aggregate processing capacity are ideal matches for executing costly DSMS operators. The recently developed Cell processor is a good example of a heterogeneous multi-core architecture and provides a powerful platform for executing data stream operators with high-performance. On the down side, exploiting the full potential of a multi-core processor like Cell is often challenging, mainly due to the heterogeneous nature of the processing elements, the software managed local memory at the co-processor side, and the unconventional programming model in general. In this paper, we study the problem of scalable execution of windowed stream join operators on multi-core processors, and specifically on the Cell processor. By examining various aspects of join execution flow, we determine the right set of techniques to apply in order to minimize the sequential segments and maximize parallelism. Concretely, we show that basic windows coupled with low-overhead pointer-shifting techniques can be used to achieve efficient join window partitioning, column-oriented join window organization can be used to minimize scattered data transfers, delay-optimized double buffering can be used for effective pipelining, rate-aware batching can be used to balance join throughput and tuple delay, and finally single-instruction multiple-data (SIMD) optimized operator code can be used to exploit data parallelism. Our experimental results show that, following the design guidelines and implementation techniques outlined in this paper, windowed stream joins can achieve high scalability (linear in the number of co-processors) by making efficient use of the extensive hardware parallelism provided by the Cell processor (reaching data processing rates of ≈13 GB/s) and significantly surpass the performance obtained form conventional high-end processors (supporting a combined input stream rate of 2,000 tuples/s using 15 min windows and without dropping any tuples, resulting in ≈8.3 times higher output rate compared to an SSE implementation on dual 3.2 GHz Intel Xeon)."
pub.1145748687,"AutoFlow: Hotspot-Aware, Dynamic Load Balancing for Distributed Stream Processing","Stream applications are widely deployed on the cloud. While modern distributed streaming systems like Flink and Spark Streaming can schedule and execute them efficiently, streaming dataflows are often dynamically changing, which may cause computation imbalance and backpressure.We introduce AutoFlow, an automatic, hotspot-aware dynamic load balance system for streaming dataflows. It incorporates a centralized scheduler that monitors the load balance in the entire dataflow dynamically and implements state migrations correspondingly. The scheduler achieves these two tasks using a simple asynchronous distributed control message mechanism and a hotspot-diminishing algorithm. The timing mechanism supports implicit barriers and a highly efficient state-migration without global barriers or pauses to operators. It also supports a time-window based load-balance measurement and feeds them to the hotspot-diminishing algorithm without user interference. We implemented AutoFlow on top of Ray, an actor-based distributed execution framework. Our evaluation based on various streaming benchmark datasets shows that AutoFlow achieves good load-balance and incurs a low latency overhead in a highly data-skew workload."
pub.1125635493,Real-time Stream Data Processing at Scale,"A typical scenario in a stream data-flow processing engine is that users submit continues queries in order to receive the computational result once a new stream of data arrives. The focus of the paper is to design a dynamic CPU cap controller for stream data-flow applications with real-time constraints, in which the result of computations must be available within a short time period, specified by the user, once a recent update in the input data occurs. It is common that the stream data-flow processing engine is deployed over a cluster of dedicated or virtualized server nodes, e.g., Cloud or Edge platform, to achieve a faster data processing. However, the attributes of incoming stream data-flow might fluctuate in an irregular way. To effectively cope with such unpredictable conditions, the underlying resource manager needs to be equipped with a dynamic resource provisioning mechanism to ensure the real-time requirements of different applications. The proposed solution uses control theory principals to achieve a good utilization of computing resources and a reduced average response time. The proposed algorithm dynamically adjusts the required quality of service (QoS) in an environment when multiple stream & data-flow processing applications concurrently run with unknown and volatile workloads. Our study confirms that such a unpredictable demand can negatively degrade the system performance, mainly due to adverse interference in the utilization of shared resources. Unlike prior research studies which assumes a static or zero correlation among the performance variability among consolidated applications, we presume the prevalence of shared-resource interference among collocated applications as a key performance-limiting parameter and confront it in scenarios where several applications have different QoS requirements with unpredictable workload demands. We design a low-overhead controller to achieve two natural optimization objectives of minimizing QoS violation amount and maximizing the average CPU utilization. The algorithm takes advantage of design principals in model predictive control theory for elastic allocation of CPU share. The experimental results confirm that there is a strong correlation in performance degradation among consolidation strategies and the system utilization for obtaining the capacity of shared resources in a non-cooperative manner. The results confirm that the proposed solution can reduce the average latency of delay-sensitive applications by 17% comparing to the results of a well established heuristic called Class-Based Weighted Fair Queuing (CFWFQ). At the same time, the proposed solution can prevent the QoS violation incidents by 62%."
pub.1154093680,An industrial case study for performance evaluation of hardware-in-the-loop simulators with a combination of network calculus and discrete-event simulation,"This paper describes a novel application of the methods of network calculus and discrete-event simulation to evaluate the performance of a hardware-in-the-loop system. Test benches are distributed computer systems including software, hardware, and networking devices. They are used for validation purposes of complex technical systems like autonomous driving systems. For this purpose, strict real-time guarantees and high data integrity is needed to continuously stream data to the device under test. To guarantee strict real-time between the simulator and the device under test a well-known technique from streaming systems is used, the playback buffer. The playback buffer and the minimum needed pre-buffer time are dimensioned with the help of adapted and mathematically proven network calculus solutions for streaming devices. Linear network calculus elements are generated based on measurements for software processing latencies and network latency. In the next step, a profound playback buffer and pre-buffer time are dimensioned by applying network calculus to the measurements. These calculations are validated with a discrete-event simulation model with trace-driven and with distributions of the processing latencies. The simulation model uses a variation of the workload to prove the system at the bottleneck. This helps in designing a robust system that can properly handle outliers and anomalies in the processing latencies."
pub.1159947767,A Streaming Data Processing Architecture Based on Lookup Tables,"Processing in memory (PIM) is a new computing paradigm that stores the function values of some input modes in a lookup table (LUT) and retrieves their values when similar input modes are encountered (instead of performing online calculations), which is an effective way to save energy. In the era of the Internet of Things, the processing of massive data generated by the front-end requires low-power and real-time processing. This paper investigates an energy-efficient processing architecture based on table lookup in phase-change memory (PCM). This architecture replaces logical-based calculations with LUT lookups to minimize power consumption and operation latency. In order to improve the efficiency of table lookup, the RISC-V instruction set has included extended lookup and data stream transmission instructions. Finally, the system architecture is validated by hardware simulation, and the performance of computing the fast Fourier transform (FFT) application is evaluated. The proposed architecture effectively improves the execution efficiency and reduces the power consumption of data flow operations."
pub.1046915644,Belief–logic conflict resolution in syllogistic reasoning: Inspection-time evidence for a parallel-process model,"An experiment is reported examining dual-process models of belief bias in syllogistic reasoning using a problem complexity manipulation and an inspection-time method to monitor processing latencies for premises and conclusions. Endorsement rates indicated increased belief bias on complex problems, a finding that runs counter to the “belief-first” selective scrutiny model, but which is consistent with other theories, including “reasoning-first” and “parallel-process” models. Inspection-time data revealed a number of effects that, again, arbitrated against the selective scrutiny model. The most striking inspection-time result was an interaction between logic and belief on premise-processing times, whereby belief – logic conflict problems promoted increased latencies relative to non-conflict problems. This finding challenges belief-first and reasoning-first models, but is directly predicted by parallel-process models, which assume that the outputs of simultaneous heuristic and analytic processing streams lead to an awareness of belief – logic conflicts than then require time-consuming resolution."
pub.1145733305,STREAM: Towards READ-based In-Memory Computing for Streaming based Data Processing,"Processing in-memory breaks von-Neumann based design principles to accelerate data-intensive applications. While analog in-memory computing is extremely energy-efficient, the low precision narrows the spectrum of viable applications. In contrast, digital in-memory computing has deterministic precision and can therefore be used to accelerate a broad range of high assurance applications. Unfortunately, the state-of-the-art digital in-memory computing paradigms rely on repeatedly switching the non-volatile memory devices using expensive WRITE operations. In this paper, we propose a framework called STREAM that performs READ-based in-memory computing for streaming-based data processing. The framework consists of a synthesis tool that decomposes high-level programs into in-memory compute kernels that are executed using non-volatile memory. The paper presents hardware/software co-design techniques to minimize the data movement between different nanoscale crossbars within the platform. The framework is evaluated using circuits from ISCAS85 benchmark suite and Suite-Sparse applications to scientific computing. Compared with WRITE-based in-memory computing, the READ-based in-memory computing improves latency and power consumption up to 139X and 14X, respectively."
pub.1137410962,Deadline-Aware Offloading for High-Throughput Accelerators,"Contemporary GPUs are widely used for throughput-oriented data-parallel workloads and increasingly are being considered for latency-sensitive applications in datacenters. Examples include recurrent neural network (RNN) inference, network packet processing, and intelligent personal assistants. These data parallel applications have both high throughput demands and real-time deadlines (40μs-7ms). Moreover, the kernels in these applications have relatively few threads that do not fully utilize the device unless a large batch size is used. However, batching forces jobs to wait, which increases their latency, especially when realistic job arrival times are considered. Previously, programmers have managed the tradeoffs associated with concurrent, latency-sensitive jobs by using a combination of GPU streams and advanced scheduling algorithms running on the CPU host. Although GPU streams allow the accelerator to execute multiple jobs concurrently, prior state-of-the-art solutions use the relatively distant CPU host to prioritize the latency-sensitive GPU tasks. Thus, these approaches are forced to operate at a coarse granularity and cannot quickly adapt to rapidly changing program behavior. We observe that fine-grain, device-integrated kernel schedulers efficiently meet the deadlines of concurrent, latency-sensitive GPU jobs. To overcome the limitations of software-only, CPU-side approaches, we extend the GPU queue scheduler to manage real-time deadlines. We propose a novel laxity-aware scheduler (LAX) that uses information collected within the GPU to dynamically vary job priority based on how much laxity jobs have before their deadline. Compared to contemporary GPUs, 3 state-of-the-art CPU-side schedulers and 6 other advanced GPU-side schedulers, LAX meets the deadlines of 1.7X – 5.0X more jobs and provides better energy-efficiency, throughput, and 99-percentile tail latency."
pub.1095361097,The effectiveness of affinity-based scheduling in multiprocessor networking,"Techniques for avoiding the high memory overheads found on many modern shared-memory multiprocessors are of increasing importance in the development of high-performance multiprocessor protocol implementations. One such technique is processor-cache affinity scheduling, which can significantly lower packet latency and substantially increase protocol processing throughput. In this paper, we evaluate several aspects of the effectiveness of affinity-based scheduling in multiprocessor network protocol processing, under packet-level and connection-level parallelization approaches. Specifically, we evaluate the performance of the scheduling technique (1) when a large number of streams are concurrently supported, (2) when processing includes copying of uncached packet data, (3) as applied to send-side protocol processing, and (4) in the presence of stream burstiness and source locality, two well-known properties of network traffic. We find that affinity-based scheduling performs well under these conditions, emphasizing its robustness and general effectiveness in multiprocessor network processing. In addition, we explore a technique which improves the caching behavior and available packet-level concurrency under connection-level parallelism, and find performance improves dramatically."
pub.1027910890,Interaction Distribution Network,"Content Distribution Network (CDN) has been effective in accelerating the access and growth for web content such as web pages and streaming audio and video. However, the relatively static and bulky data CDNs are designed to serve makes them unsuitable to support latency-sensitive interactive streams such as network games or real-time conferencing. In this position paper, we describe the concepts for an Interaction Distribution Network (IDN), which supports small, interactive data streams in a scalable manner.An IDN shares certain concepts similar to a CDN in that end-user clients connect to the closest serving proxies for accessing and updating interaction data packets. However, the key differences lies in the bi-directional nature of the interaction streams, and that the data streams may belong to a certain “interaction group,” within which some additional processing on the data are possible. An IDN may support existing instant messenger (IM) services and Massively Multiplayer Online Games (MMOGs), while enabling new usage scenarios. We discuss the key challenges, potential solutions, and implications of IDNs in this paper."
pub.1182069141,EON-1: A Brain-Inspired Processor for Near-Sensor Extreme Edge Online Feature Extraction,"For Edge AI applications, deploying online learning and adaptation on resource-constrained embedded devices can deal with low-latency sensor-generated data streams in changing environments. However, since maintaining low-latency and power-efficient inference is paramount at the Edge, online learning and adaptation on the device should impose minimal additional overhead for inference. With this goal in mind, we explore energy-efficient learning and adaptation on-device for streaming-data Edge AI applications using Spiking Neural Networks (SNNs), which follow the principles of brain-inspired computing, such as high-parallelism, neuron co-located memory and compute, and event-driven processing. We propose EON-1, a brain-inspired processor for near-sensor extreme-edge online feature extraction that integrates a fast online learning and adaptation algorithm. We report results of only 1% energy overhead for learning, by far the lowest overhead when compared to other SoTA solutions, while attaining comparable inference accuracy. Furthermore, we demonstrate that EON-1 is up for the challenge of low-latency processing of HD and UHD streaming video in real-time, with learning enabled."
pub.1093535326,DynTARM: An In-Memory Data Structure for Targeted Strong and Rare Association Rule Mining over Time-Varying Domains,"Recently, with companies and government agencies saving large repositories of time stream/temporal data, there is a large push for adapting association rule mining algorithms for dynamic, targeted querying. In addition, issues with data processing latency and results depreciating in value with the passage of time, create a need for swifter and more efficient processing. The aim of targeted association mining is to find potentially interesting implications in large repositories of data. Using targeted association mining techniques, specific implications that contain items of user interest can be found faster and before the implications have depreciated in value beyond usefulness. In this paper, the DynTARM algorithm is proposed for the discovery of targeted and rare association rules. DynTARM has the flexibility to discover strong and rare association rules from data streams within the user's sphere of interest. By introducing a measure, called the Volatility Index, to assess the fluctuation in the confidence of rules, rules conforming to different temporal patterns are discovered."
pub.1021422418,A policy-based coordination architecture for distributed complex event processing in the internet of things,"The dissemination of powered communication devices has instigated a new technological paradigm called Internet of Things (IoT). These devices are present in our life, capturing information about people's routines and using them as data stream to support many decision-making processes. Nevertheless, many IoT applications take into consideration only the information related to their local context and the data analysis is done by a cloud server, increasing the feedback information latency to the data consumers and reducing the information quality. This work adopts the use of a distributed complex event processing (CEP) to analyse data considering all the IoT devices to execute data processing and distribution through a policy-based coordination architecture, building a DCEP, called GiTo. The policies define rules for coordination processing which are monitored through the distributed CEP engine. This approach focus on information quality improvement and time reduction between data generation and information acquired by the consumers."
pub.1104346515,Hardware/Software Model of DCO-OFDM Based Visible Light Communication SoC Using DMA,"In this paper we design hardware/software model of system-on-chip based visible light communication using DCO-OFDM modulation for point-to-point data transfer between PC. The designed system incorporates the ARM microprocessor with FPGA. Several critical processing system such as FFT, Viterbi decoder and synchronizer is implemented on hardware to accelerate the process, whereas the system scheduling and TCP communication between SoC and PC is done on software. For data transfer, we use DMA for stream data communication between processing element and for memory buffering. The model has been implemented and verified on ZYNQ-7000 SoC in terms of hardware resource usages and latency."
pub.1121060349,Towards an Event Streaming Service for ATLAS data processing,"The ATLAS experiment at the LHC is gradually transitioning from the traditional file-based processing model to dynamic workflow management at the event level with the ATLAS Event Service (AES). The AES assigns finegrained processing jobs to workers and streams out the data in quasi-real time, ensuring fully efficient utilization of all resources, including the most volatile. The next major step in this evolution is the possibility to intelligently stream the input data itself to workers. The Event Streaming Service (ESS) is now in development to asynchronously deliver only the input data required for processing when it is needed, protecting the application payload fromWAN latency without creating expensive long-term replicas. In the current prototype implementation, ESS processes run on compute nodes in parallel to the payload, reading the input event ranges remotely over the network, and replicating them in small input files that are passed to the application. In this contribution, we present the performance of the ESS prototype for different types of workflows in comparison to tasks accessing remote data directly. Based on the experience gained with the current prototype, we are now moving to the development of a server-side component of the ESS. The service can evolve progressively into a powerful Content Delivery Network-like capability for data streaming, ultimately enabling the delivery of ‘virtual data’ generated on demand."
pub.1141493698,Hybrid Workflow Scheduling on Edge Cloud Computing Systems,"Internet of Things applications can be represented as workflows in which stream and batch processing are combined to accomplish data analytics objectives in many application domains such as smart home, health care, bioinformatics, astronomy, and education. The main challenge of this combination is the differentiation of service quality constraints between batch and stream computations. Stream processing is highly latency-sensitive while batch processing is more likely resource-intensive. In this work, we propose an end-to-end hybrid workflow scheduling on an edge cloud system as a two-stage framework. In the first stage, we propose a resource estimation algorithm based on a linear optimization approach, gradient descent search (GDS), and in the second stage, we propose a cluster-based provisioning and scheduling technique for hybrid workflows on heterogeneous edge cloud resources. We provide a multi-objective optimization model for execution time and monetary cost under constraints of deadline and throughput. Results demonstrate the framework performance in controlling the execution of hybrid workflows by efficiently tuning several parameters including stream arrival rate, processing throughput, and workflow complexity. In comparison to a meta-heuristics technique using Particle Swarm Optimization (PSO), the proposed scheduler provides significant improvement for large-scale hybrid workflows in terms of execution time and cost with an average of 8% and 35%, respectively."
pub.1042441692,Resa,"We propose Resa, a novel framework for robust, elastic and realtime stream processing in the cloud. In addition to traditional functionalities of streaming and cloud systems, Resa provides (i) a novel mechanism that handles dynamic additions and removals nodes in an operator, and (ii) a node re-assignment scheme that minimizes output latency using a queuing model. We have implemented Resa on top of Twitter Storm. Experiments using real data demonstrate the effectiveness and efficiency of Resa."
pub.1137142790,Stream Processing With Dependency-Guided Synchronization (Extended Version),"Real-time data processing applications with low latency requirements have led
to the increasing popularity of stream processing systems. While such systems
offer convenient APIs that can be used to achieve data parallelism
automatically, they offer limited support for computations that require
synchronization between parallel nodes. In this paper, we propose
*dependency-guided synchronization (DGS)*, an alternative programming model for
stateful streaming computations with complex synchronization requirements. In
the proposed model, the input is viewed as partially ordered, and the program
consists of a set of parallelization constructs which are applied to decompose
the partial order and process events independently. Our programming model maps
to an execution model called *synchronization plans* which supports
synchronization between parallel nodes. Our evaluation shows that APIs offered
by two widely used systems -- Flink and Timely Dataflow -- cannot suitably
expose parallelism in some representative applications. In contrast, DGS
enables implementations with scalable performance, the resulting
synchronization plans offer throughput improvements when implemented manually
in existing systems, and the programming overhead is small compared to writing
sequential code."
pub.1094457411,Study and Implementation of Cluster Hierarchical Memory System of Multicore Cryptographic Processor,"In order to improve data bandwidth and enhance memory access efficiency in multi-core cryptographic processor, a kind of Cluster Hierarchical Memory System(CHMS) is proposed. According to characteristic of cryptographic algorithms/protocols parallel processing, shared memory unit and stream memory unit are designed. Result shows that, CHMS can make related cryptographic core more tighter and reduce memory access conflicts. CHMS also has fewer memory access latency and higher efficiency."
pub.1059170473,NaNet: a configurable NIC bridging the gap between HPC and real-time HEP GPU computing,"NaNet is a FPGA-based PCIe Network Interface Card (NIC) design with GPUDirect and Remote Direct Memory Access (RDMA) capabilities featuring a configurable and extensible set of network channels. The design currently supports both standard—Gbe (1000BASE-T) and 10GbE (10Base-R)—and custom—34 Gbps APElink and 2.5 Gbps deterministic latency KM3link—channels, but its modularity allows for straightforward inclusion of other link technologies. The GPUDirect feature combined with a transport layer offload module and a data stream processing stage makes NaNet a low-latency NIC suitable for real-time GPU processing. In this paper we describe the NaNet architecture and its performances, exhibiting two of its use cases: the GPU-based low-level trigger for the RICH detector in the NA62 experiment at CERN and the on-/off-shore data transport system for the KM3NeT-IT underwater neutrino telescope."
pub.1063154157,Streamflex,"The stream programming paradigm aims to expose coarse-grained parallelism inapplications that must process continuous sequences of events. The appeal ofstream programming comes from its conceptual simplicity. A program is acollection of independent filters which communicate by the means ofuni-directional data channels. This model lends itself naturally toconcurrent and efficient implementations on modern multiprocessors. As theoutput behavior of filters is determined by the state of their inputchannels, stream programs have fewer opportunities for the errors (such asdata races and deadlocks) that plague shared memory concurrent programming. This paper introduces S<scp>tream</scp>F<scp>lex</scp>, an extension to Java which marries streams with objects and thus enables to combine, in the same Java virtual machine, stream processing code with traditional object-oriented components. S<scp>tream</scp>F<scp>lex</scp> targets high-throughput low-latency applications with stringent quality-of-service requirements. To achieve these goals, it must, at the same time, extend and restrict Java. To allow for program optimization and provide latency guarantees, the S<scp>tream</scp>F<scp>lex</scp> compiler restricts Java by imposing a stricter typing discipline on filters. On the other hand, S<scp>tream</scp>F<scp>lex</scp> extends the Java virtual machine with real-time capabilities, transactional memory and type-safe region-based allocation. The result is a rich and expressive language that can be implemented efficiently."
pub.1126567515,Near Real-Time Big Data Stream Processing Platform Using Cassandra,"Users are always impatient to get answers instantly from analytics system. If time to insight exceeds 10s of milliseconds, then the value is lost. Applications such as stock market, sensors, Twitter feed data or fraud detection can't afford to wait. This often means analyzing the inflow of data before it even stored to the database of records. Coupled with zero tolerance for data loss and the challenge gets even more daunting. In realtime Big Data scenario rather waiting for data to be collected as a whole at a long periodic interval, streaming analysis let us identify patterns and make informed decisions based on them-as data start arriving. When data are non-stationary, and patterns change with time, streaming systems adapt itself. This work describes near real-time data storage and processing approaches to analyze streams of data with respect to Cassandra NoSQL datastore. It provides an insight into optimizing Cassandra on a multi data center setup for near Real-Time Responses. The classic trade-off between low-latency and high-accuracy is conceptualized. The theoretical claims are corroborated with several thorough experimental analysis in Apache and Datastax distribution of Cassandra."
pub.1019519754,Implementation and Analysis of TCP/IP Offload Engine and RDMA Transfer Mechanisms on an Embedded System,"The speed of present-day network technology exceeds a gigabit and is developing rapidly. When using TCP/IP in these high-speed networks, a high load is incurred in processing TCP/IP protocol in a host CPU. To solve this problem, research has been carried out into TCP/IP Offload Engine (TOE) and Remote Direct Memory Access (RDMA). The TOE processes TCP/IP on a network adapter instead of using a host CPU; this reduces the processing burden on the host CPU, and RDMA eliminates any copy overhead of incoming data streams by allowing incoming data packets to be placed directly into the correct destination memory location. We have implemented the TOE and RDMA transfer mechanisms on an embedded system. The experimental results show that TOE and RDMA on an embedded system have considerable latencies despite of their advantages in reducing CPU utilization and data copy on the receiver side. An analysis of the experimental results and a method to overcome the high latencies of TOE and RDMA transfer mechanisms are presented."
pub.1136518584,Stateful Stream Processing Containerized as Microservice to Support Digital Twins in Fog Computing,"Digital twins of processes and devices use information from sensors to synchronize their state with the entities of the physical world. The concept of stream computing enables effective processing of events generated by such sensors. However, the need to track the state of an instance of the object leads to the impossibility of organizing instances of digital twins as stateless services. Another feature of digital twins is that several tasks implemented on their basis require the ability to respond to incoming events at near-real-time speed. In this case, the use of cloud computing becomes unacceptable due to high latency. Fog computing manages this problem by moving some computational tasks closer to the data sources. One of the recent solutions providing the development of loosely coupled distributed systems is a Microservice approach, which implies the organization of the distributed system as a set of coherent and independent services interacting with each other using messages. The microservice is most often isolated by utilizing containers to overcome the high overheads of using virtual machines. The main problem is that microservices and containers together are stateless by nature. The container technology still does not fully support live container migration between physical hosts without data loss. It causes challenges in ensuring the uninterrupted operation of services in fog computing environments. Thus, an essential challenge is to create a containerized stateful stream processing based microservice to support digital twins in the fog computing environment. Within the scope of this article, we study live stateful stream processing migration and how to redistribute computational activity across cloud and fog nodes using Kafka middleware and its Stream DSL API."
pub.1164025152,Poster: Enabling Flexible Edge-assisted XR,"Extended reality (XR) is touted as the next frontier of the digital future.
XR includes all immersive technologies of augmented reality (AR), virtual
reality (VR), and mixed reality (MR). XR applications obtain the real-world
context of the user from an underlying system, and provide rich, immersive, and
interactive virtual experiences based on the user's context in real-time. XR
systems process streams of data from device sensors, and provide
functionalities including perceptions and graphics required by the
applications. These processing steps are computationally intensive, and the
challenge is that they must be performed within the strict latency requirements
of XR. This poses limitations on the possible XR experiences that can be
supported on mobile devices with limited computing resources.
  In this XR context, edge computing is an effective approach to address this
problem for mobile users. The edge is located closer to the end users and
enables processing and storing data near them. In addition, the development of
high bandwidth and low latency network technologies such as 5G facilitates the
application of edge computing for latency-critical use cases [4, 11]. This work
presents an XR system for enabling flexible edge-assisted XR."
pub.1093985452,Snapshot Processing in Streaming Environments,"Computational issues related to streaming data, and in particular the monitoring and rapid correlation of multiple sources of streaming data, are becoming increasingly important in contexts ranging from business processes to crisis detection. Applications include automated commodities trading (streams of stock and commodity ticker data), medical monitoring (streams of medical information from instruments worn by or in the vicinity of patients), and the detection of security threats such as biological and chemical weapons (streams of readings from radiation and biohazard detectors, intelligence services, immigration checkpoints, and more). For example, a government system to detect bioterror attacks must correlate multiple streams of possibly low-confidence data from sensors and local and national public health information networks with cues from indicators such as news and government sources indicating geographical locations, tactics and timing of possible attacks. The results of this correlation trigger appropriate responses, such as flagging information for more in-depth analysis or sending alerts to public health officials. Monitoring and correlation applications of this type are ideal for deployment on distributed computing grids, because they have high transaction throughput, require low latency, and can be partitioned into sets of small communicating computations with regular communication patterns. An important consideration in these applications is the need to ensure that, at any given time, computations are carried out on an accurate—or at least close to accurate—picture of the environment being monitored. One way of doing this, which we call snapshot processing, is to treat collections of events that occur at approximately the same time as representing a global snapshot—a valid state—of the environment. Computation on the resulting series of snapshots is much like computation on a real-time video of the entire environment. We briefly describe our model for these stream processing computations and introduce the concept of snapshot processing."
pub.1176037016,DECOMPOSITION OF INTEGRATED HIGH-DENSITY IoT DATA FLOW,"Topicality. The concept of fog computing made it possible to transfer part of the data processing and storage tasks from the cloud to fog nodes to reduce latency. But in batch processing of integrated data streams from IoT sensors, it is sometimes necessary to distribute the tasks of the batch between the fog and cloud layers. For this, it is necessary to decompose the formed package. But the existing methods of decomposition do not meet the requirements for efficiency in high-density IoT systems. The subject of study in the article are methods of decomposition of integrated data streams. The purpose of the article is to develop a method of decomposition of an integrated data stream in a dense high-density Internet of Things fog environment. This will reduce the processing time of operational transactions. The following results were obtained. The concept of decomposition of integrated information flows in the foggy layer was implemented to transition from the batch mode to the flow mode of task processing. Within the framework of the concept, a method of selecting elementary task flows from an integrated flow is proposed. An algorithm for decomposition of the integrated flow of tasks is proposed. Conclusion. A comparison of the proposed method of processing information flows in the foggy environment of high-density IoT with the existing approach is carried out. The results of the comparison showed that the proposed method is more suitable for deployment in conditions of limited network and computing resources. It is advisable to use it on nodes of fog computing systems with a high density of IoT sensors."
pub.1086105631,Detecting Insider Threats Using RADISH: A System for Real-Time Anomaly Detection in Heterogeneous Data Streams,"We present a scalable system for high-throughput real-time analysis of heterogeneous data streams. Our architecture enables incremental development of models for predictive analytics and anomaly detection as data arrives into the system. In contrast with batch data-processing systems, such as Hadoop, that can have high latency, our architecture allows for ingest and analysis of data on the fly, thereby detecting and responding to anomalous behavior in near real time. This timeliness is important for applications such as insider threat, financial fraud, and network intrusions. We demonstrate an application of this system to the problem of detecting insider threats, namely, the misuse of an organization's resources by users of the system and present results of our experiments on a publicly available insider threat dataset."
pub.1169480581,Edge-Based Real-Time Sensor Data Processing for Anomaly Detection in Industrial IoT Applications,"The Industrial Internet of Things (IIoT), which uses devices with sensors to provide real-time insights into crucial processes, has completely changed how industries function. However, there are many problems associated with the sheer volume and speed of data created in industrial environments, particularly when it comes to anomaly detection. The development of edge-based real-time sensor data processing techniques was required because traditional cloud-based solutions frequently experience latency problems and privacy issues. This study suggests a novel method for IIoT applications that focuses on processing sensor data at the edge, close to the data source, for anomaly identification. We offer real-time analysis of sensor data without the need for continuous data transfer to the cloud by utilising the processing capabilities of edge devices, such as industrial gateways and embedded systems. To find anomalies in streams of real-time sensor data, our methodology integrates data pre-processing, feature engineering, and machine learning algorithms. This strategy not only lessens the strain on the network's bandwidth but also ensures quick reaction to urgent situations, cutting downtime and boosting operational effectiveness. Proposed system has adaptive learning features that enable it to continuously adjust to altering ambient factors and sensor properties, enhancing the precision of anomaly detection over time. We provide experimental findings that show how our edge-based anomaly detection system performs well in diverse industrial situations. The results show that, while protecting data privacy and minimising latency, our methodology outperforms conventional cloud-based methods in terms of anomaly detection performance."
pub.1095207552,NaNet: Design of FPGA-Based Network Interface Cards for Real-Time Trigger and Data Acquisition Systems in HEP Experiments,"NaNet is a modular design of a family of FPGA-based PCIe Network Interface Cards specialized for low-latency real-time operations. NaNet features a Network Interface module that implements RDMA-style communications both with the host (CPU) and the GPU accelerators memories (GPUDirect P2P/RDMA) relying on the services of a high performance PCIe Gen3×8 core. NaNet I/O Interface is highly flexible and is designed for low and predictable communication latency: a dedicated stage manages the network stack protocol in the FPGA logic offloading the host operating system from this task and thus eliminating the associated process jitter effects. Between the two aforementioned modules, stand the data processing and switch modules: The first implements application-dependent processing on streams - e.g. performing compression algorithms - while the second routes data streams between the I/O channels and the Network Interface module. This general architecture has been specialized up to now into three configurations, namely NaNet-1, NaNet3 and NaNet-10 in order to meet the requirements of different experimental setups: NaNet-l features a GbE channel plus three custom 34 Gbps serial channels and is implemented on the Altera Stratix IV FPGA Development Kit; NaNet3 is implemented on the Terasic DE5-NET Stratix V FPGA development board and supports four custom 2.5 Gbps deterministic latency optical channels; NaNet-10 features four 10GbE SFP+ ports and is also implemented on the Terasic DE5-NET board. We will provide performance results for the three NaNet implementations and describe their usage in the CERN NA62 and KM3NeT-IT underwater neutrino telescope experiments, showing that the architecture is very flexible and yet capable of matching the requirements of low-latency real-time applications with intensive I/O tasks involving the CPU and/or the GPU accelerators."
pub.1160247260,GPU-based and Streaming-enabled Implementation of Pre-processing Flow towards Enhancing Optical Character Recognition Accuracy and Efficiency,"Research has demonstrated that digital images can be pre-processed through operations such as scaling, rotation, and blurring to enhance the accuracy of optical character recognition (OCR) by emphasizing important features within the image. Our study employed the open-source Tesseract OCR and found that accuracy can be improved through pre-processing techniques including thresholding, rotation, rescaling, erosion, dilation, and noise removal, based on a dataset of 560 phone screen images. However, our CPU-based implementation of this process resulted in an average latency of 48.32 ms per image, which can hinder the processing of millions of images using OCR. To address this challenge, we parallelized the pre-processing flow on the Nvidia P100 GPU and executed it through a streaming approach, which reduced the latency to 0.825 ms and achieved a speedup factor of 58.6x compared to the serial execution. This implementation enables the use of a GPU-based OCR engine to handle multiple sources of data streams with large-scale workloads."
pub.1133876487,Analytic Study of Containerizing Stateful Stream Processing as Microservice to Support Digital Twins in Fog Computing,"Digital twins of processes and devices use information from sensors to synchronize their state with the entities of the physical world. The concept of stream computing enables effective processing of events generated by such sensors. However, the need to track the state of an instance of the object leads to the impossibility of organizing instances of digital twins as stateless services. Another feature of digital twins is that several tasks implemented on their basis require the ability to respond to incoming events at near-real-time speed. In this case, the use of cloud computing becomes unacceptable due to high latency. Fog computing manages this problem by moving some computational tasks closer to the data sources. One of the recent solutions providing the development of loosely coupled distributed systems is a Microservice approach, which implies the organization of the distributed system as a set of coherent and independent services interacting with each other using messages. The microservice is most often isolated by utilizing containers to overcome the high overheads of using virtual machines. The main problem is that microservices and containers together are stateless by nature. The container technology still does not fully support live container migration between physical hosts without data loss. It causes challenges in ensuring the uninterrupted operation of services in fog computing environments. Thus, an essential challenge is to create a containerized stateful stream processing based microservice to support digital twins in the fog computing environment. Within the scope of this article, we study live stateful stream processing migration and how to redistribute computational activity across cloud and fog nodes using Kafka middleware and its Stream DSL API."
pub.1122596181,Real-Scenario Testing of an Active Phasor Data Concentrator,"The privileged position of the Phasor Data Concentrator (PDC) in a synchrophasor-based monitoring system can be exploited to make this device the core of a distributed measurement architecture suitable for control and protection applications in power systems. Recent research work has introduced the concept of an active PDC able both to handle adaptively the latency of the input streams sent by the Phasor Measurement Units (PMUs) and to implement control logics based on the measurement values provided by the same PMUs. This paper presents and discusses experimental tests performed on a prototype of the active PDC, which manages, through the implemented advanced functionalities, a realistic number of PMUs located in two different geographical sites. First, we evaluate the PDC processing time as function of the input streams' number. Then, we investigate the feasibility and advantage of an optimised management of the PDC output streams in the presence of several PMUs characterised by different geographical location and data packet size."
pub.1146837664,Stream processing with dependency-guided synchronization,"Real-time data processing applications with low latency requirements have led to the increasing popularity of stream processing systems. While such systems offer convenient APIs that can be used to achieve data parallelism automatically, they offer limited support for computations that require synchronization between parallel nodes. In this paper, we propose dependency-guided synchronization (DGS), an alternative programming model for stateful streaming computations with complex synchronization requirements. In the proposed model, the input is viewed as partially ordered, and the program consists of a set of parallelization constructs which are applied to decompose the partial order and process events independently. Our programming model maps to an execution model called synchronization plans which supports synchronization between parallel nodes. Our evaluation shows that APIs offered by two widely used systems---Flink and Timely Dataflow---cannot suitably expose parallelism in some representative applications. In contrast, DGS enables implementations with scalable performance, the resulting synchronization plans offer throughput improvements when implemented manually in existing systems, and the programming overhead is small compared to writing sequential code."
pub.1099678086,16.8 GB/s LPDDR4-3200 @32-Bit Memory Access Bandwidth,"Today's In-Vehicle Infotainment system requires more external memory access bandwidth (MABW) [1] especially for high definition (HD) video/audio. It demands maximum utilization for SDRAM access instead of expansion for SDRAM capacity due to limitation of chip size. An LPDDR4-3200 @32-bit SDRAM supports 12.8GB/s [2], and efficiency is limited to around 95% by SDRAM AC timing. An idea to achieve even more MABW than the limitation is introduced. Placing lossless compression unit [3] at source of requests, and decompression unit (DCU) integrated inside memory controller (MC), the number of SDRAM command has been deducted while increasing bus data bandwidth. However, latency becomes a critical issue for real-time request which may be non-decompression while going through DCU with big number of processing cycles. The adaptive solution is applied in this work so that both bandwidth and latency requirement are satisfied. The non-decompression transaction is separated and processed out-of-order to relax the latency, whereas the decompression transaction goes through DCU for decompression processing. Simulation result on LPDDR4-3200 @ 32-bit shows average 16.8 GB/s MABW for data rate after decompression using video playback data stream, with real-time requirement maintaining."
pub.1094735388,Study of cache system in video signal processors,"Memory system design is especially important for video signal processing, where the video signal processor (VSP) not only requires a lot of data, but also needs a very high bandwidth and low latency. While caches become ubiquitous in modern systems, their performance still falls behind that of the processors. Therefore a number of modifications to traditional caches have emerged: victim cache, stream buffer, data prefetching techniques, etc. However, few people have studied cache memory for VSP. We present a case study based on extensive trace-driven scheduling, which shows that while stream buffer and stride prediction table are very effective for streaming video data, they should be applied in a different way in dedicated VSP with higher degrees of parallelism than in current super-scalar workstation architectures."
pub.1124195245,A stream processing architecture for heterogeneous data sources in the Internet of Things,"The number of Internet of Things (IoT) and smart devices capable of producing, consuming and exchanging information is constantly increasing. It is estimated there will be around 30 billion of them in 2020. In most cases, the structures of the information produced by such devices are completely different, thus providing heterogeneous information. This is becoming a challenge for researchers working on IoT, who need to perform homogenisation and pre-processing tasks before using the IoT data. This paper aims to provide an architecture for processing and analysing data from heterogeneous sources with different structures in IoT scopes, allowing researchers to focus on data analysis, without having to worry about the structure of the data sources. This architecture combines the real-time stream processing paradigm for information processing and transforming, together with the complex event processing for information analysis. This provides us with capability of processing, transforming and analysing large amounts of information in real time. The results obtained from the evaluation of a real-world case study about water supply network management show that the architecture can be applied to an IoT water management scenario to analyse the information in real time. Additionally, the stress tests successfully conducted for this architecture highlight that a large incoming rate of input events could be processed without latency, resulting in efficient performance of the proposed architecture. This novel software architecture is adequate for automatically detecting situations of interest in the IoT through the processing, transformation and analysis of large amounts of heterogeneous information in real time."
pub.1093610401,Elastic Complex Event Processing Exploiting Prediction,"Supporting real-time, cost-effective execution of Complex Event processing applications in the cloud has been an important goal for many scientists in recent years. Distributed Stream Processing Systems (DSPS) have been widely adopted by major computing companies as a powerful approach for large-scale Complex Event processing (CEP). However, determining the appropriate degree of parallelism of the DSPS’ components can be particularly challenging as the volume of data streams is becoming increasingly large, the rule set is becoming continuously complex, and the system must be able to handle such large data stream volumes in real-time, taking into consideration changes in the burstiness levels and data characteristics. In this paper we describe our solution to building elastic complex event processing systems on top of our distributed CEP system which combines two commonly used frameworks, Storm and Esper, in order to provide both ease of usage and scalability. Our approach makes the following contributions: (i) we provide a mechanism for predicting the load and latency of the Esper engines in upcoming time windows, and (ii) we propose a novel algorithm for automatically adjusting the number of engines to use in the upcoming windows, taking into account the cost and the performance gains of possible changes. Our detailed experimental evaluation with a real traffic monitoring application that analyzes bus traces from the city of Dublin indicates the benefits in the working of our approach. Our proposal outperforms the current state of the art technique in regards to the amount of tuples that it can process by four orders of magnitude."
pub.1131823600,Benchmarking Distributed Stream Processing Frameworks for Real Time Classical Machine Learning Applications,"As the volume of data generated is growing at an unprecedented rate, it becomes important to analyze this data in real-time. To handle the huge volume of data streaming at a high velocity, we not only require powerful machines but also means to distribute the computation involved on the multiple machines. There are several open-source distributed stream processing frameworks such as Apache {Storm, Flink, Spark} and Confluent Kafka for building real-time machine learning applications. Prior works benchmarked some of these platforms using low-level operations like filters, joins, windowed computations etc. Our work includes benchmarking these popular frameworks for their applicability to classical machine learning models: Online K-Means, Online Linear Regression and Online Logistic Regression. We study the following quantitative metrics of evaluation: throughput, latency, CPU, and memory usage. The experiments were conducted in both standalone and clusters setups to determine the scalability of the models. This study will help system designers choose the right model and the right framework, given a specific configuration on streaming data."
pub.1143903126,Big Data Pipelines on the Computing Continuum: Ecosystem and Use Cases Overview,"Organisations possess and continuously generate huge amounts of static and stream data, especially with the proliferation of Internet of Things technologies. Collected but unused data, i.e., Dark Data, mean loss in value creation potential. In this respect, the concept of Computing Continuum extends the traditional more centralised Cloud Computing paradigm with Fog and Edge Computing in order to ensure low latency pre-processing and filtering close to the data sources. However, there are still major challenges to be addressed, in particular related to management of various phases of Big Data processing on the Computing Continuum. In this paper, we set forth an ecosystem for Big Data pipelines in the Computing Continuum and introduce five relevant real-life example use cases in the context of the proposed ecosystem."
pub.1139068599,A Close Look at Multi-tenant Parallel CNN Inference for Autonomous Driving,"Convolutional neural networks (CNNs) are widely used in vision-based autonomous driving, i.e., detecting and localizing objects captured in live video streams. Although CNNs demonstrate the state-of-the-art detection accuracy, processing multiple video streams using such models in real-time imposes a serious challenge to the on-car computing systems. The lack of optimized system support, for example, could lead to a significant frame loss due to the high processing latency, which is unacceptable for safety-critical applications. To alleviate this problem, several optimization strategies such as batching, GPU parallelism, and data transfer modes between CPU/GPU have been proposed, in addition to a variety of deep learning frameworks and GPUs. It is, however, unclear how these techniques interact with each other, which particular combination performs better, and under what settings. In this paper, we set out to answer these questions. We design and develop a Multi-Tenant Parallel CNN Inference Framework, MPInfer, to carefully evaluate the performance of various parallel execution modes with different data transfer modes between CPU/GPU and GPU platforms. We find that on more powerful GPUs such as GTX 1660, it achieves the best performance when we adopt parallelism across CUDA contexts enhanced by NVIDIA Multi-Process Service (MPS), with 147.06 FPS throughput and 14.50 ms latency. Meanwhile, on embedded GPUs such as Jetson AGX Xavier, pipelining is a better choice, with 46.63 FPS throughput and 35.09 ms latency."
pub.1004350039,Evaluating Transport Protocols for Real-Time Event Stream Processing Middleware and Applications,"Real-time event stream processing (RT-ESP) applications must synchronize continuous data streams despite fluctuations in resource availability. Satisfying these needs of RT-ESP applications requires predictable QoS from the underlying publish/subscribe (pub/sub) middleware. If a transport protocol is not capable of meeting the QoS requirements within a dynamic environment, the middleware must be flexible enough to tune the existing transport protocol or switch to a transport protocol better suited to the changing operating conditions.Realizing such adaptive RT-ESP pub/sub middleware requires a thorough understanding of how different transport protocols behave under different operating conditions. This paper makes three contributions to work on achieving that understanding. First, we define ReLate2, which is an evaluation metric that combines packet latency and reliability to evaluate transport protocol performance. Second, we use the ReLate2 metric to quantify the performance of various transport protocols integrated with the OMG’s Data Distribution Service (DDS) QoS-enabled pub/sub middleware standard using our FLEXibleMiddleware AndTransports (FLEXMAT) prototype for experiments that capture performance data. Third, we use ReLate2 to pinpoint configurations involving sending rate, network loss, and number of receivers that show the pros and cons of the protocols."
pub.1139811193,Real-time Text Stream Processing: A Dynamic and Distributed NLP Pipeline,"In recent years, the need for flexible and instant Natural Language Processing (NLP) pipelines becomes more crucial. The existence of real-time data sources, such as Twitter, necessitates using real-time text analysis platforms. In addition, due to the existence of a wide range of NLP toolkits and libraries in a variety of programming languages, a streaming platform is required to combine and integrate different modules of various NLP toolkits. This study proposes a real-time architecture that uses Apache Storm and Apache Kafka to apply different NLP tasks on streams of textual data. The architecture allows developers to inject NLP modules to it via different programming languages. To compare the performance of the architecture, a series of experiments are conducted to handle OpenNLP, Fasttext, and SpaCy modules for Bahasa Malaysia and English languages. The result shows that Apache Storm achieved the lowest latency, compared with Trident and baseline experiments."
pub.1086126773,Automatic Anomaly Detection over Sliding Windows,"With the advances in the Internet of Things and rapid generation of vast amounts of data, there is an ever growing need for leveraging and evaluating event-based systems as a basis for building realtime data analytics applications. The ability to detect, analyze, and respond to abnormal patterns of events in a timely manner is as challenging as it is important. For instance, distributed processing environment might affect the required order of events, time-consuming computations might fail to scale, or delays of alarms might lead to unpredicted system behavior. The ACM DEBS Grand Challenge 2017 focuses on real-time anomaly detection for manufacturing equipments based on the observation of a stream of measurements generated by embedded digital and analogue sensors. In this paper, we present our solution to the challenge leveraging the Apache Flink stream processing framework and anomaly ordering based on sliding windows, and evaluate the performance in terms of event latency and throughput."
pub.1085044577,Performance and Dependability Evaluation of Distributed Event-based Systems,"Distributed stream processing and event-based systems are an increasingly critical component in contemporary large-scale data processing applications, and are often subject to strict latency and reliability requirements. However, to achieve scalability demands, they are often deployed on distributed clusters of heterogeneous nodes, causing unpredictable runtime performance and complex fault characteristics. The behaviour of these systems is poorly understood, and existing performance and dependability evaluation techniques are ill-equipped to handle the challenges introduced by the complex and distributed nature of event-based systems. We develop a dynamic code-injection approach to evaluate the performance and dependability of stream processing and event-based systems. Our approach supports fine-grained instrumentation of applications and their runtime infrastructure, and the dynamic injection of code mutations and faults into a production system at runtime. We demonstrate the proposed approach by performing instrumentation and code injection on a distributed Apache Spark cluster."
pub.1103198991,FastPM: An approach to pattern matching via distributed stream processing,"Pattern matching over big data is gaining momentum in recent years. Many real-time applications are involved in pattern matching over a high volume of data to discover potential tendencies, in which real-time response and concurrent processing are the key performance metrics. However, it is challenging to efficiently match over live streaming data due to: (i) the high volume of massive data, (ii) the real-time response requirement, and (iii) the concurrent matching queries. To address these challenges, we introduce a pattern model by appending a timestamp set to reduce the number of repeated patterns and propose FastPM, a distributed stream processing framework to address the high speed real-time data. Our framework combines synchronous and asynchronous mechanisms to deal with multiple matching queries simultaneously, and develops multiple techniques to enhance the efficiency of pattern matching. We implement FastPM and evaluate its performance on billions of real-world web-click data. Our empirical results demonstrate the effectiveness of FastPM on matching queries and pattern updates. On average, FastPM responds to a matching query in 0.2 s and to an update request in 0.03 s. Furthermore, FastPM is able to support 5000 matching queries simultaneously and the average query latency is 1.3 s."
pub.1009597085,Incremental placement of interactive perception applications,"Interactive perception applications, such as gesture recognition and vision-based user interfaces, process high-data rate streams with compute intensive computer vision and machine learning algorithms. These applications can be represented as data flow graphs comprising several processing stages. Such applications require low latency to be interactive so that the results are immediately available to the user. To achieve low latency, we exploit the inherent coarse grained task and data parallelism of these applications by running them on clusters of machines. This paper addresses an important problem that arises: how to place the stages of these applications on machines to minimize the latency, and in particular, how to adjust an existing schedule in response to changes in the operating conditions (perturbations) while minimizing the disruption in the existing placement (churn). To this end, we propose four incremental placement heuristics which use the HEFT scheduling algorithm as their primary building block. Through simulations and experiments on a real implementation, using diverse workloads and a range of perturbation scenarios, we demonstrate that dynamic adjustment of the schedule can improve latency by as much as 36%, while producing little churn."
pub.1023399981,A refinement theory for timed-dataflow analysis with support for reordering,"Real-time stream processing applications executed on embedded multiprocessor systems often have strict throughput and latency constraints. Violating these constraints is undesired and temporal analysis methods are therefore used to prevent such violations. These analysis methods use abstractions of the analyzed applications to simplify their temporal analysis. Refinement theories have enabled the creation of deterministic abstractions of stream processing applications that are executed on multiprocessor systems. Prominent examples of such abstract models are deterministic timed-dataflow models which can be efficiently analyzed because they only have one behavior. An important aspect of a stream processing application can be that it makes use of reordered data streams between tasks. An example is the bit-reversed ordered stream produced by a Fast Fourier Transform (FFT) task. However, existing abstraction/refinement theories do not support such reordering behavior or do not handle this type of behavior correctly. This is because existing refinement theories assume that the temporal behavior of applications is orthogonal to their functional behavior, whereas this orthogonality does not always hold in the case of reordered data streams. In this paper we introduce a new refinement theory in which the potential interaction between temporal and functional behavior is taken into account. The introduced theory supports reordering of data and can therefore be used to validate existing systems with such reordering. Furthermore, the theory enables showing that deterministic dataflow models that do not apply reordering can be used as valid abstractions of systems in which reordering takes place. The applicability of the refinement theory is demonstrated by creating deterministic timed-dataflow model abstractions of a Digital Video Broadcasting Terrestrial (DVB-T) application, and a communication network in which data is reordered. With these dataflow models the guaranteed throughput and buffer capacities of implementation options are compared."
pub.1094408967,Traffic Monitoring Using Video Analytics in Clouds,"Traffic monitoring is a challenging task on crowded roads. Traditional traffic monitoring procedures are manual, expensive, time consuming and involve human operators. They are subjective due to the very involvement of human factor and sometimes provide inaccurate/incomplete monitoring results. Large scale storage and analysis of video streams were not possible due to limited availability of storage and compute resources in the past. Recent advances in data storage, processing and communications have made it possible to store and process huge volumes of video data and develop applications that are neither subjective nor limited in feature sets. It is now possible to implement object detection and tracking, behavioural analysis of traffic patterns, number plate recognition and automate security and surveillance on video streams produced by traffic monitoring and surveillance cameras. In this paper, we present a video stream acquisition, processing and analytics framework in the clouds to address some of the traffic monitoring challenges mentioned above. This framework provides an end-to-end solution for video stream capture, storage and analysis using a cloud based GPU cluster. The framework empowers traffic control room operators by automating the process of vehicle identification and finding events of interest from the recorded video streams. An operator only specifies the analysis criteria and the duration of video streams to analyse. The video streams are then automatically fetched from the cloud storage, decoded and analysed on a Hadoop based GPU cluster without operator intervention in our framework. It reduces the latencies in video analysis process by porting its compute intensive parts to the GPU cluster. The framework is evaluated with one month of recorded video streams data on a cloud based GPU cluster. The results show a speedup of 14 times on a GPU and 4 times on a CPU when compared with one human operator analysing the same amount of video streams data."
pub.1143720154,Modelling Serverless Function Behaviours,"The serverless computing model extends potential deployment options for cloud applications, by allowing users to focus on building and deploying their code without needing to configure or manage the underlying computational resources. Cost and latency constraints in stream processing user applications often push computations closer to the sources of data, leading to challenges for dynamically distributing stream operators across the edge/fog/cloud heterogeneous nodes and the routing of data flows. Various approaches to support operator placement across edge and cloud resources and data routing are beginning to be addressed through the serverless model. Understanding how stream processing operators can be mapped into serverless functions also offers cost incentives for users – as charging is now on a subsecond basis (rather than hourly). A dynamic Petri net model of serverless functions is proposed in this work, which takes account of the computational requirements of functions, the resources on which these functions are hosted, and key parameters that impact the behaviour of serverless functions – such as warm/cold start up times. The model can be used by developers/users of serverless functions to understand how deployment optimisation can be used to reduce application time, and to analyse various scenarios on choosing function granularity, data size and cost."
pub.1095203774,High throughput low latency ldpc decoding on gpu for sdr systems,"In this paper, we present a high throughput and low latency LDPC (low-density parity-check) decoder implementation on GPUs (graphics processing units). The existing GPU-based LDPC decoder implementations suffer from low throughput and long latency, which prevent them from being used in practical SDR (software-defined radio) systems. To overcome this problem, we present optimization techniques for a parallel LDPC decoder including algorithm optimization, fully coalesced memory access, asynchronous data transfer and multi-stream concurrent kernel execution for modern GPU architectures. Experimental results demonstrate that the proposed LDPC decoder achieves 316 Mbps (at 10 iterations) peak throughput on a single GPU. The decoding latency, which is much lower than that of the state of the art, varies from 0.207 ms to 1.266 ms for different throughput requirements from 62.5 Mbps to 304.16 Mbps. When using four GPUs concurrently, we achieve an aggregate peak throughput of 1.25 Gbps (at 10 iterations)."
pub.1093795537,Towards High Performance Processing of Streaming Data in Large Data Centers,"Smart devices, mobile robots, ubiquitous sensors, and other connected devices in the Internet of Things (IoT) increasingly require real-time computations beyond their hardware limits to process the events they capture. Leveraging cloud infrastructures for these computational demands is a pattern adopted in the IoT community as one solution, which has led to a class of Dynamic Data Driven Applications (DDDA). These applications offload computations to the cloud through Distributed Stream Processing Frameworks (DSPF) such as Apache Storm. While DSPFs are efficient in computations, current implementations barely meet the strict low latency requirements of large scale DDDAs due to inefficient inter-process communication. This research implements efficient highly scalable communication algorithms and presents a comprehensive study of performance, taking into account the nature of these applications and characteristics of the cloud runtime environments. It further reduces communication costs within a node using an efficient shared memory approach. These algorithms are applicable in general to existing DSPFs and the results show significant improvements in latency over the default implementation in Apache Storm."
pub.1094432900,Declarative Network Monitoring with an Underprovisioned Query Processor,"Many of the data sources used in stream query processing are known to exhibit bursty behavior. We focus here on passive network monitoring, an application in which the data rates typically exhibit a large peak-to-average ratio. Provisioning a stream query processor to handle peak rates in such a setting can be prohibitively expensive. In this paper, we propose to solve this problem by provisioning the query processor for typical data rates instead of much higher peak data rates. To enable this strategy, we present mechanisms and policies for managing the tradeoffs between the latency and accuracy of query results when bursts exceed the steady-state capacity of the query processor. We describe the current status of our implementation and present experimental results on a testbed network monitoring application to demonstrate the utility of our approach."
pub.1105796457,Swing: Swarm Computing for Mobile Sensing,"This paper presents Swing, a framework that aggregates a swarm of mobile devices to perform collaborative computation on sensed data streams. It endows performance and efficiency to the new generation of mobile sensing applications, in which the computation is overly intensive for a single device. After studying the source of performance slowdown of the sensing applications on a single device, we design and implement Swing to manage (i) parallelism in stream processing, (ii) dynamism from mobile users, and (iii) heterogeneity from the swarm devices. We build an Android-based prototype and deploy sensing apps - face recognition and language translation - on a wireless testbed. Our evaluations show that with proper management policies, such a distributed processing framework can achieve up to 2.7x improvement in throughput and 6.7x reduction in latency, allowing intensive sensing apps to reach real-time performance goals under different device usages, network conditions and user mobility."
pub.1111648333,Scalable Distributed Top-k Join Queries in Topic-Based Pub/Sub Systems,"In this paper, we provide a novel approach that enables the execution of top-k join queries over sliding windows in a way that reduces the amount of data that need to be analyzed by the stream processing operators. The main idea is that brokers individually invoke the query on their received messages and forward the top-k results to a stream processing operator that performs the merging of the results and provides to the end-user the final top-k results. Moreover, our system exploits the Bayesian Optimization technique to determine automatically the number of top-k results that should be provided by each broker. Our approach has been developed in the Kappa architecture that exploits topic-based scalable publish/subscribe (pub/sub) systems like Apache Kafka to efficiently forward the high volume of incoming messages to distributed processing systems (i.e., Apache Spark or Apache Flink) that perform the batch and stream analytics operations. Our detailed experimental evaluation on our local cluster illustrates that we can efficiently execute top-k join queries on our system with high accuracy and low latency."
pub.1105882774,The Big Data-RTAP: Toward a Secured Video Surveillance System in Smart Environment,"Big Data is an emerged architecture and technology paradigm that is used by many organizations to extract valuable information either to take decisions. Big Data is a technique and method used to retrieve, collect, process and analyze a very big volume of unstructured and structured data. The challenge is processing and analyzing the huge volume of data coming in from network sensors. Practically, it’s too late to stop an abnormal comportment, if we collect the incoming streams and wait for many days for processing and analyzing the stored streams. Big Data in video surveillance systems, offer ETL (Extract Transform and Load) challenges related to the Van Newman Bottleneck and Data Locality. In this chapter we propose a conceptual model with architectural elements and proposed tools for monitoring in RTAP (Real Time Analytical Processing) mode smart areas.Our model is based on lambda architecture, in order to resolve the problem of latency which is imposed in transactional requests (GAB Network). We consider the real example that data comes from different sources (Automatic monitoring Centers, GAB, Facebook, Twitter, Instagram, LinkedIn, Medical Centers, Commercial Centers and any other data collected by satellites.) which is n dimension and we which to reduce with PCA algorithm the number of components to reduce the processing time and increase the speed of execution."
pub.1032674750,A hierarchical component model for large parallel interactive applications,"This paper focuses on parallel interactive applications ranging from scientific visualization, to virtual reality or computational steering. Interactivity makes them particular on three main aspects: they are endlessly iterative, use advanced I/O devices, and must perform under strong performance constraints (latency, refresh rate). A data flow graph is a common approach to describe such applications. Edges represent data streams while vertices are nodes processing incoming data streams and producing new data streams. When applications become large, this approach shows its limits in terms of maintainability and portability. In this paper, we propose to use the composite design pattern to extend this model for supporting hierarchies of components. The component hierarchy is traversed to instantiate the application and extract the data flow graph required for the execution. This approach has been implemented for the FlowVR middleware. It enables to define parametric composite components, commonly called skeletons, that can be reused in various applications. This approach proved to significantly leverage application modularity as presented in different case studies."
pub.1159192448,WG-Storm Scheduler for Distributed Stream Processing Engines,"<p>Stream Processing Engines (SPEs) allow applications to process a large amount of data in real-time. However, to schedule big data applications; the SPEs create several challenges in terms of load balancing, resource utilization, etc. As the volume of data increases over time, it also poses a challenge to predict the resource and application requirements for processing. All these factors play an important role, they can cause problems in achieving maximum throughput due to inefficiency in any of them. Most SPEs ignore the topology, which may minimize throughput during scheduling and may increase network latency. In this paper, we proposed a topology-aware and resource-aware scheduler (named WG-Storm) based on Directed Acyclic Graph (DAG) that enhances the resource usage and overall throughput using efficient tasks assignment. WG-Storm is built on Apache Storm and results are generated using the 2 linear topologies and compared with the 5 state-of-art schedulers including the (A3-Storm, Default, Isolation, Multi-tenant, and Resource-aware). Results have shown up to 30% throughput improvement using minimum resource usage in heterogeneous clusters.</p>"
pub.1124299875,4K 120fps HEVC Temporal Scalable Encoder with Super Low Delay,"This paper describes a novel 4K 120 fps (frames per second) real-time HEVC encoder for HFR (high frame rate) video encoding and transmission. HFR provides more immersive viewing experience, and real-time encoding of HFR videos with low latency and temporal scalability is required for providing HFR video services. Multichip configuration with two encoder LSIs achieves full 4K/120fps real-time encoding. The exchange of reference picture data near the spatially divided slice boundary provides cross-chip motion estimation, and maintains the coding efficiency. The encoder supports stream output with temporal scalability transmitted over one or two transmission paths. It also achieves 21.8 msec low latency processing by using motion vector restriction."
pub.1132506925,A low latency ASR-free end to end spoken language understanding system,"In recent years, developing a speech understanding system that classifies a
waveform to structured data, such as intents and slots, without first
transcribing the speech to text has emerged as an interesting research problem.
This work proposes such as system with an additional constraint of designing a
system that has a small enough footprint to run on small micro-controllers and
embedded systems with minimal latency. Given a streaming input speech signal,
the proposed system can process it segment-by-segment without the need to have
the entire stream at the moment of processing. The proposed system is evaluated
on the publicly available Fluent Speech Commands dataset. Experiments show that
the proposed system yields state-of-the-art performance with the advantage of
low latency and a much smaller model when compared to other published works on
the same task."
pub.1045937402,"Distributed, application-level monitoring for heterogeneous clouds using stream processing","As utility computing is widely deployed, organizations and researchers are turning to the next generation of cloud systems: federating public clouds, integrating private and public clouds, and merging resources at all levels (IaaS, PaaS, SaaS). Adaptive systems can help address the challenge of managing this heterogeneous collection of resources. While services and libraries exist for basic management tasks that enable implementing decisions made by the manager, monitoring is an open challenge. We define a set of requirements for aggregating monitoring data from a heterogeneous collections of resources, sufficient to support adaptive systems. We present and implement an architecture using stream processing to provide near-realtime, cross-boundary, distributed, scalable, fault-tolerant monitoring. A case study illustrates the value of collecting and aggregating metrics from disparate sources. A set of experiments shows the feasibility of our prototype with regard to latency, overhead, and cost effectiveness."
pub.1099056714,An extensible test framework for the Microsoft StreamInsight query processor,"Microsoft StreamInsight (StreamInsight, for brevity) is a platform for developing and deploying streaming applications. StreamInsight adopts a deterministic stream model that leverages a temporal algebra as the underlying basis for processing long-running continuous queries. In most streaming applications, continuous query processing demands the ability to cope with high input rates that are characterized by imperfections in event delivery (i.e., incomplete or out-of-order data). StreamInsight is architected to handle imperfections in event delivery, to generate real-time low-latency output, and to provide correctness guarantees on the resultant output. On one hand, streaming operators are similar to their well-understood relational counterparts - with a precise algebra as the basis of their behavior. On the other hand, streaming operators are unique in their non-blocking nature, which guarantees low-latency and incremental result delivery. While our deterministic temporal algebra paves the way towards easier testing of the streaming system, one unique challenge is that as the field evolves with more customers adopting streaming solutions, the semantics, behavior, and variety of operators is constantly under churn. This paper overviews the test framework for the StreamInsight query processor and highlights the challenges in verifying the functional correctness of its operators. The paper discusses the extensibility and the reusability of the proposed streaming test infrastructure, as the research and industrial communities address new and constantly evolving challenges in stream query processing."
pub.1175137181,SCIMITAR: Stochastic Computing In-Memory In-Situ Tracking ARchitecture for Event-Based Cameras,"Event-based cameras offer low latency and high-dynamic range imaging data in a sparse format that is well-suited for high-speed object tracking. Processing this sparse data in the same way as traditional camera data requires a great deal of unnecessary computation, making it difficult to take advantage of the high-effective frame rate for real-time processing. In this work, we propose an accelerator for high-speed object tracking on event-based camera data. SCIMITAR combines digital in-memory stochastic computing, in-situ stochastic stream generation, and multiple optimizations for utilizing input sparsity. SCIMITAR provides unparalleled performance with latency and energy that scale with sparsity. We demonstrate SCIMITAR performance on an object tracking application using circuit-level simulations of custom-designed compute-in-memory (CIM) macros and digital circuits. We achieve a frame processing rate of 26k frames/s with 100 regions-of-interest per frame and equivalent or better than state-of-the-art tracking accuracy. The accelerator achieves a peak throughput of 71 TOP/S and energy efficiency of 733 to 1702 TOP/S/W demonstrated on a range of event-based vision datasets, which is $5\times $ higher than other CIM solutions."
pub.1107688509,Distributed Collaborative Filtering for Batch and Stream Processing-Based Recommendations,"Nowadays, user actions are tracked and recorded by multiple websites and e-commerce platforms, allowing them to better understand their preferences and support them with specific and accurate content suggestions. Researches have proposed several recommendation approaches and addressed several challenges such as data sparsity and cold start. However, the low-scalability problem remains a major challenge when handling large volumes of user actions data. This issue becomes more challenging when it comes to real-time applications. Such constraint requires a new class of low latency recommendation approaches capable of incrementally and continuously update their knowledge and models at scale as soon as data arrives. In this paper, we focus on the user-centered collaborative filtering as one of the most adopted recommendation approaches known for its lack of scalability. We propose two distributed and scalable implementations of collaborative filtering addressing the challenges and the requirements of batch offline and incremental online recommendation scenarios. Several experiments were conducted on a distributed environment using the MovieLens dataset in order to highlight the properties and the advantages of each variant."
pub.1175108525,Status-Byte-Assisted RDMA Transmission Mechanism for Optimizing Multi-Task Video Streaming in Edge Computing,"In the context of the rapid development of edge computing, optimizing data transmission and reducing latency is crucial for efficient collaborative processing among edge servers. Traditional TCP/IP protocols are hindered by high latency and low throughput, while RDMA (Remote Direct Memory Access) technology addresses these challenges by enabling direct memory access and bypassing the operating system kernel. However, the RDMA data transmission mechanism based on sliding windows requires frequent memory status exchanges in the order of memory blocks, which can limit its ability to handle multiple concurrent tasks within a single Queue Pair (QP). To address the limitations of the traditional sliding window transmission mechanism in multi-task environments, we propose a novel RDMA data transmission mechanism that utilizes status bytes to indicate memory block utilization, which utilizes stateless server connections, and multi-task shared QP transmission strategies. In the proposed mechanism, fine-grained control over memory blocks is achieved through the status byte, thereby enabling effective multi-task real-time video stream transmission. Experimental results show that, compared to the sliding window method, the proposed status-byte-assisted RDMA transmission mechanism provides higher throughput, lower latency, and reduced resource consumption, thus enhancing system scalability and reducing CPU utilization. Moreover, this mechanism achieves more stable throughput than the sliding window method when transmitting multiple real-time video streams in edge computing scenarios, making it particularly suitable for data transmission in such environments."
pub.1109947091,Evaluating spatial-keyword queries on streaming data,"This paper provides an extensive experimental evaluation for different spatial-keyword index structures on streaming data. We extend existing snapshot spatial-keyword queries with the temporal dimension to effectively serve streaming data applications. Then, the major index structures are equipped with efficient query processing techniques and evaluated to process the extended queries. The evaluation is oriented towards a system building perspective to provide system builders with insights on supporting scalable spatial-keyword queries on fast data streams, e.g., social media streams and news streams. In particular, we have taken existing spatial-keyword index structures apart into four major building blocks that are commonly supported at a system-level. Ten different index structures are then composed as combinations of these four building blocks. The ten indexes are wholly residents in main-memory, and they are evaluated on real datasets and query locations. The index performance is measured in terms of data digestion rate in real time, main-memory footprint, and query latency. The results show the relative performance gains of both basic and hybrid index structures with abundant insights from a system point of view."
pub.1158148817,Evolving the Digital Industrial Infrastructure for Production: Steps Taken and the Road Ahead,"The Internet of Production (IoP) leverages concepts such as digital shadows,
data lakes, and a World Wide Lab (WWL) to advance today's production.
Consequently, it requires a technical infrastructure that can support the agile
deployment of these concepts and corresponding high-level applications, which,
e.g., demand the processing of massive data in motion and at rest. As such, key
research aspects are the support for low-latency control loops, concepts on
scalable data stream processing, deployable information security, and
semantically rich and efficient long-term storage. In particular, such an
infrastructure cannot continue to be limited to machines and sensors, but
additionally needs to encompass networked environments: production cells, edge
computing, and location-independent cloud infrastructures. Finally, in light of
the envisioned WWL, i.e., the interconnection of production sites, the
technical infrastructure must be advanced to support secure and
privacy-preserving industrial collaboration. To evolve today's production sites
and lay the infrastructural foundation for the IoP, we identify five broad
streams of research: (1) adapting data and stream processing to heterogeneous
data from distributed sources, (2) ensuring data interoperability between
systems and production sites, (3) exchanging and sharing data with different
stakeholders, (4) network security approaches addressing the risks of
increasing interconnectivity, and (5) security architectures to enable secure
and privacy-preserving industrial collaboration. With our research, we evolve
the underlying infrastructure from isolated, sparsely networked production
sites toward an architecture that supports high-level applications and
sophisticated digital shadows while facilitating the transition toward a WWL."
pub.1174739027,Split-FL: An Efficient Online Federated Learning Framework with Constrained Computation and Streaming Data,"To satisfy the increasing demand for enabling large-scale machine learning for low-latency multimedia data processing and data privacy on mobile edge devices, federated learning (FL) has advanced as an important learning infrastructure. However, in many practical wireless communications applications with limited resources, the conventional FL techniques suffer from heavy communication and computation overheads arising from local training and periodic global aggregation, especially for streaming data. Moreover, data heterogeneity in the distributed system breaks the classical independent and identically distributed (IID) assumption, impacting accuracy and convergence speed. In this paper, we propose a novel FL framework of Split-FL to enhance the computational efficiency of processing data streams in edge networks, which allows model customization based on hardware constraints and enables the participation of heterogeneous devices without compromising learning performance. Specifically, Split-FL involves an online learning procedure that balances the parameters updated from training data streams. Additionally, an adaptive pruning procedure is proposed to reduce computation and communication costs during the training process. The simulation results demonstrate the superiority of Split-FL over existing methods, demonstrating its high efficiency in communication and computation."
pub.1094293139,Exploiting Data Locality in FFT using Indirect Swap Network on Cell/B.E,"Communication and synchronization are two main latency issues in computing FFT on parallel architectures. Both latencies have to be either hidden or tolerated to achieve high performance. One approach to achieve this is by multithreading. Another approach to tolerate latency is to map data efficiently onto the processors' local memory and exploiting data locality. Indirect swap networks, an idea proposed in VLSI circuits can be efficiently used to compute the butterfly computations in FFT. Data mapping in the swap network topology reduces the communication overhead by half at each iteration. Cell Broadband Engine (Cell/B.E.) processor is a heterogeneous multicore processor for stream data applications and high performance computing. Its eight SIMD processing elements, Synergistic Processor Elements (SPEs), provide multi-folded parallelism. In this paper, we investigate the improved Cooley-Tukey FFT algorithm based on indirect swap network, and design the parallel algorithm taking into consideration all the features of the Cell/B.E. architecture. The performance results show that the new algorithm on Cell/B.E. is 3.7 faster than the cluster for 4K input data size and 6.4 faster than the cluster for 16K input data size at the processor level."
pub.1007210704,Cost-Effective Stream Join Algorithm on Cloud System,"Matrix-based scheme (Join-Matrix) can prefectly support distributed stream joins, especially for arbitrary join predicates, because it guarantees any tuples from two streams to meet with each other. However,the dynamics and unpredictability features of stream require quick actions on scheme changing. Otherwise, they may lead to degradation of system throughputs and increament of processing latency with the waste of system resources, such as CPUs and Memories. Since Join-Matrix model has the fixed processing architecture with replicated data, these kinds of adverseness will be magnified. Therefore, it is urgent to find a solution that preserves advantages of Join-Matrix model and promises a good usage to computation resources when it meets scheme changing. In this paper, we propose a cost-effective stream join algorithm, which ensures the adaptability of Join-Matrix but with lower resources consumption. Specifically, a varietal matrix generation algorithm is proposed to generate an irregular matrix scheme for assigning the minimal number of tasks; a lightweight migration algorithm is designed to ensure state migration at a low cost; a complete load balance process framework is described to guarantee the correctness during the scheme changing. We conduct extensive experiments to compare our method with baseline systems on both benchmarks and real-workloads, and explain the results in detail."
pub.1009006957,An Open-Source Cloud Architecture for Big Stream IoT Applications,"The Internet of Things (IoT) is shaping to a worldwide network of networks consisting of billions of interconnected heterogeneous sensor/actuator-equipped devices (denoted as “things” or “smart objects”), which are expected to exceed 50 billions by 2020. Smart objects, which will be pervasively deployed, are constrained devices with (i) limited processing power and available memory and (ii) limited communication capabilities, in terms of transmission rate and reliability. Future Smart-X applications, such as Smart Cities and Home Automation, will be fostered by the use of standard and interoperable IP-based communication protocols that smart objects are going to implement, by simplifying their development, integration, and deployment. Smart-X applications will significantly differ from traditional Internet services, in terms of: (i) the number of data sources; (ii) rate of information exchange; and, (iii) need for real-time processing. Because of these requirements, such services are denoted as “Big Stream” applications, in order to distinguish them from traditional Big Data applications. In this paper, we present an implementation of a novel Cloud architecture for Big Stream applications based on standard protocols and open-source components, which provides a scalable and efficient processing platform for IoT applications, designed to be open and extensible and to guarantee minimal latency between data generation and consumption. We also provide a performance evaluation based on experimentation in a real-world Smart Parking scenario, to assess the feasibility and scalability of the proposed architecture."
pub.1119201091,Efficient Time-Evolving Stream Processing at Scale,"Time-evolving stream datasets exist ubiquitously in many real-world
applications where their inherent hot keys often evolve over times.
Nevertheless, few existing solutions can provide efficient load balance on
these time-evolving datasets while preserving low memory overhead. In this
paper, we present a novel grouping approach (named FISH), which can provide the
efficient time-evolving stream processing at scale. The key insight of this
work is that the keys of time-evolving stream data can have a skewed
distribution within any bounded distance of time interval. This enables to
accurately identify the recent hot keys for the real-time load balance within a
bounded scope. We therefore propose an epoch-based recent hot key
identification with specialized intra-epoch frequency counting (for maintaining
low memory overhead) and inter-epoch hotness decaying (for suppressing
superfluous computation). We also propose to heuristically infer the accurate
information of remote workers through computation rather than communication for
cost-efficient worker assignment. We have integrated our approach into Apache
Storm. Our results on a cluster of 128 nodes for both synthetic and real-world
stream datasets show that FISH significantly outperforms state-of-the-art with
the average and the 99th percentile latency reduction by 87.12% and 76.34% (vs.
W-Choices), and memory overhead reduction by 99.96% (vs. Shuffle Grouping)."
pub.1012820843,Implementation of Realtime and Highspeed Phase Detector on FPGA,"We describe the hardware implementation of a phase detector module which is used in a heavy ion accelerator for real-time digital data processing. As this high-speed real-time signal processing currently exceeds the performance of the available DSP processors, we are trying to move some functionality into dedicated hardware. We implemented the phase detection algorithm using a pipeline mechanism to process one data value in every clock cycle. We used a pipelined division operation and implemented an optimized table-based arctan as the main core to compute the phase information. As the result, we are able to process the two 400 MHz incoming data streams with low latency and minimal resource allocation."
pub.1105974689,A Comparative Study of Distributed Tools for Analyzing Streaming Data,"Continuous and rapid moving data known as streaming-data need to be analyzed in a short span of time, as the size of the data may overload the storage system quickly. Traditional tools often don't help significantly in the analysis of streaming data in real time. A large number of distributed tools have been developed to process streaming data in a real-time manner. In this paper, the state of the art of stream processing tools for low-latency Big Data analytics is presented. A qualitative comparison between the most popular tools like Apache Spark, Storm, Samza and Apache S4 is provided at the end the paper."
pub.1127976753,Parallelization of Massive Multiway Stream Joins on Manycore CPUs,"Abstract
Joining a high number of data streams efficiently in terms of required memory and CPU time still poses a challenge. While binary join trees are very common in database systems, they are mostly unusable for streaming queries with tight latency constraints when the number of streaming sources is increasing. Multiway stream joins, on the other hand, are very suitable for this task since they are mostly independent of the non-optimal ordering of join operators or huge intermediate join results.In this paper, we discuss challenges but also opportunities for multiway stream joins for modern hardware, especially manycore processors. We describe different parallelization and optimization strategies to allow a streaming query to join up to 256 streams on a single CPU while keeping individual tuple response time and also memory footprint low. Our results show that a multiway join can perform magnitudes faster than a binary join tree. In addition, further tuning for efficient parallelism can improve performance again for a factor up to a magnitude."
pub.1137943415,A Novel Prefetching Scheme for Non-Volatile Cache in the AIoT Processor,"Artificial intelligence Internet of Things (AIoT) systems need more processing capability and low energy consumption on the terminals. Increasing the cache capacity at the architecture level can effectively reduce the cache missing rate and the number of off-chip memory access, which improves the performance of processors. Because of the limitation of the storage density and high leakage currents, improving SRAM cache capacity will lead to big chip area and high power consumption of processors. Emerging non-volatile memory (NVM), such as spin-transfer torque RAM (STT-RAM), has some characters such as bit width read/write and short read/write latency which are attractive options for replacing or augmenting SRAM. However, the asymmetric read/write latency of NVM brings some new challenges in designing caches. In this paper, we introduce a method to bring STT-RAM into the cache system of AIoT processors. Experimental results show that if we replace SRAM L2 cache with the same sized STT-RAM L2 cache, the out-of-order processor performance is improved up to 10 percent. We first find that for the STT-RAM cache, although the better-performing stream-based data prefetch configuration can improve the processor performance, the amount of prefetching data also increases obviously, which has negative effect on the processor performance because of the cache congestion caused by STT-RAM’s long write latency. This paper presents a novel stream-based prefetch method ANCP (Adaptive Non-volatile Cache Prefetch) to reduce the amount of prefetch-write on STT-RAM. Compared with the best-performing stream-based data prefetch configuration, ANCP reduces the prefetching issued by 11 percent on average, which obtains additional up to 8 percent performance improvement of the AIoT processor."
pub.1131108405,A Real-Time Feature Indexing System on Live Video Streams,"Most of the existing video storage systems rely on offline processing to support the feature-based indexing on video streams. The feature-based indexing technique provides an effective way for users to search video content through visual features, such as object categories (e.g., cars and persons). However, due to the reliance on offline processing, video streams along with their captured features cannot be searchable immediately after video streams are recorded. According to our investigation, buffering and storing live video steams are more time-consuming than the YOLO v3 object detector. Such observation motivates us to propose a real-time feature indexing (RTFI) system to enable instantaneous feature-based indexing on live video streams after video streams are captured and processed through object detectors. RTFI achieves its real-time goal via incorporating the novel design of metadata structure and data placement, the capability of modern object detector (i.e., YOLO v3), and the deduplication techniques to avoid storing repetitive video content. Notably, RTFI is the first system design for realizing real-time feature-based indexing on live video streams. RTFI is implemented on a Linux server and can improve the system throughput by upto 10.60x, compared with the base system without the proposed design. In addition, RTFI is able to make the video content searchable within 20 milliseconds for 10 live video streams after the video content is received by the proposed system, excluding the network transfer latency."
pub.1003310146,Building Engines and Platforms for the Big Data Age,"Big data analytics involves the collection of real-time operational data into large clusters, followed by the execution of analytics queries to derive insights from the data. The results of these insights are periodically deployed into the real-time pipeline, in order to perform business actions or raise alerts. We are currently witnessing a move towards fast data analytics, where some of the offline activities may be performed in memory, directly over the real-time input streams, in order to reduce the time taken to derive and exploit insights from the data. Further, there is an increasing emphasis on enabling data scientists to derive quick approximate insights from large volumes of offline data interactively and at low cost, i.e., without having to process the entire dataset each time. Such hybrid and interconnected workflows across offline and real-time data, stored and processed across multiple machines, and with varying latency needs and complex application logic, requires a rethinking of both data and query processing models and software artifacts that realize such models. This paper surveys the challenges and requirements created by such workflows, and summarizes our research efforts on addressing these problems."
pub.1164237037,Gpu-based and streaming-enabled implementation of pre-processing flow towards enhancing optical character recognition accuracy and efficiency,"Research has demonstrated that digital images can be pre-processed through operations such as scaling, rotation, and blurring to enhance the accuracy of optical character recognition (OCR) by  emphasizing important features within the image. Our study employed the open-source Tesseract OCR and found that accuracy can be improved through pre-processing techniques including thresholding, rotation, rescaling, erosion, dilation, and noise removal, based on a dataset of 560 phone screen images. However, our CPU-based implementation of this process resulted in an average latency of 48.32 ms per image, which can hinder the processing of millions of images using OCR.  To address this challenge, we parallelized the pre-processing flow on the Nvidia P100 GPU and executed it through a streaming approach, which reduced the latency to 0.825 ms and achieved a speedup factor of 58.6x compared to the serial execution. This implementation enables the use of a GPU-based OCR engine to handle multiple sources of data streams with large-scale workloads."
pub.1173571348,Edge2LoRa: Enabling edge computing on long-range wide-area Internet of Things,"Long-Power Wide Area Networks (LPWAN) is a low-cost solution to deploy very-large scale Internet of Things (IoT) infrastructures with minimal requirements following a classic producer/consumer model. Inevitably such deployments will require a shift towards low-latency, distributed and collaborative data aggregation models. The cloud edge computing continuum (CECC) has been proposed as an evolution of the traditional central ultra-high-end processing cloud into a continuum of collaborative processing elements distributed from the cloud to the network edge. Until today, incorporating existing centralized and monolithic LPWAN architectures in the CECC faces multiple security-related implications. We propose Edge2LoRa, a complete secure solution to incorporate LPWAN architectures in CECC enabling faster data processing while reducing the transmission of sensitive data. It improves network performance through data pre-processing, traffic flow optimization, and real-time local analysis. Edge2LoRa gradually transform existing LPWAN deployments into agile and versatile infrastructures that enable the seamless and efficient processing of data throughout the CECC while guaranteeing service continuity and full-backwards compatibility. We implement Edge2LoRa in hardware compliant with the Things Stack and the LoRaWAN v1.0.4 and v1.1. We evaluate the performance in terms of networking and computing resource utilization, quality of service and security. The results provide a clear indication of the improvements to public and private LoRaWAN infrastructures without any disruption or service degradation for existing legacy services. In public LoRaWAN deployments where large-scale IoT data streams drive big data analytics, we demonstrate core network bandwidth usage reductions of up to 90% and data processing latency improvements by a × 10 factor."
pub.1012427908,Placement of distributed stream processing over heterogeneous infrastructures,"Data Stream Processing (DSP) applications can extract, in a timely manner, valuable information from distributed data sources (e.g., sensing devices, social networks). These applications are subject to unpredictable and varying workloads and have to satisfy strict quality requirements, usually expressed in terms of latency, availability, and throughput. To successfully execute DSP applications, recent trends investigate the possibility of exploiting decentralized computing resources, which nonetheless pose new challenges due to the network and system heterogeneity, geographic distribution, and non-negligible network latencies. The doctorate work, presented in this paper, investigates the deployment of DSP applications with Quality of Service (QoS) requirements over a distributed infrastructure of heterogeneous computing and networking resources. Specifically, to support our study, we extend an open-source DSP system, Apache Storm, by providing mechanisms for executing distributed QoS-aware placement policies and self-adaptation. Then, we provide a general formulation of the optimal placement problem for DSP applications, modeling the heterogeneity of the execution environment. The ongoing research aims at investigating the following directions. First, we will design heuristics able to determine the best placement in a feasible amount of time. Second, we will investigate runtime adaptation strategies and online placement algorithms. Third, to prove the generality of our approach, we will customize the designed solutions for similar problems (e.g., service selection, container deployment)."
pub.1132594491,StrObe: Streaming Object Detection from LiDAR Packets,"Many modern robotics systems employ LiDAR as their main sensing modality due
to its geometrical richness. Rolling shutter LiDARs are particularly common, in
which an array of lasers scans the scene from a rotating base. Points are
emitted as a stream of packets, each covering a sector of the 360{\deg}
coverage. Modern perception algorithms wait for the full sweep to be built
before processing the data, which introduces an additional latency. For typical
10Hz LiDARs this will be 100ms. As a consequence, by the time an output is
produced, it no longer accurately reflects the state of the world. This poses a
challenge, as robotics applications require minimal reaction times, such that
maneuvers can be quickly planned in the event of a safety-critical situation.
In this paper we propose StrObe, a novel approach that minimizes latency by
ingesting LiDAR packets and emitting a stream of detections without waiting for
the full sweep to be built. StrObe reuses computations from previous packets
and iteratively updates a latent spatial representation of the scene, which
acts as a memory, as new evidence comes in, resulting in accurate low-latency
perception. We demonstrate the effectiveness of our approach on a large scale
real-world dataset, showing that StrObe far outperforms the state-of-the-art
when latency is taken into account, and matches the performance in the
traditional setting."
pub.1106802582,Constant-Time Sliding Window Framework with Reduced Memory Footprint and Efficient Bulk Evictions,"The fast evolution of data analytics platforms has resulted in an increasing demand for real-time data stream processing. From Internet of Things applications to the monitoring of telemetry generated in large data centers, a common demand for currently emerging scenarios is the need to process vast amounts of data with low latencies, generally performing the analysis process as close to the data source as possible. Stream processing platforms are required to be malleable and absorb spikes generated by fluctuations of data generation rates. Data is usually produced as time series that have to be aggregated using multiple operators, being sliding windows one of the most common abstractions used to process data in real-time. To satisfy the above-mentioned demands, efficient stream processing techniques that aggregate data with minimal computational cost need to be developed. In this paper we present the Monoid Tree Aggregator general sliding window aggregation framework, which seamlessly combines the following features: amortized $O(1)$ time complexity and a worst-case of $O(\log {n})$ between insertions; it provides both a window aggregation mechanism and a window slide policy that are user programmable; the enforcement of the window sliding policy exhibits amortized $O(1)$ computational cost for single evictions and supports bulk evictions with cost $O(\log {n})$; and it requires a local memory space of $O(\log {n})$. The framework can compute aggregations over multiple data dimensions, and has been designed to support decoupling computation and data storage through the use of distributed Key-Value Stores to keep window elements and partial aggregations."
pub.1111015730,Hardware for Accelerating Anonymization Transparent to Network,"With the increase in the number of Internet of Things (IoT) devices, many applications using the data accumulated from IoT devices have been proposed. Consequently, privacy violation due to data disclosure is becoming a problem. Data anonymization is a technique for publishing data while preserving data privacy and is an effective method for solving the problem of privacy violation. There are several requirements for implementing anonymization systems suitable for IoT devices, where the amount of hardware and software resources, function extensibility, and processing throughput are limited. To satisfy these requirements, anonymization hardware for anonymizing data packets from IoT devices is proposed. The proposed hardware is allocated on the intermediate location of network and anonymizes the data packet stream from IoT devices transparently. The proposed anonymization process does not affect data communication or routing. This transparency allows an additional anonymization function to be installed on all types of IoT devices without modifying the devices. The proposed hardware allows IoT devices to anonymize the data packet stream. To reduce the latency and achieve transparent anonymization at a low cost, the proposed anonymization hardware was implemented using a field-programmable gate array. The throughput and latency of the testbed of the proposed hardware were 1,286 Mbps and 330 ns, respectively. The power consumption of the proposed hardware was lower than that for a software implementation."
pub.1130822066,Meet me halfway,"From Industry 4.0-driven factories to real-time trading algorithms, businesses depend on analytics on high-velocity real-time data. Often these analytics are performed not in dedicated stream processing engines but on views within a general-purpose database to combine current with historical data. However, traditional view maintenance algorithms are not designed with both the volume and velocity of data streams in mind.  In this paper, we propose a new type of view specialized for queries involving high-velocity inputs, called continuous view. The key component of continuous views is a novel maintenance strategy, splitting the work between inserts and queries. By performing initial parts of the view's query for each insert and the remainder at query time, we achieve both high input rates and low query latency. Further, we keep the memory overhead of our views small, independent of input velocity. To demonstrate the practicality of this strategy, we integrate continuous views into our Umbra database system. We show that split maintenance can outperform even dedicated stream processing engines on analytical workloads, all while still offering similar insert rates. Compared to modern materialized view maintenance approaches, such as deferred and incremental view maintenance, that often need to materialize expensive deltas, we achieve up to an order of magnitude higher insert throughput. "
pub.1111725959,A Hardware-Oriented Algorithm for Ultra-High-Speed Object Detection,"This paper describes a novel hardware-oriented algorithm that can be implemented on a field-programmable gate array in a high-speed vision platform for detection of multiple objects with clear texture information in images of $512\times 512$ pixels at 10 000 frames per second (fps) under complex background. The proposed algorithm is specially designed for devices with limited hardware resource for high-frame-rate, high-data-throughput, and high-parallelism processing of video streams with low latency. The proposed algorithm is based on the conventional histograms of oriented gradient (HOG) descriptor and support vector machine classifier algorithms. Considering the trade-off between speed and accuracy, many hardware-based optimization operations were implemented. The data throughput is nearly 29.30 Gbps while the latency for feature extraction is 0.76 us (61 clock period). After hardware-based image processing, the source image and the detected object features can be transferred to a personal computer for recording or post-processing at 10 000 fps. Several experiments were done to demonstrate the performance of our proposed algorithm for ultra-high-speed moving object detection with clear texture information in images."
pub.1134955799,Normalized Stability,"Stochastic computing is a statistical computing scheme that represents data as serial bit streams to greatly reduce hardware complexity. The key trade-off is that processing more bits in the streams yields higher computation accuracy at the cost of more latency and energy consumption. To maximize efficiency, it is desirable to account for the error tolerance of applications and terminate stochastic computations early when the result is acceptably accurate. Currently, the stochastic computing community lacks a standard means of measuring a circuit's potential for early termination and predicting at what cycle it would be safe to terminate. To fill this gap, we propose normalized stability, a metric that measures how fast a bit stream converges under a given accuracy budget. Our unit-level experiments show that normalized stability accurately reflects and contrasts the early-termination capabilities of varying stochastic computing units. Furthermore, our application-level experiments on low-density parity-check decoding, machine learning and image processing show that normalized stability can reduce the design space and predict the timing to terminate early."
pub.1182158567,Heterogeneous Online Computational Platform for GEM-Based Plasma Impurity Monitoring Systems,"The fusion energy research field presents many intricate challenges that require resolution. Many diagnostic systems employed in experiments are approaching the limits of current technology. Implementing efficient measurements requires using an appropriate set of tools to facilitate the optimal utilization of hardware. Fusion energy measurements must provide low latency processing with the capacity for future improvements and the ability to handle complex data flows efficiently. The presented work addresses these requirements and describes the implementation of a high-performance, low-latency software platform with convenient development for soft X-ray (SXR) plasma impurities emission tracing—the Asynchronous Complex Computation Platform (AC2P). This article presents the architectural design, implementation details, and performance and latency measurements based on the raw data acquired from the WEST tokamak and laboratory tests. AC2P provides the tools to develop low-latency, multi-core, multi-device complex data flow graph scale-up solutions for measuring impurities in hot plasmas. The system has been designed to operate online during experiments, calculate the energy distribution, position and occurrence time of SXR photons, monitor the data stream’s quality and archive any abnormalities for subsequent offline verification and algorithm improvement. This article presents AC2P, which operates as part of the SXR measurement system on the WEST tokamak."
pub.1029507839,Experience in Continuous analytics as a Service (CaaaS),"Mobile applications, such as those on WebOS, increasingly depend on continuous analytics results of real-time events, for monitoring oil & gas production, watching traffic status and detecting accident, etc, which has given rise to the need of providing Continuous analytics as a Service (CaaaS). While representing a paradigm shift in cloud computing, CaaaS poses several challenges in scalability, latency, time-window semantics, transaction control and result-set staging. A data stream is infinite thus can only be analyzed in granules. We propose a continuous query model over both static relations and dynamic streaming data, which allows a long-standing SQL query instance to run cycle by cycle, each cycle for a chunk of data from the data stream, using a cut-and-rewind mechanism. We further support the cycle-based transaction model with cycle-based isolation and visibility, for delivering analytics results to the clients continuously while the query is running. To have the continuously generated analytics results staged efficiently, we developed the table-ring and label switching mechanism characterized by staging data through metadata manipulation without physical data moving and copying. To scale-out analytics computation, we support both parallel database based and network distributed Map-Reduce based infrastructure with multiple cooperating engines. We have built the proposed infrastructure by extending the PostgreSQL engine. We tested the throughput and latency of this service based on a well-known stream processing benchmark; the results show that the proposed approach is highly competitive. Our experiments indicate that the database technology can be extended and applied to real-time continuous analytics service provisioning."
pub.1086120107,FlinkMan,"We present a (soft) real-time event-based anomaly detection application for manufacturing equipment, built on top of the general purpose stream processing framework Apache Flink. The anomaly detection involves multiple CPUs and/or memory intensive tasks, such as clustering on large time-based window and parsing input data in RDF-format. The main goal is to reduce end-to-end latencies, while handling high input throughput and still provide exact results. Given a truly distributed setting, this challenge also entails careful task and/or data parallelization and balancing. We propose FlinkMan, a system that offers a generic and efficient solution, which maximizes the usage of available cores and balances the load among them. We illustrates the accuracy and efficiency of FlinkMan, over a 3-step pipelined data stream analysis, that includes clustering, modeling and querying."
pub.1130946622,Data Traffic Based on Slicing in a 5G Smart Uplink System,"The paper considers the data traffic based on slicing in a 5g mobile network uplink system. Slicing is a promising technology for the fifth generation of networks that provides optimal quality of QOS services for each specific user or group of users. Data traffic that is processed by cellular networks increases every year. Therefore, we should consider all set of traffic from VoIP to M2M devices. For example, smart devices in the healthcare system transmit big data that is sensitive to latency, but also a video stream that requires minimal latency in certain cases. The paper focuses on the successful processing of traffic through a relay node, donor microstates, and a base station. 
All traffic is divided into three levels of QoS segmentation: sensitive, less sensitive, and low-sensitivity, using the AnyLogic simulation program.
For fifth-generation 5G networks, achieving minimum latency and maximum data transfer speed within QoS is an important implementation condition. Therefore, in this paper, using simulation modeling, the main and possible results of each segment in the new generation of mobile networks are obtained.
The use of a relay node in conjunction with micro-stations can ensure optimal station load and successful data processing. Also, the solutions outlined in this paper will allow you to identify a number of areas for future research to assess possible ways to design new mobile networks, or improve existing ones."
pub.1132902830,Toward In-Network Event Detection and Filtering for Publish/Subscribe Communication Using Programmable Data Planes,"Industrial Internet of Things (I-IoT) applications require a large number of sensor data to be processed under tight delay and jitter constraints. In such applications, flexible event detection and fast reaction to critical events is an important building block. Traditional approaches use either proprietary networks and dedicated hardware or transmit sensor data towards processing elements in the Cloud or at the Network Edge, using distributed stream processing frameworks. For scalability, a large number of servers are needed and processing on commodity CPUs typically involves high and unpredictable latency. In this article, we explore how programmable data planes can be used to detect events flexibly and trigger customized and programmable actions directly from the switch program or the programmable network interface card (SmartNIC). We present FastReact-PS, an event-based publish/subscribe I-IoT processing framework in P4 language, which can be flexibly customized from the control plane. Together with stateful processing, FastReact-PS supports windowed time series analysis as well as complex event detection and processing based on Boolean logic directly in the data plane of newly emerging programmable networking devices. The logic can be adjusted dynamically from the control plane without the need for recompilation. We implement FastReact-PS in P4 and evaluate it on both a SmartNIC and a DPDK-based software switch running in user space. Our evaluation shows that the latency is reduced by one order of magnitude compared to end-host based approaches at significantly lower jitter while being scalable to processing up to 11 million events per second."
pub.1165098561,A Window into the Multiple Views of Linked Data,"RDF mapping engines enable access to existing heterogeneous data sources as RDF Knowledge Graph (KG). However, these mapping engines have two challenges: i) processing streaming data sources with changing velocity efficiently, ii) and providing a rich variety in the format of the generated KG output. To tackle these challenges, I carry out my research in 3 steps. I will first design a highly scalable data stream mapping solution to handle dynamic velocity of streaming data sources. Preliminary results indicate that our stream mapping solution outperforms state of the art engines with lower latency, constant memory usage, and higher throughput. I will then refine this architecture in a task-based fashion, aiming to be a common architecture for any kind of mapping. Finally, I will utilize the common modular mapping architecture and extend it with a component to derive an intermediate representation of the data mapping process, enabling heterogeneous to heterogeneous data mapping. The combined solution will provide a highly scalable heterogeneous to heterogeneous data stream mapping engine, enabling us to have multiple views of the underlying KG."
pub.1153498382,Data Synchronization in Non-Uniform Latency Custom DSP Designs,"Many DSP algorithms for streams of constant-sample-rate data work by processing the signal by pieces, splitting the input signal into fixed-length blocks in order to calculate some parameter related to that chunk of signal. In custom multiblock DSP architectures, this kind of operations are laid out to be carried out in parallel. This poses a challenge since these operations may have greatly different latency values, so the data needs to be synchronized somehow for the following DSP block to acknowledge the results of the operations and keep on with the computation. In our proposal, we monitor the operations in order to synchronize the output data by storing the results until all operations are done. Every output is queued up in a FIFO structure so that when all the operations are complete the results can then be transferred to the next module simultaneously. The control structure is built so that the architecture also works when the signal blocks have variable length, or when the operations have variable latency. This approach allows for a reduction in the number of registers needed for a delay chain implementation, it is more flexible if the latency of the operations change, and can be easily updated if new operations are introduced. This architecture has been used and extensively tested in an Automatic Modulation Classifier implementation on an FPGA as part of a signal processing chain with parallel calculations of linear regressions and central moments of a constant-rate signal divided into blocks of different lengths."
pub.1149355685,Exploring System and Machine Learning Performance Interactions when Tuning Distributed Data Stream Applications,"Deploying machine learning (ML) applications over distributed stream processing engines (DSPEs) such as Apache Spark Streaming is a complex procedure that requires extensive tuning along two dimensions. First, DSPEs have a vast array of system configuration parameters (such as degree of parallelism, memory buffer sizes, etc.) that need to be optimized to achieve the desired levels of latency and/or throughput. Second, each ML model has its own set of hyper-parameters that need to be tuned as they significantly impact the overall prediction accuracy of the trained model. These two forms of tuning have been studied extensively in the literature but only in isolation from each other. This position paper identifies the necessity for a combined system and ML model tuning approach based on a thorough experimental study. In particular, experimental results have revealed unexpected and complex interactions between the choices of system configuration and hyper-parameters, and their impact on both application and model performance. These findings open up new research directions in the field of self-managing stream processing systems."
pub.1119046041,Scaling Ordered Stream Processing on Shared-Memory Multicores,"Many modern applications require real-time processing of large volumes of
high-speed data. Such data processing needs can be modeled as a streaming
computation. A streaming computation is specified as a dataflow graph that
exposes multiple opportunities for parallelizing its execution, in the form of
data, pipeline and task parallelism. On the other hand, many important
applications require that processing of the stream be ordered, where inputs are
processed in the same order as they arrive. There is a fundamental conflict
between ordered processing and parallelizing the streaming computation. This
paper focuses on the problem of effectively parallelizing ordered streaming
computations on a shared-memory multicore machine.
  We first address the key challenges in exploiting data parallelism in the
ordered setting. We present a low-latency, non-blocking concurrent data
structure to order outputs produced by concurrent workers on an operator. We
also propose a new approach to parallelizing partitioned stateful operators
that can handle load imbalance across partitions effectively and mostly avoid
delays due to ordering. We illustrate the trade-offs and effectiveness of our
concurrent data-structures on micro-benchmarks and streaming queries from the
TPCx-BB benchmark. We then present an adaptive runtime that dynamically maps
the exposed parallelism in the computation to that of the machine. We propose
several intuitive scheduling heuristics and compare them empirically on the
TPCx-BB queries. We find that for streaming computations, heuristics that
exploit as much pipeline parallelism as possible perform better than those that
seek to exploit data parallelism."
pub.1061546129,Proxy-based multimedia signaling scheme using RTSP for seamless service mobility in home network,"In the ubiquitous network environment, the home network should be able to support seamless service mobility with fast signal processing before transporting data stream for providing multimedia services. In this paper, we propose a proxy-based multimedia signaling scheme using real time streaming protocol (RTSP) in order to support fast signaling in the home network. Implementation on test bed shows that the proposed scheme has the better performance than RTSP in terms of the signaling latency time."
pub.1119739854,High-Throughput and Low-Latency Digital Baseband Architecture for Energy-Efficient Wireless VR Systems,"This paper presents a novel baseband architecture that supports high-speed wireless VR solutions using 60 GHz RF circuits. Based on the experimental observations by our previous 60 GHz transceiver circuits, the efficient baseband architecture is proposed to enhance the quality of transmission. To achieve a zero-latency transmission, we define an (106,920, 95,040) interleaved-BCH error-correction code (ECC), which removes iterative processing steps in the previous LDPC ECC standardized for the near-field wireless communication. Introducing the block-level interleaving, the proposed baseband processing successfully scatters the existing burst errors to the small-sized component codes, and recovers up to 1080 consecutive bit errors in a data frame of 106,920 bits. To support the high-speed wireless VR system, we also design the massive-parallel BCH encoder and decoder, which is tightly connected to the block-level interleaver and de-interleaver. Including the high-speed analog interfaces for the external devices, the proposed baseband architecture is designed in 65 nm CMOS, supporting a data rate of up to 12.8 Gbps. Experimental results show that the proposed wireless VR solution can transfer up to 4 K high-resolution video streams without using time-consuming compression and decompression, successfully achieving a transfer latency of 1 ms."
pub.1062983809,Preferential Resource Allocation in Stream Processing Systems,"Overloaded data stream management systems (DSMS) cannot process all tuples within their response time. For some DSMS it is crucial to allocate the precious resources to process the most significant tuples. Prior work has applied shedding and spilling to permanently drop or temporarily place to disk insignificant tuples. However neither approach considers that tuple significance can be multi-tiered nor that significance determination can be costly. These approaches consider all tuples not dropped to be equally significant. Unlike these prior works, we take a fresh stance by pulling the most significant tuples forward throughout the query pipeline. Proactive Promotion (PP), a new DSMS methodology for preferential CPU resource allocation, selectively pulls the most significant tuples ahead of less significant tuples. Our optimizer produces an optimal PP plan that minimizes the processing latency of tuples in the most significant tiers in this multi-tiered precedence scheme by strategically placing significance determination operators throughout the query pipeline at compile-time and by agilely activating them at run-time. Our results substantiate that PP lowers the latency and increases the throughput for significant results when compared to the state-of-the-art shedding and traditional DSMS approaches (between 2 and 18 fold for a rich diversity of datasets) with negligible overhead."
pub.1094689915,Data Parallelism for Distributed Streaming Applications,"Streaming applications can analyze vast data streams and requires both high throughput and low latency. They are comprised of operator graphs which produce and consume data tuples where operators are stateful, selective and user-defined. The streaming programming model logically exposes task and pipeline parallelism, enabling it to develop parallel systems. Naturally it doesnot expose data parallelism, which must be extracted from streaming applications. This paper presents a compiler and runtime system that automatically extract data parallelism for distributed stream processing. Our approach is safety guarantee in presence of stateful, selective and user-defined operators. Data parallelization is secure if the sequential semantics of the applications are preserved, also the compiler ensures safety by considering dependencies on other operators in the graph and selectivity, state, partitioning of operator. The distributed runtime system ensures that tuples always exit parallel regions in the same order they would without data parallelism, using the most efficient strategy as identified by the compiler."
pub.1038698990,An empirical study on implementing highly reliable stream computing systems with private cloud,"Stream computing systems are designed for high frequency data. Such systems can deal with billions of transactions per day in real cases. Cloud technology can support distributed stream computing systems by its elastic and fault tolerant capabilities. In a real deployment environment, such as the pre-treatment system in Chinese top banks, the reliability based on user experience is key metrics for performance. Although many significant works have been proposed in the literature, they have some limitations such as less of architectural focus or difficult to implement in complex projects. This paper describes the reliability issue which is caused by the service downgrade in cloud. We use novel reliability analysis techniques, queuing theory, and software rejuvenation management techniques to build a framework for supporting stream data with low latency and fault tolerance. A real streaming system from a top bank is studied to provide the supporting data. Operational parameters such as rejuvenation window and time-out parameter are identified as key parameters for the design of a distributed stream processing system. An algorithm for reliability optimization, monitoring and forecast is also introduced. The paper also compares the improved result with original issues, which saved millions of money and reputations."
pub.1120058606,Stream-IT: Continuous and dynamic processing of production systems data - throughput bottlenecks as a case-study,"Considering the needs for continuous availability of information out of data generated in Cyber-Physical production systems, we investigate the use of continuous stream processing as a paradigm for generating useful information out of the data, to support efficient and safe operation, as well as planning activities. Our contributions and expected benefits: (i) we show possibilities to automate and pipeline the validation and analysis of the data, hence providing an automated way to improve the quality of the latter and parallelizing the two phases; (ii) we show how to induce lower latency in generating the desired information, enabling it to be continuously made available, before whole batches of data are gathered, in cost-efficient ways; (iii) besides the automation of the above procedures that are commonly done in a batch fashion and with significant manual effort by the production system analysts, we show additional options for configuring ways in which to automate deeper analysis of the data; in particular, we provide evidences about how the rich semantics of stream processing frameworks can ease the development and deployment of data analysis applications in production systems. Moreover, using the problem of bottleneck detection as a sample scenario, we illustrate the above in a concrete fashion, on cost-efficient systems, that are plausible to have in existing deployments. The experimental study is on a 2-year data-set with more than 8.5 million entries, from a system including more than 30 interconnected machines and it demonstrates the benefits of the proposed methods, in providing timely and multidimensional information from the data, enabling possibilities for deeper analyses."
pub.1094432984,MSSM: An efficient scheduling mechanism for CUDA basing on task partition,"This paper presents a multiple stream scheduling mechanism to enable parallel execution of kernels, data sending from host to device and data receiving from device to host with multiple streams in CUDA. Our mechanism can divide the kernels and bi-directional data transmission into small subtasks, and allow to easily and efficiently overlap them on the CUDA compatible graphic processing unit(GPU). To set the optimal subtask size, we have built one compute bound model for computing intensive application and one data bound model for bi-directional data transmission intensive application. Basing on the two models, we also provided three scheduling algorithms for data dependent and data independent applications to maximize the efficiency of the overlap. We have applied the mechanism to a set of benchmarks to understand the performance. The results show that our work can successfully hide the latency to achieve high performance which is very close to the optimal."
pub.1003319346,Real-time analysis of social networks leveraging the flink framework,"In this paper, we present a solution to the DEBS 2016 Grand Challenge that leverages Apache Flink, an open source platform for distributed stream and batch processing. We design the system architecture focusing on the exploitation of parallelism and memory efficiency so to enable an effective processing of high volume data streams on a distributed infrastructure. Our solution to the first query relies on a distributed and fine-grain approach for updating the post scores and determining partial ranks, which are then merged into a single final rank. Furthermore, changes in the final rank are identified so to update the output only if needed. The second query efficiently represents in-memory the evolving social graph and uses a customized Bron-Kerbosch algorithm to identify the largest communities active on a topic. We leverage on an in-memory caching system to keep the largest connected components which have been previously identified by the algorithm, thus saving computational time. The experimental results show that, on a portion of the dataset large half that provided for the Grand Challenge, our system can process up to 400 tuples/s with an average latency of 2.5 ms for the first query, and up to 370 tuples/s with an average latency of 2.7 ms for the second query."
pub.1121129421,A Diffserv Model for Video Stream,"Video plays an important role in security monitoring, intelligent traffic surveillance, streaming media and other fields. As the internet technology evolves, especially increasing of 4G network coverage further expands the scope of its application. As a kind of multimedia service with strong time correlation; video surveillance not only needs to occupy larger network bandwidth, but also requires very strict network transmission delay. This leads to many problems of video stream such as poor picture quality and long latency due to the network bandwidth and limited device processing capacity. To solve the problems above, a kind of quality of service model named Diffserv is proposed in this paper. It concludes pockets classification, traffic monitoring, congestion avoidance, network congestion management etc. Meanwhile the network test of video data stream is carried out and the test results are given."
pub.1173221233,EON-1: A Brain-Inspired Processor for Near-Sensor Extreme Edge Online Feature Extraction,"For Edge AI applications, deploying online learning and adaptation on
resource-constrained embedded devices can deal with fast sensor-generated
streams of data in changing environments. However, since maintaining
low-latency and power-efficient inference is paramount at the Edge, online
learning and adaptation on the device should impose minimal additional overhead
for inference. With this goal in mind, we explore energy-efficient learning and
adaptation on-device for streaming-data Edge AI applications using Spiking
Neural Networks (SNNs), which follow the principles of brain-inspired
computing, such as high-parallelism, neuron co-located memory and compute, and
event-driven processing. We propose EON-1, a brain-inspired processor for
near-sensor extreme edge online feature extraction, that integrates a fast
online learning and adaptation algorithm. We report results of only 1% energy
overhead for learning, by far the lowest overhead when compared to other SoTA
solutions, while attaining comparable inference accuracy. Furthermore, we
demonstrate that EON-1 is up for the challenge of low-latency processing of HD
and UHD streaming video in real-time, with learning enabled."
pub.1146546959,Anomaly Detection in Catalog Streams,"Detecting anomalies with high accuracy and real time from large amounts of streaming data is a challenge for many real-world applications, such as smart city, astronomical observations, and remote sensing. This article focuses on a special kind of stream, catalog stream, whose high-level catalog structure can be used to analyze the stream effectively. We first formulate the anomaly detection in catalog streams as a constrained optimization problem based on a catalog stream matrix. Then, a novel filtering-identifying based anomaly detection algorithm (FIAD) is proposed, which includes two complementary strategies, true event identifying and false alarm filtering, data-oriented general method and domain-oriented specific method together, to detect truly valuable anomalies. Furthermore, different kinds of attention windows are developed to provide corresponding data for various algorithm components. A scalable and lightweight catalog stream processing framework CSPF is designed to support and implement the proposed method efficiently. A prototype system is developed to evaluate the proposed algorithm. Extensive experiments are conducted on the catalog stream data sets from an operational super large field-of-view high-cadence astronomy observation. The experimental results show that the proposed method can achieve a false-positive rate as low as 0.04%, reduces the false alarms by 98.6% compared with the existing methods, and the latency to handle each catalog is 2.1 seconds (much less than the required 15 seconds). Furthermore, a total of 36 transient candidates, including seven microlensing events, 27 superflares, and two dual-superflares, are detected from 21.67 million stars (involving 1.09 million catalogs) from one observation season."
pub.1025578867,Real‐time earthquake detection and hazard assessment by ElarmS across California," ElarmS is a network‐based methodology for rapid earthquake detection, location and hazard assessment in the form of magnitude estimation and peak ground motion prediction. The methodology is currently being tested as part of the real‐time seismic system in California leveraging the resources of the California Integrated Seismic Network (CISN) and the Advanced National Seismic System. A total of 603 velocity and acceleration sensors at 383 sites across the state stream waveform data to ElarmS processing modules at three network processing centers where waveforms are reduced to a few parameters. These parameters are then collected and processed at UC Berkeley to provide a single statewide prediction of future ground shaking that is updated every second. The system successfully detected the M w 5.4 Alum Rock earthquake in northern California for which it generated an accurate hazard prediction before peak shaking began in San Francisco. It also detected the M w 5.4 Chino Hills earthquake in southern California. The median system latency is currently 11.8 sec; the median waveform data latency is 6.5 sec. "
pub.1093380107,A Fast Carrier Acquisition Architecture for High-Dynamic Weak Signal Based on GPU,"In this paper, we present a graphics processing unit (GPU)-based architecture of carrier acquisition for high-dynamic weak signal in deep space communications. To achieve high performance, the carrier acquisition procedure is parallelized by exploiting the GPU's parallel operating characteristics. Based on computer unified device architecture (CUDA), different kernels are designed to map the different phases of carrier acquisition procedure. What's more, the kernels' efficiency are improved by optimizing the internal operation parallelism of kernels and lowering the memory access latency for threads. Besides, multiple CUDA streams are designed to hide the data transfer latency between host and device. Experimental results demonstrate that the proposed GPU-based architecture achieves more than 250.3 times speedup compared to CPU-based platform."
pub.1132666234,"Low Latency, Online Processing of the High-Bandwidth Bunch-By-Bunch Observation Data From the Transverse Feedback System in the LHC","During long shutdown 2 (2019-2020) the transverse observation system (ADTObsBox) in the LHC will undergo a substantial upgrade. The purpose of this upgrade is to allow for true low latency, online processing of the 16 data-streams of transverse bunch-by-bunch, turn-by-turn positional data provided by the transverse feedback system in the LHC (ADT). This system makes both offline and online analysis of the data possible, where the emphasis will lie on online analysis, something that the older generation was not designed to provide. The result of the analysis is made available for accelerator physicists, machine operators, and engineers working with LHC. The new system allows users to capture buffers of various lengths for later analysis just like the older generation and it provides a platform for real-time analysis applications to directly capture the data with minimal latency while also providing a heterogeneous computing platform where the applications can utilize CPUs, GPUs and dedicated FPGAs. The analysis applications include bunch-by-bunch instability analysis and passive bunch-by-bunch tune extraction to name a few. The ADTObsBox system uses commodity server technology in combination with FPGA-based PCIe I/O cards. This paper will cover the design and status of the I/O cards, server, firmware, driver, analysis applications and results of early performance testing."
pub.1154887454,Optimal Rate Control for Latency-constrained High Throughput Big Data Applications,"High performance distributed systems such as distributed stream processing systems and message-passing parallel programs are often deployed on platforms that make use of vanilla TCP/IP communication, which in turn uses the conventional Nagle’s algorithm for congestion control. Recent research in Reinforcement Learning (RL) techniques to either replace or control the conventional TCP approach shows promise in achieving a greater degree of performance, especially when the demand for network resources in a multi-tenant platform is highly dynamic and infeasible to model. Existing results are, however, focused on RL for general Internet communication, with the learning objective being some combination of throughput, loss, and latency, and predominately use a continuous action space to adjust the packet rate at the sender. In this work, we propose a coefficient-free RL objective that perfectly matches the data transmission rate to the underlying communication system’s bottleneck, which naturally deters packet loss and thereby converges to the ideal throughput even in lock-free and latency-constrained Big Data applications where packets are dropped due to load shedding or exceeding latency thresholds. Our results compare favorably to other state-of-the-art objective functions using an RL framework, e.g., providing up to 48% reduction in packet loss while obtaining up to a 4% increase in overall throughput when packet latency is highly constrained."
pub.1181857617,Efficient Top-K Continuous Query Processing Over Sliding Window Model (SWM) Method on Uncertain Data Stream,"Query processing using the Uncertain Data Stream (UDS) can be complex in many technological scenarios due to inconsistencies, unclear information, and interpretation latency. As a result of both the sheer amount of data generated and the rate of change, traditional processing methods are in dire need of an upgrade. UDS consists of a finite set of states known as possible worlds (PW), and enhancing data organization can lead to more accurate extraction of user preferences. The number of possible world instances in UDS grows exponentially, making achieving Top-k query processing quickly a significant challenge. Different methods are available to handle Top-k queries in various types of UDS, and their key concerns include reducing duplicate scans of the entire dataset, enhancing uncertainty computation, and focusing on processing the latest tuple item entry. It appears that there have been limited studies conducted on the issue of UDS using the Sliding Window Model (SWM). The current approach for handling continuous queries on UDS within the SWM has proven to be ineffective, resulting in complex trade-offs between maximizing probability and generating high-scoring result sets. The challenge is to find the correct result list that satisfies a Top-k query predicate with scoring and probability. This study proposes a framework for processing Top-k queries for UDS using the sliding window model to improve efficiency. The study also discusses an improved optimization method for reducing computational redundancy in the context of the sliding window model and Top-k query processing. Overall, this research will significantly contribute to the Top-k computational query processing field."
pub.1132552073,Executing Complex Calculations in the Cloud to Enable Real-Time Data Processing,"Complex and time-consuming calculations usually cannot be processed on smartphones in a real time manner, as recent researches show. For these calculations a high-end mobile hardware setup is needed. This creates a high latency which is a drawback as the results cannot be visualised during the evaluation process upfront. This paper proposes a system for spreading the compute-intense calculations on high performance computing units to a cloud computing architecture by wrapping the complex calculations into native function calls and exposing them via a Representational State Transfer (REST) service. This system makes it possible to expose the Dynamically Linked Library (DLL) via Application Programming Interface (API) endpoints creating a reliable low latency Transport Communication Protocol (TCP) connection. Our paper shows that spreading the compute-intense calculations on high end computing units leads to reduction of computation time. This enables the user to reliably stream the results back in real time for further processing (e.g. visualisation)."
pub.1106816325,StreamDB: A Unified Data Management System for Service-Based Cloud Application,"Current data management systems are mainly divided into two categories: Database Management System (DBMS) and Data Stream Management System (DSMS). The increasing use of streaming analysis in modern service-based cloud applications has created an arms race among DBMS vendors to offer ever more sophisticated in-database streaming support, which requires handling the volume, variety, velocity and variability of fast data collections. Unfortunately, current solutions either only provide limited streaming analysis capacity and horizontal scalability (classic RDBMS) or trade off transaction processing for other properties (NoSQL DBMS), leading to the curse of no ""one size fits all"" for DBMS. In this paper, we argue that transaction processing is a relevant concept for DSMS. As a first step toward ""One Size Fits All"" Data Management System, we present StreamDB, which integrates transaction processing in DSMS as opposed to extending DBMS to support streams. First, we describe how StreamDB processes transactions in a streaming environment, then we compare our approach with traditional in-memory DBMS on typical transactional benchmarks. Our results show that StreamDB is advantageous in terms of throughput, scalability, and latency. Finally, we argue that the ideas present here provide insight on the development of next-generation data management systems and motivate further study of the challenges inherent in unifying DBMS and DSMS."
pub.1086118087,Anomaly Detection of Manufacturing Equipment via High Performance RDF Data Stream Processing,"The ACM DEBS Grand Challenge 2017 focuses on anomaly detection of manufacturing equipment. The goal of the challenge is to detect abnormal behavior of a manufacturing machine based on the observations of the stream of measurements provided. The data produced by each sensor is clustered and the state transitions between the observed clusters are modeled as a Markov chain. In this paper we present how we used WSO2 Data Analytics Server (DAS), an open source, comprehensive enterprise data analytics platform, to solve the problem. On the HOBBIT (Holistic Benchmarking of Big Linked Data) platform our solution processed 35 megabytes/second with an end-to-end mean latency of 7.5 ms at an input rate of 1 ms, while the events spent only 1 ms time on average within our grand challenge solution. The paper describes the solution we propose, the experiments' results and presents how we optimized the performance of our solution."
pub.1107851996,Impatience is a Virtue: Revisiting Disorder in High-Performance Log Analytics,"There is a growing interest in processing real-time queries over out-of-order streams in this big data era. This paper presents a comprehensive solution to meet this requirement. Our solution is based on Impatience sort, an online sorting technique that is based on an old technique called Patience sort. Impatience sort is tailored for incrementally sorting streaming datasets that present themselves as almost sorted, usually due to network delays and machine failures. With several optimizations, our solution can adapt to both input streams and query logic. Further, we develop a new Impatience framework that leverages Impatience sort to reduce the latency and memory usage of query execution, and supports a range of user latency requirements, without compromising on query completeness and throughput, while leveraging existing efficient in-order streaming engines and operators. We evaluate our proposed solution in Trill, a high-performance streaming engine, and demonstrate that our techniques significantly improve sorting performance and reduce memory usage - in some cases, by over an order of magnitude."
pub.1012898929,Designing VLSI network nodes to reduce memory traffic in a shared memory parallel computer,"Serialization of memory access can be a critical bottleneck in shared memory parallel computers. The NYU Ultracomputer, a large-scale MIMD (multiple instruction stream, multiple data stream) shared memory architecture, may be viewed as a column of processors and a column of memory modules connected by a rectangular network of enhanced 2×2 buffered crossbars. These VLSI nodes enable the network to combine multiple requests directed at the same memory location. Such requests include a new coordination primitive, fetch- and-add, which permits task coordination to be achieved in a highly parallel manner. Processing within the network is used to reduce serialization at the memory modules.To avoid large network latency, the VLSI network nodes must be high-performance components. Design tradeoffs between architectural features, asymptotic performance requirements, cycle time, and packaging limitations are complex. This report sketches the Ultracomputer architecture and discusses the issues involved in the design of the VLSI enhanced buffered crossbars which are the key element in reducing serialization."
pub.1174190868,TaPaS Co-AIE: An Open-Source Framework for Streaming-Based Heterogeneous Acceleration Using AMD AI Engines,"AMD AI Engines (AIEs) extend the design space and open up new options for coarse-grained processing in re-configurable accelerators. Pure FPGA designs for machine learning often struggle to compete with the high clock frequencies of GPUs for data-intensive workloads with only limited control flow. Having AIEs available on-chip with an FPGA fabric allows for low-latency co-processing and permits parts of an application to be placed on the most suitable kind of processing unit. Many data-heavy workloads, particularly in the AI domain, benefit from data streaming. With TaPaSCo-AIE, we present a framework for heterogeneous systems centered around data streams. Our framework focuses on AMD Versal devices and incorporates AI Engines and 100G network. We demonstrate the efficient use of TaPaSCo-AIE in a real-world evaluation based on a neural network, and achieve significant performance improvements over CPUs, and even exceed the performance of an A100 GPU."
pub.1110349808,Optimal Query-Processing-Node Discovery in IoT-Fog Computing Environment,"With the rapid growth of Internet-connected sensing and control devices, the continuous streams of data generated by them needs to be processed and stored for effective data-driven decision making. Cloud computing is one of the enabling technology that allows data to be shifted to a network of servers on the Internet for storage and processing. But, frequent interactions with cloud infrastructure significantly increases energy consumption and latency particularly in constrained wireless networks. In this paper, we present a fog based query processing technique for sensor devices that allows in-network processing lowering the dependency on cloud. The proposed framework enables a device to avail optimal processing and storage facilities via fog layer that were earlier offered by cloud. In addition, it also facilitates querying of multiple nodes that may be required to be queried to serve a specific query from a device in an optimal way. Lastly, the proposed framework allows to find an optimal node in the network to process the query and provide the required service to the device."
pub.1028599198,R-Storm,"The era of big data has led to the emergence of new systems for real-time distributed stream processing, e.g., Apache Storm is one of the most popular stream processing systems in industry today. However, Storm, like many other stream processing systems lacks an intelligent scheduling mechanism. The default round-robin scheduling currently deployed in Storm disregards resource demands and availability, and can therefore be inefficient at times. We present R-Storm (Resource-Aware Storm), a system that implements resource-aware scheduling within Storm. R-Storm is designed to increase overall throughput by maximizing resource utilization while minimizing network latency. When scheduling tasks, R-Storm can satisfy both soft and hard resource constraints as well as minimizing network distance between components that communicate with each other. We evaluate R-Storm on set of micro-benchmark Storm applications as well as Storm applications used in production at Yahoo! Inc. From our experimental results we conclude that R-Storm achieves 30-47% higher throughput and 69-350% better CPU utilization than default Storm for the micro-benchmarks. For the Yahoo! Storm applications, R-Storm outperforms default Storm by around 50% based on overall throughput. We also demonstrate that R-Storm performs much better when scheduling multiple Storm applications than default Storm."
pub.1100409809,Low-Latency Multi-Threaded Ensemble Learning for Dynamic Big Data Streams,"Real-time mining of evolving data streams involves new challenges when targeting today's application domains such as the Internet of the Things: increasing volume, velocity and volatility requires data to be processed on-the-fly with fast reaction and adaptation to changes. This paper presents a high performance scalable design for decision trees and ensemble combinations that makes use of the vector SIMD and multi core capabilities available in modern processors to provide the required throughput and accuracy. The proposed design offers very low latency and good scalability with the number of cores on commodity hardware when compared to other state-of-the art implementations. On an Intel i7-based system, processing a single decision tree is 6x faster than MOA (Java), and 7x faster than StreamDM (C++), two well-known reference implementations. On the same system, the use of the 6 cores (and 12 hardware threads) available allow to process an ensemble of 100 learners 85x faster that MOA while providing the same accuracy. Furthermore, our solution is highly scalable: on an Intel Xeon socket with large core counts, the proposed ensemble design achieves up to 16x speedup when employing 24 cores with respect to a single threaded execution."
pub.1143578268,Cyber‐physical network architecture for data stream provisioning in complex ecosystems,"Abstract Intelligent fog cyber‐physical social systems (iFog CPSS) is a novel smart city project that uses intrinsic processes to automate microservices such as edge‐to‐fog or fog‐to‐cloud monitoring of complex real‐time activities. This article presents a dynamic cyber‐physical architecture that leverages iFog layers to map location‐based services (LBS) on a spine‐leaf datacenter clos topology. Individual edge clusters are connected to the edge‐fog layer, which communicates with iFog gateways for processing streams' requests. Use‐case application of artificial intelligence (AI) in vehicular ad‐hoc networks (VANETs) is introduced for data stream provisioning. In the validation study, a secure docker‐based iFog CPS experiment is carried out using traffic trace files from C++ modeller. iFog spine‐leaf architecture for fog‐computing and cloud‐computing are compared using two key metrics. For traffic workload utilization, the results show that 83.33% of the traffic workload is utilized at the Fog layer while 16.67% is consumed in the cloud layer. For latency profile, the results indicate that Fog and cloud streams had 20.31% and 77.69%, respectively. In terms of iFog VANET spine‐leaf congestion control, three distinct algorithms are studied, namely the proposed linear routing algorithm (LRA), LEACH, and collection tree protocol (CTP). In each case, the resource utilization for VANET gave 34.45%, 32.18%, and 33.37%, respectively. Latency response gave 11.76%, 82.35%, and 5.89%, respectively. Also, the throughput scenario offered 19.61%, 39.22%, and 41.17%, respectively. Consequently, the iFog scenario offers satisfactory LBS provisioning for VANETs clusters."
pub.1145020805,SkiM: Skipping Memory LSTM for Low-Latency Real-Time Continuous Speech Separation,"Continuous speech separation for meeting pre-processing has recently become a
focused research topic. Compared to the data in utterance-level speech
separation, the meeting-style audio stream lasts longer, has an uncertain
number of speakers. We adopt the time-domain speech separation method and the
recently proposed Graph-PIT to build a super low-latency online speech
separation model, which is very important for the real application. The
low-latency time-domain encoder with a small stride leads to an extremely long
feature sequence. We proposed a simple yet efficient model named Skipping
Memory (SkiM) for the long sequence modeling. Experimental results show that
SkiM achieves on par or even better separation performance than DPRNN.
Meanwhile, the computational cost of SkiM is reduced by 75% compared to DPRNN.
The strong long sequence modeling capability and low computational cost make
SkiM a suitable model for online CSS applications. Our fastest real-time model
gets 17.1 dB signal-to-distortion (SDR) improvement with less than
1-millisecond latency in the simulated meeting-style evaluation."
pub.1119416581,R-Storm: Resource-Aware Scheduling in Storm,"The era of big data has led to the emergence of new systems for real-time
distributed stream processing, e.g., Apache Storm is one of the most popular
stream processing systems in industry today. However, Storm, like many other
stream processing systems lacks an intelligent scheduling mechanism. The
default round-robin scheduling currently deployed in Storm disregards resource
demands and availability, and can therefore be inefficient at times. We present
R-Storm (Resource-Aware Storm), a system that implements resource-aware
scheduling within Storm. R-Storm is designed to increase overall throughput by
maximizing resource utilization while minimizing network latency. When
scheduling tasks, R-Storm can satisfy both soft and hard resource constraints
as well as minimizing network distance between components that communicate with
each other. We evaluate R-Storm on set of micro-benchmark Storm applications as
well as Storm applications used in production at Yahoo! Inc. From our
experimental results we conclude that R-Storm achieves 30-47% higher throughput
and 69-350% better CPU utilization than default Storm for the micro-benchmarks.
For the Yahoo! Storm applications, R-Storm outperforms default Storm by around
50% based on overall throughput. We also demonstrate that R-Storm performs much
better when scheduling multiple Storm applications than default Storm."
pub.1147455189,Skim: Skipping Memory Lstm for Low-Latency Real-Time Continuous Speech Separation,"Continuous speech separation for meeting pre-processing has recently become a focused research topic. Compared to the data in utterance-level speech separation, the meeting-style audio stream lasts longer, has an uncertain number of speakers. We adopt the time-domain speech separation method and the recently proposed Graph-PIT to build a super low-latency online speech separation model, which is very important for the real application. The low-latency time-domain encoder with a small stride leads to an extremely long feature sequence. We proposed a simple yet efficient model named Skipping Memory (SkiM) for the long sequence modeling. Experimental results show that SkiM achieves on par or even better separation performance than DPRNN. Meanwhile, the computational cost of SkiM is reduced by 75% compared to DPRNN. The strong long sequence modeling capability and low computational cost make SkiM a suitable model for online CSS applications. Our fastest real-time model gets 17.1 dB signal-to-distortion (SDR) improvement with less than 1-millisecond latency in the simulated meeting-style evaluation."
pub.1117328086,Elasticutor,"Elasticity is highly desirable for stream systems to guarantee low latency against workload dynamics, such as surges in arrival rate and fluctuations in data distribution. Existing systems achieve elasticity using a resource-centric approach that repartitions keys across the parallel instances, i.e., executors, to balance the workload and scale operators. However, such operator-level repartitioning requires global synchronization and prohibits rapid elasticity. We propose an executor-centric approach that avoids operator-level key repartitioning and implements executors as the building blocks of elasticity. By this new approach, we design the Elasticutor framework with two level of optimizations: i) a novel implementation of executors, i.e., elastic executors, that perform elastic multi-core execution via efficient intra-executor load balancing and executor scaling and ii) a global model-based scheduler that dynamically allocates CPU cores to executors based on the instantaneous workloads. We implemented a prototype of Elasticutor and conducted extensive experiments. We show that Elasticutor doubles the throughput and achieves up to two orders of magnitude lower latency than previous methods for dynamic workloads of real-world applications."
pub.1094725983,Hardware-based Hash Functions for Network Applications,"For rich network services, it is indispensable to reconstruct TCP stream in the middle of the network. In this reconstruction function, managing a large number of streams is a crucial as an embedded hardware for achieving high-throughput processing. In this case, hashing is a well-used solution as an ID key of TCP streams. Since CRC hashes have simple tree structure of XOR logics and are perfectly composed of combination logics, they can easily be hardware implemented and enables low-latency calculation of the hash values. Therefore, CRC hash has been used in network applications where low-latency and high speed operations are necessary. Some of their usages include data identification, load balancing, encryption, and checksum. Although CRC hashes may be well suited for hardware implementation, it is not the top choice when collision rates and distribution of hash values are considered. For some network workloads, CRC hashing shows very uneven distribution of hash values and can lead to decrease in the overall performance of network systems. In this study, two hashes, Jenkins hash and MurmurHash, are implemented on a FPGA as TCP connection manager and evaluated for their aptitude for hashing used in hardware-based network applications."
pub.1128903794,Stream-Based Lossless Data Compression Applying Adaptive Entropy Coding for Hardware-Based Implementation,"Toward strong demand for very high-speed I/O for processors, physical performance growth of hardware I/O speed was drastically increased in this decade. However, the recent Big Data applications still demand the larger I/O bandwidth and the lower latency for the speed. Because the current I/O performance does not improve so drastically, it is the time to consider another way to increase it. To overcome this challenge, we focus on lossless data compression technology to decrease the amount of data itself in the data communication path. The recent Big Data applications treat data stream that flows continuously and never allow stalling processing due to the high speed. Therefore, an elegant hardware-based data compression technology is demanded. This paper proposes a novel lossless data compression, called ASE coding. It encodes streaming data by applying the entropy coding approach. ASE coding instantly assigns the fewest bits to the corresponding compressed data according to the number of occupied entries in a look-up table. This paper describes the detailed mechanism of ASE coding. Furthermore, the paper demonstrates performance evaluations to promise that ASE coding adaptively shrinks streaming data and also works on a small amount of hardware resources without stalling or buffering any part of data stream."
pub.1154973549,TiLT: A Time-Centric Approach for Stream Query Optimization and Parallelization,"Stream processing engines (SPEs) are widely used for large scale streaming analytics over unbounded time-ordered data streams. Modern day streaming analytics applications exhibit diverse compute characteristics and demand strict latency and throughput requirements. Over the years, there has been significant attention in building hardware-efficient stream processing engines (SPEs) that support several query optimization, parallelization, and execution strategies to meet the performance requirements of large scale streaming analytics applications. However, in this work, we observe that these strategies often fail to generalize well on many real-world streaming analytics applications due to several inherent design limitations of current SPEs. We further argue that these limitations stem from the shortcomings of the fundamental design choices and the query representation model followed in modern SPEs. To address these challenges, we first propose TiLT, a novel intermediate representation (IR) that offers a highly expressive temporal query language amenable to effective query optimization and parallelization strategies. We subsequently build a compiler backend for TiLT that applies such optimizations on streaming queries and generates hardware-efficient code to achieve high performance on multi-core stream query executions. We demonstrate that TiLT achieves up to 326× (20.49× on average) higher throughput compared to state-of-the-art SPEs (e.g., Trill) across eight real-world streaming analytics applications. TiLT source code is available at https://github.com/ampersand-projects/tilt.git."
pub.1125987109,Asynchronous Stochastic Computing,"Asynchronous Stochastic Computing (ASC) leverages Synchronous Stochastic Computing (SSC) advantages and addresses its drawbacks. In SSC a multiplier is a single AND gate, saving ~ 90% of power and area compared with a typical 8bit binary multiplier. The key for SSC power-area efficiency comes from mapping numbers to streams of 1s and 0s. Despite the power-area efficiency, SSC drawbacks such as long latency, costly clock distribution network (CDN), and expensive stream generation, causes the energy consumption to grow prohibitively large. In this work, we introduce the foundations for ASC using Continuous-time-Markov-chains, and analyze the computing error due to random fluctuations. In ASC data is mapped to asynchronous-continuous-time streams, which yields two advantages over the synchronous counterpart: (1) CDN elimination, and (2) better accuracy performance. We compare ASC with SSC for three applications: (1) multiplication, (2) an image processing algorithm: gamma-correction, and (3) a singlelayer of a fully-connected artificial-neural-network (ANN) using a FinFET1X technology. Our Matlab, Spice-level simulations and post-place&route (P&R) reports demonstrate that ASC yields savings of 10%-55%, 33%-44%, and 50% in latency, power, and energy respectively. These savings make ASC a good candidate to address the ultra-low-power requirements of machine learning for the IoT."
pub.1154955018,TiLT: A Time-Centric Approach for Stream Query Optimization and Parallelization,"Stream processing engines (SPEs) are widely used for large scale streaming
analytics over unbounded time-ordered data streams. Modern day streaming
analytics applications exhibit diverse compute characteristics and demand
strict latency and throughput requirements. Over the years, there has been
significant attention in building hardware-efficient stream processing engines
(SPEs) that support several query optimization, parallelization, and execution
strategies to meet the performance requirements of large scale streaming
analytics applications. However, in this work, we observe that these strategies
often fail to generalize well on many real-world streaming analytics
applications due to several inherent design limitations of current SPEs. We
further argue that these limitations stem from the shortcomings of the
fundamental design choices and the query representation model followed in
modern SPEs. To address these challenges, we first propose TiLT, a novel
intermediate representation (IR) that offers a highly expressive temporal query
language amenable to effective query optimization and parallelization
strategies. We subsequently build a compiler backend for TiLT that applies such
optimizations on streaming queries and generates hardware-efficient code to
achieve high performance on multi-core stream query executions. We demonstrate
that TiLT achieves up to 326x (20.49x on average) higher throughput compared to
state-of-the-art SPEs (e.g., Trill) across eight real-world streaming analytics
applications. TiLT source code is available at
https://github.com/ampersand-projects/tilt.git."
pub.1128751165,Pipeline-Based Linear Scheduling of Big Data Streams in the Cloud,"Nowadays, there is an accelerating need to efficiently and timely handle large amounts of data that arrives continuously. Streams of big data led to the emergence of several Distributed Stream Processing Systems (DSPS) that assign processing tasks to the available resources (dynamically or not) and route streaming data between them. Efficient scheduling of processing tasks can reduce application latencies and eliminate network congestions. However, the available DSPSs’ in-built scheduling techniques are far from optimal. In this work, we extend our previous work, where we proposed a linear scheme for the task allocation and scheduling problem. Our scheme takes advantage of pipelines to handle efficiently applications, where there is need for heavy communication (all-to-all) between tasks assigned to pairs of components. In this work, we prove that our scheme is periodic, we provide a communication refinement algorithm and a mechanism to handle many-to-one assignments efficiently. For concreteness, our work is illustrated based on Apache Storm semantics. The performance evaluation depicts that our algorithm achieves load balance and constraints the required buffer space. For throughput testing, we compared our work to the default Storm scheduler, as well as to R-Storm. Our scheme was found to outperform both the other strategies and achieved an average of 25%-40% improvement compared to Storm’s default scheduler under different scenarios, mainly as a result of reduced buffering (≈ 45% less memory). Compared to R-storm, the results indicate an average of 35%-45% improvement."
pub.1101208898,Model-driven scheduling for distributed stream processing systems,"Distributed Stream Processing Systems (DSPS) are “Fast Data” platforms that allow streaming applications to be composed and executed with low latency on commodity clusters and Clouds. Such applications are composed as a Directed Acyclic Graph (DAG) of tasks, with data parallel execution using concurrent task threads on distributed resource slots. Scheduling such DAGs for DSPS has two parts—allocation of threads and resources for a DAG, and mapping threads to resources. Existing schedulers often address just one of these, make the assumption that performance linearly scales, or use ad hoc empirical tuning at runtime. Instead, we propose model-driven techniques for both mapping and allocation that rely on low-overhead a priori performance modeling of tasks. Our scheduling algorithms are able to offer predictable and low resource needs that is suitable for elastic pay-as-you-go Cloud resources, support a high input rate through high VM utilization, and can be combined with other mapping approaches as well. These are validated for micro and application benchmarks, and compared with contemporary schedulers, for the Apache Storm DSPS."
pub.1091772038,Stimulus and Response-Locked P3 Activity in a Dynamic Rapid Serial Visual Presentation (RSVP) Task,"The current study evaluated the relationship between reaction time performance and the visual target-evoked P3 event-related potential (ERP) using a dynamic rapid serial visual presentation (RSVP) task. Electroencephalography (EEG) and reaction time measures were obtained while observers monitored an RSVP stream of short videos for infrequent targets presented among frequent nontarget distractors. Observers reaction time quartiles were used as ERP binning parameters for target- and response-locked EEG epochs. This procedure allowed us to assess neural activity related to perceptual and response processing across different levels of overt performance. P3 amplitude and latency were evaluated for both stimulus and response-locked averages. The results showed that the peak latency of the stimulus-locked P3 was maximal over central parietal electrode Pz and was significantly different between each quartile; however, these latency differences were absent when P3 latency was measured in the response-locked data. The P3 amplitude analysis revealed no significant differences between stimulus and response-locked averages within each quartile. Overall, the results suggest the peak latency of the P3 obtained in the current study reflects processes more associated with motor planning and response execution."
pub.1119739001,IoT-Stream: A Lightweight Ontology for Internet of Things Data Streams,"In recent years, the development and deployment of Internet of Things (IoT) devices has led to the generation of large volumes of real world data. Analytical models can be used to extract meaningful insights from this data. However, most of IoT data is not fully utilised, which is mainly due to interoperability issues and the difficulties to analyse data collected by heterogeneous resources. To overcome this heterogeneity, semantic technologies are used to create common models to share various data originated from heterogeneous sources. However, semantics add further overhead to data delivery, and the processing time to annotate the data with the model can increase the latency and complexity in publishing and querying the annotated data. In this paper, we present a lightweight semantic model to annotate IoT streams. The metadata descriptions that are provided in the models are used for search and discovery of the data using various attributes such as value and type. The proposed model extends commonly used ontologies such as W3C/OGC SSN ontology and its recent lightweight core, SOSA, and includes concepts to describe streaming IoT data. We also show use cases, tools and applications where the proposed model has been used."
pub.1113522366,Efficient Time-Evolving Stream Processing at Scale,"Time-evolving stream datasets exist ubiquitously in many real-world applications where their inherent hot keys often evolve over times. Nevertheless, few existing solutions can provide efficient load balancing on these time-evolving datasets while preserving low memory overhead. In this paper, we present a novel load balancing mechanism (named FISH), which can provide the efficient time-evolving stream processing at scale through recent hot keys identification and worker assignment. The key insight of this work is that the keys of time-evolving stream data can have a skewed distribution within the bounded distance of time interval. This enables to accurately identify the recent hot keys for the real-time load balancing within a bounded scope. We therefore propose an epoch-based recent hot key identification with specialized intra-epoch frequency counting (for maintaining low memory overhead) and inter-epoch hotness decaying (for suppressing superfluous computation). We also propose to heuristically infer the accurate information of remote workers through computation rather than communication for cost-efficient worker assignment. We have integrated our approach into Apache Storm. Our results on a cluster of 128 nodes for both synthetic and real-world stream datasets show that FISH significantly outperforms state-of-the-arts with the average and the 99th percentile latency reduction by 87.12 and 76.34 percent (versus W-Choices), and memory overhead reduction by 96.66 percent (versus Shuffle Grouping)."
pub.1171541971,SIM: A fast real-time graph stream summarization with improved memory efficiency and accuracy,"A graph stream composed of sequentially approaching arriving edges is commonly utilized to represent complicated structured data in interactive application systems. Since graph streams are extraordinarily vast and high velocity, efficient storage and analysis of graph streams face serious challenges. Current graph stream summarization schemes have effectively achieved the storage and management of graph streams. Unfortunately, they either cannot accomplish real-time queries or have low overall query accuracy. To this end, a novel structure, named Shared Interaction Matrix (SIM), is proposed for real-time queries rapidly and accurately with smaller memory. SIM is designed as a two-layer adjacency matrix with different structures to improve memory efficiency while preserving the key of heavy edges to support real-time measurement. Moreover, SIM leverages shared hash technology and an integral replacement strategy to boost insertion query speed and query accuracy. The performance of SIM is evaluated by conducting extensive experiments on the CPU and OVS platform. The experimental results show that SIM significantly enhances measurement accuracy and reduces insertion and query processing latency by 39.21%–93.50% while achieving real-time queries, compared with the state-of-the-art schemes."
pub.1147097511,The Vision of Edges of Internet as a Computing Fabric,"Edge computing has emerged as a major disrupting technology after Cloud computing to fill in the computational and infrastructural gaps in IoT and Mobile Cloud computing. Indeed, IoT and Mobile Cloud computing are based on rather direct connection of devices to Cloud servers and data centers, such as through gateways, and are not able, due to round trip time, to cope with demanding requirements of real-time applications for low latency, critical decision making, increased security, support to mobility, etc. The aim of computing at Edges of Internet is to alleviate the burden of IoT data stream processing to the Cloud computing by pushing part of the computations, storage, reasoning and intelligence to the Edges of the Internet, close to where data is generated and to end-users. In this introductory chapter we discuss the vision of Edges of Internet as a computing fabric to support real-time applications, high performance computing, big data and big data stream processing at large scale. The ever growing number of compute nodes (from small to large), of fast connectivity (supported by 5G technologies) and of data storage (mini/nano data centers) provide the basis to achieve the vision of computing fabric. The challenges of achieving such vision due to a number of complexities are also discussed."
pub.1137884113,VID-WIN: Fast Video Event Matching with Query-Aware Windowing at the Edge for the Internet of Multimedia Things,"Efficient video processing is a critical component in many IoMT applications
to detect events of interest. Presently, many window optimization techniques
have been proposed in event processing with an underlying assumption that the
incoming stream has a structured data model. Videos are highly complex due to
the lack of any underlying structured data model. Video stream sources such as
CCTV cameras and smartphones are resource-constrained edge nodes. At the same
time, video content extraction is expensive and requires computationally
intensive Deep Neural Network (DNN) models that are primarily deployed at
high-end (or cloud) nodes. This paper presents VID-WIN, an adaptive 2-stage
allied windowing approach to accelerate video event analytics in an edge-cloud
paradigm. VID-WIN runs parallelly across edge and cloud nodes and performs the
query and resource-aware optimization for state-based complex event matching.
VID-WIN exploits the video content and DNN input knobs to accelerate the video
inference process across nodes. The paper proposes a novel content-driven
micro-batch resizing, queryaware caching and micro-batch based utility
filtering strategy of video frames under resource-constrained edge nodes to
improve the overall system throughput, latency, and network usage. Extensive
evaluations are performed over five real-world datasets. The experimental
results show that VID-WIN video event matching achieves ~2.3X higher throughput
with minimal latency and ~99% bandwidth reduction compared to other baselines
while maintaining query-level accuracy and resource bounds."
pub.1120163579,Continuous Cross Identification in Large-Scale Dynamic Astronomical Data Flow,"In modern astronomy, Short-Timescale and Large Field-of-view (STLF) sky survey produce large volume data and face a great challenge in cross identification. Furthermore, transient survey projects are required to select the candidates fast from large volume data. However, traditional cross identification methods didn’t satisfy the observation of transient survey. We present a fast and efficient cross identification system for large-scale astronomical data streams. By receiving a high-frequency star catalog and maintaining a local star catalog, the system partitions the star catalog and cross identification with the object catalog. A coding strategy is used to manage the unique ID of the all-sky star. After processing data, all the results are stored in Redis and generate the light curve. Our experiment shows that the method could meet the strict performance requirements and good recognition accuracy on fast real-time sky survey project. Additionally, our system shows good performance in low latency large volume astronomical data processing and our system has been successfully applied in the Ground-based Wide Angle Camera (GWAC) online data processing pipeline."
pub.1093942831,Hardware Accelerator for Similarity Based Data Dedupe,"Data deduplication has proven important in backup storage systems as large amount of identical or similar data chunks exist. Recent studies have shown the great potential of data deduplication in primary storage and storage caches. Deduplications in these environments require high speed processing not to drag down production performance. This paper presents a hardware accelerator for similarity based data deduplication. It implements three compute-intensive kernel modules to improve throughput and latency in dedupe systems: sketch computation for data blocks, index searching for reference block, and delta encoding over similar blocks. Adopting pipelined computation and parallel data lookup across multiple hardware modules, our HW design is capable of processing high throughput data traffic by working on multiple data units concurrently, thus enabling wire speed dedupe for data stream where similar blocks present. Using a PC host system connected to the FPGA-based accelerator through a PCIe Gen 2 ×4 interface, our experiments show that the similarity based data dedupe performs 30% better in data reduction ratio than conventional dedupe techniques that look at identical blocks only. By comparing the hardware implementation with its software counterpart, the experimental results show that our preliminary FPGA implementation with maximum clock speed of 250MHz achieves at least 6 times improvement in latency over the software implementation running on state-of-art servers."
pub.1172920895,Towards Disaggregation-Native Data Streaming between Devices,"Disaggregation is an ongoing trend to increase flexibility in datacenters.
With interconnect technologies like CXL, pools of CPUs, accelerators, and
memory can be connected via a datacenter fabric. Applications can then pick
from those pools the resources necessary for their specific workload. However,
this vision becomes less clear when we consider data movement. Workloads often
require data to be streamed through chains of multiple devices, but typically,
these data streams physically do not directly flow device-to-device, but are
staged in memory by a CPU hosting device protocol logic. We show that
augmenting devices with a disaggregation-native and device-independent data
streaming facility can improve processing latencies by enabling data flows
directly between arbitrary devices."
pub.1144061758,Serverless data pipeline approaches for IoT data in fog and cloud computing,"With the increasing number of Internet of Things (IoT) devices, massive
amounts of raw data is being generated. The latency, cost, and other challenges
in cloud-based IoT data processing have driven the adoption of Edge and Fog
computing models, where some data processing tasks are moved closer to data
sources. Properly dealing with the flow of such data requires building data
pipelines, to control the complete life cycle of data streams from data
acquisition at the data source, edge and fog processing, to Cloud side storage
and analytics. Data analytics tasks need to be executed dynamically at
different distances from the data sources and often on very heterogeneous
hardware devices. This can be streamlined by the use of a Serverless (or FaaS)
cloud computing model, where tasks are defined as virtual functions, which can
be migrated from edge to cloud (and vice versa) and executed in an event-driven
manner on data streams. In this work, we investigate the benefits of building
Serverless data pipelines (SDP) for IoT data analytics and evaluate three
different approaches for designing SDPs: 1) Off-the-shelf data flow tool (DFT)
based, 2) Object storage service (OSS) based and 3) MQTT based. Further, we
applied these strategies on three fog applications (Aeneas, PocketSphinx, and
custom Video processing application) and evaluated the performance by comparing
their processing time (computation time, network communication and disk access
time), and resource utilization. Results show that DFT is unsuitable for
compute-intensive applications such as video or image processing, whereas OSS
is best suitable for this task. However, DFT is nicely fit for
bandwidth-intensive applications due to the minimum use of network resources.
On the other hand, MQTT-based SDP is observed with increase in CPU and Memory
usage as the number of...<truncted to fit character limit in Arxiv>"
pub.1121130446,Comparison of MongoDB and Cassandra Databases for Spectrum Monitoring As-a-Service,"Due to the growing number of devices accessing the Internet through wireless networks, the radio spectrum has become a highly contended resource. The availability of low cost radio spectrum monitoring sensors enables a geographically distributed, real-time observation of the spectrum to spot inefficiencies and to develop new strategies for its utilization. The potentially large number of sensors to be deployed and the intrinsic nature of data make this task a Big Data problem. In this work we design, implement, and validate a hardware and software architecture for wideband radio spectrum monitoring inspired to the Lambda architecture. This system offers Spectrum Sensing as a Service to let end users easily access and process radio spectrum data. To minimize the latency of services offered by the platform, we fine tune the data processing chain. From the analysis of sensor data characteristics, we design the data models for MongoDB and Cassandra, two popular NoSQL databases. A MapReduce job for spectrum visualization has been developed to show the potential of our approach and to identify the challenges in processing spectrum sensor data. We experimentally evaluate and compare the performance of the two databases in terms of application processing time for different types of queries applied on data streams with heterogeneous generation rate. Our experiments show that Cassandra outperforms MongoDB in most cases, with some exceptions depending on data stream rate."
pub.1131701653,V-PRISM: An Edge-Based IoT Architecture to Virtualize Multimedia Sensors,"Multimedia sensors have recently become a major data source, giving rise to the Internet of Multimedia Things. Since multimedia applications are usually latency-sensitive, data processing in the cloud is not always effective. A strategy to minimize delay is to process the streams closer to the data sources, exploiting the resources at the edge of the network. We propose V-PRISM, an architecture to virtualize multimedia sensors with components deployed and executed at the edge tier. V-PRISM can reduce the resource consumption of IoT devices, the network traffic, and the end-to-end delay while increasing the ROI (Return On Investment) for infrastructure providers."
pub.1094353734,Stream architectures - efficiency and programmability,"Summary form only given. Stream processors are fully programmable in a high-level language, yet are capable of achieving computation efficiency comparable to fixed-function ASIC solutions (about 20 pJ/op) and can be scaled from a Gop/s (20 mW) block to a Top/s (20 W) chip in current semiconductor technology. The parallel nature of stream processors enables their performance to scale with technology. In a 2010 45 nm technology we expect an efficiency of 1 pJ/op and performance of up to 20 Top/s (20 W). A stream processor contains an array of arithmetic units that are supplied with data by a deep and explicit register hierarchy, which also serves to decouple instruction execution from unpredictable and long-latency memory operations. This decoupled and exposed-communication architecture enables a compiler to automatically map a stream application (such as a signal-flow graph) to the processing array: employing ""stream scheduling"" to stage the high-level movement of streams, and ""communication scheduling"" to schedule the data movement in the low-level kernels. This explicit optimization of communication results in almost all data and instruction movement taking place over short wires, and hence almost all energy going to useful computation. We have built a prototype streaming signal processor, Imagine, and have demonstrated streaming applications involving video compression/decompression, wireless communication, and adaptive beam-forming. We are also designing the Merrimac supercomputer, which uses a stream processor based on the same architectural principles as Imagine, illustrating the flexibility, generality, and scalability of the streaming concept. This paper describes stream architectures, stream programming systems, and streaming applications. A comparison is made to conventional DSPs, FPGAs, and ASIC solutions."
pub.1160173403,Asynchronous Event Processing with Local-Shift Graph Convolutional Network,"Event cameras are bio-inspired sensors that produce sparse and asynchronous event streams instead of frame-based images at a high-rate. Recent works utilizing graph convolutional networks (GCNs) have achieved remarkable performance in recognition tasks, which model event stream as spatio-temporal graph. However, the computational mechanism of graph convolution introduces redundant computation when aggregating neighbor features, which limits the low-latency nature of the events. And they perform a synchronous inference process, which can not achieve a fast response to the asynchronous event signals. This paper proposes a local-shift graph convolutional network (LSNet), which utilizes a novel local-shift operation equipped with a local spatio-temporal attention component to achieve efficient and adaptive aggregation of neighbor features. To improve the efficiency of pooling operation in feature extraction, we design a node-importance based parallel pooling method (NIPooling) for sparse and low-latency event data. Based on the calculated importance of each node, NIPooling can efficiently obtain uniform sampling results in parallel, which retains the diversity of event streams. Furthermore, for achieving a fast response to asynchronous event signals, an asynchronous event processing procedure is proposed to restrict the network nodes which need to recompute activations only to those affected by the new arrival event. Experimental results show that the computational cost can be reduced by nearly 9 times through using local-shift operation and the proposed asynchronous procedure can further improve the inference efficiency, while achieving state-of-the-art performance on gesture recognition and object recognition."
pub.1135101821,Multi-channel VideoStreaming Technology for Processing Based on Distributed Computing Platform,"With the rapid development of the Internet, sensor network, and mobile Internet technology, a large number of data sets are continuously generated in the form of streaming in various application fields. At the same time, the processing of stream data has drawn more and more attention because of its application in different situations. To meet the urgent need for the processing of streaming data, there are many computing engines on the market, such as Spark and Flink. Traditional data-flow analysis has many problems in the process of streaming data, such as the high time delay, weak extensibility, and bad adaptability. In order to improve them, people use Kafka as the intermediate-cache and the computing engine that uses Spark Structured Streaming as the streaming data to realize the effective process of multichannel flow data. Considering the present demand for low latency and high flux that are analyzed by urban management in traffic video, processing the data of multichannel traffic video on distributed computing platform realized the tracking and search on motor vehicles, non-motor vehicles, and pedestrians of multiple roads and intersections."
pub.1094407272,A Preventive Auto-Parallelization Approach for Elastic Stream Processing,"Nowadays, more and more sources (connected devices, social networks, etc.) emit real-time data with fluctuating rates over time. Existing distributed stream processing engines (SPE) have to resolve a difficult problem: deliver results satisfying end-users in terms of quality and latency without over-consuming resources. This paper focuses on parallelization of operators to adapt their throughput to their input rate. We suggest an approach which prevents operator congestion in order to limit degradation of results quality. This approach relies on an automatic and dynamic adaptation of resource consumption for each continuous query. This solution takes advantage of i) a metric estimating the activity level of operators in the near future ii) the AUTOSCALE approach which evaluates the need to modify parallelism degrees at local and global scope iii) an integration into the Apache Storm solution. We show performance tests comparing our approach to the native solution of this SPE."
pub.1150224604,Data stream fusion for accurate quantile tracking and analysis,"UDDSketch is a recent algorithm for accurate tracking of quantiles in data streams, derived from the DDSketch algorithm. UDDSketch provides accuracy guarantees covering the full range of quantiles independently of the input distribution and greatly improves the accuracy with regard to DDSketch. In this paper we show how to compress and fuse two or more data streams (or datasets) by leveraging the mergeability of the UDDSketch data summaries. In general, two summaries on two data streams are said to be mergeable if there exists an algorithm that allows combining the two summaries into a single one related to the union of the two datasets, simultaneously preserving the error and size guarantees. The property of mergeability of a sketch enables the parallel and distributed processing of big volume data streams that can be compressed and fused by means of such mergeable data structures. Among the applications strictly related to accurate tracking of quantiles, requiring parallel and/or distributed processing we recall here estimating the latency of a web site, database query optimizers and the need of succinctly summarizing the distribution of values occurring over a sensor network. We prove that UDDSketch is fully mergeable and introduce PUDDSketch, a parallel version of UDDSketch suitable for message-passing based architectures. We formally prove its correctness and compare it to a parallel version of DDSketch, showing through extensive experimental results that our parallel algorithm almost always outperforms the parallel DDSketch algorithm with regard to the overall accuracy in determining the quantiles. Moreover, we also design and implement parallel versions of both the state of the art KLL and REQ sequential algorithms in order to compare and contrast PUDDSketch versus the corresponding parallel algorithms. Our experiments clearly show that PUDDSketch is faster or on par with regard to parallel running time, whilst providing simultaneously greater accuracy."
pub.1096949399,Big Data in E&P: Real-Time Adaptive Analytics and Data-Flow Architecture,"Abstract Big Data Analytics are most effective when data-in-motion are combined with data-at-rest. Stream computing is a new way of analyzing high-frequency data for real-time complex-event-processing (CEP) and for scoring data against a physics-based or empirical model for predictive analytics, without having to store the data. Hadoop Map/Reduce and other NoSQL approaches are a new way of analyzing massive volumes of data whether semistructured or unstructured, which can be used to support the E&P industry's many physics-based methods in modeling and simulation, over a wide range of disciplines from geology & geophysics to reservoir, production, & facilities engineering. High-volume data of many different types can be landed in Hadoop and analyzed without extensive transformation into a relational database model. Combining Stream computing with Hadoop Map/Reduce or massively parallel processing relational data warehousing (MPP DW) enables the analysis of high-frequency data scored against continuously updated models, known as ""Real-Time Adaptive Analytics."" This design pattern provides a low-latency ""Real-Time Data-Flow Architecture,"" which enables decisions during operational events and also at the speed of the overall business. Big Data Analytics also depends on tooling for building modeling and simulation applications and for performing analytics, as much as it depends on infrastructure. Augmenting the industry's many decades of progress in physics-based modeling and simulation, powerful empirical models can be developed with tools for statistical analytics, text analytics, AI, and machine-learning and then used as scoring and data-validation models by implementing stream computing on-site where the data are generated in oilfield operations. This paper will explore RealTime Adaptive Analytics and the Real-Time Data-Flow Architecture combining stream computing, Hadoop/NoSQL, and MPP data warehousing. Several compelling use cases in drilling and in production will be reviewed."
pub.1134395404,Research on Optimal Checkpointing-Interval for Flink Stream Processing Applications,"Nowadays various distributed stream processing systems (DSPSs) are employed to process the ever-expanding real-time data. The DSPSs are highly susceptible to system failure, and the fault-tolerance issue is a major problem, which is getting lot of attention nowadays. Flink is a popular streaming computing framework that implements a lightweight, asynchronous checkpoint technique based on the barrier mechanism to ensure high efficiency in analysing the data. In a checkpoint-based fault-tolerance mechanism, a shorter checkpoint interval can increase runtime cost of streaming applications, while a longer one will increase recovery time of failure recovery. So, selecting an optimal checkpoint interval is critical to attain high efficiency of the streaming applications. Traditional optimal checkpoint interval mechanisms usually assume that the checkpointing delay and the fault recovery time are fixed. However, both factors have a strong relation to the intensity of the application’s workload. To obtain more optimal checkpoint interval under different workload intensities, this paper proposes a performance model to estimate the tuples processing latency and a recovery model to estimate the fault recovery time. With these two models, an optimal checkpoint interval can be arrived. These models and the interval optimisation interval are verified experimentally on Flink. The results show that the proposed model can recommend an optimal checkpoint interval according to the system reliability related indicators. This proposed system optimised recovery time and performs efficiently in applications with delay constraints."
pub.1137438981,VID-WIN: Fast Video Event Matching With Query-Aware Windowing at the Edge for the Internet of Multimedia Things,"Efficient video processing is a critical component in many IoMT applications to detect events of interest. Presently, many window optimization techniques have been proposed in event processing with an underlying assumption that the incoming stream has a structured data model. Videos are highly complex due to the lack of any underlying structured data model. Video stream sources, such as CCTV cameras and smartphones are resource-constrained edge nodes. At the same time, video content extraction is expensive and requires computationally intensive deep neural network (DNN) models that are primarily deployed at high-end (or cloud) nodes. This article presents VID-WIN, an adaptive 2-stage allied windowing approach to accelerate video event analytics in an edge-cloud paradigm. VID-WIN runs parallelly across edge and cloud nodes and performs the query and resource-aware optimization for state-based complex event matching. VID-WIN exploits the video content and DNN input knobs to accelerate the video inference process across nodes. This article proposes a novel content-driven microbatch resizing, query-aware caching, and microbatch-based utility filtering strategy of video frames under resource-constrained edge nodes to improve the overall system throughput, latency, and network usage. Extensive evaluations are performed over five real-world data sets. The experimental results show that VID-WIN video event matching achieves ${\sim } 2.3\times $ higher throughput with minimal latency and ~99% bandwidth reduction compared to other baselines while maintaining query-level accuracy and resource bounds."
pub.1129079122,IoT Architecture for Urban Data-Centric Services and Applications,"In this work, we describe an urban Internet of Things (IoT) architecture, grounded in big data patterns and focused on the needs of cities and their key stakeholders. First, the architecture of the dedicated platform USE4IoT (Urban Service Environment for the Internet of Things), which gathers and processes urban big data and extends the Lambda architecture, is proposed. We describe how the platform was used to make IoT an enabling technology for intelligent transport planning. Moreover, key data processing components vital to provide high-quality IoT data streams in a near-real-time manner are defined. Furthermore, tests showing how the IoT platform described in this study provides a low-latency analytical environment for smart cities are included."
pub.1157591826,High Performance Platform to Detect Faults in the Smart Grid by Artificial Intelligence Inference,"Inferring faults throughout the power grid involves fast calculation, large scale of data, and low latency. Our heterogeneous architecture in the edge offers such high computing performance and throughput using an Artificial Intelligence (AI) core deployed in the Alveo accelerator. In addition, we have described the process of porting standard AI models to Vitis AI and discussed its limitations and possible implications. During validation, we designed and trained some AI models for fast fault detection in Smart Grids. However, the AI framework is standard, and adapting the models to Field Programmable Gate Arrays (FPGA) has demanded a series of transformation processes. Compared with the Graphics Processing Unit platform, our implementation on the FPGA accelerator consumes less energy and achieves lower latency. Finally, our system balances inference accuracy, on-chip resources consumed, computing performance, and throughput. Even with grid data sampling rates as high as 800,000 per second, our hardware architecture can simultaneously process up to 7 data streams."
pub.1148178773,Tighter NIC/GPU Integration Yields Next Level Media Processing Performance,"As the media industry further consolidates building services based on Commercial-Off-The-Shelf (COTS) hardware, the requirement for increasing performance continues to accelerate. The processing of video resolutions up to Ultra HD (UHD) and 8K, higher frame rates, increased bit depth, and High Dynamic Range (HDR) imagery, requires tighter integration and optimization between the Network Interface Controller (NIC) and the Graphical Processing Unit (GPU). Combined with simultaneous input and output of multiple streams, this creates the potential for performance bottlenecks that must be unlocked. — This paper describes how COTS hardware platforms running GPUs alongside ST 2059-2 PTP locked NICs which are accurately pacing packets according to ST 2110-21 requirements may further increase their performance throughput by reducing CPU driven data copy overhead. And in doing so, reduce processing latency and jitter, and more effectively use GPU resources while freeing up CPU resources."
pub.1134315615,Towards a Security-Aware Deployment of Data Streaming Applications in Fog Computing,"Emerging fog and edge computing environments enable the analysis of Big Data collected from devices (e.g., IoT sensors) with reduced latency compared to cloud-based solutions. In particular, many applications deal with continuous data flows in latency-sensitive domains (e.g., healthcare monitoring), where Data Stream Processing (DSP) systems represent a popular solution. However, the highly heterogeneous nature of fog/edge platforms poses several challenges for efficiently deploying DSP applications, including security and privacy issues. As data streams flow through public networks and are possibly processed within multi-tenant computing platforms, new metrics must be considered for deployment, accounting for security and privacy related concerns, besides traditionally adopted performance and cost aspects. In this chapter, we present the most relevant existing solutions for deploying DSP applications in fog/edge environments, discussing—in particular—how they address security and privacy concerns. Then, we present Security-aware DSP Placement (SDP), a formulation of the optimal deployment problem for DSP applications in fog/edge environments. Specifically, we introduce security-related application requirements in addition to non-functional ones, and show how the resolution of SDP allows us to trade-off cost and performance with privacy and data integrity objectives."
pub.1132277443,Knowledge Graph Driven Approach to Represent Video Streams for Spatiotemporal Event Pattern Matching in Complex Event Processing,"Complex Event Processing (CEP) is an event processing paradigm to perform real-time analytics over streaming data and match high-level event patterns. Presently, CEP is limited to process structured data stream. Video streams are complicated due to their unstructured data model and limit CEP systems to perform matching over them. This work introduces a graph-based structure for continuous evolving video streams, which enables the CEP system to query complex video event patterns. We propose the Video Event Knowledge Graph (VEKG), a graph-driven representation of video data. VEKG models video objects as nodes and their relationship interaction as edges over time and space. It creates a semantic knowledge representation of video data derived from the detection of high-level semantic concepts from the video using an ensemble of deep learning models. A CEP-based state optimization — VEKG-Time Aggregated Graph (VEKG-TAG) — is proposed over VEKG representation for faster event detection. VEKG-TAG is a spatiotemporal graph aggregation method that provides a summarized view of the VEKG graph over a given time length. We defined a set of nine event pattern rules for two domains (Activity Recognition and Traffic Management), which act as a query and applied over VEKG graphs to discover complex event patterns. To show the efficacy of our approach, we performed extensive experiments over 801 video clips across 10 datasets. The proposed VEKG approach was compared with other state-of-the-art methods and was able to detect complex event patterns over videos with [Formula: see text]-Score ranging from 0.44 to 0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99% and 93% of VEKG nodes and edges, respectively, with 5.19[Formula: see text] faster search time, achieving sub-second median latency of 4–20[Formula: see text]ms."
pub.1109821842,Wide-Area Spark Streaming: Automated Routing and Batch Sizing,"Modern stream processing frameworks, such as Spark Streaming, are designed to support a wide variety of stream processing applications, such as real-time data analytics in social networks. As the volume of data to be processed increases rapidly, there is a pressing need for processing them across multiple geo-distributed datacenters. However, these frameworks are not designed to take limited and varying inter-datacenter bandwidth into account, leading to longer query latencies. In this paper, we present the design and implementation of an extended Spark Streaming framework to automatically and optimally schedule tasks, select data flow routes and determine micro-batch sizes across geo-distributed datacenters in wide-area networks. To make these decisions, we propose a sparsity-regularized ADMM algorithm to efficiently solve a nonconvex optimization problem, based on readily measurable operating traces. Toward incremental real-world deployment, we take a non-intrusive approach to support flexible routing of micro-batches by adding a new DStream transformation we have developed to the existing Spark Streaming framework. As a result, our implementation can enforce scheduling decisions by modifying application workflows only. We have deployed our implementation on Amazon EC2 with emulated bandwidth constraints, and our experimental results on various types of queries have demonstrated the effectiveness of our proposed framework, as compared to the existing Spark Streaming scheduler and other data-locality-based heuristics."
pub.1117829058,On SDN-Enabled Online and Dynamic Bandwidth Allocation for Stream Analytics,"Data communication in cloud-based distributed stream data analytics often involves a collection of parallel and pipelined TCP flows. As the standard TCP congestion control mechanism and its variants are designed for achieving “fairness” among competing flows and are agnostic to the application layer contexts, the bandwidth allocation among a set of TCP flows traversing bottleneck links often leads to sub-optimal application-layer performance measures, e.g., stream processing throughput or average tuple complete latency. Motivated by this and enabled by the rapid development of the software-defined networking (SDN) techniques, in this paper, we re-investigate the design space of the bandwidth allocation problem and propose a cross-layer framework which utilizes the instantaneous information obtained from the application layer and provides on-the-fly and dynamic bandwidth adjustment algorithms for assisting the stream analytics applications achieving better performance during the runtime. We implement a prototype cross-layer bandwidth allocation framework based on a popular open-source distributed stream processing platform, Apache Storm, together with the OpenDaylight controller, and carry out extensive experiments with real-world analytical workloads on top of a local cluster consisting of ten workstations interconnected by a SDN-enabled fat-tree like testbed. The experiment results clearly validate the effectiveness and efficiency of our proposed framework and algorithms. Finally, we leverage the proposed cross-layer SDN framework and introduce an exemplary mechanism for bandwidth sharing and performance reasoning among multiple active applications and show a case of a point solution on how to approximate application-level fairness."
pub.1128852403,VESPER: A Real-time Processing Framework for Vehicle Perception Augmentation,"With today’s intelligent vehicles, there are a variety of information-rich sensors, both on and off-board, that can stream data to assist drivers. In the future, we imagine physical infrastructure capable of sensing and communicating data to vehicles to improve a driver’s awareness on the road. To process this data and present information to the driver in real-time, we introduce VESPER, a real-time processing framework and online scheduling algorithm designed to exploit distributed devices that are connected via wireless links. A significant feature of the VESPER algorithm is its ability to navigate the trade-off between accuracy and computational complexity of modern machine learning tools by adapting the workload, while still satisfying latency and throughput requirements. We refer to this capability as polymorphic computing. VESPER also scales opportunistically to leverage the computational resources of external devices. We evaluate VESPER on an image-processing pipeline and demonstrate that it outperforms offloading schemes based on static workloads."
pub.1129329147,Knowledge Graph Driven Approach to Represent Video Streams for Spatiotemporal Event Pattern Matching in Complex Event Processing,"Complex Event Processing (CEP) is an event processing paradigm to perform
real-time analytics over streaming data and match high-level event patterns.
Presently, CEP is limited to process structured data stream. Video streams are
complicated due to their unstructured data model and limit CEP systems to
perform matching over them. This work introduces a graph-based structure for
continuous evolving video streams, which enables the CEP system to query
complex video event patterns. We propose the Video Event Knowledge Graph
(VEKG), a graph driven representation of video data. VEKG models video objects
as nodes and their relationship interaction as edges over time and space. It
creates a semantic knowledge representation of video data derived from the
detection of high-level semantic concepts from the video using an ensemble of
deep learning models. A CEP-based state optimization - VEKG-Time Aggregated
Graph (VEKG-TAG) is proposed over VEKG representation for faster event
detection. VEKG-TAG is a spatiotemporal graph aggregation method that provides
a summarized view of the VEKG graph over a given time length. We defined a set
of nine event pattern rules for two domains (Activity Recognition and Traffic
Management), which act as a query and applied over VEKG graphs to discover
complex event patterns. To show the efficacy of our approach, we performed
extensive experiments over 801 video clips across 10 datasets. The proposed
VEKG approach was compared with other state-of-the-art methods and was able to
detect complex event patterns over videos with F-Score ranging from 0.44 to
0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99%
and 93% of VEKG nodes and edges, respectively, with 5.19X faster search time,
achieving sub-second median latency of 4-20 milliseconds."
pub.1137410971,Stream Floating: Enabling Proactive and Decentralized Cache Optimizations,"As multicore systems continue to grow in scale and on-chip memory capacity, the on-chip network bandwidth and latency become problematic bottlenecks. Because of this, overheads in data transfer, the coherence protocol and replacement policies become increasingly important. Unfortunately, even in well-structured programs, many natural optimizations are difficult to implement because of the reactive and centralized nature of traditional cache hierarchies, where all requests are initiated by the core for short, cache line granularity accesses. For example, long-lasting access patterns could be streamed from shared caches without requests from the core. Indirect memory access can be performed by chaining requests made from within the cache, rather than constantly returning to the core. Our primary insight is that if programs can embed information about long-term memory stream behavior in their ISAs, then these streams can be floated to the appropriate level of the memory hierarchy. This decentralized approach to address generation and cache requests can lead to better cache policies and lower request and data traffic by proactively sending data before the cores even request it. To evaluate the opportunities of stream floating, we enhance a tiled multicore cache hierarchy with stream engines to process stream requests in last-level cache banks. We develop several novel optimizations that are facilitated by stream exposure in the ISA, and subsequent exposure to caches. We evaluate using a cycle-level execution-driven gem5-based simulator, using 10 data-processing workloads from Rodinia and 2 streaming kernels written in OpenMP. We find that stream floating enables 52% and 39% speedup over an inorder and OOO core with state of art prefetcher design respectively, with 64% and 49% energy efficiency advantage."
pub.1094061376,A Novel Crossbar Scheduling for Multi-FPGA Parallel SAR Imaging System,"In multiprocessors system, crossbar scheduling networks have been widely used for system interconnection among processors or modules in SOC (System on Chip). In this paper, a parallel SAR (Synthetic Aperture Radar) image processing system which is composed of five FPGA PE (Process Element)s is taken for the basic research platform. Then a novel compact crossbar scheduling network for multi distributed parallel PEs is proposed for communication among PEs. Different scheduling strategies are provided for different kinds of source data streams which are from external raw data and PEs. In addition, resynchronization makes data streams of all PEs synchronous when all data streams are not entirely consistent due to the efficiency of memory access and bus burst transfer. Simulation and synthesis show that only little device resource is utilized by the proposed crossbar scheduling network to meet the requirements of the system. The maximum throughout of the crossbar could be above 20Gbps when the operating frequency is 100MHz, and the minimum latency is three clock cycles. The proposed crossbar scheduling as a sub-module could be easily integrated into SAR real-time imaging system."
pub.1163330068,Real-time institution video data analysis using fog computing and adaptive background subtraction,"The increasing demand for video surveillance systems has led to a surge in research towards developing smart video surveillance systems that can combat the growing levels of insecurity. However, the massive amount of video data generated by these systems has overwhelmed the storage and processing capabilities of analytic applications. This paper proposes a fog computing-based smart video surveillance system that provides less latency, network bandwidth, and response time by localizing data to the edges of the network. The proposed system incorporates two key preprocessing steps, namely adaptive key frame extraction and adaptive contour-based background subtraction, to increase the quality of detecting abnormal motions from surveillance video streams. The adaptive key frame extraction mechanism extracts key frames from the available frames in the video sequence using a sliding window technique. The high-level semantic information of video frames is learned using the visual geometry group-16 Transfer Learning (VGG-16 TL) technique to better represent the video content. The adaptive contour-based background subtraction mechanism separates target foreground pixels from the background scenes, paving the way for easy detection of abnormal motions in the video frames. The proposed system leverages fog computing to process and store data, reducing latency and improving overall performance. The fog layer performs adaptive background subtraction, contour detection, and object analysis, ensuring timely processing of video frames and minimizing the need for transmitting large amounts of data to the cloud. The proposed system is evaluated using a real-time video dataset in terms of accuracy, compression ratio, precision, recall, processing time, latency, and network bandwidth, demonstrating the efficacy of the proposed system for abnormal motion detection and data transmission. Overall, the proposed fog computing-based smart video surveillance system provides an effective solution for detecting abnormal motions in real-time institution video data with reduced latency, network bandwidth, and response time, demonstrating the potential of fog computing for video surveillance applications."
pub.1122315495,High speed and energy efficient deep neural network for edge computing,"Edge computing enables data-stream acceleration with real-time data processing without latency, and allows for efficient data processing in that large amounts of data can be processed near the source with the ability to process data without ever putting it into a public cloud adds a useful layer of security for sensitive data. The edge computing-based architecture design and analysis play key impacts for the future Internet of Things (IoT) infrastructure development. In this work, we design a low power hybrid structured deep neural network (Hybrid-DNN), which employs memristive synapses working in a hierarchical information processing fashion and delay-based spiking neural network (SNN) modules as the readout layer, and provide a novel data layout method to allow the Hybrid DNN running a computationally intensive deep learning algorithm on limited resource edge devices. Motivated by the recent findings in neuromorphic computing and edge computing, we design a hybrid structured DNN combining both depth-in-space (spatial) and depth-in-time (temporal) deep learning architectures. Our Hybrid-DNN employs memristive synapses working in a hierarchical information processing fashion and delay-based spiking neural network modules as the readout layer."
pub.1061535604,Safe Data Parallelism for General Streaming,"Streaming applications process possibly infinite streams of data and often have both high throughput and low latency requirements. They are comprised of operator graphs that produce and consume data tuples. General streaming applications use stateful, selective, and user-defined operators. The stream programming model naturally exposes task and pipeline parallelism, enabling it to exploit parallel systems of all kinds, including large clusters. However, data parallelism must either be manually introduced by programmers, or extracted as an optimization by compilers. Previous data parallel optimizations did not apply to selective, stateful and user-defined operators. This article presents a compiler and runtime system that automatically extracts data parallelism for general stream processing. Data-parallelization is safe if the transformed program has the same semantics as the original sequential version. The compiler forms parallel regions while considering operator selectivity, state, partitioning, and graph dependencies. The distributed runtime system ensures that tuples always exit parallel regions in the same order they would without data parallelism, using the most efficient strategy as identified by the compiler. Our experiments using 100 cores across 14 machines show linear scalability for parallel regions that are computation-bound, and near linear scalability when tuples are shuffled across parallel regions."
pub.1175627474,Advanced Image Analysis in Offshore Drilling Surveillance via Edge-Cloud Framework Leveraging Deep Reinforcement Learning,"The rapid increase in camera installations on off-shore drilling platforms has intensified the challenge of high-concurrency video data processing. Traditional single-cloud server video analysis is becoming inadequate, leading to heightened processing latency and bandwidth overuse. In response, we propose an edge-cloud collaborative video object detection architecture based on a GRU-Enhanced Double Deep Q-Network (GE-DDQN). Our architecture utilizes the YOLOv8 algorithm for object detection and incorporates a GE-DDQN model for efficient task offloading between edge and cloud computing. A token bucket mechanism is employed to regulate data offloading rates from edge devices, optimizing collaborative efficiency for high-performance detection. Comparative experiments on real offshore drilling platform video data underscore the superiority of our method in managing dynamic and complex video streams. The architecture demonstrates remarkable video analysis performance in this domain, achieving a precision of 90.2% and a processing speed of 25.45 FPS, marking a significant advancement in edge-cloud video analytics."
pub.1120867212,Scaling Ordered Stream Processing on Shared-Memory Multicores,"Many modern applications require realtime processing of large volumes of high speed data. Such data processing can be specified in the form of a dataflow graph that exposes multiple opportunities for parallelizing its execution, in the form of data, pipeline and task parallelism. This paper focuses on the problem of effectively parallelizing ordered streaming computations by exploiting low overhead, shared memory parallelism on a multicore machine. We propose an adaptive runtime that dynamically maps the exposed parallelism in a streaming computation to that of the machine using scheduling heuristics. Further, we address some key problems in effectively realizing ordered data parallelism. We propose a new approach for parallelizing partitioned stateful operators that can handle load imbalance across partitions effectively and mostly avoid delays due to ordering constraints. We also present a low latency, non-blocking concurrent data structure to order outputs produced by concurrent workers on an operator. Finally, we perform an in-depth empirical evaluation illustrating the trade-offs and effectiveness of our concurrent data-structures and scheduling heuristics using micro-benchmarks and on the TPCx-BB benchmark."
pub.1172725896,Indexing spatiotemporal trajectory data streams on key-value storage,"In a trajectory management system, moving objects are typically equipped with GPS devices to report their locations to a data store. Given the potentially high frequency of location updates by multiple moving objects, these data stores often operate under write-intensive conditions. Existing trajectory indexing methods, such as XZ-ordering, which are geared towards static trajectory data, may experience considerable latency under such demanding workloads. In response, this paper introduces spatio-temporal index structures that can be constructed with low latency. Trajectories are categorized into two types for storage: ‘live’ and ‘static’. Live trajectories are indexed by the Dual-Key Encoding (DKE) scheme, where each data point is represented by two key-value entries, facilitating both ID-temporal and spatial queries. Static trajectories, on the other hand, offer a more compact storage solution, reducing the overhead associated with live trajectories. Upon the completion of a trip, live trajectory data is transformed into a static trajectory entry through a compaction process. To augment the efficiency of spatial index construction for static trajectories, we introduce a new encoding scheme, XS2, coupled with an adaptive segmentation policy, AdaptSeg, to optimize trajectory segmentation, thereby enhancing index building and query processing efficiency. The indexing methods are demonstrated atop of LevelDB, an LSM-based key-value storage library, resulting the proposed LevelDBST system. Performance evaluations conducted using both synthetic and real-world datasets reveal that LevelDBST is capable of constructing spatial indexes for continuously updating trajectories with reduced latency, in comparison to traditional XZ-ordering methods. This efficiency is achieved while maintaining an acceptable balance in data accessing time and storage costs."
pub.1093359650,High-Level Synthesis In Latency Insensitive System Methodology,"This paper presents our contribution in terms of synchronization processor to a SoC design methodology based on the theory of the latency insensitive systems (LIS). This methodology 1) promotes pre-developed IPs intensive reuse, 2) segments inter-IPs interconnects with relay stations to break critical paths and 3) brings robustness to data stream irregularities to IPs by encapsulation into a synchronization wrapper. Our contribution consists in IP encapsulation into a new wrapper model containing a synchronization processor, which speed and area are optimized and synthesizability guarantied. The main benefit of our approach is to preserve the local IP performances when encapsulating them. This approach is part of the RNRT ALIPTA project, which targets design automation of intensive digital signal processing systems with GAUT [1], a high-level synthesis tool."
pub.1150102321,Edge-Cloud Collaboration for Human Activity Recognition on Multiple Subjects,"Multi-subject video analysis is one of the most important problems in the field of visual perception for human activity recognition on multiple subjects nowadays. However, multi-subject video analysis is difficult to achieve real-time performance at the edge due to the limited resources of edge devices and the high complexity of the Convolutional Neural Networks (CNN) model used in this task. The common processing method is to upload the video data to the cloud. However, due to the influence of network bandwidth, the transmission time is not fixed, and the latency cannot be guaranteed. Thus, statically deployed model configurations cannot meet some dynamically changing scenarios. To address these challenges, in this paper, we propose an edge-cloud collaboration processing system for multi-subject video stream analysis, which can dynamically configure and optimize the related configurations according to specific scenarios. Specifically, we provide an adaptive configuration optimization solution based on context awareness for edge devices with limited resources such that multi-subject video stream analysis can be processed completely at the edge. For other complex scenarios, we propose an edge-cloud collaboration method to achieve task segmentation and collaboration to meet the performance requirements of the complex scenarios. Experimental results show that our method can achieve an average accuracy of 91.3% and the latency of less than 78ms with arbitrary runtime state."
pub.1173044080,RTGDC: a real-time ingestion and processing approach in geospatial data cube for digital twin of earth,"The emergence of the Digital Twin of Earth (DTE) signifies a pivotal advancement in the field of the Digital Earth, which includes observations, simulations, and predictions regarding the state of the Earth system and its temporal evolution. While the Geospatial Data Cubes (GDCs) hold promise in probing the DTE across time, existing GDC approaches are not well-suited for real-time data ingestion and processing. This paper proposes the design and implementation of a real-time ingestion and processing approach in Geospatial Data Cube (RTGDC), achieved by ingesting real-time observation streams into data cubes. The methodology employs a publish/subscribe model for efficient observation ingestion. To optimize observation processing within the data cube, a distributed streaming computing framework is utilized in the implementation of the RTGDC. This approach significantly enhances the real-time capabilities of GDCs, providing an efficient solution that brings the realization of the DTE concept within closer reach. In RTGDC, two cases involving local and global scales were implemented, and their real-time performance was evaluated based on latency, throughput, and parallel efficiency."
pub.1093644679,CloudJet4BigData: Streamlining Big Data via an Accelerated Socket Interface,"Big data needs to feed users with fresh processing results and cloud platforms can be used to speed up big data applications. This paper describes a new data communication protocol (CloudJet) for long distance and large volume big data accessing operations to alleviate the large latencies encountered in sharing big data resources in the clouds. It encapsulates a dynamic multi-stream/multi-path engine at the socket level, which conforms to Portable Operating System Interface (POSIX) and thereby can accelerate any POSIX-compatible applications across IP based networks. It was demonstrated that CloudJet accelerates typical big data applications such as very large database (VLDB), data mining, media streaming and office applications by up to tenfold in real-world tests."
pub.1106903423,Violation Resolution in Distributed Stream Networks,"Distributed stream networks continuously track the global score of the data and alert whenever a given threshold is crossed. The global score is computed by applying a scoring function over the aggregated streams. However, the sheer volume and dynamic nature of the streams impose excessive communication overhead.Most recent approaches eliminate the need for continuous communication, by using local constraints assigned at the individual streams. These constraints guarantee that as long as no constraint is violated, the threshold is not crossed, and therefore no communication is necessary. Regrettably, local constraint violations become more and more frequent as the network grows and, in the presence of such violations, communication is inevitable.In this paper, we show that in most cases the violations can be resolved efficiently. Although our solution requires only a reduced subset of the network streams, finding the minimum resolving set is NP-hard. Through analysis of the probability for resolution, we suggest methods to select the resolving set so as to minimize the expected communication overhead and the expected latency of the process. Experimental results with both synthetic and real-life data sets demonstrate that our methods yield considerable improvements over existing approaches."
pub.1152955803,Towards data-driven additive manufacturing processes,"Additive Manufacturing (AM), or 3D printing, is a potential game-changer in medical and aerospatial sectors, among others. AM enables rapid prototyping (allowing development/manufacturing of advanced components in a matter of days), weight reduction, mass customization, and on-demand manufacturing to reduce inventory costs. At present, though, AM has been showcased in many pilot studies but has not reached broad industrial application. Online monitoring and data-driven decision-making are needed to go beyond existing offline and manual approaches. We aim at advancing the state-of-the-art by introducing the STRATA framework. While providing APIs tailored to AM printing processes, STRATA leverages common processing paradigms such as stream processing and key-value stores, enabling both scalable analysis and portability. As we show with a real-world use case, STRATA can support online analysis with sub-second latency for custom data pipelines monitoring several processes in parallel."
pub.1127177098,Compiler-Assisted Data Streaming for Regular Code Structures,"The performance of modern processors is often limited by execution stalls resulting from long memory access latencies. Compile-time optimizations, deep cache hierarchies and prefetching mechanisms already provide significant performance gains, by performing memory accesses in parallel with computation. However, they are reaching a throughput improvement limit. Hence, new solutions that effectively exploit the memory access patterns to improve processing throughput are required. To achieve this objective, a new compiler-assisted data streaming method is proposed. It leverages static analysis and code transformations with an on-chip data streaming support as a viable alternative to prefetching mechanisms for regular code structures. Static analysis is used to identify and encode memory accesses with a dedicated representation. Then, a code transformation algorithm detaches data indexation and address calculation from computation, allowing for a significant code reduction. An on-chip data stream controller, attached to the L1 data cache, is used to autonomously generate memory accesses from the pattern representation and reorganize the data transfers in streams, with the aid of stream buffers. When compared with state-of-the-art prefetchers, the proposed solution provides up to 26 percent of code reduction, an IPC improvement of 2.4x, and an average performance improvement of 40 percent."
pub.1174591662,Optimization of Flink distributed system with FPGA-based architecture,"With the continuous increase in the volume of Internet of Things (IoT) data, the limited number of servers deployed at the edge poses a pressing challenge for efficiently processing large-scale data streams. This study aims to explore the application of FPGA hardware acceleration technology to enhance the performance of the Apache Flink framework to meet the growing demands of users. The paper advocates the utilization of FPGA as a hardware accelerator for enhancing the performance of the Flink framework. This is achieved by harnessing the power of PCIe technology and integrating seamlessly with the OpenCL standard, which is tailored for heterogeneous systems. The JVM-FPGA communication mechanism is optimized using a data transmission pipeline mechanism. Through experiments and evaluations of two typical computationally intensive tasks, matrix multiplication and vector addition, it is demonstrated that the performance of the new framework is reduced in terms of latency, throughput is increased by 2.77 times, and CPU utilization is significantly enhanced. This study provides an innovative solution for efficiently processing largescale data streams in edge environments, successfully demonstrating that hardware acceleration can significantly improve the performance of the Apache Flink framework under resource constraints in edge environments, better meeting the continuously growing demands of IoT data."
pub.1095899444,Edge datastore for distributed vision analytics,"Autonomous machine vision is a powerful tool to address challenges in multiple domains including national security (for example, video surveillance), health care (for example, patient monitoring), and transportation (for example, autonomous vehicles). Distributed vision, where multiple cameras observe a specific geographic area 24/7, enables smart understanding of events in a physical environment with minimal human intervention. We observe that the cloud paradigm alone does not offer a pathway to real-time distributed vision processing. With potentially thousands of cameras, hundreds of gigabytes data per second needs to be transferred to the cloud, saturating the bandwidth of the network. More importantly, vision applications are inherently latency-critical with a high demand for real-time scene analysis (for example, feature extraction and object tracking). To meet latency requirements, computation - including both processing of raw video streams to identify objects, and analytics on this data, needs to be brought to the edge of the network. While object recognition may be done locally at the end node (next to the camera), vision analytics requires access to data generated across different nodes. For example, a subject of interest may need to be tracked across multiple cameras to identify the nature of activities. This creates a need for a low latency distributed data store communicating over a dynamic communication network (most often wireless), to be implemented at the edge. Moreover, the data store must be able to address the limited storage at the end nodes (typically gigabytes). Additionally, privacy and security are prime concerns in the design of such a distributed edge storage."
pub.1009784147,A complexity effective communication model for behavioral modeling of signal processing applications,"In this paper, we argue that the address space of memory regions that participate in inter task communication is over-specified by the traditional communication models used in behavioral modeling, resulting in sub-optimal implementations. We propose shared messaging communication model and the associated channels for efficient inter task communication of high bandwidth data streams in behavioral models of signal processing applications. In shared messaging model, tasks communicate data through special memory regions whose address space is unspecified by the model without introducing non determinism. Address space to these regions can be assigned during mapping of application to specific architecture, by exploring feasible alternatives. We present experimental results to show that this flexibility reduces the complexity (e.g., communication latency, memory usage) of implementations significantly (up to an order of magnitude)."
pub.1154514645,Event Driven Architecture for Message Streaming data driven Microservices systems residing in distributed version control system,"A large quantity of statistics is generated each day using DevOps techniques from many sources; inclusive of sensors, internet site utilization, gadget activities, and so on. This sparkling waft of activities have to be treated through modern packages. Occasion Micro-offerings enable structures extract beneficial statistics from those occurrences and help customers in making important commercial enterprise choices. In this work, an Event Driven Architecture is presented for Message Streaming data driven Microservices systems residing in distributed version control system. Processing of Messages and Stream Processing have been considered in this work. Latency in Event driven messaging system has been calculated by taking few cases of software projects. Finally, a brief overview has been given for Event-driven architecture versus message-driven architecture."
pub.1134323734,"To Share, or not to Share Online Event Trend Aggregation Over Bursty Event Streams","Complex event processing (CEP) systems continuously evaluate large workloads
of pattern queries under tight time constraints. Event trend aggregation
queries with Kleene patterns are commonly used to retrieve summarized insights
about the recent trends in event streams. State-of-art methods are limited
either due to repetitive computations or unnecessary trend construction.
Existing shared approaches are guided by statically selected and hence rigid
sharing plans that are often sub-optimal under stream fluctuations. In this
work, we propose a novel framework Hamlet that is the first to overcome these
limitations. Hamlet introduces two key innovations. First, Hamlet adaptively
decides whether to share or not to share computations depending on the current
stream properties at run time to harvest the maximum sharing benefit. Second,
Hamlet is equipped with a highly efficient shared trend aggregation strategy
that avoids trend construction. Our experimental study on both real and
synthetic data sets demonstrates that Hamlet consistently reduces query latency
by up to five orders of magnitude compared to the state-of-the-art approaches."
pub.1138989520,Klink: Progress-Aware Scheduling for Streaming Data Systems,"Modern stream processing engines (SPEs) process large volumes of events propagated at high velocity through multiple queries. To improve performance, existing SPEs generally aim to minimize query output latency by minimizing, in turn, the propagation delay of events in query pipelines. However, for queries containing commonly used blocking operators such as windows, this scheduling approach can be inefficient. Watermarks are events popularly utilized by SPEs to correctly process window operators. Watermarks are injected into the stream to signify that no events preceding their timestamp should be further expected. Through the design and development of Klink, we leverage these watermarks to robustly infer stream progress based on window deadlines and network delay, and to schedule query pipeline execution that reflects stream progress. Klink aims to unblock window operators and to rapidly propagate events to output operators while performing judicious memory management. We integrate Klink into the popular open source SPE Apache Flink and demonstrate that Klink delivers significant performance gains over existing scheduling policies on benchmark workloads for both scale-up and scale-out deployments."
pub.1138989511,"To Share, or not to Share Online Event Trend Aggregation Over Bursty Event Streams","Complex event processing (CEP) systems continuously evaluate large workloads of pattern queries under tight time constraints. Event trend aggregation queries with Kleene patterns are commonly used to retrieve summarized insights about the recent trends in event streams. State-of-art methods are limited either due to repetitive computations or unnecessary trend construction. Existing shared approaches are guided by statically selected and hence rigid sharing plans that are often sub-optimal under stream fluctuations. In this work, we propose a novel framework Hamlet that is the first to overcome these limitations. Hamlet introduces two key innovations. First, Hamlet adaptively decides at run time whether to share or not to share computations depending on the current stream properties to harvest the maximum sharing benefit. Second, Hamlet is equipped with a highly efficient shared trend aggregation strategy that avoids trend construction. Our experimental study on both real and synthetic data sets demonstrates that Hamlet consistently reduces query latency by up to five orders of magnitude compared to state-of-the-art approaches."
pub.1148796479,HYPERSONIC: A Hybrid Parallelization Approach for Scalable Complex Event Processing,"The ability to promptly and efficiently detect arbitrarily complex patterns in massive real-time data streams is a crucial requirement in many modern applications. The ever-growing scale of these applications and the sophistication of the patterns involved make it imperative to employ advanced solutions that can optimize pattern detection. One of the most prominent and well-established ways to achieve the above goal is to apply complex event processing (CEP) in a parallel manner, using a multi-core machine and/or a distributed environment. However, the inherent tightly coupled nature of CEP severely limits the scalability of the parallelization methods currently available. In this paper, we introduce a novel parallelization mechanism for efficient complex event processing over data streams. This mechanism is based on a hybrid two-tier model combining multiple layers of parallelism. By employing a fine-grained load balancing model, this multi-layered approach leads to a substantial increase in event detection throughput, while at the same time reducing the latency and the memory consumption. An extensive experimental evaluation on multiple real-life datasets shows that our approach consistently outperforms state-of-the-art CEP parallelization methods by a factor of two to three orders of magnitude."
pub.1142937391,Data Analytics and Mining,"The proliferation of the internet and advancement in technology have prompted the generation of humongous amounts of data at breakneck speeds from multitudinous sources such as IoT devices, clickstreams, social media, video surveillance systems, and various kinds of sensors. Such kind of data updates with high frequency and loses its value and relevance in a brief timeframe. Therefore, it becomes imperative to process and analyze such data on-the-fly enabling time-critical decision making in numerous applications. Real-time querying and big data streams are the new requirements. The earlier batch processing frameworks such as Hadoop endow valuable insight into what has happened in the past but aren’t able to deal with what is happening currently. Due to slower response time and high latency, it isn’t completely suitable to handle the dynamic real-time data; furthermore, the ability to make the right decisions and take proper actions at the opportune time can’t be achieved. Hence, we need additional tools to cope with these new demands. This chapter covers new technologies such as in-memory computing and stream processing, and a brief overview of real-time architectures is also presented. This chapter discusses the state-of-the-art real-time analytics platforms such as Apache Storm, Apache Spark, Apache Flink, and so on that can be applied to real-time applications, and finally this chapter concludes with the comparison of these platforms based on various essential features such as latency, throughput, delivery guarantees, and so on that can help in choosing a particular platform for certain applications. The proliferation of the internet and advancement in technology have prompted the generation of humongous amounts of data at breakneck speeds from multitudinous sources such as IoT devices, clickstreams, social media, video surveillance systems, and various kinds of sensors. Due to the internet explosion and fast technological progress, unfathomable amounts of diverse data are emitted at high speeds from numerous sources which cannot be managed by traditional processing and storage systems such as Relational Database Management Systems. Real-time analytics is a sort of big data analytics in which colossal amounts of data are processed and analyzed strictly within a specific timeline. Traditional systems have been based on slow disk storage and relational databases using a Structured Query language. Apache Spark is an open-source and distributed framework for big data processing."
pub.1041297205,Diagnosis and optimization of application prefetching performance,"Hardware prefetchers are effective at recognizing streaming memory access patterns and at moving data closer to the processing units to hide memory latency. However, hardware prefetchers can track only a limited number of data streams due to finite hardware resources. In this paper, we introduce the term streaming concurrency to characterize the number of parallel, logical data streams in an application. We present a simulation algorithm for understanding the streaming concurrency at any point in an application, and we show that this metric is a good predictor of the number of memory requests initiated by streaming prefetchers. Next, we try to understand the causes behind poor prefetching performance. We identified four prefetch unfriendly conditions and we show how to classify an application's memory references based on these conditions. We evaluated our analysis using the SPEC CPU2006 benchmark suite. We selected two benchmarks with unfavorable access patterns and transformed them to improve their prefetching effectiveness. Results show that making applications more prefetcher friendly can yield meaningful performance gains."
pub.1144107368,Serverless data pipeline approaches for IoT data in fog and cloud computing,"With the increasing number of Internet of Things (IoT) devices, massive amounts of raw data is being generated. The latency, cost, and other challenges in cloud-based IoT data processing have driven the adoption of Edge and Fog computing models, where some data processing tasks are moved closer to data sources. Properly dealing with the flow of such data requires building data pipelines, to control the complete life cycle of data streams from data acquisition at the data source, edge and fog processing, to Cloud side storage and analytics. Data analytics tasks need to be executed dynamically at different distances from the data sources and often on very heterogeneous hardware devices. This can be streamlined by the use of a Serverless (or FaaS) cloud computing model, where tasks are defined as virtual functions, which can be migrated from edge to cloud (and vice versa) and executed in an event-driven manner on data streams. In this work, we investigate the benefits of building Serverless data pipelines (SDP) for IoT data analytics and evaluate three different approaches for designing SDPs: (1) Off-the-shelf data flow tool (DFT) based, (2) Object storage service (OSS) based and (3) MQTT based. Further, we applied these strategies on three fog applications (Aeneas, PocketSphinx, and custom Video processing application) and evaluated the performance by comparing their processing time (computation time, network communication and disk access time), and resource utilization. Results show that DFT is unsuitable for compute-intensive applications such as video or image processing, whereas OSS is best suitable for this task. However, DFT is nicely fit for bandwidth-intensive applications due to the minimum use of network resources. On the other hand, MQTT-based SDP is observed with increase in CPU and Memory usage as the number of users rose, and experienced a drop in data units in the pipeline for PocketSphinx and custom video processing applications, however it performed well for Aeneas which had low size data units."
pub.1046153634,Enlister,"In this paper, we describe the concept & design of a real-time question RS (recommender system), the Enlister project, for the biggest Chinese Q&A (Questions and Answers) website and evaluate its performance on massive data from this real-world practice. We demonstrate how we weigh in among different recommendation algorithms and optimization methods. To enhance recommendation accuracy and handling time-sensitive questions, we propose a large scale real-time RS based on the combination of machine learning algorithms and the stream computing technology. Considering of algorithm flexibility and performance, we use the maximum entropy model as the fundamental model design in the CTR (click-through rate) prediction of recommendation items. In the perspective of the Enlister system architecture, we illustrate how we divide and conquer massive data processing problem with a novel stream computing design which reduces the data process latency down to seconds. Finally we analyze the online test result and prove our design concept by achieving a series of significant improvements."
pub.1142512617,CANS: Communication Limited Camera Network Self-Configuration for Intelligent Industrial Surveillance,"Realtime and intelligent video surveillance via camera networks involve computation-intensive vision detection tasks with massive video data, which is crucial for safety in the edge-enabled industrial Internet of Things (IIoT). Multiple video streams compete for limited communication resources on the link between edge devices and camera networks, resulting in considerable communication congestion. It postpones the completion time and degrades the accuracy of vision detection tasks. Thus, achieving high accuracy of vision detection tasks under the communication constraints and vision task deadline constraints is challenging. Previous works focus on single camera configuration to balance the tradeoff between accuracy and processing time of detection tasks by setting video quality parameters. In this paper, an adaptive camera network self-configuration method (CANS) of video surveillance is proposed to cope with multiple video streams of heterogeneous quality of service (QoS) demands for edge-enabled IIoT. Moreover, it adapts to video content and network dynamics. Specifically, the tradeoff between two key performance metrics, i.e., accuracy and latency, is formulated as an NP-hard optimization problem with latency constraints. A low-complexity algorithm is proposed to solve the optimization problem based on greedy searching. Simulation on real-world surveillance datasets demonstrates that the proposed CANS method achieves low end-to-end latency (13 ms on average) with high accuracy (92%) with network dynamics, which validates its effectiveness."
pub.1141111858,CANS: Communication Limited Camera Network Self-Configuration for Intelligent Industrial Surveillance,"Realtime and intelligent video surveillance via camera networks involve
computation-intensive vision detection tasks with massive video data, which is
crucial for safety in the edge-enabled industrial Internet of Things (IIoT).
Multiple video streams compete for limited communication resources on the link
between edge devices and camera networks, resulting in considerable
communication congestion. It postpones the completion time and degrades the
accuracy of vision detection tasks. Thus, achieving high accuracy of vision
detection tasks under the communication constraints and vision task deadline
constraints is challenging. Previous works focus on single camera configuration
to balance the tradeoff between accuracy and processing time of detection tasks
by setting video quality parameters. In this paper, an adaptive camera network
self-configuration method (CANS) of video surveillance is proposed to cope with
multiple video streams of heterogeneous quality of service (QoS) demands for
edge-enabled IIoT. Moreover, it adapts to video content and network dynamics.
Specifically, the tradeoff between two key performance metrics, \emph{i.e.,}
accuracy and latency, is formulated as an NP-hard optimization problem with
latency constraints. Simulation on real-world surveillance datasets
demonstrates that the proposed CANS method achieves low end-to-end latency (13
ms on average) with high accuracy (92\% on average) with network dynamics. The
results validate the effectiveness of the CANS."
pub.1181203455,Leveraging Multi-Modal Data for Efficient Edge Inference Serving,"Real-time analytics over data streams is often performed on edge devices, which offer privacy guarantees and lower-latency responses compared to centralized processing in the cloud. Data streams originating from sensors, mobile phones, or IoT devices are diverse and span multiple modalities, including RGB videos from cameras, time series data from wearable sensors, and audio signals. Previous research has focused on optimizing the individual analytical tasks associated with each stream, with a special emphasis on deep learning, which is computationally intensive and may be used to analyze video streams, among other things. While advances in deep learning have significantly improved inference accuracy (e.g. for computer vision tasks), state-of-the-art models are not well-suited for edge computing environments. Novel approaches are required to substantially reduce the computational burden, since edge systems are heterogeneous and typically have fewer GPU resources available for inference with deep learning models. We show that leveraging data from multiple modalities can complement or sometimes even replace resource-intensive inference, while maintaining or enhancing accuracy. We present DAISY: a Data-Aware Inference Serving sYstem which leverages multi-modal data to increase inference accuracy by dynamically selecting an appropriate model for each request. We thoroughly evaluate the proposed approach using state-of-the-art models and real-world data, which shows an increase in SLO attainment up to 60%, with a corresponding increase in inference accuracy of 5%."
pub.1171095619,Exploring the Dynamics of Data Transmission in 5G Networks: A Conceptual Analysis,"This conceptual analysis examines the dynamics of data transmission in 5G
networks. It addresses various aspects of sending data from cameras and LiDARs
installed on a remote-controlled ferry to a land-based control center. The
range of topics includes all stages of video and LiDAR data processing from
acquisition and encoding to final decoding, all aspects of their transmission
and reception via the WebRTC protocol, and all possible types of network
problems such as handovers or congestion that could affect the quality of
experience for end-users. A series of experiments were conducted to evaluate
the key aspects of the data transmission. These include simulation-based
reproducible runs and real-world experiments conducted using open-source
solutions we developed: ""Gymir5G"" - an OMNeT++-based 5G simulation and
""GstWebRTCApp"" - a GStreamer-based application for adaptive control of media
streams over the WebRTC protocol. One of the goals of this study is to
formulate the bandwidth and latency requirements for reliable real-time
communication and to estimate their approximate values. This goal was achieved
through simulation-based experiments involving docking maneuvers in the Bay of
Kiel, Germany. The final latency for the entire data processing pipeline was
also estimated during the real tests. In addition, a series of simulation-based
experiments showed the impact of key WebRTC features and demonstrated the
effectiveness of the WebRTC protocol, while the conducted video codec
comparison showed that the hardware-accelerated H.264 codec is the best.
Finally, the research addresses the topic of adaptive communication, where the
traditional congestion avoidance and deep reinforcement learning approaches
were analyzed. The comparison in a sandbox scenario shows that the AI-based
solution outperforms the WebRTC baseline GCC algorithm in terms of data rates,
latency, and packet loss."
pub.1118591127,Efficient data streaming multiway aggregation through concurrent algorithmic designs and new abstract data types,"Data streaming relies on continuous queries to process unbounded streams of
data in a real-time fashion. It is commonly demanding in computation capacity,
given that the relevant applications involve very large volumes of data. Data
structures act as articulation points and maintain the state of data streaming
operators, potentially supporting high parallelism and balancing the work
between them. Prompted by this fact, in this work we study and analyze
parallelization needs of these articulation points, focusing on the problem of
streaming multiway aggregation, where large data volumes are received from
multiple input streams. The analysis of the parallelization needs, as well as
of the use and limitations of existing aggregate designs and their data
structures, leads us to identify needs for proper shared objects that can
achieve low-latency and high throughput multiway aggregation. We present the
requirements of such objects as abstract data types and we provide efficient
lock-free linearizable algorithmic implementations of them, along with new
multiway aggregate algorithmic designs that leverage them, supporting both
deterministic order-sensitive and order-insensitive aggregate functions.
Furthermore, we point out future directions that open through these
contributions. The paper includes an extensive experimental study, based on a
variety of aggregation continuous queries on two large datasets extracted from
SoundCloud, a music social network, and from a Smart Grid network. In all the
experiments, the proposed data structures and the enhanced aggregate operators
improved the processing performance significantly, up to one order of
magnitude, in terms of both throughput and latency, over the commonly-used
techniques based on queues."
pub.1128045943,LightSaber: Efficient Window Aggregation on Multi-core Processors,"Window aggregation queries are a core part of streaming applications. To support window aggregation efficiently, stream processing engines face a trade-off between exploiting parallelism (at the instruction/multi-core levels) and incremental computation (across overlapping windows and queries). Existing engines implement ad-hoc aggregation and parallelization strategies. As a result, they only achieve high performance for specific queries depending on the window definition and the type of aggregation function. We describe a general model for the design space of window aggregation strategies. Based on this, we introduce LightSaber, a new stream processing engine that balances parallelism and incremental processing when executing window aggregation queries on multi-core CPUs. Its design generalizes existing approaches: (i) for parallel processing, LightSaber constructs a parallel aggregation tree (PAT) that exploits the parallelism of modern processors. The PAT divides window aggregation into intermediate steps that enable the efficient use of both instruction-level (i.e., SIMD) and task-level (i.e., multi-core) parallelism; and (ii) to generate efficient incremental code from the PAT, LightSaber uses a generalized aggregation graph (GAG), which encodes the low-level data dependencies required to produce aggregates over the stream. A GAG thus generalizes state-of-the-art approaches for incremental window aggregation and supports work-sharing between overlapping windows. LightSaber achieves up to an order of magnitude higher throughput compared to existing systems-on a 16-core server, it processes 470 million records/s with 132 ?s average latency."
pub.1123198390,Deep Q-Learning for Chunk-based Caching in Data Processing Networks,"A Data Processing Network (DPN) streams massive volumes of data collected and stored by the network to multiple processing units to compute desired results in a timely fashion. Due to ever-increasing traffic, distributed cache nodes can be deployed to store hot data and rapidly deliver them for consumption. However, prior work on caching policies has primarily focused on the potential gains in network performance, e.g., cache hit ratio and download latency, while neglecting the impact of cache on data processing and consumption. In this paper, we propose a novel framework, DeepChunk, which leverages deep Q-learning for chunk-based caching in DPN. We show that cache policies must be optimized for both network performance during data delivery and processing efficiency during data consumption. Specifically, DeepChunk utilizes a model-free approach by jointly learning limited network, data streaming, and processing statistics at runtime and making cache update decisions under the guidance of powerful deep Q-learning. It enables a joint optimization of multiple objectives including chunk hit ratio, processing stall time, and object download time while being self-adaptive under the time-varying workload and network conditions. We build a prototype implementation of DeepChunk with Ceph, a popular distributed object storage system. Our extensive experiments and evaluation demonstrate significant improvement, i.e., 43% in total reward and 39% in processing stall time, over a number of baseline caching policies."
pub.1127793525,Towards Streaming Perception,"Embodied perception refers to the ability of an autonomous agent to perceive
its environment so that it can (re)act. The responsiveness of the agent is
largely governed by latency of its processing pipeline. While past work has
studied the algorithmic trade-off between latency and accuracy, there has not
been a clear metric to compare different methods along the Pareto optimal
latency-accuracy curve. We point out a discrepancy between standard offline
evaluation and real-time applications: by the time an algorithm finishes
processing a particular frame, the surrounding world has changed. To these
ends, we present an approach that coherently integrates latency and accuracy
into a single metric for real-time online perception, which we refer to as
""streaming accuracy"". The key insight behind this metric is to jointly evaluate
the output of the entire perception stack at every time instant, forcing the
stack to consider the amount of streaming data that should be ignored while
computation is occurring. More broadly, building upon this metric, we introduce
a meta-benchmark that systematically converts any single-frame task into a
streaming perception task. We focus on the illustrative tasks of object
detection and instance segmentation in urban video streams, and contribute a
novel dataset with high-quality and temporally-dense annotations. Our proposed
solutions and their empirical analysis demonstrate a number of surprising
conclusions: (1) there exists an optimal ""sweet spot"" that maximizes streaming
accuracy along the Pareto optimal latency-accuracy curve, (2) asynchronous
tracking and future forecasting naturally emerge as internal representations
that enable streaming perception, and (3) dynamic scheduling can be used to
overcome temporal aliasing, yielding the paradoxical result that latency is
sometimes minimized by sitting idle and ""doing nothing""."
pub.1121839348,DeepChunk: Deep Q-Learning for Chunk-Based Caching in Wireless Data Processing Networks,"A Data Processing Network (DPN) streams massive volumes of data collected and stored by the network to multiple processing units to compute desired results in a timely fashion. Due to ever-increasing traffic, distributed cache nodes can be deployed to store hot data and rapidly deliver them for consumption. However, prior work on caching policies has primarily focused on the potential gains in network performance, e.g., cache hit ratio and download latency, while neglecting the impact of cache on data processing and consumption. In this paper, we propose a novel framework, DeepChunk, which leverages deep Q-learning for chunk-based caching in wireless DPN. We show that cache policies must be optimized for both network performance during data delivery and processing efficiency during data consumption. Specifically, DeepChunk utilizes a model-free approach by jointly learning limited network, data streaming, and processing statistics at runtime and making cache update decisions under the guidance of deep Q-learning. It enables a joint optimization of multiple objectives including chunk hit ratio, processing stall time, and object download time while being self-adaptive under the time-varying workload and network conditions. We build a prototype implementation of DeepChunk with Ceph, a popular distributed object storage system. Based on real-world Wifi and 4G traces, our extensive experiments and evaluation demonstrate significant improvement, i.e., 52% increase in total reward and 68% decrease in processing stall time, over a number of baseline caching policies."
pub.1125158382,Enhancement of Motion Feedback Latency for Wireless Virtual Reality in IEEE 802.11 WLANs,"Wireless virtual reality (VR) that offloads VR processing to a powerful PC and streams rendered VR image frames to a VR headset wirelessly is a promising technology for high-quality interactive VR experiences with free mobility and high immersiveness. Realizing wireless VR in IEEE 802.11 WLANs is feasible when combined with the Timewarp technique, but minimal latency is still of importance for higher quality of VR service. In this paper, we reveal that the bi-directional transmission nature of wireless VR (VR frame data in downlink and motion-feedback data in uplink) and the resulting delay of motion feedback is a major challenge of wireless VR in IEEE 802.11 WLANs. To combat this challenge, we propose three basic methods—(1) prioritizing aged motion data, (2) using reverse direction, and (3) limiting the aggregation size of downlink transmission—and demonstrate that all enhance the latency and jitter of motion feedback, and, among these, limiting the aggregation size is most effective. We then design a scheme to adjust the aggregation size of downlink transmission in conjunction with the use of reverse direction to best support responsive and frequent motion feedback while preserving the downlink transmission of VR frame data."
pub.1120693084,CO-STAR: A collaborative prediction service for short-term trends on continuous spatio-temporal data,"Over various sensory data of Internet of Things, not only the current situation but also the future trends of many fields are required instantly to promote the business. As a typical requirement, the short-term prediction on spatio-temporal data stream is imperative, but challenges still remain due to the inherent limitation of long calculative time and insufficient predictive precision. In this paper, a novel prediction service CO-STAR is proposed in the highway domain. On the continuous toll data of the whole highway network, the service employs non-parametric regression model to predict the traffic volume of all the stations periodically. Considering both spatial and temporal business characteristics, a collaborative paradigm of online stream computing and offline batch processing is adopted to balance the efficiency and precision. On the real data of one Chinese provincial highway and the simulated data, our service can hold minute-level executive latency with nearly 10 percent improvement for the predictive precision in extensive experiments."
pub.1132866779,I-Scheduler: Iterative scheduling for distributed stream processing systems,"Task allocation in Data Stream Processing Systems (DSPSs) has a significant impact on performance metrics such as data processing latency and system throughput. An application processed by DSPSs can be represented as a Directed Acyclic Graph (DAG), where each vertex represents a task and the edges show the dataflow between the tasks. Task allocation can be defined as the assignment of the vertices in the DAG to the physical compute nodes such that the data movement between the nodes is minimised. Finding an optimal task placement for DSPSs is NP-hard. Thus, approximate scheduling approaches are required to improve the performance of DSPSs. In this paper, we propose a heuristic scheduling algorithm which reliably and efficiently finds highly communicating tasks by exploiting graph partitioning algorithms and a mathematical optimisation software package. We evaluate the communication cost of our method using three micro-benchmarks, showing that we can achieve results that are close to optimal. We further compare our scheduler with two popular existing schedulers, R-Storm and Aniello et al.’s ‘Online scheduler’ using two real-world applications. Our experimental results show that our proposed scheduler outperforms R-Storm, increasing throughput by up to 30%, and improves on the Online scheduler by 20%–86% as a result of finding a more efficient schedule. 1 1 This work is an extension of I-Scheduler (Eskandari et al., 2018 [1])."
pub.1094660447,Multi-aggregate-query scheduling over data streams,"With the wide applications of data streams in many fields, such as sensor network monitoring and internet traffic control, query processing over data streams has become increasingly important. In these applications, multiple aggregate queries are registered in the system, and have different sliding window sizes and different frequency upper bounds. How to share the results of these queries is a challenge. Prior work studies how to detect common tasks of these queries and share the results by computing the common tasks only once. Hybrid scheduling first addressed this problem and used the earliest-deadline-first (EDF) method. However, this work did not present a method for computing the scheduling. We formulate the scheduling problem among multiple aggregate queries with different sliding window sizes and different frequency upper bounds over data streams and propose a combination rule to classify these queries. Then, we present an efficient scheduling algorithm to decide whether a query should be executed more often than necessary, as long as the interval between two consecutive executions is less than the frequency upper bound. We also combine our scheduling algorithm with EDF to handle underloaded and overloaded situations. An experimental study shows that our scheduling algorithms are more efficient than no scheduling and EDF in terms of the number of scanned tuples, the throughput and the latency."
pub.1139734872,Mitigating latency problems in vision-based autonomous UAVs,"In this paper, the latency problem of computer vision systems is addressed in the framework of autonomous Unmanned Aerial Vehicles. Recent advancements in sensors and embedded electronic boards made it possible to load, even on small size drones, cameras and image processing devices. Here, a navigation system based on computer vision is considered as one of the most popular applications exploiting this technology in substitution of Global Navigation Satellite System solutions. The main issues when working with a video stream are the limited frame rate (i.e., small sampling frequency), and the non negligible computational time for extracting features from the images (i.e., latency). In particular, the latency negatively affects a position controller that exploits data from the computer vision system, preventing its usage for precise positioning applications. In this paper, a possible solution is designed according to this recipe: First, a sensor fusion technique able to compensate the latency is adopted to estimate the velocity using the position of the computer vision system and the accelerations provided by a Inertial Measurement Unit. Then, a controller is developed using two feedback loops, the inner one accounting for the estimated velocity, and the outer one exploiting the delayed position. Test experiments, showing very positive results, are finally reported."
pub.1122263628,Preparatory delta phase response is correlated with naturalistic speech comprehension performance,"Abstract While human speech comprehension is thought to be an active process that involves top-down predictions, it remains unclear how predictive information is used to prepare for the processing of upcoming speech information. We aimed to identify the neural signatures of the preparatory processing of upcoming speech. Participants selectively attended to one of two competing naturalistic, narrative speech streams, and a temporal response function (TRF) method was applied to derive event-related-like neural responses from electroencephalographic data. The phase responses to the attended speech at the delta band (1–4 Hz) were correlated with the comprehension performance of individual participants, with a latency of -200–0 ms before onset over the fronto-central and left-lateralized parietal regions. The phase responses to the attended speech at the alpha band also correlated with comprehension performance, but with a latency of 650–980 ms post-onset over fronto-central regions. Distinct neural signatures were found for the attentional modulation, taking the form of TRF-based amplitude responses at a latency of 240–320 ms post-onset over the left-lateralized fronto-central and occipital regions. Our findings reveal how the brain gets prepared to process an upcoming speech in a continuous, naturalistic speech context."
pub.1086124641,Hardware Accelerated Application Integration Processing,"The growing number of (cloud) applications and devices massively increases the communication rate and volume pushing integration systems to their (throughput) limits. While the usage of modern hardware like Field Programmable Gate Arrays (FPGAs) led to low latency when employed for query and event processing, application integration adds yet unexplored processing opportunities. In this industry paper, we explore how to program integration semantics (e. g., message routing and transformation) in form of Enterprise Integration Patterns (EIP) on top of an FPGA, thus complementing the existing research on FPGA data processing. We focus on message routing, re-define the EIP for stream processing and propose modular hardware implementations as templates that are synthesized to circuits. For our real-world ""connected car"" scenario (i. e., composed patterns), we discuss common and new optimizations especially relevant for hardware integration processes. Our experimental evaluation shows competitive throughput compared to modern general-purpose CPUs and discusses the results."
pub.1132670250,Evolution of the Data Quality Monitoring and Prompt Processing System in the proto DUNE-SP experiment," The DUNE Collaboration currently operates an experimental program based at CERN which includes a beam test and an extended cosmic ray run of two large-scale prototypes of the massive Liquid Argon Time Projection Chamber (LArTPC) for the DUNE Far Detector. The volume of data collected by the single-phase prototype (protoDUNE-SP) amounts to 3PB and the sustained rate of data sent to mass storage is O (100) MB/s. Data Quality Monitoring was implemented by directing a fraction of the data stream to the protoDUNE prompt processing system (p3s) which is optimized for continuous low-latency calculation of the vital detector metrics and various graphics including event displays. It served a crucial role throughout the life cycle of the experiment. We present our experience in leveraging the CERN computing environment and operating the system over an extended period of time, while adapting to evolving requirements and computing platform. "
pub.1156191389,An LSTM-based method for Message Queue Throughput Prediction,"Message Queue has been long and widely used in cloud-based service, playing the role of communicating applications across multiple platform asynchronously. As a part of Message Oriented Middleware(MOM), it provides solutions for data transfer in distributed systems. Many open source message queues, such as Apache ActiveMQ, RabbitMQ, and Apache Kafka, are designed to be scalable and durable to process stream data with low latency and high throughput. In this paper, we introduce a recurrent neural based network which has proven to solve time-series prediction problems. Through our experiments we provide an empirical study at model parameters configuration, data processing, and model training. Our proposed method achieves better performance compared with traditional time-series prediction model."
pub.1106345246,Feature Constrained Parallel Data Processing Approach for Spatiotemporal Event Detection,"The enormous usage of Online Social Networks (OSN) leads to unleashing usage of the smart technologies in social life which becomes intertwined. Generating meaningful patterns out of various location-based streaming social big data fascinated the techies around the globe to develop innovative approaches to enhance emergency support on-demand. In the specific situation like disasters, nature setbacks down at unpredictable instances. During such circumstances, a scalable and low latency analytics approach is highly required to identify the event and its location at very high speed for recovery. In this paper, the spatiotemporal event detection approach is proposed based on dynamic features emerged from the location of interest in OSN using a map-reduce framework. The experimental results address the advantage of MapReduce paradigm is indeed suitable for scalable and high-speed data streams with minimal latency. In addition, an information theoretic emergency logistic mapper is discussed as a part of disaster recovery phase which can be accomplished via social big data analysis in near future."
pub.1144171829,A Real-time AIS Data Computing Platform Based on Flink,"Ship AIS (automatic identification system) data is an important part of water traffic perception data. It covers the core elements of ship navigation, including the dynamic information about geographical location and the static information of ship. Only in in the Yangtze River Basin, tens of millions of AIS data are generated every day. The traditional way is to store AIS data by offline processing and analysis. With the application of smart maritime, more and more applications need to master the ship dynamics in real-time. In order to realize the real-time processing and analysis of AIS data, a real-time AIS data processing and analysis platform based on the streaming computing framework is proposed. Flink is an open-source distributed stream computing framework with high throughput and low latency. It is used to realize the operations of AIS data De duplication, analysis, statistics in real-time. Through the data visualization technology, the computing results are displayed in real-time. It realizes the effective tracking of ship status and the real-time statistics of ships under the jurisdiction. It is of practical significance to improve the efficiency of ship supervision and maritime management."
pub.1112447817,Analyzing efficient stream processing on modern hardware," Modern Stream Processing Engines (SPEs) process large data volumes under tight latency constraints. Many SPEs execute processing pipelines using message passing on shared-nothing architectures and apply a partition-based scale-out strategy to handle high-velocity input streams. Furthermore, many state-of-the-art SPEs rely on a Java Virtual Machine to achieve platform independence and speed up system development by abstracting from the underlying hardware.   In this paper, we show that taking the underlying hardware into account is essential to exploit modern hardware efficiently. To this end, we conduct an extensive experimental analysis of current SPEs and SPE design alternatives optimized for modern hardware. Our analysis highlights potential bottlenecks and reveals that state-of-the-art SPEs are not capable of fully exploiting current and emerging hardware trends, such as multi-core processors and high-speed networks. Based on our analysis, we describe a set of design changes to the common architecture of SPEs to scale-up on modern hardware. We show that the single-node throughput can be increased by up to two orders of magnitude compared to state-of-the-art SPEs by applying specialized code generation, fusing operators, batch-style parallelization strategies, and optimized windowing. This speedup allows for deploying typical streaming applications on a single or a few nodes instead of large clusters. "
pub.1144316827,Energy efficient resource controller for Apache Storm,"Summary Apache Storm is a distributed processing engine that can reliably process unbounded streams of data for real‐time applications. While recent research activities mostly focused on devising a resource allocation and task scheduling algorithm to satisfy high performance or low latency requirements of Storm applications across a distributed and multi‐core system, finding a solution that can optimize the energy consumption of running applications remains an important research question to be further explored. In this article, we present a controlling strategy for CPU throttling that continuously optimize the level of consumed energy of a Storm platform by adjusting the voltage and frequency of the CPU cores while running the assigned tasks under latency constraints defined by the end‐users. The experimental results running over a Storm cluster with 4 physical nodes (total 24 cores) validates the effectiveness of proposed solution when running multiple compute‐intensive operations. In particular, the proposed controller can keep the latency of analytic tasks, in terms of 99th latency percentile, within the quality of service requirement specified by the end‐user while reducing the total energy consumption by 18% on average across the entire Storm platform."
pub.1044962254,Dynamic Load Balancing Techniques for Distributed Complex Event Processing Systems,"Applying real-time, cost-effective Complex Event processing (CEP) in the cloud has been an important goal in recent years. Distributed Stream Processing Systems (DSPS) have been widely adopted by major computing companies such as Facebook and Twitter for performing scalable event processing in streaming data. However, dynamically balancing the load of the DSPS’ components can be particularly challenging due to the high volume of data, the components’ state management needs, and the low latency processing requirements. Systems should be able to cope with these challenges and adapt to dynamic and unpredictable load changes in real-time. Our approach makes the following contributions: (i) we formulate the load balancing problem in distributed CEP systems as an instance of the job-shop scheduling problem, and (ii) we present a novel framework that dynamically balances the load of CEP engines in real-time and adapts to sudden changes in the volume of streaming data by exploiting two balancing policies. Our detailed experimental evaluation using data from the Twitter social network indicates the benefits of our approach in the system’s throughput."
pub.1092128623,Accelerating breadth-first graph search on a single server by dynamic edge trimming," Breadth-first graph search (a.k.a., BFS) is one of the typical in-memory computing models with complicated and frequent memory accesses. Existing single-server graph computing systems fail to take advantage of access pattern of BFS for performance optimization, hence suffering from a lot of extra memory latencies due to accessing no longer useful data elements of a big graph as well as wasting plenty of computing resources for processing them. In this article, we propose FastBFS, a new approach that accelerates breadth-first graph search on a single server by leverage of the access pattern during iterating over a big graph. First, FastBFS uses an edge-centric graph processing model to obtain the high bandwidth of sequential memory and/or disk access without expensive data preprocessing. Second, with a dynamic and asynchronous trimming mechanism, FastBFS can efficiently reduce the size of a big graph by eliminating useless edges in parallel with the computation. Third, FastBFS schedules I/O streams efficiently and can attain greater parallelism if an additional disk is available. We implement FastBFS by modifying the X-Stream system developed by EPFL. Our experimental results show that FastBFS can attain speedups of up to 7.9 × and 10.4 × in the computing speed compared with X-stream and GraphChi respectively. With an additional disk, the performance can be further improved."
pub.1130274931,Testing Early and Often: End-to-End Testing on the Double Asteroid Redirection Test (DART),"The NASA Double Asteroid Redirection Test (DART) is a technology demonstration mission designed, built and operated by the Johns Hopkins Applied Physics Lab (JHU/APL). The mission's primary objectives are to 1) achieve a hypervelocity kinetic impact with the secondary member of the binary asteroid (65603) Didymos and 2) downlink at least two images of the target with a pixel sample distance of 66 cm or better. Impact guidance is achieved using the onboard Small-body Maneuvering Autonomous Real-time Navigation (SMART Nav) system developed by JHU/APL. The SMART Nav system ingests images from an onboard imager, performs image processing and ultimately guides the spacecraft to impact. In parallel to SMART Nav operations, the spacecraft streams images back to the ground in real-time. At least two images are required from the last twenty seconds of imaging to achieve the desired pixel sample distance. Both onboard guidance and realtime image streaming drive strict data latency requirements. These requirements are levied across multiple subsystems and interfaces, making verification challenging. The highly-integrated spacecraft design and compressed integration schedule also preclude DART from fully testing these data-streams on the flight system before launch. DART has prioritized an early, end-to-end system-test effort with engineering model components and high-fidelity testbeds to address these concerns. This risk reduction effort seeks to demonstrate critical interfaces with adequate data latencies before the start of the Spacecraft Integration and Test phase (I&T). This paper describes 1) the overall mission and spacecraft image processing and downlink driving requirements, 2) the resultant architecture, 3) the risk reduction philosophy and 4) the demonstration plan and early results."
pub.1131109848,Hybrid Workflow Provisioning and Scheduling on Edge Cloud Computing Using a Gradient Descent Search Approach,"The dramatic growth of the Internet of Things (IoT) technology in many application domains, ranging from intelligent video surveillance, smart retail to the Internet-of-Vehicles brings new computation challenges for rationalized utilization of computing resources. IoT application execution refers to hybrid processing model of stream and batch to achieve data analytics objectives. Hybrid workflow execution combines the challenges of latency-sensitive and resource-intensive processing. To resolve these challenges, we proposed a two stages hybrid workflow scheduling framework on edge cloud computing. In the first stage, we proposed a resource estimation algorithm based on a linear optimization approach, the gradient descent search (GDS) and in the second stage, we adopted a cluster-based provisioning and scheduling technique on heterogeneous edge cloud resources. This work provides a multi-objective optimization model for execution time and monetary cost under constraints of deadline and throughput. Results demonstrated the framework performance in controlling the execution of hybrid workflows by an efficient tuning for stream processing parameters, such as arrival rate and processing throughput. Under working constraints, the proposed scheduler provides significant improvement for large hybrid workflows in terms of execution time and monetary cost with an average of 8% and 35%, respectively."
pub.1167292108,Latency-Based Inter-Operator Scheduling for CNN Inference Acceleration on GPU,"Convolutional Neural Networks (CNNs) are widely deployed on the Graphics Processing Unit (GPU) to support Deep Learning (DL) based services. Popular DL frameworks usually ignore the inter-operator parallelism when executing the inference of CNNs, which results in high inference latency. Although some inter-operator scheduling methods have been proposed, there remains a critical trade-off issue between inference latency (effectiveness) and scheduling time (efficiency). In this article, we propose LIOS, a novel latency-based heuristic inter-operator scheduling method to balance inference latency and scheduling time. In LIOS, a CNN latency model is built based on the given CNN and GPU. Then every operator is assigned a priority value to represent its importance. During each iteration of the scheduling process, LIOS identifies the current data-independent operators, selects the operator with the highest priority value, and assigns it to the GPU stream with the smallest finish time. Extensive experimental results have demonstrated the effectiveness and efficiency of LIOS. For the effectiveness, LIOS can speed up the inference of normal-size and large-size CNNs by 1.13$\sim 1.59 \times$∼1.59× compared to sequential scheduling. This result is comparable to IOS, the latest state-of-the-art scheduling method. For the efficiency, LIOS can speed up the scheduling process by 7$\sim 9210\times$∼9210× compared to IOS."
pub.1095043118,Design of Complex Event-Processing IDS in Internet of Things,"With the development of Internet of Things (IoT), there have been more and more services and applications deployed in physical spaces and information systems. Massive number of situation-aware sensors and devices are embedded in IoT environments, which produce huge amounts of data continuously for the IoT systems and platforms. Processing these data stream generated by the IoT networks with different patterns has raised new challenges for the real-time performance of intrusion detection system (IDS) in IoT environments, which has to react quickly to the hacking attacks and malicious activities to IoT. In recent years, Complex Event Processing (CEP) technology provides new solutions in the field of complex pattern identifications and real-time data processing, which can be used to improve the performance of traditional IDS in IoT environments. IDS integrated with CEP can be used to deal with patterns among events and process large volumes of messages with low latency. In this paper we proposed an event-processing IDS architecture in IoT environments on the basis of security requirements analysis for IDS. Then the implementation details for real-time event processing are also proposed, which is developed by Esper, a CEP engine for complex event processing and event series analysis."
pub.1010857232,Auto-parallelizing stateful distributed streaming applications,"Streaming applications transform possibly infinite streams of data and often have both high throughput and low latency requirements. They are comprised of operator graphs that produce and consume data tuples. The streaming programming model naturally exposes task and pipeline parallelism, enabling it to exploit parallel systems of all kinds, including large clusters. However, it does not naturally expose data parallelism, which must instead be extracted from streaming applications. This paper presents a compiler and runtime system that automatically extract data parallelism for distributed stream processing. Our approach guarantees safety, even in the presence of stateful, selective, and user-defined operators. When constructing parallel regions, the compiler ensures safety by considering an operator's selectivity, state, partitioning, and dependencies on other operators in the graph. The distributed runtime system ensures that tuples always exit parallel regions in the same order they would without data parallelism, using the most efficient strategy as identified by the compiler. Our experiments using 100 cores across 14 machines show linear scalability for standard parallel regions, and near linear scalability when tuples are shuffled across parallel regions."
pub.1013486370,"Pro Spark Streaming, The Zen of Real-Time Analytics Using Apache Spark","Learn the right cutting-edge skills and knowledge to leverage Spark Streaming to implement a wide array of real-time, streaming applications. This book walks you through end-to-end real-time application development using real-world applications, data, and code. Taking an application-first approach, each chapter introduces use cases from a specific industry and uses publicly available datasets from that domain to unravel the intricacies of production-grade design and implementation. The domains covered in Pro Spark Streaming include social media, the sharing economy, finance, online advertising, telecommunication, and IoT. In the last few years, Spark has become synonymous with big data processing. DStreams enhance the underlying Spark processing engine to support streaming analysis with a novel micro-batch processing model. Pro Spark Streaming by Zubair Nabi will enable you to become a specialist of latency sensitive applications by leveraging the key features of DStreams, micro-batch processing, and functional programming. To this end, the book includes ready-to-deploy examples and actual code. Pro Spark Streamingwill act as the bible of Spark Streaming. What You'll Learn Discover Spark Streaming application development and best practices Work with the low-level details of discretized streams Optimize production-grade deployments of Spark Streaming via configuration recipes and instrumentation using Graphite, collectd, and Nagios Ingest data from disparate sources including MQTT, Flume, Kafka, Twitter, and a custom HTTP receiver Integrate and couple with HBase, Cassandra, and Redis Take advantage of design patterns for side-effects and maintaining state across the Spark Streaming micro-batch model Implement real-time and scalable ETL using data frames, SparkSQL, Hive, and SparkR Use streaming machine learning, predictive analytics, and recommendations Mesh batch processing with stream processing via the Lambda architecture Who This Book Is For Data scientists, big data experts, BI analysts, and data architects."
pub.1094889275,Elastic Allocator: An Adaptive Task Scheduler for Streaming Query in the Cloud,"Many big data applications receive and process data in real time. These data, also known as data streams, are generated continuously and processed online in a low latency manner. Data stream is prone to change dramatically in volume, since its workload may have a variation of several orders between peak and valley periods. Fully provisioning resources for stream processing to handle the peak load is costly, while overprovisioning is wasteful when to deal with lightweight workload. Cloud computing emphasizes that resource should be utilized economically and elastically. An open question is how to allocate query task adaptively to keeping up the input rate of the data stream. Previous work focuses on using either local or global capacity information to improve the cluster CPU resource utilization, while the bandwidth utilization which is also critical to the system throughput is ignored or simplified. In this paper, we formalize the operator placement problem considering both the CPU and bandwidth usage, and introduce the Elastic Allocator. The Elastic Allocator uses a quantitative method to evaluate a nodes capacity and bandwidth usage, and exploit both the local and global resource information to allocate the query task in a graceful manner to achieve high resource utilization. The experimental results and a simple prototype built on top of Storm finally demonstrate that Elastic Allocator is adaptive and feasible in cloud computing environment, and has an advantage of improving and balancing system resource utilization."
pub.1151160862,Automating the Deployment of Artificial Intelligence Services in Multiaccess Edge Computing Scenarios,"With the increasing adoption of the edge computing paradigm, including multi-access edge computing (MEC) in telecommunication scenarios, many works have explored the benefits of adopting it. Since MEC, in general, presents a reduction in latency and energy consumption compared to cloud computing, it has been applied to deploy artificial intelligence services. This kind of service can have distinct requirements, which involve different computational resource capabilities as well different data formats or communication protocols to collect data. In this sense, we propose the VEF Edge Framework, which aims at helping the development and deployment of artificial intelligence services for MEC scenarios considering requirements as low-latency and CPU/memory consumption. We explain the VEF architecture and present experimental results obtained with a base case’s implementation: an object detection inference service deployed with VEF. The experiments measured CPU and memory usage for the VEF’s main components and the processing time for two procedures (inference and video stream handling)."
pub.1135854667,"Fog-Enabled Joint Computation, Communication and Caching Resource Sharing for Energy-Efficient IoT Data Stream Processing","Fog/edge computing has been recently regarded as a promising approach for supporting emerging mission-critical Internet of Things (IoT) applications on capacity and battery constrained devices. By harvesting and collaborating a massive crowd of devices in close proximity for computation, communication and caching resource sharing (i.e., 3C resources), it enables great potentials in low-latency and energy-efficient IoT task execution. To efficiently exploit 3C resources of fog devices in proximity, we propose F3C, a fog-enabled 3C resource sharing framework for energy-efficient IoT data stream processing by solving an energy cost minimization problem under 3C constraints. Nevertheless, the minimization problem proves to be NP-hard via reduction from a Generalized Assignment Problem (GAP). To cope with such challenge, we propose an efficient F3C algorithm based on an iterative task team formation mechanism which regards each task's 3C resource sharing as a subproblem solved by the elaborated min cost flow transformation. Via utility improving iterations, the proposed F3C algorithm is shown to converge to a stable system point. Extensive performance evaluations demonstrate that our F3C algorithm can achieve superior performance in energy saving compared to various benchmarks."
pub.1099682485,A Data Streaming Performance Evaluation Using Resource Constrained Edge Device,"High-Ievel automation comes as standard in the Industrial IoT(IIoT) environment. Automated IIoT devices lead the improvement of factory operational efficiency. They helped to reduce operational time and personnel expenses, and helped to increase operational accuracy. IIoT devices for smart factory are various in types, and they are designed to fulfill their designated works for factory in diverse ways. Their jobs include prediction of operation failure and anomaly detection to avert factory downtime. If stream data, which is generated by machines in the factory, can be processed faster than before, the downtime could be shorter and prediction could be made earlier. To make stream data processing time shorter, performing it close to the machine could be a good approach. If an IIoT device is smart enough to process data, it can obtain meaningful information by processing data by itself. We can also use another machine to collect data from small sensors, and process the data to make decision. We address this type of data processing which is performed at the machines as “Edge Computing”. “Edge” originally meant the edge of the network. If data processing can be performed without being transmitted to the cloud or another storage, it would be done faster. “Edge” devices usually have lower hardware capacity than “Server”, so we wanted to know which data processing engine can be used for edge devices. We selected three representative engines: Apache Flink, Apache Storm and Apache Spark streaming. We installed them on the Raspberry Pi 3 and evaluated their latencies and throughputs. The conclusion is that Apache Flink and Apache Spark Streaming showed competing results in all tests while Apache Storm showed worse performance than the others. It required more memory resources to ex cute its basic components, and this caused its long warming up time to perform our simple algorithms."
pub.1121981658,Lineage stash,"As cluster computing frameworks such as Spark, Dryad, Flink, and Ray are being deployed in mission critical applications and on larger and larger clusters, their ability to tolerate failures is growing in importance. These frameworks employ two broad approaches for fault tolerance: checkpointing and lineage. Checkpointing exhibits low overhead during normal operation but high overhead during recovery, while lineage-based solutions make the opposite tradeoff. We propose the lineage stash, a decentralized causal logging technique that significantly reduces the runtime overhead of lineage-based approaches without impacting recovery efficiency. With the lineage stash, instead of recording the task's information before the task is executed, we record it asynchronously and forward the lineage along with the task. This makes it possible to support large-scale, low-latency (millisecond-level) data processing applications with low runtime and recovery overheads. Experimental results for applications in distributed training and stream processing show that the lineage stash provides task execution latencies similar to checkpointing alone, while incurring a recovery overhead as low as traditional lineage-based approaches."
pub.1034644964,"Big Data Analytics with Spark, A Practitioner’s Guide to Using Spark for Large-Scale Data Processing, Machine Learning, and Graph Analytics, and High-Velocity Data Stream Processing","Big Data Analytics with Spark is a step-by-step guide for learning Spark, which is an open-source fast and general-purpose cluster computing framework for large-scale data analysis. You will learn how to use Spark for different types of big data analytics projects, including batch, interactive, graph, and stream data analysis as well as machine learning. In addition, this book will help you become a much sought-after Spark expert. Spark is one of the hottest Big Data technologies. The amount of data generated today by devices, applications and users is exploding. Therefore, there is a critical need for tools that can analyze large-scale data and unlock value from it. Spark is a powerful technology that meets that need. You can, for example, use Spark to perform low latency computations through the use of efficient caching and iterative algorithms; leverage the features of its shell for easy and interactive Data analysis; employ its fast batch processing and low latency features to process your real time data streams and so on. As a result, adoption of Spark is rapidly growing and is replacing Hadoop MapReduce as the technology of choice for big data analytics. This book provides an introduction to Spark and related big-data technologies. It covers Spark core and its add-on libraries, including Spark SQL, Spark Streaming, GraphX, and MLlib. Big Data Analytics with Spark is therefore written for busy professionals who prefer learning a new technology from a consolidated source instead of spending countless hours on the Internet trying to pick bits and pieces from different sources. The book also provides a chapter on Scala, the hottest functional programming language, and the program that underlies Spark. You’ll learn the basics of functional programming in Scala, so that you can write Spark applications in it. What's more, Big Data Analytics with Spark provides an introduction to other big data technologies thatare commonly used along with Spark, like Hive, Avro, Kafka and so on. So the book is self-sufficient; all the technologies that you need to know to use Spark are covered. The only thing that you are expected to know is programming in any language. There is a critical shortage of people with big data expertise, so companies are willing to pay top dollar for people with skills in areas like Spark and Scala. So reading this book and absorbing its principles will provide a boost—possibly a big boost—to your career."
pub.1040399245,WLAN System with Iterative Decoding of OFDM Multi-symbols,"In this paper, a transmission scheme with iterative processing of received signals is proposed for Wireless Local Area Networks (WLANs) employing spatial multiplexing. The scheme consists in applying channel coding, interleaving and iterative detection and decoding (IDD) separately for a tuple of OFDM symbols (hereinafter called an OFDM multi-symbol), transmitted simultaneously in the same signaling interval via different space streams. In a conventional approach to iterative decoding, high receiver latency occurs because of the need to wait until the whole (possibly very large) data frame has been acquired by the receiver prior to the start of any decoding. In the proposed solution, the latency is limited to single OFDM symbol interval. This is a great advantage when iterative decoding is used."
pub.1030887985,DVB-DSNG Modem High Level Synthesis in an Optimized Latency Insensitive System Context,"This paper presents our contribution in terms of synchronization processor to a SoC design methodology based on the theory of the latency insensitive systems (LIS) of Carloni et al.. This methodology 1) promotes pre-developed IPs intensive reuse, 2) segments inter-IPs interconnects with relay stations to break critical paths and 3) brings robustness to data stream irregularities to IPs by encapsulation into a synchronization wrapper. Our contribution consists in IP encapsulation into a new wrapper model containing a synchronization processor which speed and area are optimized and synthetizability guarantied. The main benefit of our approach is to preserve the local IP performances when encapsulating them. This approach is part of the RNRT ALIPTA project which targets design automation of intensive digital signal processing systems with GAUT [1], a high-level synthesis tool."
pub.1130335266,Fog Data Processing and Analytics for Health Care-Based IoT Applications,"Healthcare solutions have been improved with the onset of data processing and data analytics in the connected ecosystem. Healthcare 4.0 is all about providing personalized health care with the amalgamation of Internet of Things (IoT) devices. These devices collect and transfer the health data which is stored and processed in cloud. Cloud computing delivers accurate insights from that data but providing care in emergency demands immediate access to huge volume of data and performing analytics on it. This hampers the speed of decision-making ultimately leading to increased latency, cumbersome data storage, soaring operational costs which can’t be appropriate for chronic diseases such as Parkinson’s, Vertigo and other neurological diseases as those require continuous monitoring and real-time response. Introducing the fog layer between IoT devices and Cloud Computing helps in reducing data transmission overheads and minimizes the burden on cloud. It has the potential to tackle the issues of cloud computing by providing local data storage, suburbanized data analytics resulting into increased productivity. Data processing on fog computing should be done while considering the difference in data models, naming conventions and processing paradigms which includes stream processing, batch processing and serverless functions. Deploying fog node as analytic engine in healthcare applications can reduce the data flow on the network and provides round-the-clock engagement with patient, improved preventive care. The use cases of fog deployment can be effective remote monitoring system, equipment monitoring, smart equipment maintenance."
pub.1142523304,FPGA-Based Implementation of an Event-Driven Spiking Multi-Kernel Convolution Architecture,"This brief presents an event-driven spiking multi-kernel convolution architecture for processing address-event representation (AER) streams from dynamic vision sensor (DVS) chips. The processor architecture is designed based on leaky integrate-and-fire (LIF) neural model, and employs pipeline scheme to accelerate data processing. A new scheme for arranging neuron membrane potentials and kernels in memories is proposed, which enables row-by-row kernel processing and accelerates the multi-kernel convolution computation. An FPGA prototype of the proposed architecture is implemented on a Xilinx Zynq FPGA development board with 100 MHz clock frequency. The proposed processor architecture computes 64 filters with configurable kernel size (from $1 \times 1$ to $32 \times 32$ ) on input flow, obtaining the latency of 0.10 us to 10.33 us for convoluting an event, and the energy of 0.12 nJ and 12.08 nJ per event per convolution, respectively."
pub.1092302609,Real-Time FPGA-Based Detection of Speeded-Up Robust Features Using Separable Convolution,"In this paper, we propose a novel architecture for efficient detection of speeded-up robust features (SURF) for field-programmable gate array (FPGA). The main benefits of the proposed architecture are in real-time low-latency performance and scalability. The proposed solution provides a significant acceleration of salient points extraction that is fundamental image processing technique for vision-based methods including the simultaneous localization and mapping. Based on the presented practical results, the proposed architecture is capable of processing streaming image data at the rate of 140 Megapixels per second that roughly scales from the 640 $\times$ 480@420fps up to 1920 $\times$ 1080@60fps video streams on a low-end, low-cost FPGA solution (CycloneV). Moreover, the proposed feature detection utilizes only about 20 of logic elements of the FPGA which supports further parallel processing of multiple inputs."
pub.1093353455,A Complexity Effective Communication Model for Behavioral Modeling of Signal Processing Applications,"In this paper, we argue that the address space of memory regions that participate in inter task communication is over-specified by the traditional communication models used in behavioral modeling, resulting in sub-optimal implementations. We propose shared messaging communication model and the associated channels for efficient inter task communication of high bandwidth data streams in behavioral models of signal processing applications. In shared messaging model, tasks communicate data through special memory regions whose address space is unspecified by the model without introducing non determinism. Address space to these regions can be assigned during mapping of application to specific architecture, by exploring feasible alternatives. We present experimental results to show that this flexibility reduces the complexity (e.g., communication latency, memory usage) of implementations significantly (up to an order of magnitude)."
pub.1172220473,Normal and Resilient Mode FPGA-based Access Gateway Function Through P4-generated RTL,"Customizing packet processing is crucial in the evolving network landscape, especially with the rise of 5G telecommunications and beyond. Software-Defined Networking and programmable data planes, powered by the P4 language and FPGA-based platforms, offer dynamic network customization that can be used to implement resilient networks. With their high performance and programmability, FPGAs present cost-effective alternatives for diverse network applications, including offloading packet processing from servers. This paper introduces a configurable FPGA-based data plane implementing the Access Gateway Function (AGF). It offers a resilient operating mode to enhance network reliability and availability. The paper leverages the P4 language and the VitisNetP4 Intellectual Property to create RTL streams, enabling AGF on a pure FPGA target. The reported experimental results demonstrate that the proposed architecture can support 50K user flows with a resource utilization lower than 15% of that available in an Ultrascale+ FPGA (xcu280-fsvh2892-21-e). This leaves massive logic resources available to incorporate fault mitigation techniques and spare streams needed to enhance resiliency. Moreover, the presented workflow maintains an average latency of approximately 9 microseconds for each downstream or upstream packet."
pub.1022937053,FERARI,"In this demo, we present FERARI, a prototype that enables real-time Complex Event Processing (CEP) for large volume event data streams over distributed topologies. Our prototype constitutes, to our knowledge, the first complete, multi-cloud based end-to-end CEP solution incorporating: a) a user-friendly, web-based query authoring tool, (b) a powerful CEP engine implemented on top of a streaming cloud platform, (c) a CEP optimizer that chooses the best query execution plan with respect to low latency and/or reduced inter-cloud communication burden, and (d) a query analytics dashboard encompassing graph and map visualization tools to provide a holistic picture with respect to the detected complex events to final stakeholders. As a proof-of-concept, we apply FERARI to enable mobile fraud detection over real, properly anonymized, telecommunication data from T-Hrvatski Telekom network in Croatia."
pub.1149109003,Lunatory: A Real-Time Distributed Trajectory Clustering Framework for Web Big Data,"Web big data contains a wealth of valuable information, which can be extracted through web mining and knowledge extraction. Among them, the real-time location information of web can provide a richer calculation basis for existing applications, such as real-time monitoring systems and recommendation systems based on real-time trajectory clustering. However, as a trajectory is a sequence of user positions in the time dimension, the correlation calculation of the trajectories will inevitably incur a massive computational cost. In addition, such trajectory data is usually time-sensitive, that is, once the trajectory data has been generated and changed, the corresponding clustering results need to be output with low latency. Although the offline trajectory clustering has been well studied, extending such work to an online environment directly tends to incur (1) expensive network cost, (2) high processing latency, and (3) low accuracy results. To enable a real-time clustering on trajectory stream, we propose a distributed cLustering framework for hexagonal-based streaming trajectory (Lunatory). Lunatory covers three key components, that are: (1) Simplifier: to solve the problem of extensive network transmission in a distributed trajectory streaming system, a pivot trajectory data structure is introduced to simplify trajectories by reducing the number of samples and extracting key features; (2) Partitioner: to enhance the local computational efficiency of subsequent clustering, a hexagonal-based indexing strategy is proposed to index the pivot trajectories; (3) Executor extends DBSCAN to pivot trajectories and implements real-time trajectory clustering based on Flink. Empirical studies on real-world data validate the usefulness of our proposal and prove the huge advantage of our approach over available solutions in the literature."
pub.1132486883,Real-Time DAS VSP Acquisition and Processing on Single- and Multi-Mode Fibers,"Distributed acoustic sensing (DAS) is a rapidly evolving fiber-optic sensing technology that converts any standard fiber-optic cable into a distributed sensor capable of measuring acoustic events every meter over large (100 km+) distances. We describe a conveyance-agnostic DAS system for vertical seismic profiling (VSP) that has the capability to sense either single- or multi-mode sensing fibers, and encode all auxiliary seismic source signals directly onto the optical data stream using a series of piezo-electric fiber stretchers in-line with the sensing fiber. The DAS system can operate with latency-free synchronization via wireless telemetry between the seismic source controller and the DAS unit in either master or slave modes. Similar to geophone-based VSP data acquisition systems, the time-break signal is used to trigger data acquisition to ensure precise time synchronization at sub-millisecond accuracy between the DAS system and the seismic source. The system enables effective data management and the real-time generation of 1 millisecond sampled seismic and navigation data that conforms to the industry-standard seismic data formats, thus eliminating any latency between acquisition and field data deliverables."
pub.1172421494,Recent Advances in End-to-End Simultaneous Speech Translation,"Simultaneous speech translation (SimulST) is a demanding task that involves
generating translations in real-time while continuously processing speech
input. This paper offers a comprehensive overview of the recent developments in
SimulST research, focusing on four major challenges. Firstly, the complexities
associated with processing lengthy and continuous speech streams pose
significant hurdles. Secondly, satisfying real-time requirements presents
inherent difficulties due to the need for immediate translation output.
Thirdly, striking a balance between translation quality and latency constraints
remains a critical challenge. Finally, the scarcity of annotated data adds
another layer of complexity to the task. Through our exploration of these
challenges and the proposed solutions, we aim to provide valuable insights into
the current landscape of SimulST research and suggest promising directions for
future exploration."
pub.1176008439,Adaptive Stream Processing on Edge Devices through Active Inference,"The current scenario of IoT is witnessing a constant increase on the volume
of data, which is generated in constant stream, calling for novel architectural
and logical solutions for processing it. Moving the data handling towards the
edge of the computing spectrum guarantees better distribution of load and, in
principle, lower latency and better privacy. However, managing such a structure
is complex, especially when requirements, also referred to Service Level
Objectives (SLOs), specified by applications' owners and infrastructure
managers need to be ensured. Despite the rich number of proposals of Machine
Learning (ML) based management solutions, researchers and practitioners yet
struggle to guarantee long-term prediction and control, and accurate
troubleshooting. Therefore, we present a novel ML paradigm based on Active
Inference (AIF) -- a concept from neuroscience that describes how the brain
constantly predicts and evaluates sensory information to decrease long-term
surprise. We implement it and evaluate it in a heterogeneous real stream
processing use case, where an AIF-based agent continuously optimizes the
fulfillment of three SLOs for three autonomous driving services running on
multiple devices. The agent used causal knowledge to gradually develop an
understanding of how its actions are related to requirements fulfillment, and
which configurations to favor. Through this approach, our agent requires up to
thirty iterations to converge to the optimal solution, showing the capability
of offering accurate results in a short amount of time. Furthermore, thanks to
AIF and its causal structures, our method guarantees full transparency on the
decision making, making the interpretation of the results and the
troubleshooting effortless."
pub.1048040178,Flood,"Distributed data stream processing (DSP) is used to analyze information and raise alarms in business-critical scenarios such as financial fraud-detection, clickstream processing, network security, traffic control, or real-time KPI computations. Processing this information efficiently is very challenging because the nature of continuous streaming sources is varying in nature: often the amount of data and processing changes with time of day and day of week and frequently has unexpected spikes. Thus, the result is that most DSP computations are either over-provisioned, introducing increased cost and wasted energy, or are under-provisioned and, either incur in performance degradation or denial-of-service, or have to resort to load shedding. We demonstrate Flood, a scalable, elastic DSP engine that addresses these problems. By using a scalable computing model, MapReduce, and adequately monitoring running computations our system is able to decide, in runtime, if there is a lack or a surplus of resources. Flood then acts autonomically by requesting or releasing computing nodes, without losing tuples or redoing computation, at the same time making sure that latency and throughput requirements are guaranteed."
pub.1094761612,"Real-time, scalable route planning using a stream-processing infrastructure","A key transportation service is traffic-dependent route planning, i.e., finding a path from a source to a destination on a road network that incurs the minimum travel time delay and that considers current and future traffic conditions on all road links. This kind of route planning requires dynamic time-dependent shortest path computations, since the cost of each link is different at different intervals of time, and these time-dependent costs get updated as new traffic information is obtained. In this paper, we describe a stream-processing infrastructure that can perform scalable, real-time, time-dependent shortest path computations with high throughput and low latency on large road networks. In our performance experiments, we show that it can perform hundreds of such shortest path computations per second, with an average delay of less than 1 second on a road network with tens of thousands of nodes and links. Our experiments were carried out on the Stockholm road network, and the traffic data for purposes of getting the travel time delay on each link was derived from a real GPS data-set obtained from vehicles on this network. In this paper, we also highlight the benefits of performing dynamic, time-dependent shortest path computations. For example, our data from Stockholm suggests that computing traffic-dependent shortest paths can help decrease travel time by up to 62% compared to using a less dynamic method based on fixed road speed limits. Also the shortest path for various origin-destination pairs changes frequently during the course of a day. These findings motivate the need for highly scalable, real-time time-dependent shortest-path computations, as performed by our stream processing infrastructure."
pub.1106386706,2D Parallel Architecture for Morphological Operators Supporting Multiple Shaped Structuring Elements," This paper presents a reconfigurable 2D parallel architecture designed to implement efficiently two fundamental gray scale morphological operations: dilation and erosion. The architecture is expected to be used as a hardware core integrated in real time application. Moreover, the proposed architecture allows processing data on high resolution images with reconfigurable size and arbitrary shape of structuring elements. The main advantage of the proposed architecture is its low latency, higher throughput and higher processing frame rate. Additionally, the architecture processes data on stream which avoids the need of any buffering at input level. The architecture is successfully implemented and prototyped on Virtex-5 field programmable gate array. Implementation results show that the architecture can achieve high frame rate: for example, for a 1024x768 image and 11x11 structuring element, we reach a frame rate of 341 Fps."
pub.1131745573,Detecting Temporal Anomalies in Business Processes Using Distance-Based Methods,"Outlier detection in process mining refers to either infrequent behavior in relation to the underlying business process models or to anomalous latencies of task execution (temporal anomalies). In this work, we focus on the latter form of anomalies and we propose distance-based methods. Compared to solutions relying on probability distribution analysis and based on the experimental evaluation presented, our proposal is shown to be capable of covering both trace and event outliers, and being more efficient and effective. More specifically, running times of our technique are lower by up to an order of magnitude, while we achieve significantly higher precision and recall."
pub.1093649539,Dynamic Load Distribution in the Borealis Stream Processor**This work has been supported by the NSF under grants IIS-0086057 and IIS-0325838.,"Distributed and parallel computing environments are becoming cheap and commonplace. The availability of large numbers of CPU's makes it possible to process more data at higher speeds. Stream-processing systems are also becoming more important, as broad classes of applications require results in real-time. Since load can vary in unpredictable ways, exploiting the abundant processor cycles requires effective dynamic load distribution techniques. Although load distribution has been extensively studied for the traditional pull-based systems, it has not yet been fully studied in the context of push-based continuous query processing. In this paper, we present a correlation based load distribution algorithm that aims at avoiding overload and minimizing end-to-end latency by minimizing load variance and maximizing load correlation. While finding the optimal solution for such a problem is NP-hard, our greedy algorithm can find reasonable solutions in polynomial time. We present both a global algorithm for initial load distribution and a pair-wise algorithm for dynamic load migration."
pub.1123582659,Dynamic Control of CPU Cap Allocations in Stream Processing and Data-Flow Platforms,"This paper focuses on Timely dataflow programming model for processing streams of data. We propose a technique to define CPU resource allocation (i.e., CPU capping) with the goal to improve response time latency in such type of applications with different quality of service (QoS) level, as they are concurrently running in a shared multi-core computing system with unknown and volatile demand. The proposed solution predicts the expected performance of the underlying platform using an online approach based on queuing theory and adjusts the corrections required in CPU allocation to achieve the most optimized performance. The experimental results confirms that measured performance of the proposed model is highly accurate while it takes into account the percentiles on the QoS metrics. The theoretical model used for elastic allocation of CPU share in the target platform takes advantage of design principals in model predictive control theory and dynamic programming to solve an optimization problem. While the prediction module in the proposed algorithm tries to predict the temporal changes in the arrival rate of each data flow, the optimization module uses a system model to estimate the interference among collocated applications by continuously monitoring the available CPU utilization in individual nodes along with the number of outstanding messages in every intermediate buffer of all TDF applications. The optimization module eventually performs a cost-benefit analysis to mitigate the total amount of QoS violation incidents by assigning the limited CPU shares among collocated applications. The proposed algorithm is robust (i.e., its worst-case output is guaranteed for arbitrarily volatile incoming demand coming from different data streams), and if the demand volatility is not large, the output is optimal, too. Its implementation is done using the TDF framework in Rust for distributed and shared memory architectures. The experimental results show that the proposed algorithm reduces the average and p99 latency of delay-sensitive applications by 21% and 31.8%, respectively, while can reduce the amount of QoS violation incidents by 98% on average."
pub.1095569482,RECONFIGURABLE CACHE MEMORY ARCHITECTURE FOR INTEGRAL IMAGE AND INTEGRAL HISTOGRAM APPLICATIONS,"Cache memory can reduce the memory latency between processor and off-chip DRAM, and it usually occupies a large area of the whole system. However, for accessing integral images and integral integral histograms, which are famous for getting an arbitrary-sized block summation and histogram in a constant speed and widely implemented in many applications, the read and write mechanisms of cache are not suitable for such algorithms with stream processing characteristic. In this paper, a reconfigurable cache memory architecture is proposed with two modes: normal cache mode and Row-Based Stream Processing (RBSP) mode, which is a specific memory architecture for data accessing of integral images and integral histograms. Moreover, the data reuse scheme between different filter sizes and different rows are taken into consideration to further reduce the data access to the off-chip DRAM. In addition, a method called Memory Dividing Technique (MDT) is also proposed to further reduce the word-length. SURF algorithm and center-surround histogram salience map are implemented to verify the proposed design. The experimental results show that the proposed architecture can save 89.15% and 68.12% memory read cycle count for these two applications compared to the traditional fully-associative cache in the same memory size."
pub.1136462642,Fast and Accurate Terrain Image Classification for ASTER Remote Sensing by Data Stream Mining and Evolutionary-EAC Instance-Learning-Based Algorithm,"Remote sensing streams continuous data feed from the satellite to ground station for data analysis. Often the data analytics involves analyzing data in real-time, such as emergency control, surveillance of military operations or scenarios that change rapidly. Traditional data mining requires all the data to be available prior to inducing a model by supervised learning, for automatic image recognition or classification. Any new update on the data prompts the model to be built again by loading in all the previous and new data. Therefore, the training time will increase indefinitely making it unsuitable for real-time application in remote sensing. As a contribution to solving this problem, a new approach of data analytics for remote sensing for data stream mining is formulated and reported in this paper. Fresh data feed collected from afar is used to approximate an image recognition model without reloading the history, which helps eliminate the latency in building the model again and again. In the past, data stream mining has a drawback in approximating a classification model with a sufficiently high level of accuracy. This is due to the one-pass incremental learning mechanism inherently exists in the design of the data stream mining algorithm. In order to solve this problem, a novel streamlined sensor data processing method is proposed called evolutionary expand-and-contract instance-based learning algorithm (EEAC-IBL). The multivariate data stream is first expanded into many subspaces, and then the subspaces, which are corresponding to the characteristics of the features are selected and condensed into a significant feature subset. The selection operates stochastically instead of deterministically by evolutionary optimization, which approximates the best subgroup. Followed by data stream mining, the model learning for image recognition is done on the fly. This stochastic approximation method is fast and accurate, offering an alternative to the traditional machine learning method for image recognition application in remote sensing. Our experimental results show computing advantages over other classical approaches, with a mean accuracy improvement at 16.62%."
pub.1123685111,Application of Open-Source Big-Data Framework in Marine Information Processing,"Gao, X.; Wang, H., and Li, X., 2019. Application of open-source big-data framework in marine information processing. In: Li, L.; Wan, X.; and Huang, X. (eds.), Recent Developments in Practices and Research on Coastal Regions: Transportation, Environment and Economy. Journal of Coastal Research, Special Issue No. 98, pp. 187190. Coconut Creek (Florida), ISSN 0749-0208. Open-source big-data systems can use a variety of processing technologies. For workloads that only require batch processing, Hadoop, which is less time sensitive and less expensive than other solutions, would be a good choice. For workloads that only require stream processing, Storm can support a wider range of languages and achieve very low latency processing, but the default configuration can produce duplicate results and cannot guarantee order. Samza's tight integration with YARN and Kafka provides greater flexibility, easier-to-use multiteam usage, and simpler replication and state management. The most suitable solution depends mainly on the state of the data to be processed, the time required for processing, and the desired result. Specifically, using a full-featured solution or a solution that focuses primarily on a project requires a careful trade-off. As it matures and is widely accepted, similar issues need to be considered when evaluating any emerging and innovative solutions."
pub.1104689400,QuantCloud: Enabling Big Data Complex Event Processing for Quantitative Finance Through a Data-Driven Execution,"Quantitative Finance (QF) utilizes increasingly sophisticated mathematic models and advanced computer techniques to predict the movement of global markets, and price the derivatives and other assets. Being able to react quickly and intelligently to fast-changing markets is a decisive success factor for trading companies. To date, the rise of QF requires an integrated toolchain of enabling technologies to carry out complex event processing on the explosive growth and diversified forms of market metadata, in pursuit of a microsecond latency on an Exabyte-level dataset. Inspired by this, we present a data-driven execution paradigm that untangles the dependencies of complex processing events and integrate the paradigm with a big data infrastructure that streams time series data. This integrated platform is termed as the QuantCloud platform. Essentially, QuantCloud executes the complex event processing in a data-driven mode and manages large amounts of diversified market data in a data-parallel mode. To show its practicability and performance, we develop a prototype and benchmark by applying real-world QF research models on the New York Stock Exchange (NYSE) data. Using this prototype, we demonstrate this platform with an application to: (i) data cleaning and aggregating (including the computing of logarithmic returns from tick data and the finding the medians of grouped data) and (ii) data modeling: the autoregressive-moving average (ARMA) model. The performance results show that (a) this platform obtains a high throughput (usually in the order of millions of tick messages per second) and a sub-microsecond latency; (b) it fully executes data-dependent tasks through a data-driven execution; and (c) it implements a modular design approach for rapidly developing these data-crunching methods and QF research models. This platform resulting from an aggregated effort of the data-driven execution and big data infrastructure, offers the financial engineers with new insights and enhanced capabilities for effective and efficient incorporation of big data complex event processing technologies in their workflow."
pub.1171069623,A Vision-based Virtual Sensor to Enhance Privacy and Energy Efficiency on Edge Computing,"Integrating vision-based technologies into distributed sensor domains offers unprecedented potential for data collection. However, it raises privacy concerns over the incredible amount of extra information inadvertently carried by the video stream. On the other hand, the advent of tiny machine learning models running on edge devices with embedded GPUs/TPUs is revolutionizing computer vision and real-time tracking systems, enabling the local execution of computationally demanding tasks traditionally performed in the cloud. This study focuses on developing and characterizing vision-based virtual sensors capable of processing data from a local camera source to provide real-time measures of relevant metrics without storing or transmitting any video stream. The main advantages of vision-based virtual sensors running on the edge are data protection, reduced communication cost, and reduced detection latency. In addition, we propose a dynamic inference power manager (DIPM), based on adaptive frame rate, that allows us to explore the trade-off between power consumption and accuracy. Experimental results conducted on a real hardware platform show that the proposed virtual sensor, equipped with DIPM, can save up to 40% of the processing energy with a reduction of tracking accuracy lower than 10%, while retaining the privacy preservation benefits of virtual sensors."
pub.1131505934,Scalable Online Monitoring of Distributed Systems,"Distributed systems are challenging for runtime verification. Centralized specifications provide a global view of the system, but their semantics requires totally-ordered observations, which are often unavailable in a distributed setting. Scalability is also problematic, especially for online first-order monitors, which must be parallelized in practice to handle high volume, high velocity data streams. We argue that scalable online monitors must ingest events from multiple sources in parallel, and we propose a general model for input to such monitors. Our model only assumes a low-resolution global clock and allows for out-of-order events, which makes it suitable for distributed systems. Based on this model, we extend our existing monitoring framework, which slices a single event stream into independently monitorable substreams. Our new framework now slices multiple event streams in parallel. We prove our extension correct and empirically show that the maximum monitoring latency significantly improves when slicing is a bottleneck."
pub.1152242203,Experimental evaluation of virtual reality applications running on next-gen network scenarios with edge cloud assistance,"Volumetric video is an emerging key technology for immersive representation of 3D spaces and objects. Rendering volumetric video at client's end requires significant computational power which is challenging especially for mobile devices. One of the ways to mitigate this is to offload the rendering at the edge cloud and stream the video and audio to the thin client. Remote edge-cloud rendering may increase the end-to-end delay of the system due to the added network and processing latency which is greater than local rendering system. We investigate network latency in edge-based remote rendering over NextG networks and identify the bottleneck deteriorating the application performance. Further, we delve into the current state of the art and challenges of performing rendering remotely at the edge cloud and study the associated problems that need to be addressed in order to realize remote augmented reality(AR)/virtual reality (VR). Our prototype implementation shows effectiveness of maintaining the application QoE by prioritizing data at the level of a sub-flow and reducing the motion-to-photon latency."
pub.1107255941,A Multidomain Standards‐Based Fog Computing Architecture for Smart Cities,"Many of the problems arising from rapid urbanization and urban population growth can be solved by making cities “smart”. These smart cities are supported by large networks of interconnected and widely geo‐distributed devices, known as Internet of Things or IoT, that generate large volumes of data. Traditionally, cloud computing has been the technology used to support this infrastructure; however, some of the essential requirements of smart cities such as low‐latency, mobility support, location‐awareness, bandwidth cost savings, and geo‐distributed nature of such IoT systems cannot be met. To solve these problems, the fog computing paradigm proposes extending cloud computing models to the edge of the network. However, most of the proposed architectures and frameworks are based on their own private data models and interfaces, which severely reduce the openness and interoperability of these solutions. To address this problem, we propose a standard‐based fog computing architecture to enable it to be an open and interoperable solution. The proposed architecture moves the stream processing tasks to the edge of the network through the use of lightweight context brokers and Complex Event Processing (CEP) to reduce latency. Moreover, to communicate the different smart cities domains we propose a Context Broker based on a publish/subscribe middleware specially designed to be elastic and low‐latency and exploit the context information of these environments. Additionally, we validate our architecture through a real smart city use case, showing how the proposed architecture can successfully meet the smart cities requirements by taking advantage of the fog computing approach. Finally, we also analyze the performance of the proposed Context Broker based on microbenchmarking results for latency, throughput, and scalability."
pub.1061280719,Predictable Low-Latency Event Detection With Parallel Complex Event Processing,"The tremendous number of sensors and smart objects being deployed in the Internet of Things (IoT) pose the potential for IT systems to detect and react to live-situations. For using this hidden potential, complex event processing (CEP) systems offer means to efficiently detect event patterns (complex events) in the sensor streams and therefore, help in realizing a “distributed intelligence” in the IoT. With the increasing number of data sources and the increasing volume at which data is produced, parallelization of event detection is crucial to limit the time events need to be buffered before they actually can be processed. In this paper, we propose a pattern-sensitive partitioning model for data streams that is capable of achieving a high degree of parallelism in detecting event patterns, which formerly could only consistently be detected in a sequential manner or at a low parallelization degree. Moreover, we propose methods to dynamically adapt the parallelization degree to limit the buffering imposed on event detection in the presence of dynamic changes to the workload. Extensive evaluations of the system behavior show that the proposed partitioning model allows for a high degree of parallelism and that the proposed adaptation methods are able to meet a buffering limit for event detection under high and dynamic workloads."
pub.1094069220,FastBFS: Fast Breadth-First Graph Search on a Single Server,"Big graph computing can be performed over a single node, using recent systems such as GraphChi and X-Stream. Breadth-first graph search (a.k.a., BFS) has a pattern of marking each vertex only once as “visited” and then not using them in further computations. Existing single-server graph computing systems fail to take advantage of such access pattern of BFS for performance optimization, hence suffering from a lot of extra I/O latencies due to accessing no longer useful data elements of a big graph as well as wasting plenty of computing resources for processing them. In this paper, we propose FastBFS, a new approach that accelerates breadth-first graph search on a single server by leverage of the preceding access pattern during the BFS iterations over a big graph. First, FastBFS uses an edge-centric graph processing model to obtain the high bandwidth of sequential disk accesses without expensive data preprocessing. Second, with a new asynchronous trimming mechanism, FastBFS can efficiently reduce the size of a big graph by eliminating useless edges in parallel with the computation. Third, FastBFS schedules I/O streams efficiently and can attain greater parallelism if an additional disk is available. We implement FastBFS by modifying the X-Stream system developed by EPFL. Our experimental results show that FastBFS can outperform X-stream and GraphChi in the computing speed by up to 2.1 and 3.9 times faster respectively. With an additional disk, FastBFS can even outperform them by up to 3.6 and 6.5 times faster respectively."
pub.1103338087,Big Data Mining Algorithms for Fog Computing,"Fog computing is a contemporary distributed computing concept extending from Cloud computing, which pushes the data analytics to the edge of a sensor network as far as possible. It helps avoid performance bottleneck and data analytics latency at the central server of a Cloud. However, when Fog computing is deployed, the edge nodes are responsible in data analysis including learning and recognizing patterns from the incoming data streams. Hence it is crucial to find appropriate data mining algorithm(s) which is lightweight in operation and accurate in predictive performance. In this paper, the suitability of data mining and data stream mining algorithms are investigated in Fog computing environment. Specifically, non-black-box machine learning models such as decision trees are looked into, with a quick pre-processing function implemented by correlation-based feature selection algorithm coupled with traditional search methods and particle swarm optimization search method. The simulation is based on an IoT environment where emergency services are to be supported. The results of this paper sheds light into what/which algorithms should be designed and chosen for delivering edge intelligence under Fog computing environment."
pub.1061316857,Quanta data storage: an information processing and transportation architecture for storage area networks,"A new architecture for storage area networks (SANs) is proposed for providing efficient information processing and transportation. We deviate from the byte stream oriented transmission control protocol (TCP) transport mechanism to more storage friendly block oriented transport. Data is processed, encrypted, error checked, redundantly encoded, and stored in fixed size blocks called quanta. Each quantum is processed by an effective cross-layer protocol that collapses the protocol stack for security, iWARP and iSCSI functions, transport control, and even redundant arrays of inexpensive disk (RAID) storage. This streamlining produces a highly efficient protocol with fewer memory copies and places most of the computational burden and security safeguard on the client, while the target stores quanta from many clients with minimal processing. We propose a new network RAID storage method using the quantum concept. Also, we unify error control and flow control of the iSCSI and TCP protocols in a manner we believe is more suitable for high data rate and low latency storage applications."
pub.1141332010,An Intelligent Parallel Distributed Streaming Framework for near Real-time Science Sensors and High-Resolution Medical Images,"Our goals are to address challenges such as latency, scalability, throughput and heterogeneous data sources of streaming analytics and deep learning pipelines in science sensors and medical imaging applications. We present a prototype Intelligent Parallel Distributed Streaming Framework (IPDSF) that is capable of distributed streaming processing as well as performing distributed deep training in batch mode. IPDSF is designed to run streaming Artificial Intelligent (AI) analytic tasks using data parallelism including partitions of multiple streams of short time sensing data and high-resolution 3D medical images, and fine grain tasks distribution. We will show the implementation of IPDSF for two real world applications, (i) an Air Quality Index based on near real time streaming of aerosol Lidar backscatter and (ii) data generation of Covid-19 Computing Tomography (CT) scans using deep learning. We evaluate the latency, throughput, scalability, and quantitative evaluation of training and prediction compared against a baseline single instance. As the results, IPDSF scales to process thousands of streaming science sensors in parallel for Air Quality Index application. IPDSF uses novel 3D conditional Generative Adversarial Network (cGAN) training using parallel distributed Graphic Processing Units (GPU) nodes to generate realistic 3D high resolution Computed Tomography scans of Covid-19 patient lungs. We will show that IPDSF can reduce cGAN training time linearly with the number of GPUs."
pub.1092155450,Efficient Data Streaming Multiway Aggregation through Concurrent Algorithmic Designs and New Abstract Data Types,"Data streaming relies on continuous queries to process unbounded streams of data in a real-time fashion. It is commonly demanding in computation capacity, given that the relevant applications involve very large volumes of data. Data structures act as articulation points and maintain the state of data streaming operators, potentially supporting high parallelism and balancing the work among them. Prompted by this fact, in this work we study and analyze parallelization needs of these articulation points, focusing on the problem of streaming multiway aggregation, where large data volumes are received from multiple input streams. The analysis of the parallelization needs, as well as of the use and limitations of existing aggregate designs and their data structures, leads us to identify needs for appropriate shared objects that can achieve low-latency and high-throughput multiway aggregation. We present the requirements of such objects as abstract data types and we provide efficient lock-free linearizable algorithmic implementations of them, along with new multiway aggregate algorithmic designs that leverage them, supporting both deterministic order-sensitive and order-insensitive aggregate functions. Furthermore, we point out future directions that open through these contributions. The article includes an extensive experimental study, based on a variety of continuous aggregation queries on two large datasets extracted from SoundCloud, a music social network, and from a Smart Grid network. In all the experiments, the proposed data structures and the enhanced aggregate operators improved the processing performance significantly, up to one order of magnitude, in terms of both throughput and latency, over the commonly used techniques based on queues."
pub.1061408476,An Extreme Processor for an Extreme Experiment,"Particle physics experiments stretch processing requirements to the limit, requiring the selection of extremely rare events at tens of terabytes per second. A network of 283,392 mixed-signal MIMD processors operating in parallel at 17 tbytes/s help physicists interpret data from the world's largest particle accelerator. Particle physics and heavy-ion experiments demand greater integration and fast on-detector signal processing. We believe TRD is the first system to implement complete signal digitizing, filtering, intelligent trigger processing and readout in a single on-detector chip that avoids system noise. TRD uses multiport memories as register file inputs, multiported GRFs, and a global multiport data memory. These components support high-end multiprocessing requirements under tight latency conditions. Multi-ported memories, in particular, make it possible to couple independent data streams' very efficiently. So far, the requirements for this detector have remained largely stable. The main change was to add digital filters relatively late in the design. Of course, the first weeks of measuring actual collisions at unprecedented LHC energies are the true test of our design"
pub.1094335290,Resource Provisioning for Staging Components,"To deal with the inordinate output data volumes of current and future high end simulations, researchers are turning to online methods in which multiple software components that implement desired data analytics and visualization are run on 'staging resources' of the petascale machine, concurrently and coupled with the simulations producing these outputs. Efficient online execution of data analytics 'in the output stream', however, requires careful provisioning of staging resources, to obtain delays for analytics processing that prevent applications from blocking on stalled output, while also bounding total required staging resources. This paper addresses the 'staging provisioning' problem, assuming sets of components arranged as potentially multiple analytics/output pipelines that differ in runtime behavior and resource requirements. For such configurations, it then meets the throughput constraint of online analytics while also minimizing end-to-end pipeline latency, all based on runtime observations and predictions of component performance. Experimental evaluations demonstrate the algorithm's utility. Its complexity for minimizing latency without violating throughput constraints is O(M), where M is the number of components in the staging area."
pub.1154881193,Tighter NIC/GPU Integration Yields Next-Level Media Processing Performance,"As the media industry further consolidates building services based on commercial-off-the-shelf (COTS) hardware, the requirement for increasing performance continues to accelerate. The processing of video resolutions up to ultra-high definition (HD) (UHD) and 8K, higher frame rates, increased bit depth, and high-dynamic-range (HDR) imagery requires tighter integration and optimization between the network interface controller (NIC) and the graphical processing unit (GPU). Combined with simultaneous input and output of multiple streams; this creates the potential for performance bottlenecks that must be unlocked. This article describes how COTS hardware platforms running Graphics Processing Unit (GPU)s alongside ST 2059–2 Precision Time Protocol (PTP) locked Network Interface Cards (NIC)s, which are accurately pacing packets according to ST 2110–21 requirements, may further increase their performance throughput by reducing central processing unit (CPU)-driven data copy overhead and, in doing so, they reduce processing latency and jitter as well as more effectively use GPU resources while freeing up CPU resources."
pub.1125156800,Streaming Machine Learning Algorithms with Big Data Systems,"Designing low latency applications that can process large volumes data with higher efficiency is a challenging problem. With the limited time to process data, usage of online algorithms are becoming important in the big-data applications. Stream processing is a well-known area that has been studied for a long time. In this research, our objective is to use state of the art big-data analytic engines to implement online algorithms and compare the strengths and weaknesses in each system. We use a streaming version of Support Vector Machines (SVM) and KMeans to do the analysis. Apache Flink, Apache Storm and Twister2 streaming frameworks are used to implement these algorithms. Our study focuses on the efficiency of online training of these algorithms and the results show higher performance in Twister2 framework for these algorithms."
pub.1176141012,Efficient Learning of Event-Based Dense Representation Using Hierarchical Memories with Adaptive Update,"Leveraging the high temporal resolution of an event-based camera requires highly efficient event-by-event processing. However, dense prediction tasks require explicit pixel-level association, which is challenging for event-based processing frameworks. Existing works aggregate the events into a static frame-like representation at the cost of a much slower processing rate and high compute cost. To address this challenge, this work introduces an event-based spatiotemporal representation learning framework for efficiently solving dense prediction tasks. We uniquely handle the sparse, asynchronous events using an unstructured, set-based approach and project them into a hierarchically organized multi-level latent memory space that preserves the pixel-level structure. Low-level event streams are dynamically encoded into these latent structures through an explicit attention-based spatial association. Unlike existing works that update these memory stacks at a fixed rate, we introduce a data-adaptive update rate that recurrently keeps track of the past memory states and learns to update the corresponding memory stacks only when it has substantial new information, thereby improving the overall compute latency. Our method consistently achieves competitive performance across different event-based dense prediction tasks while ensuring much lower latency compared to the existing methods."
pub.1111599158,Digital Acquisition Chain for the Upgrade of the CERN SPS Beam Position Monitor,"This paper presents the development of the digital acquisition chain for the upgrade of the Beam Position Monitor (BPM) system for the Super Proton Synchrotron (SPS) at CERN. The front-end electronics will be based on logarithmic amplifiers and, including the digitization, it will be located inside the accelerator tunnel, with optical latency deterministic transmission to surface back-end electronics, where the data stream is processed in a Field Programmable Gate Array. The paper focuses on the processing techniques for the position estimation out of beam data conditioned by logarithmic amplifiers. The elements of the acquisition chain have been tested in laboratory and with beam signals."
pub.1154428675,Fast-Pipe: Efficient Multi-Channel Data Mapping on Edge FPGAs for Satellite Remote Sensing,"Convolutional neural network (CNN) accelerator has been gradually deployed on edge Field-Programmable Gate Arrays (FPGAs) for satellite remote sensing. However, the considerable complexity of software/hardware co-design inhibits the development of data processing applications on edge FPGAs. Moreover, the performance of edge FPGAs is restricted due to limited bandwidth and excessive software/hardware communication overhead in satellite remote sensing. To reduce co-design complexity, we propose a software/hardware data mapping framework for the deployment efficiency of the remote sensing data accelerator, called Fast-Pipe. Especially the software design of Fast-Pipe takes into account the interaction requirements of accelerator deployment and the poll and copy latency of data transfers. The data transfer driver is implemented in the user space to reduce latency with a new scheduling method and an interrupt policy for sending and receiving data. Meanwhile, an associated hardware structure is designed with Direct Memory Access (DMA) to implement data scheduling and mapping. With Fast-Pipe, the data in the software buffer is mapped into multiplexed data streams. Experimental results show that Fast-Pipe speed up 46.38× in small data transfer, and average 2× higher in large data transfer than previous work with stable data transfer speed."
pub.1126788167,Runtime verification of real-time event streams under non-synchronized arrival,"We study the problem of online runtime verification of real-time event streams. Our monitors can observe concurrent systems with a shared clock, but where each component reports observations as signals that arrive to the monitor at different speeds and with different and varying latencies. We start from specifications in a fragment of the TeSSLa specification language, where streams (including inputs and final verdicts) are not restricted to be Booleans but can be data from richer domains, including integers and reals with arithmetic operations and aggregations. Specifications can be used both for checking logical properties and for computing statistics and general numeric temporal metrics (and properties on these richer metrics). We present an online evaluation algorithm for the specification language and a concurrent implementation of the evaluation algorithm. The algorithm can tolerate and exploit the asynchronous arrival of events without synchronizing the inputs. Then, we introduce a theory of asynchronous transducers and show a formal proof of the correctness such that every possible run of the monitor implements the semantics. Finally, we report an empirical evaluation of a highly concurrent Erlang implementation of the monitoring algorithm."
pub.1153733558,A cost-effective real-time distributed computing platform with apache spark structured streaming for DAS,"Fiber-optical distributed acoustic sensor (DAS) based on the phase-sensitive optical time-domain reflectometry (Φ- OTDR), provides a highly dense and cost-effective continuous sensing network for our environment safefy monitoring in a wide range by using the existed or buried communication cable as its sensing media. However, the continuous huge spatial-temporal data stream generated by long-distance Fiber-optic Distributed Acoustic Sensor (DAS) poses a great challenge to the single central processing unit (CPU) or graphical processing unit (GPU) machine with traditional centralized processing style. Thus, a distributed computing platform is built in this paper based on Apache Spark Structured Streaming, which provides a cost-effective solution with relatively high throughput and high real time capability. The throughput is improved by 75% and latency reduces by 19.71% in virtual-machine test, while the throughput is improved by 44% and the latency reduces by 37.53% on average in an optimal four-node cluster compared to a single node in real-machine test. It provides a promising real-time distributed computing scheme for DAS especially with large or super large sensing arrays in a large range or a highly dense time-space acquisition network."
pub.1173755664,Hybrid neural network for event-based object tracking,"Event stream has been used in various vision tasks due to the low latency and high dynamic range of event camera. However, because of the temporal dynamic of event stream, convolution neural networks (CNNs) are difficult to effectively extract features from event streams to achieve object tracking tasks. Besides, SNNs is suitable for processing data with temporal information because of its spiking delivery mechanism and membrane potential accumulation over time. In this work, we propose a Hybrid Neural Network (HNNet) to achieve effective event-based single object tracking tasks by combining the advantages of SNNs and Swin-Transformer. For higher feature expression ability of SNNs, we adopt the Swin-Transformer to extract features from sparse event stream. Then we use these features to modulate the threshold of SNNs neurons. What’s more, for improving tracking performance for both special and temporal features, a cross-modality fusion module is designed to fuse the two features extracted by the Swin-Transformer and SNNs. We conduct extensive experiments on three public event-based datasets (FE240, FE108, and VisEvent) and our tracker outperforms other trackers maximum at 1.1% and 6.8% in terms of area under curve (AUC) scores and precision rate respectively."
pub.1117619132,Performance Factor Analysis and Scope of Optimization for Big Data Processing on Cluster,"Use of computational cluster for large-scale Big Data processing has attracted attention as a technology trend for its time efficiency. Modern cluster equipped with latest multi, many-core distributed shared architecture, high speed interconnect and file system, ensures high performance using message passing and multi-threading parallel approaches, also handles batch, micro-batch and stream processing of high dimensional massive dataset but running data-intensive Big Data application on compute-centric cluster imposes challenges to its performance because of several runtime overheads. In order to alleviate these bottlenecks and exploit full potential of the cluster a state of the practice, performance-oriented technical analysis covering all relevant aspects is presented in the context of Terascale Big data processing on TeraFLOPS cluster PARAM-Kanchenjunga, with identification of major factors influencing the performance or sources of these overheads related to computation, communication or IPC, memory, I/O contention, scheduling, load imbalance, synchronization, latency and network jitter; by determining their impact. As existing approaches found insufficient, to achieve possible speedup advance methods with a variety of alternatives as RDMA enabled libraries, PFS, MPI-Integrated extensions, loop tiling, hybrid parallelization are provided to consider for optimization purposes. This paper will assist to prepare performance aware design of experiments and performance modeling."
pub.1094080599,A Stream Processing Framework for On-line Optimization of Performance and Energy Efficiency on Heterogeneous Systems,"Modern processors have the potential of executing compute-intensive programs quickly and efficiently, but require applications to be adapted to their ever increasing parallelism. Here, heterogeneous systems add complexity by combining processing units with different characteristics. Scheduling should thus consider the performance of each processor as well as competing workloads and varying inputs. To assist programmers of stream processing applications in facing this challenge we present libHawaii, an open source library for cooperatively using all processors of heterogeneous systems easily and efficiently. It supports exploiting data flow, data element and task parallelism via pipelining, partitioning and demand-based allocation of consecutive work items. Scheduling is automatically adapted on-line to continuously optimize performance and energy efficiency. Our C++ library does not depend on specific hardware architectures or parallel computing frameworks. However, it facilitates maximizing the throughput of compatible GPUs by overlapping computations and memory transfers while maintaining low latencies. This paper describes the algorithms and implementation of libHawaii and demonstrates its usage on existing applications. We experimentally evaluate our library using two examples: General matrix multiplication (GEMM) is a simple yet important building block of many high-performance computing applications. Complementarily, the detection, extraction and matching of sparse image features exhibits greater complexity, including indeterministic memory access and synchronization."
pub.1120995931,GeneaLog: Fine-grained data streaming provenance in cyber-physical systems,"Streaming applications continuously process data to deliver streams of up-to-date results. Their growing adoption for data analysis in many distributed systems is motivated by their performance (in terms of processing throughput and latency) and their support for easy-to-program distributed and parallel analysis. When streaming applications are designed to detect unusual or critical events (e.g., security- or safety-related), it can be beneficial to maintain the associated source data for further analysis. This can be achieved by fine-grained data provenance, which links each detected event back to the source data that contributed to it, allowing to distinguish and isolate the source data that generated such unusual or critical events. Fine-grained data provenance can be especially useful in cyber-physical systems, such as vehicular networks and smart grids. By enabling the extraction of valuable information from raw sensor data, it could, for instance, reduce data transmission and storage requirements. Since cyber-physical systems can have heterogeneous multi-core architectures, ranging from inexpensive single-board computers to high-end servers, there is a demand for efficient provenance techniques that can take advantage of such parallel architectures with minimal overhead. Motivated by this challenge, we present GeneaLog, a novel fine-grained data provenance technique for data streaming applications. Leveraging the logical dependencies of the data, GeneaLog takes advantage of cross-layer properties of the software stack and incurs a minimal, constant size per-tuple overhead. Furthermore, it allows for a modular and efficient algorithmic implementation using only standard (instrumented) data streaming operators. This is particularly useful to distribute the provenance overheads to operators that can be run in parallel, thus leveraging multi-core architectures. We evaluate two implementations of GeneaLog, one based on Apache Flink, a widely-adopted state-of-the-art Stream Processing Engine, and one based on Liebre, an edge-tailored lightweight Stream Processing Engine. We test them both on vehicular and smart grid applications with single-board embedded devices and a high-end server, also studying how GeneaLog affects their scalability and confirming that it efficiently captures fine-grained provenance data with minimal overhead."
pub.1096143185,Locality aware management on NAND flash-based main memory for in-memory database systems,"Conventional database systems manage all data on hard disks, but due to a hard disk's frequent I/O operations, this kind of management exposes critical problems when data is huge or operations are complex and frequent. As the size of the main memory continues to increase, main memory architecture and management becomes the major research trend in big data processing. Thus, we propose an optimized NAND Flash-based main memory (NFMM) structure for in-memory database systems to achieve the goal of having DRAM like performance at the lower cost and power consumption of Flash memory. For this goal, a horizontal combination of DRAM and NAND Flash memory is designed as a main memory model for database applications. A stream buffer and a DRAM buffer are designed to compensate for the slow access latency of Flash memory. Its optimized management method is designed to enhance the accessing locality and manage the stream buffer by prefetching pages. To evaluate the performance, Redis and Yahoo! Cloud Service Benchmark (YCSB) are used. In our experiment, a stream buffer is used to improve the data transfer speed. The result shows that in the proposed system, the execution time can achieve only 1.16x to 1.21x slower on average. At the same time, optimized NAND Flash-based main memory with 40 entries of stream buffer reduces power consumption up to 25% compared to the DRAM-based main memory system."
pub.1167878130,Time Synchronization Uncertainty Estimation Methodology for data-centric Production Scenarios,"In modern data-centric production, multiple sensors, machines, equipment, and systems are connected providing a high variety and amount of data to enable insights into ongoing production. By adding more of those data sources to production, the Time Synchronization Problem becomes more intense when aggregating the data streams for analysis. To tackle the Time Synchronization Problem and perform the actual synchronization of the data, an estimation of the potential uncertainties is required. In this paper, we present a methodology to identify and describe the chain of time series uncertainties from data sources to aggregation. This calculation takes into account the uncertainties from sensory and signal processing requirements (like signal processing cycle times) to network communication latencies and jitter in the production network. In the end, each data point gets augmented with information regarding the uncertainty of its time information in terms of the minimum and maximum added processing times in the data pipeline. Using this information, the data aggregation can be adapted accordingly, and the final analysis tailored down to the actual result uncertainties. To show this method and its applicability, a use-case in battery cell manufacturing is presented including a description of how this method can be applied and the data pipeline adapted."
pub.1138307399,Keynote: Orchestrating Vehicular Computing Resources to Form Floating Virtual Edge Servers,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Emerging services for connected intelligent vehicles necessitate timely processing of vehicle-generated data. Realtime fusion of the data streams from vehicles across a city unlocks the potential of new applications that improve safety and comfort of driving. Although cloud computing offers flexible and scalable data processing services and thereby serves as a key enabler of these applications, a major challenge lies in communication overhead and latency to transfer data between vehicles and the remote servers. The concept of vehicle cloudification could be a promising solution to bridge the gap. The main idea is to form groups of connected vehicles over cellular and/or vehicle-to-vehicle (V2V) communications and use them as floating virtual edge servers (a.k.a. vehicular micro clouds). The vehicles in a vehicular micro cloud collaboratively perform data processing, communication and sensing tasks to complement the physical cloud and edge computing infrastructures. This keynote talk discusses the feasibility, fundamental building blocks and potential use cases of vehicular micro clouds."
pub.1095375773,"VLSI register, instruction and data caches suited to on chip CPU multi-threading support for real-time multi-media applications","The architecture of a CPU capable of both executing multithreaded user processes in a commercial multi-threaded operating system environment and rapidly (sub microsecond) responding to real-time events, such as those that arise in the processing and synchronization of audio-video-data streams constituting concurrent interactive multi-media sessions, is discussed in this paper. The rapid, often simple, responses of a CPU to prioritized requests requires careful design of the on chip caches and registers and management of the hazards causing latencies in the CPU pipeline. The novel incorporation of a register cache in the CPU, its design, and the design of the instruction and data caches is described."
pub.1095511032,VIIRS Ocean Color Products: A Progress Update,"In this paper, we provide overview of the progress on the evaluations of the Visible Infrared Imaging Radiometer Suite (VIIRS) ocean color products with long-term time series and more in situ data for matchup analysis. Specifically, VIIRS ocean color products include normalized water-leaving radiance spectra $nL_{w}(\lambda)$ at VIIRS five spectral bands, chlorophyll-a concentration (Chl-a), water diffuse attenuation coefficients at the wavelength of 490 nm, $K_{d}(490)$, and at the domain of photosynthetically available radiation (PAR), $K_{d}(\text{PAR})$. VIIRS ocean color products derived from the NOAA Multi-Sensor Level-1 to Level-2 (MSL12) ocean color data processing system are evaluated and compared routinely with in situ data from the Marine Optical Buoy (MOBY) and several AERONET-OC sites. In order to meet requirements from all users, we propose to routinely produce two ocean color data streams, i.e., the near-real-time (NRT) data and delayed science quality data. The NRT ocean color data stream has the advantage of quick data turn around with data latency ∼12–24 hours, while the science quality data stream has high consistence (with the mission-long data reprocessing) and accuracy of ocean color products. In addition, we have significantly improved on-orbit sensor calibration by combining the lunar calibration into the current solar calibration method. Our results show that VIIRS is now providing high-quality global ocean color products in support of the scientific research and operational applications."
pub.1145130979,QSpark: Distributed Execution of Batch & Streaming Analytics in Spark Platform,"A significant portion of research work in the past decade has been devoted on developing resource allocation and task scheduling solutions for large-scale data processing platforms. Such algorithms are designed to facilitate deployment of data analytic applications across either conventional cluster computing systems or modern virtualized data-centers. The main reason for such a huge research effort stems from the fact that even a slight improvement in the performance of such platforms can bring a considerable monetary savings for vendors, especially for modern data processing engines that are designed solely to perform high throughput or/and low-latency computations over massive-scale batch or streaming data. A challenging question to be yet answered in such a context is to design an effective resource allocation solution that can prevent low resource utilization while meeting the enforced performance level (such as 99-th latency percentile) in circumstances where contention among applications to obtain the capacity of shared resources is a non negligible performance-limiting parameter. This paper proposes a resource controller system, called QSpark, to cope with the problem of (i) low performance (i.e., resource utilization in the batch mode and p-99 response time in the streaming mode), and (ii) the shared resource interference among collocated applications in a multi-tenancy modern Spark platform. The proposed solution leverages a set of controlling mechanisms for dynamic partitioning of the allocation of computing resources, in a way that it can fulfill the QoS re-quirements of latency-critical data processing applications, while enhancing the throughput for all working nodes without reaching their saturation points. Through extensive experiments in our in-house Spark cluster, we compared the achieved performance of proposed solution against the default Spark resource allocation policy for a variety of Machine Learning (ML), Artificial Intelligence (AI), and Deep Learning (DL) applications. Experimental results show the effectiveness of the proposed solution by reducing the p-99 latency of high priority applications by 32 % during the burst traffic periods (for both batch and stream modes), while it can enhance the QoS satisfaction level by 65 % for applications with the highest priority (compared with the results of default Spark resource allocation strategy)."
pub.1092862931,Fog-Enabled Architecture for Data-Driven Cyber-Manufacturing Systems,"Over the past few decades, both small- and medium-sized manufacturers as well as large original equipment manufacturers (OEMs) have been faced with an increasing need for low cost and scalable intelligent manufacturing machines. Capabilities are needed for collecting and processing large volumes of real-time data generated from manufacturing machines and processes as well as for diagnosing the root cause of identified defects, predicting their progression, and forecasting maintenance actions proactively to minimize unexpected machine down times. Although cloud computing enables ubiquitous and instant remote access to scalable information and communication technology (ICT) infrastructures and high volume data storage, it has limitations in latency-sensitive applications such as high performance computing and real-time stream analytics. The emergence of fog computing, Internet of Things (IoT), and cyber-physical systems (CPS) represent radical changes in the way sensing systems, along with ICT infrastructures, collect and analyze large volumes of real-time data streams in geographically distributed environments. Ultimately, such technological approaches enable machines to function as an agent that is capable of intelligent behaviors such as automatic fault and failure detection, self-diagnosis, and preventative maintenance scheduling. The objective of this research is to introduce a fog-enabled architecture that consists of smart sensor networks, communication protocols, parallel machine learning software, and private and public clouds. The fog-enabled architecture will have the potential to enable large-scale, geographically distributed online machine and process monitoring, diagnosis, and prognosis that require low latency and high bandwidth in the context of data-driven cyber-manufacturing systems.Copyright © 2016 by ASME"
pub.1106862601,Moira,"The need for real-time analysis is still spreading and the number of available streaming sources is increasing. The recent literature has plenty of works on Data Stream Processing (DSP). In a streaming environment, the data incoming rate varies over time. The challenge is how to efficiently deploy these applications in a cluster. Several works have been conducted on improving the latency of the system or to minimize the allocated resources per application through time. However, to the best of our knowledge, none of the existing works takes into consideration the user needs for a specific application, which is different from one user to another. In this paper, we propose Moria, a goal-oriented framework for dynamically optimizing the resource allocation built on top of Apache Flink. The system takes actions based on the user application and on the incoming data characteristics (i.e., input rate and window size). Starting from an initial estimation of the resources needed for the user query, at each iteration we improve our cost function with the collected metrics from the monitored system about the incoming data, to fulfill the user needs. We present a series of experiments that show in which cases our dynamic estimation outperforms the baseline Apache Flink and the thumb rule estimation alone performed at the deployment of the applications."
pub.1139431920,LHC physics dataset for unsupervised New Physics detection at 40 MHz,"In particle detectors at the Large Hadron Collider, tens of terabytes of data
are produced every second from proton-proton collisions occurring at a rate of
40 megahertz. This data rate is reduced to a sustainable level by a real-time
event filter processing system which decides whether each collision event
should be kept for further analysis or be discarded. We introduce a dataset of
proton collision events which emulates a typical data stream collected by such
a real-time processing system, pre-filtered by requiring the presence of at
least one electron or muon. This dataset could be used to develop novel event
selection strategies and assess their sensitivity to new phenomena. In
particular, by publishing this dataset we intend to stimulate a community-based
effort towards the design of novel algorithms for performing unsupervised New
Physics detection, customized to fit the bandwidth, latency and computational
resource constraints of the real-time event selection system of a typical
particle detector."
pub.1174211639,Recent Advances in End-to-End Simultaneous Speech Translation,"Simultaneous speech translation (SimulST) is a demanding task that involves generating translations in real-time while continuously processing speech input. This paper offers a comprehensive overview of the recent developments in SimulST research, focusing on four major challenges. Firstly, the complexities associated with processing lengthy and continuous speech streams pose significant hurdles. Secondly, satisfying real-time requirements presents inherent difficulties due to the need for immediate translation output. Thirdly, striking a balance between translation quality and latency constraints remains a critical challenge. Finally, the scarcity of annotated data adds another layer of complexity to the task. Through our exploration of these challenges and the proposed solutions, we aim to provide valuable insights into the current landscape of SimulST research and suggest promising directions for future exploration."
pub.1004679161,"Load Shedding in MavStream: Analysis, Implementation, and Evaluation","In data stream management systems (DSMSs), Quality of Service (or QoS) requirements, as specified by users, are extremely important. To satisfy QoS requirements throughout the life of a data stream, result characteristics need to be monitored at runtime and adjustments made continuously. It has been shown that in a DSMS, switching scheduling strategies at runtime can change tuple latency requirements. DSMSs also experience significant fluctuations in input rates (termed bursty inputs). In order to meet the QoS requirements in the presence of bursty inputs, a load shedding strategy is critical. This also entails monitoring of QoS measures at run-time to meet expected QoS requirements.This paper addresses load shedding issues for MavStream, a DSMS being developed at UT Arlington. To cope with situations where the arrival rates of input streams exceed the processing capacity of the system, we have incorporated load shedders into the query processing model. The runtime optimizer continually monitors the output and decides when to turn on the shedders and how much to shed. Choice of shedders is done to minimize the error in the output. Shedders have been incorporated as part of the buffers to minimize the overhead for load shedding. Finally, load shedders are activated and deactivated dynamically by the runtime optimizer. Both random and semantic load shedding techniques are supported to match application semantics."
pub.1154007690,Joint Task Segmentation and Server Selection for Real-time Stream Data Analytics in Edge-cloud Environments,"To reduce the task completion latency and wide area network traffic for real-time stream data analytics applications, edge computing, which pushes computing resources from the remote cloud to the network edge, has drawn both researchers’ and engineers’ attention. Since edge servers are typically resource-limited while the tasks are typically computing-consuming, a practical deployment method is to split the task into two sequential parts and to execute them on the edge and the cloud separately. Combing it with the servers’ heterogeneity in terms of computation, memory, etc., we are faced with the joint task segmentation and server selection problem. To this end, this paper formulates it with the goal of minimizing the long-term energy cost while meeting the real-time data processing constraint. To solve the problem, we design an online randomized iterative algorithm, which is based on the Lyapunov optimization framework and the Markov approximation method. Finally, trace-driven simulations are conducted to evaluate the performance of the proposed algorithm and the result shows its superiority over the heuristic alternatives."
pub.1140076494,Architecture for Orchestrating Dynamic DNN-Powered Image Processing Tasks in Edge and Cloud Devices,"DNN processing on image streams has opened the possibility for new and innovative applications. Some of those would benefit from performing the computation locally, avoiding incurring into latencies due to data travelling to image processing services in the cloud, and thus allowing for faster response times. New devices like the GPU-accelerated NVIDIA Jetson family, such as the Jetson Nano, are capable of running modern DNN image processing models, offering an affordable, powerful and scalable local alternative to cloud processing. Performing local image processing can also benefit security and even GDPR compliance, potentially easing the deployment of this solutions. Not only that, but local image processing can also bring the possibility of applying these techniques in areas with reduced connectivity, where cloud-based solutions are unfeasible. In this work, we propose an architecture for the orchestration of DNN accelerated image processing on IoT devices, based on FogFlow; an orchestration platform capable of leveraging cloud and edge resources. FogFlow is part of the FIWARE initiative and is based in the NGSI family of standards, widely applied in Smart City, Smart Building and Smart Home solutions, making it an easy-to-integrate technology."
pub.1165839940,Distributed Data Locality-Aware Job Allocation,"Scheduling tasks close to their associated data is crucial in distributed systems to minimize network traffic and latency. Some Big Data frameworks like Apache Spark employ locality functions and job allocation algorithms to minimize network traffic and execution times. However, these frameworks rely on centralized mechanisms, where the master node determines data locality by allocating tasks to available workers with minimal data transfer time, ignoring variances in worker configurations and availability. To address these limitations, we propose a decentralized approach to locality-driven scheduling that grants workers autonomy in the job allocation process while factoring in workers’ configurations, such as network and CPU speed differences. Our approach is developed and evaluated on Crossflow, a distributed stream processing platform with data-aware independent worker nodes. Preliminary evaluation experiments indicate that our approach can yield up to 3.57x faster execution times when compared to the baseline centralized approach where the master controls data locality."
pub.1174243546,Rapid Likelihood Free Inference of Compact Binary Coalescences using Accelerated Hardware,"We report a gravitational-wave parameter estimation algorithm, AMPLFI, based
on likelihood-free inference using normalizing flows. The focus of AMPLFI is to
perform real-time parameter estimation for candidates detected by
machine-learning based compact binary coalescence search, Aframe. We present
details of our algorithm and optimizations done related to data-loading and
pre-processing on accelerated hardware. We train our model using binary
black-hole (BBH) simulations on real LIGO-Virgo detector noise. Our model has
$\sim 6$ million trainable parameters with training times $\lesssim 24$ hours.
Based on online deployment on a mock data stream of LIGO-Virgo data, Aframe +
AMPLFI is able to pick up BBH candidates and infer parameters for real-time
alerts from data acquisition with a net latency of $\sim 6$s."
pub.1061515313,Large-Scale Estimation in Cyberphysical Systems Using Streaming Data: A Case Study With Arterial Traffic Estimation,"Controlling and analyzing cyberphysical and robotics systems is increasingly becoming a Big Data challenge. We study the case of predicting drivers' travel times in a large urban area from sparse GPS traces. We present a framework that can accommodate a wide variety of traffic distributions and spread all the computations on a cluster to achieve small latencies. Our framework is built on Discretized Streams, a recently proposed approach to stream processing at scale. We demonstrate the usefulness of Discretized Streams with a novel algorithm to estimate vehicular traffic in urban networks. Our online EM algorithm can estimate traffic on a very large city network (the San Francisco Bay Area) by processing tens of thousands of observations per second, with a latency of a few seconds. Note to Practitioners—This work was driven by the need to estimate vehicular traffic at a large scale, in an online setting, using commodity hardware. Machine Learning algorithms combined with streaming data are not new, but it still requires deep expertise both in Machine Learning and in Computer Systems to achieve large scale computations in a tractable manner. The Streaming Spark project aims at providing an interface that abstracts out all the technical details of the computation platform (cloud, HPC, workstation, etc.). As shown in this work, Streaming Spark is suitable for implementing and calibrating nontrivial algorithms on a large cluster, and provides an intuitive yet powerful programming interface. The readers are invited to refer to the source code referred in this article for more examples. This paper presents algorithms to sample and compute densities for Gamma random variables restricted to a hyperplane (i.e., distributions of the form $T_{i}\vert\sum_{j}\alpha_{j}T_{j}=d$ with $T_{j}$ independant Gamma distributions). It is common in this case to use Gaussian random variables because of closed-form solutions to solve. If one considers positive valued distributions with heavy tails, our formulas using gamma distributions may be more suitable."
pub.1142934060,Service Provisioning for Multi-source IoT Applications in Mobile Edge Computing,"We are embracing an era of Internet of Things (IoT). The latency brought by unstable wireless networks caused by limited resources of IoT devices seriously impacts the quality of services of users, particularly the service delay they experienced. Mobile Edge Computing (MEC) technology provides promising solutions to delay-sensitive IoT applications, where cloudlets (edge servers) are co-located with wireless access points in the proximity of IoT devices. The service response latency for IoT applications can be significantly shortened due to that their data processing can be performed in a local MEC network. Meanwhile, most IoT applications usually impose Service Function Chain (SFC) enforcement on their data transmission, where each data packet from its source gateway of an IoT device to the destination (a cloudlet) of the IoT application must pass through each Virtual Network Function (VNF) in the SFC in an MEC network. However, little attention has been paid on such a service provisioning of multi-source IoT applications in an MEC network with SFC enforcement.
                  In this article, we study service provisioning in an MEC network for multi-source IoT applications with SFC requirements and aiming at minimizing the cost of such service provisioning, where each IoT application has multiple data streams from different sources to be uploaded to a location (cloudlet) in the MEC network for aggregation, processing, and storage purposes. To this end, we first formulate two novel optimization problems: the cost minimization problem of service provisioning for a single multi-source IoT application, and the service provisioning problem for a set of multi-source IoT applications, respectively, and show that both problems are NP-hard. Second, we propose a service provisioning framework in the MEC network for multi-source IoT applications that consists of uploading stream data from multiple sources of the IoT application to the MEC network, data stream aggregation and routing through the VNF instance placement and sharing, and workload balancing among cloudlets. Third, we devise an efficient algorithm for the cost minimization problem built upon the proposed service provisioning framework, and further extend the solution for the service provisioning problem of a set of multi-source IoT applications. We finally evaluate the performance of the proposed algorithms through experimental simulations. Simulation results demonstrate that the proposed algorithms are promising."
pub.1148164386,Performance Evaluation Analysis of Spark Streaming Backpressure for Data-Intensive Pipelines,"In the past decades, a significant rise in the adoption of streaming applications has changed the decision-making process for the industry and academia sectors. This movement led to the emergence of a plurality of Big Data technologies such as Apache Storm, Spark, Heron, Samza, Flink, and other systems to provide in-memory processing for real-time Big Data analysis at high throughput. Spark Streaming represents one of the most popular open-source implementations which handles an ever-increasing data ingestion and processing by using the Unified Memory Manager to manage memory occupancy between storage and processing regions dynamically, which is the focus of this study. The problem behind memory management for data-intensive stream processing pipelines is that the incoming data is faster than the downstream operators can consume. Consequently, the backpressure of Spark acts in the opposite direction of downstream operators. In such a case, the incoming data overwhelms the memory manager and provokes memory leak issues. As a result, it affects the performance of applications generating, e.g., high latency, low throughput, or even data loss. In such a case, the initial intuition motivating our work is that memory management became the critical factor in keeping processing at scale and system stability of Spark. This work provides a deep dive into Spark backpressure, evaluates its structure, presents the main characteristics to support data-intensive streaming pipelines, and investigates the current in-memory-based performance issues."
pub.1113462688,0142 The Cumulative Effect of Partial Chronic Sleep Restriction on the Neural Processing Stream in Neurologically Normal Individuals,"Event-related potentials (ERP) are averaged electroencephalogram (EEG) responses to stimuli enabling precise temporal characterization of neural processing. We analyzed the temporal evolution and course of cognitive processing impairments during a 16 day randomized, cross-over sleep restriction experiment, by analyzing visual attention (N2pc), stimulus classification (P3), and decision making (error-related negativity (ERN) and positivity (Pe)) ERPs. Eighteen healthy subjects (8 women, 10 men; age 22.8±4.88, range 18-36 years underwent sleep restriction (3 acclimation nights of 9 hours of time in bed, then 9 experimental 4 hours nights, then 3 recovery sleep nights), and control sleep (15 nights at 9 hours of time in bed) sequences in random order. EEG recordings during a visual attention task were completed once (day 3) during the acclimation period, 3 times during the experimental period (days 5, 6, 9), and once in the recovery period (day 15). Post-hoc off-line ERP data processing yielded N2pc, P3, and ERN/Pe waveforms with comparison of primary amplitude and latency measures across experimental timepoints utilizing mixed linear regression modeling. ERN and Pe amplitudes were significantly reduced during sleep restricted days 6 and 9 (both p<0.05, for Pe p<0.02), and P3 amplitude was significantly lower during sleep restricted days 9 and 15 (both p<0.02), compared to corresponding control sequence days. Pe (day 6) and P3 (days 5,6,9) fractional area latencies were also significantly delayed during sleep restriction compared to the corresponding control sequence days (both p<0.03). There were no significant N2pc amplitude or latency differences between conditions. These data suggest that stimulus classification and error-monitoring, reflecting mesial temporal and frontal neuronal network processing, are selectively impaired during chronic sleep restriction. Covert attentional shifting remained intact, implying greater allocation of top-down executive resources toward preserving attentional capacity at the cost of degraded error monitoring capacities during sleep restriction. Further research analyzing sleep homeostatic drive and vigilance is planned to determine whether these brain functions influence visual processing efficiency. NIH/NHLBI R01 HL 114676; Mayo Clinic CCaTS 1 UL1 RR024150-01"
pub.1100151053,Research of Configurable Hybrid Memory Architecture for Big Data Processing,"A configurable hybrid memory architecture (CHMA) for big data processing is proposed in this paper. It includes computing nodes and memory nodes. The computing node can be configured according to the requirement of different applications to improve the applicability of the computing system. The memory node contains the memory control chip and memory network that support to build memory system with different type of memory devices. Each memory control chip contains two memory controllers. Two key technologies are proposed to optimize the bandwidth and latency of memory access for CHMA, one is the multi-channel parallel bus structure, and the other is the cache or buffer structure inserted in memory control chip. Experimental results show that: first, two memory controller are integrated in the memory control chip can maximize the bandwidth efficiency of the memory network; second, the bandwidth of multi-channel parallel bus is balanced with the bandwidth of two memory channels; third, when the cache or buffer structure is inserted in memory control chip, the latency of memory access is reduced and the bandwidth of memory access is improved, the memory access bandwidth of 64-thread stream OpenMP program is increased by 16.86% and the execution speed of NPB-MPI scientific computing applications are improved by an average of 6%."
pub.1172434391,Poster: Profiling Event Vision Processing on Edge Devices,"As RGB camera resolutions and frame-rates improve, their increased energy requirements make it challenging to deploy fast, efficient, and low-power applications on edge devices. Newer classes of sensors, such as the biologically inspired neuromorphic event-based camera, capture only changes in light intensity per-pixel to achieve operational superiority in sensing latency (O(μs)), energy consumption (O(mW)), high dynamic range (140dB), and task accuracy such as in object tracking, over traditional RGB camera streams. However, highly dynamic scenes can yield an event rate of up to 12MEvents/second, the processing of which could overwhelm resource-constrained edge devices. Efficient processing of high volumes of event data is crucial for ultra-fast machine vision on edge devices. In this poster, we present a profiler that processes simulated event streams from RGB videos into 6 variants of framed representations for DNN inference on an NVIDIA Jetson Orin AGX, a representative edge device. The profiler evaluates the trade-offs between the volume of events evaluated, the quality of the processed event representation, and processing time to present the design choices available to an edge-scale event camera-based application observing the same RGB scenes. We believe that this analysis opens up the exploration of novel system designs for real-time low-power event vision on edge devices."
pub.1013519416,An on-line algorithm for cluster detection of mobile nodes through complex event processing,"Clusters of mobile elements, such as vehicles and humans, are a common mobility pattern of interest for many applications. The on-line detection of them from large position streams of mobile entities is a challenging task because it requires algorithms that are capable of continuously and efficiently processing the high volume of position updates in a timely manner. Currently, the majority of approaches for cluster detection operate in batch mode, where position updates are recorded during time periods of certain length and then batch processed by an external routine, thus delaying the result of the cluster detection until the end of the time period. However, if the monitoring application requires results at a higher frequency than the one delivered by batch algorithms, then results might not reflect the current clustering state of the entities. To overcome this limitation, in this paper we propose DG2CEP, an algorithm that combines the well-known density-based clustering algorithm DBSCAN with the data stream processing paradigm Complex Event Processing (CEP) to achieve continuous, on-line detection of clusters. Our experiments with synthetic and real world datasets indicate that DG2CEP is able to detect the formation and dispersion of clusters with small latency and higher similarity to DBSCAN׳s output than batch-based approaches."
pub.1181548180,Rapid likelihood free inference of compact binary coalescences using accelerated hardware,"We report a gravitational-wave parameter estimation algorithm, AMPLFI, based on likelihood-free inference using normalizing flows. The focus of AMPLFI is to perform real-time parameter estimation for candidates detected by machine-learning based compact binary coalescence search, Aframe. We present details of our algorithm and optimizations done related to data-loading and pre-processing on accelerated hardware. We train our model using binary black-hole (BBH) simulations on real LIGO-Virgo detector noise. Our model has ∼6 million trainable parameters with training times ≲24 h. Based on online deployment on a mock data stream of LIGO-Virgo data, Aframe + AMPLFI is able to pick up BBH candidates and infer parameters for real-time alerts from data acquisition with a net latency of ∼6 s."
pub.1093886293,"A Real Time, Low Latency, FPGA Implementation of the 2-D Discrete Wavelet Transformation for Streaming Image Applications","In this paper, we present an architecture and a hardware implementation of the $2- D$ Discrete Wavelet Transformation (DWT) for applications where row-based raw image data is streamed in at high bandwidths and local buffering of the entire image is not feasible. The architecture is especially suited for multi-spectral imager systems, such as on board an imaging satellite, however can be used in any application where time to next image constraints require real-time processing of multiple images. The latency that is introduced as the images stream through the DWT filter and the amount of locally stored image data is a function of the image and tile size. For an $n_{1} x n_{2}$ size image processed using $(n_{1}/k_{1})\ x\ (n_{2}/k_{2})$ sized tiles the latency is equal to the time elapsed to accumulate a $(1/k_{1})$ portion of one image. In addition, a $(2/k_{1})$ portion of each image is buffered locally. The proposed hardware has been implemented on an FPGA and is part of a JPEG2000 compression system designed as a payload for a Low Earth Orbit (LEO) micro-satellite, which will be launched in August 2003."
pub.1144953430,A Sensor-aware Offloading Model for IoT Edge Computing,"In IoT sensor network environment, offloading is an important factor that affects all design objectives. Since massive amounts of data are collected every second to the gateway and so immediate processing is difficult, offloading is critical to quickly eliminate worthless data in advance. Similar sensor data are continuously generated except in abnormal situations such as sudden changes and failure events. Therefore, the amount of data processing and frequency of data transmission can be greatly reduced by classifying, filtering, and compressing the data. In addition, more meaningful IoT context can be analyzed by combining multiple sensor data, since the sensor values generated by each sensor has its own characteristics. The previous offloading techniques mainly focused on minimizing latency without using data context and data resizing. Therefore, a new filtering technique is required to enhance the offloading efficiency through precision control using sensor context patterns. This paper proposes a new sensor-aware context offloading model called SCOM to support efficient data filtering services for the edge-based IoT environment. The architecture of SCOM consists of three layers of sensor context, pattern context and transmission context. SCOM exploits context-aware stream pattern matching using general string matching based on slide window for sensor stream offloading. Experiments show that the performance gain of SCOM reaches to 14.8% with respect to the operation throughput. Since proposed data layering and pattern-based offloading scheme can improve the sensor data filtering performance in edge gateways, it can be used for IoT sensor monitoring applications."
pub.1124216786,A Survey on Map-Matching Algorithms,"The map-matching is an essential preprocessing step for most of the trajectory-based applications. Although it has been an active topic for more than two decades and, driven by the emerging applications, is still under development. There is a lack of categorisation of existing solutions recently and analysis for future research directions. In this paper, we review the current status of the map-matching problem and survey the existing algorithms. We propose a new categorisation of the solutions according to their map-matching models and working scenarios. In addition, we experimentally compare three representative methods from different categories to reveal how matching model affects the performance. Besides, the experiments are conducted on multiple real datasets with different settings to demonstrate the influence of other factors in map-matching problem, like the trajectory quality, data compression and matching latency."
pub.1051002987,Low bandwidth desktop and video streaming for collaborative tiled display environments,"High-resolution display environments built on networked, multi-tile displays have emerged as an enabling tool for collaborative, distributed visualization work. They provide a means to present, compare, and correlate data in a broad range of formats and coming from a multitude of different sources. Visualization of these distributed data resources may be achieved from a variety of clustered processing and display resources for local rendering and may be streamed on demand and in real-time from remotely rendered content. The latter is particularly important when multiple users want to concurrently share content from their personal devices to further augment the shared workspace. This paper presents a high-quality video streaming technique allowing remotely generated content to be acquired and streamed to multi-tile display environments from a range of sources and over a heterogeneous wide area network.The presented technique uses video compression to reduce the entropy and therefore required bandwidth of the video stream. Compressed video delivery poses a series of challenges for display on tiled video walls which are addressed in this paper. These include delivery to the display wall from a variety of devices and localities with synchronized playback, seamless mobility as users move and resize the video streams across the tiled display wall, and low latency video encoding, decoding, and display necessary for interactive applications. The presented technique is able to deliver 1080p resolution, multimedia rich content with bandwidth requirements below 10 Mbps and low enough latency for constant interactivity. A case study is provided, comparing uncompressed and compressed streaming techniques, with performance evaluations for bandwidth use, total latency, maximum frame rate, and visual quality."
pub.1172329246,Comparative Performance Benchmarking of Data Ingestion Tools in the Hadoop Ecosystem,"In the rapidly evolving domain of big data, the efficiency of data ingestion tools is pivotal for the scalability and performance of data processing systems. Tools such as Sqoop, Flume, and Kafka play crucial roles within the Hadoop ecosystem, each catering to different aspects of data loading and real-time data handling. Given their significance, benchmarking these tools to understand their performance metrics, scalability, and operational nuances becomes essential. This paper aims to provide a comprehensive benchmark analysis of these popular data ingestion tools, offering insights into their comparative strengths and weaknesses. The methodology adopted for this benchmarking study involves a structured testing framework that evaluates the tools based on key performance indicators such as throughput, latency, scalability, and fault tolerance. Tests are conducted under controlled environments with varying data volumes and ingestion scenarios to mimic real-world usage as closely as possible. The methodology ensures that each tool is evaluated on a level playing field with standardized data sets and metrics defined for comparison. Key findings from the study indicate significant differences in the performance and suitability of each tool depending on the specific data ingestion requirements. For instance, Sqoop shows robustness in batch processing large datasets, Flume excels in log data aggregation, and Kafka demonstrates superior capabilities in handling real-time data streams with low latency. These results underscore the importance of choosing the right tool based on the data ingestion needs of an organization. The implications of these findings extend to system design and selection, where decision-makers are equipped with empirical data to guide the architecture of their data processing systems. The performance benchmarks provide a foundation for optimizing data ingestion pipelines, ultimately enhancing system efficiency and reducing operational costs. This benchmarking study not only aids in selecting the appropriate tool but also in configuring the tools optimally to leverage their strengths in the context of specific business requirements and data characteristics."
pub.1152491376,"A Centralized Support Infrastructure (CSI) to Manage CPS Digital Twin, towards the Synchronization between CPS Deployed on the Shopfloor and Their Digital Representation","Cyber Physical System (CPS) nameplate values change over time due to situation and strain. This chapter describes the future CPS as equipped with special assets named Functional Models to be uploaded to Centralized Support Infrastructure (CSI) for synchronization and data analysis. Functional Models are essentially software routines that are run against data sent by the CPS. In a nutshell, the microservice architecture is the evolution of the classical Service Oriented Architecture (SOA) in which the application is seen as a suite of small services, each devoted to a single activity. Big Data technologies are becoming innovation drivers in industry. The CSI is required to handle unprecedented volumes of data generated by the digital representation of the factory in order to keep updated the CPS nameplate information. The speed layer is in charge of processing infinite streams of information. It is the purpose of the Speed Layer to offer a low latency, real-time data processing."
pub.1068341025,An Efficient Distributed Data Processing Method for Smart Environment,"Background/Objectives: In current times, huge volume of data at a very high velocity gets generated through social media and various sensors in embedded systems that are connected to the Internet which causes a very Big data problem. These challenging Big data’s need to be processed and stored by traditional Relational Database Management Systems (RDBMS). Due to this reason the need for new software solutions has emerged for managing the Big data in an efficient, scalable and smart way. Methods/Statistical Analysis: In this study, an approach to combine the concept of batch processing and stream processing to an end where we can query the data set which also supports Adhoc Querying with less latency, that can be run on any Large scale Machine Learning Algorithms for recognizing any interest pattern in the streaming data set was employed. The functionalities of Hadoop ecosystem’s tool HIVE can also be used to produce the results to Adhoc queries, User Defined Functions (UDF) similar to writing a SQL Stored Procedures in the Spark System. An interface with SerDes which is Serialization and De-serialization that helps us to talk to the standard stream where we can exactly query the dataset are employed. Findings: : By proposing a new software solution AllJoyn Lambda, in which AllJoyn is integrated in the lambda architecture and the prototype implementation of the architecture is done using Apache Hadoop Yarn over Apache Spark Streaming are presented . This study illuminates the high velocity streaming data set on a database without losing any data from the streaming domain, to support Adhoc Querying from the data set and to provide a mechanism for fast data processing and analytics using Large Scale Machine Learning. This paper highlights the analysis of large scale dataset processing, handling challenges, and its comprehensive systematic review. Applications/Improvements: From this study, we conclude that, building a smart environment by using the big data setup platform improves and enhances the results for the smart environment.Keywords: AllJoyn Lambda Architecture, Big Data Analytics, Internet of Things, Smart Environment, Spark Streaming"
pub.1170778549,Improving Real-Time Data Streams Performance on Autonomous Surface Vehicles using DataX,"In the evolving Artificial Intelligence (AI) era, the need for real-time algorithm processing in marine edge en-vironments has become a crucial challenge. Data acquisition, analysis, and processing in complex marine situations require sophisticated and highly efficient platforms. This study optimizes real-time operations on a containerized distributed processing platform designed for Autonomous Surface Vehicles (ASV) to help safeguard the marine environment. The primary objective is to improve the efficiency and speed of data processing by adopting a microservice management system called DataX. DataX leverages containerization to break down operations into modular units, and resource coordination is based on Kubernetes. This combination of technologies enables more efficient resource management and real-time operations optimization, contributing significantly to the success of marine missions. The platform was developed to address the unique challenges of managing data and running advanced algorithms in a marine context, which often involves limited connectivity, high latencies, and energy restrictions. Finally, as a proof of concept to justify this platform's evolution, experiments were carried out using a cluster of single-board computers equipped with GPUs, running an AI-based marine litter detection application and demonstrating the tangible benefits of this solution and its suitability for the needs of maritime missions."
pub.1122234970,"F3C: Fog-enabled Joint Computation, Communication and Caching Resource Sharing for Energy-Efficient IoT Data Stream Processing","Fog/edge computing has been recently regarded as a promising approach for supporting emerging mission-critical Internet of Things (IoT) applications on capacity and battery constrained devices. By harvesting and collaborating a massive crowd of devices in close proximity for computation, communication and caching resource sharing (i.e., 3C resources), it enables great potentials in low-latency and energy-efficient IoT task execution. To efficiently exploit 3C resources of fog devices in proximity, we propose F3C, a fog-enabled 3C resource sharing framework for energy-efficient IoT data stream processing by solving an energy cost minimization problem under 3C constraints. Nevertheless, the minimization problem proves to be NP-hard via reduction to a Generalized Assignment Problem (GAP). To cope with such challenge, we propose an efficient F3C algorithm based on an iterative task team formation mechanism which regards each task's 3C resource sharing as a subproblem solved by the elaborated min cost flow transformation. Via utility improving iterations, the proposed F3C algorithm is shown to converge to a stable system point. Extensive performance evaluations demonstrate that our F3C algorithm can achieve superior performance in energy saving compared to various benchmarks."
pub.1092105963,"Fog Computing: Issues, Challenges and Future Directions","In Cloud Computing, all the processing of the data collected by the node is done in the central server. This involves a lot of time as data has to be transferred from the node to central server before the processing of data can be done in the server. Also it is not practical to stream terabytes of data from the node to the cloud and back. To overcome these disadvantages, an extension of cloud computing, known as fog computing, is introduced. In this, the processing of data is done completely in the node if the data does not require higher computing power and is done partially if the data requires high computing power, after which the data is transferred to the central server for the remaining computations. This greatly reduces the time involved in the process and is more efficient as the central server is not overloaded. Fog is quite useful in geographically dispersed areas where connectivity can be irregular. The ideal use case requires intelligence near the edge where ultra-low latency is critical, and is promised by fog computing. The concepts of cloud computing and fog computing will be explored and their features will be contrasted to understand which is more efficient and better suited for real-time application."
pub.1085913448,GVoS,"The exponential increase of online videos greatly enriches the life of users but also brings huge numbers of near-duplicate videos (NDVs) that seriously challenge the video websites. The video websites entail NDV-related applications such as detection of copyright violation, video monitoring, video re-ranking, and video recommendation. Since these applications adopt different features and different processing procedures due to diverse scenarios, constructing separate and special-purpose systems for them incurs considerable costs on design, implementation, and maintenance. In this article, we propose a general NDV system on Storm (GVoS)—a popular distributed real-time stream processing platform—to simultaneously support a wide variety of video applications. The generality of GVoS is achieved in two aspects. First, we extract the reusable components from various applications. Second, we conduct the communication between components via a mechanism called Stream Shared Message (SSM) that contains the video-related data. Furthermore, we present an algorithm to reduce the size of SSM in order to avoid the data explosion and decrease the network latency. The experimental results demonstrate that GVoS can achieve performance almost the same as the customized systems. Meanwhile, GVoS accomplishes remarkably higher systematic versatility and efficiently facilitates the development of various NDV-related applications."
pub.1036064394,Addressing BI Transactional Flows in the Real-Time Enterprise Using GoldenGate TDM,"It’s time to visit low latency and reliable real-time (RT) infrastructures to support next generation BI applications instead of continually debating the need and notion of real-time. The last few years have illuminated some key paradigms affecting data management. The arguments put forth to move away from traditional DBMS architectures have proven persuasive - and specialized architectural data stores are being adopted in the industry [1]. The change from traditional database pull methods towards intelligent routing/push models is underway, causing applications to be redesigned, redeployed, and re-architected. One direct result of this is that despite original warnings about replication [2] – enterprises continue to deploy multiple replicas to support both performance, and high availability of RT applications, with an added complexity around manageability of heterogeneous computing systems. The enterprise is overflowing with data streams that require instantaneous processing and integration, to deliver faster visibility and invoke conjoined actions for RT decision making, resulting in deployment of advanced BI applications as can be seen by stream processing over RT feeds from operational systems for CEP [3]. Given these various paradigms, a multitude of new challenges and requirements have emerged, thereby necessitating different approaches to management of RT applications for BI. The purpose of this paper is to offer a viewpoint on how RT affects critical operational applications, evolves the weight of non-critical applications, and pressurizes availability/data-movement requirements in the underlying infrastructure. I will discuss how the GoldenGate TDM platform is being deployed within the RTE to manage some of these challenges particularly around RT dissemination of transactional data to reduce latency in data integration flows, to enable real-time reporting/DW, and to increase availability of underlying operational systems. Real world case studies will be used to support the various discussion points. The paper is an argument to augment traditional DI flows with a real-time technology (referred to as transactional data management) to support operational BI requirements."
pub.1141007588,Streaming Data Priority Scheduling Framework for Autonomous Driving by Edge,"In recent years, intelligent vehicles like autonomous vehicles generate a huge amount of sensing data continuously. The computations on those data streams are far beyond the processing capacity of on-board computing. To deal with the streaming data process in real-time, the deployment of streaming data processing system by edge turns to the first choice in terms of performance. However, the existing frameworks cannot satisfy the complicated demands from autonomous driving tasks and lack the ability in supporting the task priority scheduling. In this paper, we propose a streaming data priority scheduling framework for autonomous driving by edge on Spark Streaming and make an implementation on Spark 2.3.0. The proposed framework can identify the priorities among different data processing tasks and implement the task scheduling based on non-preemptive priority queuing theory. To meet differentiated service level requirements, the proposed non-preemptive priority queuing scheduling mechanism considers the priority category of tasks, the distance between vehicles and edge nodes, and the priority weight of vehicles. Experiments show that this mechanism can effectively identify the priority information of different tasks from different vehicles and reduce the end-to-end latency of high-priority tasks by up to 46% than low-priority tasks."
pub.1094044464,Joint application of spread spectrum and OFDM modulation for microwave radio communication used for Unmanned Aerial Vehicle,"The wireless communication system used by an Unmanned Aerial Vehicle has special requirements. Two separate data streams are used, one for telemetry and control of the vehicle, and one for payload. The first stream is of relatively low data rate, but must be very robust. The second stream requires high data rate, but a best effort approach is acceptable. Realtime remote control usage mandates low response time (turn around latency) of the system, which demands small frames and frequent direction changes over the half duplex channel. These two different requirements must be accounted for in the design of the modulation system. In this paper our solution based on the joint application of a chirp spread spectrum modulation with a correlator based demodulator for the telemetry and control data stream and a high data rate orthogonal frequency division multiplexing (OFDM) modulation for the payload is presented. The OFDM communication occupies the same bandwidth as the spread spectrum modulation, but - unlike the Chirp signal - utilizes its full bandwidth. By using a correlation detector for all timing recovery tasks and keeping the same timing for the OFDM part of the frame, all control loops normally needed for this task (symbol synchronization PLL, COSTAS loop) can be eliminated. This results in the whole signal processing path being a finite impulse response system which aids us in a highly pipelineable design implementation and makes it possible to create a high speed implementation with a low-end field programmable gate array (FPG A) circuit."
pub.1162966709,Noise-Shaping Binary-to-Stochastic Converters for Reduced-Length Bit-Streams,"Stochastic computations have attracted significant attention for applications with moderate fixed-point accuracy requirements, as they offer minimal complexity. In these systems, a stochastic bit-stream encodes a data sample. The derived bit-stream is used for processing. The bit-stream length determines the computation latency for bit-serial implementations and hardware complexity for bit-parallel ones. Noise shaping is a feedback technique that moves the quantization noise outside the bandwidth of interest of a signal. This article proposes a technique that builds on noise shaping and reduces the length of the stochastic bit-stream required to achieve a specific Signal-to-Quantization-Noise Ratio (SQNR). The technique is realized by digital units that encode binary samples into stochastic streams, hereafter called as binary-to-stochastic converters. Furthermore, formulas are derived that relate the bit-stream length reduction to the signal bandwidth. First-order and second-order converters that implement the proposed technique are analyzed. Two architectures are introduced, distinguished by placing a stochastic converter either inside or outside of the noise-shaping loop. The proposed bit-stream length reduction is quantitatively compared to conventional binary-to-stochastic converters for the same signal quality level. Departing from conventional approaches, this article employs bit-stream lengths that are not a power of two, and proposes a modified stochastic-to-binary conversion scheme as a part of the proposed binary-to-stochastic converter. Particularly, SQNR gains of 29.8 dB and 42.1 dB are achieved for the first-order and second-order converters compared to the conventional converters for equal-length bit-streams and low signal bandwidth. The investigated converters are designed and synthesized at a 28-nm FDSOI technology for a range of bit widths."
pub.1110363845,A Novel 1D-Convolution Accelerator for Low-Power Real-time CNN processing on the Edge,"With the rise of deep learning, the demand for real-time edge intelligence is greater than ever. Current algorithm and hardware realizations often focus on the cloud paradigm and maintain the assumption that the entire frames data is available in large batches. As a result, obtaining real-time AI inference at the edge has been a tough goal due to tight-latency awareness as well as streaming nature of the data. There is an inherent need for novel architectures that can realize latency-aware agile deep learning algorithms at the edge. This paper introduces a novel joint algorithm architecture approach to enable real-time low-power Convolutional Neural Network (CNN) processing on edge devices. The core of the proposed approach is utilizing 1D dimensional convolution with an architecture that can truly benefit from the algorithm optimization. On the algorithm side, we present a novel training and inference based on 1D convolution. On the architecture side, we present a novel data flow architecture with the capability of performing on-the-fly 1D convolution over the pixel stream. Our results on Xilinx Zynq-7000 FPGA for SqueezeNet demonstrates only 2% lost in accuracy while maintaining real-time processing of 60 frames per second with only 1.73W power consumption. The Dynamic power consumption is 7.3X lower than regular 2D convolution CNN for performing the same frame rate, and 4.3X less than Nvidia Jetson TX2 total power, performing only 30 frame per second."
pub.1095529295,Exploiting Maximal Overlap for Non-Contiguous Data Movement Processing on Modern GPU-Enabled Systems **This research is supported in part by National Science Foundation grants #OCI-1148371 and #CCF-1213084 and #ACI-1450440.,"GPU accelerators are widely used in HPC clusters due to their massive parallelism and high throughput-per-watt. Data movement continues to be the major bottleneck on GPU clusters, more so when data is non-contiguous, which is common in scientific applications. CUDA-Aware MPI libraries optimize non-contiguous data movement processing using latency-oriented techniques such as using GPU kernels to accelerate the packing/unpacking operations. Although they optimize the latency of a single operation, the inherent restrictions of existing designs limit their efficiency for throughput oriented patterns. Indeed, none of the existing designs fully exploit the parallelism abundantly available on GPUs to provide high throughput and efficient resource utilization by enabling maximal overlap. In this paper, we propose novel designs for CUDA-Aware MPI libraries to achieve efficient GPU resource utilization and maximal overlap between CPUs and GPUs for non-contiguous data processing and movement. The proposed designs take advantage of several CUDA features, such as Hyper-Q/multi-streams and callback functionality, to deliver high performance and efficiency. To the best of our knowledge, this is the first such study that aims at achieving high throughput and efficient GPU utilization for non-contiguous MPI data processing and movement to/from GPUs. For intra-node inter-GPU ping-pong experiments using DDTBench, the proposed designs shows up to 54%, 67%, 61% performance improvement on the SPECFEM3D_oc, SPECFEM3D_cm and WRF_y_sa benchmarks respectively. The proposed designs also deliver up to 33% improvement on the total execution time over the existing designs for the HaloExchange-based application kernel that models the communication pattern of the MeteoSwiss weather forecasting model over 32 GPU nodes on Wilkes GPU cluster."
pub.1171511211,From cloud and fog computing to federated-fog computing: A comparative analysis of computational resources in real-time IoT applications based on semantic interoperability,"In contemporary computing paradigms, the evolution from cloud computing to fog computing and the recent emergence of federated-fog computing have introduced new challenges pertaining to semantic interoperability, particularly in the context of real-time applications. Fog computing, by shifting computational processes closer to the network edge at the local area network level, aims to mitigate latency and enhance efficiency by minimising data transfers to the cloud. Building upon this, federated-fog computing extends the paradigm by distributing computing resources across diverse organisations and locations, while maintaining centralised management and control. This research article addresses the inherent problematics in achieving semantic interoperability within the evolving architectures of cloud computing, fog computing, and federated-fog computing. Experimental investigations are conducted on a diverse node-based testbed, simulating various end-user devices, to emphasise the critical role of semantic interoperability in facilitating seamless data exchange and integration. Furthermore, the efficacy of federated-fog computing is rigorously evaluated in comparison to traditional fog and cloud computing frameworks. Specifically, the assessment focuses on critical factors such as latency time and computational resource utilisation while processing real-time data streams generated by Internet of Things (IoT) devices. The findings of this study underscore the advantages of federated-fog computing over conventional cloud and fog computing paradigms, particularly in the realm of real-time IoT applications demanding high performance (lowering CPU usage to 20%) and low latency (with picks up to 300ms). The research contributes valuable insights into the optimisation of processing architectures for contemporary computing paradigms, offering implications for the advancement of semantic interoperability in the context of emerging federated-fog computing for IoT applications."
pub.1156421931,A data infrastructure for heterogeneous telemetry adaptation. Application to Netflow-based cryptojacking detection,"The increasing development of cryptocurrencies has brought cryptojacking as a new security threat in which attackers steal computing resources for cryptomining. The digitization of the supply chain is a potential major target for cryptojacking due to the large number of different infrastructures involved. These different infrastructures provide information sources that can be useful to detect cryptojacking, but with a wide variety of data formats and encodings. This paper describes the Semantic Data Aggregator (SDA), a normalization and aggregation system based on data modelling and low-latency processing of data streams that facilitates the integration of heterogeneous information sources. As a use case, the paper describes a Cryptomining Detection System (CDS) based on network traffic flows processed by a machine learning engine. The results show how the SDA is leveraged in this use case to obtain aggregated information that improves the performance of the CDS."
pub.1070961405,Fault Tolerant Distributed Stream Processing based on Backtracking,"Since distributed stream analytics is treated as a kind of cloud service, there exists a pressing need for its reliability and fault-tolerance, to guarantee the streaming data tuples to be processed in the order of their generation in every dataflow path, with each tuple processed once and only once. Currently there exist two kind approaches: one treats the whole process as a single transaction, and therefore suffers from the loss of intermediate results during failures; the other relies on the receipt of acknowledgement (ACK) to decide whether moving forward to emit the next resulting tuple or resending the current one after timeout, on the per-tuple basis, thus incurs extremely high latency penalty. In contradistinction to the above, we propose the backtrack mechanism for failure recovery, which allows a task to process tuples continuously without waiting for ACKs and without resending tuples in the failure-free case, but to request (ASK) the source tasks to resend the missing tuples only when it is restored from a failure which is a rare case thus has limited impact on the overall performance.The specific hard problem for building a transaction layer on-top of an existing stream processing platform consists in how to keep track the physical input/output messaging channels in order to realize re-messaging during failure recovery. Our solution is characterized by tracking physical messaging channels logically, for that we introduce the notions of virtual channel, task alias and messageId-set in reasoning, recording and communicating the channel information. We also provide a designated messaging channel, separated from the regular dataflow channel, for signaling ACK/ASK messages and for resending tuples, in order to avoid interrupting the regular order of data transfer.We have implemented the proposed mechanisms on Fontainebleau, the distributed stream analytics infrastructure we developed on top of Storm. As a principle, we ensure all the transactional properties to be system supported and transparent to users. Our experience shows the novelty and efficiency of the proposed mechanisms."
pub.1163656149,On Data Processing through the Lenses of S3 Object Lambda,"Despite that Function-as-a-Service (FaaS) has settled down as one of the fundamental cloud programming models, it is still evolving quickly. Recently, Amazon has introduced S3 Object Lambda, which allows a user-defined function to be automatically invoked to process an object as it is being downloaded from S3. As with any new feature, careful study thereof is the key to elucidate if S3 Object Lambda, or more generally, if inline serverless data processing, is a valuable addition to the cloud. For this reason, we conduct an extensive measurement study of this novel service, in order to characterize its architecture and performance (in terms of coldstart latency, TTFB times, and more). We particularly put an eye on the streaming capabilities of this new form of function, as it may open the door to empower existing serverless systems with stream processing capacities. We discuss the pros and cons of this new capability through several workloads, concluding that S3 Object Lambda can go far beyond its original purpose and be leveraged as a building block for more complex abstractions."
pub.1150923300,EDeNN: Event Decay Neural Networks for low latency vision,"Despite the success of neural networks in computer vision tasks, digital
'neurons' are a very loose approximation of biological neurons. Today's
learning approaches are designed to function on digital devices with digital
data representations such as image frames. In contrast, biological vision
systems are generally much more capable and efficient than state-of-the-art
digital computer vision algorithms. Event cameras are an emerging sensor
technology which imitates biological vision with asynchronously firing pixels,
eschewing the concept of the image frame. To leverage modern learning
techniques, many event-based algorithms are forced to accumulate events back to
image frames, somewhat squandering the advantages of event cameras.
  We follow the opposite paradigm and develop a new type of neural network
which operates closer to the original event data stream. We demonstrate
state-of-the-art performance in angular velocity regression and competitive
optical flow estimation, while avoiding difficulties related to training SNN.
Furthermore, the processing latency of our proposed approach is less than 1/10
any other implementation, while continuous inference increases this improvement
by another order of magnitude."
pub.1092302639,NanoStreams: A Microserver Architecture for Real-Time Analytics on Fast Data Streams,"Ever increasing power consumption has created great interest in energy-efficient microserver architectures but they lack the computational, networking, and storage power necessary to cope with real-time data analytics. We propose NanoStreams, an integrated architecture comprising an ARM-based microserver, coupled via a novel, low latency network interface, Nanowire, to an Analytics-on-Chip architecture implemented on Field Programmable Gate Array (FPGA) technology; the architecture comprises ARM cores for performing low latency transactional processing, integrated with programmable, energy efficient Nanocore processors for high-throughput streaming analytics. The paper outlines the complete system architecture, hardware level detail, compiler, network protocol, and programming environment. We present experiments from the financial services sector, comparing a state-of-the-art server based on Intel Sandy Bridge processors, an ARM based Calxeda ECS-1000 microserver and ODROID XU3 node, with the NanoStreams microserver architecture using an industrial workload. For end-to-end workload, the NanoStreams microserver achieves energy savings up to 10.7, 5.87 and 5 compared to the Intel server, Calxeda microserver and ODROID node, respectively."
pub.1138301733,Efficient XML data placement schemes over multiple mobile wireless broadcast channels,"Broadcasting is one of the basic ways to access XML data via mobile wireless networks. In these networks, XML data are disseminated over a wireless broadcast channel and mobile clients can connect to this channel via portable devices and receive the XML data they need. To date, various indexing methods have been proposed to broadcast the XML data over mobile wireless broadcast networks, each of which attempts to reduce the latency and energy consumption of mobile devices in processing the different types of XML queries. Most of the proposed indexing methods use only a single wireless broadcast channel for XML data dissemination. Due to the sequential data access over a wireless broadcast channel, the access time to retrieve the desired XML data over a broadcast channel is longer if the length of generated XML data stream is large. This paper proposes two new XML data placement schemes to disseminate the XML data over multiple wireless broadcast channels by partitioning the XML data stream into several partitions and placing them into multiple wireless broadcast channels. The goal of both of which is a further reduction of the access time during the process of XML querying over the broadcast channels. According to the experimental results, the proposed XML data placement schemes have achieved the goal of further reducing the access time, especially when the size of XML document to be disseminated is large."
pub.1167359393,Utilizing Flink and Kafka Technologies for Real-Time Data Processing: A Case Study,"In today's very competitive business world, being able to use data to its fullest in real time has become a key differentiation. This paper looks at how two cutting-edge technologies, Apache Flink and Apache Kafka, work together and how they are changing the way real-time data is processed and analyzed. With its fault-tolerant framework made for collecting data from many sources, Apache Kafka is a leader in reliability and scalability when it comes to ingesting data. Apache Flink is the perfect partner for Kafka because it is great at stream processing and low-latency event handling. This paper carefully explains how these technologies work together to create a complete set of tools for handling and analyzing data in real time. The paper goes into detail about how Flink and Kafka can work together, showing how data streams can be handled and intelligently put together to produce insights that can be used. This set of tools, which was created after a lot of study and real-world experience, helps organizations that want to start using real-time data in new ways. Evaluations of performance, scalability, and real-world applications show that this integrated method has a real effect. Beyond just talking about ideas, this study paper gives organizations a step-by-step plan for how to use real-time data to improve their decision-making. By taking advantage of how well Flink and Kafka work together, companies can become more flexible, quick to respond, and creative."
pub.1020864126,Energy-efficient adaptive networked datacenters for the QoS support of real-time applications,"In this paper, we develop the optimal minimum-energy scheduler for the adaptive joint allocation of the task sizes, computing rates, communication rates and communication powers in virtualized networked data centers (VNetDCs) that operate under hard per-job delay-constraints. The considered VNetDC platform works at the Middleware layer of the underlying protocol stack. It aims at supporting real-time stream service (such as, for example, the emerging big data stream computing (BDSC) services) by adopting the software-as-a-service (SaaS) computing model. Our objective is the minimization of the overall computing-plus-communication energy consumption. The main new contributions of the paper are the following ones: (i) the computing-plus-communication resources are jointly allotted in an adaptive fashion by accounting in real-time for both the (possibly, unpredictable) time fluctuations of the offered workload and the reconfiguration costs of the considered VNetDC platform; (ii) hard per-job delay-constraints on the overall allowed computing-plus-communication latencies are enforced; and, (iii) to deal with the inherently nonconvex nature of the resulting resource optimization problem, a novel solving approach is developed, that leads to the lossless decomposition of the afforded problem into the cascade of two simpler sub-problems. The sensitivity of the energy consumption of the proposed scheduler on the allowed processing latency, as well as the peak-to-mean ratio (PMR) and the correlation coefficient (i.e., the smoothness) of the offered workload is numerically tested under both synthetically generated and real-world workload traces. Finally, as an index of the attained energy efficiency, we compare the energy consumption of the proposed scheduler with the corresponding ones of some benchmark static, hybrid and sequential schedulers and numerically evaluate the resulting percent energy gaps."
pub.1181084484,Real-time processing and optimization strategies for IoT data streams,"Abstract  With the development of industrial IoT and the arrival of smart manufacturing, the field of edge computing has gained more and more attention. However, traditional industrial computing scenarios relying on industrial clouds make data latency a greater challenge. In this paper, for the contradiction between edge devices and task resource allocation encountered in edge computing scenarios in smart manufacturing, we propose an industrial internet task scheduling model for smart manufacturing and introduce a scheduling node state matrix to realize the state management of each scheduling subtask. Aiming at the problem of multiple tasks seizing resources in a complex, intelligent manufacturing environment, the study combines the caching mechanism to realize the task offloading computational processing of order scheduling, in which the caching mechanism is used to solve the problem of computational resource limitations at the edge. It is found through simulation that when the computational task factor  ξ k  =2 is larger, more offloading power is allowed to be transmitted to the edge ser ver for computation. For computational tasks with smaller task factor  ξ k  , the device tends to allocate more computational rate to that computational task. Eventually the data queue length will be continuously reduced and the data queue is concentrated in the interval of very small values, this result verifies that the task scheduling algorithm is able to perform task scheduling efficiently and reduce the latency. "
pub.1113540180,Low-latency Speaker-independent Continuous Speech Separation,"Speaker independent continuous speech separation (SI-CSS) is a task of converting a continuous audio stream, which may contain overlapping voices of unknown speakers, into a fixed number of continuous signals each of which contains no overlapping speech segment. A separated, or cleaned, version of each utterance is generated from one of SI-CSS’s output channels nondeterministically without being split up and distributed to multiple channels. A typical application scenario is transcribing multi-party conversations, such as meetings, recorded with microphone arrays. The output signals can be simply sent to a speech recognition engine because they do not include speech overlaps. The previous SI-CSS method uses a neural network trained with permutation invariant training and a data-driven beamformer and thus requires much processing latency. This paper proposes a low-latency SI-CSS method whose performance is comparable to that of the previous method in a microphone array-based meeting transcription task. This is achieved (1) by using a new speech separation network architecture combined with a double buffering scheme and (2) by performing enhancement with a set of fixed beamformers followed by a neural post-filter."
pub.1166320754,Live4D: A Real-time Capture System for Streamable Volumetric Video,"Volumetric video holds promise for virtual and augmented reality (VR/AR) applications but faces challenges in interactive scenarios due to high hardware costs, complex processing and substantial data streams. In this paper, we introduce Live4D, a cost-effective, real-time volumetric video generation and streaming system using an RGB-only camera setup. We propose a novel deep implicit surface reconstruction algorithm, that combined neural signed distance field with observed truncated signed distance field to generate the watertight meshes with low latency. Moreover, we achieve a robust non-rigid tracking method that provides temporal stability to the meshes while resisting tracking failure cases. Experimental results show that Live4D achieves a performance of 24fps using mid-range graphic cards and exhibits an end-to-end latency of 95ms. The system enables live streaming of volumetric video within a 20Mbps bandwidth requirement, positioning Live4D as a promising solution for real-time 3D vision content creation in the growing VR/AR industry."
pub.1122707092,Haren,"In modern Stream Processing Engines (SPEs), numerous diverse applications, which can differ in aspects such as cost, criticality or latency sensitivity, can co-exist in the same computational node. When these differences need to be considered to control the performance of each application, custom scheduling of operators to threads is of key importance. Many solutions have been proposed regarding schedulers that allocate threads to operators to optimize specific metrics (e.g., latency) but there is still lack of a middleware that allows arbitrarily complex scheduling strategies to be seamlessly plugged on top of an SPE. We demonstrate Haren, a general scheduling middleware that fills this gap. Haren can be integrated into SPEs through a compact interface and efficiently enforce user-defined scheduling rules. This demo shows how Haren makes it is possible to adapt the use of computational resources over time to meet the goals of a variety of user-defined scheduling policies."
pub.1155310842,Callipepla: Stream Centric Instruction Set and Mixed Precision for Accelerating Conjugate Gradient Solver,"The continued growth in the processing power of FPGAs coupled with high bandwidth memories (HBM), makes systems like the Xilinx U280 credible platforms for linear solvers which often dominate the run time of scientific and engineering applications. In this paper, we present Callipepla, an accelerator for a preconditioned conjugate gradient linear solver (CG). FPGA acceleration of CG faces three challenges: (1) how to support an arbitrary problem and terminate acceleration processing on the fly, (2) how to coordinate long-vector data flow among processing modules, and (3) how to save off-chip memory bandwidth and maintain double (FP64) precision accuracy. To tackle the three challenges, we present (1) a stream-centric instruction set for efficient streaming processing and control, (2) vector streaming reuse (VSR) and decentralized vector flow scheduling to coordinate vector data flow among modules and further reduce off-chip memory access latency with a double memory channel design, and (3) a mixed precision scheme to save bandwidth yet still achieve effective double precision quality solutions. To the best of our knowledge, this is the first work to introduce the concept of VSR for data reusing between on-chip modules to reduce unnecessary off-chip accesses and enable modules working in parallel for FPGA accelerators. We prototype the accelerator on a Xilinx U280 HBM FPGA. Our evaluation shows that compared to the Xilinx HPC product, the XcgSolver, Callipepla achieves a speedup of 3.94x, 3.36x higher throughput, and 2.94x better energy efficiency. Compared to an NVIDIA A100 GPU which has 4x the memory bandwidth of Callipepla, we still achieve 77% of its throughput with 3.34x higher energy efficiency. The code is available at https://github.com/UCLA-VAST/Callipepla."
pub.1095618960,FPGA Based Parallel Implementation of Morphological Filters,"This paper presents a parallel algorithm and its hardware architecture for implementing 2-D gray-scale morphological operations namely dilation and erosion using rectangular flat top structuring elements. The proposed architecture supports parallel extension whereby throughput and processing frame rate is enhanced. The architecture is fully generic and runtime programmable with respect to image size and structuring elements size respectively. The main advantage of the architecture is its low latency, lower internal memory requirements, higher processing frame rate and throughput which makes it more amenable to real time applications. Additionally, it makes use of stream processing which eliminates the need for buffering image data, whereby memory overhead is minimized. The architecture has been synthesized using Xilinx Design Suite 14.2 ISE and prototyped on Virtex 5 FPGA Board and verified using xilinx ISIM Simulator. The proposed architecture has been tested for images of varied gray-scale geometric dimension and the results shows satisfactory performance."
pub.1040264260,Effective Task Scheduling for Embedded Systems Using Iterative Cluster Slack Optimization,"To solve computationally expensive problems, multiple processor SoCs (MPSoCs) are frequently used. Mapping of applications to MPSoC architectures and scheduling of tasks are key problems in system level design of embedded systems. In this paper, a cluster slack optimization algorithm is described, in which the tasks in a cluster are simultaneously mapped and scheduled for heterogeneous MPSoC architectures. In our approach, the tasks are iteratively clustered and each cluster is optimized by using the branch and bound technique to capitalize on slack distribution. The proposed static task mapping and scheduling method is applied to pipelined data stream processing as well as for batch processing. In pipelined processing, the tradeoff between throughput and memory cost can be exploited by adjusting a weighting parameter. Furthermore, an energy-aware task mapping and scheduling algorithm based on our cluster slack optimization is developed. Experimental results show improvement in latency, throughput and energy."
pub.1025275753,Stream Processing of a Neural Classifier II,"This article presents a real-time Fuzzy ART neural classifier for skin segmentation implemented on a Graphics Processing Unit (GPU). GPUs have evolved into powerful programmable processors, becoming increasingly used in time-dependent research fields such as dynamics simulation, database management, computer vision or image processing. GPUs are designed following a Stream Processing Model and each new generation of commodity graphics cards incorporates rather more powerful and flexible GPUs (Owens, 2005). In the last years General Purpose GPU (GPGPU) computing has established as a well-accepted application acceleration technique. The GPGPU phenomenon belongs to larger research areas: homogeneous and heterogenous multi-core computing. Research in these fields is driven by factors as the Moore’s Gap. Today’s uni-processors follow a 90/100 rule, where 90 percent of the processor is passive and 10 percent is doing active work. By contrast, multi-core processors try to follow the same general rule but with 10 percent passive and 90 percent active processors when working at full throughput. Single processor Central Processing Units (CPUs) were designed for executing general purpose programs comprised of sequential instructions operating on single data. Designers tried to optimize complex control requirements with minimum latency, thus many transistors in the chip are devoted to branch prediction, out of order execution and caching. In the article Stream Processing of a Neural Classifier I several terms and concepts related to GPGPU were introduced. A detailed description of the Fuzzy ART ANN implementation on a commodity graphics card, exploiting the GPU’s parallelism and vector capabilities, was given. In this article, the aforementioned Fuzzy ART GPU-designed implementation is configured for robust real-time skin recognition. Both learning and testing processes are done on the GPU using chrominance components in TSL (Tint, Saturation and Luminance) color space. The Fuzzy ART ANN implementation recognizes skin tone pixels at a rate of 270 fps on an NVIDIA GF7800GTX GPU."
pub.1101372132,"Chapter 12 Big Data, Data Streaming, and the Mobile Cloud","This chapter covers three of the most exciting and demanding classes of cloud applications, Big Data, data streaming, and mobile cloud computing. Big Data and data streaming applications require low-latency, scalability, versatility, and a high degree of fault-tolerance. Achieving these qualities at scale is extremely challenging. The chapter starts with a discussion of defining attributes of Big Data and of the Mesa datastore, Spanner, and F1 databases developed at Google for storing massive amounts of data. Many cloud applications process Big Data; data analytics is an important class of such applications. Bootstrapping techniques offer a low latency alternative for responding to queries of very large datasets. Such techniques and approximate query processing are analyzed next along with applications combining mathematical modeling with simulation and measurements, the so-called dynamic, data-driven applications. Computer clouds host many classes of data streaming applications, ranging from content delivery to applications consuming a continuous stream of events. The scale of the cloud infrastructure makes it possible to add mission-critical applications demanding very high availability. Scale amplifies variability often causing heavy-tail distributions of critical performance metrics, e.g., latency. Mobile cloud computing enables the execution of mobile applications on mobile devices and computer clouds and lies at the intersection of cloud computing, wireless networks, and mobile devices. Mobile devices generate data stored on the cloud and shared with others and act as both producers and consumers of data. After an introduction to mobile computing and its applications, the chapter covers energy efficiency of mobile computing and then presents alternative mobile computing models including Cloudlets. The chapter continues with the discussion of the mobile edge clouds and Markov Decision Processes."
pub.1094214057,Optical Dielectric Rod Antenna for On-chip Communications,"Recent advances in micro-fabrication technologies have enabled an exponential increase in the density of on-chip components. Many-core chips are expected to be the main stream of future central processing units (CPUs). Dual-and quad-core CPUs are already available; however, if additional cores are integrated on a single chip, the low power efficiency and high signal latency appear as serious issues. Traditional core-to-core communication is enabled using copper wires. As more cores are integrated, the sizes of metallic interconnects shrink, while the network complexity increases. As a result, the power losses in the interconnection network are considerably higher. It is estimated that a 256-core CPU will have power loss in excess of 150W [1]. Due to this large network loss, the cores in a many-core system must also serve as signal repeaters before passing on signals. In this case, the signal latency is dramatically increased, since data packets may require many hops to find their destinations."
pub.1107204294,Active Phasor Data Concentrator performing adaptive management of latency,"The Phasor Data Concentrator (PDC) is a function in charge to receive and combine the time-tagged synchrophasor data from Phasor Measurement Units (PMUs). The tasks of the PDC can include data handling, processing, and storage. Collected data are forwarded to the next higher-level element of the hierarchical monitoring architecture, which means either an operational center or a higher level PDC. Definitions of the terminology, functional descriptions and the test procedures concerning the PDC can be found in the guide IEEE C37.244-2013. In particular, in order for the PDC to ensure good latency performance, its interfacing with both the lower hierarchical level (i.e. PMUs with different features) and the higher one must be done in a reasonable time. It is worth noting that, while PMUs and PDCs were originally conceived for transmission systems, they are now expected to become key elements also for the monitoring of modern distribution grids. In this evolving and complex scenario, the PDC could play a crucial and active role. In this paper, an active PDC with advanced functionalities is proposed to manage the delay of several PMU streams so that an original adaptive data aggregation policy is implemented to allow compliance with time constraints of real-time applications."
pub.1118854911,Strategies for Big Data Analytics through Lambda Architectures in Volatile Environments,"Expectations regarding the future growth of Internet of Things (IoT)-related
technologies are high. These expectations require the realization of a
sustainable general purpose application framework that is capable to handle
these kinds of environments with their complexity in terms of heterogeneity and
volatility. The paradigm of the Lambda architecture features key
characteristics (such as robustness, fault tolerance, scalability,
generalization, extensibility, ad-hoc queries, minimal maintenance, and
low-latency reads and updates) to cope with this complexity. The paper at hand
suggest a basic set of strategies to handle the arising challenges regarding
the volatility, heterogeneity, and desired low latency execution by reducing
the overall system timing (scheduling, execution, monitoring, and faults
recovery) as well as possible faults (churn, no answers to executions). The
proposed strategies make use of services such as migration, replication,
MapReduce simulation, and combined processing methods (batch- and
streaming-based). Via these services, a distribution of tasks for the best
balance of computational resources is achieved, while monitoring and management
can be performed asynchronously in the background. %An application of batch and
stream-based methods are proposed to reduce the latency."
pub.1146546975,Stochastic Computing in Beyond Von-Neumann Era: Processing Bit-Streams in Memristive Memory,"Stochastic Computing (SC) is an alternative computing paradigm that promises high robustness to noise and outstanding area- and power-efficiency compared to traditional binary. It also enables the design of fully parallel and scalable computations. Despite its advantage, SC suffers from long latency and high energy consumption compared to conventional binary computing, especially with current CMOS technology. The cost of conversion between binary and stochastic representation takes a significant cost with CMOS circuits. In-Memory Computation (IMC) is introduced to accelerate Big Data applications by removing the data movement between memory and processing units, and by providing massive parallelism. In this work, we explore the efforts in employing IMC for fast and energy-efficient SC system design. We specially focus on memristors as an emerging technology that promises efficient memory and computation beyond CMOS. We discuss the potentials and challenges for realizing efficient SC systems in memory."
pub.1115226882,ShuntFlow,"Streaming processing is an important and growing class of applications for analyzing continuous streams of real time data. Sliding-window aggregations (SWAGs) dominate the computation time in such applications and dictate an unprecedented computation capacity which poses a great challenge to the computing architectures. General-purpose processors cannot efficiently handle SWAGs because of the specific computation patterns. This paper proposes an efficient accelerator architecture for ubiquitous SWAGs, called ShuntFlow. ShuntFlow is a typical type of Kernel Processing Unit (KPU) where ""Kernel"" represent two main categories of SWAG operations widely used in streaming processing. Meanwhile, we propose a shunt rule to enable ShuntFlow to efficiently handle SWAGs with arbitrary parameters. As a case study, we implemented ShuntFlow on an Altera Arria 10 AX115N FPGA board at 150 MHz and compared it to previous approaches. The experimental results show that ShuntFlow provides a tremendous throughput and latency advantage over CPU and GPU implementations on both reduce-like and index-like SWAGs."
pub.1143316655,Adaptive Learning on Fog-Cloud Collaborative Architecture for Stream Data Processing,"Recently, learning from continuously evolving streaming data attracts many researchers, especially when this data is inclined to change trends regularly (i.e. concept drift). Unluckily, conventional mining techniques and algorithms are proved inadequate to solve this problem, in which the model’s performance degrades in stationary data, let alone in the case of data streams. Correspondingly, adjusting the model manually and constantly is ineffective, and with the current growth of the data size, it becomes impractical as well. However, automatic adaptive learning methods and algorithms could be a good solution, but they are seldom exploited in IoT business applications to address this issue. To that end, we aim to tackle the problem of concept drift occurs in time-series streaming data on IoT applications, in general, meteorology prediction, in specific. And taking into account the rapidly growing structure of fog-cloud computing, we attempt to leverage the powerful computation of the cloud layer as well as the closeness to IoT devices of the fog layer to design and implement a cooperative fog-cloud architecture in order to produce a faster and more accurate model. Our main objective is to obtain high performance and low latency, as these are the key requirements for real-time or nearly real-time data processing on IoT applications. The experimental findings have confirmed the study’s hypothesis, where the adaptive model on the suggested cooperative fog-cloud architecture reduces both the error of prediction by 20% and the overall time for training the model by almost 41% compared to the baseline model."
pub.1175933483,Accelerating Network Resource Allocation in LoRaWAN via Distributed Big Data Computing,"LoRaWAN is a Low Power infrastructure for the Internet of Things (IoT) with a centralized architecture where a single node, the network server, handles all data collection and network management decisions. Given the proliferation and widespread adoption of IoT devices, it becomes essential to incorporate Big Data paradigms at the network server to efficiently manage the enormous volumes of data. In this paper, we introduce a distributed and high-performance methodology for resource allocation in dense LoRaWAN networks, addressing the scalability issues that arise when processing large amounts of information from IoT devices, such as radio link quality. Our contributions establish the groundwork for a distributed implementation of the EXPLORA-C allocation strategy, capable of efficiently operating in large-scale networks. We present two approaches for implementing this distributed scheme: the Multi-Thread (MT) scheme and the Fully-Distributed (FD) scheme. Furthermore, we demonstrate the feasibility of this distributed implementation on top of the NebulaStream stream-based end-to-end data management platform. To validate the proposed approach, we exploit our co-simulation framework, EXPLoSIM, where the distributed implementation is fed with data from a simulated LoRaWAN network. This validation shows significant savings in execution time, latency, and scalability. Additionally, we generalize the concept by decomposing a centralized data aggregation scheme into a chain of stream-processing operators, which can be dynamically allocated across device, Edge, and Cloud levels. In the best scenario, our approach improves metrics such as execution time and data reduction by over 90% when compared to its centralized operation."
pub.1175934282,Outlier Resilient Online Multivariate Change Point Detection using Subsequence Divergence Estimation in Sensor Data Streams,"Outliers in multivariate sensor signals affect the performance of online change point detection algorithms. Data pre-processing to eliminate outliers is a standard solution, but it may be challenging for real-time applications. In this work, we propose an unsupervised change detection framework that consists of three stages. The first stage rapidly detects changes in multivariate sensor data streams in real-time using the KL divergence estimator. The idea is to capture the abrupt and gradual variations using two rolling windows distant by a few subsequences. The second stage deals with the sensor stream redundancy issue by making small perturbations to ensure the positive definiteness of the covariance matrix. The third stage uses an optimal ensemble winsorization strategy to make the framework robust against outliers. In the absence of labeled datasets, the paper presents two algorithms to generate synthetic data with abrupt and gradual changes. The proposed framework is compared with a few popular and contemporary change detection methods using a number of synthetic, benchmark, and experimental datasets. Results show our framework outperforms the state-of-the-art methods in almost every aspect. The addition of perturbation to solve the issue of the singular covariance matrix brings more stability to the framework. Extensive experiments demonstrate that the proposed framework maintains impressive performance even in the presence of outliers. The framework’s robustness against outliers, combined with minimal latency time, makes it suitable for real-time decision-making applications dealing with sensor streams."
pub.1119384810,Low-Latency Speaker-Independent Continuous Speech Separation,"Speaker independent continuous speech separation (SI-CSS) is a task of
converting a continuous audio stream, which may contain overlapping voices of
unknown speakers, into a fixed number of continuous signals each of which
contains no overlapping speech segment. A separated, or cleaned, version of
each utterance is generated from one of SI-CSS's output channels
nondeterministically without being split up and distributed to multiple
channels. A typical application scenario is transcribing multi-party
conversations, such as meetings, recorded with microphone arrays. The output
signals can be simply sent to a speech recognition engine because they do not
include speech overlaps. The previous SI-CSS method uses a neural network
trained with permutation invariant training and a data-driven beamformer and
thus requires much processing latency. This paper proposes a low-latency SI-CSS
method whose performance is comparable to that of the previous method in a
microphone array-based meeting transcription task.This is achieved (1) by using
a new speech separation network architecture combined with a double buffering
scheme and (2) by performing enhancement with a set of fixed beamformers
followed by a neural post-filter."
pub.1166295325,RED-SP-CoDel: Random early detection with static priority scheduling and controlled delay AQM in programmable data planes,"Emerging network application paradigms, such as the Tactile Internet, re-emphasize the need for different Quality of Service (QoS) levels. Due to the large packet buffers in the underlying network data plane, Active Queue Management (AQM) is generally required to curtail packet latencies for flows requiring high QoS levels. At the same time, programmable data planes, such as P4, enable packet processing at line-speed, albeit with limited packet processing functionalities. However, the existing AQM mechanisms that support QoS differentiation are too complex to readily run on P4, while the existing AQM mechanisms that run on P4 do generally not support effective QoS differentiation. We address this gap by developing to the best of our knowledge the first AQM mechanism that supports effective QoS differentiation while running on P4. Specifically, we propose SP-CoDel, which combines the well-known Controlled Delay (CoDel) AQM mechanism with Static Priority (SP) scheduling for QoS differentiation. Also, we propose RED-SP-CoDel, which adds a RED AQM component to SP-CoDel so as to make the AQM with priorities essentially parameterless. As a community resource contribution, we substantially extend the existing P4Simulator to the novel fused P4-NS3 Simulator so as to enable the evaluation of packet processing mechanisms through the combined functionalities of P4 device emulation and NS3 packet simulation. Evaluations conducted with the P4 reference switch model in the P4-NS3 Simulator indicate that SP-CoDel and RED-SP-CoDel provide high QoS to high-priority data streams, i.e., significantly reduce latency and packet loss compared to CoDel, while effectively mitigating bufferbloat."
pub.1091641325,DITIR,"The prosperity of mobile social network and location-based services, e.g., Uber, is backing the explosive growth of spatial temporal streams on the Internet. It raises new challenges to the underlying data store system, which is supposed to support extremely high-throughput trajectory insertion and low-latency querying with spatial and temporal constraints. State-of-the-art solutions, e.g., HBase, do not render satisfactory performance, due to the high overhead on index update. In this demonstration, we present DITIR, our new system prototype tailored to efficiently processing temporal and spacial queries over historical data as well as latest updates. Our system provides better performance guarantee, by physically partitioning the incoming data tuples on their arrivals and exploiting a template-based insertion schema, to reach the desired ingestion throughput. Load balancing mechanism is also introduced to DITIR, by using which the system is capable of achieving reliable performance against workload dynamics. Our demonstration shows that DITIR supports over 1 million tuple insertions in a second, when running on a 10-node cluster. It also significantly outperforms HBase by 7 times on ingestion throughput and 5 times faster on query latency."
pub.1063161703,Automatic communication optimizations through memory reuse strategies,"Modern parallel architectures are emerging with sophisticated hardware consisting of hierarchically placed parallel processors and memories. The properties of memories in a system vary wildly, not only quantitatively (size, latency, bandwidth, number of banks) but also qualitatively (scratchpad, cache). Along with the emergence of such architectures comes the need for effectively utilizing the parallel processors and properly managing data movement across memories to improve memory bandwidth and hide data transfer latency. In this paper, we describe some of the high-level optimizations that are targeted at the improvement of memory performance in the R-Stream compiler, a high-level source-to-source automatic parallelizing compiler. We direct our focus in this paper on optimizing communications (data transfers) by improving memory reuse at various levels of an explicit memory hierarchy. This general concept is well-suited to the hardware properties of GPGPUs, which is the architecture that we concentrate on for this paper. We apply our techniques and obtain performance improvement on various stencil kernels including an important iterative stencil kernel in seismic processing applications where the performance is comparable to that of the state-of-the-art implementation of the kernel by a CUDA expert."
pub.1067368174,MillWheel,"MillWheel is a framework for building low-latency data-processing applications that is widely used at Google. Users specify a directed computation graph and application code for individual nodes, and the system manages persistent state and the continuous flow of records, all within the envelope of the framework's fault-tolerance guarantees. This paper describes MillWheel's programming model as well as its implementation. The case study of a continuous anomaly detector in use at Google serves to motivate how many of MillWheel's features are used. MillWheel's programming model provides a notion of logical time, making it simple to write time-based aggregations. MillWheel was designed from the outset with fault tolerance and scalability in mind. In practice, we find that MillWheel's unique combination of scalability, fault tolerance, and a versatile programming model lends itself to a wide variety of problems at Google."
pub.1062210647,SU‐FF‐I‐98: Accelerating B‐Spline Registration Using Graphics Processing Units," Purpose : To develop a highly data‐parallel version of B‐spline registration within the stream‐processing model, suitable for use on cheap stream processors such as graphics processing units (GPUs). Method and Materials: We first develop a grid‐alignment technique and a data structure that greatly improves the computation speed of B‐spline registration on the CPU. The basic idea here is to align the B‐spline grid with the voxel grid, so that the volume is partitioned into tiles of equal size. The use of equal sized tiles is important since it allows the coefficient multipliers used for B‐spline interpolation to be pre‐computed. Then, using this data structure, we develop a data‐parallel version of B‐spline registration suitable for use on a GPU. We have developed a streaming algorithm for B‐spline registration within the Compute Unified Device Architecture (or CUDA), a programming abstraction for general‐purpose computing on GPUs, recently introduced by NVidia. Results : Using both phantom (swine lung) and synthetic images, we have validated and compared the GPU‐based algorithm with the B‐spline registration routines in the Insight Toolkit (ITK version 3.10.1). Our CPU implementation that uses the grid‐alignment technique runs up to 400 times faster than ITK. The GPU‐based algorithm achieves further speedup; it runs about four times faster per iteration than our CPU version, that is, about 1600 times faster than ITK B‐splines. The registration quality achieved by the GPU was found to be excellent, with nearly identical displacement vectors compared to the CPU version. Conclusions : Improvements in the processing speed of deformable registration algorithms are a powerful driving force for clinical applications. Interventional procedures such as image‐guided surgery and image‐guided radiotherapy require very low latencies in imaging and analysis, and improvements in processing speed mean that deformable registration can be used in such settings. "
pub.1155788430,Nereus: A Distributed Stream Band Join System With Adaptive Range Partitioning,"Many real-time applications in consumer electronics rely on stream band join as a fundamental operation. With two streams, the band join operation targets at obtaining the pairs of tuples which are separately included in the two streams and have close values within a user specified range. Range partitioning keeps tuples with close values in a partition. For band join, the cost of employing range partitioning is less join cost than that of employing other partitioning strategies. However, the distribution of real-world data from consumer electronics over the range is skewed, causing severe load imbalance among instances in the distributed system that employs existing static range partitioning. Load migration can alleviate the load imbalance. For range partitioning, the migration has two kinds of objectives. Migrating load to the instance with adjacent partition controls the number of partitions. However, it causes an unacceptable high cost for migrating span multiple instances. While directly migrating a split partition to the lightest instance is low cost. However, it leads to an uncontrollable number of partitions. The system for consumer electronic applications cannot tolerate high migration cost and an uncontrollable number of partitions, which result in high latency and low throughput. In this work, we propose an adaptive range partitioning strategy to ensure a controllable number of partitions and load balancing with low cost. We implement Nereus, a distributed stream band join system. Nereus designs a migration benefit model using queuing theory measure, which integrates the benefits of partition’s change and load balancing. Such a design can obtain the most beneficial migration, which achieves low migration cost and an appropriate number of partitions. We conduct comprehensive experiments using large-scale datasets from real-world applications to evaluate this design. The results show that Nereus improves the throughput by 51% and reduces the processing latency by 99%, compared to existing designs."
pub.1028647548,Automatic communication optimizations through memory reuse strategies,"Modern parallel architectures are emerging with sophisticated hardware consisting of hierarchically placed parallel processors and memories. The properties of memories in a system vary wildly, not only quantitatively (size, latency, bandwidth, number of banks) but also qualitatively (scratchpad, cache). Along with the emergence of such architectures comes the need for effectively utilizing the parallel processors and properly managing data movement across memories to improve memory bandwidth and hide data transfer latency. In this paper, we describe some of the high-level optimizations that are targeted at the improvement of memory performance in the R-Stream compiler, a high-level source-to-source automatic parallelizing compiler. We direct our focus in this paper on optimizing communications (data transfers) by improving memory reuse at various levels of an explicit memory hierarchy. This general concept is well-suited to the hardware properties of GPGPUs, which is the architecture that we concentrate on for this paper. We apply our techniques and obtain performance improvement on various stencil kernels including an important iterative stencil kernel in seismic processing applications where the performance is comparable to that of the state-of-the-art implementation of the kernel by a CUDA expert."
pub.1094153336,Solving the Multi-operator Placement Problem in Large-Scale Operator Networks,"Processing streams of data in an overlay network of operators distributed over a wide-area network is a common idea shared by different applications such as distributed event correlation systems and large-scale sensor networks. In order to utilize network resources efficiently and allow for the parallel deployment of a large number of large-scale operator networks, suitable placement algorithms are vital that place operators on physical nodes. In this paper, we present a distributed placement algorithm that minimizes the bandwidth-delay product of data streams between operators of the network in order to reduce the induced network load. Since the fundamental optimization problem is NP-hard, we propose a heuristic solution. First, we calculate an optimal solution in an intermediate continuous search space, called latency space. Subsequently the continuous solution is mapped to the physical network. Our evaluations show that this algorithm reduces the resulting network load significantly compared to state of the art algorithms and achieves results close to the optimum."
pub.1150162926,Reduced Complexity Matrix Inversions in Slow Time-Varying MIMO Channels,"The intensifying demand for data rate and connectivity has resulted in multi-user multiple-input multiple-output (MU-MIMO) deployments. MU-MIMO allows multiple data streams to transmit concurrently in the same spectrum band. These mutually interfering streams need to be processed at the base station (BS), leading to substantial computational complexity requirements. Linear MIMO detectors/precoders are popular due to their relatively low complexity. However, matrix inversion is a challenging task in linear detectors/precoders. Especially in experimental platforms, software-based inversions are infeasible for a large number of users. This work presents Matrix Inversion on Channel Approximation (MICA), a novel method that aims to reduce the complexity of matrix inversion by exploiting the characteristics of channel correlation in the time domain. In low-mobility scenarios (user speeds less than 20km/h), MICA can reduce the average complexity and processing latency required for computing the inverse of 64 × 12 channel matrices by about 90% compared to a conventional scheme, while maintaining almost the same error rate performance."
pub.1155241876,Approximate Computing-Based Processing of MEA Signals on FPGA,"Microelectrode arrays (MEAs) are essential equipment in neuroscience for studying the nervous system’s behavior and organization. MEAs are arrays of parallel electrodes that work by sensing the extracellular potential of neurons in their proximity. Processing the data streams acquired from MEAs is a computationally intensive task requiring parallelization. It is performed using complex signal processing algorithms and architectural templates. In this paper, we propose using approximate computing-based algorithms on Field Programmable Gate Arrays (FPGAs), which can be very useful in custom implementations for processing neural signals acquired from MEAs. The motivation is to provide better performance gains in the system area, power consumption, and latency associated with real-time processing at the cost of reduced output accuracy within certain bounds. Three types of approximate adders are explored in different configurations to develop the signal processing algorithms. The algorithms are used to build approximate processing systems on FPGA and then compare them with the accurate system. All accurate and approximate systems are tested on real biological signals with the same settings. Results show an enhancement in processing speed of up to 37.6% in some approximate systems without a loss in accuracy. In other approximate systems, the area reduction is up to 14.3%. Other systems show the trade between processing speed, accuracy, and area."
pub.1086126798,The Event Crowd,"Event processing systems involve the processing of high volume and variety data which has inherent uncertainties like incomplete event streams, imprecise event recognition etc. With the emergence of crowdsourcing platforms, the performance of event processing systems can be enhanced by including 'human-in-the-loop' to leverage their cognitive ability. The resulting crowd-sourced event processing can cater to the problem of event uncertainty and veracity by using humans to verify the results. This paper introduces the first hybrid crowd-enabled event processing engine. The paper proposes a list of five event crowd operators that are domain and language independent and can be used by any event processing framework. These operators encapsulate the complexities to deal with crowd workers and allow developers to define an event-crowd hybrid workflow. The operators are: Annotate, Rank, Verify, Rate, and Match. The paper presents a proof of concept of event crowd operators, schedulers, poolers, aggregators in an event processing system. The paper demonstrates the implementation of these operators and simulates the system with various performance metrics. The experimental evaluation shows that throughput of the system was 7.86 events per second with average latency of 7.16 seconds for 100 crowd workers. Finally, the paper concludes with avenues for future research in crowd-enabled event processing."
pub.1095253051,MPFC: Massively Parallel Firewall Circuits,"The process of matching the header fields of network packets against a set of rules is a performance critical task of firewalls. Software-based solutions have no chance to keep pace with the ever-growing data rates in high-speed networks. However, specialized filtering hardware is costly because complex logic is required in order to be able to apply arbitrary rulesets to a packet stream. By adapting the implemented logic to the specific firewall ruleset, FPGAs allow for much more specifically tailored and thus more efficient processing than ruleset-independent circuits in an ASIC. We present MPFC, a method to generate customized firewall circuits in the form of synthesizable VHDL code for FPGA configuration. The highly parallel MPFC circuits achieve a deterministic throughput of one packet per clock cycle, can be operated at high clock frequencies, and provide orders of magnitudes shorter processing latencies than previous work in this direction."
pub.1146973490,An architecture for multi-layer object coding in 2D game streaming using shared data in a multi-user environment,"Through cloud computing, systems and services that offload game logic and rendering to a streaming server that provides content to a hardware-independent client are becoming more common. But there are still challenges on cloud gaming systems to keep low bandwidth usage and latency at acceptable rates. This work proposes a novel technique, called LACES (LAyer Caching gamE Streaming), to separate 2D game content into multiple layers that allows a server to encode and stream them separately, reusing workloads for multiple clients and updating only modified parts of a frame. We measured server workload, especially processing time spent on encoding tasks, stream bitrates and overall cache reuse. Using our methods, we were able to reduce encoding time in over 35% for cases with high number of layers and reduce stream bitrates in up to 90%, by having the client store a local client-side cache and update the layer data with small update packets, with no noticeable user input delay or impact on video resolution and quality. Finally, when allowing the cache generated at server-side to be reused for new users, we reduced encoding times to negligible amounts after the initial runs where the cache is being generated."
pub.1181659746,Scalable AI Solutions for IoT-based Healthcare Systems using Cloud Platforms,"This research offers a solution for AI in IoT-based health care systems that can help large scale implementation of AI-based cloud platforms for real-time data processing and analysis. The IoT sensors with the cloud-enabled environment supported by the CNNs and LSTM networks for health condition monitoring allows real-time management of patient health status. Based on the experimental context, the system was able to prove that the system can actually filter large data stream and work even through the fluctuating network environment. Specifically, the performance targets such as the time taken to process the data, AI output accuracy, the ability of the system to scale and the robustness of the system were assessed. The results demonstrate that the proposed solution has accuracy of up to 96% in health predictions and is easily scalable in terms of the number of IoT devices involved. Network latency was also well controlled, whereby the lowest network latency for networks was measured in 5G networks. Such conclusions prove that the application of cloud-based AI systems can enhance the conditions of the healthcare provisioning, in terms of timely identification of patients with certain health conditions."
pub.1095491757,An efficient VLSI implementation of H.264/AVC intraframe transcoder,"The number of different display terminals increased steadily, from HD TV to mobile phone TV and transcoding has become an indispensable operation in video processing. In the most cases, transcoding has to be done in real time but H.264/AVC intra-frame decoding and encoding contain a set of computation-intensive coding tools forming a loop in which the data are strongly dependant. Parallelization of each function isn't though effortless. In this paper, we present an optimized transcoding chain for AVC intra-frame stream. The transcoding chain is characterized by several operators based on loop iterations and working on $4 \times 4$ luma or $2 \times 2$ chroma blocs. This generates heavy latency. Ours approaches uses loop unrolling and data parallelization. A tradeoff is done between critical path and number of cycles in order to improve global latency. The architecture described in this paper includes a powerful CAVLC coder and decoder, an optimized transform-quantization and a frequency selection function for, respectively, requantization and quick decimation of the high frequency values in a quantified coefficient block. This whole system performs an efficient transcoding operation. Our design, thanks to a high parallelization, can decode then recode a video stream in a 1080p format at 30 frames per second (fps) in real time at the frequency of 47Mhz. This design has been implemented in a Virtex 5 FPGA. Each block is fully described giving the surface occupied and the timing diagram."
pub.1169582325,The new real-time radar-gauge-CML adjustment system pyRADMAN at DWD,"Adjusting weather radar data with ground-based precipitation observations is an established way to overcome radar-specific uncertainties. Most commonly, rain gauge data is used for this task. Commercial microwave links (CMLs) deployed by mobile network operators offer another source of rainfall information that can be used to adjust weather radar data. One of the main advantages of CMLs for this task is the real-time availability of their data with a latency of less than a minute. In addition, their large number, with high densities in particular in urban regions, and the path-averaging nature of their measurements have the potential to improve radar adjustment at short aggregation times. We developed the Python framework pyRADMAN which is capable of merging weather radar with rain gauge and CML data with selectable temporal aggregations from minutes to hours. The path-averaging nature of the CML data is considered when merging with the gridded radar data. Computational efficiency has been taken into consideration in all implementations allowing a full countrywide radar adjustment for Germany, including the required processing of CML rainfall estimates, within 2 minutes with a pure Python implementation. pyRADMAN has now been continuously operating at DWD in real time since August 2023. Currently, real-time data streams from the gridded weather radar composite (based on 17 radar sites), ~1500 rain gauges, and ~5000 CMLs are handled by pyRADMAN, and products consisting of different combinations of sensors are produced for several aggregation times and latencies.  We will show the general concept of pyRADMAN and present results from merging radar data with rain gauge and CML data. Our analysis will consist of selected events and monthly statistics. Results will be shown for aggregation times from 5 to 60 minutes and latencies of production from 5 to 20 minutes (increasing the number of available rain gauges for merging with increasing latency)."
pub.1038468044,VoxNet: Reducing Latency in High Data Rate Applications,"High data-rate sensing is a challenging aspect of WSN research due to the large amounts of data generated by each sensor node. When data is being generated faster than it can be transported over multiple network hops, there is a need to apply on-node, in-network event detection, filtering and other data processing techniques. Although contingent on the specific application, signals in the audible acoustic spectrum must typically be sampled at kHz rates. This makes acoustics a particularly challenging phenomena in the high date rate class.In the context of high data-rate sensing, this chapter describes in detail the deployment of a specific application on a platform for distributed acoustic sensing applications called VoxNet. The application, on-line source localization of animals from their vocalizations is a compelling tool for evolutionary biologists; timely in-field position estimates can enable biologists to augment their observations.A ten-day deployment of VoxNet highlighted several important problems which could not have been predicted in advance, largely related to the instrumentation of the system and end-to-end latency from event detection to position estimate. Using the extensive log data gathered during the deployment, two strategies to improve end-to-end system timeliness were developed. These are Lazy Grouping, a centralized algorithm which performs on-line grouping of event data and facilitates its collection from the network, and an Adaptation policy which allows nodes in the network to individually and dynamically evaluate whether to process data locally based on previous data transfers. Whilst the design and evaluation of these refinements is based on application-specific experiences, the techniques themselves are transferable to a variety of high data rate applications."
pub.1061256360,Differentiating data collection for cloud environment monitoring,"In a growing number of information processing applications, data takes the form of continuous data streams rather than traditional stored databases. Monitoring systems that seek to provide monitoring services in cloud environment must be prepared to deal gracefully with huge data collections without compromising system performance. In this paper, we show that by using a concept of urgent data, our system can shorten the response time for most ‘urgent’ queries while guarantee lower bandwidth consumption. We argue that monitoring data can be treated differently. Some data capture critical system events; the arrival of these data will significantly influence the monitoring reaction speed which is called urgent data. High speed urgent data collections can help system to react in real time when facing fatal errors. A cloud environment in production, MagicCube, is used as a test bed. Extensive experiments over both real world and synthetic traces show that when using urgent data, monitoring system can lower the response latency compared with existing monitoring approaches."
pub.1158322942,Scalable Low-Cost Sorting Network with Weighted Bit-Streams,"Sorting is a fundamental function in many applications from data processing to database systems. For high performance, sorting-hardware based sorting designs are implemented by conventional binary or emerging stochastic computing (SC) approaches. Binary designs are fast and energy-efficient but costly to implement. SC-based designs, on the other hand, are area and power-efficient but slow and energy-hungry. So, the previous studies of the hardware-based sorting further faced scalability issues. In this work, we propose a novel scalable low-cost design for implementing sorting networks. We borrow the concept of SC for the area- and power efficiency but use weighted stochastic bit-streams to address the high latency and energy consumption issue of SC designs. A new lock and swap (LAS) unit is proposed to sort weighted bit-streams. The LAS-based sorting network can determine the result of comparing different input values early and then map the inputs to the corresponding outputs based on shorter weighted bit-streams. Experimental results show that the proposed design approach achieves much better hardware scalability than prior work. Especially, as increasing the number of inputs, the proposed scheme can reduce the energy consumption by about 3.8% - 93% compared to prior binary and SC-based designs."
pub.1150861847,Attentive Decision-making and Dynamic Resetting of Continual Running SRNNs for End-to-End Streaming Keyword Spotting,"Efficient end-to-end processing of continuous and streaming signals is one of the key challenges for Artificial Intelligence (AI) in particular for Edge applications that are energy-constrained. Spiking neural networks are explored to achieve efficient edge AI, employing low-latency, sparse processing, and small network size resulting in low-energy operation. Spiking Recurrent Neural Networks (SRNNs) achieve good performance on sample data at excellent network size and energy. When applied to continual streaming data, like a series of concatenated keyword samples, SRNNs, like traditional RNNs, recognize successive information increasingly poorly as the network dynamics become saturated. SRNNs process concatenated streams of data in three steps: i) Relevant signals have to be localized. ii) Evidence then needs to be integrated to classify the signal, and finally, iii) the neural dynamics must be combined with network state resetting events to remedy network saturation. Here we show how a streaming form of attention can aid SRNNs in localizing events in a continuous stream of signals, where a brain-inspired decision-making circuit then integrates evidence to determine the correct classification. This decision then leads to a delayed network reset, remedying network state saturation. We demonstrate the effectiveness of this approach on streams of concatenated keywords, reporting high accuracy combined with low average network activity as the attention signal effectively gates network activity in the absence of signals. We also show that the dynamic normalization effected by the attention mechanism enables a degree of environmental transfer learning, where the same keywords obtained in different circumstances are still correctly classified. The principles presented here also carry over to similar applications of classical RNNs and thus may be of general interest for continual running applications."
pub.1000764312,Elastic complex event processing,"Complex Event Processing (CEP) systems are designed to process large amount of information by simultaneously evaluating multiple queries over event streams. Two main requirements imposed by the users of the CEP systems are: (1) ability to process high throughput event data and (2) ability to answer queries with very low latency. In order to meet above requirements CEP systems are becoming increasingly distributed. Distribution of queries as well as event streams across multiple nodes facilitates increasing the throughput of CEP systems while simultaneously maintaining low response times. The widespread adoption of cloud computing and the accompanying pay-as-you-go model has added new dimensions to the problem of complex event processing in a distributed system. Nowadays, it is not only important to be able to scale the processing out to a large number of nodes, it is also equally important to be able to scale the processing down, as soon as the load or user requirements decrease. The ability to scale processing up and down along with the load and user requirements is called elasticity. The goal of the thesis described in this paper is to develop a component allowing for elastic scaling of distributed CEP systems in response to variations in the load and contractual obligations regarding the quality of service. To this end, the thesis described in this paper will address following three major topics: (1) multi query optimization, (2) operator placement in distributed environments, and (3) cost efficiency. This paper outlines the state of art for the three aforementioned topics and presents the overall draft of the solution for the problem of the elastic complex event processing."
pub.1118236868,Evaluating Accumulo Performance for a Scalable Cyber Data Processing Pipeline,"Streaming, big data applications face challenges in creating scalable data
flow pipelines, in which multiple data streams must be collected, stored,
queried, and analyzed. These data sources are characterized by their volume (in
terms of dataset size), velocity (in terms of data rates), and variety (in
terms of fields and types). For many applications, distributed NoSQL databases
are effective alternatives to traditional relational database management
systems. This paper considers a cyber situational awareness system that uses
the Apache Accumulo database to provide scalable data warehousing, real-time
data ingest, and responsive querying for human users and analytic algorithms.
We evaluate Accumulo's ingestion scalability as a function of number of client
processes and servers. We also describe a flexible data model with effective
techniques for query planning and query batching to deliver responsive results.
Query performance is evaluated in terms of latency of the client receiving
initial result sets. Accumulo performance is measured on a database of up to 8
nodes using real cyber data."
pub.1120032566,Software-accelerated Service-oriented Router for Edge and Fog Service Enhancement Using Advanced Stream Content Analysis,"Fog services have been gaining momentum due to the demand generated by IoT-based services that require data and services to be provided at the edge of the network. This phenomenon creates several problems for Internet systems, such as security, and data management. Particularly, edge and fog computing addresses these problems and provides new services by solving these issues. To this end, a service-oriented router (SoR) was proposed and extended to provide edge and fog services by performing authorized stream content analysis (ASCA). Packet Capture (Libpcap) library was used to develop the initial implementation of the SoR to perform ASCA using conventional hardware and software platforms. However, Libpcap implementation together with conventional hardware demonstrated limited performance. Consequently, this paper proposes a method using Intel technologies such as the Intel Data Plane Development Kit and Hyperscan to accelerate stream processing and string matching on SoRs. The paper compares both implementations of SoRs and evaluates the performance with regard to packet throughput, CPU utilization, and memory usage. Furthermore, the latency of major functions is evaluated under Libpcap-based and DPDK-based SoR implementations. The results demonstrate an increase of more than 0.8Gbps in bandwidth in a 1Gbps link using less hardware utilization when SoR is implemented using Intel technologies compared to the original Libpcap implementation."
pub.1149507819,Detecting trading trends in streaming financial data using Apache Flink,"Modern financial analytics rely on high-volume streams of event notifications that report live market fluctuations based on supply and demand. Accurately identifying trends or breakout patterns based on the Exponential Moving Average (EMA) in the development of an instrument's price early on is an important challenge, so as to buy while the price is low and sell before a downtrend begins. This paper aims to solve the above challenge with a distributed, event-streaming solution built using Apache Flink. We present and implement a solution that leverages customized window operators to calculate the EMA and find breakout patterns, using event generation parallelism to facilitate the rapid processing of the input stream uses sinks to collect and output results, and scales easily on a distributed Flink cluster. We empirically test our design on metrics specified by the benchmarking platform for the DEBS 2022 Grand Challenge and observe a throughput of 45 batches per second and an average latency of 120 ms."
pub.1169589590,A new low latency and low-cost force-balance accelerograph for earthquake and structural monitoring and for early waring applications.,"GEObit Instruments are proud to announce the market release of a new low-cost and low -latency seismic accelerograph, the GEO-T200, for earthquake monitoring, early waring applications, and structural monitoring. The device mainly consists of two sections, the triaxial sensor sensor and the digitizer. The architecture and the hardware is based on the GEObit GEOtiny platform. The sensing elements are based in a re-designed previous generation GEObit force balance acceleration sensor unit [1], providing very high dynamic range 160+dB, and wide bandwidth, flat response DC to 260Hz. The acceleration range is user configurable and can be set between +/-4g to +/-0.5g but other ranges are also available upon request. The digitizer is based on a 24bit ADC and provides high effective dynamic range 140dB, high sampling rate up to 4000sps, integrates seedlink server and the earthworm chain. The device is based on a locally running open-source components ported on ARM Linux board. It is able to apply local signal processing and trigger detection based on multiple schemes (amplitude, STA/LTA etc.) through open-source components ported from the Earthworm toolchain and transmit pick times over MQTT with ultra-low latency based signaling for trigger event distribution supporting multiple centralized or distributed schemes. It Supports ethernet port and Wi-Fi. Also supports continuous data stream, triggered data stream (level, LTA/LTA, both) or both. The device is housed into a small cylindrical enclosure, aluminum made, IP68 with dimensions 120mm diameter and 143mm height. Three leveling legs are provided along with a central bolt for proper mounting of the device. An bright OLED lcd screen reports the user about the instrument operation and state of health. The SOH stream is also transmitted in real time over TCP.   References: [1]: Design, Modeling, and Evaluation of a Class-A Triaxial Force-Balance Accelerometer of Linear Based Geometry” N. Germenis, G. Dimitrakakis, E. Sokos, and P. Nikolakopoulos Seismol. Res. Lett. 93, 2138–2146, doi: 10.1785/0220210102"
pub.1149360878,Novel Big Data Networking Framework Using Multihoming Optimization for Distributed Stream Computing,"One of the main technologies for big data networking framework is online multihoming optimization that is large-scale dimension table association technology in a distributed environment. It is often used in applications like real-time suggestion and research. Big data is concerned with the quality of large datasets that are distributed. These datasets demand sophisticated network technologies to adequately transmit massive share files. Dimension table association is the process of integrating multihoming stream data with offline stored dimension table data and executing data processing using novel big data frameworks, as described in this study. The current technological options for dimension table connection are assessed first, followed by accompanying optimization technologies and the design route of mainstream distributed engines. The dimension table data query is the one that has been optimized with the greatest performance. Nonetheless, the typical optimization approach is influenced by the dimension, table size, and the design route of the mainstream distributed engine—limits on data flow rate. Second, due to the limitations of existing optimization technologies for the overall consideration of the cluster in a distributed environment, a computing model suited for hybrid computing of offline batch data and real-time streaming data is provided, followed by a single-point reading. Dimension table data, the dimension table associated data technique for distribution and calculation after segmentation, and optimization of the dimension table associated calculation logic adapt to a larger dimension table scale and are no longer restricted to data connections. Since optimizing the query of dimension table, data is employed to reduce the I/O overhead and delay caused by querying dimension table in big data. Finally, both the suggested and standard dimension table association technologies are implemented on the Apache Flink stream computing engine. Through trials, the throughput and latency on data created by Alibaba’s “Double Eleven” are compared, demonstrating the usefulness of dimension table association techniques for Distributed Stream Computing optimization by utilizing multihoming networks."
pub.1173230572,Dynamic heatmap pyramid computation for massive high-parallel spatial streaming in urban environments,"It is necessary to integrate data and information effectively in order to build an integrated digital and refined urban system. A multi-granularity and multi-view urban heat map formed by integrating multi-source urban information flow can assist in immediate decision-making. Managing urban big data in the form of streams requires a stable operating environment that can efficiently handle millions of sensors and devices connected to the Internet of Things (IoT) in a highly concurrent environment. Urban management data contain spatiotemporal multi-dimensional information that presents the complexity of spatiotemporal dynamic associations, further increasing the difficulty of data streaming. To overcome these challenges, we have proposed a spatiotemporal-pyramid (ST-pyramid) model that organizes multidimensional and dynamic data streams logically using a data partition strategy based on geographical grid subdivision. Accordingly, we have proposed a load-balanced heatmap pyramid computation framework that can be used to build streaming processing procedures in a distributed environment. In addition, we implemented an urban traffic heatmap prototype system based on an open-source Flink framework. The experimental results show that the real-time heatmap pyramid construction algorithm proposed in this paper has high throughput, low latency, and flexible scalability and can provide large-scale public services in time in digital earth construction."
pub.1101541038,Design and Simulation of 5G Massive MIMO Kernel Algorithm on SIMD Vector Processor,"In cellular communications, recently Multi In Multi Out (MIMO) and Massive MIMO research is getting attention for the need of high data rates in Long Term Evolution Advanced(LTE-A) and 5G Communications. In MIMO baseband signal processing at physical layer, both the channel estimation and the detection algorithms play a crucial role. In this paper it is discussed the estimation algorithms least square (LS) and minimum mean square error (MMSE) and the channel detection algorithms Zero Forcing (ZF) and MMSE. Currently none of the channel estimation algorithms of LTEA offers twin advantages of low battery consumption and very low latency, which is a key requirement of 5G. It is expected the massive MIMO with 128 or more antennas will be a norm at 5G base stations. To achieve the ultra-low latency, the matrix computations for massive MIMO are the very big bottleneck in realizing the channel estimation and massive MIMO detection algorithms. For the optimization of the 5G Massive MIMO channel estimation and detection algorithms, the prerequisite is massive complex matrix inversion speed. In this paper, a parallel processing based coding scheme is proposed by using Gauss-Jordan elimination kernel algorithm on a single instruction multiple data (SIMD) stream vector processor to realize a complex matrix inverse with optimum speed which is the need of 5G channel estimation and detection."
pub.1000425784,Timely Semantics: A Study of a Stream-Based Ranking System for Entity Relationships,"In recent years, search engines have started presenting semantically relevant entity information together with document search results. Entity ranking systems are used to compute recommendations for related entities that a user might also be interested to explore. Typically, this is done by ranking relationships between entities in a semantic knowledge graph using signals found in a data source as well as type annotations on the nodes and links of the graph. However, the process of producing these rankings can take a substantial amount of time. As a result, entity ranking systems typically lag behind real-world events and present relevant entities with outdated relationships to the search term or even outdated entities that should be replaced with more recent relations or entities.This paper presents a study using a real-world stream-processing based implementation of an entity ranking system, to understand the effect of data timeliness on entity rankings. We describe the system and the data it processes in detail. Using a longitudinal case-study, we demonstrate (i) that low-latency, large-scale entity relationship ranking is feasible using moderate resources and (ii) that stream-based entity ranking improves the freshness of related entities while maintaining relevance."
pub.1093749485,Evaluating Accumulo Performance for a Scalable Cyber Data Processing Pipeline,"Streaming, big data applications face challenges in creating scalable data flow pipelines, in which multiple data streams must be collected, stored, queried, and analyzed. These data sources are characterized by their volume (in terms of dataset size), velocity (in terms of data rates), and variety (in terms of fields and types). For many applications, distributed NoSQL databases are effective alternatives to traditional relational database management systems. This paper considers a cyber situational awareness system that uses the Apache Accumulo database to provide scalable data warehousing, realtime data ingest, and responsive querying for human users and analytic algorithms. We evaluate Accumulo's ingestion scalability as a function of number of client processes and servers. We also describe a flexible data model with effective techniques for query planning and query batching to deliver responsive results. Query performance is evaluated in terms of latency of the client receiving initial result sets. Accumulo performance is measured on a database of up to 8 nodes using real cyber data."
pub.1168787122,"Microservices and serverless functions—lifecycle, performance, and resource utilisation of edge based real-time IoT analytics","Edge Computing harnesses resources close to the data sources to reduce end-to-end latency and allow real-time process automation for verticals such as Smart City, Healthcare and Industry 4.0. Edge resources are limited when compared to traditional Cloud data centres; hence the choice of proper resource management strategies in this context becomes paramount. Microservice and Function as a Service architectures support modular and agile patterns, compared to a monolithic design, through lightweight containerisation, continuous integration/deployment and scaling. The advantages brought about by these technologies may initially seem obvious, but we argue that their usage at the Edge deserves a more in-depth evaluation. By analysing both the software development and deployment lifecycle, along with performance and resource utilisation, this paper explores microservices and two alternative types of serverless functions to build edge real-time IoT analytics. In the experiments comparing these technologies, microservices generally exhibit slightly better end-to-end processing latency and resource utilisation than serverless functions. One of the serverless functions and the microservices excel at handling larger data streams with auto-scaling. Whilst serverless functions natively offer this feature, the choice of container orchestration framework may determine its availability for microservices. The other serverless function, while supporting a simpler lifecycle, is more suitable for low-invocation scenarios and faces challenges with parallel requests and inherent overhead, making it less suitable for real-time processing in demanding IoT settings."
pub.1172575104,"INTEGRATION OF IOT WITH CLOUD, FOG, AND EDGE COMPUTING: A REVIEW","Purpose of review. The paper provides an in-depth exploration of the integration of Internet of Things (IoT) technologies with cloud, fog, and edge computing paradigms, examining the transformative impact on computational architectures. Approach to review. Beginning with an overview of IoT's evolution and its surge in global adoption, the paper emphasizes the increasing importance of integrating cloud, fog, and edge computing to meet the escalating demands for real-time data processing, low-latency communication, and scalable infrastructure in the IoT ecosystem. The survey meticulously dissects each computing paradigm, highlighting the unique characteristics, advantages, and challenges associated with IoT, cloud computing, edge computing, and fog computing. The discussion delves into the individual strengths and limitations of these technologies, addressing issues such as latency, bandwidth consumption, security, and data privacy. Further, the paper explores the synergies between IoT and cloud computing, recognizing cloud computing as a backend solution for processing vast data streams generated by IoT devices. Review results. Challenges related to unreliable data handling and privacy concerns are acknowledged, emphasizing the need for robust security measures and regulatory frameworks. The integration of edge computing with IoT is investigated, showcasing the symbiotic relationship where edge nodes leverage the residual computing capabilities of IoT devices to provide additional services. The challenges associated with the heterogeneity of edge computing systems are highlighted, and the paper presents research on computational offloading as a strategy to minimize latency in mobile edge computing. Fog computing's intermediary role in enhancing bandwidth, reducing latency, and providing scalability for IoT applications is thoroughly examined. Challenges related to security, authentication, and distributed denial of service in fog computing are acknowledged. The paper also explores innovative algorithms addressing resource management challenges in fog-IoT environments. Conclusions. The survey concludes with insights into the collaborative integration of cloud, fog, and edge computing to form a cohesive computational architecture for IoT. The future perspectives section anticipates the role of 6G technology in unlocking the full potential of IoT, emphasizing applications such as telemedicine, smart cities, and enhanced distance learning. Cybersecurity concerns, energy consumption, and standardization challenges are identified as key areas for future research."
pub.1016582249,Memory reuse optimizations in the R-Stream compiler,"We propose a new set of automated techniques to optimize memory reuse in programs with explicitly managed memory. Our techniques are inspired by hand-tuned seismic kernels on GPUs. The solutions we develop reduce the cost of transferring data across multiple memories with different bandwidth, latency and addressability properties. They result in reduction of communication volumes from main memory and faster execution speeds, comparable to hand-tuned implementations, for out-of-place stencils. We discuss various steps of our source-to-source compiler infrastructure and focus on specific optimizations which comprise: flexible generation of different granularities of communications with respect to computations, reduction of redundant transfers, reuse of data across processing elements using a globally addressable local memory and reuse of data within the same processing elements using a local private memory. The models of memory we consider in our techniques support the GPU model with device, shared and register memories. The techniques we derive are generally applicable and their formulation within our compiler can be extended to other types of architectures."
pub.1140193263,Unlimited Vector Extension with Data Streaming Support,"Unlimited vector extension (UVE) is a novel instruction set architecture extension that takes streaming and SIMD processing together into the modern computing scenario. It aims to overcome the shortcomings of state-of-the-art scalable vector extensions by adding data streaming as a way to simultaneously reduce the overheads associated with loop control and memory access indexing, as well as with memory access latency. This is achieved through a new set of instructions that pre-configure the loop memory access patterns. These attain accurate and timely data prefetching on predictable access patterns, such as in multidimensional arrays or in indirect memory access patterns. Each of the configured data streams is associated to a general-purpose vector register, which is then used to interface with the streams. In particular, iterating over a given stream is simply achieved by reading/writing to the corresponding input/output stream, as the data is instantly consumed/produced. To evaluate the proposed UVE, a proof-of-concept gem5 implementation was integrated in an out-of-order processor model, based on the ARM Cortex-A76, thus taking into consideration the typical speculative and out-of-order execution paradigms found in high-performance computing processors. The evaluation was carried out with a set of representative kernels, by assessing the number of executed instructions, its impact on the memory bus and its overall performance. Compared to other state-of-the-art solutions, such as the upcoming ARM Scalable Vector Extension (SVE), the obtained results show that the proposed extension attains average performance speedups over 2.4 x for the same processor configuration, including vector length."
pub.1156189749,CNN-assisted Road Sign Inspection on the Computing Continuum,"Processing rapidly growing data encompasses complex workflows that utilize the Cloud for high-performance computing and the Fog and Edge devices for low-latency communication. For example, autonomous driving applications require inspection, recognition, and classification of road signs for safety inspection assessments, especially on crowded roads. Such applications are among the famous research and industrial exploration topics in computer vision and machine learning. In this work, we design a road sign inspection workflow consisting of 1) encoding and framing tasks of video streams captured by camera sensors embedded in the vehicles, and 2) convolutional neural network (CNN) training and inference models for accurate visual object recognition. We explore a matching theoretic algorithm named CODA [1] to place the workflow on the computing continuum, targeting the workflow processing time, data transfer intensity, and energy consumption as objectives. Evaluation results on a real computing continuum testbed federated among four Cloud, Fog, and Edge providers reveal that CODA achieves 50%-60% lower completion time, 33%-59% lower CO2 emissions, and 19%-45% lower data transfer intensity compared to two stateof-the-art methods."
pub.1164950392,Edge Computing-Enabled Road Condition Monitoring: System Development and Evaluation,"Real-time pavement condition monitoring provides highway agencies with timely
and accurate information that could form the basis of pavement maintenance and
rehabilitation policies. Existing technologies rely heavily on manual data
processing, are expensive and therefore, difficult to scale for frequent,
networklevel pavement condition monitoring. Additionally, these systems require
sending large packets of data to the cloud which requires large storage space,
are computationally expensive to process, and results in high latency. The
current study proposes a solution that capitalizes on the widespread
availability of affordable Micro Electro-Mechanical System (MEMS) sensors, edge
computing and internet connection capabilities of microcontrollers, and
deployable machine learning (ML) models to (a) design an Internet of Things
(IoT)-enabled device that can be mounted on axles of vehicles to stream live
pavement condition data (b) reduce latency through on-device processing and
analytics of pavement condition sensor data before sending to the cloud
servers. In this study, three ML models including Random Forest, LightGBM and
XGBoost were trained to predict International Roughness Index (IRI) at every
0.1-mile segment. XGBoost had the highest accuracy with an RMSE and MAPE of
16.89in/mi and 20.3%, respectively. In terms of the ability to classify the IRI
of pavement segments based on ride quality according to MAP-21 criteria, our
proposed device achieved an average accuracy of 96.76% on I-70EB and 63.15% on
South Providence. Overall, our proposed device demonstrates significant
potential in providing real-time pavement condition data to State Highway
Agencies (SHA) and Department of Transportation (DOTs) with a satisfactory
level of accuracy."
pub.1095007551,A Resource-Aware and Time-Critical IoT Framework,"Internet of Things (IoT) systems produce great amount of data, but usually have insufficient resources to process them in the edge. Several time-critical IoT scenarios have emerged and created a challenge of supporting low latency applications. At the same time cloud computing became a success in delivering computing as a service at affordable price with great scalability and high reliability. We propose an intelligent resource allocation system that optimally selects the important IoT data streams to transfer to the cloud for processing. The optimization runs on utility functions computed by predictor algorithms that forecast future events with some probabilistic confidence based on a dynamically recalculated data model. We investigate ways of reducing specifically the upload bandwidth of IoT video streams and propose techniques to compute the corresponding utility functions. We built a prototype for a smart squash court and simulated multiple courts to measure the efficiency of dynamic allocation of network and cloud resources for event detection during squash games. By continuously adapting to the observed system state and maximizing the expected quality of detection within the resource constraints our system can save up to 70% of the resources compared to the naive solution."
pub.1160032891,SPYRAPTOR: A Stream-based Smart Query System for Real-Time Threat Hunting within Enterprise,"In view of the concealment and destructiveness of insider threats, to detect insider threats is very important for protecting the security of enterprises and organizations. Especially for complex insider threat scenarios, current detection methods still have many limitations. Although log-based cyber threat hunting may be an effective solution, non-trivial efforts of manual query construction hinder its use. In this paper, we propose a stream-based smart query system for real-time threat hunting within enterprise (SPYRAPTOR). Built upon system auditing frameworks, SPYRAPTOR constructs a threat behavior graph based on historical anomalous audit data and information on personnel and asset of the enterprise. An Insider Threat Query Language (ITQL) and an ITQL query synthesis mechanism are provided to synthesize the ITQL query strategy of insider threat scenarios based on the threat behavior graph. An efficient query execution system parses ITQL queries and implement real-time hunting of insider threat scenarios on the stream processing engine. We conduct experiments based on the CERT dataset and the results show that SPYRAPTOR achieves an excellent performance (precision of 0.91, recall of 0.89 and low detection latency) and outperforms the state-of-the-art methods."
pub.1100100330,Kea: A Computation Offloading System for Smartphone Sensor Data,"Nowadays smartphones are equipped with many sensors which applications can continuously invoke to acquire real-time sensor information, such as GPS tracking. Due to the resource-constrained nature of the smartphones, it is often beneficial if the processing of the sensor data is offloaded to a remote resource. However, the decision to offload the computation depends on a multitude of factors such as the hardware capabilities of the phone, the communication energy and latency and the characteristics of the stream computations, e.g., window size, sensor frequency and operational complexity. In this paper we introduce Kea, a profiling-based computation offloading system that automatically decides whether offloading is beneficial for smartphones. The decision making is based on two criteria: the power consumption of the application and the elapsed time for processing the sensor data. Our evaluation results show that unexpected factors such as CPU frequency scaling and the network state also influence the decision-making process. In addition, we show that Kea's profiling overhead is negligible."
pub.1164649197,A data infrastructure for heterogeneous telemetry adaptation: application to Netflow-based cryptojacking detection,"The increasing development of cryptocurrencies has brought cryptojacking as a new security threat in which attackers steal computing resources for cryptomining. The digitization of the supply chain is a potential major target for cryptojacking due to the large number of different infrastructures involved. These different infrastructures provide information sources that can be useful to detect cryptojacking, but with a wide variety of data formats and encodings. This paper describes the semantic data aggregator (SDA), a normalization and aggregation system based on data modelling and low-latency processing of data streams that facilitates the integration of heterogeneous information sources. As a use case, the paper describes a cryptomining detection system (CDS) based on network traffic flows processed by a machine learning engine. The results show how the SDA is leveraged in this use case to obtain aggregated information that improves the performance of the CDS."
pub.1094800452,Mars: Real-time Spatio-temporal Queries on Microblogs,"Mars demonstration exploits the microblogs location information to support a wide variety of important spatiotemporal queries on microblogs. Supported queries include range, nearest-neighbor, and aggregate queries. Mars works under a challenging environment where streams of microblogs are arriving with high arrival rates. Mars distinguishes itself with three novel contributions: (1) Efficient in-memory digestion/expiration techniques that can handle microblogs of high arrival rates up to 64,000 microblog/sec. This also includes highly accurate and efficient hopping-window based aggregation for incoming microblogs keywords. (2) Smart memory optimization and load shedding techniques that adjust in-memory contents based on the expected query load to trade off a significant storage savings with a slight and bounded accuracy loss. (3) Scalable real-time query processing, exploiting Zipf distributed microblogs data for efficient top-$k$ aggregate query processing. In addition, Mars employs a scalable real-time nearest neighbor and range query processing module that employs various pruning techniques so that it serves heavy query workloads in real time. Mars is demonstrated using a stream of real tweets obtained from Twitter firehose with a production query workload obtained from Bing web search. We show that Mars serves incoming queries with an average latency of less than 4 msec and with 99% answer accuracy while saving up to 70%of storage overhead for different query loads."
pub.1151334712,Two-stage Scheduling of Stream Computing for Industrial Cloud-edge Collaboration,"As the Industrial Internet of Things (IIoT) develops, intelligent services applying stream computing, such as industrial robot health management, are requiring higher timeliness of data processing, which may involve scheduling of stream tasks. However, traditional scheduling methods are no longer suitable for the currently widely used cloud-edge collaboration mode, not considering the cloud-edge heterogeneity, and focusing on the scheduling of single tasks instead of the optimization of the total tasks. To improve the performance of the cloud-edge collaboration, this paper establishes a practical model for task scheduling considering respectively cloud-edge environment collaboration models. We propose a novel two-stage scheduling method for IIoT. The algorithm utilizes the idea of maximum flow to divide the task into cloud-edge deployment schemes and find the best partitioning scheme, and then deploy the operator for the edge domain based on the network topology by using dynamic programming. Experimental results show that the proposed method could reduce 7.27% the cloud-edge bandwidth usage compared with the highest greedy algorithm for traffic difference, 24.33% end-to-end latency and 11.18% back-pressure rate compared with SBON."
pub.1095827763,Modeling Tryptic Digestion on the Cell be Processor,"The Cell BE is a heterogeneous multi-core processor offering multiple levels of parallelism. When these are properly leveraged, the Cell BE demonstrates impressive performance acceleration for several high performance computing applications, including exact string matching on streaming data. The present study investigates the suitability of the Cell BE for a string matching problem of relevance to proteomics' the identification of tryptic digest points based on the presence of a short sequence motif. Three string matching algorithms are implemented and evaluated over several proteomic datasets. In its first application to bioinformatics, Parabix, a method of high-throughput XML stream processing which relies on bit transposition and the effective use of single-instruction multiple-data (SIMD) instructions, is applied here with great success. This method performs very well when the protein database is preprocessed in the form of parallel bit streams. Double buffering is also critical to hide the latency of DMA data transfers. Performance results are computed for both the cycle-accurate Cell BE simulator and also using real hardware. This problem is also placed in the larger context of using the Cell BE to achieve hypothesis-driven protein identification."
pub.1158872323,Integrated Photonic Encoder for Terapixel Image Processing,"Modern lens designs are capable of resolving >10 gigapixels, while advances
in camera frame-rate and hyperspectral imaging have made Terapixel/s data
acquisition a real possibility. The main bottlenecks preventing such high
data-rate systems are power consumption and data storage. In this work, we show
that analog photonic encoders could address this challenge, enabling high-speed
image compression using orders-of-magnitude lower power than digital
electronics. Our approach relies on a silicon-photonics front-end to compress
raw image data, foregoing energy-intensive image conditioning and reducing data
storage requirements. The compression scheme uses a passive disordered photonic
structure to perform kernel-type random projections of the raw image data with
minimal power consumption and low latency. A back-end neural network can then
reconstruct the original images with structural similarity exceeding 90%. This
scheme has the potential to process Terapixel/s data streams using less than
100 fJ/pixel, providing a path to ultra-high-resolution data and image
acquisition systems."
pub.1094080248,Statistical Technique for Online Anomaly Detection Using Spark Over Heterogeneous Data from Multi-Source VMware Performance Data,"Anomaly detection refers to the identification of patterns in a dataset that do not conform to expected patterns. Depending on the domain, the non-conformant patterns are assigned various tags, e.g. anomalies, outliers, exceptions, malwares and so forth. Online anomaly detection aims to detect anomalies in data flowing in a streaming fashion. Such stream data is commonplace in today's cloud data centers that house a large array of virtual machines(VM) producing vast amounts of performance data in real-time. Sophisticated detection mechanism will likely entail collation of data from heterogeneous sources with diversified data format and semantics. Therefore, detection of performance anomaly in this context requires a distributed framework with high throughput and low latency. Apache Spark is one such framework that represents the bleeding-edge amongst its contemporaries. In this paper, we have taken up the challenge of anomaly detection in VMware based cloud data centers. We have employed a Chi-square based statistical anomaly detection technique in Spark. We have demonstrated how to take advantage of the high processing power of Spark to perform anomaly detection on heterogeneous data using statistical techniques. Our approach is optimally designed to cope with the heterogeneity of input data streams and the experiments we conducted testify to its efficacy in online anomaly detection."
pub.1153524802,GRAPH-BASED FOG COMPUTING NETWORK MODEL,"IoT networks generate numerous amounts of data that is then transferred to the cloud for processing. Transferring data cleansing and parts of calculations towards these edge-level networks improves system’s, latency, energy consumption, network bandwidth and computational resources utilization, fault tolerance and thus operational costs. On the other hand, these fog nodes are resource-constrained, have extremely distributed and heterogeneous nature, lack horizontal scalability, and, thus, the vanilla SOA approach is not applicable to them. Utilization of Software Defined Network (SDN) with task distribution capabilities advocated in this paper addresses these issues. Suggested framework may utilize various routing and data distribution algorithms allowing to build flexible system most relevant for particular use-case. Advocated architecture was evaluated in agent-based simulation environment and proved its’ feasibility and performance gains compared to conventional event-stream approach."
pub.1121473138,QBS-tree: A Spatial Index with High Update Efficiency for Real-time Processing System,"Along with the popularization of big data applications, real-time data analysis is playing an increasingly important role in analytic data applications. The distributed parallel processing framework is a right choice for real-time big data processing since it provides an effective solution which facilitates processing of massive data stream with low latency. Furthermore, among the real-time applications, location based service is recognized as the most challenging one, especially under expectation on fast response to distribution change and minimal resource consumption. We find that the fundamental reason for this is that the index performance is a stumbling block. Specifically, the original intention of index design is to speed up retrieval speed; in other words, the index is more useful for operations that have more queries than updates. However, in the distributed parallel processing framework, each incoming tuple is both a query request and an update request. In this paper, we propose a new tree index, quad balanced space tree called QBS-tree, to solve this problem, with practical algorithms to support dynamic index items assignment for distributed parallel processing framework. Specifically, make tree index’s tolerability to the index items’ unbalanced distribution in space be more resilient through a configurable balance factor. Also, through the lazy update mechanism, let some updates offset each other, and the upper node’s adjustment covers the underlying node’s. Experiments show that QBS-tree responds to a new tuple 1300 times faster than the traditional tree index."
pub.1107081098,On-sky results and performance of low latency centroiding algorithms for adaptive optics implemented in FPGA,"We present a fast and reconfigurable architecture of centroiding for wavefront sensing implemented in FPGA, with a short latency in the order of microseconds, due to the tight integration design of a CMOS image sensor and the FPGA, and the fast algorithm implementation. Data are processed straight away as they arrive, so there is no need to use external storage in a high capacity memory, that will generate inconvenient delays for real time applications. In this architecture data are processed in a pipeline or parallel fashion when convenient, by means of a stream processing with moving filters algorithm (SCoG), or fixed regions in the image sensor typical from a Shack-Hartmann array using the 1st Fourier Coefficient (1FC). In this work timing and performance of both algorithms are presented, as well as on-sky tests when attached to a 12-inch Meade LX200 telescope aiming for bright stars. The results show a latency no longer than 10 microseconds, for a region of 300 pixels per row in the image sensor. These architecture provide a promising solution as a part of an adaptive optics system design for astronomy and space situational awareness applications. Possible applications are astronomy and space situational awareness. We show some preliminary results for on-sky tests using these two centroiding algorithms."
pub.1091513341,SCARFF: A scalable framework for streaming credit card fraud detection with spark,"The expansion of the electronic commerce, together with an increasing confidence of customers in electronic payments, makes of fraud detection a critical factor. Detecting frauds in (nearly) real time setting demands the design and the implementation of scalable learning techniques able to ingest and analyse massive amounts of streaming data. Recent advances in analytics and the availability of open source solutions for Big Data storage and processing open new perspectives to the fraud detection field. In this paper we present a Scalable Real-time Fraud Finder (SCARFF) which integrates Big Data tools (Kafka, Spark and Cassandra) with a machine learning approach which deals with imbalance, nonstationarity and feedback latency. Experimental results on a massive dataset of real credit card transactions show that this framework is scalable, efficient and accurate over a big stream of transactions."
pub.1116851584,Presto: SQL on Everything,"Presto is an open source distributed query engine that supports much of the SQL analytics workload at Facebook. Presto is designed to be adaptive, flexible, and extensible. It supports a wide variety of use cases with diverse characteristics. These range from user-facing reporting applications with sub-second latency requirements to multi-hour ETL jobs that aggregate or join terabytes of data. Presto’s Connector API allows plugins to provide a high performance I/O interface to dozens of data sources, including Hadoop data warehouses, RDBMSs, NoSQL systems, and stream processing systems. In this paper, we outline a selection of use cases that Presto supports at Facebook. We then describe its architecture and implementation, and call out features and performance optimizations that enable it to support these use cases. Finally, we present performance results that demonstrate the impact of our main design decisions."
pub.1095647614,Embedded Vision System for Real-Time Object Tracking using an Asynchronous Transient Vision Sensor,"This paper presents an embedded vision system for object tracking applications based on a 128times128 pixel CMOS temporal contrast vision sensor. This imager asynchronously responds to relative illumination intensity changes in the visual scene, exhibiting a usable dynamic range of 120 dB and a latency of under 100 mus. The information is encoded in the form of address-event representation (AER) data. An algorithm for object tracking with 1 millisecond timestamp resolution of the AER data stream is presented. As a real-world application example, vehicle tracking for a traffic-monitoring is demonstrated in real time. The potential of the proposed algorithm for people tracking is also shown. Due to the efficient data pre-processing in the imager chip focal plane, the embedded vision system can be implemented using a low-cost, low-power digital signal processor"
pub.1137548554,A USB 3.0 High Speed Digital Readout System with Dynamic Frame Rate Processing for ISFET Lab-on-Chip Platforms,"This paper presents a USB 3.0 based readout platform for real-time ion imaging applications. The front end utilizes an ion-imaging array containing 16 k ISFET pixels to capture ion diffusion at 6100 fps. Operating at 200 MHz, the chip streams ion information at a data rate of 762.94 Mb/s. The back-end readout system employs a FIFO-to-USB bridge (FT601Q) that supports Super Speed (5 Gbps) to perform real-time data streaming and operates in bulk transfer mode to ensure data integrity. Implemented on a Xilinx Virtex UltraScale+ FPGA, the system involves a high-throughput data path through the on-board DDR4 SDRAMs, based on which a ring buffer is designed to provide the 2 GB buffering capacity. The readout system can operate in real time regardless of the USB glitches encountered on the Windows OS when involving data polling. In addition, a simple differencing algorithm with threshold detection is integrated into the back end for dynamic frame rate operation. The proposed readout system performs real-time ion imaging at high speed as well as streams the collected images for visualization within a latency of 25 ms, achieving state-of-the-art performance for high-speed Lab-on-Chip applications."
pub.1132250476,Improving query performance on dynamic graphs,"Querying large models efficiently often imposes high demands on system resources such as memory, processing time, disk access or network latency. The situation becomes more complicated when data are highly interconnected, e.g. in the form of graph structures, and when data sources are heterogeneous, partly coming from dynamic systems and partly stored in databases. These situations are now common in many existing social networking applications and geo-location systems, which require specialized and efficient query algorithms in order to make informed decisions on time. In this paper, we propose an algorithm to improve the memory consumption and time performance of this type of queries by reducing the amount of elements to be processed, focusing only on the information that is relevant to the query but without compromising the accuracy of its results. To this end, the reduced subset of data is selected depending on the type of query and its constituent filters. Three case studies are used to evaluate the performance of our proposal, obtaining significant speedups in all cases."
pub.1169728787,Design and Implementation of a Configurable Fully Compliant DVB-S2 LDPC Encoder for High Data-Rate Downlink Payload,"This work centres on designing and implementing a Low-Density Parity-Check (LDPC) Encoder on a Xilinx Field Programmable Gate Array (FPGA). The encoder will be part of the Digital Video Broadcasting Satellite 2nd generation (DVB-S2) Transmitter Intellectual Property (IP) for a High Data-Rate Downlink Telemetry System in the context of Earth Exploration Satellite Service. The objective is to design an LDPC encoder with three main features. First, the design will prioritize maximizing data processing speed to ensure the efficient transmission and reception of the payload. Second, the encoder will comply with the DVB-S2 Standard for all possible data rates. This will optimize transmission efficiency by adapting to varying channel conditions, utilizing Adaptive Coding and Modulation (ACM) and Variable Coding and Modulation (VCM) techniques. Lastly, the input and output interfaces of the LDPC Encoder will be designed for high reconfigurability, allowing easy adaptation to different operational requirements and facilitating seamless integration into diverse systems. AXI Stream Pipelined Architecture: The LDPC Encoder will utilize an AXI Stream Pipelined architecture. This architecture choice will enhance data transfer efficiency between different functional blocks within the FPGA design, minimizing latency and maximizing overall system performance."
pub.1143579709,Disatra: A Real-Time Distributed Abstract Trajectory Clustering,"Trajectory clustering is regarded as the building block of many applications in trajectory data mining. Nowadays, the ubiquity of positioning devices generates massive trajectory data continuously, which enables various real-time applications, like traffic congestion analysis, accident detection, and traveling group detection. However, these applications rely on an efficient distributed algorithm to cluster trajectory streams in real time. In this paper, we propose a real-time distributed trajectory clustering algorithm to solve this problem. The algorithm starts with a trajectory abstraction process, which compresses trajectories of arbitrary lengths into uniform data structures, named as abstract trajectories, to address the data skewness. Then, a Geohash-based indexing strategy is proposed to partition the abstract trajectories so that clustering can be performed locally with no cross-node interaction. Finally, we design a density-based clustering algorithm on abstract trajectory which achieves a similar accuracy compared with existing clustering methods applied on original trajectories, but with much higher efficiency. Extensive experiments conducted on a real-world dataset show that our approach generates similar clustering results with significantly higher throughput and lower latency, which enable the online clustering."
pub.1118895345,SCARFF: a Scalable Framework for Streaming Credit Card Fraud Detection with Spark,"The expansion of the electronic commerce, together with an increasing
confidence of customers in electronic payments, makes of fraud detection a
critical factor. Detecting frauds in (nearly) real time setting demands the
design and the implementation of scalable learning techniques able to ingest
and analyse massive amounts of streaming data. Recent advances in analytics and
the availability of open source solutions for Big Data storage and processing
open new perspectives to the fraud detection field. In this paper we present a
SCAlable Real-time Fraud Finder (SCARFF) which integrates Big Data tools
(Kafka, Spark and Cassandra) with a machine learning approach which deals with
imbalance, nonstationarity and feedback latency. Experimental results on a
massive dataset of real credit card transactions show that this framework is
scalable, efficient and accurate over a big stream of transactions."
pub.1141458633,Guiding DART to Impact — the FPGA SoC Design of the DRACO Image Processing Pipeline,"The Double Asteroid Redirection Test (DART) is NASA’s first planetary defense mission, scheduled to launch in late 2021 / early 2022. The spacecraft carries a single instrument, the Didymos Reconnaissance and Asteroid Camera for OpNav (DRACO). During the mission, DRACO imagery is used to guide the spacecraft to its target - the smaller asteroid, Dimorphos, of the Didymos binary asteroid system. The impact with Dimorphos will be observed from Earth and will validate key parameters of kinetic deflection mission design. Unlike a traditional instrument, DRACO performs most of the image processing onboard the spacecraft, in the spacecraft avionics. It is implemented within the Field Programmable Gate Array (FPGA) residing on the Single Board Computer (SBC) in the DART Processor Module (PM). As the DART SBC FPGA is a complex System on a Chip (SoC), implementing both the traditional spacecraft data systems functionality as well as the DRACO image processing functionality, the design was partitioned in sub-components for development by multiple teams. This paper focuses on development of the DRACO Image Processing Pipeline (DRIP) component of the FPGA. In particular, it explores the subtleties of interfacing to the instrument data stream, the image processing functionality, data handling, storage, data movement, and the embedded LEON3 processor. Novel aspects of the DRIP implementation include a one-pass Connected Component Analysis (CCA) engine, low-latency packet streaming capability, software-managed Command & Data Handling (C&DH), and a unique triple-buffering architecture for off-chip buffering and image data calibration. Also discussed are testing and verification efforts, which presented unique challenges due to the volume of data and the real-time aspects of the image processing pipeline."
pub.1148285980,Intelligent Live Video Streaming for Object Detection,"These days, sensors and cameras are being deployed on an increasingly large scale. Furthermore, the rapid development of machine learning models for computer vision now presents novel opportunities for the use of artificial intelligence (AI) and Internet of Things (IoT) combinations in various application scenarios. However, challenges remain in supporting low-latency video streaming from distributed mobile IoT devices under dynamic network environments, and overcoming video data quality degradation that results from weather “noise”, which reduces the accuracy of AI-based data analyses such as object detection. In this paper, we propose a live video stream processing system for supporting intelligent services that integrates the following features. First, to cope with dynamic networks and achieve low latency, our approach employs a peer-to-peer (P2P)-based virtual network at the edge and a multi-tiered architecture composed of IoT cameras, edge, and cloud servers. Second, we construct a flexible messaging system for video analysis built upon SINETStream, which is a messaging system that adopts a topic-based pub/sub model. Third, we implement a framework that can remove weather-related (rain, snow, and fog) noise by applying weather classification and adaptive noise removal models that improve the accuracy of video analysis from data collected outdoors. The latency, throughput, and image quality benchmark experiments conducted to validate the feasibility of our proposed system showed that the process resulted in image quality improvements of approximately 30% (on average)."
pub.1111649148,Communication Model for Parallel Iterative Stream Processing,"Stream processing systems are used for a plethora of low-latency applications that deal with high volumes and varieties of data. In general, streaming applications are formulated as a fixed number of inter-connected operators, where the operator graph indicates the sequence of computations that apply to the streaming data in motion. Typically an iterative operation in a streaming application is done by embedding the iteration in a single operator (which excludes multiple operator instances and hence not scalable) or by using a sequence of operators, one for each iteration (which requires knowledge of the number of iterations a priori). However, the number of iterations of some applications such as k-means clustering cannot be determined at the creation of the application as it depends on the properties of the data received at runtime and the convergence criterion. Hence only a limited number of iterative computations can be executed using this approach. In this paper, we propose a communication model to support intra-operator communication so that iterative computations, including those with an arbitrary number of iterations, can be efficiently executed in streaming applications. We show that the proposed model can support different iterative algorithms that have complex communication patterns. Finally, through evaluating a number of parallel iterative algorithms using large-scale datasets, we demonstrate the scalability and performance of our proposed communication model and compare it to the existing approaches used for constructing iterative streaming applications."
pub.1007052876,Pattern-driven prefetching for multimedia applications on embedded processors,"Multimedia applications in general and video processing, such as the MPEG4 Visual stream decoders, in particular are increasingly popular and important workloads for future embedded systems. Due to the high computational requirements, the need for low power, high performance embedded processors for multimedia applications is growing very fast. This paper proposes a new data prefetch mechanism called pattern-driven prefetching. PDP inspects the sequence of data cache misses and detects recurring patterns within that sequence. The patterns that are observed are based on the notions of the inter-miss stride (memory address stride between two misses) and the inter-miss interval (number of cycles between two misses). According to the patterns being detected, PDP initiates prefetch actions to anticipate future accesses and hide memory access latencies. PDP includes a simple yet effective stop criterion to avoid cache pollution and to reduce the number of additional memory accesses. The additional hardware needed for PDP is very limited making it an effective prefetch mechanism for embedded systems. In our experimental setup, we use cycle-level power/performance simulations of the MPEG4 Visual stream decoders from the MoMuSys reference software with various video streams. Our results show that PDP increases performance by as much as 45%, 24% and 10% for 2KB, 4KB and 8KB data caches, respectively, while the increase in external memory accesses remains under 0.6%. In conjunction with these performance increases, system-level (on-chip plus off-chip) energy reductions of 20%, 11.5% and 8% are obtained for 2KB, 4KB and 8KB data caches, respectively. In addition, we report significant speedups (up to 160%) for various other multimedia applications. Finally, we also show that PDP outperforms stream buffers."
pub.1032389838,Mobile QoS management using complex event processing,"This paper describes a use case and a corresponding prototype of a system that provides end-to-end Quality-of-Service (QoS) management for mobile broadband telecommunications service operations using measurements collected from end-user devices. Traditionally mobile telecommunications systems manage performance by monitoring network elements. With a device-based approach the main source of monitored information are end-user devices, where users run their applications and where they directly experience the effects of variations in performance in the sub-systems that comprise the mobile application delivery chain: data center, network and device. In this solution, user devices measure and report transaction latencies combined with location data and bandwidth capacity usage. These measurement event streams are collected and analyzed by a CEP application that provides near real-time insight on the state of the network. It identifies problem conditions such as congested base stations and high latency devices, detects opportunities to correct problems and initiates actions when conditions apply, specifically involving offloading of traffic to WiFi networks. Since QoS management in mobile networks is highly dependent on the dynamic location and speed of devices, this CEP application heavily leverages geospatial functions in the selected CEP platform."
pub.1095009068,CEPaaS: Complex Event Processing as a Service,"Complex Event Processing (CEP) is a technology for performing continuous operations on fast and distributed streams of data. By using CEP, companies can obtain real-time insights, create competitive advantage, and, ultimately unlock the potential of Big Data. Nevertheless, despite this recent surge of interest, the CEP market is still dominated by solutions that are costly and inflexible or too low-level and hard to operate. To overcome these adoption barriers, this research proposes the creation of a CEP as a Service (CEPaaS) system to provide CEP functionalities to users together with the advantages of the Software as a Service (SaaS) model, such as no up-front investment and low maintenance cost. To ensure the success of such a system, however, many complex requirements must be satisfied, such as low latency processing, fault tolerance, and query execution isolation. To satisfy these requirements, this paper also presents an architecture and implementation for this CEPaaS system based on three main pillars: multi-cloud architecture, container management systems, and extensible multi-tenant design. Experimental results demonstrate that the proposed system achieves the goal of offering CEP functionalities as a scalable and fault-tolerant service."
pub.1132240234,Time and Cost Efficient Cloud Resource Allocation for Real-Time Data-Intensive Smart Systems,"Cloud computing is the de facto platform for deploying resource- and data-intensive real-time applications due to the collaboration of large scale resources operating in cross-administrative domains. For example, real-time systems are generated by smart devices (e.g., sensors in smart homes that monitor surroundings in real-time, security cameras that produce video streams in real-time, cloud gaming, social media streams, etc.). Such low-end devices form a microgrid which has low computational and storage capacity and hence offload data unto the cloud for processing. Cloud computing still lacks mature time-oriented scheduling and resource allocation strategies which thoroughly deliberate stringent QoS. Traditional approaches are sufficient only when applications have real-time and data constraints, and cloud storage resources are located with computational resources where the data are locally available for task execution. Such approaches mainly focus on resource provision and latency, and are prone to missing deadlines during tasks execution due to the urgency of the tasks and limited user budget constraints. The timing and data requirements exacerbate the efficient task scheduling and resource allocation problems. To cope with the aforementioned gaps, we propose a time- and cost-efficient resource allocation strategy for smart systems that periodically offload computational and data-intensive load to the cloud. The proposed strategy minimizes the data files transfer overhead to computing resources by selecting appropriate pairs of computing and storage resources. The celebrated results show the effectiveness of the proposed technique in terms of resource selection and tasks processing within time and budget constraints when compared with the other counterparts."
pub.1135691282,Poster: Data-Aware Edge Sampling for Aggregate Query Approximation,"Data stream processing is an increasingly important topic due to the prevalence of smart devices and the demand for realtime analytics. One estimate suggests that we should expect nine smart-devices per person by the year 2025 [1]. These devices generate data which might include sensor readings from a smart home, event or system logs on a device, or video feeds from surveillance cameras. As the number of devices increases, the cost of streaming the device data to the cloud over the wide-area network (WAN) will also increase substantially. Transferring and querying this data efficiently has become the focus of much academic research [2]–[5]. Edge computation affords us the opportunity to address this problem by utilizing resources close to the devices. Edge resources have many different use cases, including minimizing end-to-end latency or maximizing throughput [6], [7]. We restrict our focus to minimizing the required WAN bandwidth, which is an effort to address the increase in data volume."
pub.1149116338,Remote Health Monitoring using IoT and Edge Computing,"The Healthcare sector has significantly developed a lot. All Nations started giving more importance to the medical field with the appearance of the new Covid19. But, some countries have been struggling with poor infrastructure in the form of insufficient medical equipment and a shortage of manpower in the medical stream. In such a situation, Remote health monitoring will lighten the burden and ease the process. Currently, smart healthcare is a combination of IoT and cloud architecture. The IoT sensors keep track of patients' health and provide clinically relevant data, which can be used for further processing. If we send all of the raw data generated by the sensors to the cloud for bulk data processing and analysis to make real-time decisions. It would impose several risks such as latency issues, bandwidth congestion, network reliability, high storage cost, and security-related issues which can negatively impact the healthcare industry on whole. To overcome the abovementioned issues, we offer “Edge Computing, A new emerging technology that allows data processing to be done closer to the data generating device”. Our main aim is to strengthen the existing system. In this paper, the proposed system will continuously collect three main vital parameters from patients in real-time and process the collected data both on the cloud and edge architecture to compare and determine which technology is best suited for time-sensitive applications."
pub.1119379325,Latency Analysis for Sequential Detection in Low-Complexity Binary Radio Systems,"We consider the problem of making a quick decision in favor of one of two
possible physical signal models while the numerical measurements are acquired
by sensing devices featuring minimal digitization complexity. Therefore, the
digital data streams available for statistical processing are binary and
exhibit temporal and spatial dependencies. To handle the intractable
multivariate binary data model, we first consider sequential tests for
exponential family distributions. Within this generic probabilistic framework,
we identify adaptive approximations for the log-likelihood ratio and the
Kullback-Leibler divergence. The results allow designing sequential detectors
for binary radio systems and analyzing their average run-time along classical
arguments of Wald. In particular, the derived tests exploit the spatio-temporal
correlation structure of the analog sensor signals engraved into the binary
measurements. As an application, we consider the specification of binary
sensing architectures for cognitive radio and GNSS spectrum monitoring where
our results characterize the sequential detection latency as a function of the
temporal oversampling and the number of antennas. Finally, we evaluate the
efficiency of the proposed algorithms and illustrate the accuracy of our
analysis via Monte-Carlo simulations."
pub.1121628122,Latency Analysis for Sequential Detection in Low-Complexity Binary Radio Systems,"We consider the problem of making a quick decision in favor of one of two possible physical signal models while the numerical measurements are acquired by sensing devices featuring minimal digitization complexity. Therefore, the digital data streams available for statistical processing are binary and exhibit temporal and spatial dependencies. To handle the intractable multivariate binary data model, we first consider sequential tests for exponential family distributions. Within this generic probabilistic framework, we identify adaptive approximations for the log-likelihood ratio and the Kullback-Leibler divergence. The results allow designing sequential detectors for binary radio systems and analyzing their average run-time along classical arguments of Wald. In particular, the derived tests exploit the spatio-temporal correlation structure of the analog sensor signals engraved into the binary measurements. As an application, we consider the specification of binary sensing architectures for cognitive radio and GNSS spectrum monitoring where our results characterize the sequential detection latency as a function of the temporal oversampling and the number of antennas. Finally, we evaluate the efficiency of the proposed algorithms and illustrate the accuracy of our analysis via Monte-Carlo simulations."
pub.1085612855,Efficient and Versatile FPGA Acceleration of Support Counting for Stream Mining of Sequences and Frequent Itemsets,"
                    Stream processing has become extremely popular for analyzing huge volumes of data for a variety of applications, including IoT, social networks, retail, and software logs analysis. Streams of data are produced continuously and are mined to extract patterns characterizing the data. A class of data mining algorithm, called
                    generate-and-test
                    , produces a set of candidate patterns that are then evaluated over data. The main challenges of these algorithms are to achieve high throughput, low latency, and reduced power consumption. In this article, we present a novel power-efficient, fast, and versatile hardware architecture whose objective is to monitor a set of target patterns to maintain their frequency over a stream of data. This accelerator can be used to accelerate data-mining algorithms, including itemsets and sequences mining.
                  
                  
                    The massive fine-grain reconfiguration capability of field-programmable gate array (FPGA) technologies is ideal to implement the high number of pattern-detection units needed for these intensive data-mining applications. We have thus designed and implemented an IP that features high-density FPGA occupation and high working frequency. We provide detailed description of the IP internal micro-architecture and its actual implementation and optimization for the targeted FPGA resources. We validate our architecture by developing a co-designed implementation of the Apriori Frequent Itemset Mining (FIM) algorithm, and perform numerous experiments against existing hardware and software solutions. We demonstrate that FIM hardware acceleration is particularly efficient for large and low-density datasets (i.e.,
                    long-tailed
                    datasets). Our IP reaches a data throughput of 250 million items/s and monitors up to 11.6k patterns simultaneously, on a prototyping board that overall consumes 24W in the worst case. Furthermore, our hardware accelerator remains generic and can be integrated to other
                    generate and test
                    algorithms.
                  "
pub.1173611049,"Research, Applications and Prospects of Event-Based Pedestrian Detection: A Survey","Event-based cameras, inspired by the biological retina, have evolved into
cutting-edge sensors distinguished by their minimal power requirements,
negligible latency, superior temporal resolution, and expansive dynamic range.
At present, cameras used for pedestrian detection are mainly frame-based
imaging sensors, which have suffered from lethargic response times and hefty
data redundancy. In contrast, event-based cameras address these limitations by
eschewing extraneous data transmissions and obviating motion blur in high-speed
imaging scenarios. On pedestrian detection via event-based cameras, this paper
offers an exhaustive review of research and applications particularly in the
autonomous driving context. Through methodically scrutinizing relevant
literature, the paper outlines the foundational principles, developmental
trajectory, and the comparative merits and demerits of eventbased detection
relative to traditional frame-based methodologies. This review conducts
thorough analyses of various event stream inputs and their corresponding
network models to evaluate their applicability across diverse operational
environments. It also delves into pivotal elements such as crucial datasets and
data acquisition techniques essential for advancing this technology, as well as
advanced algorithms for processing event stream data. Culminating with a
synthesis of the extant landscape, the review accentuates the unique advantages
and persistent challenges inherent in event-based pedestrian detection,
offering a prognostic view on potential future developments in this
fast-progressing field."
pub.1127011969,Towards a Cascading Reasoning Framework to Support Responsive Ambient-Intelligent Healthcare Interventions,"In hospitals and smart nursing homes, ambient-intelligent care rooms are equipped with many sensors. They can monitor environmental and body parameters, and detect wearable devices of patients and nurses. Hence, they continuously produce data streams. This offers the opportunity to collect, integrate and interpret this data in a context-aware manner, with a focus on reactivity and autonomy. However, doing this in real-time on huge data streams is a challenging task. In this context, cascading reasoning is an emerging research approach that exploits the trade-off between reasoning complexity and data velocity by constructing a processing hierarchy of reasoners. Therefore, a cascading reasoning framework is proposed in this paper. A generic architecture is presented allowing to create a pipeline of reasoning components hosted locally, in the edge of the network, and in the cloud. The architecture is implemented on a pervasive health use case, where medically diagnosed patients are constantly monitored, and alarming situations can be detected and reacted upon in a context-aware manner. A performance evaluation shows that the total system latency is mostly lower than 5 seconds, allowing for responsive intervention by a nurse in alarming situations. Using the evaluation results, the benefits of cascading reasoning for healthcare are analyzed."
pub.1104195437,Bubble execution,"
                    Enabling interactive data exploration at cloud scale requires minimizing end-to-end query execution latency, while guaranteeing fault tolerance, and query execution under resource-constraints. Typically, such a query execution involves orchestrating the execution of hundreds or thousands of related tasks on cloud scale clusters. Without any resource constraints, all query tasks can be scheduled to execute simultaneously (gang scheduling) while connected tasks stream data between them. When the data size referenced by a query increases, gang scheduling may be resource-wasteful or un-satisfiable with a limited, per-query resource budget. This paper introduces B
                    ubble
                    E
                    xecution
                    , a new query processing framework for interactive workloads at cloud scale, that balances cost-based query optimization, fault tolerance, optimal resource management, and execution orchestration. Bubble execution involves dividing a query execution graph into a collection of query sub-graphs (bubbles), and scheduling them within a per-query resource budget. The query operators (tasks) inside a bubble stream data between them while fault tolerance is handled by persisting temporary results at bubble boundaries. Our implementation enhances our JetScope service, for interactive workloads, deployed in production clusters at Microsoft. Experiments with TPC-H queries show that bubble execution can reduce resource usage significantly in the presence of failures while maintaining performance competitive with gang execution.
                  "
pub.1121477957,A Study on Big Data Processing Frameworks: Spark and Storm,"Today’s internet world impose a trade-off between Peta-byte to Exa-byte being created in digital computer world attributable enormous volume of unstructured datasets being generating from diverse social sites, IOT, Google, Twitter, Yahoo, monitoring surroundings through sensors, etc., is big data (BD). Because second to second doubles the datasets volume size but the shortage of smooth dynamic processing, analysis and scalability techniques. Because the recent high-speed decade we applied only extant methods and common tools about the gigabyte data process and perform computations on whole world huge data. Apache open free source Hadoop is the latest BD weapon can process zetta-byte dimensions of databases by its most developed and popular components as HDFS and map reduce (MR), to get done excellent storage features magnificent and reliable processing on zetta-byte of datasets. MR likes more famous software, popular framework for handling BD existing issues with full parallel, highly distributed, and most scalable manner. Despite, Hadoop, map and reduces tasks have more limitations like poor allocating custom resources, stream way processing, shortage of latency, the deficit of efficient performance, imperfection of optimization, the real-time trend of computations and diverse logical elucidation. We significant most modern progressive features computing procedures. This examination paper shows Apache fastest spark tool, world latest and fastest tool is apache storm has efficient frameworks to conquer those limitations."
pub.1061541849,Reactive Resource Provisioning Heuristics for Dynamic Dataflows on Cloud Infrastructure,"The need for low latency analysis over high-velocity data streams motivates the need for distributed continuous dataflow systems. Contemporary stream processing systems use simple techniques to scale on elastic cloud resources to handle variable data rates. However, application QoS is also impacted by variability in resource performance exhibited by clouds and hence necessitates autonomic methods of provisioning elastic resources to support such applications on cloud infrastructure. We develop the concept of “dynamic dataflows” which utilize alternate tasks as additional control over the dataflow's cost and QoS. Further, we formalize an optimization problem to represent deployment and runtime resource provisioning that allows us to balance the application's QoS, value, and the resource cost. We propose two greedy heuristics, centralized and sharded, based on the variable-sized bin packing algorithm and compare against a Genetic Algorithm (GA) based heuristic that gives a near-optimal solution. A large-scale simulation study, using the linear road benchmark and VM performance traces from the AWS public cloud, shows that while GA-based heuristic provides a better quality schedule, the greedy heuristics are more practical, and can intelligently utilize cloud elasticity to mitigate the effect of variability, both in input data rates and cloud resource performance, to meet the QoS of fast data applications."
pub.1127308005,Real Time Anomaly Detection Techniques Using PySpark Frame Work,"The identification of anomaly in a network is a process of observing keenly the minute behavioral changes from the usual pattern followed. These are often referred with different names malware, exceptions, and anomaly or as outlier according to the dominion of the application.  Though many works have emerged for the detection of the outlier, the identification of the abnormality in the multiple source data stream structure is still under research. To identify the abnormalities in the cloud data center that is encompassed with the multiple-source VMWare, by observing the behavioral changes in the load of the CPU, utilization of the memory etc. consistently the paper has developed a real time identification process. The procedure followed utilizes the PySpark to compute the batches of data and make predictions, with minimized delay. Further a flat-increment based clustering is used to frame the normal attributes in the PySpark Structure. The latencies in  computing the tuple while clustering and predicting, was compared for PySpark, Storm and other dispersed structure that were used in processing the batches of data and was experimentally found that the processing time of tuple in a PySpark was much lesser compared to the other methods."
pub.1148032361,Cloudprofiler: TSC-based inter-node profiling and high-throughput data ingestion for cloud streaming workloads,"To conduct real-time analytics computations, big data stream processing
engines are required to process unbounded data streams at millions of events
per second. However, current streaming engines exhibit low throughput and high
tuple processing latency. Performance engineering is complicated by the fact
that streaming engines constitute complex distributed systems consisting of
multiple nodes in the cloud. A profiling technique is required that is capable
of measuring time durations at high accuracy across nodes. Standard clock
synchronization techniques such as the network time protocol (NTP) are limited
to millisecond accuracy, and hence cannot be used.
  We propose a profiling technique that relates the time-stamp counters (TSCs)
of nodes to measure the duration of events in a streaming framework. The
precision of the TSC relation determines the accuracy of the measured duration.
The TSC relation is conducted in quiescent periods of the network to achieve
accuracy in the tens of microseconds. We propose a throughput-controlled data
generator to reliably determine the sustainable throughput of a streaming
engine. To facilitate high-throughput data ingestion, we propose a concurrent
object factory that moves the deserialization overhead of incoming data tuples
off the critical path of the streaming framework. The evaluation of the
proposed techniques within the Apache Storm streaming framework on the Google
Compute Engine public cloud shows that data ingestion increases from $700$
$\text{k}$ to $4.68$ $\text{M}$ tuples per second, and that time durations can
be profiled at a measurement accuracy of $92$ $\mu\text{s}$, which is three
orders of magnitude higher than the accuracy of NTP, and one order of magnitude
higher than prior work."
pub.1094608904,Combining Offsets with Precedence Constraints to Improve Temporal Analysis of Cyclic Real-Time Streaming Applications,"Stream processing applications executed on multiprocessor systems usually contain cyclic data dependencies due to the presence of bounded FIFO buffers and feedback loops, as well as cyclic resource dependencies due to the usage of shared processors. In recent works it has been shown that temporal analysis of such applications can be performed by iterative fixed-point algorithms that combine dataflow and response time analysis techniques. However, these algorithms consider resource dependencies based on the assumption that tasks on shared processors are enabled simultaneously, resulting in a significant overestimation of interference between such tasks. This paper extends these approaches by integrating an explicit consideration of precedence constraints with a notion of offsets between tasks on shared processors, leading to a significant improvement of temporal analysis results for cyclic stream processing applications. Moreover, the addition of an iterative buffer sizing enables an improvement of temporal analysis results for acyclic applications as well. The performance of the presented approach is evaluated in a case study using a WLAN transceiver application. It is shown that 56% higher throughput guarantees and 52% smaller end-to-end latencies can be determined compared to state-of-the-art."
pub.1167264004,Novel Cyber Incident Management System for 5G-based Critical Infrastructures,"Modern critical infrastructure with diverse systems requires a security information and event management (SIEM) system for unified monitoring against cyber threats. This system collects log data, performs real-time analysis, flags threats, triggers alerts, and advises response strategies. Enhanced by AI, Internet of Things, and cloud technologies, modern SIEM systems have significantly improved and optimized threat detection. This research examines the functionality, basic operation, and comparative capabilities of current SIEM systems. In addition, a universal event correlation and cybersecurity incident management system was designed and studied specifically for 5G networks. Hybrid security data storage models were also developed to ensure fast search, scale with data volume, and interface with external storage. The research also formulated models for distributed data bus operation, which enables fast processing of large data streams with minimal latency and high resilience. The proposed system addresses key cybersecurity challenges and meets global standards for establishing cyber incident management systems in 5G-based critical infrastructure."
pub.1094025833,Online Association Rule Mining over Fast Data,"To extract useful and actionable information in real-time, the information technology (IT) world is coping with big data problems today. In this paper, we present implementation details and performance results of ReCEPtor, our system for ""online"" Association Rule Mining (ARM) over big and fast data streams. Specifically, we added Apriori and two different FP-Growth algorithms inside Esper Complex Event Processing (CEP) engine and compared their performances using LastFM social music site data. Our most important findings show that online ARM can generate (1) more unique rules, (2) with higher throughput, and (3) much sooner (lower latency) than offline rule mining. In addition, we have found many interesting and realistic musical preference rules such as ""George HarrisonàBeatles"". We demonstrate a sustained rate of ~15K rows/sec per core. We hope that our findings can shed light on the design and implementation of other fast data analytics systems in the future."
pub.1147192461,Synthetic Aperture Radar Image Formation and Processing on an MPSoC,"Satellite remote sensing acquisitions are usually processed after downlink to a ground station. The satellite travel time to the ground station adds to the total latency, increasing the time until a user can obtain the processing results. Performing the processing and information extraction onboard of the satellite can significantly reduce this time. In this study, synthetic aperture radar (SAR) image formation as well as ship detection and extreme weather detection were implemented in a multiprocessor system on a chip (MPSoC). Processing steps with high computational complexity were ported to run on the programmable logic (PL), achieving significant speed-up by implementing a high degree of parallelization and pipelining as well as efficient memory accesses. Steps with lower complexity run on the processing system (PS), allowing for higher flexibility and reducing the need for resources in the PL. The achieved processing times for an area covering 375 km2 were approximately 4 s for image formation, 16 s for ship detection, and 31 s for extreme weather detection. These developments combined with new downlink concepts for low-rate information data streams show that the provision of satellite remote sensing results to end users in less than 5 min after acquisition is possible using an adequately equipped satellite."
pub.1161706785,"Generic Architecture for Multisource Physiological Signal Acquisition, Processing and Classification Based on Microservices","The use of IoT devices is increasing and their integration into healthcare is growing. Therefore, there is a need to develop microservice-oriented hardware-software architectures that integrate all the stages from the acquisition of physiological signals to their processing and classification. In addition, the integration of physiological signals from different sources is a must in order to increase the knowledge of the monitored person’s condition. In this context, the focus of this work has been to identify all the necessary workflow phases in this type of architecture, focusing mainly on scalability, replication and redundancy of the different services. This work proposes an architecture generic in terms of the number of sensors to be included and their acquisition requirements (sampling frequency and latency). We have chosen to include network protocols such as the Laboratory Stream Layer for data synchronisation and streaming. To this end, infrastructure as a service and as a machine have been included."
pub.1181738668,"Saccade onset, not fixation onset, best explains early responses across the human visual cortex during naturalistic vision","Visual processing has traditionally been investigated using static viewing paradigms, where participants are presented with streams of randomized stimuli. Observations from such experiments have been generalized to naturalistic vision, which is based on active sampling via eye movements. In studies of naturalistic vision, visual processing stages are thought to be initiated at the onset of fixations, equivalent to a stimulus onset. Here we test whether findings from static visual paradigms translate to active, naturalistic vision. Utilizing head-stabilized magnetoencephalography (MEG) and eye tracking data of 5 participants who freely explored thousands of natural images, we show that saccade onset, not fixation onset, explains most variance in latency and amplitude of the early sensory component M100. Source-projected MEG topographies of image and saccade onset were anticorrelated, demonstrating neural dynamics that share similar topographies but produce oppositely oriented fields. Our findings challenge the prevailing approach for studying natural vision and highlight the role of internally generated signals in the dynamics of sensory processing."
pub.1095314922,Two Parameter Workload Characterization for Improved Dataflow Analysis Accuracy,"Real-time stream processing applications, such as radios, can often be modeled intuitively with dataflow models. Given the Worst-Case Execution Times (WCETs) of the tasks, which characterizes workload with one parameter, dataflow analysis techniques have been used to compute the minimum throughput and maximum latency of these applications. However, a large difference between the WCETs of the tasks and their average execution times can result in a large difference between the computed worst-case throughput and the actual obtained throughput. To reduce the difference between the worst-case throughput, determined by analysis, and the actual obtained throughput, we introduce in this paper a two parameter $(\sigma, \rho)$ workload characterization of the tasks to improve the accuracy of dataflow analysis. The $(\sigma, \rho)$ workload characterization captures information on the maximum cumulative execution time of consecutive executions of a task and can therefore be seen as a generalization of the WCET characterization. We show how the $(\sigma, \rho)$ workload characterization can be used in combination with several types of dataflow graphs and how it can be used to improve the temporal analysis results of real-time stream processing applications. We illustrate this for a DVB-T radio application, a car-radio application and a data-dependent MP3 playback application."
pub.1148602373,KafkaDirect: Zero-copy Data Access for Apache Kafka over RDMA Networks,"Apache Kafka is an open-source distributed publish-subscribe system, which is widely used in data centers for messaging between applications, log aggregation, and stream processing. The existing Kafka implementation uses TCP/IP for communication, which has various inefficiencies such as a high message dispatch cost due to OS involvement and excessive memory copies. Recently, the availability of cost-effective RDMA-capable network controllers within data centers and cloud infrastructures have encouraged many modern applications to adopt RDMA networking, which offers the potential to outperform classical TCP/IP. We introduce KafkaDirect, an extension to Apache Kafka, that uses RDMA to accelerate the three most network intensive datapaths: record production, record replication, and record consumption. In this work, we explore the design choices including which RDMA operations to use to take full advantage of offloaded communication. Our RDMA design relies on one-sided RDMA requests to attain true zero-copy communication completely avoiding the need for using intermediate buffers in Kafka servers, thereby ensuring low latency and high throughput communication. KafkaDirect can offer up to 9x increase in throughput for both Kafka producers and Kafka consumers, and can provide 4x and 50x reduction in latency for Kafka producers and Kafka consumers, respectively."
pub.1146381254,"Chapter 12 Big Data, data streaming, and the mobile cloud","This chapter covers three of the most exciting and demanding classes of cloud applications: Big Data, data streaming, and mobile cloud computing. Big Data and data-streaming applications require low-latency, scalability, versatility, and a high degree of fault tolerance. Achieving these qualities at scale is extremely challenging. The defining attributes of Big Data are analyzed in Section 12.1. The next sections discuss how Big Data is stored and processed. High-capacity datastores and databases are necessary to store the very large volumes of data. Scaling data warehouses and databases poses its own challenges. The Mesa datastore and Spanner and F1 databases developed at Google are discussed in Section 12.2, while another class of Big Data applications, combining mathematical modeling with simulation and measurements, and the dynamic, data-driven applications (DDAS), are discussed in Section 12.3. Clouds host several classes of data-streaming applications, ranging from content-delivery data streaming to applications consuming a continuous stream of events. Such applications are discussed in Sections 12.4, 12.5, and 12.6. Mobile devices, such as smartphones, tablets, laptops, and wearable devices, are ubiquitous and indispensable for living in a modern society. Mobile devices are in a symbiotic relationship with computer clouds, and their users benefit from the democratization of data processing. The user of a mobile device has access to the vast amounts of computing cycles and storage available on computer clouds. Mobile cloud computing enables the execution of mobile applications on mobile devices and on computer clouds. Mobile devices act as producers and consumers of the data stored on clouds and shared with others. Section 12.7 is an introduction to mobile computing and its applications, while Section 12.8 covers energy efficiency of mobile computing. Section 12.9 analyzes the effects of latency and presents alternative mobile computing models including cloudlets. Scale allows us to add mission-critical applications demanding very high availability, a topic discussed in Section 12.10. Scale amplifies variability often causing heavy-tail distributions of critical performance metrics as in the case of latency discussed in Section 12.11. Edge computing and Markov decision processes are analyzed in Section 12.12. Bootstrapping techniques for data analytics and approximate query processing are covered in Sections 12.13 and 12.14. Further readings and exercises and problems comprise Sections 12.15 and 12.16."
pub.1169822381,Hardware and Software Integration of Machine Learning Vision System Based on NVIDIA Jetson Nano,"This study investigates the capabilities and flexibility of edge devices for real-time data processing near the source. A configurable Nvidia Jetson Nano system is used to deploy nine pre-trained computer vision models, demonstrating proficiency in local data processing, analysis and providing real-time feedback. Additionally, the system offers deployment control via a customized Graphical User Interface (GUI) and proves very low-latency inference re-stream to other local devices using the G-Streamer framework. The Machine Learning models which cover a wide range of applications, including image classification, object recognition or detection, depth estimation and semantic segmentation, show potential for IoT and industrial applications. Further, the fusion of these capabilities with AI and machine learning algorithms unveils a promising perspective for substantial industrial redevelopment. This research underscores the strategic significance of edge devices in modern computational frameworks and their potential role in future technological advancements."
pub.1145901090,Graph-based Asynchronous Event Processing for Rapid Object Recognition,"Different from traditional video cameras, event cam- eras capture asynchronous events stream in which each event encodes pixel location, trigger time, and the polarity of the brightness changes. In this paper, we introduce a novel graph-based framework for event cameras, namely SlideGCN. Unlike some recent graph-based methods that use groups of events as input, our approach can efficiently process data event-by-event, unlock the low latency nature of events data while still maintaining the graph’s structure internally. For fast graph construction, we develop a radius search algorithm, which better exploits the partial regular structure of event cloud against k-d tree based generic methods. Experiments show that our method reduces the computational complexity up to 100 times with respect to current graph-based methods while keeping state-of-the-art performance on object recognition. Moreover, we verify the superiority of event-wise processing with our method. When the state becomes stable, we can give a prediction with high confidence, thus making an early recognition."
pub.1094242366,E-Storm: Replication-Based State Management in Distributed Stream Processing Systems,"Apache Storm is a fault-tolerant, distributed in-memory computation system for processing large volumes of high-velocity data in real-time. As an integral part of the fault-tolerance mechanism, Storm's state management is achieved by a checkpointing framework, which commits states regularly and recovers lost states from the latest checkpoint. However, this method involves a remote data store for state preservation and access, resulting in significant overheads to the performance of error-free execution. In this paper, we propose E-Storm, a replication-based state management system that actively maintains multiple state backups on different worker nodes. We build a prototype on top of Storm by extending it with monitoring and recovery modules to support inter-task state transfer whenever needed. The experiments carried out on synthetic and real-world streaming applications confirm that E-Storm outperforms the existing checkpointing method in terms of the resulting application performance, obtaining as much as 9.44 times throughput improvement while reducing the application latency down to 9.8%."
pub.1139931511,Design and implementation of a cloud-based event-driven architecture for real-time data processing in wireless sensor networks,"The growth of the Internet of Things (IoTs) and the number of connected devices is driven by emerging applications and business models. One common aim is to provide systems able to synchronize these devices, handle the big amount of daily generated data and meet business demands. This paper proposes a cost-effective cloud-based architecture using an event-driven backbone to process many applications’ data in real-time, called REDA. It supports the Amazon Web Service (AWS) IoT core, and it opens the door as a free software-based implementation. Measured data from several wireless sensor nodes are transmitted to the cloud running application through the lightweight publisher/subscriber messaging transport protocol, MQTT. The real-time stream processing platform, Apache Kafka, is used as a message broker to receive data from the producer and forward it to the correspondent consumer. Micro-services design patterns, as an event consumer, are implemented with Java spring and managed with Apache Maven to avoid the monolithic applications’ problem. The Apache Kafka cluster co-located with Zookeeper is deployed over three availability zones and optimized for high throughput and low latency. To guarantee no message loss and to simulate the system performances, different load tests are carried out. The proposed architecture is reliable in stress cases and can handle records goes to 8000 messages in a second with low latency in a cheap hosted and configured architecture."
pub.1095829717,A High Speed Network Interface Card for Optical Burst Switched Networks**Copyright 2004 © MCNC-RDI. All rights reserved.,Optical Burst Switching (OBS) is emerging as an optical networking technology combining best of the features of optical circuit-switching technology and optical packet-switching technology. This paper discusses architecture and hardware implementation of a Network Interface Card (NIC) for an OBS network. The NIC implements a specific low-latency signaling protocol lust-in-Time (JIT) for OBS networks. This NIC can handle data streams up to 1 Gbps. Average OBS JIT signaling message processing time with this NIC is on the order of 1 microsecond. This is the first JIT OBS NIC implementation known to the authors. Complementary efforts are on-going to develop other elements for the OBS network.
pub.1094590054,Linear-Time Online Action Detection from 3D Skeletal Data Using Bags of Gesturelets,"Sliding window is one direct way to extend a successful recognition system to handle the more challenging detection problem. While action recognition decides only whether or not an action is present in a pre-segmented video sequence, action detection identifies the time interval where the action occurred in an unsegmented video stream. Sliding window approaches can however be slow as they maximize a classifier score over all possible sub-intervals. Even though new schemes utilize dynamic programming to speed up the search for the optimal sub-interval, they require offline processing on the whole video sequence. In this paper, we propose a novel approach for online action detection based on 3D skeleton sequences extracted from depth data. It identifies the sub-interval with the maximum classifier score in linear time. Furthermore, it is suitable for real-time applications with low latency."
pub.1165167245,Error analysis of a multi-sensor maritime targeting system,"Targeting systems are subject to multiple sources of error when operating in complex environments. To reduce the effect of these errors, modern targeting systems generally include both imaging and RF sensors. Data processing then provides target detection and classification information, and the detection streams are combined using a data fusion scheme to produce an optimal target location estimate with an associated latency. In this paper, the performance of a multi-sensor system in a maritime application is investigated using a mathematical simulator that has been developed to provide the system performance error analysis for different engagement scenarios and test conditions. This simulator is described together with the sources of targeting error such as image motion blur and radar glint. Additionally, the impact of flare and chaff countermeasures on the targeting performance is reviewed in terms of different types of target recognition and tracking algorithms."
pub.1003257191,Exploring microcontrollers in GPUs,"Recent graphics processing units (GPUs) integrate wimpy microcontrollers on a chip. They are often used to execute firmware code configuring the functional units of GPUs. This paper opens up the programming of these microcontrollers and explores how to utilize them for GPU resource management. Our prototype system provides a compiler suite for NVIDIA's GPU microcontrollers with its basis on the Low Level Virtual Machine (LLVM) infrastructure. As a proof of concept, we develop fully-functional firmware using our compiler and provide a basic performance evaluation. The experimental results demonstrate that the overhead of introducing our firmware is suppressed to within 2.3%, as compared to the native proprietary firmware, while the impact of overhead is no greater than 0.01% of the total execution time according to microbenchmarks. We also show that a complementary use of microcontrollers can reduce the latency of data transfers with concurrent multiple data streams."
pub.1163644529,Graph-based Asynchronous Event Processing for Rapid Object Recognition,"Different from traditional video cameras, event cameras capture asynchronous
events stream in which each event encodes pixel location, trigger time, and the
polarity of the brightness changes. In this paper, we introduce a novel
graph-based framework for event cameras, namely SlideGCN. Unlike some recent
graph-based methods that use groups of events as input, our approach can
efficiently process data event-by-event, unlock the low latency nature of
events data while still maintaining the graph's structure internally. For fast
graph construction, we develop a radius search algorithm, which better exploits
the partial regular structure of event cloud against k-d tree based generic
methods. Experiments show that our method reduces the computational complexity
up to 100 times with respect to current graph-based methods while keeping
state-of-the-art performance on object recognition. Moreover, we verify the
superiority of event-wise processing with our method. When the state becomes
stable, we can give a prediction with high confidence, thus making an early
recognition. Project page: \url{https://zju3dv.github.io/slide_gcn/}."
pub.1068943574,Complex-Event Processing for the Intelligent Field,"This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 167817, ’Implementation of Complex-Event Processing for the Intelligent Field,’ by Muhammad Al-Gosayir, Moayad Al-Nammi, Waleed Awadh, Abdullah Al-Bar, and Nasser Nasser, Saudi Aramco, prepared for the 2014 SPE Intelligent Energy Conference and Exhibition, Utrecht, the Netherlands, 1-3 April. The paper has not been peer reviewed. Every day, reservoir and production engineers are presented with massive streams of real-time data from all kinds of intelligent-field equipment. The existing systems for data cleansing and summary are based on batch processing; hence, engineers find it challenging to make the right decision on time and do not have the capacity to instantly detect interesting patterns as data arrive in streams. This paper addresses the architecture, implementation, and benefits of complex-event processing (CEP) as a solution for the intelligent field. Introduction CEP is an innovative, rising technology designed to handle large amounts of data with minimal latency in real time. This technology can aid in the detection of trends and anomalies in the data stream such as unusual buildup or drawdown in well pressure in real time. Furthermore, CEP is an event-driven solution, meaning that it is triggered by events such as changes in downhole pressure in order to perform computational logic to calculate average reservoir pressure. This process will in turn provide reservoir and production engineers with clean real- time data and notifications of prominent events. Overview of Intelligent-Field Structure. For the implementation of the intelligent field, Saudi Aramco has adopted a four-layered architecture. These layers are surveillance, integration, optimization, and innovation, as shown in Fig. 1. The surveillance layer is responsible for continuous monitoring of real-time production data and makes use of data-management tools to ensure the validity of the data. The integration layer processes real-time data to detect trends and anomalies. These anomalies are referred to reservoir engineers for analysis and resolution. The optimization layer streamlines field-optimization capabilities and management recommendations. The innovation layer stores event knowledge and triggers optimization processes and actions throughout the field’s life cycle. This final layer captures “intelligence” and injects it into the system. Real-time data traverse multiple zones of instrumentation, networks, and data servers before they reach the corporate database and the engineer’s desktop (Fig. 2). A typical piece of data captured by a sensor will be transmitted automatically from the sensor and will pass through multiple data servers connected through wired/wireless networks. The data eventually reside in the corporate database."
pub.1131701998,GSLICE,"The increasing demand for cloud-based inference services requires the use of Graphics Processing Unit (GPU). It is highly desirable to utilize GPU efficiently by multiplexing different inference tasks on the GPU. Batched processing, CUDA streams and Multi-process-service (MPS) help. However, we find that these are not adequate for achieving scalability by efficiently utilizing GPUs, and do not guarantee predictable performance. GSLICE addresses these challenges by incorporating a dynamic GPU resource allocation and management framework to maximize performance and resource utilization. We virtualize the GPU by apportioning the GPU resources across different Inference Functions (IFs), thus providing isolation and guaranteeing performance. We develop self-learning and adaptive GPU resource allocation and batching schemes that account for network traffic characteristics, while also keeping inference latencies below service level objectives. GSLICE adapts quickly to the streaming data's workload intensity and the variability of GPU processing costs. GSLICE provides scalability of the GPU for IF processing through efficient and controlled spatial multiplexing, coupled with a GPU resource re-allocation scheme with near-zero (< 100s) downtime. Compared to default MPS and TensorRT, GSLICE improves GPU utilization efficiency by 60--800% and achieves 2--13X improvement in aggregate throughput."
pub.1105742364,Optimizing data transmission and access of the incremental clustering algorithm using CUDA: A case study,"Incremental clustering algorithms can find wide applications in real-time streaming data processing and massive data analysis. Such algorithms need to continuously load data, and thus data transmission and access can induce non-negligible time overhead. Additionally, we have proposed two algorithms to exploit high data parallelism for incremental clustering on CUDA-enabled GPGPU: the Top-down (TD) algorithm and Moderate-granularity (MG) algorithm. In this paper, we adopt TD and MG algorithms as a case study to optimize data transmission and access based on CUDA. First, we reinterpret the two algorithms in the point view of overlapping read/write and computing operations on CUDA-warp level. Second, we adjust the flow of TD and MG algorithms to enhance data locality. As a result, shared memory can be sufficiently utilized. Third, we reorder input data points to raise data rate of global memory through coalesced memory access. Fourth, we hide part of data transmission latency by running multiple CUDA streams. Experiment results validated the efficiency of our optimizations."
pub.1120359562,Studying the Impact of CPU and Memory Controller Frequencies on Power Consumption of the Jetson TX1,"Nowadays, heterogeneous unified memory architecture platforms are becoming increasingly common. These platforms incorporate several co-processors on a single chip with a shared physical memory. The use cases for such platforms can vary dramatically. On the one hand, they can be used in the context of Edge computing, which cannot tolerate high latency and has strict energy/power constraints. On the other hand, motivated by their growing computing capabilities, and their energy-efficiency, many have considered replacing traditional bulky servers with these platforms to deliver the same computing power but with lower energy budget. This study is an exploratory step to understand the trade-off between power consumption, processing time, and throughput on a low-power heterogeneous platform. We focus on data stream processing workloads by characterizing several common computing kernels found in computer vision algorithms. Our preliminary experiments on NVIDIA Jetson TX1 show that it is possible reduce power consumption by up to 12%."
pub.1176131378,Improving Network Resource Utilization for Distributed Wireless Sensing Applications,"Edge-assisted wireless sensing is increasingly popular, where complex neural network models perform inference tasks on wireless channel state information (CSI) data streamed from IoT devices. However large volumes of CSI data sent across the network for inference can significantly impact network bandwidth and reduce the Quality of Experience. This paper tackles the challenge of optimizing network resource utilization in wireless sensing systems by compressing and subsampling CSI streams. We evaluate methods that quantize and selectively subsample CSI data before transmission to the edge server, which is then fed to the inference models. Such approach reduces bandwidth and computational load, improving data transmission and processing efficiency. Experiments conducted in two real testbeds (indoors as well as outdoors) show how CSI compression preserves sensing information integrity while enhancing system performance in terms of latency, energy efficiency, and throughput. By integrating quantization and subsampling with edge computing, this work enhances wireless sensing systems, making them more scalable and efficient in utilizing network resources."
pub.1159718201,INEv: In-Network Evaluation for Event Stream Processing,"Complex event processing (CEP) detects situations of interest by evaluating queries over event streams. Once CEP is used in networked applications, the distribution of query evaluation among the event sources enables performance optimization. Instead of collecting all events at one location for query evaluation, sub-queries are placed at network nodes to reduce the data transmission overhead. Yet, existing techniques either place such sub-queries at exactly one node in the network, which neglects the benefits of truly distributed evaluation, or are agnostic to the network structure, which ignores transmission costs due to the absence of direct network links. To overcome the above limitations, we propose INEV graphs for in-network evaluation of CEP queries with rich semantics, including Kleene closure and negation. Our idea is to introduce fine-granular routing of partial results of sub-queries as an additional degree of freedom in query evaluation: We exploit events already disseminated in the network as part of one sub-query, when evaluating another one. We show how to instantiate INEv graphs by splitting a query workload into sub-queries, placing them at network nodes, and forwarding of their results to other nodes. Also, we characterize INEv graphs that guarantee correct and complete query evaluation, and discuss their construction based on a cost model that unifies transmission and processing latency. Our experimental results indicate that INEv graphs can reduce transmission costs for distributed CEP by up to eight orders of magnitude compared to baseline strategies."
pub.1109770029,CGAcc: A Compressed Sparse Row Representation-Based BFS Graph Traversal Accelerator on Hybrid Memory Cube,"Graph traversal is widely used in map routing, social network analysis, causal discovery and many more applications. Because it is a memory-bound process, graph traversal puts significant pressure on the memory subsystem. Due to poor spatial locality and the increasing size of today’s datasets, graph traversal consumes an ever-larger part of application execution time. One way to mitigate this cost is memory prefetching, which issues requests from the processor to the memory in anticipation of needing certain data. However, traditional prefetching does not work well for graph traversal due to data dependencies, the parallel nature of graphs and the need to move vast amounts of data from memory to the caches. In this paper, we propose a compressed sparse row representation-based graph accelerator on the Hybrid Memory Cube (HMC), called CGAcc. CGAcc combines Compressed Sparse Row (CSR) graph representation with in-memory prefetching and processing to improve the performance of graph traversal. Our approach integrates the prefetching and processing in the logic layer of a 3D stacked Dynamic Random-Access Memory (DRAM) architecture, based on Micron’s HMC. We selected HMC to implement CGAcc because it can provide quite high bandwidth and low access latency. Furthermore, this device has multiple DRAM layers connected to internal logic to control memory access and perform rudimentary computation. Using the CSR representation, CGAcc deploys prefetchers in the HMC to exploit the short transaction latency between the logic and DRAM layers. By doing this, it can also avoid large data movement costs. In the runtime, CGAcc pipelines the prefetching to fetch data from DRAM arrays to improve memory-level parallelism. To further reduce the access latency, several optimized internal caches are also introduced to hold the prefetched data to be Processed In-Memory (PIM). A comprehensive evaluation shows the effectiveness of CGAcc. Experimental results showed that, compared to a conventional HMC main memory equipped with a stream prefetcher, CGAcc achieved an average 3.51× speedup with moderate hardware cost."
pub.1094050855,Max-Hashing Fragments for Large Data Sets Detection,"The standard way to detect known digital objects inside a stream of bytes consists in using a string matching algorithm initialized with a dictionary containing the objects to detect. Depending on the application, the algorithm may be implemented in software or with dedicated hardware, to speedup the processing. Nevertheless, such approach requires an automaton with a complexity that is linear in the size of the dictionary. Large dictionaries result in large automatons that must be stored in high-latency memories, therefore limiting the processing speed. We propose a fast algorithm tailored to FPGA implementation to detect the transfer of known digital objects from their fragments. For illustrative purpose, the algorithm is applied to the detection of more than 100 000 known JPEG files by just inspecting the IP packets captured during an FTP transfer. Results demonstrate excellent true/false positive rates with nearly no limit on the number of objects to detect and the transfer rates."
pub.1094386334,User-Constraint and Self-Adaptive Fault Tolerance for Event Stream Processing Systems,"Event Stream Processing (ESP) Systems are currently enabling a renaissance in the data processing area as they provide results at low latency compared to the traditional MapReduce approach. Although the majority of ESP systems offer some form of fault tolerance to their users, the provided fault tolerance scheme is often not tailored to the application at hand. For example, active replication is well suited for critical applications where unresponsiveness due to a background recovery process is not acceptable. However, for other classes of applications without such tight constraints, the use of passive replication, based on checkpoints and logging, is a better choice as it can save a significant amount of resources compared to active replication. In this paper, we present StreamMine3G, a fault tolerant and elastic ESP system which employs several fault tolerance schemes, such as passive and active replication as well as intermediate alternatives such as active and passive standby. In order to free the user from the burden of choosing the correct scheme for the application at hand, StreamMine3G is equipped with a fault-tolerance controller that transitions between the employed schemes during runtime in response to the evolution of the given workload and the user's provided constraints (recovery time and semantics, i.e., gap or precise). Our evaluation shows that the overall resource footprint for fault tolerance can be considerably reduced using our adaptive approach without consequences to the recovery time."
pub.1136452637,"AutoFlow: Hotspot-Aware, Dynamic Load Balancing for Distributed Stream Processing","Stream applications are widely deployed on the cloud. While modern
distributed streaming systems like Flink and Spark Streaming can schedule and
execute them efficiently, streaming dataflows are often dynamically changing,
which may cause computation imbalance and backpressure. We introduce AutoFlow,
an automatic, hotspot-aware dynamic load balance system for streaming
dataflows. It incorporates a centralized scheduler which monitors the load
balance in the entire dataflow dynamically and implements state migrations
correspondingly. The scheduler achieves these two tasks using a simple
asynchronous distributed control message mechanism and a hotspot-diminishing
algorithm. The timing mechanism supports implicit barriers and a highly
efficient state-migration without global barriers or pauses to operators. It
also supports a time-window based load-balance measurement and feeds them to
the hotspot-diminishing algorithm without user interference. We implemented
AutoFlow on top of Ray, an actor-based distributed execution framework. Our
evaluation based on various streaming benchmark dataset shows that AutoFlow
achieves good load-balance and incurs a low latency overhead in highly
data-skew workload."
pub.1095658053,ES2: Aiming at an Optimal Virtual I/O Event Path,"Improving the performance of I/O virtualization is a key issue for cloud and datacenter infrastructures, especially with the rapid increase of network interconnection speeds. Previous efforts have made the performance overhead associated with the virtual I/O data path largely negligible. The remaining bottlenecks mainly lie in the event path: hypervisor interventions trigger costly virtual machine (VM) exits and lead to dramatical performance degradation. Aiming at an optimal virtual I/O event path, we propose ES2, a comprehensive scheme that simultaneously improves bidirectional I/O event delivery between guest VMs and their devices. ES2 can provide efficient I/O request delivery, non-exit interrupt delivery and enhanced I/O responsiveness. Moreover, it does not require any modification to guest operating system (OS) or compromise any virtualization benefit. We demonstrate that ES2 greatly reduces VM exit rate with the time in guest (TIG) for I/O processing above 96% for TCP streams and 99% for UDP streams, increases guest throughput by 1.8x for Memcached and 2x for Apache, and keeps guest latency at a very low level."
pub.1062408300,Space-time block coding for undersea acoustic links,"Current research in undersea communication scenario focuses on the applications of undersea acoustic networks in environmental data collection, pollution monitoring, offshore surveillance, coastal surveillance, etc. The various constraints encountered in undersea acoustic communications are mainly low bandwidth, high latency, high failure rates of acoustic links, and multipath propagation effects. Multipath effects in undersea environments are due to the reflection of sound by the sea surface and sea bottom as well as refraction of sound in water. As a result, the receiver gets a bewildering mix of signals. The receiving subsystems of the undersea networks perform special signal processing techniques for regenerating the data streams from the multipath composite signals in a MIMO configurable network. The performance of Space-Time Block Coding (STBC) diversity technique has been adopted in this paper for improving the reliability of the undersea network. In the simplest form of MIMO configuration, two transmitting transducers send out the data stream and its modified versions in two time slots on independently fading environments, leading to substantial improvements in the error rate performance. For a shallow water medium range channel, which exhibits the Rayleigh fading characteristics, the error rate performance has been found to be 7 x 10-6 for 16-QAM based STBC."
pub.1107672124,Cloud Resource Scaling for Time-Bounded and Unbounded Big Data Streaming Applications,"Recent advancements in technology have led to a deluge of big data streams that require real-time analysis with strict latency constraints. A major challenge, however, is determining the amount of resources required by applications processing these streams given their high volume, velocity and variety. The majority of research efforts on resource scaling in the cloud are investigated from the cloud provider’s perspective with little consideration for multiple resource bottlenecks. We aim at analyzing the resource scaling problem from an application provider’s point of view such that efficient scaling decisions can be made. This paper provides two contributions to the study of resource scaling for big data streaming applications in the cloud. First, we present a Layered Multi-dimensional Hidden Markov Model (LMD-HMM) for managing time-bounded streaming applications. Second, to cater to unbounded streaming applications, we propose a framework based on a Layered Multi-dimensional Hidden Semi-Markov Model (LMD-HSMM). The parameters in our models are evaluated using modified Forward and Backward algorithms. Our detailed experimental evaluation results show that LMD-HMM is very effective with respect to cloud resource prediction for bounded streaming applications running for shorter periods while the LMD-HSMM accurately predicts the resource usage for streaming applications running for longer periods."
pub.1140131342,SNR: Network-aware Geo-Distributed Stream Analytics,"Emerging applications such as those running on the Internet of Things (IoT) devices produce constant data streams that need to be processed in real-time. Distributed stream processing systems (DSPs), with geographically distributed cluster networks interconnected via wide area network (WAN) links, have recently gained interest in handling these applications. How-ever, these applications have stringent requirements such as low-latency and high bandwidth that must be guaranteed to ensure the quality of service (QoS). These application requirements raise fundamental DSPs resource management and scheduling challenge. In this paper, we formulate the problem of placement of worker nodes on a geo-distributed DSPs cluster network as a multi-criteria decision-making problem and propose an additive weighting-based approach to solve it. The proposed solution finds the trade-off among different network parameters and allows executing the tasks according to the desired performance metrics. We evaluated the proposed approach using the Yahoo! streaming benchmark on a testbed and compare it against mechanisms deployed in Apache Spark, Apache Storm, and Apache Flink. The results of the evaluation show that our approach improves the performance of Spark up to 2.2x-7.2x, Storm up to 1.2x-3.4x, and Flink up to 1.4x-3.3x compared to other approaches, which makes our approach useful for use in practical environments."
pub.1118746105,"Demystifying Fog Computing: Characterizing Architectures, Applications and Abstractions","Internet of Things (IoT) has accelerated the deployment of millions of
sensors at the edge of the network, through Smart City infrastructure and
lifestyle devices. Cloud computing platforms are often tasked with handling
these large volumes and fast streams of data from the edge. Recently, Fog
computing has emerged as a concept for low-latency and resource-rich processing
of these observation streams, to complement Edge and Cloud computing. In this
paper, we review various dimensions of system architecture, application
characteristics and platform abstractions that are manifest in this Edge, Fog
and Cloud eco-system. We highlight novel capabilities of the Edge and Fog
layers, such as physical and application mobility, privacy sensitivity, and a
nascent runtime environment. IoT application case studies based on first-hand
experiences across diverse domains drive this categorization. We also highlight
the gap between the potential and the reality of Fog computing, and identify
challenges that need to be overcome for the solution to be sustainable.
Together, our article can help platform and application developers bridge the
gap that remains in making Fog computing viable."
pub.1121628410,The development of a multi-band handheld fusion camera,"A longwave infrared (LWIR) handheld surveillance camera has been modified through the addition of a second sensor which provides both visible (RGB) and near-infrared (NIR) image streams. The challenges and constraints imposed on the development process of this Handheld Fusion Camera (HHFC) are described, and the approach to the dual and tri-band image fusion processing schemes is presented. The physical characteristics of the existing camera acted as a major constraint on the HHFC design with the Size, Weight, and Power (SWaP) requirements restricting the choice of both the additional sensor as well as the processor engine available within the camera. The primary use of the HHFC is in groundbased security and surveillance operations which is challenging in terms of variability in the scene content. Establishing an effective processing architecture is critical to both image interpretability by the user, and operational effectiveness. The HHFC allows the user to view different image streams including enhanced single-band image data as well as both dual and tri-band fused imagery. Such flexibility allows the user to select the best imagery for their immediate requirements. Power consumption and latency figures have been minimised by the use of relatively simple arithmetical fusion algorithms combined with an Adaptive Weight Map (AWM) for regional-based optimisation. In practice, the potential performance gain achieved is necessarily limited by the required performance robustness, and this trade-off was critical to the HHFC design and the final image processing solution."
pub.1138783828,A High-Throughput Hardware Accelerator for Network Entropy Estimation Using Sketches,"Network traffic monitoring uses empirical entropy to detect anomalous events such as various types of attacks. However, the exact computation of the entropy in high-speed networks is a difficult process due to the limited memory resources available in the data plane hardware. In this paper, we present a method and hardware accelerator to approximate the empirical entropy of a large data set with high throughput and sublinear memory requirements. Our method uses streaming algorithms that exploit the fine-grained parallelism of existing hardware platforms for data plane processing, such as field-programmable gate arrays (FPGAs). The method uses sketches to compute the cardinality of the stream and the frequencies of the top-K elements on line, and then it estimates the contribution to the entropy of the rest of the stream assuming a simple uniform distribution for these elements. Implemented on a Xilinx UltraScale+ ZCU102 FPGA, the accelerator implements the method using only on-chip memory, with less than 50% resource usage. Tested on real network traces of up to 120 million packets and more than 5 million flows, the accelerator estimates the empirical entropy with less than 1.5% mean relative error and $21~\mu \text{s}$ latency, and supports a minimum throughput of 204 gigabits per second."
pub.1093426737,"Demystifying Fog Computing: Characterizing Architectures, Applications and Abstractions","Internet of Things (IoT) has accelerated the deployment of millions of sensors at the edge of the network, through Smart City infrastructure and lifestyle devices. Cloud computing platforms are often tasked with handling these large volumes and fast streams of data from the edge. Recently, Fog computing has emerged as a concept for low-latency and resource-rich processing of these observation streams, to complement Edge and Cloud computing. In this paper, we review various dimensions of system architecture, application characteristics and platform abstractions that are manifest in this Edge, Fog and Cloud ecosystem. We highlight novel capabilities of the Edge and Fog layers, such as physical and application mobility, privacy sensitivity, and a nascent runtime environment. IoT application case studies based on first-hand experiences across diverse domains drive this categorization. We also highlight the gap between the potential and the reality of Fog computing, and identify challenges that need to be overcome for the solution to be sustainable. Taken together, our article can help platform and application developers bridge the gap that remains in making Fog computing viable."
pub.1007678075,Bursty event detection from microblog: a distributed and incremental approach,"Summary As a new form of social media, microblogs (e.g., Twitter and Weibo) are playing an important role in people's daily life. With the rise in popularity and size of microblogs, there is a need for distributed approaches that can detect bursty event with low latency from the short‐text data stream. In this paper, we propose a distributed and incremental temporal topic model for microblogs called Bursty Event dEtection (BEE+). BEE+ is able to detect bursty events from short‐text dataset and model the temporal information. And BEE+ processes the post‐stream incrementally to track the topic drifting of events over time. Therefore, the latent semantic indices are preserved from one time period to the next. In order to achieve real‐time processing, we design a distributed execution framework based on Spark engine. To verify its ability to detect bursty event, we conduct experiments on a Weibo dataset of 6,360,125 posts. The results show that BEE+ can outperform the baselines for detecting the meaningful bursty events and track the topic drifting. Copyright © 2015 John Wiley & Sons, Ltd."
pub.1175362333,An Investigation on Handling IoT Big Data with RBSEE Architecture,"Internet of Things (IoT) devices have resulted in an unprecedented surge in data generation, presenting both opportunities and challenges in data management and analytics. This paper investigates the feasibility and effectiveness of utilizing the RBSEE (Resource-Based Storage, Edge Computing, and Elasticity) architecture for handling IoT Big Data. Firstly, the paper elucidates the fundamental concepts of IoT and Big Data, highlighting the unique characteristics and challenges associated with managing large volumes of heterogeneous data streams generated by IoT devices. Subsequently, it introduces the RBSEE architecture, which integrates resource-based storage, edge computing, and elasticity to address the scalability, latency, and resource constraints inherent in IoT environments. The architecture's ability to distribute data storage and processing tasks across edge devices and cloud infrastructure while dynamically adapting to fluctuating workloads makes it particularly suitable for handling IoT Big Data. In any typical IoT ecosystem, real-time and instantaneous handling of request and dataset is pivotal and a fundamental property. This aims to propose energy efficient distributed system architecture for persuasive, privacy-preserved and real time handling of data generated from IoT devices."
pub.1120058358,Towards an Infrastructure Enabling the Internet of Production,"New levels of cross-domain collaboration between manufacturing companies throughout the supply chain are anticipated to bring benefits to both suppliers and consumers of products. Enabling a fine-grained sharing and analysis of data among different stakeholders in an automated manner, such a vision of an Internet of Production (IoP) introduces demanding challenges to the communication, storage, and computation infrastructure in production environments. In this work, we present three example cases that would benefit from an IoP (a fine blanking line, a high pressure die casting process, and a connected job shop) and derive requirements that cannot be met by today’s infrastructure. In particular, we identify three orthogonal research objectives: (i) real-time control of tightly integrated production processes to offer seamless low-latency analysis and execution, (ii) storing and processing heterogeneous production data to support scalable data stream processing and storage, and (iii) secure privacy-aware collaboration in production to provide a basis for secure industrial collaboration. Based on a discussion of state-of-the-art approaches for these three objectives, we create a blueprint for an infrastructure acting as an enabler for an IoP."
pub.1093454281,Dela — Sharing Large Datasets Between Hadoop Clusters,"Big data has, in recent years, revolutionised an ever-growing number of fields, from machine learning to climate science to genomics. The current state-of-the-art for storing large datasets is either object stores or distributed filesystems, with Hadoop being the dominant open-source platform for managing ‘Big Data’. Existing large-scale storage platforms, however, lack support for the efficient sharing of large datasets over the Internet. Those systems that are widely used for the dissemination of large files, like BitTorrent, need to be adapted to handle challenges such as network links with both high latency and high bandwidth, and scalable storage backends that are optimised for streaming and not random access. In this paper, we introduce Dela, a peer-to-peer data-sharing service integrated into the Hops Hadoop platform that provides an end-to-end solution for dataset sharing. Dela is designed for large-scale storage backends and data transfers that are both non-intrusive to existing TCP network traffic and provide higher network throughput than TCP on high latency, high bandwidth network links, such as transatlantic network links. Dela provides a pluggable storage layer, implementing two alternative ways for clients to access shared data: stream processing of data as it arrives with Kafka, and traditional offline access to data using the Hadoop Distributed Filesystem. Dela is the first step for the Hadoop platform towards creating an open dataset ecosystem that supports user-friendly publishing, searching, and downloading of large datasets."
pub.1136541770,Real-time Traffic Jam Detection and Congestion Reduction Using Streaming Graph Analytics,"Traffic congestion is a problem in day to day life, especially in big cities. Various traffic control infrastructure systems have been deployed to monitor and improve the flow of traffic across cities. Real-time congestion detection can serve for many useful purposes that include sending warnings to drivers approaching the congested area and daily route planning. Most of the existing congestion detection solutions combine historical data with continuous sensor readings and rely on data collected from multiple sensors deployed on the road, measuring the speed of vehicles. While in our work we present a framework that works in a pure streaming setting where historic data is not available before processing. The traffic data streams, possibly unbounded, arrive in real-time. Moreover, the data used in our case is collected only from sensors placed on the intersections of the road. Therefore, we investigate in creating a real-time congestion detection and reduction solution, that works on traffic streams without any prior knowledge. The goal of our work is 1) to detect traffic jams in real-time, and 2) to reduce the congestion in the traffic jam areas. In this work, we present a real-time traffic jam detection and congestion reduction framework: 1) We propose a directed weighted graph representation of the traffic infrastructure network for capturing dependencies between sensor data to measure traffic congestion; 2) We present online traffic jam detection and congestion reduction techniques built on a modern stream processing system, i.e., Apache Flink; 3) We develop dynamic traffic light policies for controlling traffic in congested areas to reduce the travel time of vehicles. Our experimental results indicate that we are able to detect traffic jams in real-time and deploy new traffic light policies which result in 27% less travel time at the best and 8% less travel time on average compared to the travel time with default traffic light policies. Our scalability results show that our system is able to handle high-intensity streaming data with high throughput and low latency."
pub.1119513787,Muppet: MapReduce-Style Processing of Fast Data,"MapReduce has emerged as a popular method to process big data. In the past
few years, however, not just big data, but fast data has also exploded in
volume and availability. Examples of such data include sensor data streams, the
Twitter Firehose, and Facebook updates. Numerous applications must process fast
data. Can we provide a MapReduce-style framework so that developers can quickly
write such applications and execute them over a cluster of machines, to achieve
low latency and high scalability? In this paper we report on our investigation
of this question, as carried out at Kosmix and WalmartLabs. We describe
MapUpdate, a framework like MapReduce, but specifically developed for fast
data. We describe Muppet, our implementation of MapUpdate. Throughout the
description we highlight the key challenges, argue why MapReduce is not well
suited to address them, and briefly describe our current solutions. Finally, we
describe our experience and lessons learned with Muppet, which has been used
extensively at Kosmix and WalmartLabs to power a broad range of applications in
social media and e-commerce."
pub.1130333286,Management of Load‐Balancing Data Stream in Interposer‐Based Network‐on‐Chip Using Specific Virtual Channels,"The interaction between cores and memory blocks, in multiprocessor chips and smart systems, has always been a concern as it affects network latency, memory capacity, and power consumption. A new 2.5-dimensional architecture has been introduced in which the communication between the processing elements and the memory blocks is provided through a layer called the interposer. If the core wants to connect to another, it uses the top layer, and if it wants to interact with the memory blocks, it uses the interposer layer. In a case that coherence traffic at the processing layer increases to the extent that congestion occurs, a part of this traffic may be transferred to the interposer network under a mechanism called load balancing. When coherence traffic is moved to the interposer layer, as an alternative way, this may interfere with memory traffic. This paper introduces a mechanism in which the aforementioned interference may be avoided by defining two different virtual channels and using multiple links which specifically determines which memory block is going to be accessed. Our method is based on the destination address to recognize which channel and link should be selected while using the interposer layer. The simulation results show that the proposed mechanism has improved by 32% and 14% latency compared to the traditional load-balancing and unbalanced mechanisms, respectively."
pub.1144211815,Designing a Streaming Data Coalescing Architecture for Scientific Detector ASICs with Variable Data Velocity,"Scientific detectors are a key technological enabler for many disciplines. Application-specific integrated circuits (ASICs) are used for many of these scientific detectors. Until recently, pixel detector ASICs have been used mainly for analog signal processing of the charge from the sensor layer and the transmission of raw pixel data off the detector ASIC. However, with the availability of more advanced ASIC technology nodes for scientific application, more digital functionality from the computing domains (e.g., compression) can be integrated directly into the detector ASIC to increase data velocity. However, these computing functionalities often have high and variable latency, whereas scientific detectors must operate in real-time (i.e., stall-free) to support continuous streaming of sampled data. This paper presents an example from the domain of pixel detectors with on-chip data compression for X-ray science applications. To address the challenges of variable-sized data from a parallel stream of compressors, we present an ASIC design architecture to coalesce variable-length data for transmission over a fixed bit-width network interface."
pub.1141964581,Designing a Streaming Data Coalescing Architecture for Scientific Detector ASICs with Variable Data Velocity,"Scientific detectors are a key technological enabler for many disciplines.
Application-specific integrated circuits (ASICs) are used for many of these
scientific detectors. Until recently, pixel detector ASICs have been used
mainly for analog signal processing of the charge from the sensor layer and the
transmission of raw pixel data off the detector ASIC. However, with the
availability of more advanced ASIC technology nodes for scientific application,
more digital functionality from the computing domains (e.g., compression) can
be integrated directly into the detector ASIC to increase data velocity.
However, these computing functionalities often have high and variable latency,
whereas scientific detectors must operate in real-time (i.e., stall-free) to
support continuous streaming of sampled data. This paper presents an example
from the domain of pixel detectors with on-chip data compression for X-ray
science applications. To address the challenges of variable-sized data from a
parallel stream of compressors, we present an ASIC design architecture to
coalesce variable-length data for transmission over a fixed bit-width network
interface."
pub.1067368066,Muppet,"MapReduce has emerged as a popular method to process big data. In the past few years, however, not just big data, but fast data has also exploded in volume and availability. Examples of such data include sensor data streams, the Twitter Firehose, and Facebook updates. Numerous applications must process fast data. Can we provide a MapReduce-style framework so that developers can quickly write such applications and execute them over a cluster of machines, to achieve low latency and high scalability? In this paper we report on our investigation of this question, as carried out at Kosmix and WalmartLabs. We describe MapUpdate, a framework like MapReduce, but specifically developed for fast data. We describe Muppet, our implementation of MapUpdate. Throughout the description we highlight the key challenges, argue why MapReduce is not well suited to address them, and briefly describe our current solutions. Finally, we describe our experience and lessons learned with Muppet, which has been used extensively at Kosmix and WalmartLabs to power a broad range of applications in social media and e-commerce."
pub.1134634072,Service Provisioning for IoT Applications with Multiple Sources in Mobile Edge Computing,"We are embracing an era of Internet of Things (IoTs). However, the latency brought by unstable wireless networks and computation failures caused by limited resources on IoT devices seriously impacts the quality of service of user experienced. To address these shortcomings, the Mobile Edge Computing (MEC) platform provides a promising solution for the service provisioning of IoT applications, where edge-clouds (cloudlets) are co-located with wireless access points in the proximity of IoT devices, and the service response latency can be significantly reduced. Meanwhile, each IoT application usually imposes a service function chain enforcement for its data transmission, which consists of different service functions in a specified order, and each data packet transfer in the network from the gateways of IoT devices to the destination must pass through each of the service functions in order. In this paper, we study IoT-driven service provisioning in an MEC network for various IoT applications with service function chain requirements, where an IoT application consists of multiple data streams from different IoT sources that will be uploaded to the MEC network for aggregation, processing, and storage. We first formulate a novel cost minimization problem for IoT-driven service provisioning in MEC networks. We then show that the problem is NP-hard, and propose an IoT-driven service provisioning framework for IoT applications, which consists of streaming data uploading from multiple IoT sources to the MEC network, data stream aggregation and routing, and Virtual Network Function (VNF) instance placement and sharing in cloudlets in the MEC network. In addition, we devise an efficient algorithm for the problem, built upon the proposed service framework. We finally evaluate the performance of the proposed algorithm through experimental simulations. Experimental results demonstrate that the proposed algorithm is promising, compared with the lower bound on the optimal solution of the problem and another comparison heuristic."
pub.1094649224,Characterization and Correction of Data Loss in a High Bandwidth Passive Radar System,"Passive radar receivers use transmitters of opportunity such as digital television (DTV) broadcast to detect targets. The next generation Manastash Ridge Radar (MRR) is designed without the use of an analog downconverter to observe transmitters up to 1.5 GHz. In addition to fast sampling, the receiver is built around a Xilinx Virtex-5 field programmable gate array (FPGA) for software-defined, flexible, and real-time, low latency processing. The FPGA channelizes data from up to four antennas and streams 8-bit IQ data through a 10 GbE link to a data recorder. Challenging the capacity of this link is extremely desirable, as it will allow the user to save a wide RF spectrum to disk for experimental processing. In the fastest use of this link, we observe up to 10 frequency-adjacent DTV stations simultaneously, however packet loss occurs. We present here characterization of this loss in real data, simulations of how this loss propagates through the processing chain and affects the final data product, suggestions for correcting this loss, and apply these strategies to real detections of aircraft on each of four antennas. The results of the simulations suggest that the radar system can absorb even 50 % data loss while losing only 3 dB in detectability of targets and completely recover accurate range and Doppler velocity estimates. The detection of an aircraft with our system in the presence of 12 % data loss follows the trends observed in simulation. This encouraging result shows that systems with high processing gain are incredibly robust to noise and the sacrifice of lost data in the face of observing more RF spectrum (more transmitters) is truly not a sacrifice at all."
pub.1104047580,Secure and Sustainable Load Balancing of Edge Data Centers in Fog Computing,"Fog computing is a recent research trend to bring cloud computing services to network edges. EDCs are deployed to decrease the latency and network congestion by processing data streams and user requests in near real time. EDC deployment is distributed in nature and positioned between cloud data centers and data sources. Load balancing is the process of redistributing the work load among EDCs to improve both resource utilization and job response time. Load balancing also avoids a situation where some EDCs are heavily loaded while others are in idle state or doing little data processing. In such scenarios, load balancing between the EDCs plays a vital role for user response and real-time event detection. As the EDCs are deployed in an unattended environment, secure authentication of EDCs is an important issue to address before performing load balancing. This article proposes a novel load balancing technique to authenticate the EDCs and find less loaded EDCs for task allocation. The proposed load balancing technique is more efficient than other existing approaches in finding less loaded EDCs for task allocation. The proposed approach not only improves efficiency of load balancing; it also strengthens the security by authenticating the destination EDCs."
pub.1123793422,AIR: A Light-Weight Yet High-Performance Dataflow Engine based on Asynchronous Iterative Routing,"Distributed Stream Processing Systems (DSPSs) are among the currently most
emerging topics in data management, with applications ranging from real-time
event monitoring to processing complex dataflow programs and big data
analytics. The major market players in this domain are clearly represented by
Apache Spark and Flink, which provide a variety of frontend APIs for SQL,
statistical inference, machine learning, stream processing, and many others.
Yet rather few details are reported on the integration of these engines into
the underlying High-Performance Computing (HPC) infrastructure and the
communication protocols they use. Spark and Flink, for example, are implemented
in Java and still rely on a dedicated master node for managing their control
flow among the worker nodes in a compute cluster.
  In this paper, we describe the architecture of our AIR engine, which is
designed from scratch in C++ using the Message Passing Interface (MPI),
pthreads for multithreading, and is directly deployed on top of a common HPC
workload manager such as SLURM. AIR implements a light-weight, dynamic sharding
protocol (referred to as ""Asynchronous Iterative Routing""), which facilitates a
direct and asynchronous communication among all client nodes and thereby
completely avoids the overhead induced by the control flow with a master node
that may otherwise form a performance bottleneck. Our experiments over a
variety of benchmark settings confirm that AIR outperforms Spark and Flink in
terms of latency and throughput by a factor of up to 15; moreover, we
demonstrate that AIR scales out much better than existing DSPSs to clusters
consisting of up to 8 nodes and 224 cores."
pub.1144336208,Routing Protocol Security for Low-Power and Lossy Networks in the Internet of Things,"The Internet of Things (IoT) is a two-decade-old technology that allows the interconnection of various cyber-physical objects located anywhere in the world over the internet. A cyber-physical object is an inanimate object with the ability to connect itself to the internet and communicate with other similar objects. This communication is enabled by communication protocols that bridge the gap between hardware devices and the user by ensuring secure communication with optimum efficiency. As the number of IoT devices is increasing exponentially every year with the current estimation of more than 35 billion IoT devices according to a study conducted by findstack.com, there is a need for a lightweight and highly secure communication protocol for IoT devices. This creates a need for handling all the dynamic data exchanges in real-time. Hence, Big Data and IoT technologies are often proposed together for processing and analyzing data streams. Incorporating Big Data analytics in IoT applications enables real-time information transfer, data monitoring and processing easily. To facilitate efficient communication in IoT networks, the Routing Protocol for Low-Power and Lossy Networks (RPL) comes into the picture which is a low power consuming routing protocol that operates on IEEE 802.15. This paper aims to encrypt the RPL Protocol using cryptographic algorithms and compare the results with a non-encrypted RPL Protocol using Cooja Simulator so that the data is securely communicated for Big Data analysis. The (Secure Hash Algorithm) SHA3 algorithm had been used to encrypt the sink nodes of the RPL protocol and the parameters such as average power consumption, network graph, latency is analyzed and compared. The results of this experiment show a considerable drop in latency and power consumption after deploying cryptographic encryption."
pub.1129853373,Traffic Prediction Framework for OpenStreetMap using Deep Learning based Complex Event Processing and Open Traffic Cameras,"Displaying near-real-time traffic information is a useful feature of digital
navigation maps. However, most commercial providers rely on
privacy-compromising measures such as deriving location information from
cellphones to estimate traffic. The lack of an open-source traffic estimation
method using open data platforms is a bottleneck for building sophisticated
navigation services on top of OpenStreetMap (OSM). We propose a deep
learning-based Complex Event Processing (CEP) method that relies on publicly
available video camera streams for traffic estimation. The proposed framework
performs near-real-time object detection and objects property extraction across
camera clusters in parallel to derive multiple measures related to traffic with
the results visualized on OpenStreetMap. The estimation of object properties
(e.g. vehicle speed, count, direction) provides multidimensional data that can
be leveraged to create metrics and visualization for congestion beyond commonly
used density-based measures. Our approach couples both flow and count measures
during interpolation by considering each vehicle as a sample point and their
speed as weight. We demonstrate multidimensional traffic metrics (e.g. flow
rate, congestion estimation) over OSM by processing 22 traffic cameras from
London streets. The system achieves a near-real-time performance of 1.42
seconds median latency and an average F-score of 0.80."
pub.1131999482,AIR: A Light-Weight Yet High-Performance Dataflow Engine based on Asynchronous Iterative Routing,"Distributed Stream Processing Engines (DSPEs) are currently among the most emerging topics in data management, with applications ranging from real-time event monitoring to processing complex dataflow programs and big data analytics. In this paper, we describe the architecture of our AIR engine, which is designed from scratch in C++ using the Message Passing Interface (MPI), pthreads for multithreading, and is directly deployed on top of a common HPC workload manager such as SLURM. AIR implements a light-weight, dynamic sharding protocol (referred to as “Asynchronous Iterative Routing”), which facilitates a direct and asynchronous communication among all worker nodes and thereby completely avoids any additional communication overhead with a dedicated master node. With its unique design, AIR fills the gap between the prevalent scale-out (but Java-based) architectures like Apache Spark and Flink, on one hand, and recent scale-up (and C++ based) prototypes such as StreamBox and PiCo, on the other hand. Our experiments over various benchmark settings confirm that AIR performs as good as the best scale-up SPEs on a single-node setup, while it outperforms existing scale-out DSPEs in terms of processing latency and sustainable throughput by a factor of up to 15 in a distributed setting."
pub.1090576225,Register Pre-Allocation based Folded Discrete Tchebichef Transformation Technique for Image Compression,"Recently, the large size data, power and real-time processing abilities are major issues in Digital Signal Processing/multimedia applications which require an adaptable architecture. The tool used for computing data decorrelation in the image processing applications refers Discrete Tchebichef Transform (DTT) which offers better performance than the DCT due to its bitstream coding capabilities. This paper proposes a novel model of Discrete Tchebichef Transform (DTT) architecture with Register Pre-allocation based Folded Architecture (RPFA) for image compression. Through the cross-connection of folded architecture, the number of register usage is reduced. A Partial Cross Split Vedic Multiplier (PCSVM) method is introduced in the proposed DTT architecture. This multiplier design involves the cross function of the Vedic multiplier with the split pattern of multiplication binary stream. The optimal design of DTT architecture yields a minimum amount of FlipFlop (FF) counts, a latency and power consumption. The proposed PCSVM achieves higher Peak Signal to Noise Ratio (PSNR), better Structural Similarity Index (SSIM), lower delay, area, power consumption, Power-Delay Product (PDP), Mean Square Error (MSE) than the existing multiplier architectures. The proposed RPF-DTT architecture achieves a significant reduction in the resource consumption than the exact and approximate DTT architectures."
pub.1094217649,Backtrack-based Failure Recovery in Distributed Stream Processing,"Since stream analytics is treated as a kind of cloud service, there exists a pressing need for its reliability and fault-tolerance. In a streaming process, the parallel and distributed tasks are chained in a graph-structure with each task transforming a stream to a new stream; the transaction property guarantees the streaming data, called tuples, to be processed in the order of their generation in every dataflow path, with each tuple processed once and only once. The failure recovery of a task allows the previously produced results to be corrected for eventual consistency, which is different from the instant consistency of global state enforced by the failure recovery of general distributed systems, and therefore presents new technical challenges. Transactional stream processing typically requires every task to checkpoint its execution state, and when it is restored from a failure, to have the last state recovered from the checkpoint and missing tuple re-acquired and processed. Currently there exist two kind approaches: one treats the whole process as a single transaction, and therefore suffers from the loss of intermediate results during failures; the other relies on the receipt of acknowledgement (ACK) to decide whether moving forward to emit the next resulting tuple or resending the current one after timeout, on the per-tuple basis, thus incurs extremely high latency penalty. In contradistinction to the above, we propose the backtrack mechanism for failure recovery, which allows a task to process tuples continuously without waiting for ACKs and without resending tuples in the failure-free case, but to request (ASK) the source tasks to resend the missing tuples only when it is restored from a failure which is a rare case thus has limited impact on the overall performance. We have implemented the proposed mechanisms on Fontainebleau, the distributed stream analytics infrastructure we developed on top of Storm. As a principle, we ensure all the transactional properties to be system supported and transparent to users. Our experience shows that the ASK-based recovery mechanism significantly outperforms the ACK-based one."
pub.1136667553,OPTWEB: A Lightweight Fully Connected Inter-FPGA Network for Efficient Collectives,"Modern FPGA accelerators can be equipped with many high-bandwidth network I/Os, e.g., 64 x 50 Gbps, enabled by onboard optics or co-packaged optics. Some dozens of tightly coupled FPGA accelerators form an emerging computing platform for distributed data processing. However, a conventional indirect packet network using Ethernet's Intellectual Properties imposes an unacceptably large amount of the logic for handling such high-bandwidth interconnects on an FPGA. Besides the indirect network, another approach builds a direct packet network. Existing direct inter-FPGA networks have a low-radix network topology, e.g., 2-D torus. However, the low-radix network has the disadvantage of a large diameter and large average shortest path length that increases the latency of collectives. To mitigate both problems, we propose a lightweight, fully connected inter-FPGA network called OPTWEB for efficient collectives. Since all end-to-end separate communication paths are statically established using onboard optics, raw block data can be transferred with simple link-level synchronization. Once each source FPGA assigns a communication stream to a path by its internal switch logic between memory-mapped and stream interfaces for remote direct memory access (RDMA), a one-hop transfer is provided. Since each FPGA performs input/output of the remote memory access between all FPGAs simultaneously, multiple RDMAs efficiently form collectives. The OPTWEB network provides 0.71-μsec start-up latency of collectives among multiple Intel Stratix 10 MX FPGA cards with onboard optics. The OPTWEB network consumes 31.4 and 57.7 percent of adaptive logic modules for aggregate 400-Gbps and 800-Gbps interconnects on a custom Stratix 10 MX 2100 FPGA, respectively. The OPTWEB network reduces by 40 percent the cost compared to a conventional packet network."
pub.1061329931,A 128$\times$128 120 dB 15 $\mu$s Latency Asynchronous Temporal Contrast Vision Sensor,"This paper describes a 128$\times$128 pixel CMOS vision sensor. Each pixel independently and in continuous time quantizes local relative intensity changes to generate spike events. These events appear at the output of the sensor as an asynchronous stream of digital pixel addresses. These address-events signify scene reflectance change and have sub-millisecond timing precision. the output data rate depends on the dynamic content of the scene and is typically orders of magnitude lower than those of conventional frame-based imagers. By combining an active continuous-time front-end logarithmic photoreceptor with a self-timed switched-capacitor differencing circuit, the sensor achieves an array mismatch of 2.1% in relative intensity event threshold and a pixel bandwidth of 3 kHz under 1 klux scene illumination. Dynamic range is $> \,$120 dB and chip power consumption is 23mW. Event latency shows weak light dependency with a minimum of 15 $\mu$s at $> \,$1 klux pixel illumination. the sensor is built in a 0.35 $\mu$m 4M2P process. It has 40$\times $40$\ {\mu{\hbox{m}}}^{2}$ pixels with 9.4% fill factor. By providing high pixel bandwidth, wide dynamic range, and precisely timed sparse digital output, this silicon retina provides an attractive combination of characteristics for low-latency dynamic vision under uncontrolled illumination with low post-processing requirements."
pub.1152748412,Hand gesture recognition using 802.11ad mmWave sensor in the mobile device,"We explore the feasibility of AI assisted hand-gesture recognition using
802.11ad 60GHz (mmWave) technology in smartphones. Range-Doppler information
(RDI) is obtained by using pulse Doppler radar for gesture recognition. We
built a prototype system, where radar sensing and WLAN communication waveform
can coexist by time-division duplex (TDD), to demonstrate the real-time
hand-gesture inference. It can gather sensing data and predict gestures within
100 milliseconds. First, we build the pipeline for the real-time feature
processing, which is robust to occasional frame drops in the data stream. RDI
sequence restoration is implemented to handle the frame dropping in the
continuous data stream, and also applied to data augmentation. Second,
different gestures RDI are analyzed, where finger and hand motions can clearly
show distinctive features. Third, five typical gestures (swipe, palm-holding,
pull-push, finger-sliding and noise) are experimented with, and a
classification framework is explored to segment the different gestures in the
continuous gesture sequence with arbitrary inputs. We evaluate our architecture
on a large multi-person dataset and report > 95% accuracy with one CNN + LSTM
model. Further, a pure CNN model is developed to fit to on-device
implementation, which minimizes the inference latency, power consumption and
computation cost. And the accuracy of this CNN model is more than 93% with only
2.29K parameters."
pub.1137854781,Hand gesture recognition using 802.11ad mmWave sensor in the mobile device,"We explore the feasibility of AI assisted hand-gesture recognition using 802.11ad 60GHz (mmWave) technology in smartphones. Range-Doppler information (RDI) is obtained by using pulse Doppler radar for gesture recognition. We built a prototype system, where radar sensing and WLAN communication waveform can coexist by time-division duplex (TDD), to demonstrate the real-time hand-gesture inference. It can gather sensing data and predict gestures within 100 milliseconds. First, we build the pipeline for the real-time feature processing, which is robust to occasional frame drops in the data stream. RDI sequence restoration is implemented to handle the frame dropping in the continuous data stream, and also applied to data augmentation. Second, different gestures RDI are analyzed, where finger and hand motions can clearly show distinctive features. Third, five typical gestures (swipe, palm-holding, pull-push, finger-sliding and noise) are experimented with, and a classification framework is explored to segment the different gestures in the continuous gesture sequence with arbitrary inputs. We evaluate our architecture on a large multi-person dataset and report > 95% accuracy with one CNN + LSTM model. Further, a pure CNN model is developed to fit to on-device implementation, which minimizes the inference latency, power consumption and computation cost. And the accuracy of this CNN model is more than 93% with only 2.29K parameters."
pub.1017716873,The next database revolution,"Database system architectures are undergoing revolutionary changes. Most importantly, algorithms and data are being unified by integrating programming languages with the database system. This gives an extensible object-relational system where non-procedural relational operators manipulate object sets. Coupled with this, each DBMS is now a web service. This has huge implications for how we structure applications. DBMSs are now object containers. Queues are the first objects to be added. These queues are the basis for transaction processing and workflow applications. Future workflow systems are likely to be built on this core. Data cubes and online analytic processing are now baked into most DBMSs. Beyond that, DBMSs have a framework for data mining and machine learning algorithms. Decision trees, Bayes nets, clustering, and time series analysis are built in; new algorithms can be added. There is a rebirth of column stores for sparse tables and to optimize bandwidth. Text, temporal, and spatial data access methods, along with their probabilistic reasoning have been added to database systems. Allowing approximate and probabilistic answers is essential for many applications. Many believe that XML and xQuery will be the main data structure and access pattern. Database systems must accommodate that perspective. External data increasingly arrives as streams to be compared to historical data; so stream-processing operators are being added to the DBMS. Publish-subscribe systems invert the data-query ratios; incoming data is compared against millions of queries rather than queries searching millions of records. Meanwhile, disk and memory capacities are growing much faster than their bandwidth and latency, so the database systems increasingly use huge main memories and sequential disk access. These changes mandate a much more dynamic query optimization strategy - one that adapts to current conditions and selectivities rather than having a static plan. Intelligence is moving to the periphery of the network. Each disk and each sensor will be a competent database machine. Relational algebra is a convenient way to program these systems. Database systems are now expected to be self-managing, self-healing, and always-up. We researchers and developers have our work cut out for us in delivering all these features."
pub.1173701574,Multi-Stream Scheduling of Inference Pipelines on Edge Devices - a DRL Approach," Low-power edge devices equipped with Graphics Processing Units (GPUs) are a popular target platform for real-time scheduling of inference pipelines. Such application-architecture combinations are popular in Advanced Driver-Assistance Systems (ADAS) for aiding in the real-time decision-making of automotive controllers. However, the real-time throughput sustainable by such inference pipelines is limited by resource constraints of the target edge devices. Modern GPUs, both in edge devices and workstation variants, support the facility of concurrent execution of computation kernels and data transfers using the primitive of streams , also allowing for the assignment of priority to these streams. This opens up the possibility of executing computation layers of inference pipelines within a multi-priority, multi-stream environment on the GPU. However, manually co-scheduling such applications while satisfying their throughput requirement and platform memory budget may require an unmanageable number of profiling runs. In this work, we propose a Deep Reinforcement Learning (DRL) based method for deciding the start time of various operations in each pipeline layer while optimizing the latency of execution of inference pipelines as well as memory consumption. Experimental results demonstrate the promising efficacy of the proposed DRL approach in comparison with the baseline methods, particularly in terms of real-time performance enhancements, schedulability ratio, and memory savings. We have additionally assessed the effectiveness of the proposed DRL approach using a real-time traffic simulation tool IPG CarMaker. "
pub.1151404471,Fluid Batching: Exit-Aware Preemptive Serving of Early-Exit Neural Networks on Edge NPUs,"With deep neural networks (DNNs) emerging as the backbone in a multitude of
computer vision tasks, their adoption in real-world applications broadens
continuously. Given the abundance and omnipresence of smart devices in the
consumer landscape, ""smart ecosystems'' are being formed where sensing happens
concurrently rather than standalone. This is shifting the on-device inference
paradigm towards deploying centralised neural processing units (NPUs) at the
edge, where multiple devices (e.g. in smart homes or autonomous vehicles) can
stream their data for processing with dynamic rates. While this provides
enhanced potential for input batching, naive solutions can lead to subpar
performance and quality of experience, especially under spiking loads. At the
same time, the deployment of dynamic DNNs, comprising stochastic computation
graphs (e.g. early-exit (EE) models), introduces a new dimension of dynamic
behaviour in such systems. In this work, we propose a novel early-exit-aware
scheduling algorithm that allows sample preemption at run time, to account for
the dynamicity introduced both by the arrival and early-exiting processes. At
the same time, we introduce two novel dimensions to the design space of the NPU
hardware architecture, namely Fluid Batching and Stackable Processing Elements,
that enable run-time adaptability to different batch sizes and significantly
improve the NPU utilisation even at small batches. Our evaluation shows that
the proposed system achieves an average 1.97x and 6.7x improvement over
state-of-the-art DNN streaming systems in terms of average latency and tail
latency service-level objective (SLO) satisfaction, respectively."
pub.1121480415,"Efficient Sparse Matrix-Vector Multiplication on GPUs using the CSR Format, Pinned Memory and Overlap Data Transfer","The performance of sparse matrix vector multiplication (SpMV) is important to computational scientists. However, the SpMV on graphics processing units (GPUs) has poor performance due to irregular memory access patterns, load imbalance, and reduced parallelism. On the other hand, researchers who have tried to optimize the performance of SpMV using storage formats other than CSR (Compressed Storage Row), experienced extra time in the conversion between formats. we propose to optimize the performance of SpMV by reducing the latency of copying data between host and device, so we present CSR-Async, a new program that takes into account CSR-Vector for the kernel code in GPU and uses pinned memory for host vectors and makes asynchronous copies form host to device and vice verse making use of non-default streams and overlap data transfer. CSR-Async has better performance than CSR-Vector and CSR-Scalar, since it is 2.26 and 1.73 times faster respectively."
pub.1020535877,Verification of Printer Datapaths Using Timed Automata,"In multiprocessor systems with many data-intensive tasks, a bus may be among the most critical resources. Typically, allocation of bandwidth to one (high-priority) task may lead to a reduction of the bandwidth of other tasks, and thereby effectively slow down these tasks. WCET analysis for these types of systems is a major research challenge. In this paper, we show how the dynamic behavior of a memory bus and a USB in a realistic printer application can be faithfully modeled using timed automata. We analyze, using Uppaal, the worst case latency of scan jobs with uncertain arrival times in a setting where the printer is concurrently processing an infinite stream of print jobs."
pub.1160004486,Keynote: The Elastic AI Ecosystem — Towards A Holistic Pervasive System for Adaptive Artificial Intelligence,"The structure of pervasive application systems that use artificial intelligence (AI) is getting more complicated. On the one hand, AI is moving closer to its data sources, often sensors embedded in the environment or worn by users. Originally, sensors would stream their data to remote Cloud centers, where it would be processed and stored. In recent years, Edge systems have emerged, moving the AI-based processing of sensor data into the local environment of a pervasive system. Now, data is processed directly on the embedded sensor devices, allowing for tight control of data privacy and latency but at the same time forcing the AI to work with minimal compute power, memory, and energy. On the other hand, not everything can be done locally, either because the necessary AI models are too resource hungry or because they integrate data from many sources and locations. Therefore, it is often necessary and beneficial to connect local AI with Edge and Cloud AI and to let the system determine dynamically which part of the overall system should be executed where."
pub.1119514702,Efficient transmission of measurement data from FPGA to embedded system via Ethernet link,"This paper presents a system consisting of the FPGA IP core, the simple
network protocol and the Linux device driver, capable of efficient and reliable
data transmission from a low resources FPGA chip to the Linux-based embedded
computer system, via a private Ethernet network (consisting of a single segment
or a few segments connected via an Ethernet switch). The embedded system may
optionally process the acquired data, and distribute them further, using
standard network protocols. Proposed design targets cost-efficient multichannel
data acquisition systems, in which multiple FPGA based front end boards (FEB)
should transmit the stream of acquired data to the computer network,
responsible for their final processing and archiving. The presented solution
allows to minimize the cost of data concentration due to use of inexpensive
Ethernet network infrastructure. The work is mainly focused on minimization of
resources consumption in the FPGA, and minimization of acknowledge latency in
the Linux based system - which allows to achieve high throughput in spite of
use of inexpensive FPGA chips with small internal memory."
pub.1181497593,AI-powered threat detection in surveillance systems: A real-time data processing framework,"The increasing need for enhanced security has driven the adoption of AI-powered threat detection in surveillance systems. Traditional surveillance methods, reliant on manual monitoring, are often inefficient in detecting complex, evolving threats in real time. This review proposes a comprehensive real-time data processing framework for AI-powered threat detection in surveillance systems, designed to automate and optimize threat identification, classification, and response. The framework integrates AI algorithms, including machine learning and deep learning models, to analyze vast amounts of surveillance data from various sources such as video feeds, audio recordings, and sensor inputs. It utilizes techniques like object detection, facial recognition, and anomaly detection to identify potential threats, while leveraging stream processing frameworks (e.g., Apache Kafka, Apache Flink) to ensure low-latency, real-time analysis. Edge computing is incorporated to reduce network bottlenecks and enable faster decision-making closer to the data source. The framework also addresses the challenges of high data volume and velocity, as well as the need for scalable, flexible infrastructure. Security measures such as encryption, identity and access management (IAM), and compliance with data privacy regulations ensure that sensitive information is protected. The inclusion of continuous model training allows the system to adapt to emerging threats and reduce false positives and negatives. Case studies from urban environments, critical infrastructure, and law enforcement demonstrate the practical applications and effectiveness of this AI-driven approach. By integrating real-time data processing with advanced AI models, the framework provides a robust solution for improving the efficiency and accuracy of threat detection in modern surveillance systems. This research contributes to the growing field of AI-enhanced security, paving the way for future advancements in intelligent surveillance."
pub.1119129261,SAQL: A Stream-based Query System for Real-Time Abnormal System Behavior Detection,"Recently, advanced cyber attacks, which consist of a sequence of steps that
involve many vulnerabilities and hosts, compromise the security of many
well-protected businesses. This has led to the solutions that ubiquitously
monitor system activities in each host (big data) as a series of events, and
search for anomalies (abnormal behaviors) for triaging risky events. Since
fighting against these attacks is a time-critical mission to prevent further
damage, these solutions face challenges in incorporating expert knowledge to
perform timely anomaly detection over the large-scale provenance data.
  To address these challenges, we propose a novel stream-based query system
that takes as input, a real-time event feed aggregated from multiple hosts in
an enterprise, and provides an anomaly query engine that queries the event feed
to identify abnormal behaviors based on the specified anomalies. To facilitate
the task of expressing anomalies based on expert knowledge, our system provides
a domain-specific query language, SAQL, which allows analysts to express models
for (1) rule-based anomalies, (2) time-series anomalies, (3) invariant-based
anomalies, and (4) outlier-based anomalies. We deployed our system in NEC Labs
America comprising 150 hosts and evaluated it using 1.1TB of real system
monitoring data (containing 3.3 billion events). Our evaluations on a broad set
of attack behaviors and micro-benchmarks show that our system has a low
detection latency (<2s) and a high system throughput (110,000 events/s;
supporting ~4000 hosts), and is more efficient in memory utilization than the
existing stream-based complex event processing systems."
pub.1087285123,Stimulus Location is Processed Faster than Stimulus Colour,"There is a convergence of anatomical, electrophysiological, neuropsychological, and psychophysical data to support the dissociation of visual pathways into two main streams projecting from occipital to frontal cortex via the posterior parietal lobe (dorsal route) and via the inferotemporal lobe (ventral route). It is usually assumed that the dorsal route provides information that is useful for driving an action toward the stimulus (ie metric properties, such as localisation), whereas the ventral route extracts information useful for identifying it (ie intrinsic properties, such as colour). It is known that pointing movements can be reoriented to a novel target location within a short delay (about 110 ms), even when the target jump cannot be detected because of saccadic suppression. Electrophysiological studies have suggested that inputs to the dorsal pathway have a latency shorter than inputs to the ventral pathway. We compared latencies of visuomotor processing for colour and location during a pointing task. Target location and/or colour were altered upon movement onset. Instructions were to correct movement direction or to interrupt the movement according to the target change. We found that in both cases colour processing was slower (by about 100 ms) than location processing of the same target. Performance observed for identical movement speed was always higher for location responses whereas movement duration spontaneously chosen by subjects was longer when they had to process colour. Strikingly, corrections were also observed with the interruption instruction. We conclude that (1) colour is processed more slowly than location, and (2) automatic corrections can be observed prior to response inhibition for fast movements."
pub.1150988239,A Survey on Mobile Edge Computing for Video Streaming: Opportunities and Challenges,"5G communication brings substantial improvements in the quality of service
provided to various applications by achieving higher throughput and lower
latency. However, interactive multimedia applications (e.g., ultra high
definition video conferencing, 3D and multiview video streaming, crowd-sourced
video streaming, cloud gaming, virtual and augmented reality) are becoming more
ambitious with high volume and low latency video streams putting strict demands
on the already congested networks. Mobile Edge Computing (MEC) is an emerging
paradigm that extends cloud computing capabilities to the edge of the network
i.e., at the base station level. To meet the latency requirements and avoid the
end-to-end communication with remote cloud data centers, MEC allows to store
and process video content (e.g., caching, transcoding, pre-processing) at the
base stations. Both video on demand and live video streaming can utilize MEC to
improve existing services and develop novel use cases, such as video analytics,
and targeted advertisements. MEC is expected to reshape the future of video
streaming by providing ultra-reliable and low latency streaming (e.g., in
augmented reality, virtual reality, and autonomous vehicles), pervasive
computing (e.g., in real-time video analytics), and blockchain-enabled
architecture for secure live streaming. This paper presents a comprehensive
survey of recent developments in MEC-enabled video streaming bringing
unprecedented improvement to enable novel use cases. A detailed review of the
state-of-the-art is presented covering novel caching schemes, optimal
computation offloading, cooperative caching and offloading and the use of
artificial intelligence (i.e., machine learning, deep learning, and
reinforcement learning) in MEC-assisted video streaming services."
pub.1146282890,Online correlation for unlabeled process events: A flexible CEP-based approach,"Process mining is a sub-field of data mining that focuses on analyzing timestamped and partially ordered data. This type of data is commonly called event logs. Each event is required to have at least three attributes: case ID, task ID/name, and timestamp to apply process mining techniques. Thus, any missing information need to be supplied first. Traditionally, events collected from different sources are manually correlated. While this might be acceptable in an offline setting, this is infeasible in an online setting. Recently, several use cases have emerged that call for applying process mining in an online setting. In such scenarios, a stream of high-speed and high-volume events continuously flow, e.g. IoT applications, with stringent latency requirements to have insights about the ongoing process. Thus, event correlation must be automated and occur as the data is being received. We introduce an approach that correlates unlabeled events received on a stream. Given a set of start activities, our approach correlates unlabeled events to a case identifier. Our approach is probabilistic. That implies a single uncorrelated event can be assigned to zero or more case identifiers with different probabilities. Moreover, our approach is flexible. That is, the user can supply domain knowledge in the form of constraints that reduce the correlation space. This knowledge can be supplied while the application is running. We realize our approach using complex event processing (CEP) technologies. We implemented a prototype on top of Esper, a state of the art industrial CEP engine. We compare our approach to baseline approaches. The experimental evaluation shows that our approach outperforms the throughput and latency of the baseline approaches. It also shows that using real-life logs, the accuracy of our approach can compete with the baseline approaches."
pub.1117767437,Distributed Operator Placement for IoT Data Analytics Across Edge and Cloud Resources,"The number of Internet of Things applications is forecast to grow exponentially within the coming decade. Owners of such applications strive to make predictions from large streams of complex input in near real time. Cloud-based architectures often centralize storage and processing, generating high data movement overheads that penalize real-time applications. Edge and Cloud architecture pushes computation closer to where the data is generated, reducing the cost of data movements and improving the application response time. The heterogeneity among the edge devices and cloud servers introduces an important challenge for deciding how to split and orchestrate the IoT applications across the edge and the cloud. In this paper, we extend our IoT Edge Framework, called R-Pulsar, to propose a solution on how to split IoT applications dynamically across the edge and the cloud, allowing us to improve performance metrics such as end-to-end latency (response time), bandwidth consumption, and edge-to-cloud and cloud-to-edge messaging cost. Our approach consists of a programming model and real-world implementation of an IoT application. The results show that our approach can minimize the end-to-end latency by at least 38% by pushing part of the IoT application to the edge. Meanwhile, the edge-to-cloud data transfers are reduced by at least 38%, and the messaging costs are reduced by at least 50% when using the existing commercial edge cloud cost models."
pub.1166541934,FET-OPU: A Flexible and Efficient FPGA-Based Overlay Processor for Transformer Networks,"There are already some works on accelerating transformer networks with field-programmable gate array (FPGA). However, many accelerators focus only on attention computation or suffer from fixed data streams without flexibility. Moreover, their hardware performance is limited without schedule optimization and full use of hardware resources. In this article, we propose a flexible and efficient FPGA-based overlay processor, named FET-OPU. Specifically, we design an overlay architecture for general accelerations of transformer networks. We propose a unique matrix multiplication unit (MMU), which consists of a processing element (PE) array based on modified DSP-packing technology and a FIFO array for data caching and rearrangement. An efficient non-linear function unit (NFU) is also introduced, which can calculate arbitrary single input non-linear functions. We also customize an instruction set for our overlay architecture, dynamically controlling data flows by instructions generated on the software side. In addition, we introduce a two-level compiler and optimize the parallelism and memory allocation schedule. Experimental results show that our FET-OPU achieves 7.33-21.27× speedup and 231× less energy consumption compared with CPU, and 1.56-4.08× latency reduction with 5.85-66.36× less energy consumption compared with GPU. Furthermore, we observe 1.56-8.21× better latency and 5.28-6.24× less energy consumption compared with previously customized FPGA/ASIC accelerators and can be 2.05× faster than NPE with 5.55× less energy consumption."
pub.1107688563,PerceptRank: A Real-Time Learning to Rank Recommender System for Online Interactive Platforms,"In highly interactive platforms with continuous and frequent content creation and obsolescence, other factors besides relevance may alter users’ perceptions and choices. Besides, making personalized recommendations in these application domains imposes new challenges when compared to classic recommendation use cases. In fact, the required recommendation approaches should be able to ingest and process continuous streams of data online, at scale and with low latency while making context dependent dynamic suggestions. In this work, we propose a generic approach to deal jointly with scalability, real-time and cold start problems in highly interactive online platforms. The approach is based on several consumer decision-making theories to infer users’ preferences. In addition, it tackles the recommendation problem as a learning-to-rank problem that exploits a heterogeneous information graph to estimate users’ perceived value towards items. Although the approach is addressed to streaming environments, it has been validated in both offline batch and online streaming scenarios. The first evaluation has been carried out using the MovieLens dataset and the latter targeted the news recommendation domain using a high-velocity stream of usage data collected by a marketing company from several large scale online news portals. Experiments show that our proposition meets real world production environments constraints while delivering accurate suggestions and outperforming several state-of-the-art approaches."
pub.1129393932,TinTiN,"Cyber-Physical Systems (CPS) rely on data stream processing for high-throughput, low-latency analysis with correctness and accuracy guarantees (building on deterministic execution) for monitoring, safety or security applications. The trade-offs in processing performance and results' accuracy are nonetheless application-dependent. While some applications need strict deterministic execution, others can value fast (but possibly approximated) answers. Despite the existing literature on how to relax and trade strict determinism for efficiency or deadlines, we lack a formal characterization of levels of determinism, needed by industries to assess whether or not such trade-offs are acceptable. To bridge the gap, we introduce the notion of D-bounded eventual determinism, where D is the maximum out-of-order delay of the input data. We design and implement TinTiN, a streaming middleware that can be used in combination with user-defined streaming applications, to provably enforce D-bounded eventual determinism. We evaluate TinTiN with a real-world streaming application for Advanced Metering Infrastructure (AMI) monitoring, showing it provides an order of magnitude improvement in processing performance, while minimizing delays in output generation, compared to a state-of-the-art strictly deterministic solution that waits for time proportional to D, for each input tuple, before generating output that depends on it."
pub.1118865788,Real-time Data Acquisition and Processing System for MHz Repetition Rate Image Sensors,"One of the optimization goals of a particle accelerator is to reach the
highest possible beam peak current. For that to happen the electron bunch
propagating through the accelerator should be kept relatively short along the
direction of its travel. In order to obtain a better understanding of the beam
composition it is crucial to evaluate the electric charge distribution along
the micrometer-scale packets. The task of the Electro-Optic Detector (EOD) is
to imprint the beam charge profile on the spectrum of light of a laser pulse.
The actual measurement of charge distribution is then extracted with a
spectrometer based on a diffraction grating.
  The article focuses on developed data acquisition and processing system
called the High-speed Optical Line Detector (HOLD). It is a 1D image
acquisition system which solves several challenges related to capturing,
buffering, processing and transmitting large data streams with use of the FPGA
device. It implements a latency-optimized custom architecture based on the AXI
interfaces. The HOLD device is realized as an FPGA Mezzanine Card (FMC) carrier
with single High Pin-Count connector hosting the KIT KALYPSO detector.
  The solution presented in the paper is probably one of the world fastest line
cameras. Thanks to its custom architecture it is capable of capturing at least
10 times more frames per second than fastest comparable commercially available
devices."
pub.1002824487,GigE Vision Data Acquisition for Visual Servoing using SG/DMA Proxying,"In many domains such as robotics and industrial automation, a growing number of Control Applications utilize cameras as a sensor. Such Visual Servoing Systems increasingly rely on Gigabit Ethernet (GigE) as a communication backbone and require real-time execution. The implementation on small, low-power embedded platforms suitable for the respective domain is challenging in terms of both computation and communication. Whilst advances in CPU and Field Programmable Gate Array (FPGA) technology enable the implementation of computationally heavier Image Processing Pipelines, the interface between such platforms and an Ethernet-based communication backbone still requires careful design to achieve fast and deterministic Image Acquisition. Although standardized Ethernet-based camera protocols such as GigE Vision unify camera configuration and data transmission, traditional software-based Image Acquisition is insufficient on small, low-power embedded platforms due to tight throughput and latency constraints and the overhead caused by decoding such multi-layered protocols. In this paper, we propose Scatter-Gather Direct Memory Access (SG/DMA) Proxying as a generic method to seamlessly extend the existing network subsystem of current Systems-on-Chip (SoCs) with hardware-based filtering capabilities. Based thereon, we present a novel mixed-hardcore/softcore GigE Vision Framegrabber capable of directly feeding a subsequent in-stream Image Processing Pipeline with sub-microsecond acquisition latency. By rerouting all incoming Ethernet frames to our GigE Vision Bridge using SG/DMA Proxying, we are able to separate image and non-image data with zero CPU and memory intervention and perform Image Acquisition at full line rate of Gigabit Ethernet (i.e., 125 Mpx/s for grayscale video). Our experimental evaluation shows the benefits of our proposed architecture on a Programmable SoC (pSoC) that combines a fixed-function multi-core SoC with configurable FPGA fabric."
pub.1154201897,On the CPU Usage of Deep Learning Models on an Edge Device,"Applications that use the Internet of Things (IoT) capture massive amounts of raw data from sensors and actuators and frequently transmit this data to cloud data centers for processing and analysis. However, due to variable and unpredictable data generation rates and network latency, sending data to a cloud data center can result in a performance bottleneck. Data processing could occur at the network’s edge with the emergence of Fog and Edge computing-hosted microservices. Detecting and tracking objects from images, videos, and live streams are two of the fastest-growing computer vision applications increasingly being deployed at the edge. You Only Look Once (YOLO) models are highly optimized deep learning methods for object detection. This paper analyzes the CPU usage of four YOLO models on an edge device, an Nvidia Jetson Nano, at two different power budgets 5 and 10 W. Results show that the average CPU usage of the four YOLO models is low in 10 W power mode compared to 5 W power mode, except for YOLOv4-tiny. Furthermore, the number of frames per second processed by the four models remains relatively the same when switching from the 10 to 5 W power modes."
pub.1024967682,SpamWatcher,"The proliferation of mobile devices, coupled with continuous connectivity, has resulted in a world where massive amounts of data is being produced, on a daily basis, as a result of online interactions between people. These interactions are often captured as relationships in a social network graph, by service providers such as mobile carriers or social web applications. Social network analysis is becoming a common technique for extracting business intelligence from social network graphs in order to improve customer experience and provide better service. Some applications in this domain require processing massive data flows with high throughput and low-latency, in order to deliver timely results. SpamWatcher is a streaming social network analysis application that fits this description. It is used for real-time filtering of short messages in mobile communications, with the goal of preventing spam. The ever increasing volume of mobile users and rates of messages make realtime detection of spam a challenging problem with respect to performance and scalability. In this paper, we present a solution for the SpamWatcher application using the IBM wire-speed processor - a system-on-a-chip with specialized co-processors and integrated network I/O. This solution goes beyond the state-of-the-art by (i) using a novel implementation technique that takes advantage of the pattern matching accelerator to minimize the latency of spam detection, and (ii) employing hardware primitives to reduce the overhead caused by thread synchronization in order to achieve good scalability with respect to number of cores used. Furthermore, the solution is implemented on System S - a commercial-grade stream processing middleware. We evaluate our approach using real-world data sets and experimentally demonstrate the substantial performance improvements it achieves compared to previously published results."
pub.1138967769,Dynamic Diverse Summarisation in Heterogeneous Graph Streams: a Comparison between Thesaurus/Ontology-based and Embeddings-based Approaches,"Nowadays, there is a lot of attention drawn in smart environments, like Smart Cities and Internet of Things. These environments generate data streams that could be represented as graphs, which can be analysed in real-time to satisfy user or application needs. The challenges involved in these environments, ranging from the dynamism, heterogeneity, continuity, and high-volume of these real-world graph streams create new requirements for graph processing algorithms. We propose a dynamic graph stream summarisation system with the use of embeddings that provides expressive graphs while ensuring high usability and limited resource usage. In this paper, we examine the performance comparison between our embeddings-based approach and an existing thesaurus/ontology-based approach (FACES) that we adapted in a dynamic environment with the use of windows and data fusion. Both approaches use conceptual clustering and top-k scoring that can result in expressive, dynamic graph summaries with limited resources. Evaluations show that sending top-k fused diverse summaries, results in 34% to 92% reduction of forwarded messages and redundancy-awareness with an F-score ranging from 0.80 to 0.95 depending on the k compared to sending all the available information without top-k scoring. Also, the summaries' quality follows the agreement of ideal summaries determined by human judges. The summarisation approaches come with the expense of reduced system performance. The thesaurus/ontology-based approach proved 6 times more latency-heavy and 3 times more memory-heavy compared to the most expensive embeddings-based approach while having lower throughput but provided slightly better quality summaries."
pub.1174579066,FlexSP:(1 + β)-Choice based Flexible Stream Partitioning for Stateful Operators,"Stream partitioning has a fundamental effect on the efficiency of data parallelism in distributed stream processing systems. The skewed and time-varying nature of streaming data makes it challenging to achieve load balancing while minimizing the cost incurred. The requirement of adaptivity further complicates the problem, that the partitioning mechanism should not only be able to capture the changes in workload and adjust itself but also be quite tolerant of the changes because of the lag in statistics. Existing approaches use one-choice or multiple-choice schemes to make tradeoffs between these factors, but they tend to treat them as opposites, which either fails to achieve good load balancing or incurs excessive cost. There is a lack of deeper insight into how partitioning behavior affects load balancing, cost, and adaptivity when the keys have a different number of candidate choices. Also, it requires a flexible partitioning scheme to allow different trade-offs among the three factors for various scenarios. To address the issues mentioned above, we propose a novel (1 + β)-choice based stream partitioning scheme, which splits β ∈ (0, 1) part of keys selectively to have multiple candidate choices. We demonstrate that just splitting β part of the keys is sufficient to achieve optimal load balancing while minimizing cost and providing the required adaptivity to workload variance. In a new perspective, we analyze the relationship among load balancing, cost, and adaptivity, as the theoretical foundation of getting proper β and the corresponding number of choices. Experiments on Apache Flink demonstrate that our approach outperforms state-of-the-art solutions, improving throughput by 7.3 × and reducing latency by 85%."
pub.1139575643,Zeph: Cryptographic Enforcement of End-to-End Data Privacy,"As increasingly more sensitive data is being collected to gain valuable
insights, the need to natively integrate privacy controls in data analytics
frameworks is growing in importance. Today, privacy controls are enforced by
data curators with full access to data in the clear. However, a plethora of
recent data breaches show that even widely trusted service providers can be
compromised. Additionally, there is no assurance that data processing and
handling comply with the claimed privacy policies. This motivates the need for
a new approach to data privacy that can provide strong assurance and control to
users. This paper presents Zeph, a system that enables users to set privacy
preferences on how their data can be shared and processed. Zeph enforces
privacy policies cryptographically and ensures that data available to
third-party applications complies with users' privacy policies. Zeph executes
privacy-adhering data transformations in real-time and scales to thousands of
data sources, allowing it to support large-scale low-latency data stream
analytics. We introduce a hybrid cryptographic protocol for privacy-adhering
transformations of encrypted data. We develop a prototype of Zeph on Apache
Kafka to demonstrate that Zeph can perform large-scale privacy transformations
with low overhead."
pub.1154093698,Edge Computing in Micro Data Centers for Firefighting in Residential Areas of Future Smart Cities,"5G standardization is going to reach its end, so research on 6G has started, driven by the scientific and industrial communities. 5G and especially 6G, are going to provide resources to enhance every aspect of human life via communication networks and computing. Among the different verticals, emergency services are one of the most important parts of making smart cities of the future a reality. In this context, firefighting is highly important for security and safety. However, firefighting requires ultra-reliable and low-latency communications since firefighters can be provided with advanced guidance during fire control with the employment of distributed sensors, robots, etc. Also, the use of machine learning algorithms is important for firefighters to analyze and make decisions based on the audiovisual and possibly tactile information they collect. Such a scenario cannot leverage the current cloud computing paradigm, which has significant latency issues. That is why it is important to study and design edge computing paradigms to address the goals of these scenarios and use cases since they can be capable of fulfilling latency and reliability requirements. In this situation, this work looks into how computing at Edge Micro Data Centers (EMDC) can be used to improve how fires are predicted and managed. We propose a novel three-stage architecture. The initial stage focuses on prediction and classification of the fire occurrence based on available sensor data at the EMDC, whereas the second stage deals with the fire occurrence confirmation using a convolutional neural network (CNN) classification model. After the fire occurrence has been confirmed, the final stage notifies the tenants and streams 360-degree monitoring video to the nearby fire station after processing at EMDC. The results showed that the proposed architecture can realize firefighting services with low latency. to the best of the authors' knowledge, this is the first work studying and experimentally evaluating this communication scenario by also involving prediction via intelligence."
pub.1015011662,Manycore GPU processing of repeated range queries over streams of moving objects observations,"Summary The ability to timely process significant amounts of continuously updated spatial data is mandatory for an increasing number of applications. Parallelism enables such applications to face this data‐intensive challenge and allows the devised systems to feature low latency and high scalability. In this paper, we focus on a specific data‐intensive problem concerning the repeated processing of huge amounts of range queries over massive sets of moving objects, where the spatial extent of queries and objects is continuously modified over time. To tackle this problem and significantly accelerate query processing, we devise a hybrid CPU/GPU pipeline that compresses data output and saves query processing work. The devised system relies on an ad‐hoc spatial index leading to a problem decomposition that results in a set of independent data‐parallel tasks. The index is based on a point‐region quadtree space decomposition and allows to tackle effectively a broad range of spatial object distributions, even those very skewed. Also, to deal with the architectural peculiarities and limitations of the GPUs, we adopt non‐trivial GPU data structures that avoid the need of locked memory accesses while favouring coalesced memory accesses, thus enhancing the overall memory throughput. To the best of our knowledge, this is the first work that exploits GPUs to efficiently solve repeated range queries over massive sets of continuously moving objects, possibly characterized by highly skewed spatial distributions. In comparison with state‐of‐the‐art CPU‐based implementations, our method highlights significant speedups in the order of 10 − 20×, depending on the dataset. Copyright © 2016 John Wiley & Sons, Ltd."
pub.1181825252,A Machine Learning-Driven Wireless System for Structural Health Monitoring,"The paper presents a wireless system integrated with a machine learning (ML)
model for structural health monitoring (SHM) of carbon fiber reinforced polymer
(CFRP) structures, primarily targeting aerospace applications. The system
collects data via carbon nanotube (CNT) piezoresistive sensors embedded within
CFRP coupons, wirelessly transmitting these data to a central server for
processing. A deep neural network (DNN) model predicts mechanical properties
and can be extended to forecast structural failures, facilitating proactive
maintenance and enhancing safety. The modular design supports scalability and
can be embedded within digital twin frameworks, offering significant benefits
to aircraft operators and manufacturers. The system utilizes an ML model with a
mean absolute error (MAE) of 0.14 on test data for forecasting mechanical
properties. Data transmission latency throughout the entire system is less than
one second in a LAN setup, highlighting its potential for real-time monitoring
applications in aerospace and other industries. However, while the system shows
promise, challenges such as sensor reliability under extreme environmental
conditions and the need for advanced ML models to handle diverse data streams
have been identified as areas for future research."
pub.1173390683,Runtime Instrumentation for Reactive Components (Extended Version),"Reactive software calls for instrumentation methods that uphold the reactive
attributes of systems. Runtime verification imposes another demand on the
instrumentation, namely that the trace event sequences it reports to monitors
are sound -- that is, they reflect actual executions of the system under
scrutiny. This paper presents RIARC, a novel decentralised instrumentation
algorithm for outline monitors meeting these two demands. The asynchronous
setting of reactive software complicates the instrumentation due to potential
trace event loss or reordering. RIARC overcomes these challenges using a
next-hop IP routing approach to rearrange and report events soundly to
monitors.
  RIARC is validated in two ways. We subject its corresponding implementation
to rigorous systematic testing to confirm its correctness. In addition, we
assess this implementation via extensive empirical experiments, subjecting it
to large realistic workloads to ascertain its reactiveness. Our results show
that RIARC optimises its memory and scheduler usage to maintain latency
feasible for soft real-time applications. We also compare RIARC to inline and
centralised monitoring, revealing that it induces comparable latency to inline
monitoring in moderate concurrency settings, where software performs
long-running, computationally-intensive tasks, such as in Big Data stream
processing."
pub.1101187684,A low-latency optical switch architecture using integrated μm SOI-based contention resolution and switching,"The urgent need for high-bandwidth and high-port connectivity in Data Centers has boosted the deployment of optoelectronic packet switches towards bringing high data-rate optics closer to the ASIC, realizing optical transceiver functions directly at the ASIC package for high-rate, low-energy and low-latency interconnects. Even though optics can offer a broad range of low-energy integrated switch fabrics for replacing electronic switches and seamlessly interface with the optical I/Os, the use of energy- and latency-consuming electronic SerDes continues to be a necessity, mainly dictated by the absence of integrated and reliable optical buffering solutions. SerDes undertakes the role of optimally synergizing the lower-speed electronic buffers with the incoming and outgoing optical streams, suggesting that a SerDes-released chip-scale optical switch fabric can be only realized in case all necessary functions including contention resolution and switching can be implemented on a common photonic integration platform. In this paper, we demonstrate experimentally a hybrid Broadcast-and-Select (BS) / wavelength routed optical switch that performs both the optical buffering and switching functions with μm-scale Silicon-integrated building blocks. Optical buffering is carried out in a silicon-integrated variable delay line bank with a record-high on-chip delay/footprint efficiency of 2.6ns/mm2 and up to 17.2 nsec delay capability, while switching is executed via a BS design and a silicon-integrated echelle grating, assisted by SOA-MZI wavelength conversion stages and controlled by a FPGA header processing module. The switch has been experimentally validated in a 3x3 arrangement with 10Gb/s NRZ optical data packets, demonstrating error-free switching operation with a power penalty of <5dB."
pub.1128873880,Intrusion Detection in Industrial Networks via Data Streaming,"Given the increasing threat surface of industrial networks due to distributed, Internet-of-Things (IoT) based system architectures, detecting intrusions in Industrial IoT (IIoT) systems is all the more important, due to the safety implications of potential threats. The continuously generated data in such systems form both a challenge but also a possibility: data volumes/rates are high and require processing and communication capacity but they contain information useful for system operation and for detection of unwanted situations.In this chapter we explain that stream processing (a.k.a. data streaming) is an emerging useful approach both for general applications and for intrusion detection in particular, especially since it can enable data analysis to be carried out in the continuum of edge-fog-cloud distributed architectures of industrial networks, thus reducing communication latency and gradually filtering and aggregating data volumes. We argue that usefulness stems also due to facilitating provisioning of agile responses, i.e. due to potentially smaller latency for intrusion detection and hence also improved possibilities for intrusion mitigation. In the chapter we outline architectural features of IIoT networks, potential threats and examples of state-of-the art intrusion detection methodologies. Moreover, we give an overview of how leveraging distributed and parallel execution of streaming applications in industrial setups can influence the possibilities of protecting these systems. In these contexts, we give examples using electricity networks (a.k.a. Smart Grid systems).We conclude that future industrial networks, especially their Intrusion Detection Systems (IDSs), should take advantage of data streaming concept by decoupling semantics from the deployment."
pub.1130161419,Data-Centric Distributed Computing on Networks of Mobile Devices,"In the last few years, we have seen a significant increase both in the number and capabilities of mobile devices, as well as in the number of applications that need more and more computing and storage resources. Currently, in order to deal with this growing need for resources, applications make use of cloud services. This raises some problems, namely high latency, considerable use of energy and bandwidth, and the unavailability of connectivity infrastructures. Given this context, for some applications it makes sense to do part, or all, of the computations locally on the mobile devices themselves. In this paper we present Oregano, a framework for distributed computing on mobile devices, capable of processing batches or streams of data generated on mobile device networks, without requiring centralized services. Contrary to current state-of-the-art, where computations and data are sent to worker mobile devices, Oregano performs computations where the data is located, significantly reducing the amount of exchanged data."
pub.1173736171,End-to-End Orchestration of NextG Media Services over the Distributed Compute Continuum,"NextG (5G and beyond) networks, through the increasing integration of
cloud/edge computing technologies, are becoming highly distributed compute
platforms ideally suited to host emerging resource-intensive and
latency-sensitive applications (e.g., industrial automation, extended reality,
distributed AI). The end-to-end orchestration of such demanding applications,
which involves function/data placement, flow routing, and joint
communication/computation/storage resource allocation, requires new models and
algorithms able to capture: (i) their disaggregated microservice-based
architecture, (ii) their complex processing graph structures, including
multiple-input multiple-output processing stages, and (iii) the opportunities
for efficiently sharing and replicating data streams that may be useful for
multiple functions and/or end users. To this end, we first identify the
technical gaps in existing literature that prevent efficiently addressing the
optimal orchestration of emerging applications described by information-aware
directed acyclic graphs (DAGs). We then leverage the recently proposed Cloud
Network Flow optimization framework and a novel functionally-equivalent
DAG-to-Forest graph transformation procedure to design IDAGO (Information-Aware
DAG Orchestration), a polynomial-time multi-criteria approximation algorithm
for the optimal orchestration of NextG media services over NextG
compute-integrated networks."
pub.1144089357,ELIχR: Eliminating Computation Redundancy in CNN-Based Video Processing,"Video processing frequently relies on applying convolutional neural networks (CNNs) for various tasks, including object tracking, real-time action classification, and image recognition. Due to complicated network design, processing even a single frame requires many operations, leading to low throughput and high latency. This process can be parallelized, but since consecutive images have similar content, most of these operations produce identical results, leading to inefficient usage of parallel hardware accelerators. In this paper, we present ELIχR, a software system that systematically addresses this computation redundancy problem in an architecture-independent way, using two key techniques. First, ELIχR implements a lightweight change propagation algorithm to automatically determine which data to recompute for each new frame based on changes in the input. Second, ELIχR implements a dynamic check to further reduce needed computations by leveraging special operators in the model (e.g., ReLU), and trading off accuracy for performance. We evaluate ELIχR on two real-world models, Inception V3 and Resnet-50, and two video streams. We show that ELIχR running on the CPU produces up to 3.49X speedup (1.76X on average) compared with frame sampling, given the same accuracy and real-time processing requirements, and we describe how our approach can be applied in an architecture-independent way to improve CNN performance in heterogeneous systems."
pub.1095540521,Accelerating H.264/HEVC Video Slice Processing Using Application Specific Instruction Set Processor,"Video coding standards (e.g. H.264, HEVC) use slice, consisting of a header and payload video data, as an independent coding unit for low latency encode-decode and better transmission error resiliency. In typical video streams, decoding the slice header is quite simple that can be done on standard embedded RISC processor architectures. However, universal decoding scenarios require handling worst case slice header complexity that grows to un-manageable level, well beyond the capacity of most embedded RISC processors. Hardwiring of slice processing control logic is potentially helpful but it reduces flexibility to tune the decoder for error conditions – an important differentiator for the end user. The paper presents a programmable approach to accelerate slice header decoding using an Application Specific Instruction Set Processor (ASIP). Purpose built instructions, built as extensions to a RISC processor (ARP32), accelerate slice processing by 30% for typical cases, reaching up to 70% for slices with worst case decoding complexity. The approach enables real time universal video decode for all slice-complexity-scenarios without sacrificing the flexibility, adaptability to customize, differentiate the codec solution via software programmability."
pub.1093802377,Active replication at (almost) no cost,"MapReduce has become a popular programming paradigm in the domain of batch processing systems. Its simplicity allows applications to be highly scalable and to be easily deployed on large clusters. More recently, the Map Reduce approach has been also applied to Event Stream Processing (ESP) systems. This approach, which we call StreamMapReduce, enabled many novel applications that require both scalability and low latency. Another recent trend is to move distributed applications to public clouds such as Amazon EC2 rather than running and maintaining private data centers. Most cloud providers charge their customers on an hourly basis rather than on CPU cycles consumed. However, many applications, especially those that process online data, need to limit their CPU utilization to conservative levels (often as low as 50%) to be able to accommodate natural and sudden load variations without causina unacceptable deterioration in responsiveness. In this paper, we present a new fault tolerance approach based on active replication for StreamMapReduce systems. This approach is cost effective for cloud consumers as well as cloud providers. Cost effectiveness is achieved by fully utilizing the acquired computational resources without performance degradation and by reducing the need for additional nodes dedicated to fault tolerance."
pub.1133873871,Scalable Data-Intensive Geocomputation: A Design for Real-Time Continental Flood Inundation Mapping,"The convergence of data-intensive and extreme-scale computing behooves an integrated software and data ecosystem for scientific discovery. Developments in this realm will fuel transformative research in data-driven interdisciplinary domains. Geocomputation provides computing paradigms in Geographic Information Systems (GIS) for interactive computing of geographic data, processes, models, and maps. Because GIS is data-driven, the computational scalability of a geocomputation workflow is directly related to the scale of the GIS data layers, their resolution and extent, as well as the velocity of the geo-located data streams to be processed. Geocomputation applications, which have high user interactivity and low end-to-end latency requirements, will dramatically benefit from the convergence of high-end data analytics (HDA) and high-performance computing (HPC). In an application, we must identify and eliminate computational bottlenecks that arise in a geocomputation workflow. Indeed, poor scalability at any of the workflow components is detrimental to the entire end-to-end pipeline. Here, we study a large geocomputation use case in flood inundation mapping that handles multiple national-scale geospatial datasets and targets low end-to-end latency. We discuss the benefits and challenges for harnessing both HDA and HPC for data-intensive geospatial data processing and intensive numerical modeling of geographic processes. We propose an HDA+HPC geocomputation architecture design that couples HDA (e.g., Spark)-based spatial data handling and HPC-based parallel data modeling. Key techniques for coupling HDA and HPC to bridge the two different software stacks are reviewed and discussed."
pub.1146343235,A Noise Filter for Dynamic Vision Sensor Based on Spatiotemporal Correlation and Hot Pixel Detection,"As a new type of neuromorphic sensor, the events-driven Dynamic Vision Sensor (DVS) has the advantages of low latency and wide dynamic range. The development of DVS overcomes the application limitations of traditional cameras in the field of high speed robots and autonomous driving. However, there are abundant unwanted noise events contained in the output of DVS, which severely affects the subsequent data-processing. To tackle the problem, a novel noise filter for DVS is proposed in this paper. Firstly, since the noise is composed of hot pixel events with high-frequency and random background activities, a time window is constructed to continuously detect hot pixels. Secondly, a spatiotemporal correlation-based denoising approach for each incoming event is conducted, while excluding its surrounding hot pixels, to avoid the negative effect caused by their high temporal correlation on the filter. Finally, the hot pixels are compensated with the events that are most probably to occur depending on the adjacent events, to achieve the completeness and precision of the output event stream. Experiments in different scenes demonstrate that the proposed noise filter can effectively eliminate the noise from the event stream of DVS and obviously outperforms the baseline method."
pub.1142256743,Review of PMU Algorithms Suitable for Real-Time Operation With Digital Sampled Value Data,"Phasor Measurement Unit (PMU) instruments are continuously evolving to reflect the needs of electrical grids for enhanced and more accurate monitoring of the AC signal parameters, and to contribute towards optimization of real-time control tasks. In turn, enhanced monitoring can contribute to a more stable and reliable power supply. PMU standards are also being updated to reflect the latest performance requirements PMUs and new technological developments. As described in standard IEC/IEEE 60255-118-1, a PMU may receive streams of timestamped digital samples using the IEC 61850-9-2 Sample Values (SV) protocol, instead of traditional analog signals. This means that the signal conditioning and sampling parts can be located at a different location from the computational unit. This approach requires remote processing, so additional time delay is introduced that affects the reporting latency of the PMU. Therefore, not all the PMU algorithms presented in the literature will be valid candidates to operate with SV data and at the same time comply with reporting latency requirements. To address this issue, the paper presents a literature review of PMU algorithms, to identify algorithms which are suitable for real-time operation with the SV data protocol. Among many proposed PMU algorithms, only a few of them can estimate synchrophasor, frequency, and rate of change of frequency (ROCOF) on a sample per sample basis. Recommendations on selecting PMU algorithms for hardware implementation that complies with SV data has been also provided."
pub.1171515925,Edge-Cloud Collaborative Streaming Video Analytics with Multi-agent Deep Reinforcement Learning,"Streaming video analytics focuses on the real-time analysis of streaming video data from multiple resources, such as security cameras, and IoT devices with video capabilities. It involves applications of various techniques to extract valuable information from live video streams. Edge computing and cloud computing facilitate video stream analytics by utilizing computation resources across both ends, enabling both high accuracy and low latency. However, video streaming behaviours are dynamic and constantly evolving across the edge and the cloud. The network conditions, computing resources, and video content can change rapidly, making it crucial to continuously adjust the analytics methods to provide accurate results. Previous works both based on deep neural networks (DNNs) or heuristic algorithms learn a suitable deployment plan for streaming video analytics applications from historical data or synthetic data and therefore are not able to capture the dynamics. Hence, we propose reinforcement learning-based methods that can adapt to ongoing changes in video streaming behaviours. To ensure the scalability of video analytics in distributed environments, we implement OSMOTICGATE2, a distributed streaming video analytics system that features optimized processing pipelines and multi-agent RL-based controllers for fast adapting the system configurations across the edge and the cloud. Experiments on a real testbed show that our method outperforms baselines, assuring real-time video analysis and high accuracy in dynamic and distributed environments."
pub.1130079700,Introduction to Dynamic Stochastic Computing,"Stochastic computing (SC) is an old but reviving computing paradigm for its simple data path that can perform various arithmetic operations. It allows for low power implementation, which would otherwise be complex using the conventional positional binary coding. In SC, a number is encoded by a random bit stream of 0s and 1s with an equal weight for every bit. However, a long bit stream is usually required to achieve a high accuracy. This requirement inevitably incurs a long latency and high energy consumption in an SC system. In this article, we present a new type of stochastic computing that uses dynamically variable bit streams, which is, therefore, referred to as dynamic stochastic computing (DSC). In DSC, a random bit is used to encode a single value from a digital signal. A sequence of such random bits is referred to as a dynamic stochastic sequence. Using a stochastic integrator, DSC is well suited for implementing accumulation-based iterative algorithms such as numerical integration and gradient descent. The underlying mathematical models are formulated for functional analysis and error estimation. A DSC system features a higher energy efficiency than conventional computing using a fixed-point representation with a power consumption as low as conventional SC. It is potentially useful in a broad spectrum of applications including signal processing, numerical integration and machine learning."
pub.1100679054,Automatic Scaling of Resources in a Storm Topology,"In the Big Data era, the batch processing of large volumes of data is simply not enough - data needs to be processed fast to support continuous reactions to changing conditions in real-time. Distributed stream processing systems have emerged as platforms of choice for applications that rely on real-time analytics, with Apache Storm [2] being one of the most prevalent representatives. Whether deployed on physical or virtual infrastructures, distributed stream processing systems are expected to make the most out of the available resources, i.e., achieve the highest throughput or lowest latency with the minimum resource utilisation. However, for Storm - as for most such systems - this is a cumbersome trial-and-error procedure, tied to the specific workload that needs to be processed and requiring manual tweaking of resource-related topology parameters. To this end, we propose ARiSTO, a system that automatically decides on the appropriate amount of resources to be provisioned for each node of the Storm workflow topology based on user-defined performance and cost constraints. ARiSTO employs two mechanisms: a static, model-based one, used at bootstrap time to predict the resource-related parameters that better fit the user needs and a dynamic, rule-based one that elastically auto-scales the allocated resources in order to maintain the desired performance even under changes in load. The experimental evaluation of our prototype proves the ability of ARiSto to efficiently decide on the resource-related configuration parameters, maintaining the desired throughput at all times."
pub.1170173888,Predictive and Near-Optimal Sampling for View Materialization in Video Databases,"Scalable video query optimization has re-emerged as an attractive research topic in recent years. The OTIF system, a video database with cutting-edge efficiency, has introduced a new paradigm of utilizing view materialization to facilitate online query processing. Specifically, it stores the results of multi-object tracking queries to answer common video queries with sub-second latency. However, the cost associated with view materialization in OTIF is prohibitively high for supporting large-scale video streams. In this paper, we study efficient MOT-based view materialization in video databases. We first conduct a theoretical analysis and establish two types of optimality measures that serve as lower bounds for video frame sampling. In order to minimize the number of processed video frames, we propose a novel predictive sampling framework, namely LEAP, exhibits near-optimal sampling performance. Its efficacy relies on a data-driven motion manager that enables accurate trajectory prediction, a compact object detection model via knowledge distillation, and a robust cross-frame associator to connect moving objects in two frames with a large time gap. Extensive experiments are conducted in 7 real datasets, with 7 baselines and a comprehensive query set, including selection, aggregation and top-k queries. The results show that with comparable query accuracy to OTIF, our LEAP can reduce the number of processed video frames by up to 9× and achieve 5× speedup in query processing time. Moreover, LEAP demonstrates impressive throughput when handling large-scale video streams, as it leverages a single NVIDIA RTX 3090ti GPU to support real-time MOT-based view materialization from 160 video streams simultaneously."
pub.1106923177,Join query optimization techniques for complex event processing applications,"Complex event processing (CEP) is a prominent technology used in many modern applications for monitoring and tracking events of interest in massive data streams. CEP engines inspect real-time information flows and attempt to detect combinations of occurrences matching predefined patterns. This is done by combining basic data items, also called ""primitive events"", according to a pattern detection plan, in a manner similar to the execution of multi-join queries in traditional data management systems. Despite this similarity, little work has been done on utilizing existing join optimization methods to improve the performance of CEP-based systems.
                  In this paper, we provide the first theoretical and experimental study of the relationship between these two research areas. We formally prove that the CEP Plan Generation problem is equivalent to the Join Query Plan Generation problem for a restricted class of patterns and can be reduced to it for a considerably wider range of classes. This result implies the NP-completeness of the CEP Plan Generation problem. We further show how join query optimization techniques developed over the last decades can be adapted and utilized to provide practically efficient solutions for complex event detection. Our experiments demonstrate the superiority of these techniques over existing strategies for CEP optimization in terms of throughput, latency, and memory consumption."
pub.1150768940,Real-time Radar Gesture Classification with Spiking Neural Network on SpiNNaker 2 Prototype,"Neuromorphic hardware has been emerging in recent years, seeking various applications to explore its uniqueness, limitations, and possibilities. As a representative application and research area, gesture recognition is gaining wider popularity, while the conflict of spiking neural network (SNN) size and available memory of neuromorphic edge-AI can be a thorny issue, which is even intensified by the demand for continuously processing input data stream from the sensor in the real-world scenario since a certain amount of memory is required to ensure that no data loss or overwrite happens. In this paper, an SNN-based real-time radar gesture recognition closed-loop system is proposed, with Infineon's 60 GHz radar continuously capturing motion and the multi-core neuromorphic hardware SpiNNaker 2 serving as the backend to classify gestures with an SNN. A PC is used to preprocess the data and manipulate an actuator. This system requires less than 8 k operation cycles per processor for each radar frame and achieves a classification accuracy of 98.83% with an 8-bit quantized model, with only 134.2 kB memory usage on three processing elements (PEs) and low energy cost. Besides, it performs well in the real-time closed-loop test with 35 ms latency."
pub.1118955939,Join Query Optimization Techniques for Complex Event Processing Applications,"Complex event processing (CEP) is a prominent technology used in many modern
applications for monitoring and tracking events of interest in massive data
streams. CEP engines inspect real-time information flows and attempt to detect
combinations of occurrences matching predefined patterns. This is done by
combining basic data items, also called primitive events, according to a
pattern detection plan, in a manner similar to the execution of multi-join
queries in traditional data management systems. Despite this similarity, little
work has been done on utilizing existing join optimization methods to improve
the performance of CEP-based systems. In this paper, we provide the first
theoretical and experimental study of the relationship between these two
research areas. We formally prove that the CEP Plan Generation problem is
equivalent to the Join Query Plan Generation problem for a restricted class of
patterns and can be reduced to it for a considerably wider range of classes.
This result implies the NP-completeness of the CEP Plan Generation problem. We
further show how join query optimization techniques developed over the last
decades can be adapted and utilized to provide practically efficient solutions
for complex event detection. Our experiments demonstrate the superiority of
these techniques over existing strategies for CEP optimization in terms of
throughput, latency, and memory consumption."
pub.1037105262,Design and Optimization of the VideoWeb Wireless Camera Network,"Sensor networks have been a very active area of research in recent years. However, most of the sensors used in the development of these networks have been local and nonimaging sensors such as acoustics, seismic, vibration, temperature, humidity. The emerging development of video sensor networks poses its own set of unique challenges, including high-bandwidth and low latency requirements for real-time processing and control. This paper presents a systematic approach by detailing the design, implementation, and evaluation of a large-scale wireless camera network, suitable for a variety of practical real-time applications. We take into consideration issues related to hardware, software, control, architecture, network connectivity, performance evaluation, and data-processing strategies for the network. We also perform multiobjective optimization on settings such as video resolution and compression quality to provide insight into the performance trade-offs when configuring such a network and present lessons learned in the building and daily usage of the network."
pub.1112190621,SC-SD: Towards Low Power Stochastic Computing Using Sigma Delta Streams,"Processing data using Stochastic Computing (SC) requires only $\sim$ 7% of the area and power of the typical binary approach. However, SC has two major drawbacks that eclipse any area and power savings. First, it takes $sim$ 99% more time to finish a computation when compared with the binary approach, since data is represented as streams of bits. Second, the Linear Feedback Shift Registers (LFSRs) required to generate the stochastic streams increment the power and area of the overall SC-LFSR system. These drawbacks result in similar or higher area, power, and energy numbers when compared with the binary counterpart. In this work, we address these drawbacks by applying SC directly on Pulse Density Modulated (PDM) streams. Most modern Systems on Chip (SoCs) already include Analog to Digital Converters (ADCs). The core of $\Sigma\Delta$ -ADCs is the $\Sigma\Delta$ Modulator whose output is a PDM stream. Our approach (SC-SD) simplifies the system hardware in two ways. First, we drop the filter stage at the ADC and, second, we replace the costly Stochastic Number Generators (SNGs) with $\Sigma\Delta$ -Modulators. To further lower the system complexity, we adopt an Asynchronous $\Sigma\Delta$ -Modulator $(\mathrm{A}\Sigma\Delta \mathrm{M})$ architecture. We design and simulate the $\mathrm{A}\Sigma\Delta \mathrm{M}$: using an industry-standard 1×FinFET11In modern technologies the node number does not refer to any one feature in the process, and foundries use slightly different conventions; we use 1x to denote the 14/16nm FinFET nodes offered by the foundry. technology with foundry models. We achieve power savings of 81 % in SNG compared to the LFSR approach. To evaluate how this area and power savings scale to more complex applications, we implement Gamma Correction, a popular image processing algorithm. For this application, our simulations show that SC-SD can save 98%-11% in the total system latency and 50%-38% in power consumption when compared with the SC-LFSR approach or the binary counterpart. In modern technologies the node number does not refer to any one feature in the process, and foundries use slightly different conventions; we use 1x to denote the 14/16nm FinFET nodes offered by the foundry."
pub.1118846679,Data Acquisition Architecture and Online Processing System for the HAWC gamma-ray observatory,"The High Altitude Water Cherenkov observatory (HAWC) is an air shower array
devised for TeV gamma-ray astronomy. HAWC is located at an altitude of 4100 m
a.s.l. in Sierra Negra, Mexico. HAWC consists of 300 Water Cherenkov Detectors,
each instrumented with 4 photomultiplier tubes (PMTs). HAWC re-uses the
Front-End Boards from the Milagro experiment to receive the PMT signals. These
boards are used in combination with Time to Digital Converters (TDCs) to record
the time and the amount of light in each PMT hit (light flash). A set of VME
TDC modules (128 channels each) is operated in a continuous (dead time free)
mode. The TDCs are read out via the VME bus by Single-Board Computers (SBCs),
which in turn are connected to a gigabit Ethernet network. The complete system
produces ~ 500 MB/s of raw data. A high-throughput data processing system has
been designed and built to enable real-time data analysis. The system relies on
off-the-shelf hardware components, an open-source software technology for data
transfers (ZeroMQ) and a custom software framework for data analysis (AERIE).
Multiple trigger and reconstruction algorithms can be combined and run on
blocks of data in a parallel fashion, producing a set of output data streams
which can be analyzed in real time with minimal latency (< 5 s). This paper
provides an overview of the hardware set-up and an in-depth description of the
software design, covering both the TDC data acquisition system and the
real-time data processing system. The performance of these systems is also
discussed."
pub.1100544484,Event-based stereo matching using semiglobal matching,"In this article, we focus on the problem of depth estimation from a stereo pair of event-based sensors. These sensors asynchronously capture pixel-level brightness changes information (events) instead of standard intensity images at a specified frame rate. So, these sensors provide sparse data at low latency and high temporal resolution over a wide intrascene dynamic range. However, new asynchronous, event-based processing algorithms are required to process the event streams. We propose a fully event-based stereo three-dimensional depth estimation algorithm inspired by semiglobal matching. Our algorithm considers the smoothness constraints between the nearby events to remove the ambiguous and wrong matches when only using the properties of a single event or local features. Experimental validation and comparison with several state-of-the-art, event-based stereo matching methods are provided on five different scenes of event-based stereo data sets. The results show that our method can operate well in an event-driven way and has higher estimation accuracy."
pub.1061662386,Lineage Encoding: An Efficient Wireless XML Streaming Supporting Twig Pattern Queries,"In this paper, we propose an energy and latency efficient XML dissemination scheme for the mobile computing. We define a novel unit structure called G-node for streaming XML data in the wireless environment. It exploits the benefits of the structure indexing and attribute summarization that can integrate relevant XML elements into a group. It provides a way for selective access of their attribute values and text content. We also propose a lightweight and effective encoding scheme, called Lineage Encoding, to support evaluation of predicates and twig pattern queries over the stream. The Lineage Encoding scheme represents the parent-child relationships among XML elements as a sequence of bit-strings, called Lineage Code(V, H), and provides basic operators and functions for effective twig pattern query processing at mobile clients. Extensive experiments using real and synthetic data sets demonstrate our scheme outperforms conventional wireless XML broadcasting methods for simple path queries as well as complex twig pattern queries with predicate conditions."
pub.1094140245,Real-Time GPU-Based Software Beamformer Designed for Advanced Imaging Methods Research,"High computational demand is known to be a technical hurdle for real-time implementation of advanced methods like synthetic aperture imaging (SAI) and plane wave imaging (PWI) that work with the pre-beamform data of each array element. In this paper, we present the development of a software beamformer for SAI and PWI with real-time parallel processing capacity. Our beamformer design comprises a pipelined group of graphics processing units (GPU) that are hosted within the same computer workstation. During operation, each available GPU is assigned to perform demodulation and beamforming for one frame of pre-beamform data acquired from one transmit firing (e.g. point firing for SAl). To facilitate parallel computation, the GPUs have been programmed to treat the calculation of depth pixels from the same image scanline as a block of processing threads that can be executed concurrently, and it would repeat this process for all scanlines to obtain the entire frame of image data - i.e. low-resolution image (LRI). To reduce processing latency due to repeated access of each GPU's global memory, we have made use of each thread block's fast-shared memory (to store an entire line of pre-beamform data during demodulation), created texture memory pointers, and utilized global memory caches (to stream repeatedly used data samples during beamforming). Based on this beamformer architecture, a prototype platform has been implemented for SAI and PWI, and its LRI processing throughput has been measured for test datasets with 40 MHz sampling rate, 32 receive channels, and imaging depths between 5-15 cm. When using two Fermi-class GPUs (GTX-470), our beamformer can compute LRIs of 512-by-255 pixels at over 3200 fps and 1300 fps respectively for imaging depths of 5 cm and 15 cm. This processing throughput is roughly 3.2 times higher than a Tesla-class GPU (GTX-275)."
pub.1098922827,FPGA implementation of a high speed network interface card for optical burst switched networks,"Current generation of FPGAs with integrated high-speed transceivers provide a useful tool for prototyping various networking applications. We discuss a case study in developing a high-speed network interface card (NIC) for communication networks. The NIC implements a low-latency signaling protocol called Just-in-Time (JIT) for Optical Burst Switched networks. The main processing engine inside the FPGA runs at 62.5MHz and can handle data streams up to 1Gbps. The NIC utilizes the FPGA's gigabit transceiver cores, standard PCS/PMA and MAC layers and uses an optical front-end card to transmit data optically on specific wavelengths. The JIT engine in the NIC processes three kinds of messages - messages from the host, the network and internal timing messages associated with various timer events. The engine implements layer 3 functions typically implemented in software - like generating signaling messages and maintaining all active connections. This allows very fast setup and teardown of connections than otherwise possible."
pub.1090940320,An efficient hardware architecture of CAVLC encoder based on stream processing,"The paper presents an efficient implementation of Context-Adaptive Variable Length Coding (CAVLC) entropy encoder in H.264/AVC standard. The architecture is designed with a parallel structure targeting real-time video compression. The intensive memory access demand in the syntax element coding stage is lowered by using the proposed arithmetic table elimination technique. The packing stage implementation is interleaved with syntax element generation stage and includes fast methods for syntax elements concatenation. The register update method performs concatenation of the bitstream of previously processed sub-blocks and the syntax codewords of the currently processed sub-block. The CAVLC encoder processes 4 × 4 sub-block coefficients in parallel, introducing the initial latency of 12 clock cycles, after which the full pipeline of the data encoding on the sub-block level is performed, and 16 residuals are processed at each clock cycle. The achieved high throughput allows the encoding core to perform real-time processing of 8K UHD (4320p) video sequences with a frame rate of 30 frames/s."
pub.1005066944,DFS-superMPx: Low-cost parallel processing system for machine vision and image processing,"The architecture of a low-cost hosted MIMD parallel processing system containing parallel processor chips interconnected by a hierarchy of crossbars is described. The parallel processing system is attached to the system bus of the host and uses the operating system and programming environment of the host. Each parallel processor chip contains 64 processors. The processors in a chip are simple in their architecture and structured so that data streams can be processed efficiently using dataflow semantics. A static dataflow model of computation is assumed for programming the chip. Arithmetic, logical, multiply, conditional branch, and select instructions are supported. Each processor has a 16-bit data path and a microcontroller. The processors in a chip are clustered for reducing data communication latency time. Eight processors are grouped into a cluster and there are eight clusters in a chip. Segmented and switched buses are used for intra and inter cluster communication in a chip. A global bus is provided for supplying instructions to the processors during program setup and to communicate the status of the processors during program execution. Two global buses are provided for data transfer between the external memory or I/O devices and the data memories of the processors. Each chip has two ports with 16 bits of data, 16 bits for processor address, and control signals for connecting to other chips using a hierarchical crossbar interconnection network. The interconnection network is based on a 16 X 16 crossbar chip with 32 ports (16 paths) capable of connecting 16 processor chips or 15 processor chips and a second level of crossbar chip. With two levels of crossbar chips it is possible to connect 225 parallel processor chips and achieve one Teraop in a shoebox sized system. The applications selected for the parallel processing system are image processing, machine vision, video compression and decompression, and 3-D imaging."
pub.1118278915,Linear-time Online Action Detection From 3D Skeletal Data Using Bags of Gesturelets,"Sliding window is one direct way to extend a successful recognition system to
handle the more challenging detection problem. While action recognition decides
only whether or not an action is present in a pre-segmented video sequence,
action detection identifies the time interval where the action occurred in an
unsegmented video stream. Sliding window approaches for action detection can
however be slow as they maximize a classifier score over all possible
sub-intervals. Even though new schemes utilize dynamic programming to speed up
the search for the optimal sub-interval, they require offline processing on the
whole video sequence. In this paper, we propose a novel approach for online
action detection based on 3D skeleton sequences extracted from depth data. It
identifies the sub-interval with the maximum classifier score in linear time.
Furthermore, it is invariant to temporal scale variations and is suitable for
real-time applications with low latency."
pub.1172184348,Distance based prefetching algorithms for mining of the sporadic requests associations,"Modern storage systems intensively utilize data prefetching algorithms while
processing sequences of the read requests. Performance of the prefetching
algorithm (for instance increase of the cache hit ratio of the cache system -
CHR) directly affects overall performance characteristics of the storage system
(read latency, IOPS, etc.). There are widely known prefetching algorithms that
are focused on the discovery of the sequential patterns in the stream of
requests. This study examines a family of prefetching algorithms that is
focused on mining of the pseudo random (sporadic) patterns between read
requests - sporadic prefetching algorithms. The key contribution of this paper
is that it discovers a new, lightweight family of distance-based sporadic
prefetching algorithms (DBSP) that outperforms the best previously known
results on MSR traces collection.Another important contribution of this paper
is a thorough description of the procedure for comparing the performance of
sporadic prefetchers."
pub.1171579399,Fault Tolerance and Error Handling Techniques in Apache Kafka,"Real Time streaming framework enables the availability of data for users in near real time. For achieving the high throughput, low latency and to avoid loss of any events at runtime, real time streaming frameworks has their own methods to accomplish this task of message handling, but there are scenarios when due to huge volume of data at the source or due to other environment or Infrastructure related issues, some of the critical data might get lost in the process. This article evaluates methods to handle data rejections in conjunction with a fault-tolerant error handling recommendations for real time data streaming. There are many frameworks available for streaming the events in real time, in this article Apache Kafka is used to examine most frequent situations when an error can occur. This article will evaluate the key performance factors in terms of capability of the framework to recover when downtime occurs. Apache Kafka [15] is a distributed real time data streaming platform; it was developed by Linked inn and currently used by many applications across the industry. It works in producer consumer fashion and with correct configuration and set up, high Throughput with low latency can be achieved. This article will explain the configurations provided with the framework that can be utilized to design and develop optimized event streaming applications. This article is explaining the different parameters that can be used to design and develop the stream processing applications and what are the available options to handle the errors during the streaming to avoid any negative impact to the application."
pub.1044409817,Parallel gesture recognition with soft real-time guarantees,"Applying imperative programming techniques to process event streams, like those generated by multi-touch devices and 3D cameras, has significant engineering drawbacks. Declarative approaches solve these problems but have not been able to scale on multicore systems while providing guaranteed response times. We propose PARTE, a parallel scalable complex event processing engine which allows a declarative definition of event patterns and provides soft real-time guarantees for their recognition. It extends the state-saving Rete algorithm and maps the event matching onto a graph of actor nodes. Using a tiered event matching model, PARTEprovides upper bounds on the detection latency. Based on the domain-specific constraints, PARTE's design relies on a combination of 1) lock-free data structures; 2) safe memory management techniques; and 3) message passing between Rete nodes. In our benchmarks, we measured scalability up to 8 cores, outperforming highly optimized sequential implementations."
pub.1100745672,Data acquisition architecture and online processing system for the HAWC gamma-ray observatory," The High Altitude Water Cherenkov observatory (HAWC) is an air shower array devised for TeV gamma-ray astronomy. HAWC is located at an altitude of 4100 m a.s.l. in Sierra Negra, Mexico. HAWC consists of 300 Water Cherenkov Detectors, each instrumented with 4 photomultiplier tubes (PMTs). HAWC re-uses the Front-End Boards from the Milagro experiment to receive the PMT signals. These boards are used in combination with Time to Digital Converters (TDCs) to record the time and the amount of light in each PMT hit (light flash). A set of VME TDC modules (128 channels each) is operated in a continuous (dead time free) mode. The TDCs are read out via the VME bus by Single-Board Computers (SBCs), which in turn are connected to a gigabit Ethernet network. The complete system produces ≈ 500 MB/s of raw data. A high-throughput data processing system has been designed and built to enable real-time data analysis. The system relies on off-the-shelf hardware components, an open-source software technology for data transfers (ZeroMQ) and a custom software framework for data analysis (AERIE). Multiple trigger and reconstruction algorithms can be combined and run on blocks of data in a parallel fashion, producing a set of output data streams which can be analyzed in real time with minimal latency ( < 5 s). This paper provides an overview of the hardware set-up and an in-depth description of the software design, covering both the TDC data acquisition system and the real-time data processing system. The performance of these systems is also discussed."
pub.1134853625,"Edgine, A Runtime System for IoT Edge Applications","Abstract
The diffusion of Internet of Things (IoT) technologies has paved the way to new applications and services. In this context, developers need tools for efficient design and implementation. This paper proposes Edgine (Edge engine), a cross-platform open-source edge computing system. The system is the edge computing extension of Measurify, a cloud Application Programming Interface (API) dedicated to the collection and processing of measurements from the field. Particularly, Edgine can be remotely programmed to perform various kinds of processing on the field sensors’ data streams, thus allowing optimizing resource utilization, and reducing latency, bandwidth, transmission energy and computational burden on the cloud side. This paper presents three simple application cases that show effectiveness and versatility of the tool. To the best of our knowledge this is the first end-to-end development system dedicated to IoT measurements, open source, programmable on both the edge and cloud side, platform independent and non-cloud-vendor locked."
pub.1093391034,Towards Better Scalability for IoT-Cloud Interactions via Combined Exploitation of MQTT and CoAP,"It is manifest the growing research and industrial interest in scalable solutions for the efficient integration of large amounts of deployed sensors and actuators (Internet of Things-IoT-devices) and cloud-hosted virtualized resources for elastic storage and processing (including big data online stream processing). Such relevant attention is also demonstrated by the emergence of interesting IoT-cloud platforms from industry and open-source communities, as well as by the flourishing research area of fog/edge computing, where decentralized virtual resources at edge nodes can support enhanced scalability and reduced latency via locality-based optimizations. In this perspective, this paper proposes an innovative distributed architecture combining machine-to-machine industry-mature protocols (i.e., MQTT and CoAP) in an original way to enhance the scalability of gateways for the efficient IoT-cloud integration. In addition, the paper presents how we have applied the approach in the practical experience of efficiently and effectively extending the implementation of the open-source gateway that is available in the industry-oriented Kura framework for IoT."
pub.1111104330,LoTTA: Energy-Efficient Processor for Always-On Applications,"Various use cases in the era of Internet-of-Things (IoT) demand processor devices to have low energy consumption in order to maximize the battery life. In addition to energy constraints, there is often a need to both swiftly execute control-oriented code to provide low reaction times and to occasionally perform real time signal processing tasks efficiently. As a response to these requirements, we propose LoTTA, an extremely energy-efficient exposed datapath core. Its transport-triggered programming model helps in lowering the execution latency via low cost data forwarding. Control efficiency is achieved by an optimized control unit with zero delay slot branches and predicated execution. An instruction register file is included for frequently executed program hot spots to reduce the instruction stream energy consumption. These features allow the processor to execute CHStone and EEMBC CoreMark benchmarks on average with 19% fewer cycles compared to a 6-stage LM32, a traditional RISC core with similar datapath resources. The core consumes 53% less energy on average compared to the RISC core. When including the instruction stream overheads, in the best case, LoTTA saves 79% energy, and on average 40%."
pub.1094491010,Quaternionic Upsampling: Hyperspherical Techniques for 6 DoF Pose Tracking,"Fast real-time tracking is an integral component of modern 3D computer vision pipelines. Despite their advantages in accuracy and reliability, optical trackers suffer from limited acquisition rates depending either on intrinsic sensor capabilities or physical limitations such as exposure time. Moreover, data transmission and image processing produce latency in the pose stream. We introduce quaternionic upsampling to overcome these problems. The technique models the pose parameters as points on multidimensional hyperspheres in (dual) quaternion space. In order to upsample the pose stream, we present several methods to sample points on geodesics and piecewise continuous curves on these manifolds and compare them regarding accuracy and computation efficiency. With the unified approach of quaternionic upsampling, both interpolation and extrapolation in pose space can be done by continuous linear variation of only one sampling parameter. Since the method can be implemented rather efficiently, pose rates of over 4 kHz and future pose predictions with an accuracy of 128 11m and 0.5°are possible in real-time. The method does not depend on a special tracking algorithm and can thus be used for any arbitrary 3 DoF or 6 DoF rotation or pose tracking system."
pub.1156044164,Edge-Enhanced QoS Aware Compression Learning for Sustainable Data Stream Analytics,"Existing Cloud systems involve large volumes of data streams being sent to a centralised data centre for monitoring, storage and analytics. However, migrating all the data to the cloud is often not feasible due to cost, privacy, and performance concerns. However, Machine Learning (ML) algorithms typically require significant computational resources, hence cannot be directly deployed on resource-constrained edge devices for learning and analytics. Edge-enhanced compressive offloading becomes a sustainable solution that allows data to be compressed at the edge and offloaded to the cloud for further analysis, reducing bandwidth consumption and communication latency. The design and implementation of a learning method for discovering compression techniques that offer the best QoS for an application is described. The approach uses a novel modularisation approach that maps features to models and classifies them for a range of Quality of Service (QoS) features. An automated QoS-aware orchestrator has been designed to select the best autoencoder model in real-time for compressive offloading in edge-enhanced clouds based on changing QoS requirements. The orchestrator has been designed to have diagnostic capabilities to search appropriate parameters that give the best compression. A key novelty of this work is harnessing the capabilities of autoencoders for edge-enhanced compressive offloading based on portable encodings, latent space splitting and fine-tuning network weights. Considering how the combination of features lead to different QoS models, the system is capable of processing a large number of user requests in a given time. The proposed hyperparameter search strategy (over the neural architectural space) reduces the computational cost of search through the entire space by up to 89%. When deployed on an edge-enhanced cloud using an Azure IoT testbed, the approach saves up to 70% data transfer costs and takes 32% less time for job completion. It eliminates the additional computational cost of decompression, thereby reducing the processing cost by up to 30%."
pub.1105211941,Toward Haptic Communications Over the 5G Tactile Internet,"Touch is currently seen as the modality that will complement audition and vision as a third media stream over the Internet in a variety of future haptic applications which will allow full immersion and that will, in many ways, impact society. Nevertheless, the high requirements of these applications demand networks which allow ultra-reliable and low-latency communication for the challenging task of applying the required quality of service for maintaining the user’s quality of experience at optimum levels. In this survey, we enlist, discuss, and evaluate methodologies and technologies of the necessary infrastructure for haptic communication. Furthermore, we focus on how the fifth generation of mobile networks will allow haptic applications to take life, in combination with the haptic data communication protocols, bilateral teleoperation control schemes and haptic data processing needed. Finally, we state the lessons learned throughout the surveyed research material along with the future challenges and infer our conclusions."
pub.1051311137,CSDFa,"Real-time stream processing applications, such as Software Defined Radio applications, are often executed concurrently on multiprocessor systems. A unified data flow model and analysis method have been proposed that can be used to simultaneously determine the amount of pipeline and coarse-grained data parallelism required to meet the temporal constraints of such applications. However, this unified model is only defined for Synchronous Data Flow (SDF) graphs. Defining a unified model for a more expressive model such as Cyclo-Static Data Flow (CSDF) is not possible, because auto-concurrency can cause a time-dependent order of tokens and dependencies. This paper introduces the Cyclo-Static Data Flow with Auto-concurrency (CSDFa) model. In CSDFa, tokens have indices and the consumption order of tokens is static and time-independent. This allows expressing and trading off pipeline and coarse-grained data parallelism in a single, unified model. Furthermore, we introduce a new type of circular buffer that implements the same static order as is used by the CSDFa model. The overhead of operations on this buffer is independent of the amount of auto-concurrency, which corresponds to the constant firing durations in the CSDFa model. Exploiting the trade-off between data and pipeline parallelism with the CSDFa model is demonstrated with a part of a FMCW radar processing pipeline. We show that the CSDFa model enables optimizing the balance between processing units and memory, resulting in a significant reduction of silicon area. Additionally, it is shown that reducing the maximum allowed latency increases the minimum required amount of data parallelism by up to a factor of 16."
pub.1144140505,Live Demonstration: Real-Time Calcium Trace Extraction from Large-Field-of-View Miniscope,"We demonstrate a prototype system ACTEV for acceleration of calcium trace extraction from video captured by a large-field-of-view (LFOV) Miniscope at 22.8 fps. The ACTEV consists of an FPGA accelerator for the calcium image processing, an interface PCB for the data transmission and a GUI software for the user interaction. The FPGA accelerator takes a stream of 512 × 512 calcium images as input, executes motion correction, image enhancement and trace extraction from 1024 cells per frame with <1ms latency. The PCB transmits the raw calcium video and extracted traces to the host computer over an Ethernet interface. The GUI displays the received images and traces simultaneously for the user. The user can observe the motion corrected video and extracted traces from individual cells as the rat freely runs on a linear track."
pub.1095232728,Parallel video surveillance on the multi-core cell broadband engine,"The IBM Cell Broadband Engine (BE) is a multicore processor with a PowerPC host processor (PPE) and 8 synergic processor engines (SPEs). The Cell BE architecture is designed to improve upon conven tional processors in terms of memory latency, bandwidth and power computation. In this paper, we discuss the parallelization, implementation and performance of a video surveillance application on the IBM Cell BE. We report the Video surveillance application's performance measured on a computer with one Cell processor and with varying numbers of synergic processor engines enabled. These results were compared to the results obtained on the Cell's single PPE with all 8 SPEs disabled. The results indicate that our video surveillance application performs approximately 16 times faster on the Cell BE than modern RISC processors by processing input data from five separate surveillance video streams in parallel."
pub.1147191671,Performance Evaluation of Apache Kafka – A Modern Platform for Real Time Data Streaming,"Current generation businesses become more demanding on timely availability of data. Many real-time data streaming tools and technologies are capable to meet business expectations. Apache Kafka is one of the capable open-source distributed scalable technology that enables real-time data streaming with good throughput and latency. In traditional batch processing, data is getting processed in groups or batches but in streaming services, data records are handled separately and there is a flow of data processing that is continuous and real-time. Once Data is available at the source, Kafka can detect and stream it in real-time to the target application. After doing the literature survey it was observed that there are insufficient experiments have been done till now with a variety of volumes and with different values of the number of partitions and polling intervals. The purpose of this study is to elaborate on Apache Kafka implementation and evaluate its performance. This study will analyse key performance indicators for the streaming platform and will provide useful insights from it. These insights will help to design optimized applications in Apache Kafka. Based on gaps identified after the literature survey, multiple experiments have been conducted for the producer and consumer API (Application Programming interface). Configuration of Kafka with Apache Zookeeper helped to drive the results which are captured in tabular form for different values of polling intervals, volumes, and partitions. Data for all test runs have been analysed further to drive the conclusions as mentioned in the results section. This study provides valuable insights about the utilization of CPU (Central Processing Unit) and memory for Apache Kafka streaming on changing volumes, also elaborates the impacts on streaming performance when key configurations are getting changed."
pub.1175960614,Design Scalable Data Pipelines For Ai Applications,"The specific focus of this paper is to review the data pipeline at a larger scale for the use of AI, which addresses the '3V's, that is, volume, variety, and velocity. This research aims to discover and compare architectural approaches and tools that enhance the speed, solidity, and scalability of data feeds for AI. These include working in hardware accelerators, cloud-based working, and the approaches to managing data right from ingestion to production. It is essential to demonstrate that the requirements for large-scale solutions such as FPGAs, Containers, and model life cycle management can improve the latency and throughput of an AI data stream using simulations and real-time scenarios. The study shows the importance of combining these advanced technologies to cover the standard challenges in AI data analysis, including the difficulties in data processing and the necessity of real-time analysis. Lastly, the study identifies optimal practices for implementing data pipelines when choosing the most accurate and effective way to feed the growing demand for artificial intelligence models while ensuring the support needed to run and maintain those AI-driven models in production. Published: 26 January 2021"
pub.1152550124,A Survey on Mobile Edge Computing for Video Streaming: Opportunities and Challenges,"5G communication brings substantial improvements in the quality of service provided to various applications by achieving higher throughput and lower latency. However, interactive multimedia applications (e.g., ultra high definition video conferencing, 3D and multiview video streaming, crowd-sourced video streaming, cloud gaming, virtual and augmented reality) are becoming more ambitious with high volume and low latency video streams putting strict demands on the already congested networks. Mobile Edge Computing (MEC) is an emerging paradigm that extends cloud computing capabilities to the edge of the network i.e., at the base station level. To meet the latency requirements and avoid the end-to-end communication with remote cloud data centers, MEC allows to store and process video content (e.g., caching, transcoding, pre-processing) at the base stations. Both video on demand and live video streaming can utilize MEC to improve existing services and develop novel use cases, such as video analytics, and targeted advertisements. MEC is expected to reshape the future of video streaming by providing ultra-reliable and low latency streaming (e.g., in augmented reality, virtual reality, and autonomous vehicles), pervasive computing (e.g., in real-time video analytics), and blockchain-enabled architecture for secure live streaming. This paper presents a comprehensive survey of recent developments in MEC-enabled video streaming bringing unprecedented improvement to enable novel use cases. A detailed review of the state-of-the-art is presented covering novel caching schemes, optimal computation offloading, cooperative caching and offloading and the use of artificial intelligence (i.e., machine learning, deep learning, and reinforcement learning) in MEC-assisted video streaming services."
pub.1093415247,Cloud Resource Scaling for Big Data Streaming Applications Using a Layered Multi-dimensional Hidden Markov Model,"Recent advancements in technology have led to a deluge of data that require real-time analysis with strict latency constraints. A major challenge, however, is determining the amount of resources required by big data stream processing applications in response to heterogeneous data sources, streaming events, unpredictable data volume and velocity changes. Over-provisioning of resources for peak loads can be wasteful while under-provisioning can have a huge impact on the performance of the streaming applications. The majority of research efforts on resource scaling in the cloud are investigated from the cloud provider's perspective, they focus on web applications and do not consider multiple resource bottlenecks. We aim at analyzing the resource scaling problem from a big data streaming application provider's point of view such that efficient scaling decisions can be made for future resource utilization. This paper proposes a Layered Multi-dimensional Hidden Markov Model (LMD-HMM) for facilitating the management of resource auto-scaling for big data streaming applications in the cloud. Our detailed experimental evaluation shows that LMD-HMM performs best with an accuracy of 98%, outperforming the single-layer hidden markov model."
pub.1107307635,Multi-Query Optimization in Wide-Area Streaming Analytics,"Wide-area data analytics has gained much attention in recent years due to the increasing need for analyzing data that are geographically distributed. Many of such queries often require real-time analysis on data streams that are continuously being generated across multiple locations. Yet, analyzing these geo-distributed data streams in a timely manner is very challenging due to the highly heterogeneous and limited bandwidth availability of the wide-area network (WAN). This paper examines the opportunity of applying multi-query optimization in the context of wide-area streaming analytics, with the goal of utilizing WAN bandwidth efficiently while achieving high throughput and low latency execution. Our approach is based on the insight that many streaming analytics queries often exhibit common executions, whether in consuming a common set of input data or performing the same data processing. In this work, we study different types of sharing opportunities and propose a practical online algorithm that allows streaming analytics queries to share their common executions incrementally. We further address the importance of WAN awareness in applying multi-query optimization. Without WAN awareness, sharing executions in a wide-area environment may lead to performance degradation. We have implemented our WAN-aware multi-query optimization in a prototype implementation based on Apache Flink. Experimental evaluation using Twitter traces on a real wide-area system deployment across geo-distributed EC2 data centers shows that our technique is able to achieve 21% higher throughput while saving WAN bandwidth consumption by 33% compared to a WAN-aware, sharing-agnostic system."
pub.1120359548,On the Fog-Cloud Cooperation: How Fog Computing can address latency concerns of IoT applications,"Fog computing emerged as a new computing paradigm which moves the computing power to the proximity of users, from core to the edge of the network. It is known as the extension of Cloud computing and it offers inordinate opportunities for real-time and latency-sensitive IoT applications. An IoT application consists of a set of dependent Processing Elements (PEs) defined as operations performed on data streams and can be modeled as a Directed Acyclic Graph (DAG). Each PE performs a variety of low-level computation on the incoming data such as aggregation or filtering. A key challenge is to decide how to distribute such PEs over the resources, in order to minimize the overall response time of the entire PE graph. This problem is known as distributed PE scheduling and placement problem. In this work, we try to address the question of how fog computing paradigm can help reducing the IoT application response time by efficiently distributing PE graphs over the Fog-Cloud continuum. We mathematically formulate the fundamental characteristics of IoT application and Fog infrastructure, then model the system as an optimization problem using Gravitational Search Algorithm (GSA) meta-heuristic technique. Our proposed GSA model is evaluated by comparing it with a well-known evolutionary algorithm in the literature via simulation. Also, a comparative analysis with the legacy cloud infrastructure is done in order to show the significant impact of fog presence on the performance of PE processing. Evaluation of our model demonstrates the efficiency of our approach comparing to the current literature."
pub.1065140097,Emerging optical access network technologies for 5G wireless [invited],"With the advancement of radio access networks, more and more mobile data content needs to be transported by optical networks. Mobile fronthaul is an important network segment that connects centralized baseband units (BBUs) with remote radio units in cloud radio access networks (C-RANs). It enables advanced wireless technologies such as coordinated multipoint and massive multiple-input multiple-output. Mobile backhaul, on the other hand, connects BBUs with core networks to transport the baseband data streams to their respective destinations. Optical access networks are well positioned to meet the first optical communication demands of C-RANs. To better address the stringent requirements of future generations of wireless networks, such as the fifth-generation (5G) wireless, optical access networks need to be improved and enhanced. In this paper, we review emerging optical access network technologies that aim to support 5G wireless with high capacity, low latency, and low cost and power per bit. Advances in high-capacity passive optical networks (PONs), such as 100 Gbit/s PON, will be reviewed. Among the topics discussed are advanced modulation and detection techniques, digital signal processing tailored for optical access networks, and efficient mobile fronthaul techniques. We also discuss the need for coordination between RAN and PON to simplify the overall network, reduce the network latency, and improve the network cost efficiency and power efficiency."
pub.1120597373,Probabilistic data structures for big data analytics: A comprehensive review,"An exponential increase in the data generation resources is widely observed in last decade, because of evolution in technologies such as-cloud computing, IoT, social networking, etc. This enormous and unlimited growth of data has led to a paradigm shift in storage and retrieval patterns from traditional data structures to Probabilistic Data Structures (PDS). PDS are a group of data structures that are extremely useful for Big data and streaming applications in order to avoid high-latency analytical processes. These data structures use hash functions to compactly represent a set of items in stream-based computing while providing approximations with error bounds so that well-formed approximations get built into data collections directly. Compared to traditional data structures, PDS use much less memory and constant time in processing complex queries. This paper provides a detailed discussion of various issues which are normally encountered in massive data sets such as-storage, retrieval, query,etc. Further, role of PDS in solving these issues is also discussed where these data structures are used as temporary accumulators in query processing. Several variants of existing PDS along with their application areas have also been explored which give a holistic view of domains where these data structures can be applied for efficient storage and retrieval of massive data sets. Mathematical proofs of various parameters considered in the PDS have also been discussed in the paper. Moreover, the relative comparison of various PDS with respect to various parameters is also explored."
pub.1154093012,Multi-step Attack Detection and Mitigation Enhancing In-Network Flow Classification,"Recent in-network flow classification methods are able to run within the data plane of network switches allowing intrusion detection at linerate. Although this enables fine-grained, scalable and timely analysis, outcomes are still subject to uncertainty, aggravated by limited data plane resources and language constraints of respective programs, and hence associated with higher risk of misclassification. Countering these deficiencies by leveraging increased computational capabilities at CPU level of the switches helps to obtain more sophisticated analysis decisions. However, exporting metadata for packet streams from data plane into the network operating system space to run downstream analysis is associated with additional delay and also limited regarding the amount of data that can be shared to ensure scalable processing. To address this trade-off, a certainty-based approach that selectively combines advantages of lower latency and data locality at the data plane with higher computational power and analysis complexity in the operating system is proposed. Therefore, a two-tier flow classification method integrates initial in-network inference on early subflow metadata with advanced decision support provided by a subsequent machine learning-based analysis step. In addition, load and flow monitoring is employed to track long-term and voluminous heavy hitters, supporting a flow detour and throttling mechanism to assist in controlling the volume and velocity of suspicious packet streams. Evaluations show that, first, the cooperative behavior using both traffic classification steps allows for improved accuracies while providing scalable, timely and certain decisions. Second, considered steps to handle potentially malicious heavy hitters allow traffic control to reduce negative impacts on benign flows."
pub.1093939518,A low-power asynchronous data-path for a FIR filter bank,"This paper describes a number of design issues relating to the implementation of low-power asynchronous signal processing circuits. Specifically, the paper addresses the design of a dedicated processor structure that implements an audio FIR filter bank which is part of an industrial application. The algorithm requires a fixed number of steps and the moderate speed requirement allows a sequential implementation. The latter, in combination with a huge predominance of numerically small data values in the input data stream, is the key to a low-power asynchronous implementation. Power is minimized in two ways: by reducing the switching activity in the circuit, and by applying adaptive scaling of the supply voltage, in order to exploit the fact that the average case latency as 2-3 times better than the worst case. The paper reports on a study of properties of real life data, and discusses the implications it has on the choice of architecture, handshake-protocol, data-encoding, and circuit design. This includes a tagging scheme that divides the data-path into slices, and an asynchronous ripple carry adder that avoids a completion tree."
pub.1121958051,A hardware accelerator for edge detection in high-definition video using cellular neural networks,"This paper presents the architecture of a hardware accelerator for a cellular neural network (CeNN) with an application to real-time edge detection on visible-range and infrared video. The accelerator features fully-pipelined processing elements (PEs) that exploit the data parallelism in the algorithm to perform an iteration of the CeNN on a stream of video data with high throughput. The memory architecture exploits the locality of reference in the CeNN, so that each PE uses only 5 line buffers to store pixel, state, and output data, thus achieving low on-chip memory utilization. Implemented on a Xilinx XC7A200T FPGA running at 245MHz, the accelerator performs edge detection on 1080p video using a single CeNN iteration with a throughput of 118 frames per second (fps), a total latency of 15.7 μ s, and 618mW of power consumption. The architecture features static reconfiguration to store built-in kernels and to add more PEs to support multiple iterations of the CeNN algorithm. More kernels can be added dynamically through a serial interface."
pub.1094058889,Switcherland: a QoS communication architecture for workstation clusters,"Computer systems have become powerful enough to process continuous data streams such as video or animated graphics. While processing power and communication bandwidth of today's systems typically are sufficient, quality of service (QoS) guarantees as required for handling such data types cannot be provided by these systems in adequate ways. We present Switcherland, a scalable communication architecture based on crossbar switches that provides QoS guarantees for workstation clusters in the form of reserved bandwidth and bounded transmission delays. Similar to the ATM technology Switcherland provides QoS guarantees with the help of service classes, that is, data transfers are characterized as variable bit rare traffic or constant bit rate traffic. However, unlike LAN technologies, Switcherland is optimized for cluster computing in that (i) it serves as a backplane interconnection fabric as well as a LAN, (ii) it extends support for service classes by also covering the end nodes of the network, (iii) it provides low latency in the order of one microsecond per switch, and (iv) it uses a communication model based on a global memory to simplify programming."
pub.1148786485,Network-aware worker placement for wide-area streaming analytics,"Many organizations leverage Distributed Stream processing systems (DPSs) to get insights from the data generated by different users/devices, e.g., the Internet of Things (IoT) devices or user clicks on a website, on geographically distributed datacenters. The worker nodes in such environments are connected through Wide Area Network (WAN) links with various delays and bandwidth. Therefore, minimizing the execution latency of a task on the worker nodes while using the links with enough bandwidth and lower cost to steer the traffic of the applications is a challenging task. In this paper, we formulate the worker node placement for a geo-distributed DSPs network as a multi-criteria decision-making problem. Then, we propose an additive weighting-based approach to solve it. The users can prioritize the worker node placement according to the network-relevant parameters. We also propose a framework that can be integrated with the current DPSs to execute the tasks. We test our placement approach on three widely used stream processing systems, i.e., Apache Spark, Apache Storm, and Apache Flink, on three custom graphs adopted from the real cloud providers. We run the streaming query of the Yahoo! streaming benchmark on these three DPSs. The experimental results show that our approach improves the performance of Spark up to 2.2x–7.2x, Storm up to 1.2x–3.4x, and Flink up to 1.4x–3.3x compared with other placement approaches, which makes our framework useful for use in practical environments."
pub.1093869858,SpeedStream: A Real-Time Stream Data Processing Platform in the Cloud,"SpeedStream is a universal distributed platform that can handle with massive data flows with the features of low coupling, high availability, low latency and high scalability. Focusing on the core technologies of real-time stream computing platform in cloud environment, this paper conducts a series of researches and implementation of the system. First of all, aiming at the availability of real-time streaming computing platform, we design a high availability framework based on Zookeeper. It ensures fault detection and recovery of process level and node level timely by monitoring heartbreak of each modules and strategy of fault migration. Secondly, in order to increase the application types of the platform, by means of directed cycle detection and iteration protection, we design a real-time streaming computing model that based on directed graph with sources and sinks, which can not only satisfy the needs of common DAG computing services, but also support iteration computing services including directed cycle, bidirectional arcs and annular arcs. In addition, the platform can realize personalized task scheduling strategy for users by establishing task allocation matrix and optimize task allocation model. Finally, in order to solve the many-to-many dynamic load-balancing between tasks, we apply scheduler with status and distributed session table. It overcomes the difficulty of maintaining consistency of session without global session table. We also testified the convergence of this method. The experiment indicates that the throughput and data processing delay of SpeedStream are superior to other alternatives in dealing with the businesses of iteration applications, high traffic fluctuation applications, and high demand of load-balancing applications. This platform provides reliable, universal, and real-time solutions to process massive data flows, such as to process the real-time trading data in e-cornmerce, to analyze sensing flow in internet of things, and monitor traffics of the Internet."
pub.1111002978,Benchmarking the NVIDIA V100 GPU and Tensor Cores,"The V100 GPU is the newest server-grade GPU produced by NVIDIA and introduces a number of new hardware and API features. This paper details the results of benchmarking the V100 GPU and demonstrates that it is a significant generational improvement, increasing memory bandwidth, cache bandwidth, and reducing latency. A major new addition is the Tensor core units, which have been marketed as deep learning acceleration features that enable the computation of a $$4\times 4\times 4$$ half precision matrix-multiply-accumulate operation in a single clock cycle. This paper confirms that the Tensor cores offer considerable performance gains for half precision general matrix multiplication; however, programming them requires fine control of the memory hierarchy that is typically unnecessary for other applications."
pub.1063165515,Switcherland,"Computer systems have become powerful enough to process continuous data streams such as video or animated graphics. While processing power and communication bandwidth of today's systems typically are sufficient, quality of service (QoS) guarantees as required for handling such data types cannot be provided by these systems in adequate ways.We present Switcherland, a scalable communication architecture based on crossbar switches that provides QoS guarantees for workstation clusters in the form of reserved bandwidth and bounded transmission delays. Similar to the ATM technology Switcherland provides QoS guarantees with the help of service classes, that is, data transfers are characterized as variable bit rate traffic or constant bit rate traffic. However, unlike LAN technologies, Switcherland is optimized for cluster computing in that (i) it serves as a backplane interconnection fabric as well as a LAN, (ii) it extends support for service classes by also covering the end nodes of the network, (iii) it provides low latency in the order of one microsecond per switch, and (iv) it uses a communication model based on a global memory to simplify programming."
pub.1136769015,Cloud Simulation and Virtualization for Testing of Critical Energy Infrastructure Components,"Cloud computing and cloud infrastructure are beginning to be applied in order to ensure effective functioning of management system of Critical Energy Infrastructure. This is improved informativeness of Supervisory Control and Data Acquisition system by the application of additional Wide-area Measurement and Control Systems (WAMCSs), since most of all processing and transition operations of data information streams of the WAMCSs may be performed by using on clouds. In light of these circumstances, it is importantly to get extensive modeling results considering wide area range of input data and latency time between critical energy infrastructure components and cloud infrastructure components. However, researchers were not, yet, had possibility to receive more adequate results based only mathematical modeling process. In this case, simulation models considering the cost time are more preferable than mathematical models. In the spirit development of this conception, authors are proposing to employ the simulation model based on virtual technology and Matlab Simulink toolkit in order to get modeling results for more important on-line mode of functioning of the WAMCSs."
pub.1094917594,Data Mining with Big Data,"In an Information technology world, the ability to effectively process massive datasets has become integral to a broad range of scientific and other academic disciplines. We are living in an era of data deluge and as a result, the term “Big Data” is appearing in many contexts. It ranges from meteorology, genomics, complex physics simulations, biological and environmental research, finance and business to healthcare. Big Data refers to data streams of higher velocity and higher variety. The infrastructure required to support the acquisition of Big Data must deliver low, predictable latency in both capturing data and in executing short, simple queries. To be able to handle very high transaction volumes, often in a distributed environment; and support flexible, dynamic data structures. Data processing is considerably more challenging than simply locating, identifying, understanding, and citing data. For effective large-scale analysis all of this has to happen in a completely automated manner. This requires differences in data structure and semantics to be expressed in forms that are computer understandable, and then “robotically” resolvable. There is a strong body of work in data integration, mapping and transformations. However, considerable additional work is required to achieve automated error-free difference resolution. This paper proposes a framework on recent research for the Data Mining using Big Data."
pub.1116478487,IRESE: An intelligent rare-event detection system using unsupervised learning on the IoT edge," In the recent years, a rapid growth of IoT devices has been observed, which in turn results in a huge amount of data produced from multiple sources towards the most disparate cloud platforms or the Internet in general. In a typical cloud-centric approach, the data produced by these devices is simply transmitted over the Internet, for consumption and/or storage. However, with the exponential growth in data production rates, the available network resources are becoming the actual bottleneck of this huge data flowing. Therefore, several challenges are appearing in the coming years, which are mainly related to data transmission, processing, and storage along the so-called cloud-to-thing continuum. In fact, one of the most critical requirements of several IoT applications is low latency, which often hinders raw data consumption to happen at the opposite endpoint with respect to its production. In the context of IoT data stream analytics, for instance, the detection of anomalies or rare-events is one of the most demanding tasks, as it needs prompt detection to increase its significance. In this respect, Fog and Edge Computing seem to be the correct paradigms to alleviate these stringent demands in terms of latency and bandwidth as, by leveraging on re-configurable IoT gateways and smart devices able to support the distribution of the overall computational task, they envisage to liquefy data processing along the way from the sensing device to a cloud endpoint. In this paper, we will present IRESE, that is a rare-event detection system able to apply unsupervised machine learning techniques on the incoming data, directly on affordable gateways located in the IoT edge. Notwithstanding the proposed approach enjoys the benefits of a fully unsupervised learning approach, such as the ability to learn from unlabeled data, it has been tested against various audio rare-event categories, such as gunshot, glass break, scream, and siren, achieving precision and recall measures above 90% in detecting such events."
pub.1061536257,Cost-Aware Streaming Workflow Allocation on Geo-Distributed Data Centers,"The virtual machine (VM) allocation problem in cloud computing has been widely studied in recent years, and many algorithms have been proposed in the literature. Most of them have been successfully applied to batch processing models such as MapReduce; however, none of them can be applied to streaming workflow well because of the following weaknesses: 1) failure to capture the characteristics of tasks in streaming workflow for the short life cycle of data streams; 2) most algorithms are based on the assumptions that the price of VMs and traffic among data centers (DCs) are static and fixed. In this paper, we propose a streaming workflow allocation algorithm that takes into consideration the characteristics of streaming work and the price diversity among geo-distributed DCs, to further achieve the goal of cost minimization for streaming big data processing. First, we construct an extended streaming workflow graph (ESWG) based on the task semantics of streaming workflow and the price diversity of geo-distributed DCs, and the streaming workflow allocation problem is formulated into mixed integer linear programming based on the ESWG. Second, we propose two heuristic algorithms to reduce the computational space based on task combination and DC combination in order to meet the strict latency requirement. Finally, our experimental results demonstrate significant performance gains with lower total cost and execution time."
pub.1147032351,A Distributed Real-Time Recommender System for Big Data Streams,"In today's data-driven world, recommender systems (RS) play a crucial role to
support the decision-making process. As users become continuously connected to
the internet, they become less patient and less tolerant to obsolete
recommendations made by an RS, e.g., movie recommendations on Netflix or books
to read on Amazon. This, in turn, requires continuous training of the RS to
cope with both the online fashion of data and the changing nature of user
tastes and interests, known as concept drift. Streaming (online) RS has to
address three requirements: continuous training and recommendation, handling
concept drifts, and ability to scale. Streaming recommender systems proposed in
the literature mostly, address the first two requirements and do not consider
scalability. That is because they run the training process on a single machine.
Such a machine, no matter how powerful it is, will eventually fail to cope with
the volume of the data, a lesson learned from big data processing. To tackle
the third challenge, we propose a Splitting and Replication mechanism for
building distributed streaming recommender systems. Our mechanism is inspired
by the successful shared-nothing architecture that underpins contemporary big
data processing systems. We have applied our mechanism to two well-known
approaches for online recommender systems, namely, matrix factorization and
item-based collaborative filtering. We have implemented our mechanism on top of
Apache Flink. We conducted experiments comparing the performance of the
baseline (single machine) approach with our distributed approach. Evaluating
different data sets, improvement in processing latency, throughput, and
accuracy have been observed. Our experiments show online recall improvement by
40\% with more than 50\% less memory consumption."
pub.1093307794,A “Fast Data” Architecture: Dashboard for Anomalous Traffic Analysis in Data Networks,"Fast Data is a new Big Data computing paradigm that ensures requirements such as Real-Time processing of continuous data stream, storage at high rates and low latency with no data losses. In this work we propose a “Fast Data” architecture for a specific kind of software application in which input data arrive very fast and the results for each processed data have to match such input rates. We applied this architecture to build a Dashboard for Anomalous Traffic Analysis in Data Networks. In order to fulfil the requirements of Real-Time processing and no data losses, we carry out a design that consists of a pattern of dynamic tree of process pipelines, where the number of branches increases proportionally to the input data rate. Two different approaches have been followed to implement this design pattern: one based in a well-known set of products from the Big Data ecosystem; and the other built with Kafka, Zookeeper and a set of components designed and implemented by us. These two implementations have been compared in terms of velocity and scalability performance. As a result, the implementation built with our own components is significantly faster and scalable than the traditional one. The good results obtained by using both the design pattern of dynamic tree of process pipelines and our implementation make them very suitable for its use in other scenarios and applications such as smart cities, environment monitoring, industry 4.0, distributed control systems, etc."
pub.1003282346,Cloud Support Data Management Infrastructure for Upcoming Smart Cities,"This paper presents a novel large scale data management model for future smart cities. The system is exploiting the emerging cloud computing services availability over the globe by processing the collected data at the edge. To handle the large scale requirements in smart city, we introduce a Mobile Edge Computing (MEC) framework in order to increase the reliability of the deployed applications. MEC is a promising framework to reduce the cloud core utilization as well as to provide applications that require low latency to mobile end users. As near as possible to the location of the end user, MEC relates to the mobile network applications and data stream acceleration through caching and/or compressing of relevant data at the edge of the mobile network. Although MEC is not yet deployed in real-life systems, recent studies discussed some technical details and related concepts. This paper studies the definition of MEC and similar concepts with respect to typical application scenarios, like in smart cities, and provides scope and limitations that may be encountered when implementing and deploying MEC. Then, a MEC framework is used to explore the data management infrastructure for upcoming smart cities."
pub.1119245757,LIFT: Reinforcement Learning in Computer Systems by Learning From Demonstrations,"Reinforcement learning approaches have long appealed to the data management
community due to their ability to learn to control dynamic behavior from raw
system performance. Recent successes in combining deep neural networks with
reinforcement learning have sparked significant new interest in this domain.
However, practical solutions remain elusive due to large training data
requirements, algorithmic instability, and lack of standard tools. In this
work, we introduce LIFT, an end-to-end software stack for applying deep
reinforcement learning to data management tasks. While prior work has
frequently explored applications in simulations, LIFT centers on utilizing
human expertise to learn from demonstrations, thus lowering online training
times. We further introduce TensorForce, a TensorFlow library for applied deep
reinforcement learning exposing a unified declarative interface to common RL
algorithms, thus providing a backend to LIFT. We demonstrate the utility of
LIFT in two case studies in database compound indexing and resource management
in stream processing. Results show LIFT controllers initialized from
demonstrations can outperform human baselines and heuristics across latency
metrics and space usage by up to 70%."
pub.1140324165,Deep Reinforcement Learning for Scheduling in an Edge Computing‐Based Industrial Internet of Things," The demand for improving productivity in manufacturing systems makes the industrial Internet of things (IIoT) an important research area spawned by the Internet of things (IoT). In IIoT systems, there is an increasing demand for different types of industrial equipment to exchange stream data with different delays. Communications between massive heterogeneous industrial devices and clouds will cause high latency and require high network bandwidth. The introduction of edge computing in the IIoT can address unacceptable processing latency and reduce the heavy link burden. However, the limited resources in edge computing servers are one of the difficulties in formulating communication scheduling and resource allocation strategies. In this article, we use deep reinforcement learning (DRL) to solve the scheduling problem in edge computing to improve the quality of services provided to users in IIoT applications. First, we propose a hierarchical scheduling model considering the central‐edge computing heterogeneous architecture. Then, according to the model characteristics, a deep intelligent scheduling algorithm (DISA) based on a double deep Q network (DDQN) framework is proposed to make scheduling decisions for communication. We compare DISA with other baseline solutions using various performance metrics. Simulation results show that the proposed algorithm is more effective than other baseline algorithms. "
pub.1156762224,AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs,"Unmanned aerial vehicles (UAVs) equipped with computer vision capabilities have been widely utilized in several remote sensing applications, such as precision agriculture, environmental monitoring, and surveillance. However, the commercial usage of these UAVs in such applications is mostly performed manually, with humans being responsible for data observation or offline processing after data collection due to the lack of on board AI on edge. Other technical methods rely on the cloud computation offloading of AI applications, where inference is conducted on video streams, which can be unscalable and infeasible due to remote cloud servers’ limited connectivity and high latency. To overcome these issues, this paper presents a new approach to using edge computing in drones to enable the processing of extensive AI tasks onboard UAVs for remote sensing. We propose a cloud–edge hybrid system architecture where the edge is responsible for processing AI tasks and the cloud is responsible for data storage, manipulation, and visualization. We designed AERO, a UAV brain system with onboard AI capability using GPU-enabled edge devices. AERO is a novel multi-stage deep learning module that combines object detection (YOLOv4 and YOLOv7) and tracking (DeepSort) with TensorRT accelerators to capture objects of interest with high accuracy and transmit data to the cloud in real time without redundancy. AERO processes the detected objects over multiple consecutive frames to maximize detection accuracy. The experiments show a reduced false positive rate (0.7%), a low percentage of tracking identity switches (1.6%), and an average inference speed of 15.5 FPS on a Jetson Xavier AGX edge device."
pub.1138899733,The synergy of complex event processing and tiny machine learning in industrial IoT,"Focusing on comprehensive networking, the Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness in factory operations. Various intelligent sensors play a central role, as they generate a vast amount of real-time data that can provide insights into manufacturing. Complex event processing (CEP) and machine learning (ML) have been developed actively in the last years in IIoT to identify patterns in heterogeneous data streams and fuse raw data into tangible facts. In a traditional compute-centric paradigm, the raw field data are continuously sent to the cloud and processed centrally. As IIoT devices become increasingly pervasive, concerns are raised since transmitting such an amount of data is energy-intensive, vulnerable to be intercepted, and subjected to high latency. Decentralized on-device ML and CEP provide a solution where data is processed primarily on edge devices. Thus communications can be minimized. However, this is no mean feat because most IIoT edge devices are resource-constrained with low power consumption. This paper proposes a framework that exploits ML and CEP's synergy at the edge in distributed sensor networks. By leveraging tiny ML and CEP, we now shift the computation from the cloud to the resource-constrained IIoT devices and allow users to adapt on-device ML models and CEP reasoning rules flexibly on the fly. Lastly, we demonstrate the proposed solution and show its effectiveness and feasibility using an industrial use case of machine safety monitoring."
pub.1048133989,Energy and Memory Efficient Mapping of Bitonic Sorting on FPGA,"Parallel sorting networks are widely employed in hardware implementations for sorting due to their high data parallelism and low control overhead. In this paper, we propose an energy and memory efficient mapping methodology for implementing bitonic sorting network on FPGA. Using this methodology, the proposed sorting architecture can be built for a given data parallelism while supporting continuous data streams. We propose a streaming permutation network (SPN) by ""folding"" the classic Clos network. We prove that the SPN is programmable to realize all the interconnection patterns in the bitonic sorting network. A low cost design for sorting with minimal resource usage is obtained by reusing one SPN . We also demonstrate a high throughput design by trading off area for performance. With a data parallelism of p (2 ≤ p ≤ N/ log2 N), the high throughput design sorts an N-key sequence with latency O(N/p), throughput (# of keys sorted per cycle) O(p) and uses O(N) memory. This achieves optimal memory efficiency (defined as the ratio of throughput to the amount of on-chip memory used by the design) of O(p/N). Another noteworthy feature of the high throughput design is that only single-port memory rather than dual-port memory is required for processing continuous data streams. This results in 50% reduction in memory consumption. Post place-and-route results show that our architecture demonstrates 1.3x ∼1.6x improvment in energy efficiency and 1.5x ∼ 5.3x better memory efficiency compared with the state-of-the-art designs."
pub.1175652471,A Machine Learning-Driven Wireless System for Structural Health Monitoring,"The paper presents a wireless system integrated with a machine learning (ML) model for structural health monitoring (SHM) of carbon fiber reinforced polymer (CFRP) structures, primarily targeting aerospace applications. The system collects data via carbon nanotube (CNT) piezoresistive sensors embedded within CFRP coupons, wirelessly transmitting these data to a central server for processing. A deep neural network (DNN) model predicts mechanical properties and can be extended to forecast structural failures, facilitating proactive maintenance and enhancing safety. The modular design supports scalability and can be embedded within digital twin frameworks, offering significant benefits to aircraft operators and manufacturers. The system utilizes an ML model with a mean absolute error (MAE) of 0.14 on test data for forecasting mechanical properties. Data transmission latency throughout the entire system is less than one second in a LAN setup, highlighting its potential for real-time monitoring applications in aerospace and other industries. However, while the system shows promise, challenges such as sensor reliability under extreme environmental conditions and the need for advanced ML models to handle diverse data streams have been identified as areas for future research."
pub.1128890440,Event-Based Neuromorphic Vision for Autonomous Driving,"As a bio-inspired and emerging sensor, an event-based neuromorphic vision sensor has a different working principle compared to the standard frame-based cameras, which leads to promising properties of low energy consumption, low latency, high dynamic range (HDR), and high temporal resolution. It poses a paradigm shift to sense and perceive the environment by capturing local pixel-level light intensity changes and producing asynchronous event streams. Advanced technologies for the visual sensing system of autonomous vehicles from standard computer vision to event-based neuromorphic vision have been developed. In this tutorial-like article, a comprehensive review of the emerging technology is given. First, the course of the development of the neuromorphic vision sensor that is derived from the understanding of biological retina is introduced. The signal processing techniques for event noise processing and event data representation are then discussed. Next, the signal processing algorithms and applications for event-based neuromorphic vision in autonomous driving and various assistance systems are reviewed. Finally, challenges and future research directions are pointed out. It is expected that this article will serve as a starting point for new researchers and engineers in the autonomous driving field and provide a birds-eye view to both neuromorphic vision and autonomous driving research communities."
pub.1182131973,A Hardware Accelerator for Quantile Estimation of Network Packet Attributes,"Measuring statistical properties of network traffic can improve our understanding of traffic distribution and help us detect short and long-term anomalies. However, computing the exact value of these properties requires significant storage and computation, which limits their application in high-speed networks. Hardware accelerators provide the computational power to process a large sequence of network packets with high throughput and low latency, but their performance is ultimately limited by the amount of on-chip memory available on the device. Consequently, researchers have proposed sketch-based algorithms to estimate properties of a data stream with sub linear memory and theoretical estimation error bounds. In this paper, we present a streaming algorithm and hardware accelerator for quantile estimation, which is based on the architecture of the KLL sketch. Implemented on an AMD Virtex XCU55 UltraScale+ FPGA, the accelerator operates at a clock frequency of 356 MHz, thereby achieving a minimum line rate of 182 Gbps and a maximum estimation latency of 4.33 µs. When processing a set of 10 real traffic traces of up to 123 million packets, the accelerator estimates 1000 packet-size quantiles per trace with a median error of 0.39% or less, and a maximum error of 1.3% or less across all traces."
pub.1174082508,Online Detection of Outstanding Quantiles with QuantileFilter,"In quantile estimation within a stream of key-value pairs, recent work has made significant progress in query flexibility, supporting quantile estimation for any key using a unified statistical structure. However, despite this flexibility, their query speed falls behind, unable to match the high speed of online data insertion. This “offline query + online insertion” model is not ideal for online quantile estimation. Our goal is to online detect keys whose quantiles exceed a user-queried threshold in real-time, such as identifying the user whose 95 % latency exceeds 200ms in network data. These keys, termed “Quantile-Outstanding Keys,” are vital for anomaly detection in streaming data. In this paper, we propose QuantileFilter, the first approximate algorithm specifically designed for detecting quantile-outstanding keys. QuantileFilter overcomes existing limitations by 1) enabling fast online computation, capable of handling streaming data in real-time with a constant processing time for each data item, accelerating the state-of-the-art (SOTA) by 10 ~ 100 times, and 2) maintaining high space efficiency, saving 50 ~ 500 times storage space compared to the SOTA while maintaining the same accuracy. All associated code is available on GitHub."
pub.1181666909,LiteQUIC: Improving QoE of Video Streams by Reducing CPU Overhead of QUIC,"QUIC is the underlying protocol of the next generation HTTP/3, serving as the major vehicle delivering video data nowadays. As a userspace protocol based on UDP, QUIC features low transmission latency and has been widely deployed by content providers. However, the high computational overhead of QUIC shifts system knobs to CPUs in high-bandwidth scenarios. When CPU resources become the constraint, HTTP/3 exhibits even lower throughput than HTTP/1.1. In this paper, we carefully analyze the performance bottleneck of QUIC and find it results from ACK processing, packet sending, and data encryption. By reducing the ACK frequency, activating UDP generic segmentation offload (GSO), and incorporating PicoTLS, a high-performance encryption library, the CPU overhead of QUIC could be effectively reduced in stable network environments. However, simply reducing the ACK frequency also impairs the transmission throughput of QUIC under poor network conditions. To solve this, we develop LiteQUIC, which involves two mechanisms towards alleviating the overhead of ACK processing in addition to GSO and PicoTLS. We evaluate LiteQUIC in the DASH-based video streaming, and the results show that LiteQUIC achieves 1.2× higher average bitrate and 93.3% lower rebuffering time than an optimized version of QUIC with GSO and PicoTLS."
pub.1172598639,Lightweight Block and Stream Cipher Algorithm: A Review,"Most of the Internet of Things (IoT), cell phones, and Radio Frequency Identification (RFID) applications need high speed in the execution and processing of data. this is done by reducing, system energy consumption, latency, throughput, and processing time. Thus, it will affect against security of such devices and may be attacked by malicious programs. Lightweight cryptographic algorithms are one of the most ideal methods Securing these IoT applications. Cryptography obfuscates and removes the ability to capture all key information patterns ensures that all data transfers occur Safe, accurate, verified, legal and undeniable.  Fortunately, various lightweight encryption algorithms could be used to increase defense against various attacks to preserve the privacy and integrity of such applications. In this study, an overview of lightweight encryption algorithms, and methods, in addition, a modern technique for these algorithms also will be discussed. Besides, a survey for the algorithm that would use minimal power, require less time, and provide acceptable security to low-end IoT devices also introduced, Evaluating the results includes an evaluation of the algorithms reviewed and what was concluded from them. Through the review, we concluded that the best algorithms depend on the type of application used. For example, Lightweight block ciphers are one of the advanced ways to get around security flaws."
pub.1107291602,Internet of Things as a Service (iTaaS): Challenges and solutions for management of sensor data on the cloud and the fog,"Building upon cloud, IoT and smart sensors technologies we design and develop an IoT as a Service (iTaaS) framework, that transforms a user’s mobile device (e.g. a smart phone) to an IoT gateway which allows for fast and efficient data streams transmission to the cloud. We develop a two-fold solution, based on micro-services for the IoT (users’ smart devices) and the cloud side (back-end services). iTaaS includes configurations for (a) the IoT side to support data collection from IoT devices to a gateway on a real time basis and, (b) the cloud back-end side to support data sharing, storage and processing. iTaaS provides the technology foreground to enable immediate application deployments in the domain of interest. An obvious and promising implementation of this technology is e-Health and remote health monitoring. As a proof of concept we implement a real time remote patient monitoring system that integrates the proposed framework and uses Bluetooth Low Energy (BLE) pulse oximeter and heart rate monitoring sensing devices. The experimental analysis shows fast data collection, as (for our experimental setup) data is transmitted from the IoT side (i.e. the gateway) to the cloud in less than 130 ms. We also stress the back-end system with high user concurrency (e.g. with 40 users per second) and high data streams (e.g. 240 data records per second) and we show that the requests are executed at around 1 s, a number that signifies a satisfactory performance by considering the number of requests, the network latency and the relatively small size of the Virtual Machines implementing services on the cloud (2 GB RAM, 1 CPU and 20 GB hard disk size)."
pub.1175775412,Neuromorphic Facial Analysis with Cross-Modal Supervision,"Traditional approaches for analyzing RGB frames are capable of providing a
fine-grained understanding of a face from different angles by inferring
emotions, poses, shapes, landmarks. However, when it comes to subtle movements
standard RGB cameras might fall behind due to their latency, making it hard to
detect micro-movements that carry highly informative cues to infer the true
emotions of a subject. To address this issue, the usage of event cameras to
analyze faces is gaining increasing interest. Nonetheless, all the expertise
matured for RGB processing is not directly transferrable to neuromorphic data
due to a strong domain shift and intrinsic differences in how data is
represented. The lack of labeled data can be considered one of the main causes
of this gap, yet gathering data is harder in the event domain since it cannot
be crawled from the web and labeling frames should take into account event
aggregation rates and the fact that static parts might not be visible in
certain frames. In this paper, we first present FACEMORPHIC, a multimodal
temporally synchronized face dataset comprising both RGB videos and event
streams. The data is labeled at a video level with facial Action Units and also
contains streams collected with a variety of applications in mind, ranging from
3D shape estimation to lip-reading. We then show how temporal synchronization
can allow effective neuromorphic face analysis without the need to manually
annotate videos: we instead leverage cross-modal supervision bridging the
domain gap by representing face shapes in a 3D space."
pub.1122419811,Context-Aware Placement of Industry 4.0 Applications in Fog Computing Environments,"The fourth industrial revolution, widely known as Industry 4.0, is realizable through widespread deployment of Internet of Things (IoT) devices across the industrial ambiance. Due to communication latency and geographical distribution, Cloud-centric IoT models often fail to satisfy the Quality of Service requirements of different IoT applications assisting Industry 4.0 in real time. Therefore, Fog computing focuses on harnessing edge resources to place and execute these applications in the proximity of data sources. Since most of the Fog nodes are heterogeneous, distributed, and resource-constrained, it is challenging to place Industry 4.0-oriented applications (I4OAs) over them ensuring time-optimized service delivery. Diversified data sensing frequency of different industrial IoT devices and their data size further intensify the application placement problem. To address this issue, in this article we propose a context-aware application placement policy for Fog environments. Our policy coordinates the IoT device-level contexts with the capacity of Fog nodes and minimizes the service delivery time of various I4OAs such as image processing and robot navigation applications. It also ensures that the streams of input data flowing toward the placed applications neither congest the network nor increase the computing overhead of host Fog nodes significantly. Performance of the proposed policy is evaluated in both real-world and simulated Fog environments and compared with the existing placement policies. The experiment results show that our policy offers overall 16 improvement in service latency, network relaxation, and computing overhead management compared to other placement policies."
pub.1093739971,On effective data supply for multi-issue processors,"Emerging multi-issue microprocessors require effective data supply to sustain multiple instruction processing. The data cache structure, the backbone of data supply, has been organized and managed as one large homogenous resource, offering little flexibility for selective caching. While memory latency hiding techniques and multi-ported caches are critical to effective data supply, we show in this paper that even ideal non-blocking multi-ported caches fail to be sufficient in and of themselves in supplying data. We evaluate an approach in which the first level (L1) data cache is partitioned into multiple (multi-lateral) subcaches. The data reference stream of a running program is subdivided into two classes, and each class is mapped to a specific subcache whose management policy is more suitable for the access pattern of its class. This sort of selective organization and caching retains more useful data in the L1 Cache, which translates to more cache hits, less cache-memory bus contention and overall improvement in execution time. Our simulations show that a multi-lateral L1 cache of (8+1)KB total size generally performs as well as, and in some cases better than, an ideal multiported 16 KB cache structure in supplying data."
pub.1124346891,L-Heron: An open-source load-aware online scheduler for Apache Heron,"Apache Heron has emerged as a promising Data Stream Processing System (DSPS). However, it lacks intelligent scheduling strategy, which results in significant performance degradation for streaming applications in certain scenarios. In this paper, we first illustrate the inefficiencies and challenges of Heron default scheduling in current practice through experimental observations and analysis. Motivated by our observations, we propose L-Heron, an online scheduler based on Heron, which has the following features: (i) based on runtime information, it can improve the data processing efficiency by using the load-aware online scheduling, which heuristically minimizes the overall communication overhead by identifying the traffic load; (ii) it is load aware, which can effectively balance the workload of a topology to avoid heavy performance loss caused by overloading of worker nodes; (iii) it provides an online scheduling interface that is transparent to users, which allows users to focus on their scheduling logic and easily deploy them to the system. Additionally, we have evaluated L-Heron on well-known example topologies and a realistic application. Extensive experimental results show that the effectiveness of L-Heron is consistent among multiple metrics including the system completion latency, inter-node traffic, CPU utilization and throughput, with respect to Heron and recent related work."
pub.1168442146,Model-driven development of data intensive applications over cloud resources,"The proliferation of sensors over the last years has generated large amounts
of raw data, forming data streams that need to be processed. In many cases,
cloud resources are used for such processing, exploiting their flexibility, but
these sensor streaming applications often need to support operational and
control actions that have real-time and low-latency requirements that go beyond
the cost effective and flexible solutions supported by existing cloud
frameworks, such as Apache Kafka, Apache Spark Streaming, or Map-Reduce
Streams. In this paper, we describe a model-driven and stepwise refinement
methodological approach for streaming applications executed over clouds. The
central role is assigned to a set of Petri Net models for specifying functional
and non-functional requirements. They support model reuse, and a way to combine
formal analysis, simulation, and approximate computation of minimal and maximal
boundaries of non-functional requirements when the problem is either
mathematically or computationally intractable. We show how our proposal can
assist developers in their design and implementation decisions from a
performance perspective. Our methodology allows to conduct performance
analysis: The methodology is intended for all the engineering process stages,
and we can (i) analyse how it can be mapped onto cloud resources, and (ii)
obtain key performance indicators, including throughput or economic cost, so
that developers are assisted in their development tasks and in their decision
taking. In order to illustrate our approach, we make use of the pipelined
wavefront array."
pub.1100350635,Model-driven development of data intensive applications over cloud resources," The proliferation of sensors over the last years has generated large amounts of raw data, forming data streams that need to be processed. In many cases, cloud resources are used for such processing, exploiting their flexibility, but these sensor streaming applications often need to support operational and control actions that have real-time and low-latency requirements that go beyond the cost effective and flexible solutions supported by existing cloud frameworks, such as Apache Kafka, Apache Spark Streaming, or Map-Reduce Streams. In this paper, we describe a model-driven and stepwise refinement methodological approach for streaming applications executed over clouds. The central role is assigned to a set of Petri Net models for specifying functional and non-functional requirements. They support model reuse, and a way to combine formal analysis, simulation, and approximate computation of minimal and maximal boundaries of non-functional requirements when the problem is either mathematically or computationally intractable. We show how our proposal can assist developers in their design and implementation decisions from a performance perspective. Our methodology allows to conduct performance analysis: The methodology is intended for all the engineering process stages, and we can (i) analyse how it can be mapped onto cloud resources, and (ii) obtain key performance indicators, including throughput or economic cost, so that developers are assisted in their development tasks and in their decision taking. In order to illustrate our approach, we make use of the pipelined wavefront array."
pub.1151898907,Improving GPU performance in multimedia applications through FPGA based adaptive DMA controller,"
                    Purpose
                    Deep learning techniques are unavoidable in a variety of domains such as health care, computer vision, cyber-security and so on. These algorithms demand high data transfers but require bottlenecks in achieving the high speed and low latency synchronization while being implemented in the real hardware architectures. Though direct memory access controller (DMAC) has gained a brighter light of research for achieving bulk data transfers, existing direct memory access (DMA) systems continue to face the challenges of achieving high-speed communication. The purpose of this study is to develop an adaptive-configured DMA architecture for bulk data transfer with high throughput and less time-delayed computation.
                  
                  
                    Design/methodology/approach
                    The proposed methodology consists of a heterogeneous computing system integrated with specialized hardware and software. For the hardware, the authors propose an field programmable gate array (FPGA)-based DMAC, which transfers the data to the graphics processing unit (GPU) using PCI-Express. The workload characterization technique is designed using Python software and is implementable for the advanced risk machine Cortex architecture with a suitable communication interface. This module offloads the input streams of data to the FPGA and initiates the FPGA for the control flow of data to the GPU that can achieve efficient processing.
                  
                  
                    Findings
                    This paper presents an evaluation of a configurable workload-based DMA controller for collecting the data from the input devices and concurrently applying it to the GPU architecture, bypassing the hardware and software extraneous copies and bottlenecks via PCI Express. It also investigates the usage of adaptive DMA memory buffer allocation and workload characterization techniques. The proposed DMA architecture is compared with the other existing DMA architectures in which the performance of the proposed DMAC outperforms traditional DMA by achieving 96% throughput and 50% less latency synchronization.
                  
                  
                    Originality/value
                    The proposed gated recurrent unit has produced 95.6% accuracy in characterization of the workloads into heavy, medium and normal. The proposed model has outperformed the other algorithms and proves its strength for workload characterization.
                  "
pub.1141006475,An Analysis of Computational Resources of Event-Driven Streaming Data Flow for Internet of Things: A Case Study,"Abstract
                  Information and communication technologies backbone of a smart city is an Internet of Things (IoT) application that combines technologies such as low power IoT networks, device management, analytics or event stream processing. Hence, designing an efficient IoT architecture for real-time IoT applications brings technical challenges that include the integration of application network protocols and data processing. In this context, the system scalability of two architectures has been analysed: the first architecture, named as POST architecture, integrates the hyper text transfer protocol with an Extract-Transform-Load technique, and is used as baseline; the second architecture, named as MQTT-CEP, is based on a publish-subscribe protocol, i.e. message queue telemetry transport, and a complex event processor engine. In this analysis, SAVIA, a smart city citizen security application, has been deployed following both architectural approaches. Results show that the design of the network protocol and the data analytic layer impacts highly in the Quality of Service experimented by the final IoT users. The experiments show that the integrated MQTT-CEP architecture scales properly, keeps energy consumption limited and thereby, promotes the development of a distributed IoT architecture based on constraint resources. The drawback is an increase in latency, mainly caused by the loosely coupled communication pattern of MQTT, but within reasonable levels which stabilize with increasing workloads."
pub.1094613414,Bandwidth-Aware Data Filtering in Edge-Assisted Wireless Sensor Systems,"By placing processing-capable devices at the edge of local wireless access networks, Edge Computing architectures have been recently proposed to connect mobile devices to computational power through a one-hop low-latency wireless link. In this paper, we propose a new design where edge assistance is used to control local data filtering at the mobile devices in bandwidth and energy constrained systems. We focus on realtime monitoring applications, where the video input from mobile devices is processed to centrally detect and recognize objects. The edge processor controls the activation and deactivation of local classifiers implemented by the mobile devices to remove useless portions of video frames. The objective is to adapt the video stream to time-varying bandwidth constraints, while minimizing the additional energy consumption introduced by local processing. To this end, an optimization problem is formulated for a loss function embodying the balance between the risk of violating the available bandwidth and the cost of overly-conservative data filtering. The edge assists the local decision by extracting parameters of the video, such as density of objects of interest in a frame, which influence the output of the sensor. Numerical results, obtained by performing a measurement campaign based on a real implementation, illustrate the tension between energy and bandwidth use for a Haar-feature based cascade classifier."
pub.1146014405,Proactive Attack Detection at the Edge through an Ensemble Deep Learning Model,"The new form of the Web involves numerous devices present in two infrastructures, i.e., the Internet of Things (IoT) and the Edge Computing (EC) infrastructure. IoT devices are adopted to record ambient data and host lightweight processing to provide support for applications offered to end users. EC is placed between the IoT and Cloud and can be the host of more advanced processing activities. It has gained popularity due to the increased computational resources compared to the IoT and the decreased latency in the provision of responses compared to the Cloud. A high number of nodes may be present at the EC that should secure the Quality of Service (QoS) of the desired applications. Apparently, EC nodes become central points where the collected data are collected and processed. Data processing (especially when data are sensitive) imposes various security issues that should be mitigated in order to maintain high QoS levels and the uninterrupted functioning of EC nodes. In this paper, motivated by the need of the increased security, we propose an ensemble scheme for the detection of attacks in the EC. Our distributed scheme relies on the adoption of deep learning to proactively detect potential malfunctions. Our model is embedded in EC nodes and is continuously applied upon the streams of data transferred by IoT devices to the EC. We present the details of our approach and evaluate it through a variety of simulation scenarios. Our intention is to reveal the strengths and weaknesses of the provided model when adopted in a very dynamic environment like the EC."
pub.1113473293,Efficient resource provisioning for elastic Cloud services based on machine learning techniques,"Automated resource provisioning techniques enable the implementation of elastic services, by adapting the available resources to the service demand. This is essential for reducing power consumption and guaranteeing QoS and SLA fulfillment, especially for those services with strict QoS requirements in terms of latency or response time, such as web servers with high traffic load, data stream processing, or real-time big data analytics. Elasticity is often implemented in cloud platforms and virtualized data-centers by means of auto-scaling mechanisms. These make automated resource provisioning decisions based on the value of specific infrastructure and/or service performance metrics. This paper presents and evaluates a novel predictive auto-scaling mechanism based on machine learning techniques for time series forecasting and queuing theory. The new mechanism aims to accurately predict the processing load of a distributed server and estimate the appropriate number of resources that must be provisioned in order to optimize the service response time and fulfill the SLA contracted by the user, while attenuating resource over-provisioning in order to reduce energy consumption and infrastructure costs. The results show that the proposed model obtains a better forecasting accuracy than other classical models, and makes a resource allocation closer to the optimal case."
pub.1131296454,High-throughput Digital Readout System for Real-time Ion Imaging using CMOS ISFET Arrays,"This paper demonstrates a novel readout platform for ISFET-based ion imagers which is capable of performing high-throughput data acquisition and real-time monitoring on high-speed chemical reactions. The front end employs a 128 x 128 array of integrated ISFET pH sensors fabricated in unmodified CMOS process. The array operates at a frame rate of up to 3000 fps for detecting hydrogen ion diffusion, generating a maximum data stream of 491.52 Mbps. A digital readout system consisting of a readout module for data buffering, an AXI master controller for accessing on-board DDR3 memory and a PCIe subsystem for transmitting data packets is implemented on an Alinx AX7103 development board to link the chip and the PC. The platform capabilities are demonstrated with a real-time ion imaging experiment, by observing the diffusion of NaOH pills in water within 320 ms, visualized on screen with a latency of 0.15 s. Lastly, different image processing algorithms including Gaussian, Bilateral and Non-local Mean are evaluated for noise reduction and an accelerator for the optimum filter is implemented for real-time ion-imaging."
pub.1040142609,Stateful hardware decompression in networking environment,"Compression and Decompression can significantly lower the network bandwidth requirements for common internet traffic. Driven by the demands of an enterprise network intrusion system, this paper defines and examines the requirements of popular dictionary-based decompression in the real-time network processing scenario. In particular, a ""stateful"" decompression is required that arises out of the packet oriented nature of current networks, where the decompression of the data of a packet depends on the decompressed contents of its preceeding packets composing the same data stream. We propose an effective hardware decompression acceleration engine, which fetches the history data into the accelerator's fast memory on-demand and hides the associated latency by exploring the parallelism of the dictionary-based decompression process. We specify and evaluate various design and implementation options of the fetch-on-demand mechanism, i.e. prefetch most frequently used history, on-accelerator history buffer management, and reuse of fetched history data. Through simulation-based performance study, we show the effectiveness of the proposed mechanism on hiding the overhead of stateful decompression. We further show the effects of the design options and the impact on the overall performance of the network service stack of an intrusion prevension system."
pub.1030471469,Real-time hardware implementation of a speed FSBMA used in H.264/AVC,"Nowadays, H.264/AVC reaches very high resolutions, flexible depending on the application, which particularly leads to a very large amount of data compression. The motion estimation defines the most-complex process and the most interesting at the same time; its complexity appears with a consumption of more than 60% of total encoding time which make it difficult to achieve real-time encoding required in many video applications, but the interesting part is the flexibility to make its implementation variant according to the application. For this purpose, the following paper presents a speed real-time hardware implementation of integer motion estimation used in H.264/AVC, the method is based on the full-search block-matching algorithm. The proposed architecture calculates best motion vector using a parallel process. It is composed of three processor modules and a comparator three values. Implementation results based on Field Programmable Gate Array devices uses Xilinx Virtex7 XC7VX550T show performance characteristics like low latency, high processing speed reaching 473 MHz of frequency. The processing capacity is up to 1080HD video streams with a search range of 48 × 48."
pub.1094576146,A Patient-Centered Medical Environment with Wearable Sensors and Cloud Monitoring,"In this paper, an integrated wearable application platform with physiological sensors are presented. We use the wearable devices to collect the electrocardiography (ECG) and respiration (RESP) signals. This wearable device is a prototype with analog front-end, a microcontroller, and a Bluetooth module. We attach the electrodes on the thorax to record single-lead ECG signal and thorax impedance variation of the users. The mobile phone is a platform for dealing with the digital signal processing. We design an Android app with convenient user interface for every application. With discrete wavelet transform (DWT), we can easily detect the important features such as P wave, QRS complex and T wave and reduce the interference of noise. We develop an application with the wearable device for the emotion recognition. The extracted-features of biomedical signals are implemented with methods by the cloud computing. We implement the cloud computing by Apache Storm, which can transmit the data by streaming via 3G/Wi-Fi. With the proposed stream processing framework for the relaxation state calculation with wearable ECG sensors, under 5 people monitoring simultaneously, the latency and response time can be reduced by 10 times."
pub.1010123324,Complex event processing for reactive security monitoring in virtualized computer systems,"The number of security incidents in computer systems is steadily increasing, despite intrusion detection and prevention mechanisms deployed as countermeasures. Many existing intrusion detection and prevention systems struggle to keep up with new threats posed by zero-day attacks and/or have serious performance impacts through extensive monitoring, questioning their effectiveness in most real-life scenarios. In this paper, we present a new approach for reactive security monitoring in a virtualized computer environment based on minimally-intrusive dynamic sensors deployed vertically across virtualization layers and horizontally within a virtual machine instance. The sensor streams are analyzed using a novel federation of complex event processing engines and an optimized query index to maximize the performance of continuous queries, and the results of the analysis are used to trigger appropriate actions on different virtualization layers in response to detected security anomalies. Furthermore, a novel event store that supports fast event logging is utilized for offline analysis of collected historical data. Experiments show that the proposed system can execute tens of thousands of complex, stateful detection rules simultaneously and trigger actions efficiently and with low latency."
pub.1134744427,Towards Distributed Edge-based Systems,"In the past few years, researchers from academia and industry stakeholders suggest adding more computational resources (i.e., storage, networking, and processing) closer to the end-users and IoT domain, respectively, at the edge of the network. Such computation entities perceived as edge devices aim to overcome high-latency issues between the cloud and the IoT domain. Thus, processing IoT data streams closer to the end-users and IoT domain can solve several operational challenges. Since then, a plethora of application-specific IoT systems are introduced, mainly hard-coded, inflexible, and limited extensibility for future changes. Additionally, most IoT systems maintain a centralized design to operate without considering the dynamic nature of edge networks. In this paper, we discuss some of the research issues, challenges, and potential solutions to enable: i) deploying edge functions on edge resources in a distributed manner and ii) deploying and scaling edge applications on-premises of Edge-Cloud infrastructure. Additionally, we discuss in detail the three-tier Edge-Cloud architecture. Finally, we introduce a conceptual framework that aims to enable easy configuration and deployment of edge-based systems on top of heterogeneous edge infrastructure and present our vision within a smart city scenario."
pub.1099531071,Time-Critical Computing on a Single-Chip Massively Parallel Processor,"The requirement of high performance computing at low power can be met by the parallel execution of an application on a possibly large number of programmable cores. However, the lack of accurate timing properties may prevent parallel execution from being applicable to time-critical applications. We illustrate how this problem has been addressed by suitably designing the architecture, implementation, and programming model, of the Kalray MPPA®-256 single-chip many-core processor. The MPPA®-256 (Multi-Purpose Processing Array) processor integrates 256 processing engine (PE) cores and 32 resource management (RM) cores on a single 28nm CMOS chip. These VLIW cores are distributed across 16 compute clusters and 4 I/O subsystems, each with a locally shared memory. On-chip communication and synchronization are supported by an explicitly addressed dual network-on-chip (NoC), with one node per compute cluster and 4 nodes per I/O subsystem. Off-chip interfaces include DDR, PCI and Ethernet, and a direct access to the NoC for low-latency processing of data streams. The key architectural features that support time-critical applications are timing compositional cores, independent memory banks inside the compute clusters, and the data NoC whose guaranteed services are determined by network calculus. The programming model provides communicators that effectively support distributed computing primitives such as remote writes, barrier synchronizations, active messages, and communication by sampling. POSIX time functions expose synchronous clocks inside compute clusters and mesosynchronous clocks across the MPPA®-256 processor."
pub.1167718174,Edge-Computing-Enabled Abnormal Activity Recognition for Visual Surveillance,"Due to the ever increasing number of closed circuit television (CCTV) cameras worldwide, it is the need of the hour to automate the screening of video content. Still, the majority of video content is manually screened to detect some anomalous incidence or activity. Automatic abnormal event detection such as theft, burglary, or accidents may be helpful in many situations. However, there are significant difficulties in processing video data acquired by several cameras at a central location, such as bandwidth, latency, large computing resource needs, and so on. To address this issue, an edge-based visual surveillance technique has been implemented, in which video analytics are performed on the edge nodes to detect aberrant incidents in the video stream. Various deep learning models were trained to distinguish 13 different categories of aberrant incidences in video. A customized Bi-LSTM model outperforms existing cutting-edge approaches. This approach is used on edge nodes to process video locally. The user can receive analytics reports and notifications. The experimental findings suggest that the proposed system is appropriate for visual surveillance with increased accuracy and lower cost and processing resources."
pub.1117676794,Gray Box Modeling Methodology for Runtime Prediction of Apache Spark Jobs,"Nowadays, many data centers facilitate data processing and acquisition by developing multiple Apache Spark jobs which can be executed in private clouds with various parameters. Each job might take various application parameters which influence its execution time. Some examples of application parameters can be a selected area of interest in spatiotemporal data processing application or a time range of events in a complex event stream processing application. To predict its runtime accurately, these application parameters shall be considered during constructing its runtime model. Runtime prediction of Spark jobs allows us to schedule them efficiently in order to utilize cloud resources, increase system throughput, reduce job latency and meet customers requirements, e.g. deadlines and QoS. Also, the prediction is considered as important advantage when using a pay-as-you-go pricing model. In this paper, we present a gray box modeling methodology for runtime prediction of each individual Apache Spark job in two steps. The first one is building a white box model for predicting the input RDD size of each stage relying on prior knowledge about its behaviour and taking the application parameters into consideration. The second one is extracting a black box runtime model of each task by observing its runtime metrics according to various allocated resources and variant input RDD sizes. The modeling methodology is validated with experimental evaluation on a real-world application, and the results show a high matching accuracy which reached 83–94% of the actual runtime of the tested application."
pub.1124215209,Machine Learning Models for Stock Prediction Using Real-Time Streaming Data,"In recent years stock prediction has attracted a lot of attention to the researchers in financial sectors. Apart from the static log data, streaming data has also been proven to be a perennial source of data analysis collected in real-time, which basically deals with the continuous flow of data carrying information from sources like websites, mobile phone applications, server logs, social websites, trading floors, etc. The classifying model made out of historical data can be relentlessly honed to give even more accurate results since its outcome is always compared to the next tick of the clock. In this study, an attempt is made to develop machine learning models to predict the potential prices of a company’s stock which helps in making financial decisions. Spark streaming has been considered for the processing of humongous data and data ingestion tools like NodeJS have been further used for analysis. Earlier researches are made on the same concept but the present goal of the study is to develop such a model that is scalable, fault-tolerant and has a lower latency. The model rests on a distributed computing architecture called the Lambda Architecture which helps in attaining the goals as intended. Upon analysis, it is found that prediction of stock values is more accurate when support vector regression is applied. The historical stock values are considered as supervised datasets for training the models."
pub.1130877843,Where and When: Event-Based Spatiotemporal Trajectory Prediction from the iCub’s Point-Of-View,"Fast, non-linear trajectories have been shown to be more accurately visually measured, and hence predicted, when sampled spatially (that is when the target position changes) rather than temporally, i.e. at a fixed-rate as in traditional frame-based cameras. Event-cameras, with their asynchronous, low latency information stream, allow for spatial sampling with very high temporal resolution, improving the quality of the data and the accuracy of post-processing operations. This paper investigates the use of Long Short-Term Memory (LSTM) networks with event-cameras spatial sampling for trajectory prediction. We show the benefit of using an Encoder-Decoder architecture over parameterised models for regression on event-based human-to-robot handover trajectories. In particular, we exploit the temporal information associated to the events stream to predict not only the incoming spatial trajectory points, but also when these will occur in time. After having studied the proper LSTM input/output sequence length, the network performance are compared to other regression models. Then, prediction behavior and computational time are analysed for the proposed method. We carry out the experiment using an iCub robot equipped with event-cameras, addressing the problem from the robot perspective."
pub.1085345140,Adaptive Avatar Handoff in the Cloudlet Network,"In a traditional big data network, data streams generated by User Equipments (UEs) are uploaded to the remote cloud (for further processing) via the Internet. However, moving a huge amount of data via the Internet may lead to a long End-to-End (E2E) delay between a UE and its computing resources (in the remote cloud) as well as severe traffic jams in the Internet. To overcome this drawback, we propose a cloudlet network to bring the computing and storage resources from the cloud to the mobile edge. Each base station is attached to one cloudlet and each UE is associated with its Avatar in the cloudlet to process its data locally. Thus, the E2E delay between a UE and its computing resources in its Avatars is reduced as compared to that in the traditional big data network. However, in order to maintain the low E2E delay when UEs roam away, it is necessary to hand off Avatars accordingly—it is not practical to hand off the Avatars’ virtual disks during roaming as this will incur unbearable migration time and network congestion. We propose the LatEncy Aware Replica placemeNt (LEARN) algorithm to place a number of replicas of each Avatar's virtual disk into suitable cloudlets. Thus, the Avatar can be handed off among its cloudlets (which contain one of its replicas) without migrating its virtual disk. Simulations demonstrate that LEARN reduces the average E2E delay. Meanwhile, by considering the capacity limitation of each cloudlet, we propose the LatEncy aware Avatar hanDoff (LEAD) algorithm to place UEs’ Avatars among the cloudlets such that the average E2E delay is minimized. Simulations demonstrate that LEAD maintains the low average E2E delay."
pub.1154094378,Cost-Efficient Scheduling of Streaming Applications in Apache Flink on Cloud,"Stream processing has been gaining extensive attention in the past few years. Apache Flink is a new generation of distributed stream processing engines that can process a great deal of data in real-time with low latency. But the default scheduler of Flink adopts a random task scheduling strategy, which does not consider the cost and load balancing in the cloud environment. In this article, a cost-efficient task scheduling algorithm (CETSA) and a cost-efficient load balancing algorithm (LBA-CE) for Flink are proposed to reduce the job execution cost while optimizing load balancing. First, a cost-efficient model and a load balancing model based on Flink are constructed. Then, the core mechanism of Flink task scheduling is improved based on the cost-efficient model and the improved task scheduler is implemented. In addition, the concept of node adaptation is introduced into cost-efficient scheduling according to the load balancing model, ensuring that the cluster load is balanced as much as possible while reducing the cost in a heterogeneous cluster. Extensive experiments have been performed with Hibench's Wordcount and Fixwindow workloads in the cloud environment. The experimental results indicate that compared to the baseline scheduling algorithm, the proposed algorithms reduce the cost by about 37.9% and 20.2% on average, and the load deviation of the cluster is reduced by about 23.1% and 24.6% on average, respectively. In summary, the proposed algorithms in this paper can significantly reduce the cost of executing jobs and optimize the load balancing of the cluster in Flink."
pub.1146054214,On the Performance of Link Space Communications using NB-LDPC Codes on Embedded Parallel Systems,"This paper introduces a novel concept for exploiting low-power edge graphics processing units (GPUs) for decoding higher-order non-binary low-density parity-check (LDPC) codes within a good performance level. In the proposed remote system, we exploit the asynchronous and simultaneous use of CPU and GPU resources, time-encoded data streams, and the concept of multi-codeword decoding. We report a coding gain superior to 1dB compared to the binary counterpart for the optimal sum-product algorithm (SPA). We compare our proposed solution against dedicated application-specific integrated-circuit (ASIC) designs, showing that, although behind, the edge GPU is competitive in terms of performance and energy, while supporting a significantly reduced development effort. Moreover, the experiments confirm that the proposed edge architecture provides a promising framework for Galois fields of order up to 256 and also from short to moderate code length equivalent to the binary (128, 64) and (512, 256) codes, supporting efficient and low-latency remote processing, reaching 2 Mbit/s, in conformity with the CCSDS-231 standard, under a global 7W power budget."
pub.1145414782,Performance Optimization of Serverless Edge Computing Function Offloading Based on Deep Reinforcement Learning,"How to make task offloading strategy to meet the stringent latency requirements of IoT applications under the constraints of resource-limited edge server is a key challenge in edge computing. Deep Reinforcement Learning (DRL) based methods have been proposed to ensure long-term running performance optimization by interacting with the edge computing environment to learn offloading strategy. However, there are some challenges in deploying DRL offloading applications in edge computing, such as the extensive and diversity of experience data required for the training of DRL agents and accompanied by extremely high exploration costs. Recently, serverless computing or Function as a Service (FaaS) is a new event-driven computing paradigm that uses lightweight containers to run task functions and which has been introduced in diverse applications of edge computing, AI inference prediction and stream processing that have stringent latency requirements. To address the above challenges, in the serverless edge computing (SEC) environment, we propose an experience-sharing deep reinforcement learning-based distributed function offloading method, called ES-DRL. Each edge serverless platform (EFaaS) in ES-DRL obtains the system states of the local SEC environment, feeds it into the local DRL agent network and gets the function offloading strategy. Then, each EFaaS uploads the experience data interacting with the environment to the global shared replay buffer in cloud and optimizes the parameters of the local network by randomly sampling a batch of data from it. A population-guided policy search method is introduced to speed up the convergence of the DRL agent and avoid falling into the local optimum. Experimental results show that ES-DRL improves the convergence speed and function success rate and reduces the average latency of the function by 10%-34% compared to the existing task offloading methods."
pub.1122264586,Automatic Translation of Spatio-Temporal Logics to Streaming-Based Monitoring Applications for IoT-Equipped Autonomous Agents,"Environments in which IoT-equipped autonomous agents and humans tightly interact require safety rules that monitor the agents' behaviors. In this context, expressive and human-comprehensible rules based on Spatio-Temporal Logics (STLs) are desirable because they are informative and easy to maintain. Unfortunately, STLs usually build on ad-hoc platforms implementing the logic semantics. We tackle this limitation with a mechanism to transparently compile STL rules to monitoring applications composed of standard data streaming operators, thus opening up the use of high-throughput and low-latency Stream Processing Engines for monitoring rule compliance in realistic, data-rich IoT scenarios. Our contribution can favor a broader and faster adoption of STLs for IoT-equipped agent monitoring by separating the concerns of designing a rule from those of implementing its semantics. Together with our formal description of how to translate STLs to the streaming domain, we evaluate our prototype implementation based on Apache Flink, studying the effects of parameters such as time and space resolution on the monitoring performance."
pub.1137574711,A Survey of Lightweight Cryptography for Power-Constrained IoT Devices: Security Challenges and Issues,"As the Internet of Things (IoT) is growing exponentially, the number of data shared between IoT devices is increasing at an exponential rate. Almost all IoT devices are battery concerned devices in which power is very low, and these devices are interconnected and communicated to perform certain tasks, and also to transmit confidential and sensitive data over a communication channel. The IoT now enables power-restricted systems for communication, processing, and communication decisions. In stratified IoT networks, there are many problems and challenges, including protection, cheap power usage in computers, restricted battery size, memory space, high efficiency, and minimal communication network latency. We address in this paper a state-of-the-art lightweight cryptographic algorithm that includes lightweight block ciphers, hash functions, stream ciphers, high-performance systems, and low power-constrained devices and IoT network tools in detail. The lightweight cryptography algorithms are evaluated based on key size, block size, round size, and structure. We also explore the security framework, challenges, and key solutions for the power-constrained device of the IoT system."
pub.1175491948,Distributed Federated and Incremental Learning for Electric Vehicles Model Development in Kafka-ML,"With the increasing development and deployment of new systems for efficient and clean mobility, Electric Vehicles (EVs) are becoming more and more common among people. Those produce large amounts of data streams that need to be collected and analyzed to understand user needs and improve their performance. For this purpose, Artificial Intelligence (AI) techniques are playing a very important role. Within this context, Kafka-ML is a Machine Learning (ML) framework that enables the consumption and processing of data streams and allows the flexible management and deployment of neural networks throughout their entire life cycle. Kafka-ML can work with Distributed Neural Networks (DNN) which reduce latency and response times, perform incremental training over time allowing models to adapt to data on the fly, and carry out Federated Learning (FL) processes for this type of algorithms so a more robust global model can be created while maintaining data privacy and security, but all this separately. This work has considered the joint implementation of FL, for anonymous data sharing, incremental learning for continuous training of the models, and DNN for distribution of the models across different points on the map. All this applied within a Vehicle-to-everything (V2X) domain where EV usage and charge data can be shared to improve the user experience, as well as to better understand the behavior of this type of vehicles and their charging points to achieve savings, and how it affects people daily lives. An evaluation of the system related to this EV use case is presented to demonstrate the viability of the tool."
pub.1125321062,Job scheduler for streaming applications in heterogeneous distributed processing systems,"In this study, we investigated the problem of scheduling streaming applications on a heterogeneous cluster environment and, based on our previous work, developed the maximum throughput scheduler algorithm (MT-Scheduler) for streaming applications. The proposed algorithm uses a dynamic programming technique to efficiently map the application topology onto the heterogeneous distributed system based on computing and data transfer requirements, while also taking into account the capacity of the underlying cluster resources. The proposed approach maximizes the system throughput by identifying and minimizing the time incurred at the computing/transfer bottleneck. The MT-Scheduler supports scheduling applications structured as a directed acyclic graph. We conducted experiments using three Storm microbenchmark topologies in both simulation and real Apache Storm environments. In terms of the performance evaluation, we compared the proposed MT-Scheduler with the simulated round robin and the default Storm scheduler algorithms. The results indicated that the MT-Scheduler outperforms the default round robin approach in terms of both the average system latency and throughput."
pub.1138615437,Cloud — Edge Offloading Model for Vehicular Traffic Analysis,"The proliferation of smart sensing and computing devices, capable of collecting a vast amount of data, has made the gathering of the necessary vehicular traffic data relatively easy. However, the analysis of these big data sets requires computational resources, which are currently provided by the Cloud Data Centers. Nevertheless, the Cloud Data Centers can have unacceptably high latency for vehicular analysis applications with strict time requirements. The recent introduction of the Edge computing paradigm, as an extension of the Cloud services, has partially moved the processing of big data closer to the data sources, thus addressing this issue. Unfortunately, this unlocked multiple challenges related to resources management. Therefore, we present a model for scheduling of vehicular traffic analysis applications with partial task offloading across the Cloud –/– Edge continuum. The approach represents the traffic applications as a set of interconnected tasks composed into a workflow that can be partially offloaded to the Edge. We evaluated the approach through a simulated Cloud - Edge environment that considers two representative vehicular traffic applications with a focus on video stream analysis. Our results show that the presented approach reduces the application response time up to eight times while improving energy efficiency by a factor of four."
pub.1163946247,Distributed out-of-memory NMF on CPU/GPU architectures,"We propose an efficient distributed out-of-memory implementation of the non-negative matrix factorization (NMF) algorithm for heterogeneous high-performance-computing systems. The proposed implementation is based on prior work on NMFk, which can perform automatic model selection and extract latent variables and patterns from data. In this work, we extend NMFk by adding support for dense and sparse matrix operation on multi-node, multi-GPU systems. The resulting algorithm is optimized for out-of-memory problems where the memory required to factorize a given matrix is greater than the available GPU memory. Memory complexity is reduced by batching/tiling strategies, and sparse and dense matrix operations are significantly accelerated with GPU cores (or tensor cores when available). Input/output latency associated with batch copies between host and device is hidden using CUDA streams to overlap data transfers and compute asynchronously, and latency associated with collective communications (both intra-node and inter-node) is reduced using optimized NVIDIA Collective Communication Library (NCCL) based communicators. Benchmark results show significant improvement, from 32X to 76x speedup, with the new implementation using GPUs over the CPU-based NMFk. Good weak scaling was demonstrated on up to 4096 multi-GPU cluster nodes with approximately 25,000 GPUs when decomposing a dense 340 Terabyte-size matrix and an 11 Exabyte-size sparse matrix of density 10-6$$10^{-6}$$."
pub.1020688122,On hiding latency in reconfigurable systems,"Recursive solutions are effective software techniques that are difficult to map into hardware due to their dependency on input size and data values. As a result, most high-level design tools do not allow for recursive calls. In this paper we present a technique for mapping the merge-sort algorithm, as a case study, into a reconfigurable system. Our mapping employs an on-line prediction method to reconfigure the necessary hardware only when the need arises, and to hide the reconfiguration delay. As a result, our implementation uses the smallest possible size hardware to sort an input data stream without prior knowledge of its length and eliminates the reconfiguration delay penalty. We outline a reconfigurable system with self-organizing multiple-buses as the communication subsystem. The processing elements and memory modules are connected to the multiple-buses as a linear array. We also demonstrate the effectiveness of adding simple LUTs to the multiple-buses in improving the throughput by allowing for pipelining at the word level."
pub.1167151665,Context Caching for IoT-Based Applications: Opportunities and Challenges,"The Internet of Things (IoT) is growing at a rapid pace. Applications using IoT technologies have transformed many activities to be digitized enabling more productivity, economy, and quality of work. Context Management Platforms (CMPs) that unify heterogeneous streams of big IoT data and derive insights of a sensed environment (called context) using inferencing, massively enhance the smartness of IoT-based applications. Handling this massive scale of data and processing in an IoT-ecosystem for context-aware applications are both time and resource consuming, especially consid-ering the bottlenecks in network and processing resources. While traditional data caching is a time-proven technique to enable low-latency delivery of data items for a superior perceived user experience, work in context caching is extremely limited. Context information is different from typically discussed forms of data in many ways. Context-aware traditional data caching techniques have limited applicability to caching context information due to many unique challenges. These challenges can be categorized by features of context, context quality demands, techniques for caching con-text information, and context cache memory technologies. We categorically discuss each challenge in this article supported by real-world scenarios and experimental results. We contend that context caching is distinct from traditional data caching techniques and highlight the importance of con-text caching for time-critical, adaptive, context-aware applications. This article aims to demystify the unique research opportunities and challenges when developing context caching techniques for scale and efficiency objectives of CMPs and provide directions for future work."
pub.1093533067,Cooperative Safety: a Combination of Multiple Technologies,"Governmental Transportation Authorities' interest in Car to Car and Car to Infrastructure has grown dramatically over the last few years in order to increase the road safety and reduce traffic emission. The achievement of these objectives is subject to development of three aspects: Transmission, Localization and Sensor Networks. New wireless technique evolved form current WiFi technology shall be able to curb down the timing latency to achieve timely and efficient communication among vehicles. Relative positioning is essential to predict whether two cars are on route of collision. Experts estimate that positioning accuracy must be below one meter in order to provide the necessary reaction time. Many technical issues exist in this field as current GPS solutions do not provide this level of accuracy. There are multiple standalone approaches existing for sensing networks including imaging, radar and lidar. In order to create fault tolerant SIL3 compliant systems, data fusion is obligatory. The amalgamation of these different data streams requires powerful multicore processing to recognize and react to multiple concurrent scenarios."
pub.1023884513,Cooperative safety,"Governmental Transportation Authorities' interest in Car to Car and Car to Infrastructure has grown dramatically over the last few years in order to increase the road safety and reduce traffic emission. The achievement of these objectives is subject to development of three aspects: Transmission, Localization and Sensor Networks. New wireless technique evolved form current WiFi technology shall be able to curb down the timing latency to achieve timely and efficient communication among vehicles. Relative positioning is essential to predict whether two cars are on route of collision. Experts estimate that positioning accuracy must be below one meter in order to provide the necessary reaction time. Many technical issues exist in this field as current GPS solutions do not provide this level of accuracy. There are multiple standalone approaches existing for sensing networks including imaging, radar and lidar. In order to create fault tolerant SIL3 compliant systems, data fusion is obligatory. The amalgamation of these different data streams requires powerful multicore processing to recognize and react to multiple concurrent scenarios."
pub.1095274758,STREAMING WORKLOAD GENERATOR FOR TESTING BILLING MEDIATION PLATFORM IN TELECOM INDUSTRY,"Billing Mediation Platform (BMP) in Telco is used to process real-time streams of Call Detail Records (CDRs) which can number tens of billions a day. The comprehensive records generated by BMPs can be used for billing and accounting, fraud detection, campaign management, spam filtering, traffic analysis, and churn prediction. Many of these applications are characterized by real-time processing requiring high throughput, low-latency analysis of CDRs. Testing such BMPs has different dimensions, stress testing of analytics for scalability, correctness of analytics, what-if scenarios, all of which require CDRs with realistic volumetric and contextual properties. We propose WLG, a framework for testing and benchmarking BMPs which involves generating high volumes of CDRs representative of real-world data. The framework is flexible in its ability to express and tune the workload generation to simulate CDRs from broad range of traffic patterns while preserving different spatio-temporal correlations and content-level information observed in real-world CDRs."
pub.1162634893,TinyPillarNet: Tiny Pillar-Based Network for 3D Point Cloud Object Detection at Edge,"Limited by huge computational cost, high inference latency and large memory consumption, existing 3D point cloud object detection methods are hard to be deployed on Internet of Things (IoT) edge devices. To handle this challenge, we present an extremely tiny framework termed TinyPillarNet. This framework leverages innovative pillar encoder to represent point cloud as immensely tiny pseudo-maps for extremely shrinking the input 3D sensing data. Moreover, a compact dual-stream feature extraction network is put forward to respectively extract intrinsic feature and distributional saliency map, which jointly boosts the detection precision with the lowest hardware cost. Extended experiments on KITTI benchmark demonstrated that our TinyPillarNet yields applicable precision with a record tiny weight size of 1.69 MB at a high inference speed of 1.67 times faster than the current record. Furthermore, the specially designed prototype verification system achieves a superior energy efficiency, which outperforms the similar deep learning based point cloud processing solutions on FPGA with a big margin."
pub.1168818271,An AIoT-Based Approach to High-Throughput Grain Quality Control: Application to the Food Industry,"This paper contributes to the Internet-of-Things (IoT) and Sensor Network Symposium track by presenting a novel IoT-based solution for quality control in the grain industry. We present an innovative mechanical setup integrated with a sensor network for high-throughput seed scanning. This setup collaborates with an edge computing model, showcasing a real-world IoT application in industry. Using a dataset of 36,479 seed images spanning 8 different classes-including healthy grains and various defects-we train and validate deep learning models. The edge computing model ensures real-time, low-latency image analysis, essential for swift and precise grain quality classification. We stream the analyzed data to both the cloud and a local manufacturing execution system (MES) using IoT protocols, enabling on-the-fly optimization of processing devices. This approach transforms quality control in the grain industry, highlighting potential gains in efficiency, waste reduction, and quality assurance."
pub.1143613141,EdgeBOL,"Supporting Edge AI services is one of the most exciting features of future mobile networks. These services involve the collection and processing of voluminous data streams, right at the network edge, so as to offer real-time and accurate inferences to users. However, their widespread deployment is hampered by the energy cost they induce to the network. To overcome this obstacle, we propose a Bayesian learning framework for jointly configuring the service and the Radio Access Network (RAN), aiming to minimize the total energy consumption while respecting desirable accuracy and latency thresholds. Using a fully-fledged prototype with a software-defined base station (BS) and a GPU-enabled edge server, we profile a state-of-the-art video analytics AI service and identify new performance trade-offs. Accordingly, we tailor the optimization framework to account for the network context, the user needs, and the service metrics. The efficacy of our proposal is verified in a series of experiments and comparisons with neural network-based benchmarks."
pub.1143611388,Research and Implementation of Rule-oriented Subscription Application Based on Stream Computing,"With the increasing of video and image data in public security, it is urgent to store massive data and implement relevant application technologies. The public security video and image database system had been effective in real-time storage of massive data and related practical applications. This paper discussed the core processing technology called SparkStreaming, and introduced the core function in public security video and image database system which is rule-oriented subscription application. At the same time, the specific implementation algorithm in the paper was introduced in detail. As a practical case of large data batch computing system, the design of the system framework can build a low latency, high throughput rate, continuous and reliable running streaming system. Through relevant comparative experiments, the data process ability of the algorithm under different operation parameters configuration is obtained. Comparative experiments provided a powerful guidance for the later platform deployment, estimating resources and allocating them reasonably based on the size of data traffic. Other related applications based on SparkStreaming technology can refer to the design and algorithm in this paper, such as rule- oriented disposition application, etc. In the end, this paper proposed the improvement directions in the shortcomings of the algorithm implementation."
pub.1095595781,A C++ Shared-Memory Ring-Buffer Framework for Large-Scale Data Acquisition Systems,"Large-Scale Data Acquisition Systems are an essential part in many scientific experiments, and the processing of the large amount of data, represents a challenge in designing systems capable of managing such volume of data. Owing to the nature of this type of experiments, the processes responsible for gathering the data from devices which measure real-world phenomena, and those processes in charge of distributing the data to monitoring and/or controlling systems, shall communicate with accuracy and reliability. By running those processes concurrently in a multi-processor computer system such requirements of accuracy and reliability can be achieved. In this paper, we present the design of a C++ framework which implements a ring-buffer by using shared-memory as a fast mechanism of data communication among processes. Likewise, the framework controls the access to data in the shared ring-buffer by implementing inter-process synchronization objects in shared-memory. The effectiveness of the proposed solution is evaluated by evaluating the latency time from when a new data is written into the shared ring-buffer and the longest instant when such a data is gathered. After the experimental test, the results show that it is possible to develop a C++ framework for helping programmers to create data acquisition system when a large-scale data-stream is involved, getting suitable performance by using shared-memory."
pub.1169427638,D-Alarm: An Efficient Driver Drowsiness Detection and Alarming System,"In the coming years, the demand for IoT devices connecting our homes, cities, and industries will grow significantly. We now inhabit a world where interconnected smart devices can be controlled through single commands, fostering an era of automation, particularly in smart homes. IoT is at the forefront of this transformation, with many of these devices relying on machine learning models to decode data and make accurate predictions. However, as the number of connected devices escalates, network congestion becomes a challenge. To address this, incorporating machine learning intelligence into edge devices is essential. Edge computing, which involves processing data closer to its source, reduces latency and enables faster analysis. Consequently, IoT data can be gathered and processed at the edge, reducing the reliance on cloud-based solutions. Yet, deploying machine learning models on small IoT microcontroller units presents memory and computation constraints. This paper introduces a deep learning model for driver engagement recognition in real-time video streams, alongside optimization techniques for deployment on edge devices like Nvidia Jetson. The goal is to facilitate the execution of machine learning models on compact, low-power integrated circuits, addressing this critical bottleneck."
pub.1134175016,Buffer Sizing for Multimedia Flows in Packet-Switching NoCs,"Wormhole packet switching, used in many NoC designs, introduces jitter. This may produce violations of application deadlines. Several works in the literature propose stream workload models, and techniques for buffer sizing. These works do not consider the concurrency between different flows, or the NoC model is too abstract, masking the jitter introduced by data packaging and router processing. One technique to deal with jitter is to introduce a decoupling buffer (D-buffer) on the target IP. This buffer receives data from the NoC with jitter, while the target IP consumes data from this buffer at the application rate, without jitter. Two problems must be solved to implement D-buffers: (i) which size must the buffer have? (ii) how much time should be expected before data consumption starts (threshold)? This work proposes a general method to define D-buffer size and threshold, considering the influence of packaging, arbitration, routing and concurrency between flows. A traffic model for stream applications is detailed and used to characterize jitter sources in wormhole packet switching. Experimental results demonstrate the impact on multimedia flows with fixed and variable packet sizes (from real traffic traces). Simple traffic models employing constant frame sizes result in small D-buffers. On the other hand, employing multimedia frames from application traces (i.e. real application data) increases buffer size and threshold while still suppressing jitter. Another parameter analyzed in the paper is the percentage of deadline violations as a function of D-buffer size. The method guarantees throughput with no deadline violation and does not modify the NoC structure. The cost of adding such D-buffers is an increased latency and extra silicon area."
pub.1061615049,Discrete Event Simulation and Virtual Reality Use in Industry: New Opportunities and Future Trends,"This paper reviews the area of combined discrete event simulation (DES) and virtual reality (VR) use within industry. While establishing a state of the art for progress in this area, this paper makes the case for VR DES as the vehicle of choice for complex data analysis through interactive simulation models, highlighting both its advantages and current limitations. This paper reviews active research topics such as VR and DES real-time integration, communication protocols, system design considerations, model validation, and applications of VR and DES. While summarizing future research directions for this technology combination, the case is made for smart factory adoption of VR DES as a new platform for scenario testing and decision making. It is put that in order for VR DES to fully meet the visualization requirements of both Industry 4.0 and Industrial Internet visions of digital manufacturing, further research is required in the areas of lower latency image processing, DES delivery as a service, gesture recognition for VR DES interaction, and linkage of DES to real-time data streams and Big Data sets."
pub.1155134532,An event-based data processing system using Kafka container cluster on Kubernetes environment,"Smart manufacturing has become a big trend of a new industrial revolution in the manufacturing industry. The advancement of the Internet of Things has made production more efficient and effective through the automated collecting data system and Big Data technology. Dealing with a large amount of real-time production data will be a significant issue for intelligent manufacturing. This paper uses Apache Kafka’s high-performance, low-latency data stream processing platform to process data collection and store it in the Big Data System. Kafka was deployed through Kubernetes, where it has improved on the architecture’s scalability and applies this architecture to the aerospace manufacturing autoclave. These data are then used to analyze the autoclave equipment anomaly. Testing performed on the Kafka Producer Throughput demonstrates that in the event that all other parameters remain unchanged, the real throughput will increase along with the increase in the throughput limit that is being used. For instance, when the throughput limit is 1.2 million, the maximum throughput of this experiment is reached at 1.13 million transactions per second, while the transfer rate is 552.88 megabytes per second (MB/s). The value of the fetch size parameter is set to 10,48,576 by default (1 M). It takes half a time and a quarter of a time down, and it takes up to 2.5 times the value that was preset before you can witness the change in the parameters that affect the performance. The performance achieves its peak of 1.43 million data transferred per second at a speed of 347.93 megabytes per second, and the performance after that has a tendency to remain consistent."
pub.1095266317,"On 3G LTE Terminal Implementation — Standard, Algorithms, Complexities and Challenges","Currently, 3GPP standardizes an evolved UTRAN (E-UTRAN) within the Release 8 Long Term Evolution (LTE) project. Targets include higher spectral efficiency, lower latency, higher peak data rate when compared to previous 3GPP air inter-faces. The air interface of E-UTRAN is based on OFDMA and MIMO in downlink and on SCFDMA in uplink. Main challenges for a terminal implementation include efficient realization of the inner receiver, especially for channel estimation and equalisation, and the outer receiver including a turbo decoder which needs to handle data rates of up to 75 Mbps per spatial MIMO stream. We show that the inner receiver can nicely and straightforwardly be parallelized due to frequency domain processing. In addition to the computational complexity of even a simple linear equaliser, one of the challenges is an efficient implementation considering necessary flexibility for different MIMO modes, power consumption and silicon area. This paper will briefly overview the current LTE standard, highlight a functional data flow through the single entities of an LTE terminal and elaborate more on possible first implementation details, including sample algorithms and first complexity estimates."
pub.1105141763,FPGA Sensor Fusion System Design for IMU Arrays,"Different navigation systems have different requirements for attitude estimation, positioning, and control. To achieve high-accuracy at low-cost, several low-cost MEMS Inertial Measurement Units (IMU's) may be used instead of one high-performance but high-cost and power hungry mechanical IMU. The low-cost MEMS sensors require sensor fusion to aggregate several streams of low-quality sensor data into one high-quality data stream. Signal processing algorithms, such as the Kalman Filter (KF), are used to estimate and combine the output states of IMU arrays using matrix-based iterative techniques. Large IMU arrays are beneficial for estimating more than one type of physical quantities and reducing noise variance, but the underlying matrix dimensions of each KF variable increase drastically with array size. The brute force, iterative updating of these matrices using FPGAs or ASICs is not feasible due to the limitations on digital hardware resources. This paper addresses the scalability problem of IMU array sensor fusion using a specialized vector processor designed specifically to achieve real-time, high-throughput, IMU sensor array fusion based on the KF paradigm. The vector processor has been implemented in Artix-7 FPGA and shown to outperform a scalar processor by 100% in latency for a 100-component vector with the throughput being linear in the number of IMU sensors up to the limits of the FPGA resources. The tradeoffs between vector size, memory requirements, and sampling rates are also fully quantified."
pub.1130825155,CrocodileDB in action,"Existing stream processing and continuous query processing systems eagerly maintain standing queries by consuming all available resources to finish the jobs at hand, which can be a major source of wasting CPU cycles and memory resources. However, users sometimes do not need to see the up-to-date query result right after the data is ready, and thus allow a slackness of time before the result is returned, which provides new opportunities to avoid wasting resources. We proposed CrocodileDB, a resource-efficient database, where users specify a performance goal representing the maximally allowed slackness of time and the system generates a query plan to minimize resource consumption (e.g. memory consumption or CPU cycles) while meeting this performance goal at the same time. In this paper, we demonstrate how users interact with CrocodileDB and show how the time slackness enables our optimization of reducing CPU consumption: Incrementability-aware Query Processing (InQP). With the slackness specified by users, InQP can reduce computing resource waste by selectively deferring the execution of parts of a query that are not amenable to incremental executions (i.e. outputting tuples that can be deleted by later executions in a high probability). In this demonstration, users can set the performance goal as a trade-off between CPU consumption and query latency, and observe the CPU usages and other statistics to understand how InQP reduces computing resources."
pub.1156894621,MemPool: A Scalable Manycore Architecture with a Low-Latency Shared L1 Memory,"Shared L1 memory clusters are a common architectural pattern (e.g., in
GPGPUs) for building efficient and flexible multi-processing-element (PE)
engines. However, it is a common belief that these tightly-coupled clusters
would not scale beyond a few tens of PEs. In this work, we tackle scaling
shared L1 clusters to hundreds of PEs while supporting a flexible and
productive programming model and maintaining high efficiency. We present
MemPool, a manycore system with 256 RV32IMAXpulpimg ""Snitch"" cores featuring
application-tunable functional units. We designed and implemented an efficient
low-latency PE to L1-memory interconnect, an optimized instruction path to
ensure each PE's independent execution, and a powerful DMA engine and system
interconnect to stream data in and out. MemPool is easy to program, with all
the cores sharing a global view of a large, multi-banked, L1 scratchpad memory,
accessible within at most five cycles in the absence of conflicts. We provide
multiple runtimes to program MemPool at different abstraction levels and
illustrate its versatility with a wide set of applications. MemPool runs at 600
MHz (60 gate delays) in typical conditions (TT/0.80 V/25 {\deg}C) in 22 nm FDX
technology and achieves a performance of up to 229 GOPS or 180 GOPS/W with less
than 2% of execution stalls."
pub.1025620007,Optimistic Byzantine fault tolerance,"The primary concern of traditional Byzantine fault tolerance is to ensure strong replica consistency by executing incoming requests sequentially according to a total order. Speculative execution at both clients and server replicas has been proposed as a way of reducing the end-to-end latency. In this article, we introduce optimistic Byzantine fault tolerance. Optimistic Byzantine fault tolerance aims to achieve higher throughput and lower end-to-end latency by using a weaker replica consistency model. Instead of ensuring strong safety as in traditional Byzantine fault tolerance, nonfaulty replicas are brought to a consistent state periodically and on-demand in optimistic Byzantine fault tolerance. Not all applications are suitable for optimistic Byzantine fault tolerance. We identify three types of applications, namely, realtime collaborative editing, event stream processing, and services constructed with conflict-free replicated data types, as good candidates for applying optimistic Byzantine fault tolerance. Furthermore, we provide a design guideline on how to achieve eventual consistency and how to recover from conflicts at different replicas. In optimistic Byzantine fault tolerance, a replica executes a request immediately without first establishing a total order of the message, and Byzantine agreement is used only to establish a common state synchronization point and the set of individual states needed to resolve conflicts. The recovery mechanism ensures both replica consistency and the validity of the system by identifying and removing the operations introduced by faulty clients and server replicas."
pub.1117328208,Serving deep neural networks at the cloud edge for vision applications on mobile platforms,"The proliferation of high resolution cameras on embedded devices along with the growing maturity of deep neural networks (DNNs) has spawned powerful mobile vision applications. To enable applications on mobile devices, the offloading approach processes live video streams using DNNs on server-class GPU accelerators. However, their use in latency constrained applications is particularly challenging because of the large and unpredictable round-trip latency from mobile devices to the cloud computing resources. As a consequence, system designers routinely look for ways to offload to local servers at the cloud edge, known as the cloudlet. This paper explores the potential of serving multiple DNNs using the cloudlet model to implement complex vision applications on mobile devices. We present DeepQuery, a new mobile offloading system that is capable to serve DNNs with different structures for a wide range of tasks including object detection and tracking, scene graph detection, and video description. DeepQuery provides application programming interfaces to offload applications programed as Directed Acyclic Graphs of DNN queries, and employs data parallelization and input batching techniques to reduce processing delays. To improve GPU utilization, it co-locates real-time and delay-tolerant tasks on shared GPUs, and exploits a predictive and plan-ahead approach to alleviate resource contention caused by co-locating. We evaluate DeepQuery and demonstrate its effectiveness using several real world applications."
pub.1163500881,MemPool: A Scalable Manycore Architecture With a Low-Latency Shared L1 Memory,"Shared L1 memory clusters are a common architectural pattern (e.g., in GPGPUs) for building efficient and flexible multi-processing-element (PE) engines. However, it is a common belief that these tightly-coupled clusters would not scale beyond a few tens of PEs. In this work, we tackle scaling shared L1 clusters to hundreds of PEs while supporting a flexible and productive programming model and maintaining high efficiency. We present MemPool, a manycore system with 256 RV32IMAXpulpimg “Snitch” cores featuring application-tunable functional units. We designed and implemented an efficient low-latency PE to L1-memory interconnect, an optimized instruction path to ensure each PE's independent execution, and a powerful DMA engine and system interconnect to stream data in and out. MemPool is easy to program, with all the cores sharing a global view of a large, multi-banked, L1 scratchpad memory, accessible within at most five cycles in the absence of conflicts. We provide multiple runtimes to program MemPool at different abstraction levels and illustrate its versatility with a wide set of applications. MemPool runs at 600 MHz (60 gate delays) in typical conditions (TT/0.80 V/25 ${}^{\boldsymbol{\circ}}$∘C) in 22 nm FDX technology and achieves a performance of up to 229 GOPS or 180 GOPS/W with less than 2% of execution stalls."
pub.1094545766,Effective Optimistic-Checker Tandem Core Design through Architectural Pruning,"The performance of many important commercial workloads, such as on-line transaction processing, is limited by the frequent stalls due to off-chip instruction and data accesses. These applica- tions are characterized by irregular control flow and complex data access patterns that render many low-cost prefetching schemes, such as stream-based and stride-based prefetching, ineffective. For such applications, correlation-based prefetching, which is ca- pable of capturing complex data access patterns, has been shown to be a more promising approach. However, the large instruction and data working sets of these applications require extremely large correlation tables, making these tables impractical to be im- plemented on-chip. This paper proposes the epoch-based correla- tion prefetcher, which cost-effectively stores its correlation table in main memory and exploits the concept of epochs to hide the long latency of its correlation table access, and which attempts to elim- inate entire epochs instead of individual instruction and data miss- es. Experimental results demonstrate that the epoch-based correlation prefetcher, which requires minimal on-chip real estate to implement, improves the performance of a suite of important commercial benchmarks by 13% to 31% and significantly outper- forms previously proposed correlation prefetchers."
pub.1168064208,Reconfigurable Event-Driven Spiking Neuromorphic Computing near High-Bandwidth Memory,"This paper presents the hardware architecture and processing dataflow of a general-purpose reconfigurable, spike event-driven computing platform for running bio-inspired neural networks in real-time. Near-memory computing architecture and hardware-aware compiler optimization facilitate low-latency, distributed parallel execution of user-specified network models of arbitrary connection topology. The massively parallel processing architecture scales to implementation of very large networks through hierarchical address-event routing (HiAER) of spikes implementing long-range synaptic connectivity across the network. Large-size bio-inspired AI model capacity and high-performance neuronal network emulation in parallel hardware are enabled by virtue of memory-efficient network storage and execution dataflow optimized for sparse networks. Implemented on a high-end Field Programmable Gate Array (FPGA), the HiAER-Spike platform captures axonal and neuronal events in internal FPGA memories which, when activated, fetch synaptic connectivity from lookup tables stored in High Bandwidth Memory (HBM) 2.0 to update the neuron membrane state variables. Threshold crossing events trigger output spikes which in turn propagate as axonal activity through the network. The system streams input and output spike events over a PCIe 3.0 interface with the host station. A compiler toolchain generates the necessary FPGA and HBM hardware configuration for partitioning and mapping of the network topology and synaptic connectivity onto the HBM data structure. The memory organization and microarchitecture of the FPGA implementation supports 4M axons, 4M neurons, and 1B synapses, and leverages the nominal 450GBps throughput of HBM 2.0 for a throughput of 0.1 tera synaptic operations per second (TSynOps). The architecture efficiently handles both sparse connectivity and sparse activity for robust and low-latency event-driven inference for both edge and cloud computing, with examples demonstrating spike-based visual inference."
pub.1095467215,A Distributed Tree Data Structure for Real-Time OLAP On Cloud Architectures,"In contrast to queries for on-line transaction processing (OLTP) systems that typically access only a small portion of a database, OLAP queries may need to aggregate large portions of a database which often leads to performance issues. In this paper we introduce CR-OLAP, a Cloud based Real-time OLAP system based on a new distributed index structure for OLAP, the distributed PDCR tree, that utilizes a cloud infrastructure consisting of $(m+1)$ multi-core processors. With increasing database size, CR-OLAP dynamically increases $m$ to maintain performance. Our distributed PDCR tree data structure supports multiple dimension hierarchies and efficient query processing on the elaborate dimension hierarchies which are so central to OLAP systems. It is particularly efficient for complex OLAP queries that need to aggregate large portions of the data warehouse, such as “report the total sales in all stores located in California and New York during the months February-May of all years”. We evaluated CR-OLAP on the Amazon EC2 cloud, using the TPC-DS benchmark data set. The tests demonstrate that CR-OLAP scales well with increasing number of processors, even for complex queries. For example, on an Amazon EC2 cloud instance with eight processors, for a TPC-DS OLAP query stream on a data warehouse with 80 million tuples where every OLAP query aggregates more than 50% of the database, CR-OLAP achieved a query latency of 0.3 seconds which can be considered a real time response."
pub.1122517246,Enforcing Crash Consistency of Evolving Network Analytics in Non-Volatile Main Memory Systems,"Evolving graph processing has enabled the modeling of many complex network systems, e.g., online social networks and gene networks. Existing in-memory graph data structures cannot effectively exploit the current and ongoing adoption of emerging non-volatile main memory (NVMM) for two reasons. (1) Ephemeral graph data structures are not crash-consistent nor durable for NVMM. Corruption is likely when updating its correlated application-defined data and runtime states in the face of hardware or software failures. (2) NVMM writes and reads may incur higher latency than DRAM. Placing the data structures in NVMM may result in a significant loss of performance. In this paper, we propose a novel persistent evolving graph data structure, named NVGRAPH, for both computing and in-memory storage of evolving graphs in NVMM. We devise NVGRAPH as a multi-version data structure, wherein a minimum of one version of its data is stored in NVMM to provide the desired durability at runtime for failure recovery, and another version is stored in both DRAM and NVMM to reduce the NVMM-induced memory latency. We dynamically transform the layout of NVGRAPH including changing the size of its partition in DRAM and the position of its base snapshot exploiting network properties and data access patterns of workloads. For the evaluation of NVGRAPH, we implement four representative real-world graph applications: pagerank, BFS, influence maximization, and rumor source detection. The experimental results show that the performance of NVGRAPH is comparable to other in-memory data structure (e.g., CSR and LLAMA) while using 70% less DRAM. It scales well up to 10 billion edges and 201 snapshots and supports crash consistency. It offers up to the 21X speedup of execution time compared to the scale-up graph computation approaches (e.g., GraphChi and X-stream)."
pub.1173603614,Parallel continuous skyline query over high-dimensional data stream windows,"Real-time multi-criteria decision-making applications in fields like high-speed algorithmic trading, emergency response, and disaster management have driven the development of new types of preference queries. This is an example of a skyline search. Multi-criteria decision-making utilizes the skyline operator to extract highly significant tuples or useful data points from extensive sets of multi-dimensional databases. The user’s settings determine the results, which include all tuples whose attribute vector remains undefeated by another tuple. The extracted tuples are commonly known as the skyline set. Lately, there has been a growing trend in research studies to perform skyline queries on data stream applications. These queries consist of extracting desired records from sliding windows and removing outdated records from incoming data sets that do not meet user requirements. The datasets in these applications are extremely large and exhibit a wide range of dimensions that vary over time. Consequently, the skyline query is considered a computationally demanding task, with the challenge of achieving a real-time response within an acceptable duration. We must transport and process enormous quantities of data. Traditional skyline algorithms have faced new challenges due to limitations in data transmission bandwidth and latency. The transfer of vast quantities of data would affect performance, power efficiency, and reliability. Consequently, it is imperative to make alterations to the computer paradigm. Parallel skyline queries have attracted the attention of both scholars and the business sector. The study of skyline queries has focused on sequential algorithms and parallel implementations for multicore processors, primarily due to their widespread use. While previous research has focused on sequential algorithms, there is a limitation to comprehensive studies that specifically address modern parallel processors. While numerous articles have been published regarding the parallelization of regular skyline queries, there is a limited amount of research dedicated specifically to the parallel processing of continuous skyline queries. This study introduces PRSS, a continuous skyline technique for multicore processors specifically designed for sliding window-based data streams. The efficacy of the proposed parallel implementation is demonstrated through tests conducted on both real-world and synthetic datasets, encompassing various point distributions, arrival rates, and window widths. The experimental results for a dataset characterized by a large number of dimensions and cardinality demonstrate significant acceleration."
pub.1021060048,39. Electrophysiological diagnosis of early cns involvement in HIV-1 infection – Pilot study,"IntroductionThe aim of our study was to verify the possibility of early identification of HIV-related neural injury using visual evoked potentials (VEPs) in neurologically asymptomatic HIV seropositives. In the CART (combination antiretroviral therapy) era, the prevalence of neurocognitive impairment remains high, up to 50%, and HIV-associated neurocognitive disorder (HAND) has shifted towards a milder clinical presentation. Such a mild clinical presentation can escape detection [1].MethodsThe examination consisted of the Montreal Cognitive Assessment (MoCA) [2], and VEPs to pattern-reversal, motion-onset stimulation (radial movement), and of visual ERPs recorded during an odd-ball test (http://www.lfhk.cuni.cz/elf). Subjects: 9 homosexual men and 3 heterosexually infected women were examined in this study. All patients had ⩾350×106 CD4 cells/l blood at inclusion. The duration of the HIV infection was 0.5 – 7years, and mean age of the patients was 35 (24–50) years.ResultsP100 potential was recorded with no alteration in the VEP amplitude or latency. However, in 5 patients with CD4 counts 350–750×106cells/l, there was prolonged latency of the N160 peak compared to the reference values obtained in our laboratory, which suggests a dysfunction of the motion-processing (magnocellular system or the dorsal cortical stream) [3]. P300 latencies and MoCA results were within the normal range in all HIV patients.ConclusionsOur data suggest that motion-onset VEPs may be a sensitive measure of subclinical visual pathway dysfunction in early HIV-1 infection, however, we did not reveal any cognitive decline in this pilot group. Subsequently, the patients will be investigated using VEPs and visual ERPs at six-month intervals to evaluate the long-term development of the visual pathway involvement in HIV patients."
pub.1116595702,An advanced comparison on big data world computing frameworks,"Today's internet world becomes really introducing a landmark between Peta to Exa byte which is significantly generating an enormous size of data while computing the digital things including the format of each datasets which signifies highly unstructured because which could be generating from different social sites, IOT, Google engine, Twitter, Yahoo, monitoring and controlling through sensors essentially called big data. Because of this fast era, we apply just contemporary techniques with common tools regarding having focused performing, smooth process and to execute computations on huge data. Though such tremendous universal data has some shortcomings for getting effective processing, analyzing the universe immense datasets and scalability techniques. Apache open free source Hadoop does the latest big data weapon which can process Zetta byte dimensions of databases by its most developed and popular components as hdfs and Map Reduce, to make up vast storage facility plus great administration in the sense to process zettabyte of datasets as powerfully, flexible. MR likes more famous software popular structure for handling big-data existing issues with full parallel, highly distributed and most scalable manner. However, public and unrestricted source tools on Hadoop, map reduces become major limitations like poor allocate process on needy resources working regarding stream-oriented processing, Shortage significant viewpoints like latency, dynamic manner execution, optimization, computing as online and diverse logical solutions. We consider significant various complex data computing orientated techniques. This study paper address Apache fastest spark tool, online-oriented tool public and unrestricted source and Flink are in Apache project are efficient frameworks to conquer that limitation."
pub.1137884519,The Synergy of Complex Event Processing and Tiny Machine Learning in Industrial IoT,"Focusing on comprehensive networking, big data, and artificial intelligence,
the Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness
in factory operations. Various sensors and field devices play a central role,
as they generate a vast amount of real-time data that can provide insights into
manufacturing. The synergy of complex event processing (CEP) and machine
learning (ML) has been developed actively in the last years in IIoT to identify
patterns in heterogeneous data streams and fuse raw data into tangible facts.
In a traditional compute-centric paradigm, the raw field data are continuously
sent to the cloud and processed centrally. As IIoT devices become increasingly
pervasive and ubiquitous, concerns are raised since transmitting such amount of
data is energy-intensive, vulnerable to be intercepted, and subjected to high
latency. The data-centric paradigm can essentially solve these problems by
empowering IIoT to perform decentralized on-device ML and CEP, keeping data
primarily on edge devices and minimizing communications. However, this is no
mean feat because most IIoT edge devices are designed to be computationally
constrained with low power consumption. This paper proposes a framework that
exploits ML and CEP's synergy at the edge in distributed sensor networks. By
leveraging tiny ML and micro CEP, we shift the computation from the cloud to
the power-constrained IIoT devices and allow users to adapt the on-device ML
model and the CEP reasoning logic flexibly on the fly without requiring to
reupload the whole program. Lastly, we evaluate the proposed solution and show
its effectiveness and feasibility using an industrial use case of machine
safety monitoring."
pub.1170327885,LevelST: Stream-based Accelerator for Sparse Triangular Solver,"Over the past decade, much progress has been made to advance the acceleration of sparse linear operators such as SpMM and SpMV on FPGAs. Nevertheless, few works have attempted to address sparse triangular solver (SpTRSV) acceleration, and the performance boost is limited. SpTRSV is an elementary linear operator for many numerical methods, such as the least-square method. These methods, among others, are widely used in various areas, such as physical simulation and signal processing. Therefore, accelerating SpTRSV is crucial. However, many challenges impede accelerating SpTRSV, including (1) resolving dependencies between elements during forward or backward substitutions, (2) random access and unbalanced workloads across memory channels due to sparsity, (3) latency incurred by off-chip memory access for large matrices or vectors, and (4) data reuse for an unpredictable data sharing pattern. To address these issues, we have designed LevelST, the first FPGA accelerator leveraging high bandwidth memory (HBM) for solving sparse triangular systems. LevelST features (1) algorithm-hardware co-design of stream-based dependency resolution with reduced off-chip data movement, (2) resource sharing that improves resource utilization to scale up the architecture, (3) index modulo scheduling to balance workload, and (4) selective data prefetching from off-chip memory. LevelST is prototyped on an AMD Xilinx U280 HBM FPGA and evaluated with 16 sparse triangular matrices. Compared with the NVIDIA V100 and RTX 3060 GPUs over the cuSPARSE library, LevelST achieves a 2.65x speedup and 9.82x higher energy efficiency than the best of the V100 GPU and RTX 3060 GPU. The code is released on https://github.com/OswaldHe/LevelST (DOI: https://doi.org/10.5281/zenodo.10463345)."
pub.1149587196,PROMENADE: A big data platform for handling city complex networks with dynamic graphs,"Continuous data streams, generated by modern sensed cities, open many opportunities and perspectives in terms of developing new innovative services. To exploit this potential, flexible and scalable platforms are needed to ease the design, development, deployment, and operations of new city services. In recent years, several problem-specific platforms have been proposed in different application domains; however, to boost the evolution of smart cities, we claim the need for city-oriented platforms that can be easily customized to address different day-to-day life challenging problems. In this paper, we present the main architectural challenges and solutions proposed for the design of a novel open-source platform (named PROMENADE) characterized by: i) a data-driven graph-based modeling support to ensure high generality for addressing disparate problems related to the networked nature of many city infrastructures and systems, ii) the dynamic nature of the graph entities updated in real-time from different sources (e.g., IoT/Edge networks, data providers, etc.), and iii) high efficiency, scalability and flexibility to easily support new city services. The platform is designed around a general-purpose core that provides a set of built-in standard features such as data ingestion, storage, processing, and visualization exposed as a collection of containerized microservices. A specialization of the platform has been developed for road networks monitoring. It has been deployed in OpenShift/Kubernetes and tested using realistic datasets collected from the city of Lyon, France. The analysis addresses an important problem of big data processing pipelines: the synchronization between data ingestion and processing in order to produce an accurate result in useful time. To this end, we study different approaches for synchronization and show how the end-to-end latency is kept under control by leveraging the scalability of the platform."
pub.1123731039,mVideo: Edge Computing Based Mobile Video Processing Systems,"Computer vision is widely used to detect anomalies in video processing systems for public safety. Applying Deep Neural Networks (i.e., DNNs) in computer vision can achieve a high detection accuracy but it requires a huge amount of computing power, storage space, and video data. Thus, DNNs-based video analytics is mostly deployed in the cloud with video data steaming from a set of stationary cameras. There are mainly three issues in this setting. First, steaming a huge amount of video data from cameras to cloud leads to high bandwidth consumption and latency. Second, when DNNs are deployed on resource-limited devices like edge nodes to reduce communication costs, it is hard to achieve a high detection accuracy. Third, stationary cameras can only collect a limited amount of video data that covers a small area, so it barely satisfies the needs of the real-time analytics in applications like public safety. We propose a mobile edge computing-based video stream processing platform, mVideo, which conducts video analytics making full use of resources at the collaborative edge and cloud nodes. On the mVideo, a mechanism is designed to partition a video analysis task based on available resources on the mobile edge node. Then, the edge nodes pre-process video data using a lightweight DNN model and upload the results to cloud nodes for further analysis. Thus mVideo not only collects video data that covers a large area, but also reduces the communication costs. To validate the proposed platform, a face recognition application is deployed on the mVideo prototype. Experimental results reveal that compared with the existing cloud computing model, mVideo reduces video data volume transmitted to the cloud nodes and power consumption by up to 99.5% and 96.2%, respectively. mVideo also improves the execution time by 90.0% to optimize mobile video analytics performance."
pub.1154047123,Performance Assessment of Multi-GNSS Real-Time Products from Various Analysis Centers,"The performance of real-time precise point positioning (PPP) relies primarily on the availability and quality of orbit and clock corrections. In this research, we collected data streams from 12 real-time mount points of IGS Real-Time Service (RTS) or analysis centers for a one-month period and conducted a performance assessment, including product latency and data availability, accuracy of orbit, clock and positioning performance. The epoch availability of GPS, GLONASS, Galileo and BDS was more than 98.5%, 95.79%, 94.20% and 85.9%, respectively. In addition, the orbit and clock errors of different real-time corrections was investigated. Then, PPP in static and kinematic for 16 IGS stations was conducted. The results show the real-time PPP for different products has a longer convergence time and a slightly worse accuracy than those of the post-processing PPP. For static PPP over 24 h, the real-time products of WHU had the best performance, with a mean RMSE of 1.0 cm in the horizontal and vertical directions and a median convergence time of 12.0 min. The products of CAS had the faster convergence speed due to the shortest product latency. Regarding real-time kinematic PPP for GPS only in an hourly batch, the real-time products of WHU and ESA performed best with a mean RMSE of 10.8 cm and 9.5 cm in the horizontal and vertical directions, respectively. Additionally, the PPP for different real-time products with the multi-GNSS combination obtained higher accuracy than those with GPS only in post-processing or real-time mode, and the PPP with the GPS/GLONASS/Galileo/BDS combination had the fastest convergence speed and best positioning performance. The hourly based kinematic PPP results of CAS, DLR, GFZ and WHU with the GREC combination had positioning errors smaller than 5.2 cm."
pub.1150510764,Pervasive AI for IoT Applications: A Survey on Resource-Efficient Distributed Artificial Intelligence,"Artificial intelligence (AI) has witnessed a substantial breakthrough in a variety of Internet of Things (IoT) applications and services, spanning from recommendation systems and speech processing applications to robotics control and military surveillance. This is driven by the easier access to sensory data and the enormous scale of pervasive/ubiquitous devices that generate zettabytes of real-time data streams. Designing accurate models using such data streams, to revolutionize the decision-taking process, inaugurates pervasive computing as a worthy paradigm for a better quality-of-life (e.g., smart homes and self-driving cars.). The confluence of pervasive computing and artificial intelligence, namely Pervasive AI, expanded the role of ubiquitous IoT systems from mainly data collection to executing distributed computations with a promising alternative to centralized learning, presenting various challenges, including privacy and latency requirements. In this context, an intelligent resource scheduling should be envisaged among IoT devices (e.g., smartphones, smart vehicles) and infrastructure (e.g., edge nodes and base stations) to avoid communication and computation overheads and ensure maximum performance. In this paper, we conduct a comprehensive survey of the recent techniques and strategies developed to overcome these resource challenges in pervasive AI systems. Specifically, we first present an overview of pervasive computing, its architecture, and its intersection with artificial intelligence. We then review the background, applications and performance metrics of AI, particularly Deep Learning (DL) and reinforcement learning, running in a ubiquitous system. Next, we provide a deep literature review of communication-efficient techniques, from both algorithmic and system perspectives, of distributed training and inference across the combination of IoT devices, edge devices and cloud servers. Finally, we discuss our future vision and research challenges."
pub.1173117433,A neuromorphic electronic artist for robotic painting,"Recent advances in computer vision and deep learning have led to a surge of interest in the field of AI-generated art, including digital image creation and robot-assisted painting. Traditional painting machines rely on static images and offline processing to incorporate visual feedback into their painting process. However, this approach does not consider the dynamic nature of painting and fails to decompose complex overlapping patterns into individual strokes. As an alternative to frame-based RGB cameras, neuromorphic cameras capture changes in light intensity within a scene via asynchronous event streams, promising to overcome some of the inherent limitations of traditional computer vision techniques. In this project, a robotic system for physical painting is presented which utilizes event-based visual input from a Dynamic Vision Sensor (DVS) camera. To take advantage of the camera's ultra-low latency and sparse encoding, the proposed system also employs event-based information processing, implemented with spiking neural networks on the neuromorphic DynapSE-1 processor. The robotic system receives DVS sensory data which represents the trajectory of a brush stroke and computes the required joint velocities to recreate the stroke with a 6-DOF robotic arm in a closed-loop manner. The controller additionally integrates tactile feedback from a force-torque sensor to dynamically adjust the end-effector’s distance towards the canvas depending on the brush’s deformation. Within the scope of the project, it was further demonstrated how speed information about a perceived brush stroke can be extracted from DVS data. The system was tested in a real-world setting and successfully generated a collection of physical brush strokes. The proposed network is a first step towards a fully spiking robotic controller with the ability to seamlessly incorporate event-based sensory feedback, providing ultra-low latency responsiveness. Beyond its utility in robot-assisted painting, the developed network is applicable to any robotic task requiring real-time adaptive control."
pub.1093881570,Time-critical computing on a single-chip massively parallel processor,"The requirement of high performance computing at low power can be met by the parallel execution of an application on a possibly large number of programmable cores. However, the lack of accurate timing properties may prevent parallel execution from being applicable to time-critical applications. We illustrate how this problem has been addressed by suitably designing the architecture, implementation, and programming model, of the Kalray MPPA®-256 single-chip many-core processor. The MPPA® −256 (Multi-Purpose Processing Array) processor integrates 256 processing engine (PE) cores and 32 resource management (RM) cores on a single 28nm CMOS chip. These VLIW cores are distributed across 16 compute clusters and 4 I/O subsystems, each with a locally shared memory. On-chip communication and synchronization are supported by an explicitly addressed dual network-on-chip (NoC), with one node per compute cluster and 4 nodes per I/O subsystem. Off-chip interfaces include DDR, PCI and Ethernet, and a direct access to the NoC for low-latency processing of data streams. The key architectural features that support time-critical applications are timing compositional cores, independent memory banks inside the compute clusters, and the data NoC whose guaranteed services are determined by network calculus. The programming model provides communicators that effectively support distributed computing primitives such as remote writes, barrier synchronizations, active messages, and communication by sampling. POSIX time functions expose synchronous clocks inside compute clusters and mesosynchronous clocks across the MPPA®-256 processor."
pub.1012969069,Proactive scaling of distributed stream processing work flows using workload modelling,"In recent years there has been significant development in the area of distributed stream processing systems (DSPS) such as Apache Storm, Spark, and Flink. These systems allow complex queries on streaming data to be distributed across multiple worker nodes in a cluster. DSPS often provide the tools to add/remove resources and take advantage of cloud infrastructure to scale their operation. However, the decisions behind this are generally left to the administrators of these systems. There have been several studies focused on finding optimal operator deployments of DSPS operators across a cluster. However, these systems often do not optimise with regard to a given Service Level Agreement (SLA) and where they do, they do not take incoming workload into account. To our knowledge there has been little or no work based around proactively scaling the DSPS with regard to incoming workload in order to maintain SLAs. This PhD will focus on predicting incoming workloads using time series analysis. In order to assess whether a given predicted workload will breach a SLA the response of a DSPS work flow to incoming workload will be modelled using a queuing theory approach. The intention is to build a system that can tune the parameters of this queuing theoretic model, using output metrics such as end-to-end latency and throughput, as the DSPS is running. The end result will be a system that can identify potential SLA breaches before they happen, and initiate a proactive scaling response. Initially, Apache Storm will be used as the test DSPS, however it is anticipated that the system developed during this PhD will be applicable to other DSPS that use a graph-based description of the streaming work flow e.g. Apache Spark and Flink."
pub.1094707857,QoS- and Contention- Aware Resource Provisioning in a Stream Processing Engine,"This paper addresses the shared resource contention problem associated with the auto-parallelization of running queries in distributed stream processing engines. In such platforms, analyzing a large amount of data often requires to execute user-defined queries over continues raw-inputs in a parallel fashion at each single host. However, previous studies showed that the collocated applications can fiercely compete for shared resources, resulting in a severe performance degradation among applications. This paper presents an advanced resource allocation strategy for handling scenarios in which the target applications have different quality of service (QoS) requirements while shared-resource interference is considered as a key performance-limitina parameter. To properly allocate the best possible resource to each query, the proposed controller predicts the performance degradation of the running pane-level as well as the window-level queries when co-running with other queries. This is addressed as an optimization problem where a set of cost functions is defined to achieve the following goals: a) reduce the sum of QoS violation incidents over all machines; b) keep the CPU utilization level within an accepted range; and c) avoid fierce shared resource interference among collocated applications. Particle swarm optimization is used to find an acceptable solution at each round of the controlling period. The performance of the proposed solution is benchmarked with Round-Robin and best-effort strategies, and the experimental results clearly demonstrate that the proposed controller has the following advantages over its opponents: it increases the overall resource utilization by 15% on average while can reduce the average tuple latencies by 14%. It also achieves an average 123% improvement in preventing QoS violation incidents."
pub.1132041407,ACEP: an adaptive strategy for proactive and elastic processing of complex events,"The execution of complex event processing (CEP) applications on a set of clustered homogenous computing nodes is latency-sensitive, especially when workload conditions widely change at runtime. To manage the varying workloads of nodes in a scalable and cost-effective manner, adjusting the application parallelism at runtime is critical. To tackle the scalability challenge, we have extended an existing parallelization model called PARS that only supports stateless CEP operators and runs operators in parallel regions without changing the number of computing nodes assigned to parallel regions. We have added new features to PARS in support of stateful operators by introducing local controllers and new initiator and terminator event types, making partitioning fully transparent to application developers. We have proved the correctness of this extended model, called PARS+ , with respect to its presented formal definition. We have then used PARS+ as the base parallelization model in formulation of an adaptive strategy called ACEP to auto-scale operators including the stateful operators. Scaling decisions are governed by a predictive performance model that uses a control-theoretic method for estimating the resource and latency costs of each operator at runtime. The loads of clustered compute nodes are monitored, and compute nodes in a parallel region are reconfigured at runtime to ensure a balanced load on all compute nodes, accruing minimum cost to parallelize a stateful operator. ACEP minimizes network delays because it does not force using the shared state or techniques that employ state migration. We have built an event generator to simulate event sources and experimentally evaluate the ACEP strategy in terms of response time and resource costs. Two variant implementations of ACEP have been compared with the elastic strategy presented by Xiao et al. (2018), and the findings demonstrated that our implementations had adapted themselves better in different resource and response-time-sensitive scenarios, with lower response time (6%) and lower resource cost (8%)."
pub.1151491776,Accurate and Efficient Frame-based Event Representation for AER Object Recognition,"As a new type of vision sensor with high temporal resolution, high dynamic range and low power consumption, event cameras are increasingly used in the field of computer vision. Since the output of an event camera is a sparse and discrete event stream and individual event contains very little information, it is crucial to convert the event stream into a suitable event representation to facilitate the extraction of features and the recognition of the objects. In order to construct clear and complete features from raw event data efficiently, this paper proposes a frame-based three-channel event representation method, in which the temporal channels take the timestamp of the latest event at each pixel as its value to avoid contour overlapping caused by object motion and summarizing timestamps, and an event-count channel is constructed based on the number of events at each pixel to retain the overall spatial distribution of object contours. An outlier processing is also proposed to avoid “feature disappearance” based on the statistical distribution of the event count. We validate our method on several public event-based datasets and compare it with the existing state-of-the-art event representation methods, experimental results show that our method achieves the best classification accuracy with a lower time complexity, and it also yields a high level of robustness to latency."
pub.1004045955,GPU-accelerated name lookup with component encoding,"Named Data Networking (NDN) aims at redesigning the current Internet: using names to identify the wanted contents instead of using IP addresses to locate the end hosts, with the goal of substantially improving the data retrieval efficiency. Different from IP routers, NDN routers forward packets by names. An NDN name is composed of a number of length-variable components, causing the name to be tens or even hundreds of characters in length. Meanwhile, NDN routing tables could be several orders of magnitude larger than the current IP routing tables. This kind of complex name constitution plus the huge-sized name table makes wire speed name lookup an extremely challenging task.In pursuit of overcoming this challenge, we propose a Name Component Encoding (NCE) solution that assigns codes (integers) to name components. Along with an elaborate one-dimensional transition array and a local code allocation algorithm, NCE performs every node transition by a single memory access to boost lookup speed besides greatly compressing storage space. Moreover, we implement the name lookup engine on a GPU platform to exploit GPU’s massive parallel processing power; furthermore, pipeline and CUDA multi-stream techniques are applied to GPU to increase lookup throughout while reducing lookup latency. Experimental results demonstrate that, under the constraint of 100μs latency, our GPU-based name lookup engine can achieve 51.78 million searches per second on a name table containing 10 million prefixes. NCE also saves 59.57% memory cost comparing with the character trie and supports around 900K prefix insertions and 1.2 million prefix deletions per second."
pub.1109772472,An Efficient Compressive Sensing Method for Connected Health Applications,"The sensitive domain of healthcare intensifies the shortcomings associated with internet of things (IoT) based remote health monitoring systems in terms of their high-energy consumption and big data issues such as latency and privacy, caused by, the continuous stream of raw data. Hence, in the development of their remote elderly monitoring system (REMS), the authors focus on using embedded multicore architectures as powerful IoT edge devices and energy efficient signal acquisition and processing techniques to elevate such limitations. This study addresses the design of sparsifying matrices for electroencephalogram (EEG) signals in the context of compressed sensing. These signals are known to be non-sparse in both time and standard transform domains. The designed matrices are adapted to the data and are based on the autoregressive modeling of the signal and the singular value decomposition (SVD) of the impulse response matrix of the linear predictive coding (LPC) filter. To facilitate the hardware implementation and to prolong the life of the wearable node, the measurement matrix is chosen to be binary. The proposed algorithm has been applied to the EEGLab dataset ‘eeglab data set’ with an average normalized mean square error of 0.068."
pub.1143453679,An ultra-high-speed hardware accelerator for image reconstruction and stereo rectification on event-based camera,"Event-based cameras are novel bio-inspired vision sensors, which sense brightness changes rather than the actual intensity level. In contrast to conventional cameras, such cameras capture new information about the scene in the form of sparse events at a very short latency. Recently, some researchers have studied on efficient event-based camera reconstruction approaches to obtain high-quality images. These efforts make performing stereo vision based on event-cameras being possible. However, traditional stereo architecture cannot process such high-speed image streams generated by the event-based camera in real-time (in the order of μ s), which calls for new approaches. In this paper, we provide a set of practical solutions on visual image reconstruction and stereo rectification for spike camera, which serves two important procedures of stereo vision. We first provide FPGA-accelerated hardware architecture to achieve ultra-high-speed visual image reconstruction with full texture of natural scenes from spike data. Then, an FPGA-accelerated ultra-high-speed stereo rectification architecture is proposed to rectify the reconstructed images generated for stereo vision. In this architecture, a fully pipelined calculation module is designed to process the complex coordinate transformation operations, which puts in pipelined registers to maximize the clock frequency. To further improve the throughput, we propose a parallel processing architecture that uses multiple processing elements (PEs) to process multiple pixels per cycle. In addition, we design a memory management unit (MMU) to optimize the memory usage of the hardware resource. The whole architecture is effectively implemented on a Xilinx Zynq7100 FPGA chip. We evaluate the proposed architecture with different settings. The experiments show that our architecture can process the spike streams in real-time manner."
pub.1084708955,CBP: A New Parallelization Paradigm for Massively Distributed Stream Processing,"Resource efficiency is essential for distributed stream processing engines (DSPEs), in which a streaming application is modeled as an operator graph where each operator is parallelized into a number of instances to meet the low-latency and high-throughput requirements. The major objectives of optimizing resource efficiency in DSPEs include minimizing the communication cost by collocating the tasks that transfer a lot of data between each other, and by dynamically configuring the systems according to the load variations at runtime. In the current literature, most proposals handle these two optimizations separately, and a shallow integration of these techniques, such as performing the two optimizations one after another, would result in a suboptimal solution. In this paper, we present component-based parallelization (CBP), a new paradigm for optimizing the resource efficiency of DSPEs, which provides a framework for a deeper integration of the two optimizations. In the CBP paradigm, the operators are encapsulated into a set of non-overlapping components, in which operators are parallelized consistently, i.e., using the same partitioning key, and hence the intra-component communication is eliminated. According to the changes of workload, each component can be adaptively partitioned into multiple instances, each of which is deployed on a computing node. We build a cost model to capture both the communication cost and adaptation cost of a CBP plan, and then propose several optimization algorithms. We implement the CBP scheme and the optimization algorithms on top of Apache Storm, and verify its efficiency by an extensive experiment study."
pub.1029792164,Scalable real-time OLAP on cloud architectures,"In contrast to queries for on-line transaction processing (OLTP) systems that typically access only a small portion of a database, OLAP queries may need to aggregate large portions of a database which often leads to performance issues. In this paper we introduce CR-OLAP, a scalable Cloud based Real-time OLAP system based on a new distributed index structure for OLAP, the distributed PDCR tree. CR-OLAP utilizes a scalable cloud infrastructure consisting of multiple commodity servers (processors). That is, with increasing database size, CR-OLAP dynamically increases the number of processors to maintain performance. Our distributed PDCR tree data structure supports multiple dimension hierarchies and efficient query processing on the elaborate dimension hierarchies which are so central to OLAP systems. It is particularly efficient for complex OLAP queries that need to aggregate large portions of the data warehouse, such as “report the total sales in all stores located in California and New York during the months February–May of all years”. We evaluated CR-OLAP on the Amazon EC2 cloud, using the TPC-DS benchmark data set. The tests demonstrate that CR-OLAP scales well with increasing number of processors, even for complex queries. For example, for an Amazon EC2 cloud instance with 16 processors, a data warehouse with 160 million tuples, and a TPC-DS OLAP query stream where each query aggregates between 60% and 95% of the database, CR-OLAP achieved a query latency of below 0.3 s which can be considered a real time response."
pub.1106137176,Enabling Flexible Resource Allocation in Mobile Deep Learning Systems,"Deep learning provides new opportunities for mobile applications to achieve higher performance than before. Rather, the deep learning implementation on mobile device today is largely demanding on expensive resource overheads, imposes a significant burden on the battery life and limited memory space. Existing methods either utilize cloud or edge infrastructure that require to upload user data, however, resulting in a risk of privacy leakage and large data transfers; or adopt compressed deep models, nevertheless, downgrading the algorithm accuracy. This paper provides DeepShark, a platform to enable mobile devices with the ability of flexible resource allocation in using commercial-off-the-shelf (COTS) deep learning systems. Compared to existing approaches, DeepShark seeks a balanced point between time and memory efficiency by user requirements, breaks down sophisticated deep model into code block stream and incrementally executes such blocks on system-on-chip (SoC). Thus, DeepShark requires significantly less memory space on mobile device and achieves the default accuracy. In addition, all referred user data of model processing is handled locally, thus to avoid unnecessary data transfer and network latency. DeepShark is now developed on two COTS deep learning systems, i.e., Caffe and TensorFlow. The experimental evaluations demonstrate its effectiveness in the aspects of memory space and energy cost."
pub.1170552438,Advanced Privacy-Preserving Framework for Enhancing Fog Computing to Secure IoT Data Stream,"The proposed privacy-preserving framework based on fog computing for securing IoT data was examined through 10 experiment trials, with each trial dissecting a number of performance related metrics. Specifically, across the trials, latency values varied between 45 and 55 milliseconds, which signified that communication overhead was minute and that data were processed efficiently. Throughput values also varied considerably, yet only between 95 and 110 megabits per second, which signalled that the framework would allow processing data at high speeds. The rates of resource utilization measured in terms of MHR from CPU and Mused in the memory of specific fog nodes, varied between 58% and 76% . Regarding the scalability of the proposed framework, it was assessed based on the data collected and divided into the corresponding categories. From the energy consumption analysis, the values varied between 470 and 530 joules , which was recognized as the change caused by the shifting performance of the proposed IoT solution. Finally, communication overhead values varied from 970 to 1050 bytes, showing the differences in the effects which privacy-preserving frameworks have on data transmission. In conclusion, the results indicate that the proposed complex is immensely efficient in terms of protecting sensitive IoT data, ensuring a high level of security, preserving privacy, maintaining the current performance, and being adjusted to the new threats and security challenges."
pub.1094938710,Marlin: Taming the Big Streaming Data in Large Scale Video Similarity Search,"The extreme volume and staggeringly increasing rate inevitably produce unprecedented pressure on any large scale video sharing and hosting systems. Among the efforts to mitigate this pressure, content-based video similarity search is becoming more and more important with the exponential growth of the data size. Though various approaches have been proposed to address this problem, they are mainly focusing on the retrieval accuracy thus bringing video features with high complexity. Due to the complexity of the feature, these systems are based on the assumption that features representing videos have been obtained offline and stored in the database statically. However, the on-call efforts to move the feature extraction and similarity search from offline to online have been ignored in previous work. In this paper, we propose Marlin, a streaming data processing pipeline that efficiently extracts video features and retrieves video similarity information in a large scale video data system. We design a streaming feature extractor to handle the videos streaming into the system and establish the fined-grained resource allocation with a resource-aware data abstraction layer over streaming data to allocate computing resources among the videos with various resource demands. Besides that, we are pipelining the feature extraction and similarity search process with a distributed feature index, which supports real-time query and incremental index update. The experimental and the extensive real-world workload driven simulation results show that the proposed stream processing architecture achieves 25X speedup against the sequential feature extraction algorithm and 23X speedup against the sequential similarity search with a subsecond similarity query latency for a single request."
pub.1138571755,FERMAT: FPGA-Accelerated Heterogeneous Computing Platform Near NVMe Storage,"This paper proposes FERMAT, a versatile FPGA-accelerated near-storage computing platform that aims at significantly reducing data latency and energy consumption for data-intensive applications running on a heterogeneous computing system. Two key ideas are contributing to FERMAT’s success. Firstly, FERMAT, through creating direct and parallel I/O channels between a processor and NVMe (Non-Volatile Memory Express) storage with reconfigurable digital fabric as well as bypassing all OS software stack, can significantly reduce unnecessary data movements in order to deliver low-latency and high- bandwidth I/O. Secondly, FERMAT, through ""pre-computing"" a large amount of data near NVMe storage with FPGA-based computing engines, can effectively shift part of computing in a target application to the data source. To further facilitate the deployment of FERMAT, we provide general system-level support and an effective abstraction to the near-storage computing such that FERMAT can be used on any platform equipped with an NVMe storage and achieve overall higher performance. To fully validate this proposed approach, in hardware, we have designed and implemented an open-source FPGA-based self-managed NVMe controller that 1) directly connects an FPGA-based accelerator with the storage while bypassing all software stacks, and 2) transforms an FPGA device into an in-line computing engine, where multiple user-programmable streaming accelerators concurrently process file streams, that greatly improves data-intensive applications’ performance. In software, 3) we designed a dedicated software stack equipping FPGA-accelerated storage with a user-space filesystem supporting all the common file operations on modern Linux systems and a flexible and thread-safe compute engine programming interface to ease user control on the compute engine’s functionality. We measured the performance of FERMAT against the baseline with five benchmarks from three categories: security, graph query, and graph analysis. FERMAT demonstrated significant speedups ranging from 1.8x to 782.5x in processing throughput."
pub.1173735823,STAL: Spike Threshold Adaptive Learning Encoder for Classification of Pain-Related Biosignal Data,"This paper presents the first application of spiking neural networks (SNNs)
for the classification of chronic lower back pain (CLBP) using the EmoPain
dataset. Our work has two main contributions. We introduce Spike Threshold
Adaptive Learning (STAL), a trainable encoder that effectively converts
continuous biosignals into spike trains. Additionally, we propose an ensemble
of Spiking Recurrent Neural Network (SRNN) classifiers for the multi-stream
processing of sEMG and IMU data. To tackle the challenges of small sample size
and class imbalance, we implement minority over-sampling with weighted sample
replacement during batch creation. Our method achieves outstanding performance
with an accuracy of 80.43%, AUC of 67.90%, F1 score of 52.60%, and Matthews
Correlation Coefficient (MCC) of 0.437, surpassing traditional rate-based and
latency-based encoding methods. The STAL encoder shows superior performance in
preserving temporal dynamics and adapting to signal characteristics.
Importantly, our approach (STAL-SRNN) outperforms the best deep learning method
in terms of MCC, indicating better balanced class prediction. This research
contributes to the development of neuromorphic computing for biosignal
analysis. It holds promise for energy-efficient, wearable solutions in chronic
pain management."
pub.1130515877,Readout system of the ALICE Fast Interaction Trigger,"The Fast Interaction Trigger (FIT) detector will be essential for the operation of the ALICE experiment at CERN during Run 3 and 4 of the LHC . FIT will serve as an interaction trigger, luminometer, the first indicator of the vertex position, and the forward multiplicity counter. It will also provide the precise collision time for the TOF-based particle identification, yield the centrality and interaction plane for flow measurements, and measure cross sections of diffractive processes. In order to cope with an increased interaction rate of up to 1 MHz in proton-proton (pp) collisions and up to 50 kHz in Pb-Pb collisions, a new readout system for the ALICE FIT detector has been designed and implemented. FIT readout system is compatible both with the triggered and continuous (triggers-less) ALICE readout modes. The GBT-FPGA based FIT readout and trigger systems allow to stream data up to 5.5 MHz event rate with 3.2 Gbps data rate. The trigger processing time is only 225 ns. With the additional 200 ns delay along the connecting signal cables, the total latency of the FIT trigger is 425 ns."
pub.1095982462,Striim,"Real-time decisions and insights over real-time data have become the essential mantra of success for many enterprises. The real-time data is generated from a multitude of sources and they come in a streaming fashion with high volume and velocity. The data could be machine generated e.g. clickstream data, logs, sensor data from IoT devices or human generated e.g. social data, mission critical transactional data. This is causing a technological shift from storage driven architectures to event driven architectures for enterprises to be able to capture, integrate and analyze these large sets of data for real-time decision making. Striim is a novel end-to-end analytics platform that enables business users to easily develop and deploy analytical applications that can generate real-time insights over real-time streaming data; business users and developers use a SQL-like declarative language (that has been extended to include streaming semantics) to write application logic in Striim. Striim provides high-throughput, low-latency event processing on commodity hardware with a scale-out architecture. In this paper, we describe the architecture of Striim and discuss some of the key aspects of the platform (a) built-in real-time data capture including streaming change data capture from transactional databases (ii) a natively built storage and query engine that uses modern data structures like skip lists to store streaming window data and performs query optimization, planning and run-time code generation (iii) enabling application de-coupling using persisted streams."
pub.1163261663,INR-Arch: A Dataflow Architecture and Compiler for Arbitrary-Order Gradient Computations in Implicit Neural Representation Processing,"An increasing number of researchers are finding use for nth-order gradient
computations for a wide variety of applications, including graphics,
meta-learning (MAML), scientific computing, and most recently, implicit neural
representations (INRs). Recent work shows that the gradient of an INR can be
used to edit the data it represents directly without needing to convert it back
to a discrete representation. However, given a function represented as a
computation graph, traditional architectures face challenges in efficiently
computing its nth-order gradient due to the higher demand for computing power
and higher complexity in data movement. This makes it a promising target for
FPGA acceleration. In this work, we introduce INR-Arch, a framework that
transforms the computation graph of an nth-order gradient into a
hardware-optimized dataflow architecture. We address this problem in two
phases. First, we design a dataflow architecture that uses FIFO streams and an
optimized computation kernel library, ensuring high memory efficiency and
parallel computation. Second, we propose a compiler that extracts and optimizes
computation graphs, automatically configures hardware parameters such as
latency and stream depths to optimize throughput, while ensuring deadlock-free
operation, and outputs High-Level Synthesis (HLS) code for FPGA implementation.
We utilize INR editing as our benchmark, presenting results that demonstrate
1.8-4.8x and 1.5-3.6x speedup compared to CPU and GPU baselines respectively.
Furthermore, we obtain 3.1-8.9x and 1.7-4.3x lower memory usage, and 1.7-11.3x
and 5.5-32.8x lower energy-delay product. Our framework will be made
open-source and available on GitHub."
pub.1150161455,A key role of the hippocampal P3 in the attentional blink,"Abstract The attentional blink (AB) refers to an impaired identification of target stimuli (T2), which are presented shortly after a prior target (T1) within a rapid serial visual presentation (RSVP) stream. It has been suggested that the AB is related to a failed transfer of T2 into working memory and that hippocampus (HC) and entorhinal (EC) cortex are regions crucial for this transfer. Since the event-related P3 component has been linked to inhibitory processes, we hypothesized that the hippocampal P3 elicited by T1 may impact on T2 processing within HC and EC. To test this hypothesis, we reanalyzed microwire data from 21 patients, who performed an RSVP task, during intracranial recordings for epilepsy surgery assessment (Reber et al., 2017). We identified T1-related hippocampal P3 components in the local field potentials (LFPs) and determined the temporal onset of T2 processing in HC/EC based on single-unit response onset activity. In accordance with our hypothesis, T1-related single-trial P3 amplitudes at the onset of T2 processing were clearly larger for unseen compared to seen T2-stimuli. Moreover, increased T1-related single-trial P3 peak latencies were found for T2[unseen] versus T2[seen] trials in case of lags 1 to 3, which was in line with our predictions. In conclusion, our findings support inhibition models of the AB and indicate that the hippocampal P3 elicited by T1 plays a central role in the AB."
pub.1014726641,A speed FPGA hardware accelerator based FSBMA-VBSME used in H.264/AVC,"Image and video processing applications represent major challenge concerning real-time embedded systems. In video coding, adjacent frames are similar; this correlation can be exploited to reduce the amount of data to be transmitted, in this case reducing temporal redundancies. Actually, H.264/AVC is the most popular standard; the high performance that offers magnifies the difficulty of a real-time implementation. This complexity is mainly related to the operation of the motion estimation and requires high computational power. This paper presents an efficient hardware implementation of integer motion estimation for H.264/AVC encoder. The considered methodology is based on full search block matching algorithm for its regular algorithm implementation. The proposed architecture enables variable block size motion estimation and computes 41 motion vectors values (MVs) resulted from each 16 × 16 bloc and its derived sub-blocks. The proposed architecture calculates the best MV using a parallel process composed of three processor modules and a set of comparators three values. Implementation results based on field-programmable gate arrays devices uses Xilinx Virtex7 XC7VX550T show performance characteristics like low latency reduced up to 80 %, high processing speed reaching 443 MHz of frequency. The processing capacity is up to 1920 × 1088 HD video streams with a search range of 48 × 48."
pub.1175390778,High-Throughput GPU Implementation of Dilithium Post-Quantum Digital Signature,"Digital signatures are fundamental building blocks in various protocols to provide integrity and authenticity. The development of the quantum computing has raised concerns about the security guarantees afforded by classical signature schemes. CRYSTALS-Dilithium is an efficient post-quantum digital signature scheme based on lattice cryptography and has been selected as the primary algorithm for standardization by the National Institute of Standards and Technology. In this work, we present a high-throughput GPU implementation of Dilithium. For individual operations, we employ a range of computational and memory optimizations to overcome sequential constraints, reduce memory usage and IO latency, address bank conflicts, and mitigate pipeline stalls. This results in high and balanced compute throughput and memory throughput for each operation. In terms of concurrent task processing, we leverage task-level batching to fully utilize parallelism and implement a memory pool mechanism for rapid memory access. We propose a dynamic task scheduling mechanism to improve multiprocessor occupancy and significantly reduce execution time. Furthermore, we apply asynchronous computing and launch multiple streams to hide data transfer latencies and maximize the computing capabilities of both CPU and GPU. Across all three security levels, our GPU implementation achieves over 160× speedups for signing and over 80× speedups for verification on both commercial and server-grade GPUs. This achieves microsecond-level amortized execution times for each task, offering a high-throughput and quantum-resistant solution suitable for a wide array of applications in real systems."
pub.1139066432,PDU Normalizer Engine for Heterogeneous In-Vehicle Networks in Automotive Gateways,"In this work, authors propose the concept of Protocol Data Unit (PDU) normalization for heterogeneous In-Vehicle Networks (IVN) in automotive gateways (GW). Through the development of the so-called PDU Normalizer Engine (PDUNE), it is possible to create a novel protocol-agnostic frame abstraction layer for PDU and signal gatewaying functions. It consists of normalizing the format of the frames present in the GW ingress ports of any kind (e.g. CAN, LIN, FlexRay or Ethernet). That is, the PDUNE transforms the ingress frames into new refactored frames which are independent of their original network protocol, and this occurs in an early stage before being processed across the different stages of the GW controller till reaching the egress ports, optimizing thus not only the processing itself but also the resources and latencies involved. The hardware (HW) implementation of the PDUNE exploits Software Defined Networking (SDN) architectural concepts by decomposing each ingress frame in two streams: a data frame moving across the data plane and an instruction frame provided with all the necessary metadata that –in parallel and synchronously to the data frame– evolves through the different processing stages of the GW controller performed directly in HW from the control plane. The PDUNE has been synthesized as a coarse-grain configurable HW accelerator (HWA) or co-processor attachable to the system CPU of the GW controller, aimed at contributing towards future automotive zonal GW solutions targeting heterogeneous IVNs with stringent real-time routing and tunneling functional constraints."
pub.1137494376,FAST: A framework for high-performance medical image computing and visualization,"Medical image processing and visualization is often computationally demanding. Ultrasound images are acquired in real-time and needs to be processed at a high framerate with low latency. Computed tomography (CT) and magnetic resonance imaging (MRI) create large three dimensional volumes with sizes up to 512 × 512 × 800 voxels. In digital pathology, whole slide microscopy images can have an extreme image size of up to 200, 000 × 100, 000 pixels, which does not even fit into the memory of most computers. Thus, there is a need for smart data storage, processing and visualization methods to handle medical image data. The development of FAST started in 2014, the goal was to create an open-source framework which made GPU and parallel processing of medical images easy and portable. While there existed popular image processing libraries such as the visualization toolkit (VTK), insight toolkit (ITK) and OpenCV, the GPU processing capabilities were still implemented ad-hoc and often implied copying data back and forth from the GPU and CPU. Thus it was decided to use the new OpenCL API to create a cross-platform framework designed bottom-up with GPU processing at the very core. One of the design goals was to remove the burden of moving data back and forth from different processors and memory spaces from the developer. Instead, the developer requests access to the data on a given processor, and FAST will copy and update data as needed. Now, seven years later FAST version 3.2 is released, it still uses OpenCL 1.2 and OpenGL 3.3 at the core of almost all of its operations. FAST can stream images in real-time from ultrasound scanners, webcameras, Intel’s RealSense depth camera, and read many different formats from disk including medical formats such as DICOM, Metaimage and huge microscopy images stored as tiled image pyramids. FAST uses a processing pipeline concept, meaning that you define a pipeline as multiple processing and visualization steps first, then initiate the processing by executing the pipeline. The advantages of this is that it’s easy to change data sources and processing steps. The same pipeline used to process an ultrasound image on disk, can be used to process a real-time stream of ultrasound images. Today FAST pipelines can be created with C++, Python 3 and even without any programming using simple text files. The pipeline approach also opens up possibilities for load balancing and tuning based on analyzing the pipeline as computational graphs, although this has not yet been implemented. In the last five years or so, deep neural networks have become the standard for almost all image processing tasks. Many high-performance frameworks for deep neural network inference already exist, but have very different APIs and use different formats for storing neural network models. FAST now provides a common API for neural networks with multiple backends such as NVIDIA’s TensorRT, Intel’s OpenVINO and Google’s TensorFlow. This removes the burden of the us"
pub.1151445602,A signal packet router for the upgrade of the Muon spectrometer at the ATLAS experiment,"We present the design, characterization, and production of a signal packet router for the upgrade of the ATLAS forward muon detector. The router serves as a packet switch in one of the primary trigger paths. It handles up to 12 serial inputs at 4.8 Gbps/input from on-detector electronics and outputs four 4.8 Gbps to the trigger processing circuits. In the 12-to-4 packet switching process, the input streams are decoded and synchronized to a common clock domain for data packets forwarding and NULL packets suppression. Challenges in the design include operations of multi-giga hertz serial links with low and constant latency (<100 ns) for packet routing, as well as mitigation of radiation-induced effects in the harsh radiation environment over 10 years of detector operation. The Router design has been fully characterized and demonstrated to meet all specifications. In total 256 Routers were installed in the detector. Its design concept can be used in other similar applications, particularly in a radiation environment."
pub.1105345910,Distributed Scheduling of Event Analytics across Edge and Cloud,"
                    Internet of Things (IoT) domains generate large volumes of high-velocity event streams from sensors, which need to be analyzed with low latency to drive decisions. Complex Event Processing (CEP) is a Big Data technique to enable such analytics and is traditionally performed on Cloud Virtual Machines (VM). Leveraging captive IoT edge resources in combination with Cloud VMs can offer better performance, flexibility, and monetary costs for CEP. Here, we formulate an optimization problem for
                    energy-aware placement of CEP queries
                    , composed as an analytics dataflow, across a collection of edge and Cloud resources, with the goal of minimizing the end-to-end latency for the dataflow. We propose a Genetic Algorithm (GA) meta-heuristic to solve this problem and compare it against a brute-force optimal algorithm (BF). We perform detailed real-world benchmarks on the compute, network, and energy capacity of edge and Cloud resources. These results are used to define a realistic and comprehensive simulation study that validates the BF and GA solutions for 45 diverse CEP dataflows, LAN and WAN setup, and different edge resource availability. We compare the GA and BF solutions against random and Cloud-only baselines for different configurations for a total of 1,764 simulation runs. Our study shows that GA is within 97% of the optimal BF solution that takes hours, maps dataflows with 4--50 queries in 1--26s, and only fails to offer a feasible solution ≤20% of the time.
                  "
pub.1090913961,"A Low-Cost, Wearable Opto-Inertial 6-DOF Hand Pose Tracking System for VR","In this paper, a low cost, wearable six Degree of Freedom (6-DOF) hand pose tracking system is proposed for Virtual Reality applications. It is designed for use with an integrated hand exoskeleton system for kinesthetic haptic feedback. The tracking system consists of an Infrared (IR) based optical tracker with low cost mono-camera and inertial and magnetic measurement unit. Image processing is done on LabVIEW software to extract the 3-DOF position from two IR targets and Magdwick filter has been implemented on Mbed LPC1768 board to obtain orientation data. Six DOF hand tracking outputs filtered and synchronized on LabVIEW software are then sent to the Unity Virtual environment via User Datagram Protocol (UDP) stream. Experimental results show that this low cost and compact system has a comparable performance of minimal Jitter with position and orientation Root Mean Square Error (RMSE) of less than 0.2 mm and 0.15 degrees, respectively. Total Latency of the system is also less than 40 ms."
pub.1182031493,Real-Time Streaming in Distributed and Cooperative Sensing Networks,"Actual-time streaming in dispensed and cooperative sensing networks is a vital software program area for wireless networks because it allows transmitting, storing, and conveying significant volumes of real-time information. This period provides an opportunity to expand its functions, including environmental monitoring and control, business Automation, and Surveillance. In a distributed and cooperative sensing network, the nodes for sensing are coupled to a not uncommon communication community, and its standard realtime stream enables data streaming to be implemented through the shadow of a sluggard river. Most of the time, messages across the switch at a This results in that any message can be sent to be processed repeatedly before moving out of date in a very short time. Various methods can be used in a distributed and cooperative sensing network to ensure successful actualtime streaming. These include the Pub/Sub communication pattern, which enables nodes to subscribe to events and be notified when new events occur. Mirroring is used to ensure that each node is always given the latest data. At last, digital signal processing methods are used to reduce latency and increase the accuracy of specific data. These techniques support the performance and scalability required for an extensive application to provide real-time streaming in a distributed and cooperative sensing network."
pub.1041446601,Customization methodology for implementation of streaming aggregation in embedded systems,"Streaming aggregation is a fundamental operation in the area of stream processing and its implementation provides various challenges. Data flow management is traditionally performed by high performance computing systems. However, nowadays there is a trend of implementing streaming operators in low power embedded devices, due to the fact that they often provide increased performance per watt in comparison with traditional high performance systems. In this work, we present a methodology for the customization of streaming aggregation implemented in modern low power embedded devices. The methodology is based on design space exploration and provides a set of customized implementations that can be used by developers to perform trade-offs between throughput, latency, memory and energy consumption. We compare the proposed embedded system implementations of the streaming aggregation operator with the corresponding HPC and GPGPU implementations in terms of performance per watt. Our results show that the implementations based on low power embedded systems provide up to 54 and 14 times higher performance per watt than the corresponding Intel Xeon and Radeon HD 6450 implementations, respectively."
pub.1118548940,Distributed Scheduling of Event Analytics across Edge and Cloud,"Internet of Things (IoT) domains generate large volumes of high velocity
event streams from sensors, which need to be analyzed with low latency to drive
decisions. Complex Event Processing (CEP) is a Big Data technique to enable
such analytics, and is traditionally performed on Cloud Virtual Machines (VM).
Leveraging captive IoT edge resources in combination with Cloud VMs can offer
better performance, flexibility and monetary costs for CEP. Here, we formulate
an optimization problem for energy-aware placement of CEP queries, composed as
an analytics dataflow, across a collection of edge and Cloud resources, with
the goal of minimizing the end-to-end latency for the dataflow. We propose a
Genetic Algorithm (GA) meta-heuristic to solve this problem, and compare it
against a brute-force optimal algorithm (BF). We perform detailed real-world
benchmarks on the compute, network and energy capacity of edge and Cloud
resources. These results are used to define a realistic and comprehensive
simulation study that validates the BF and GA solutions for 45 diverse CEP
dataflows, LAN and WAN setup, and different edge resource availability. We
compare the GA and BF solutions against random and Cloud-only baselines for
different configurations, for a total of 1764 simulation runs. Our study shows
that GA is within 97% of the optimal BF solution that takes hours, maps
dataflows with 4 - 50 queries in 1 - 26 secs, and only fails to offer a
feasible solution <= 20% of the time."
pub.1106021739,An Initial Characterization of the Emu Chick,"The Emu Chick is a prototype system designed around the concept of migratory memory-side processing. Rather than transferring large amounts of data across power-hungry, high-latency interconnects, the Emu Chick moves lightweight thread contexts to near-memory cores before the beginning of each memory read. The current prototype hardware uses FPGAs to implement cache-less “Gossamer” cores for doing computational work and a stationary core to run basic operating system functions and migrate threads between nodes. In this initial characterization of the Emu Chick, we study the memory bandwidth characteristics of the system through benchmarks like STREAM, pointer chasing, and sparse matrix vector multiply. We compare the Emu Chick hardware to architectural simulation and Intel Xeon-based platforms. While it is difficult to accurately compare prototype hardware with existing systems, our initial evaluation demonstrates that the Emu Chick uses available memory bandwidth more efficiently than a more traditional, cache-based architecture. Moreover, the Emu Chick provides stable, predictable performance with 80% bandwidth utilization on a random-access pointer chasing benchmark with weak locality."
pub.1061714294,ITP: An Image Transport Protocol for the Internet,"Images account for a significant and growing fraction of Web downloads. The traditional approach to transporting images uses TCP, which provides a generic reliable in-order byte-stream abstraction, but which is overly restrictive for image data. We analyze the progression of image quality at the receiver with time, and show that the in-order delivery abstraction provided by a TCP-based approach prevents the receiver application from processing and rendering portions of an image when they actually arrive. The end result is that an image is rendered in bursts interspersed with long idle times rather than smoothly. This paper describes the design, implementation, and evaluation of the image transport protocol (ITP) for image transmission over loss-prone congested or wireless networks. ITP improves user-perceived latency using application-level framing (ALF) and out-of-order application data unit (ADU) delivery, achieving significantly better interactive performance as measured by the evolution of peak signal-to-noise ratio (PSNR) with time at the receiver. ITP runs over UDP, incorporates receiver-driven selective reliability, uses the congestion manager (CM) to adapt to network congestion, and is customizable for specific image formats (e.g., JPEG and JPEG2000). ITP enables a variety of new receiver post-processing algorithms such as error concealment that further improve the interactivity and responsiveness of reconstructed images. Performance experiments using our implementation across a variety of loss conditions demonstrate the benefits of ITP in improving the interactivity of image downloads at the receiver."
pub.1116885211,Low-latency graph streaming using compressed purely-functional trees,"There has been a growing interest in the graph-streaming setting where a continuous stream of graph updates is mixed with graph queries. In principle, purely-functional trees are an ideal fit for this setting as they enable safe parallelism, lightweight snapshots, and strict serializability for queries. However, directly using them for graph processing leads to significant space overhead and poor cache locality. This paper presents C-trees, a compressed purely-functional search tree data structure that significantly improves on the space usage and locality of purely-functional trees. We design theoretically-efficient and practical algorithms for performing batch updates to C-trees, and also show that we can store massive dynamic real-world graphs using only a few bytes per edge, thereby achieving space usage close to that of the best static graph processing frameworks. To study the efficiency and applicability of our data structure, we designed Aspen, a graph-streaming framework that extends the interface of Ligra with operations for updating graphs. We show that Aspen is faster than two state-of-the-art graph-streaming systems, Stinger and LLAMA, while requiring less memory, and is competitive in performance with the state-of-the-art static graph frameworks, Galois, GAP, and Ligra+. With Aspen, we are able to efficiently process the largest publicly-available graph with over two hundred billion edges in the graph-streaming setting using a single commodity multicore server with 1TB of memory."
pub.1094581571,An image transport protocol for the Internet,"Images account for a significant and growing fraction of Web downloads. The traditional approach to transporting images uses TCP, which provides a generic reliable, in-order byte-stream abstraction, but which is overly, restrictive for image data. We analyze the progression of image quality at the receiver with time and show that the in-order delivery abstraction provided by a TCP-based approach prevents the receiver application from processing and rendering portions of an image when they, actually, arrive. The end result is that an image is rendered in bursts interspersed with long idle times rather than smoothly. This paper describes the design, implementation, and evaluation of the Image Transport Protocol (ITP) for image transmission over loss-prone congested or wireless networks. ITP improves user-perceived latency using application level framing (ALF) and out-of-order pplication data unit (ADU) delivery achieving significantly better interactive performance as measured by the evolution of peak signal-to-noise ratio (PSNR) with time at the receiver ITP runs over UDP, incorporates receiver-driven selective reliability uses the congestion manager (CM) to adapt to network congestion, and is customizable for specific image formats (e.g., JPEG and JPEG2000). ITP enables a variety of new receiver post-processing algorithms such as error concealment that further improve the interactivity and responsiveness of reconstructed images. Performance experiments using our implementation across a variety of loss conditions demonstrate the benefits of ITP in improving the interactivity of image downloads at the receiver."
pub.1169769924,RCFS: rate and cost fair CPU scheduling strategy in edge nodes,"With the rapid advancement of 5G mobile networks and Internet of Things technology, an increasing number of data-intensive applications are generating massive amounts of information, such as face recognition, video stream analysis, and augmented reality. These applications not only demand significant computational resources but also require high real-time performance, posing challenges to the existing cloud-computing model. Deploying these data-intensive applications in a mobile edge computing environment can reduce response time for processing user tasks and meet low-latency requirements. However, conventional CPU scheduling strategies fail to effectively enhance the performance of data-intensive applications on edge nodes with limited computing resources. In this study, we focus on real-time application feature awareness and propose a strategy considering data arrival rate and cost fair CPU scheduling (RCFS) with two components: CPU resource allocation based on process weights and process scheduling based on distributed weighted round-robin. In compared to the default scheduling strategy in Linux, named Completely Fair Scheduler, experiments on edge nodes show that the RCFS strategy can effectively enhance CPU utilization, reduce running time of data-intensive applications, and improve the system’s data throughput. In the best-case scenario, the RCFS strategy achieved a remarkable increase in CPU utilization of data-intensive applications by up to 98.5%, reduced running time by up to 45.5%, and improved data throughput by up to 83.6%."
pub.1163036854,Internet of Medical Things (IoMT) Fog-Based Smart Health Monitoring,"Real-time monitoring is one of the Emerging duties in IoMT, and numerous systems have been developed to ensure the patient health monitor with efficient sensors capable of sensing, processing, and wireless communication may be used to collect data for environmental and smart health monitoring. These sensors are connected via wireless sensor networks. They transfer data to the cloud via IoT protocols and technology for storage and processing. This allows for the prediction of possible equipment breakdowns using past data. At times, the volume of data transmitted to the cloud or the time it takes to transfer data to the cloud and back to the sensors/actuators might be excessive. Moving some of the processing closer to the sensors can help minimize the amount of network and cloud resources consumed in these instances. In order to give cloud access, fog installations need to determine the architecture for joining sensors and gateways. Sensors often create data streams that may be pre-processed, aggregated, or filtered before they reach the cloud. Gateways can also be utilized to undertake data analytics. As a result, fog organization is critical for balancing computational load and network resource utilization on public clouds in order to reduce latency and save money. Service irregularity detection is a type of predictive maintenance that may be carried out even if no data from prior equipment failures is available. When available, machine-learning algorithms based on binary classification are used to predict breakdowns in the near future, allowing for repairs or replacements to be scheduled. The prediction models are trained and assessed using historical data, which includes information on prior equipment failures. Because historical data might be massive, real-time cloud storage is a possibility, leading in cloud-based predictive maintenance. This paper proposes a fog-based smart-health monitoring system with a single feed-forward neural network learning method to resolve this issue. An efficient cognitive-based machine learning method was compared to another efficient related work in this paper."
pub.1168335893,The Big Data Value Chain for the Provision of AI-Enabled Energy Analytics Services,"In order to support decision-making problems on the energy sector, like energy forecasting and demand prediction, analytics services are developed that assist users in extracting useful inferences on energy related data. Such analytics services use AI techniques to extract useful knowledge on collected data from energy infrastructure like smart meters and sensors. The big data value chain describes the steps of big data life cycle from collecting, pre-processing, storing and querying energy consumption data for high-level user-driven services. With the exponential growth of networking capabilities and the Internet of Things (IoT), data from the energy sector is arriving with a high throughput taking the problem of calculating big data analytics to a new level. This research will review existing approaches for big data energy analytics services and will further propose a framework for facilitating AI-enabled energy analytics taking into consideration all the requirements of analytics services through the entire big data value chain from data acquisition and batch and stream data ingestion, to creating proper querying mechanisms. Those query mechanisms will in turn enable the execution of queries on huge volumes of energy consumption data with low latency, and establishing high-level data visualizations. The proposed framework will also address privacy and security concerns regarding the big data value chain and allow easy applicability and adjustment on various use cases on energy analytics."
pub.1133224864,ZyON: Enabling Spike Sorting on APSoC-Based Signal Processors for High-Density Microelectrode Arrays,"Multi-Electrode Arrays and High-Density Multi-Electrode Arrays of sensors are a key instrument in neuroscience research. Such devices are evolving to provide ever-increasing temporal and spatial resolution, paving the way to unprecedented results when it comes to understanding the behaviour of neuronal networks and interacting with them. However, in some experimental cases, in-place low-latency processing of the sensor data acquired by the arrays is required. This poses the need for high-performance embedded computing platforms capable of processing in real-time the stream of samples produced by the acquisition front-end to extract higher-level information. Previous work has demonstrated that Field-Programmable Gate Array and All-Programmable System-On-Chip devices are suitable target technology for the implementation of real-time processors of High-Density Multi-Electrode Arrays data. However, approaches available in literature can process a limited number of channels or are designed to execute only the first steps of the neural signal processing chain. In this work, we propose an All-Programmable System-On-Chip based implementation capable of sorting neural spikes acquired by the sensors, to associate the shape of each spike to a specific firing neuron. Our system, implemented on a Xilinx Z7020 All-Programmable System-On-Chip is capable of executing on-line spike sorting up to 5500 acquisition channels, 43x more than state-of-the-art alternatives, supporting 18KHz acquisition frequency. We present an experimental study on a commonly used reference dataset, using on-line refinement of the sorting clusters to improve accuracy up to 82%, with only 4% degradation with respect to off-line analysis."
pub.1147931152,A Framework for Event-based Computer Vision on a Mobile Device,"We present the first publicly available Android framework to stream data from
an event camera directly to a mobile phone. Today's mobile devices handle a
wider range of workloads than ever before and they incorporate a growing gamut
of sensors that make devices smarter, more user friendly and secure.
Conventional cameras in particular play a central role in such tasks, but they
cannot record continuously, as the amount of redundant information recorded is
costly to process. Bio-inspired event cameras on the other hand only record
changes in a visual scene and have shown promising low-power applications that
specifically suit mobile tasks such as face detection, gesture recognition or
gaze tracking. Our prototype device is the first step towards embedding such an
event camera into a battery-powered handheld device. The mobile framework
allows us to stream events in real-time and opens up the possibilities for
always-on and on-demand sensing on mobile phones. To liaise the asynchronous
event camera output with synchronous von Neumann hardware, we look at how
buffering events and processing them in batches can benefit mobile
applications. We evaluate our framework in terms of latency and throughput and
show examples of computer vision tasks that involve both event-by-event and
pre-trained neural network methods for gesture recognition, aperture robust
optical flow and grey-level image reconstruction from events. The code is
available at https://github.com/neuromorphic-paris/frog"
pub.1119371743,Exploiting Fine-Grain Ordered Parallelism in Dense Matrix Algorithms,"Dense linear algebra kernels are critical for wireless applications, and the
oncoming proliferation of 5G only amplifies their importance. Many such matrix
algorithms are inductive, and exhibit ample amounts of fine-grain ordered
parallelism -- when multiple computations flow with fine-grain
producer/consumer dependences, and where the iteration domain is not easily
tileable. Synchronization overheads make multi-core parallelism ineffective and
the non-tileable iterations make the vector-VLIW approach less effective,
especially for the typically modest-sized matrices. Because CPUs and DSPs lose
order-of-magnitude performance/hardware utilization, costly and inflexible
ASICs are often employed in signal processing pipelines. A programmable
accelerator with similar performance/power/area would be highly desirable. We
find that fine-grain ordered parallelism can be exploited by supporting: 1.
fine-grain stream-based communication/synchronization; 2. inductive data-reuse
and memory access patterns; 3. implicit vector-masking for partial vectors; 4.
hardware specialization of dataflow criticality. In this work, we propose,
REVEL, as a next-generation DSP architecture. It supports the above features in
its ISA and microarchitecture, and further uses a novel vector-stream control
paradigm to reduce control overheads. Across a suite of linear algebra kernels,
REVEL outperforms equally provisioned DSPs by 4.6x-37x in latency and achieves
a performance per mm 2 of 8.3x. It is only 2.2x higher power to achieve the
same performance as ideal ASICs, at about 55% of the combined area."
pub.1104520084,Elastic CPU Cap Mechanism for Timely Dataflow Applications,"Sudden surges in the incoming workload can cause adverse consequences on the run-time performance of data-flow applications. Our work addresses the problem of limiting CPU associated with the elastic scaling of timely data-flow (TDF) applications running in a shared computing environment while each application can possess a different quality of service (QoS) requirement. The key argument here is that an unwise consolidation decision to dynamically scale up/out the computing resources for responding to unexpected workload changes can degrade the performance of some (if not all) collocated applications due to their fierce competition getting the shared resources (such as the last level cache). The proposed solution uses a queue-based model to predict the performance degradation of running data-flow applications together. The problem of CPU cap adjustment is addressed as an optimization problem, where the aim is to reduce the quality of service violation incidents among applications while raising the CPU utilization level of server nodes as well as preventing the formation of bottlenecks due to the fierce competition among collocated applications. The controller uses and efficient dynamic method to find a solution at each round of the controlling epoch. The performance evaluation is carried out by comparing the proposed controller against an enhanced QoS-aware version of round robin strategy which is deployed in many commercial packages. Experimental results confirmed that the proposed solution improves QoS satisfaction by near to 148% on average while it can reduce the latency of processing data records for applications in the highest QoS classes by near to 19% during workload surges."
pub.1061534323,Large-capacity high-throughput low-cost pipelined CAM using pipelined CTAM,"A novel approach toward realizing a large capacity high-throughput pipelined content addressable memory (CAM) or associative memory (AM) at low cost has been described. It employs only commercial random access memory (RAM) along with a simple binary search pipeline (BSPL). In order to search a (2/sup n/-1)-word search key field (SKF) storage RAM, the BSPL employs n identical and simple binary search processing elements, each having its local copy of the SKF storage RAM (SKFSR). The SKFSR stores, in an ordered manner, the SKFs of all the words in the CAM, whereas a data field storage RAM (DFSR) stores the unordered CAM data words. The n-times replicated SKFSR, along with the n-processor BSPL, functions as a simple n-stage pipelined content-to-address memory (CTAM). The CTAM, a new kind of memory that performs the inverse function of a RAM, serves as the first and the most important stage in the proposed 3-stage pipelined CAM architecture. In response to the stream of input query words (search keys) fed to the pipelined CAM, the pipelined CTAM first produces the corresponding stream of SKFSR addresses where the query words reside. These SKFSR addresses corresponding to the queries are next mapped back by the second stage, namely, the address mapping RAM, to their original addresses in the CAM, i.e., in the DFSR, which had been altered due to data ordering in the SKFSR. Now, the third stage of the pipelined CAM, namely, the DFSR, is read out at the mapped, i.e., the original CAM addresses, to obtain the desired stream of responses from the CAM. An augmented version of the pipelined CTAM has been designed to handle the presence of duplicate search keys in the CAM. A few illustrative examples of querying a simple database stored in the CAM have been included. The proposed pipelined CAM has a modular and highly scalable architecture. Its throughput rate, which is independent of the CAM size, is a little less than the RAM access rate and its latency is a little more than (n+2) times the RAM access time."
pub.1137929492,Event-LSTM: An Unsupervised and Asynchronous Learning-based Representation for Event-based Data,"Event cameras are activity-driven bio-inspired vision sensors, thereby
resulting in advantages such as sparsity,high temporal resolution, low latency,
and power consumption. Given the different sensing modality of event camera and
high quality of conventional vision paradigm, event processing is predominantly
solved by transforming the sparse and asynchronous events into 2D grid and
subsequently applying standard vision pipelines. Despite the promising results
displayed by supervised learning approaches in 2D grid generation, these
approaches treat the task in supervised manner. Labeled task specific ground
truth event data is challenging to acquire. To overcome this limitation, we
propose Event-LSTM, an unsupervised Auto-Encoder architecture made up of LSTM
layers as a promising alternative to learn 2D grid representation from event
sequence. Compared to competing supervised approaches, ours is a task-agnostic
approach ideally suited for the event domain, where task specific labeled data
is scarce. We also tailor the proposed solution to exploit asynchronous nature
of event stream, which gives it desirable charateristics such as speed
invariant and energy-efficient 2D grid generation. Besides, we also push
state-of-the-art event de-noising forward by introducing memory into the
de-noising process. Evaluations on activity recognition and gesture recognition
demonstrate that our approach yields improvement over state-of-the-art
approaches, while providing the flexibilty to learn from unlabelled data."
pub.1094577318,Design of an Advanced Wearable Sensor Dlatform for Multi Applications,"In the emerging Internet-of-Things (loT), wireless sensors are the vital mediators between the physical world and the cyber space. In many telemonitoring and interactive applications, high-rate data streams up to MB/s may need to be transported from the wireless sensors to the cloud computing servers over the Internet and processed by the remote servers in quasi-real time fashion. These applications generally demand substantial reduction of communication bandwidth, response latency and power consumption of these Internet-based cyber-physical systems. To tackle these demands for efficient use of communication, computing and power resources, we developed an advanced wearable sensor platform (A WSP) which integrated a powerful system-on-chip (SoC) processor, a smart power management unit, a highly accurate real-time clock and multi-function peripherals into a miniature module. This A WSP designed to function is capable of performing sophisticated data pre-processing including real-time artifact and noise removal, data compression, and even feature extraction before uploading the data to other devices like mobile phone, computer, etc. Furthermore, with the installation of embedded real-time Linux operating system, this sensor provides a familiar and powerful software development environment for system developers to build their computation-intensive real-time applications. This paper presents the design and development process."
pub.1094367220,Online map-matching based on Hidden Markov model for real-time traffic sensing applications,"In many Intelligent Transportation System (ITS) applications that crowd-source data from probe vehicles, a crucial step is to accurately map the GPS trajectories to the road network in real time. This process, known as map-matching, often needs to account for noise and sparseness of the data because (1) highly precise GPS traces are rarely available, and (2) dense trajectories are costly for live transmission and storage. We propose an online map-matching algorithm based on the Hidden Markov Model (HMM) that is robust to noise and sparseness. We focused on two improvements over existing HMM-based algorithms: (1) the use of an optimal localizing strategy, the variable sliding window (VSW) method, that guarantees the online solution quality under uncertain future inputs, and (2) the novel combination of spatial, temporal and topological information using machine learning. We evaluated the accuracy of our algorithm using field test data collected on bus routes covering urban and rural areas. Furthermore, we also investigated the relationships between accuracy and output delays in processing live input streams. In our tests on field test data, VSW outperformed the traditional localizing method in terms of both accuracy and output delay. Our results suggest that it is viable for low latency applications such as traffic sensing."
pub.1047514241,Dynamic Cooperative Base Station Selection Scheme for Downlink CoMP in LTE-Advanced Networks,"Coordinated Multi-Point (CoMP) transmission is a technique proposed to enhance the spectral efficiency and system throughput in an interference limited cellular networks. In CoMP joint processing (JP) scheme multiple base stations (BSs) are coordinately transmit data streams to each user. As more than two base stations are involved, abundant spatial resources are exploited and more backhaul spectrum for JP cooperation is required. The backhaul architecture for CoMP JP is crucial to provide low latency, unlimited capacity, less power consumption, and perfect synchronization among the BSs. However, satisfying all these constraints is impossible as the number of cooperative BSs increases for each user. In this paper, a dynamic cooperative base station selection scheme is proposed to reduce the backhaul load for CoMP user by selecting the appropriate number of coordinated BSs from the CoMP cluster to ensure the certain quality of service (QoS). In particular, for cell edge user the number of cooperative BSs per user has been selected in order to achieve reduced overhead and the allocation of backhaul capacity is performed under the max–min fairness criterion. Simulation results show that the proposed selection scheme achieves significant performance improvement than other transmission modes in terms of the average sum rate per backhaul use and minimal total power consumption."
pub.1147061657,"PIMulator-NN: An Event-Driven, Cross-Level Simulation Framework for Processing-In-Memory-Based Neural Network Accelerators","Processing-in-memory (PIM) architecture has been proposed to accelerate state-of-the-art neuro-inspired algorithms, such as deep neural networks. In this article, we present PIMulator-NN, an event-driven, cross-level simulation framework for PIM-based neural network accelerators. By employing an event-driven simulation mechanism, PIMulator-NN is able to model architecture details and capture design details of the architecture. Moreover, we integrate the main-stream circuit-level simulation framework with PIMulator-NN to accurately simulate the area, latency, and energy consumption of analog computation units. To demonstrate the usage of PIMulator-NN, we implement several PIM designs with PIMulator-NN and perform detailed simulation. The simulation results show that memory access and interconnects make considerable impacts on system-level performance and energy. Note that such results are hard to be captured by conventional performance model-based estimations. We found some anti common-sense results while modeling the architecture details with PIMulator-NN. With several architecture templates, PIMulator-NN provides the users with a platform to build up their PIM architecture quickly. PIMulator-NN is able to capture the impacts of different design choices (e.g., dataflow, interconnect, data parallelism, etc.), and this could enable users to explore their design space efficiently."
pub.1089949463,Electrophysiology of Attention,"Abstract This chapter describes how auditory and visual attention can be studied using event‐related potentials (ERPs). In audition, the mismatch negativity (MMN), a component of the ERP elicited by any discriminable change in an auditory stimulus stream, provides an excellent index of attention‐independent auditory processing: MMN data convincingly show that accurate central sound representations emerge even in the absence of attention. ERPs also index the two principal brain mechanisms that are involved in involuntary attention switch to unattended auditory stimulation, with the N1 component being related to attention switch to sound onset and the MMN as well as P3a components reflecting attention switch to sound change. Furthermore, the newly discovered reorienting negativity (RON) component seems to be related to the focusing of attention to task‐relevant aspects of stimulation following distraction. The processing negativity (PN), in turn, provides an on‐line index of auditory selective attention. In the visual modality, several ERP components are modulated by selective attention, the earliest effect being caused by visuo‐spatial attention which even affected the P1 generated in extrastriate cortex with a onset latency of about 70 ms post‐stimulus. The subsequent N1 component may also tap into attentional functions and, in addition, slow, PN‐type of selection negativities and positivities can be obtained, depending on the selection criteria (e.g., color or form) and the attentional demands."
pub.1126637260,SMART Paths for Latency Reduction in ReRAM Processing-In-Memory Architecture for CNN Inference,"This research work proposes a design of an analog ReRAM-based PIM
(processing-in-memory) architecture for fast and efficient CNN (convolutional
neural network) inference. For the overall architecture, we use the basic
hardware hierarchy such as node, tile, core, and subarray. On the top of that,
we design intra-layer pipelining, inter-layer pipelining, and batch pipelining
to exploit parallelism in the architecture and increase overall throughput for
the inference of an input image stream. We also optimize the performance of the
NoC (network-on-chip) routers by decreasing hop counts using SMART
(single-cycle multi-hop asynchronous repeated traversal) flow control. Finally,
we experiment with weight replications for different CNN layers in VGG (A-E)
for large-scale data set ImageNet. In our simulation, we achieve 40.4027 TOPS
(tera-operations per second) for the best-case performance, which corresponds
to over 1029 FPS (frames per second). We also achieve 3.5914 TOPS/W
(tera-operaions per second per watt) for the best-case energy efficiency. In
addition, the architecture with aggressive pipelining and weight replications
can achieve 14X speedup compared to the baseline architecture with basic
pipelining, and SMART flow control achieves 1.08X speedup in this architecture
compared to the baseline. Last but not least, we also evaluate the performance
of SMART flow control using synthetic traffic."
pub.1166541870,INR-Arch: A Dataflow Architecture and Compiler for Arbitrary-Order Gradient Computations in Implicit Neural Representation Processing,"An increasing number of researchers are finding use for nth-order gradient computations for a wide variety of applications, including graphics, meta-learning (MAML), scientific computing, and most recently, implicit neural representations (INRs). Recent work shows that the gradient of an INR can be used to edit the data it represents directly without needing to convert it back to a discrete representation. However, given a function represented as a computation graph, traditional architectures face challenges in efficiently computing its nth-order gradient due to the higher demand for computing power and higher complexity in data movement. This makes it a promising target for FPGA acceleration. In this work, we introduce INR-Arch, a framework that transforms the computation graph of an nth-order gradient into a hardware-optimized dataflow architecture. We address this problem in two phases. First, we design a dataflow architecture that uses FIFO streams and an optimized computation kernel library, ensuring high memory efficiency and parallel computation. Second, we propose a compiler that extracts and optimizes computation graphs, automatically configures hardware parameters such as latency and stream depths to optimize throughput, while ensuring deadlock-free operation, and outputs High-Level Synthesis (HLS) code for FPGA implementation. We utilize INR editing as our benchmark, presenting results that demonstrate 1.8-4.8x and 1.5-3.6x speedup compared to CPU and GPU baselines respectively. Furthermore, we obtain 3.1-8.9x and 1.7-4.3x lower memory usage, and 1.7-11.3x and 5.5-32.8x lower energy-delay product. Our framework will be made open-source and available on GitHub.****https://github.com/sharc-lab/inr-arch https://github.com/sharc-lab/inr-arch"
pub.1173596635,Integrating and validating crowdsourced data for improved weather predictions in MET Nordic,"The MET Nordic dataset, developed by the Norwegian Meteorological Institute (MET Norway), offers high-resolution (1 km) near-surface meteorological variables for Scandinavia, Finland, and the Baltic countries. Derived through statistical post-processing of numerical model outputs, MET Nordic serves two main purposes: historical weather reconstruction and real-time weather monitoring with short-term forecasts. This presentation focuses on the real-time stream (MET Nordic RT), which updates hourly with a 20-minute latency and maintains an operational archive dating back to 2012.The near-surface variables included in MET Nordic RT are: two-metre temperature, precipitation, air pressure at sea level, relative humidity, wind speed and direction, solar global radiation, long-wave downwelling radiation, cloud area fraction. The dataset combines data from the MetCoOp Ensemble Prediction System (MEPS) and diverse observational inputs, including -for temperature and precipitation- crowdsourced data from consumer-grade weather stations managed by citizens. The integration of such opportunistic data sources has enhanced the precision of reconstruction analysis and short-term forecasts, particularly temperature predictions in regions with sparse professional meteorological stations.This study describes the automatic quality control system employed to vet incoming data, ensuring reliability in statistical processing. The system uses a range of validation techniques—ranging from basic range checks to sophisticated spatial consistency tests via Bayesian inference—to mitigate the risks posed by the inherently variable quality of crowdsourced data. Our findings underscore the importance of treating such data as a network to capitalize on its dense, high-resolution coverage. The quality control software, Titanlib, is important to this process and it is freely available for use at https://github.com/metno/titanlib.The integration of data from Netatmo’s crowdsourced network into MET Nordic can be regarded as a success story. This now heavily influences MET Norway's work on upgrading our main quality control system through the internal project CONFIDENT."
pub.1119400201,ParPaRaw: Massively Parallel Parsing of Delimiter-Separated Raw Data,"Parsing is essential for a wide range of use cases, such as stream
processing, bulk loading, and in-situ querying of raw data. Yet, the
compute-intense step often constitutes a major bottleneck in the data ingestion
pipeline, since parsing of inputs that require more involved parsing rules is
challenging to parallelise. This work proposes a massively parallel algorithm
for parsing delimiter-separated data formats on GPUs. Other than the
state-of-the-art, the proposed approach does not require an initial sequential
pass over the input to determine a thread's parsing context. That is, how a
thread, beginning somewhere in the middle of the input, should interpret a
certain symbol (e.g., whether to interpret a comma as a delimiter or as part of
a larger string enclosed in double-quotes). Instead of tailoring the approach
to a single format, we are able to perform a massively parallel FSM simulation,
which is more flexible and powerful, supporting more expressive parsing rules
with general applicability. Achieving a parsing rate of as much as 14.2 GB/s,
our experimental evaluation on a GPU with 3584 cores shows that the presented
approach is able to scale to thousands of cores and beyond. With an end-to-end
streaming approach, we are able to exploit the full-duplex capabilities of the
PCIe bus and hide latency from data transfers. Considering the end-to-end
performance, the algorithm parses 4.8 GB in as little as 0.44 seconds,
including data transfers."
pub.1143693255,Research on DC Network Transmission Handover Technology Based on User Mode Sharing,"RDMA over Converged Ethernet (RoCE) and Data Plane Development Kit (DPDK) TCP are both high-performance transmission technologies for Data center (DC), and they are often mixed due to different characteristics. When the application scenario or network status changes, the application needs to be switched from one communication mode to another. The current method is to stop one mode of communication and then re-establish another mode of network connection, which is less efficient. This paper constructs a DC network transmission handover technology based on user mode sharing, which includes: user mode integrated driver, end-to-end transmission handover, and multi-mode synchronous caching technology. User mode integration driver transfers the lower-level authority of the Ethernet card and RoCE upwards. On this basis, technologies such as end-to-end transmission handover and multi-mode synchronous cache can be established. The end-to-end transmission handover improves the overall performance and reduces the performance loss of switching streams one by one. Multi-mode synchronous cache technology applies synchronous processing technology to the user-mode driven technology, which reduces the performance loss caused by cache retransmission during handover. The simulation experiment verifies that the new technology has the characteristics of low latency in various switching scenarios."
pub.1093315409,BALCON: BAckward Loss CONcealment Mechanism for Scalable Video Dissemination in Opportunistic Networks,"Opportunistic networks suffer some coarse characteristics as frequent disruptions, high loss ratios and uncertain data delivery. The transmission of video content even worsens the delivery problem because of the large size and continuous nature of the medium. Therefore, loss concealment is inducted as a part of the solution, alongside other delivery enhancement mechanisms. Loss concealment techniques are usually used Forwards, where the source must spend extra processing and transmission volume to enable the receiver to recover lost data parts. These techniques are not only associated with prolonged latencies and video encoding manipulations, but they are also sensitive to the loss pattern, which might leave the receiver in passive suspension if a key part for recovery was missing. In this work, we propose a Backward loss concealment mechanism, which allows the receiver to react to a certain amount of loss. Missing parts of the scalable video stream are compensated by a composite solution that encloses 1) video frame loss error concealment, and 2) network demands, which are initiated and cast in search for missing parts from other nodes in the network. Simulation-driven experiments prove the applicability of the backward mechanism, with a limited capability depending on the target application requirements."
pub.1061535379,Accelerating Pattern Matching Using a Novel Parallel Algorithm on GPUs,"Graphics processing units (GPUs) have attracted a lot of attention due to their cost-effective and enormous power for massive data parallel computing. In this paper, we propose a novel parallel algorithm for exact pattern matching on GPUs. A traditional exact pattern matching algorithm matches multiple patterns simultaneously by traversing a special state machine called an Aho-Corasick machine. Considering the particular parallel architecture of GPUs, in this paper, we first propose an efficient state machine on which we perform very efficient parallel algorithms. Also, several techniques are introduced to do optimization on GPUs, including reducing global memory transactions of input buffer, reducing latency of transition table lookup, eliminating output table accesses, avoiding bank-conflict of shared memory, coalescing writes to global memory, and enhancing data transmission via peripheral component interconnect express. We evaluate the performance of the proposed algorithm using attack patterns from Snort V2.8 and input streams from DEFCON. The experimental results show that the proposed algorithm performed on NVIDIA GPUs achieves up to 143.16-Gbps throughput, 14.74 times faster than the Aho-Corasick algorithm implemented on a 3.06-GHz quad-core CPU with the OpenMP. The library of the proposed algorithm is publically accessible through Google Code."
pub.1093322254,Edge Computing for Interactive Media and Video Streaming,"Video streaming and computer games are among the most popular and highest bandwidth consuming media in the Internet. Video contents consume around 70% of the total bandwidth usage in the Internet today. Advancements in media generation tools, high processing power, and high speed connectivity have enabled generation of live, interactive, multiview media generation. Cognitive assisted and online multi-player gaming have unlocked new horizons for gaming experience. However, such interactive gaming, and multi-view and 360-degree view videos, are currently limited by delay intolerance and excessive bandwidth usage. Edge computing is the name of a set of new technologies, such as cloudlets, micro data centers, fog, and mobile edge computing. It aims to provide storage and computational resources near to user at the network edge, to minimize latency and response time. Edge computing is foreseen as a significant enabler of Internet of Every Thing (IoET) era by extending the cloud services and resources at the end of the network to deliver very low latency and real-time communication. It can provide significant services to video and gaming applications and enable new stream of interactive multimedia era. In this paper, we highlight some of the potentials and prospects of edge computing for interactive media, and present some preliminary works in the area. We shed light on how edge computing can be used to tackle various challenges faced by todays interactive media application. We also present the benefits of using edge computing to save cost, bandwidth, and energy in multimedia applications, video streaming, and transcoding."
pub.1144916996,Promenade: A Big Data Platform for Handling City Complex Networks with Dynamic Graphs,"Continuous data streams, generated by modern sensed cities, open many opportunities and perspectives in terms of developing new innovative services. To exploit this potential, flexible and scalable platforms are needed to ease the design, development, deployment, and operations of new city services. In recent years, several problem-specific platforms have been proposed in different application domains; however, to boost the evolution of smart cities, we claim the need for city-oriented platforms that can be easily customized to address different day-to-day life challenging problems. In this paper, we present the main architectural challenges and solutions proposed for the design of a novel open-source platform (named PROMENADE) characterized by: (i) a graph-based data-driven modeling support to ensure high generality for addressing disparate problems related to the networked nature of many city infrastructures and systems, (ii) the big dynamic nature of its data/graph entities which come in a real-time fashion from different sources (e.g., IoT/Edge networks, data providers, etc.), and (iii) its high efficiency, flexibility, and scalability to easily support new city services. The platform is designed around a core that provides a set of built-in functions for graph metrics computation exposed as a collection of containerized microservices. A specialization of the platform, called PROMENADE-v2.0, has been developed for road networks monitoring. It has been deployed in Openshift/Kubernetes and tested using realistic datasets collected from the city of Lyon, France. The analysis addresses an important problem of big data processing pipelines: the synchronization between data ingestion and processing in order to produce an accurate result in useful time. To this end, we study different approaches for synchronization and show how end-to-end latency is kept under control by leveraging the scalability of the platform."
pub.1152974797,High-Throughput GPU Implementation of Dilithium Post-Quantum Digital Signature,"Digital signatures are fundamental building blocks in various protocols to
provide integrity and authenticity. The development of the quantum computing
has raised concerns about the security guarantees afforded by classical
signature schemes. CRYSTALS-Dilithium is an efficient post-quantum digital
signature scheme based on lattice cryptography and has been selected as the
primary algorithm for standardization by the National Institute of Standards
and Technology. In this work, we present a high-throughput GPU implementation
of Dilithium. For individual operations, we employ a range of computational and
memory optimizations to overcome sequential constraints, reduce memory usage
and IO latency, address bank conflicts, and mitigate pipeline stalls. This
results in high and balanced compute throughput and memory throughput for each
operation. In terms of concurrent task processing, we leverage task-level
batching to fully utilize parallelism and implement a memory pool mechanism for
rapid memory access. Considering the impact of varying repetition numbers in
Dilithium on the overall execution time and hardware utilization, we propose a
dynamic task scheduling mechanism to improve multiprocessor occupancy and
significantly reduce execution time. Furthermore, we apply asynchronous
computing and launch multiple streams to hide data transfer latencies and
maximize the computing capabilities of both CPU and GPU. Across all three
security levels, our GPU implementation can concurrently compute ten thousand
tasks in less than 32 miliseconds for signing and 15 miliseconds for
verification on both commercial and server-grade GPUs. This achieves
microsecond-level amortized execution times for each task, offering a
high-throughput and quantum-resistant solution suitable for a wide array of
applications in real systems."
pub.1174896218,Low-Complexity Constrained Coding Schemes for Two-Dimensional Magnetic Recording,"The two-dimensional magnetic recording (TDMR) technology promises a remarkable increase in data storage density using the already-existing magnetic materials. Advanced TD signal processing techniques are required for this technology to fulfill its potential. Constrained coding, which prevents TD error-prone data patterns from being written, is among those techniques. In this paper, we propose lexicographically-ordered constrained (LOCO) coding schemes that offer low complexity, low latency, and low error propagation for TDMR systems with wide read heads, where coding can be applied on groups of 3 down tracks each. In particular, we introduce simple plus LOCO (SP-LOCO) and simple T LOCO (ST-LOCO) coding schemes, where codes defined over GF(2) and GF(4), respectively, are used instead of codes defined over GF (8), allowing the separation of uncoded streams. To better understand the behavior of the constrained-coded system, we derive the probability of transition, both in the horizontal and vertical directions, for the case of SP-LOCO and ST-LOCO coding, and we compare them to the case of optimal, rate-wise, LOCO codes. Moreover, we introduce simulation results on a practical TDMR model that demonstrate the performance gains achieved via the proposed schemes."
pub.1086119703,Modeling Aggregate Input Load of Interoperable Smart City Services,"The Internet of Things (IoT) is expanding and reaching the maturity level beyond initial deployments. An integrative and interoperable IoT platform proves to be a suitable execution environment for Smart City services because users simultaneously use multiple services, while an IoT platform enables cross-service data sharing. A large number of various IoT and mobile devices as well as the corresponding services can generate tremendous input load on an underlying IoT platform. Thus, it is crucial to analyze the overall input rate on Smart City services to ensure predefined quality of service (e.g., low latency required by some IoT services). An aggregate input rate which characterizes a real world deployment can be used to check if a platform is able to adequately support multiple services running in parallel and to evaluate its overall performance. In this paper we review IoT-based Smart City services to identify key applications characterizing the domain, e.g., smart mobility, smart utilities, and citizen-driven mobile crowd sensing services. Next, we analyze the potential load which such applications pose on IoT services that continuously process the generated data streams. The analysis is used to create a model estimating an aggregate load generated by Smart City applications. We simulate a number of characteristic application compositions to provide insight about the aggregate input load and its potential impact on the performance of Smart City services. The proposed model is a first step towards predicting the processing load of Smart City services to facilitate the assessment and planning of required resources for continuous processing of sensor data in the context of Smart City services."
pub.1061332864,Parallel Acceleration of SAM Algorithm and Performance Analysis,"Advances in sensor and computer technology are revolutionizing the way that remote sensing data with hundreds or even thousands of channels for the same area on the surface of the earth is collected, managed and analyzed. In this paper, the classical Spectral Angle Mapper (SAM) algorithm, which is fit for parallel and distributed computing, is implemented by using Graphic Processing Units (GPU) and distributed cluster respectively to accelerate the computations. A quantitative performance comparison between Compute Unified Device Architecture (CUDA) and MATLAB platform is given by analyzing result of different parallel architectures' implementation of the same SAM algorithm. Especially for the property of GPU, this paper studied the balance between resource acquirement of each thread and the number of active blocks, and the impact of computational complexity on speedup. In addition, page-locked memory and stream are also introduced to make CPU and GPU work collaboratively. Moreover, we improved the SAM algorithm, in which several training samples are instead of a single one. Experimental results on hyperspectral data have shown that recognition result of the improved SAM algorithm is better than that only using single spectrum. On the other hand, the GPU parallel implementation achieves a higher speedup comparing with the multithread CPU counterpart. and the asynchronous transfer function of CUDA covers the data transmission latency effectively, thus improves the devices' resource occupancy significantly."
pub.1155758676,Delay optimization and energy balancing algorithm for improving network lifetime in fixed wireless sensor networks,"Since wireless in terms of energy-restricted processes, dispersion radii, processing power limitations, buffers, bandwidth-limited connections, active network topologies, and network stream of traffic outlines, sensor networks provide difficult design issues. The number of hops and latency are decreased if there is a relay mote because it interacts directly with relay motes that are closer to the destination mote. The tremendous intensive research in the area of Wireless Sensor Networks (WSN) has gained a lot of significance among the technical community and research. The job of WSN is to sense the data using sensor motes, pass on the data to the destination detection mote which is associated with a processing center and can be used in multiple spans of Internet of Things (IoT) applications. Wireless sensor network has a set of sensor motes. By making use of sensor mote placement strategy all the sensor motes are spread in an area with each mote having its own exceptional location. Internet of things applications are delay sensitive those applications have a challenge of forming the complete path at a lower delay constraint. The proposal is to modify the game theory energy balancing algorithm by making use of relay motes so that overall network lifetime is increased. It has been proved that modified GTEB is better with respect to existing algorithms in terms of delay, figure of hops, energy depletion, figure of alive motes, figure of dead motes, lifespan ratio, routing overhead and throughput."
pub.1125163867,Cluster-size optimization within a cloud-based ETL framework for Big Data,"The ability to analyze the available data is a valuable asset for any successful business, especially when the analysis yields meaningful knowledge. One of the key processes for acquiring such ability is the Extract-Transform-Load (ETL) process. For Big Data, ETL requires a significant effort and it is a very challenging task to be performed in a cost-effective way. There are quite a few examples in the literature that describe an architecture for cost-effective ETL but none of the available examples are complete enough and they are usually evaluated in narrow problem domains. The ones that are more general, require specific implementation details. In this paper we propose a cloud-based ETL framework where we use a general cluster-size optimization algorithm, while providing implementation details, and is able to perform the required job within a predefined, and thus known, time. We evaluated the algorithm by executing three scenarios regarding data aggregation during ETL: (i) ETL with no aggregation; (ii) aggregation based on predefined columns or time intervals; and (iii) aggregation within single user sessions spanning over arbitrary time intervals. The execution of the three ETL scenarios in a production setting showed that the cluster size could be optimized so it can process the required data volume within a predefined and thus, expected, latency. The scalability was evaluated on Amazon AWS Hadoop clusters by processing user logs collected with Kinesis streams with datasets ranging from 30 GB to 2.6 TB."
pub.1138615371,Cost-Efficient Request Dispatching in Geo-distributed Cloud Gaming Infrastructure,"Cloud gaming is a recent approach to gaming, where processing for the games occurs on well-equipped servers within the cloud and a video stream is returned to the user, meaning users can play high-end games on devices that lack computational power. Different games require different amounts of computational resources and computation times. It would be desirable to efficiently pack a number of servers with multiple games at once, however this is complicated within a geo-distributed cloud system as we must consider that not every data center can fulfil every game request due to latency requirements. Within this work, we present shadow routing algorithms to distribute game requests to cloud data centers and also to pack the servers within the data centers with these game requests. These algorithms are designed to operate in order to minimize total cost from server hire and bandwidth usage, and we prove their performance is asymptotically close to optimal. An experiment using realistic arrival rates is given, and the results verify our theory within a realistic context. Also shown using proof and experimentation is that the algorithms can adapt themselves to periodic changes as demand raises and falls while remaining close to the optimal, which is a particular weak point of other schemes."
pub.1169921296,Enhancing Networked Embedded Systems Using Deep Learning Techniques,"Improving performance and flexibility is crucial in the field of networked embedded systems. Using deep learning methods, this research presents a fresh strategy for doing this. Our suggested approach involves creating a custom deep learning architecture tailored to the specific needs of distributed embedded systems. This strategy makes use of Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Long Short-Term Memory Networks (LSTMs) to evaluate real-time data streams, expedite decision-making, and intelligently adapt to ever-changing environments. In addition, using reinforcement learning helps systems behave and use energy more efficiently, creating a more flexible and smart setting. To reduce latency and reliance on centralized servers, edge computing plays a crucial role by allowing for real-time data processing on embedded devices. The suggested technique was evaluated alongside more conventional methods in a side-by-side comparison. Imaginary numbers were utilized for demonstration purposes. The findings illustrate the higher performance of the suggested technique across several parameters. When compared to baseline deep learning methods like Convolutional Neural Networks, Recurrent Neural Networks, Long Short-Term Memory Networks, Generative Adversarial Networks, Federated Learning, Transfer Learning, and Time Series Analysis with Deep Learning, the proposed method shows marked improvements in accuracy, latency, energy efficiency, robustness, security, scalability, and resource utilization. Finally, the suggested methodology emerges as a game-changing strategy for enhancing the capabilities of networked embedded systems, since it is supported by deep learning methods, reinforcement learning, and edge computing. Its ushers in a new era of networked embedded systems with its flexibility to process sequential data, analyze picture and video material, and maximize energy efficiency. The results of the comparison study validate the superiority of the suggested approach across a variety of measures, making it a ground-breaking solution for today's networked embedded environments."
pub.1160509689,E-Track: Eye Tracking with Event Camera for Extended Reality (XR) Applications,"Eye tracking is an essential functionality to enable extended reality (XR) applications. However, the latency and power constraints of an XR headset are tight. Unlike fix-rate frame-based RGB cameras, the event camera senses brightness changes and generates asynchronous sparse events with high temporal resolution. Although the event camera exhibits suitable characteristics for eye tracking in XR systems, processing an event-based data stream is a challenging task. In this paper, we present an event-based eye-tracking system that extracts pupil features. It is the first system that operates only with an event camera and requires no additional sensing hardware. We first propose an event-to-frame conversion method that encodes the events triggered by eye motion into a 3-channel frame. Secondly, we train a Convolutional Neural Network (CNN) on 24 subjects to classify the events representing the pupil. Finally, we employ a region of interest (RoI) mechanism that tracks pupil location and reduces the amount of CNN inference by 96%. Our eye-tracking pipeline is able to locate the pupil with an error of 3.68 pixels at 160 mW system power."
pub.1119295549,A Microbenchmark Characterization of the Emu Chick,"The Emu Chick is a prototype system designed around the concept of migratory
memory-side processing. Rather than transferring large amounts of data across
power-hungry, high-latency interconnects, the Emu Chick moves lightweight
thread contexts to near-memory cores before the beginning of each memory read.
The current prototype hardware uses FPGAs to implement cache-less ""Gossamer
cores for doing computational work and a stationary core to run basic operating
system functions and migrate threads between nodes. In this multi-node
characterization of the Emu Chick, we extend an earlier single-node
investigation (Hein, et al. AsHES 2018) of the the memory bandwidth
characteristics of the system through benchmarks like STREAM, pointer chasing,
and sparse matrix-vector multiplication. We compare the Emu Chick hardware to
architectural simulation and an Intel Xeon-based platform. Our results
demonstrate that for many basic operations the Emu Chick can use available
memory bandwidth more efficiently than a more traditional, cache-based
architecture although bandwidth usage suffers for computationally intensive
workloads like SpMV. Moreover, the Emu Chick provides stable, predictable
performance with up to 65% of the peak bandwidth utilization on a random-access
pointer chasing benchmark with weak locality."
pub.1115218747,A microbenchmark characterization of the Emu chick,"The Emu Chick is a prototype system designed around the concept of migratory memory-side processing. Rather than transferring large amounts of data across power-hungry, high-latency interconnects, the Emu Chick moves lightweight thread contexts to near-memory cores before the beginning of each memory read. The current prototype hardware uses FPGAs to implement cache-less “Gossamer” cores for computational work and rely on a typical stationary core (PowerPC) to run basic operating system functions and migrate threads between nodes. In this multi-node characterization of the Emu Chick, we extend an earlier single-node investigation [1] of the memory bandwidth characteristics of the system through benchmarks like STREAM, pointer chasing, and sparse matrix-vector multiplication. We compare the Emu Chick hardware to architectural simulation and an Intel Xeon-based platform. Our results demonstrate that for many basic operations the Emu Chick can use available memory bandwidth more efficiently than a more traditional, cache-based architecture although bandwidth usage suffers for computationally intensive workloads like SpMV. Moreover, the Emu Chick provides stable, predictable performance with up to 65% of the peak bandwidth utilization on a random-access pointer chasing benchmark with weak locality."
pub.1147586042,Optimize AI pipelines with SYCL and OpenVINO,"Sensor data processing pipelines that are a ”mix” of feature-engineered and deep learning based processing have become prevalent today. For example, sensor fusion of point cloud data with RGB image streams is common in autonomous mobile robots and self-driving technology. The state-of-the-art in computer vision for extracting semantic information from RGB data is using deep learning today, and great advancements have been made recently in LiDAR odometry based on deep learning [x]. At the same time, other processing components in ”mixed” pipelines still use feature-engineered approaches that are not relying on deep neural nets. Embedded compute platforms in robotics systems are inherently heterogeneous in nature, often with a variety of CPUs, (integrated) GPUs, VPUs, and so on. This means that there is a growing need to implement ”mixed” pipelines on heterogeneous platforms that include a variety of xPUs. We want such pipeline implementations to benefit from the latest advancements in data- and thread-parallel computation, as well as state-of-the-art in optimized inference of AI DNN models. SYCL and OpenVINO are two open, industry supported APIs that allow a developer to do so. It is not only important to optimize the individual components of the processing pipeline - it is at least as important to also optimize the data flow and minimize data copies. This provides a way to benefit from the efficiencies in inference runtime and compute graph optimizations provided by OpenVINO, in combination with the extensibility that SYCL brings in implementing custom or non-DNN components. Similarly, the use of compatible synchronization primitives allows the different runtimes to schedule work more efficiently on the hardware and avoid execution hiccups. In this talk, we will demonstrate the mechanisms and primitives provided by both SYCL and OpenVINO to optimize the dataflow between, and efficient execution of the workloads implemented in the respective APIs. We will provide an example and show the impact on the overall throughput and latency of the end-to-end processing pipeline. The audience will learn to recognize inefficiencies in their pipelines using profiling tools, and understand how to optimize those inefficiencies using an easy-to-follow optimization recipe. Finally, we will provide guidance to developers of inference engines other than OpenVINO on how to integrate similar interoperability features into their APIs, so that they too can offer optimized SYCL-enabled AI pipelines to their users."
pub.1067368287,Piggybacking on social networks," The popularity of social-networking sites has increased rapidly over the last decade. A basic functionalities of social-networking sites is to present users with streams of events shared by their friends. At a systems level, materialized per-user views are a common way to assemble and deliver such event streams on-line and with low latency. Access to the data stores, which keep the user views, is a major bottleneck of social-networking systems. We propose to improve the throughput of these systems by using social piggybacking, which consists of processing the requests of two friends by querying and updating the view of a third common friend. By using one such hub view, the system can serve requests of the first friend without querying or updating the view of the second. We show that, given a social graph, social piggybacking can minimize the overall number of requests, but computing the optimal set of hubs is an NP-hard problem. We propose an O (log n ) approximation algorithm and a heuristic to solve the problem, and evaluate them using the full Twitter and Flickr social graphs, which have up to billions of edges. Compared to existing approaches, using social piggybacking results in similar throughput in systems with few servers, but enables substantial throughput improvements as the size of the system grows, reaching up to a 2-factor increase. We also evaluate our algorithms on a real social networking system prototype and we show that the actual increase in throughput corresponds nicely to the gain anticipated by our cost function. "
pub.1096144233,Learning-to-Rank for Real-Time High-Precision Hashtag Recommendation for Streaming News,"We address the problem of real-time recommendation of streaming Twitter hashtags to an incoming stream of news articles. The technical challenge can be framed as large scale topic classification where the set of topics (i.e., hashtags) is huge and highly dynamic. Our main applications come from digital journalism, e.g., promoting original content to Twitter communities and social indexing of news to enable better retrieval and story tracking. In contrast to the state-of-the-art that focuses on topic modelling approaches, we propose a learning-to-rank approach for modelling hashtag relevance. This enables us to deal with the dynamic nature of the problem, since a relevance model is stable over time, while a topic model needs to be continuously retrained. We present the data collection and processing pipeline, as well as our methodology for achieving low latency, high precision recommendations. Our empirical results show that our method outperforms the state-of-the-art, delivering more than 80% precision. Our techniques are implemented in a real-time system that is currently under user trial with a big news organisation."
pub.1093683894,CTC: an End-To-End Flow Control Protocol for Multi-Core Systems-on-Chip,"We propose Connection then Credits (CTC) as a new end-to-end flow control protocol to handle message-dependent deadlocks in networks-on-chip (NoC) for multicore systems-on-chip. CTC is based on the classic end-to-end credit-based flow control protocol but differs from it because it uses a network interface micro-architecture where a single credit counter and a single input data queue are shared among all possible communications. This architectural simplification reduces the area occupation of the network interfaces and increases their design reuse: for instance, the same network interface can be used to connect a core independently of the number of incoming and outgoing communications. CTC, however, requires a handshake preamble to initialize the credit counter in the sender network interface based on the buffering capacity of the receiver network interface. While this necessarily introduces a latency overhead in the transfer of a message, simulation-based experimental results show that the penalty in performance is limited when large messages need to be transferred, thus making CTC a valid solution for particular classes of applications such as video stream processing."
pub.1132736848,SPATIO: end-uSer Protection Against ioT IntrusiOns,"The Internet of Things (IoT) is an emerging technology field where large numbers of physical objects communicate between themselves using Internet technology. IoT solutions are very diverse, ranging from simple toys to industrial applications. There are currently billions of IoT devices connected to the Internet, and this number has been growing exponentially in the recent years. The large amount of data being generated from many devices in an IoT network makes it difficult to collect and analyze all the data. However, with this growth there also comes a growing security concern. With the use of IoT devices in the industrial and healthcare sectors, for example, a security incident can have far reaching consequences in the real world. It is imperative to detect attacks as fast as possible, in time to prevent significant damage. The continuous flow of data may be handled with a stream processing approach, a data processing paradigm in which high-rate data sources are processed and generate results on the fly. Based on this approach, we propose SPATIO (end-uSer Protection Against ioT IntrusiOns), an anomaly detection system designed for the IoT using machine learning to discover and alert on anomalies happening in an IoT network but takes a fog computing approach by using devices on the IoT network, such as routers, to collect and transform network traffic into flow metrics. Doing this transformation closer to the edge reduces the bandwidth cost on the network and allows anonymization of data before being sent outside the network, to the cloud or a server running outlier detection algorithm to generate timely alerts of network anomalies. We evaluate SPATIO by developing a prototype testing it on an existing public dataset of IoT attacks. We measured the accuracy of the machine learning approach, reaching close to 80% detection rate in the best scenario, and compared the performance of offloading work to gateway devices in the IoT network versus a centralized approach, in which the fog approach shows advantages in both network load and attack detection latency. In this chapter, the authors propose SPATIO, an anomaly detection system designed for the Internet of Things (IoT) using machine learning to discover and alert on anomalies happening in an IoT network but takes a fog computing approach by using devices on the IoT network, such as routers, to collect and transform network traffic into flow metrics. They evaluate SPATIO by developing a prototype testing it on an existing public dataset of IoT attacks. The authors measure the accuracy of the machine learning approach, reaching close to 80% detection rate in the best scenario, and compared the performance of offloading work to gateway devices in the IoT network versus a centralized approach, in which the fog approach shows advantages in both network load and attack detection latency. They summarize the data flow between the components of the SPATIO architecture. Network traffic is collected from I"
pub.1061538192,The Connection-Then-Credit Flow Control Protocol for Heterogeneous Multicore Systems-on-Chip,"Connection-then-credits (CTC) is a novel end-to-end flow control protocol to handle message-dependent deadlocks in best-effort networks-on-chip (NoC) for embedded multicore systems-on-chip (SoCs). CTC is based on the classic end-to-end credit-based flow control protocol but differs from it because it uses a network interface microarchitecture where a single credit counter and a single input data queue are shared among all possible communications. This architectural simplification reduces the area occupation of the network interfaces and increases their design reuse; for instance, the same network interface can be used to connect a core independently of the number of incoming and outgoing communications. CTC, however, requires a handshake preamble to initialize the credit counter in the sender network interface based on the buffering capacity of the receiver network interface. While this necessarily introduces a latency overhead in the transfer of a message, simulation-based experimental results show that the penalty in performance is limited when large messages need to be transferred, thus, making CTC a valid solution for particular classes of applications such as video stream processing."
pub.1137069840,Image fusion systems for surveillance applications: design options and constraints for a tri-band camera,"Image fusion provides an attractive technical solution for surveillance applications, offering improved task performance for imaging systems. Through the fusion process, information from different spectral bands is combined to form a single video stream which gives an improved scene understanding together with an enhanced target detection and recognition capability. In this paper, ground-based surveillance requirements are used to define a tri-band image fusion camera comprising LWIR, NIR, and Visible spectral bands. The design approach used addresses both the performance and commercialisation challenges encountered in the development of an image fusion camera. These challenges are discussed in the context of earlier image fusion systems where useful lessons were learnt. Establishing an effective processing architecture is critical to image interpretation, and the functional design is presented. The tri-band camera design allows the user to view different image streams including enhanced single-band image data as well as both dual and tri-band fused imagery. Such flexibility enables the selection of the best imagery for specific scenarios and viewing conditions. The physical characteristics are a major constraint for handheld camera designs where the size, weight, and power limitations dictate both the choice of sensors as well as the processor card. For the design presented here, power consumption and latency figures are minimised using relatively simple arithmetical fusion algorithms which are combined with an adaptive colour weight map for regional optimisation. Example results are presented to illustrate the various technical challenges and trade-offs undertaken within this development programme."
pub.1092100438,Incentivising Resource Sharing in Edge Computing Applications,"There is increasing realisation that edge devices, which are closer to a user, can play an important part in supporting latency and privacy sensitive applications. Such devices have also continued to increase in capability over recent years, ranging in complexity from embedded resources (e.g. Raspberry Pi, Arduino boards) placed alongside data capture devices to more complex “micro data centres”. Using such resources, a user is able to carry out task execution and data storage in proximity to their location, often making use of computing resources that can have varying ownership and access rights. Increasing performance requirements for stream processing applications (for instance), which incur delays between the client and the cloud have led to newer models of computation, which requires an application workflow to be split across data centre and edge resource capabilities. With recent emergence of edge/fog computing it has become possible to migrate services to micro-data centres and to address the performance limitations of traditional (centralised data centre) cloud based applications. Such migration can be represented as a cost function that involves incentives for micro-data centres to host services with associated quality of services and experience. Business models need to be developed for creating an open edge cloud environment where micro-data centres have the right incentives to support service hosting, and for large scale data centre operators to outsource service execution to such micro data centres. We describe potential revenue models for micro-data centers to support service migration and serve incoming requests for edge based applications. We present several cost models which involve combined use of edge devices and centralised data centres."
pub.1011361400,Parallel LDPC Decoding on the Cell/B.E. Processor,"Low-Density Parity-Check (LDPC) codes are among the best error correcting codes known and have been recently adopted by data transmission standards, such as the second generation for Satellite Digital Video Broadcasting (DVB-S2) and WiMAX. LDPC codes are based on sparse parity-check matrices and use message-passing algorithms, also known as belief propagation, which demands very intensive computation. For that reason, VLSI dedicated architectures have been proposed in the past few years, to achieve real-time processing. This paper proposes a new flexible and programmable approach for LDPC decoding on a heterogeneous multicore Cell Broadband Engine (Cell/B.E.) architecture. Very compact data structures were developed to represent the bipartite graph for both regular and irregular LDPC codes. They are used to map the irregular behavior of the Sum-Product Algorithm (SPA) used in LDPC decoding into a computing model that expresses parallelism and locality of data by decoupling computation and memory accesses. This model can be used in general for exploiting capabilities of modern multicore architectures. For the Cell/B.E., in particular, stream-based programs were developed for simultaneous multicodeword LDPC decoding by using SIMD features and a low-latency DMA-based data communication mechanism between processors. Experimental results show significant throughputs that compare well with state-of-the-art VLSI-based solutions."
pub.1120981459,Forecasting a Storm: Divining Optimal Configurations using Genetic Algorithms and Supervised Learning,"With the advent of Big Data platforms like Apache Storm, computations once deemed infeasible locally become possible at scale. However, doing so entails orchestrating powerful yet expensive clusters. With its focus on stream processing, Storm optimizes for low-latency and high throughput. However, to realize this goal and thereby maximize the utility of these clusters’ resources, operators must execute these tasks under their optimal configurations. Yet, the search space for finding such configurations is so vast and time-consuming to explore so as to be effectively intractable due to issues like the temporal overhead of testing new candidate configurations, the sheer number of permutations of parameters within each configuration and their interdependence among each other. In order to efficiently cover the search space, we automate the process with genetic algorithms. Moreover, we fuse this technique not only with additional cluster information gleaned from JMX profiling and Storm performance data but also with classifiers constructed from training data from past executions of a plethora of Storm topologies. Utilizing a diverse set of Storm benchmark topologies as evaluation data, we show that the fully enhanced genetic algorithms can efficiently find configurations that perform on average 4.67x better than “rules of thumb”-derived manual baselines. Moreover, we demonstrate that our fully refined classifiers enhance the GA throughput on average across the topologies by 22% while reducing search time by a factor of 6.47x."
pub.1107611619,One Platform Rules All,"In the big data era, traditional relational database systems cannot effectively handle the big volume of data due to their limited scalability. People are seeking new ways to tackle the problem of big data. After Google published its work of MapReduce, Hadoop (an open-source implementation of 192MapReduce) has risen to be the de facto standard tool for big data processing. People have applied Hadoop to various big data application scenarios, which show the power of Hadoop. However, the 1.0 version of Hadoop supports only one computing model of MapReduce, which is not efficient enough to provide higher performance. Now Hadoop has evolved into Hadoop 2.0 (YARN). Hadoop 2.0 has a newly designed architecture, which separates resource management and job scheduling. Hadoop 2.0 supports other computing models besides MapReduce, including complex computing work expressed in a DAG (directed acyclic graph). People also try to improve the execution layer of Hadoop, such as the work of Tez from Hortonworks, to provide lower latency. In the meantime, AMP lab of California University at Berkeley brought out Spark, which now draws more and more attention from academia and industry. The Spark ecosystem includes the core and four major components surrounding it, including Spark SQL for structured data processing, Spark Streaming for stream data processing, MLLib for machine learning, and GraphX for graph data processing. In essence, Spark and Hadoop provide similar functionalities; however, in some application scenarios, Spark outperforms Hadoop by many times. Hadoop and Spark are two ecosystems. Both of them can play the central role in future big data warehouses. On one hand, they are replacement to each other; on the other hand, they can be used together to get work done. For example, people can use Spark in exploratory analysis and get instant feedback, and use Hadoop to consolidate all data in one place, and conduct a thorough analysis on the whole data set. In this chapter, we analyze limitations of different technologies and the business requirements behind the continuous innovations. We also try to point out some lessons that the database research community and the database industry should have learned. This chapter analyzes limitations of different technologies and the business requirements behind the continuous innovations. It points out some lessons that the database research community and database industry should have learned. In the big data era, traditional relational database systems cannot effectively handle the big volume of data due to their limited scalability. People are seeking new ways to tackle the problem of big data. After Google published its work of MapReduce, Hadoop has risen to be the de facto standard tool for big data processing. In early 1970s, IBM scientist E. F. Codd, who later won Turing Award, published the famous paper of ""A Relational Model of Data for Large Shared Data Banks"". The paper laid down the theoretic foundation of "
pub.1044976477,iConn," Recently, the graphics processing unit (GPU) has made significant progress as a general-purpose parallel processor. The CPU and GPU cooperate together to solve data-parallel and control-intensive real-world applications in an optimized fashion. For example, emerging heterogeneous computing architectures such as Intel Sandy Bridge and AMD Fusion integrate the functionality of the CPU and GPU in a single die. However, the single-die CPU-GPU heterogeneous computing architecture faces the challenge of tight budget of die area. The conventional homogenous interconnect fails to provide satisfactory performance by fully exploiting the given area budget in the heterogeneous processing era.   In this article, we aim to implement an interconnect network within an area budget for a CPU-GPU heterogeneous computing architecture. We propose iConn, a 2D mesh-style on-chip heterogeneous communication infrastructure. In iConn, a set of GPU logical units such as the stream processors, the texture units, and the rendering output units form a computing unit (CU). Differing from conventional homogenous router design, iConn adopts nonuniform on-chip routers in order to meet the unique communication demands from each single CPU and CU. The routers can also dynamically allocate their buffers across all virtual channels (VCs) to meet the latency requirements of CPUs and CUs. Moreover, the memory controller scheduling algorithm is modified from traditional load-over-store scheduling in order to prioritize the traffic. Our simulation results show that iConn improves the performance of CPUs by 23.0% and CUs by 9.4%. "
pub.1025188594,Performance of the High Sensitivity Open Source Multi-GNSS Assisted GNSS Reference Server.,"The Open Source GNSS Reference Server (OSGRS) exploits the GNSS Reference Interface Protocol (GRIP) to provide assistance data to GPS receivers. Assistance can be in terms of signal acquisition and in the processing of the measurement data. The data transfer protocol is based on Extensible Mark-up Language (XML) schema. The first version of the OSGRS required a direct hardware connection to a GPS device to acquire the data necessary to generate the appropriate assistance. Scenarios of interest for the OSGRS users are weak signal strength indoors, obstructed outdoors or heavy multipath environments. This paper describes an improved version of OSGRS that provides alternative assistance support from a number of Global Navigation Satellite Systems (GNSS). The underlying protocol to transfer GNSS assistance data from global casters is the Networked Transport of RTCM (Radio Technical Commission for Maritime Services) over Internet Protocol (NTRIP), and/or the RINEX (Receiver Independent Exchange) format. This expands the assistance and support model of the OSGRS to globally available GNSS data servers connected via internet casters. A variety of formats and versions of RINEX and RTCM streams become available, which strengthens the assistance provisioning capability of the OSGRS platform. The prime motivation for this work was to enhance the system architecture of the OSGRS to take advantage of globally available GNSS data sources. Open source software architectures and assistance models provide acquisition and data processing assistance for GNSS receivers operating in weak signal environments. This paper describes test scenarios to benchmark the OSGRSv2 performance against other Assisted-GNSS solutions. Benchmarking devices include the SPOT satellite messenger, MS-Based & MS-Assisted GNSS, HSGNSS (SiRFstar-III) and Wireless Sensor Networks Assisted-GNSS. Benchmarked parameters include the number of tracked satellites, the Time to Fix First (TTFF), navigation availability and accuracy. Three different configurations of Multi-GNSS assistance servers were used, namely Cloud-Client-Server, the Demilitarized Zone (DMZ) Client-Server and PC-Client-Server; with respect to the connectivity location of client and server. The impact on the performance based on server and/or client initiation, hardware capability, network latency, processing delay and computation times with their storage, scalability, processing and load sharing capabilities, were analysed. The performance of the OSGRS is compared against commercial GNSS, Assisted-GNSS and WSN-enabled GNSS devices. The OSGRS system demonstrated lower TTFF and higher availability."
pub.1173642265,Optimizing and Scaling the 3D Reconstruction of Single-Particle Imaging,"An X-ray free electron laser (XFEL) facility can produce on the order of 1,000,000 extremely bright X-ray light pulses per second. Using an XFEL to image the atomic structure of a molecule requires fast analysis of an enormous amount of data, estimated to exceed one terabyte per second and requiring petabytes of storage. The SpiniFEL application provides such analysis by determining the 3D structure of proteins from single-particle imaging (SPI) experiments performed using XFELs, but it needs significantly better performance and efficiency to scale and keep up with the terabyte-per-second data production. Thus, this paper addresses the high-performance computing optimizations and scaling needed to improve this 3D reconstruction of SPI data. First, we optimize data movement, memory efficiency, and algorithms to improve the per-node computational efficiency and deliver a 5.28× speedup over the baseline GPU implementation. In addition, we achieved a 485× speedup for the post-analysis reconstruction resolution, which previously took as long as the 3D reconstruction of SPI data. Second, we present a novel distributed shared-memory computational algorithm to hide data latency and load-balance network traffic, thus enabling the processing of 128× more orientations than previously possible. Third, we conduct an exploratory study over the hyperparameter space for the SpiniFEL application to identify the optimal parameters for our underlying target hardware, which ultimately led to an up to 1.25× speedup for the number of streams. Overall, we achieve a 6.6× speedup (i.e., 5.28×1.25) over the previous fastest GPUMPI-based SpiniFEL realization."
pub.1127081640,ParPaRaw,"Parsing is essential for a wide range of use cases, such as stream processing, bulk loading, and in-situ querying of raw data. Yet, the compute-intense step often constitutes a major bottleneck in the data ingestion pipeline, since parsing of inputs that require more involved parsing rules is challenging to parallelise. This work proposes a massively parallel algorithm for parsing delimiter-separated data formats on GPUs. Other than the state-of-the-art, the proposed approach does not require an initial sequential pass over the input to determine a thread's parsing context. That is, how a thread, beginning somewhere in the middle of the input, should interpret a certain symbol (e.g., whether to interpret a comma as a delimiter or as part of a larger string enclosed in double-quotes). Instead of tailoring the approach to a single format, we are able to perform a massively parallel finite state machine (FSM) simulation, which is more flexible and powerful, supporting more expressive parsing rules with general applicability. Achieving a parsing rate of as much as 14.2 GB/s, our experimental evaluation on a GPU with 3 584 cores shows that the presented approach is able to scale to thousands of cores and beyond. With an end-to-end streaming approach, we are able to exploit the full-duplex capabilities of the PCIe bus and hide latency from data transfers. Considering the end-to-end performance, the algorithm parses 4.8 GB in as little as 0.44 seconds, including data transfers."
pub.1061621993,Distributing Internet Services to the Network'S Edge,"In the context of industrial information technology, the Internet and World Wide Web increasingly are seen as a solution to the problem of providing “anywhere, anytime” services. In the classical view of an Internet-enabled IT infrastructure, services are requested and consumed by a user (e.g., a human requesting plant production data from his or her desktop) and data are provided by an origin server (e.g., a Web server located in a plant that can authenticate users, implement encryption, serve data, and source multimedia streams). This rather simplistic view works well if the number of users is small, the complexity of services required is modest, and the real-time response requirements are lax. However, it fails to scale when one accounts for the complexities of modern networking: many simultaneous users, potentially operating in multiple languages; many complex data types, including incompatible display formats; many differing schemes for implementing privacy and security through many combinations of authentication and encryption. In this paper we propose an alternative—a client/edge server/origin server architecture that can distribute some complex data processing and device interface tasks to a network edge device, the NetEdge. We show how this device can support services thought to be useful to the industrial environment, such as language translation, image transcoding, access device adaptation, virus scanning, content assembly, local content insertion, and caching. The proposal is a win–win situation for all participants: industrial content providers need maintain only one copy of their content, yet consumers are provided with richer services and device-independent interfaces. Although the services provided define the utility of the product, the heart of the NetEdge is its rule engine. Rules specify which service requests, crossing specified processing points, invoke which service callouts. We explore how a proxylet interface connects the rule engine, through Java and C APIs, to the callout engine. We close with performance measurements of the NetEdge throughput and latency characteristics."
pub.1045308075,Automatic recovering of RTSP sessions in mobile telephones using JADE-LEAP,"Streaming is a technique used to transmit information over the network so that its issuing from the server, its communication along the network and its downloading and processing on the client overlapp, and moreover it must not be saved in the client memory. This technique is very effective for transmitting multimedia information (video) and is very suitable for mobile phones because there is no need to store the video in its memory that it is limited (data storage is still quite limited in the last generation mobile phones indeed). While the transmission speed of wireless networks has increased significantly, this technique is still very effective because it hides the latency of the network. On the other hand, the various wireless communication technologies used by mobile phones today (mainly wireless fidelity (WiFi), Bluetooth and 3G) are very likely to suffer physical intermittent disconnections making them miss the video streaming session every time a mobile phone loses network coverage for a certain time and forces him to reconnect manually, with the consequent loss of effectiveness of the streaming technique (that is lost due to the effectiveness of concealment of network latency). This work shows an automatic recovery solution for lost video stream and missed session by creating a new mechanism based on proxies implemented with software agents of the Java platform software development framework-lightweight extensible agent platform (JADE-LEAP). As far as our knowledge, this is the first time the efficient use of this platform for streaming video on mobile phones is tested. In addition, the experimental results obtained are very promising for its effectiveness and relevance to the practical case studies we've tested."
pub.1117918485,"Fog Based Architectures for IoT: Survey on Motivations, Challenges and Solution Perspectives","Advent of New age applications encompassing IoT, 5G wireless systems, Artificial Intelligence, real time analytics and stream mining has added new challenges to how cloud computing paradigm handles computing, storage and bandwidth. Many of the applications from the aforesaid domains are time critical and hence have stringent latency requirements and Network bandwidth constraints. It is impractical to depend on the cloud for processing the voluminous amount of data produced by these applications. Hence change in the computing- storage and Networking architectures of these cloud based systems is inevitable. The emerging “Fog Computing” paradigm promises to address the challenges specified above. Fog moves significant amount of computing, storage, communication and Networking away from cloud and performs the same in proximity to end user, thereby addressing the latency and bandwidth requirements related issues. Hence, Fog architecture enhances the distributed nature of an application and adds agility and efficiency to the applications by reducing the dependency on Cloud. However, this emerging Fog computing paradigm also poses its own set of challenges which must be understood and addressed in order to provide a viable framework of computing. This paper presents a thorough study on motivations behind the fog computing paradigm and various challenges in proposing a viable fog computing architecture for the resource intensive and time critical applications. Number of existing solution perspectives to address these challenges is studied and their strengths, benefits and drawbacks are analyzed. Also, scope for future research in this domain are highlighted."
pub.1174767337,Darkness-Adaptive Action Recognition: Leveraging Efficient Tubelet Slow-Fast Network for Industrial Applications,"Infrared (IR) technology has emerged as a solution for monitoring dark environments. It offers resilience to shifting illumination, appearance changes, and shadows, with applications spanning self-driving cars, robotics, nighttime security, and many other fields. While existing state-of-the-art RGB-based human action recognition (AR) models exhibit limitations in scalability for action understanding under uncertain, low-light, or dark conditions. Integrating these with IR data faces challenges due to changes in modality, high resource demands, and strict latency requirements. Such issues hinder the deployment of these technologies in real-world settings. To overcome these challenges, we introduce a novel slow-fast tubelet (SFT) processing framework designed for efficient and accurate AR in IR-based scenarios. The SFT framework comprises three modules: tubelet preprocessing (TPP), feature extraction, and the feature lateral connection and recognition module (FELCM). The TPP module refines IR streams by extracting the region of interest, filtering detected objects, removing noise, and generating tubelets of refined frames. The FELCM processes refined tubelet through two pathways, where the fast tubelet path operates at a high rate and the slow tubelet path operates at a slow rate. These pathways interconnect through lateral connections, facilitating mutual updates, and enhancing the prediction efficiency. We conducted extensive experiments on benchmark datasets, including NTURGB-D 120 and infrared action recognition (InfAR). The results demonstrate that our proposed SFT framework surpasses state-of-the-art approaches in terms of accuracy (2.7% and 3.3% improvement, respectively), computational cost, and inference latency while maintaining the competitive recognition performance. Our framework's promising results underscore its potential for direct deployment in real-world applications."
pub.1135110613,4K 120fps HEVC Encoder with Multi-Chip Configuration,"This paper describes a novel 4K 120fps (frames per second) real-time HEVC (High Efficiency Video Coding) encoder for high-frame-rate video encoding and transmission. Motion portrayal problems such as motion blur and jerkiness may occur in video scenes containing fast-moving objects or quick camera panning. A high-frame-rate solves such problems and provides a more immersive viewing experience that can express even the fast-moving scenes without discomfort. It can also be used in remote operation for scenes with high motion, such as VAR (Video Assistant Referee) systems in sports. Real-time encoding of high-frame-rate videos with low latency and temporal scalability is required for providing such high-frame-rate video services. The proposed encoder achieves full 4K/120fps real-time encoding, which is twice the current 4K service frame rate of 60fps, by multichip configuration with two encoder LSI. Exchange of reference picture data near a spatially divided slice boundary provides cross-chip motion estimation, and maintains the coding efficiency. The encoder supports temporal-scalable coding mode, in which it output stream with temporal scalability transmitted over one or two transmission paths. The encoder also supports the other mode, low-delay coding mode, in which it achieves 21.8msec low-latency processing through motion vector restriction. Evaluation of the proposed encoder's multichip configuration shows that the BD-bitrate (the average rate of bitrate increase), compared to simple slice division without inter-chip transfer, is -2.86% at minimum and -2.41% on average in temporal-scalable coding mode. The proposed encoder system will open the door to the next generation of high-frame-rate UHDTV (ultra-high-definition television) services."
pub.1119427054,Low-Latency Graph Streaming Using Compressed Purely-Functional Trees,"Due to the dynamic nature of real-world graphs, there has been a growing
interest in the graph-streaming setting where a continuous stream of graph
updates is mixed with arbitrary graph queries. In principle, purely-functional
trees are an ideal choice for this setting due as they enable safe parallelism,
lightweight snapshots, and strict serializability for queries. However,
directly using them for graph processing would lead to significant space
overhead and poor cache locality.
  This paper presents C-trees, a compressed purely-functional search tree data
structure that significantly improves on the space usage and locality of
purely-functional trees. The key idea is to use a chunking technique over trees
in order to store multiple entries per tree-node. We design
theoretically-efficient and practical algorithms for performing batch updates
to C-trees, and also show that we can store massive dynamic real-world graphs
using only a few bytes per edge, thereby achieving space usage close to that of
the best static graph processing frameworks.
  To study the efficiency and applicability of our data structure, we designed
Aspen, a graph-streaming framework that extends the interface of Ligra with
operations for updating graphs. We show that Aspen is faster than two
state-of-the-art graph-streaming systems, Stinger and LLAMA, while requiring
less memory, and is competitive in performance with the state-of-the-art static
graph frameworks, Galois, GAP, and Ligra+. With Aspen, we are able to
efficiently process the largest publicly-available graph with over two hundred
billion edges in the graph-streaming setting using a single commodity multicore
server with 1TB of memory."
pub.1044746552,Streaming Video Using Dynamic Rate Shaping and TCP Congestion Control,"We present a new technique for streaming real time video on today's Internet, based ondynamic rate shapingandTCP congestion control. Dynamic rate shaping is a signal processing technique that adapts the rate of compressed video (MPEG-1, MPEG-2, H.26x) to dynamically varying bandwidth constraints. This provides an interface (or filter) between the source and the network, with which the encoder's output (either live or stored) can be perfectly matched to the network's available bandwidth. We couple this adaptation capability with the use of a new semi-reliable protocol that uses the TCP congestion window to pace the delivery of data into the network, but without using other TCP algorithms that are poorly suited to real time media. Use of TCP congestion control ensures that the protocol competes fairly with all other TCP data and that it optimally shares the available bandwidth. It also avoids the latency problems commonly associated with TCP. In addition, we describe a real application that uses this approach to stream MPEG video on the Internet. We present several experiments, performed in both a controlled environment and the wide area Internet, that were used to evaluate the effectiveness and fairness of the scheme. The results show that the proposed solution achieves superior video quality while at the same time providing fairness by sharing bandwidth equally with other non-real-time connections."
pub.1151028518,Performance optimization of serverless edge computing function offloading based on deep reinforcement learning,"It is difficult for resource-constrained edge servers to simultaneously meet the performance requirements of all the latency-sensitive Internet of Things (IoT) applications in edge computing. Therefore, it is a significant challenge to efficiently generate a task offloading strategy. Recently, deep reinforcement learning (DRL)-based task offloading methods have been studied to ensure long-term performance optimization. However, there are challenges in existing DRL-based task offloading methods, such as insufficient sample diversity and high exploration cost. To optimize the performance of edge computing and facilitate the development and deployment of event-driven IoT applications, the serverless edge computing model has emerged. It combines serverless computing, also known as Function as a Service (FaaS), with edge computing and has been adopted in edge AI inference and prediction, stream processing, face recognition, etc. In this paper, an experience-sharing deep reinforcement learning-based distributed function offloading method called ES-DRL is proposed in the setting of a combined stateful and stateless execution model for serverless edge computing. ES-DRL adopts a distributed learning architecture, where each edge FaaS (EFaaS) obtains the current state of the local environment and inputs them to the local DRL agent, which outputs the function offloading strategy. Then, each EFaaS uploads the experience data interacting with the environment to a global shared replay buffer located in the cloud and randomly draws a batch of data from it to optimize the parameters of the local network. A population-guided policy search method is introduced to speed up the convergence of the DRL agent and avoid falling into the local optimum. The experimental results demonstrate that ES-DRL can reduce the average latency by up to approximately 17 percent compared to the existing DRL-based task offloading method."
pub.1136000418,Application-aware resource allocation and data management for MEC-assisted IoT service providers,"To support the growing demand for data-intensive and low-latency IoT applications, Multi-Access Edge Computing (MEC) is emerging as an effective edge-computing approach enabling the execution of delay-sensitive processing tasks close to end-users. However, most of the existing works on resource allocation and service placement in MEC systems overlook the unique characteristics of new IoT use cases. For instance, many IoT applications require the periodic execution of computing tasks on real-time data streams that originate from devices dispersed over a wide area. Thus, users requesting IoT services are typically distant from the data producers. To fill this gap, the contribution of this work is two-fold. Firstly, we propose a MEC-compliant architectural solution to support the operation of multiple IoT service providers over a common MEC platform deployment, which enables the steering and shaping of IoT data transport within the platform. Secondly, we model the problem of service placement and data management in the proposed MEC-based solution taking into account the dependencies at the data level between IoT services and sensing resources. Our model also considers that caches can be deployed on MEC hosts, to allow the sharing of the same data between different IoT services with overlapping geographical scope, and provides support for IoT services with heterogeneous QoS requirements, such as different frequencies of periodic task execution. Due to the complexity of the optimisation problem, a heuristic algorithm is proposed using linear relaxation and rounding techniques. Extensive simulation results demonstrate the efficiency of the proposed approach, especially when traffic demands generated by the service requests are not uniform."
pub.1008906551,OpenMDSP: Extending OpenMP to Program Multi-Core DSPs,"Multi-core digital signal processors (DSPs) are widely used in wireless telecommunication, core network transcoding, industrial control, and audio/video processing technologies, among others. In comparison with general-purpose multi-processors, multi-core DSPs normally have a more complex memory hierarchy, such as on-chip core-local memory and non-cache-coherent shared memory. As a result, efficient multi-core DSP applications are very difficult to write. The current approach used to program multi-core DSPs is based on proprietary vendor software development kits (SDKs), which only provide low-level, non-portable primitives. While it is acceptable to write coarse-grained task-level parallel code with these SDKs, writing fine-grained data parallel code with SDKs is a very tedious and error-prone approach. We believe that it is desirable to possess a high-level and portable parallel programming model for multi-core DSPs. In this paper, we propose OpenMDSP, an extension of OpenMP designed for multi-core DSPs. The goal of OpenMDSP is to fill the gap between the OpenMP memory model and the memory hierarchy of multi-core DSPs. We propose three classes of directives in OpenMDSP, including 1) data placement directives that allow programmers to control the placement of global variables conveniently, 2) distributed array directives that divide a whole array into sections and promote the sections into core-local memory to improve performance, and 3) stream access directives that promote big arrays into core-local memory section by section during parallel loop processing while hiding the latency of data movement by the direct memory access (DMA) of a DSP. We implement the compiler and runtime system for OpenMDSP on FreeScale MSC8156. The benchmarking results show that seven of nine benchmarks achieve a speedup of more than a factor of 5 when using six threads."
pub.1122154199,Internet of Things and Artificial Intelligence,"The escalation in connected Internet of Things (IoT) devices has created a huge demand for artificial intelligence (AI)-based methodologies. Though revolutionary, IoT is associated with limited computational capabilities in terms of storage and processing. IoT alone also suffers from certain issues such as privacy, reliability, security, and performance. Hence, IoT requires integration with the Cloud of Things (CoT), which paves the path to resolve many of these issues. With the exponential growth of IoT applications, traditional centralized cloud computing faces challenges such as limited capacity, network crashes, high latency, and many more. Because the number of devices in smart environments is on rise, the response time required by these devices requires IoT data to be managed and processed near the same location where it is being generated (i.e., at the edge). Fog computing too complements IoT and edge computing by providing data storage and data processing at IoT devices instead of directing the data to the cloud. Hence, such IoT-based architectures are required to preserve the benefits of both IoT-enabled cloud computing, edge processing, and fog processing. AI and its supplements provide support in achieving IoT-based environments. However, most AI and machine-learning algorithms require a substantial amount of processing power, which may or may not be available at the edge. Current researchers hope to develop some architecture that strikes a positive balance between all the benefits and requirements of IoT and edge computing. This chapter includes an introduction to IoT and AI, IoT and edge computing, and IoT and fog computing, and their various building blocks. Discussion of the benefits and security aspects of edge and fog computing is also included. This chapter focuses on delivering a brief introduction on Internet of Things (IoT) with Artificial Intelligence (AI), edge computing, and fog computing, and the comparisons between cloud computing and edge and fog computing. The IoT field requires the interdisciplinary nature of the technologies. As discussed by Atzori, the usefulness of IoT can be implicit in the application domain at the intersection of the following three paradigms: Internet oriented, things oriented, and semantic oriented. IoT requires sensors to be embedded into all kinds of devices, which provide streams of data through Internet connectivity to a central location. Machine learning (ML) provides the ability to perform pattern detection in the available datasets. Apart from ML, other AI technologies, such as computer vision and speech recognition, help in making smart decisions in certain situations. Edge computing permits data produced by IoT devices to be processed nearest to where it has been created instead of transferring the data to clouds via long courses."
pub.1095841448,Beyond the Fog: Bringing Cross-Platform Code Execution to Constrained IoT Devices,"Considering the prediction that there will be over 50 billion devices connected to the Internet of Things (IoT) in the near future, the demand for efficient ways to process data streams generated by sensors grows ever larger, highlighting the necessity to re-evaluate current approaches, such as sending all data to the cloud for processing and analysis. In this paper, we explore one of the methods for improving this scenario: bringing the computation closer to data sources. By executing the code on the IoT devices themselves instead of on the network edge or the cloud, solutions can better meet the latency requirements of several applications, avoid problems with slow and intermittent network connections, prevent network congestion, and potentially save energy by reducing communication. To this end, we propose the LMC framework and compare it with Edgent, an open-source project that is under development by the Apache Incubator. By using a DragonBoard 410c to execute a simple filter, an outlier detector, and a program that calculates the FFT, we obtained results that indicate that LMC outperforms Edgent when dynamic translation is disabled for both of them and is more suitable for lightweight quick queries otherwise. More importantly, the LMC also enables us to perform cross-platform code execution on small, cheap devices that do not have enough resources to run Edgent, like the NodeMCU 1.0."
pub.1018535159,Optimization of Analytic Data Flows for Next Generation Business Intelligence Applications,"This paper addresses the challenge of optimizing analytic data flows for modern business intelligence (BI) applications. We first describe the changing nature of BI in today’s enterprises as it has evolved from batch-based processes, in which the back-end extraction-transform-load (ETL) stage was separate from the front-end query and analytics stages, to near real-time data flows that fuse the back-end and front-end stages. We describe industry trends that force new BI architectures, e.g., mobile and cloud computing, semi-structured content, event and content streams as well as different execution engine architectures. For execution engines, the consequence of “one size does not fit all” is that BI queries and analytic applications now require complicated information flows as data is moved among data engines and queries span systems. In addition, new quality of service objectives are desired that incorporate measures beyond performance such as freshness (latency), reliability, accuracy, and so on. Existing approaches that optimize data flows simply for performance on a single system or a homogeneous cluster are insufficient. This paper describes our research to address the challenge of optimizing this new type of flow. We leverage concepts from earlier work in federated databases, but we face a much larger search space due to new objectives and a larger set of operators. We describe our initial optimizer that supports multiple objectives over a single processing engine. We then describe our research in optimizing flows for multiple engines and objectives and the challenges that remain."
pub.1152373564,Fog Data Processing and Analytics for Agriculture IoT Data Streams,"Pervasiveness rise of smart devices and sensor-based gadgets in building IoT systems is increasing unprecedented in technological innovations. The emergence of the Internet of Things technologies reshaped nearly every sector including agriculture. The agriculture sector contributes a significant figure to the country’s economy and it has a wide-ranging involvement in the advancement of human civilization. In the current scenario, the proper techniques of farming are needed as utmost propriety for better crop quality and quantity in a high-competition market. Crop disease prediction is key to shattering the problems of the farmer, reducing the usage of insecticides, and pesticides, and improving the financial conditions of the farmer. The Internet of Things and data analytics possess the ability to positively modernize the agricultural sector. However, Internet of Things-based applications needed to be deployed on a platform that offers real-time experience, reduced latency, and optimal bandwidth usage. Fog computing extends the computational power closer to the edge network where the devices reside and facilitate edge intelligence. In this paper, the fog importance, fog computing architecture along with the perceptions of data analysis in a fog environment, and emerging research challenges are discussed. Furthermore, in this paper an IoT-Fog based framework for the prediction of crop disease is proposed. The proposed framework consists of four phases, sensor layer, fog layer, cloud layer and End-user. The proposed framework may be beneficial in the farming domain for the analysis of crop disease prediction in the early stage and may reduce the chances of disease outbreaks in the field"
pub.1132681589,L0TP+: the Upgrade of the NA62 Level-0 Trigger Processor,"
                    The L0TP+ initiative is aimed at the upgrade of the FPGA-based Level-0 Trigger Processor (L0TP) of the NA62 experiment at CERN for the post-LS2 data taking, which is expected to happen at 100% of design beam intensity, corresponding to about 3.3 × 10
                    12
                    protons per pulse on the beryllium target used to produce the kaons beam. Although tests performed at the end of 2018 showed a substantial robustness of the L0TP system also at full beam intensity, there are several reasons to motivate such an upgrade: i) avoid FPGA platform obsolescence, ii) make room for improvements in the firmware design leveraging a more capable FPGA device, iii) add new functionalities, iv) support the 4 beam intensity increase foreseen in future experiment upgrades. We singled out the Xilinx Virtex UltraScale+ VCU118 development board as the ideal platform for the project. L0TP+ seamless integration into the current NA62 TDAQ system and exact matching of L0TP functionalities represent the main requirements and focus of the project; nevertheless, the final design will include additional features, such as a PCIe RDMA engine to enable processing on CPU and GPU accelerators, and the partial reconfiguration of trigger firmware starting from a high level language description (C/C++). The latter capability is enabled by modern High Level Synthesis (HLS) tools, but to what extent this methodology can be applied to perform complex tasks in the L0 trigger, with its stringent latency requirements and the limits imposed by single FPGA resources, is currently being investigated. As a test case for this scenario we considered the online reconstruction of the RICH detector rings on an HLS generated module, using a dedicated primitives data stream with PM hits IDs. Besides, the chosen platform supports the Virtex Ultrascale+ FPGA wide I/O capabilities, allowing for straightforward integration of primitive streams from additional sub-detectors in order to improve the performance of the trigger.
                  "
pub.1094862160,GAMT: A Fast and Scalable IP Lookup Engine for GPU-based Software Routers,"Recently, the Graphics Processing Unit (GPU) has been proved to be an exciting new platform for software routers, providing high throughput and flexibility. However, it is still a challenging task to deploy some core routing functions into GPU-based software routers with anticipatory performance and scalability, such as IP address lookup. Existing solutions have good performance, but their scalability to IPv6 and frequent updates are not so encouraging. In this paper, we investigate GPU's characteristics in parallelism and memory accessing, and then encode a multi-bit trie into a state-jump table. On this basis, a fast and scalable IP lookup engine called GPU-Accelerated Multi-bit Trie (GAMT) has been presented. According to our experiments on real-world routing data, based on the multi-stream pipeline, GAMT enables lookup speeds as high as 1072 and 658 Million Lookups Per Second (MLPS) for IPv4/6 respectively, when performing a $16 M$ traffic under highly frequent updates (70,000 updates/s). Even using a small batch size, GAMT can still achieve 339 and 240 MLPS respectively, while keeping the average lookup latency below 100 µs. These results show clearly that GAMT makes significant progress on both scalability and performance."
pub.1170435546,Conan's Bow Tie: A Streaming Voice Conversion for Real-Time VTuber Livestreaming,"Recent years have witnessed a dramatic growing trend of Virtual YouTubers (VTubers) as a new business on social media, such as YouTube, Twitch, and TikTok. However, a significant challenge arises when VTuber voice actors face health issues or retire, jeopardizing the continuity of their avatar’s recognizable voices. A potential solution reminiscent of Conan’s Bow Tie voice changer in the popular animation Case Closed (i.e., Detective Conan) has inspired our work. To make this a reality, we introduce VTuberBowTie, a user-friendly streaming voice conversion system for real-time VTuber livestreaming. We propose an innovative streaming voice conversion approach that tackles the challenges of limited context modeling and bidirectional context dependence inherent to conventional real-time voice conversion. Rather than individually processing the voice stream in data chunks, our approach adopts a fully sequential structure that leverages contextual information preceding the input chunk, thereby expanding the perceptual range and enabling seamless concatenation. Moreover, we developed a ready-to-use interaction interface for VTuberBowTie and deployed it on various computing platforms. The experimental results show that VTuberBowTie can achieve high-quality voice conversion in a streaming manner with a latency of 179.1ms on CPU and 70.8ms on GPU while providing users a friendly interactive experience."
pub.1175096768,MECM2M: An MEC Based M2M System Considering Mobile Nodes as Compute Servers,"In addition to stationary sensors/actuators (nodes), mobile nodes such as vehicles and smartphones are connected to M2M (Machine-to-Machine) systems. Some M2M systems take into account node mobility, while some M2M systems regard vehicles as compute servers. N one of them, however, handle the mobility of nodes that act as compute servers. This paper proposes an MEC (Multi-access Edge Computing) based M2M system called MECM2M, which can handle the mobility of nodes acting as sensors/actuators and compute servers. The design policy of MECM2M is analogous to that of the Internet. The Internet provides two fundamental services: reliable byte stream and unreliable datagram. How data is used is up to applications. MECM2M provides M2M applications with three fundamental services as the M2M services: area-based data acquisition, node-based data acquisition, and node-based action instruction. How sensing data is used is up to M2M applications. MECM2M introduces the virtual nodes as digital twins in the virtual space for the physical nodes. The virtual nodes conceal differences from the physical nodes and provide a uniform interface. The relation among the virtual nodes, the physical nodes, and the locations of the physical nodes is managed by a domain ontology called Onto- Mecm2m,which enables effective processing of the three fundamental services. The evaluation results on an MECM2M emulator show that the response times of the three fundamental services satisfy the latency constraints, i.e., 100 ms, defined in an ETSI (European Telecommunications Standards Institute) document."
pub.1128926233,Asynchronous Scrambled Coded Multiple Access for 5G Non-Orthogonal Multiple Access: System Level Performance,"Asynchronous Scrambled Coded Multiple Access (ASCMA) technology presents several features that make it well suited for IoT and mMTC type applications over state-of-the-art terrestrial wireless networks. These include low-rate error correction coding combined with low complexity receiver processing and an ability to transmit data asynchronously using single as well as multiple-streams from a user terminal. The resulting advantages in terms of spectral efficiency, low-latency, and reduced power consumption have been quantified previously using link-level simulations. In this paper, we evaluate the system level performance of a 5G New Radio (NR) mobile network employing ASCMA for uplink mMTC type applications. Using an accurate system level simulator, we evaluate two different scheduling schemes for ASCMA, the first assumes a grant-based scheduling of user terminals over available network resources and the second assumes random access or grant-free scheduling of the same. A novel abstraction method is developed to aid the system level evaluations. Using extensive simulations, we demonstrate the ability of ASCMA technology to provide significant improvements to key network performance indicators such as packet loss rate, average retransmission rate, average cell throughput and cell-edge throughput."
pub.1136499203,A Follow-the-Leader Strategy Using Hierarchical Deep Neural Networks with Grouped Convolutions,"The task of following-the-leader is implemented using a hierarchical deep neural network (DNN) end-to-end driving model to match the direction and speed of a target pedestrian. The model uses a classifier DNN to determine if the pedestrian is within the field of view of the camera sensor. If the pedestrian is present, the image stream from the camera is fed to a regression DNN which simultaneously adjusts the autonomous vehicle’s steering and throttle to keep cadence with the pedestrian. If the pedestrian is not visible, the vehicle uses a straightforward exploratory search strategy to reacquire the tracking objective. The classifier and regression DNNs incorporate grouped convolutions to boost model performance as well as to significantly reduce parameter count and compute latency. The models are trained on the intelligence processing unit (IPU) to leverage its fine-grain compute capabilities to minimize time-to-train. The results indicate very robust tracking behavior on the part of the autonomous vehicle in terms of its steering and throttle profiles, while requiring minimal data collection to produce. The throughput in terms of processing training samples has been boosted by the use of the IPU in conjunction with grouped convolutions by a factor ∼3.5$$\sim \,3.5$$ for training of the classifier and a factor of ∼7$$\sim \,7$$ for the regression network. A recording of the vehicle tracking a pedestrian has been produced and is available on the web."
pub.1143875769,hARMS: A Hardware Acceleration Architecture for Real-Time Event-Based Optical Flow,"Event-based vision sensors produce asynchronous event streams with high
temporal resolution based on changes in the visual scene. The properties of
these sensors allow for accurate and fast calculation of optical flow as events
are generated. Existing solutions for calculating optical flow from event data
either fail to capture the true direction of motion due to the aperture
problem, do not use the high temporal resolution of the sensor, or are too
computationally expensive to be run in real time on embedded platforms. In this
research, we first present a faster version of our previous algorithm, ARMS
(Aperture Robust Multi-Scale flow). The new optimized software version (fARMS)
significantly improves throughput on a traditional CPU. Further, we present
hARMS, a hardware realization of the fARMS algorithm allowing for real-time
computation of true flow on low-power, embedded platforms. The proposed hARMS
architecture targets hybrid system-on-chip devices and was designed to maximize
configurability and throughput. The hardware architecture and fARMS algorithm
were developed with asynchronous neuromorphic processing in mind, abandoning
the common use of an event frame and instead operating using only a small
history of relevant events, allowing latency to scale independently of the
sensor resolution. This change in processing paradigm improved the estimation
of flow directions by up to 73% compared to the existing method and yielded a
demonstrated hARMS throughput of up to 1.21 Mevent/s on the benchmark
configuration selected. This throughput enables real-time performance and makes
it the fastest known realization of aperture-robust, event-based optical flow
to date."
pub.1160804279,Suit Up: AI MoCap,"We present a novel marker-based motion capture (MoCap) technology. Instead of leveraging initialization and tracking for marker labeling as traditional solutions do, the present system is built upon real-time and low-latency data-driven models and optimization techniques, offering new possibilities and overcoming limitations currently present in the MoCap landscape. Even though we similarly begin with unlabeled markers captured with optical sensing within a capturing area, our approach diverges as we follow a data-driven and optimization pipeline to simultaneously denoise the markers and robustly and accurately solve the skeleton per frame. Similarly to traditional marker-based options, our work demonstrates higher stability and accuracy than inertial and/or markerless optical MoCap. Inertial MoCap lacks absolute positioning and suffers from drifting, therefore, it is almost impossible to achieve comparable positional accuracy. Markerless solutions lack the existence of a strong prior (i.e., markers) to increase the capturing precision, while, due to the heavier workload, the capturing frequency cannot easily scale, resulting in inaccuracies in fast movements. On the other hand, traditional marker-based motion capture heavily relies on high-quality marker data, assuming precise localization, outlier elimination and consistent marker tracking. In contrast, our innovative approach operates without such assumptions, effectively mitigating input noise, including ghost markers, occlusions, marker swaps, misplacement and mispositioning. This noise tolerance enables our system to function seamlessly with cameras with lower cost and specifications. Our method introduces body structure invariance, empowering automatic marker layout configuration by selecting from a diverse pool of models trained with different marker layouts. Our proposed MoCap technology integrates various consumer-grade optical sensors, leverages efficient data acquisition, succeeds in precise marker position estimation and allows for spatio-temporal alignment of multi-view streams. Sequentially, by incorporating data-driven models, our system achieves low latency and real-time rate performances. Finally, efficient body optimization techniques further improve the final MoCap solving, enabling seamless integration into various applications requiring real-time, accurate and robust motion capture. Concluding, real-time communities can be benefited from our MoCap which is a) affordable; with the use of low-cost equipment, b) scalable; with processing on the edge, c) portable; with easy setup and spatial calibration, d) robust; on heavy occlusions, marker removal and camera coverage and e) flexible; no need for super precise marker placement, super precise camera calibration, body calibration per actor or manual marker configuration."
pub.1171650707,Quality of Service-Constrained Online Routing in High Throughput Satellites,"High throughput satellites (HTSs) outpace traditional satellites due to their multi-beam transmission. The rise of low Earth orbit mega constellations amplifies HTS data rate demands to terabits/second with acceptable latency. This surge in data rate necessitates multiple modems, often exceeding single device capabilities. Consequently, satellites employ several processors, forming a complex packet-switch network. This can lead to potential internal congestion and challenges in adhering to strict quality of service (QoS) constraints. While significant research exists on constellation-level routing, a literature gap remains on the internal routing within a single HTS. The intricacy of this internal network architecture presents a significant challenge to achieve high data rates. This paper introduces an online optimal flow allocation and scheduling method for HTSs. The problem is presented as a multi-commodity flow instance with different priority data streams. An initial full time horizon model is proposed as a benchmark. We apply a model predictive control (MPC) approach to enable adaptive routing based on current information and the forecast within the prediction time horizon while allowing for deviation of the latter. Importantly, MPC is inherently suited to handle uncertainty in incoming flows. Our approach minimizes the packet loss by optimally and adaptively managing the priority queue schedulers and flow exchanges between satellite processing modules. Central to our method is a routing model focusing on optimal priority scheduling to enhance data rates and maintain QoS. The model’s stages are critically evaluated, and results are compared to traditional methods via numerical simulations. Through simulations, our method demonstrates performance nearly on par with the hindsight optimum, showcasing its efficiency and adaptability in addressing satellite communication challenges."
pub.1168166742,Swarm Intelligence based Deep Learning Method for Health Monitoring System using Internet of Things (IoT),"The research conducted is significant because there is an increasing demand for reliable health monitoring, particularly in underserved and far-flung areas. However, issues with data quality, network optimization, and resource limits might hinder the efficacy of data collected by IoT devices, which otherwise provide until then unimaginable prospects. Swarm intelligence and deep learning together can improve these areas, making IoT-based health monitoring more effective. Efficient sensor placement, data transmission, and processing of massive, frequently noisy health data streams are among the biggest obstacles to widespread adoption of IoT-based health monitoring. Using swarm intelligence algorithms to optimize sensor placement and deep learning for data analysis, the proposed Internet of Things-based swarm health monitoring system (IoT-SHMS) approach addresses the aforementioned challenges. IoT-SHMS uses swarm intelligence techniques, such as Particle Swarm Optimization (PSO), to optimize sensor location and network topologies. This helps to increase the reliability of health prediction while decreasing operational expenses. Constant patient monitoring, early detection of health anomalies, and the detection of falls are a few of the numerous applications of an IoT-SHMS approach in the medical field. It additionally has applications in telemedicine, wellness monitoring, and emergency response systems to give doctors and nurses up-to-date information in a timely manner. Extensive simulation evaluations are presented to prove that IoT-SHMS works as intended. The advantage of our technology over conventional approaches is illustrated by the performance criteria of accuracy, latency, and energy efficiency. Based on these findings, IoT-SHMS appears to be a viable option for healthcare applications in the Internet of Things environment"
pub.1164902949,Quality of Service-Constrained Online Routing in High Throughput Satellites,"High throughput satellites (HTSs) outpace traditional satellites due to their
multi-beam transmission. The rise of low Earth orbit mega constellations
amplifies HTS data rate demands to terabits/second with acceptable latency.
This surge in data rate necessitates multiple modems, often exceeding single
device capabilities. Consequently, satellites employ several processors,
forming a complex packet-switch network. This can lead to potential internal
congestion and challenges in adhering to strict quality of service (QoS)
constraints. While significant research exists on constellation-level routing,
a literature gap remains on the internal routing within a single HTS. The
intricacy of this internal network architecture presents a significant
challenge to achieve high data rates.
  This paper introduces an online optimal flow allocation and scheduling method
for HTSs. The problem is presented as a multi-commodity flow instance with
different priority data streams. An initial full time horizon model is proposed
as a benchmark. We apply a model predictive control (MPC) approach to enable
adaptive routing based on current information and the forecast within the
prediction time horizon while allowing for deviation of the latter.
Importantly, MPC is inherently suited to handle uncertainty in incoming flows.
Our approach minimizes the packet loss by optimally and adaptively managing the
priority queue schedulers and flow exchanges between satellite processing
modules. Central to our method is a routing model focusing on optimal priority
scheduling to enhance data rates and maintain QoS. The model's stages are
critically evaluated, and results are compared to traditional methods via
numerical simulations. Through simulations, our method demonstrates performance
nearly on par with the hindsight optimum, showcasing its efficiency and
adaptability in addressing satellite communication challenges."
pub.1143750291,The Approach to Flow Management in Virtual Computational Environment for Up-to-Day Telecom Networks,"Data streams that are transmitted over communication networks are growing very quickly, this is facilitated by the progress of the service such as IoT, M2M, etc. The provision of high-quality communication services is increasingly influenced by the efficiency of performing calculations in the data centers of telecom operators, whose resources are limited. SDN technologies make it possible to redistribute transmitted data streams in networks to reduce their load, but they require more computing resources. Telecom operators’ networks not only transmit significant amounts of information, but also process very large information flows in their computing nodes, including virtual ones. Controlling the transfer process with a large number of threads becomes a bottleneck. To service flows in SDN, an application (at the application level) call is required, which entails overloading the computational resources involved in processing incoming flows. The main approach for dealing with the computing resources overload in such systems is load balancers that should distribute the load between computing nodes efficiently. However, such approaches lead to increased latency as the flow peak rate increases and there is a computing resources limitation. In this chapter, to reduce the time for making decisions when load balancing, «the endless train» method is proposed. This method, instead of analyzing the input flow state and simultaneously the resources state, analyses the state of the computational resources only to make a decision regarding needed resources based on the current task requirements. This allows reducing the time for making a decision on the choice of a server serving the input flow. To test the efficiency of the proposed method, an implementation scheme was developed using MS Azure. The process of dynamically deploying additional virtual servers (virtual machines) to handle threads in the event of overload was tested. The testing results show the effectiveness for overload prevention but the computational recourses usage is increased."
pub.1094065227,Multisensory Interactions of Audiovisual Stimuli Presented at Different Locations in Auditory-Attention Tasks: A Event-related Potential (ERP) Study,"we applied behavioral and event-related potential (ERP) measures to study the multisensory interactions of audiovisual stimuli presented at different locations in an auditory-attention task in which an irrelevant visual stimulus occasionally accompanied the auditory stimulus. A stream of visual (V), auditory (A), spatially congruent audiovisual (AV _Con), and spatially incongruent audiovisual (AV _Inc) stimuli were randomly presented to the left and/or right side; subjects covertly attended to the auditory stimuli on either the left or right side and promptly responded to auditory targets on that side. In present study, we only investigated the results which were related with multisensory interactions of audiovisual stimuli presented at different locations. ERPs to A, V_Inc which is at opposite locations with A, and AV_Inc stimuli were recorded. The neural basis of multisensory interactions of audiovisual stimuli presented at different location was studied by comparing ERPs to the AV_Inc stimuli with the sum of the ERPs to the unimodal A and V_Inc stimuli. ERPs results showed that main audiovisual interactions were found as a negativity in mid-central scalp sites around a latency of 300ms during the later stage of processing. The present results revealed the neural basis of multisensory interaction of audiovisual stimuli presented at different locations in an auditory-attention task. The study can offer base data for multi-sensor data fusion algorithms and robotics."
pub.1061515668,Efficient Distributed Query Processing,"A variety of wireless networks, including applications of Wireless Sensor Networks, Internet of Things, and Cyber-physical Systems, increasingly pervade our homes, retail, transportation systems, and manufacturing processes. Traditional approaches communicate data from all sensors to a central system, and users (humans or machines) query this central point for results, typically via the web. As the number of deployed sensors, and thus generated data streams, is increasing exponentially, this traditional approach may no longer be sustainable or desirable in some application contexts. Therefore, new approaches are required to allow users to directly interact with the network, for example, requesting data directly from sensor nodes. This is difficult, as it requires every node to be capable of point-to-point routing, in addition to identifying a subset of nodes that can fulfil a user's query. This paper presents Dragon, a platform that allows any node in the network to identify all nodes that satisfy user queries, i.e., request data from nodes, and relay the result to the user. The Dragon platform achieves this in a fully distributed way. No central orchestration is required, network overheads are low, and latency is improved over existing comparable methods. Dragon is evaluated on networks of various topologies and different network densities. It is compared with the state-of-the-art algorithms based on summary trees, like Innet and SENS-Join. Dragon is shown to outperform these approaches up to 88% in terms of network traffic required, also a proxy for energy efficiency, and 84% in terms of processing delay. Note to Practitioners—This work is motivated by the continuing deluge of constrained, wirelessly connected sensing and control devices. Networks of communicable sensors and actuators are finding increased applicability across a range of industries and application scenarios. They are often thought of as a subset of the ‘” internet of Things.” However, due to the inherent difficulty in building theses systems technically and in terms of balancing the tradeoffs between (economic) cost and performance (e.g., energy, latency, reliability, and determinism), uptake has been slow. The community is relatively small and therefore has not overcome all of the problems that present themselves considering required functionality of industrial applications. There is a need to find new ways to interact with these devices, particularly those with heterogeneous attributes. There is also clear motivation to progress from traditional system architectures, whereby all data sensed are transmitted to centralized storage and management platform, to decentralized means of interrogation and control. This work proposes a solution to this problem, describing and evaluating a novel framework to query constrained networked devices based on two key improvements over the current art. The first is construction and management of a dynamic routing mechanism that facilitates the "
pub.1123479352,A Unified Framework for Speech Separation,"Speech separation refers to extracting each individual speech source in a
given mixed signal. Recent advancements in speech separation and ongoing
research in this area, have made these approaches as promising techniques for
pre-processing of naturalistic audio streams. After incorporating deep learning
techniques into speech separation, performance on these systems is improving
faster. The initial solutions introduced for deep learning based speech
separation analyzed the speech signals into time-frequency domain with STFT;
and then encoded mixed signals were fed into a deep neural network based
separator. Most recently, new methods are introduced to separate waveform of
the mixed signal directly without analyzing them using STFT. Here, we introduce
a unified framework to include both spectrogram and waveform separations into a
single structure, while being only different in the kernel function used to
encode and decode the data; where, both can achieve competitive performance.
This new framework provides flexibility; in addition, depending on the
characteristics of the data, or limitations of the memory and latency can set
the hyper-parameters to flow in a pipeline of the framework which fits the task
properly. We extend single-channel speech separation into multi-channel
framework with end-to-end training of the network while optimizing the speech
separation criterion (i.e., Si-SNR) directly. We emphasize on how tied kernel
functions for calculating spatial features, encoder, and decoder in
multi-channel framework can be effective. We simulate spatialized reverberate
data for both WSJ0 and LibriSpeech corpora here, and while these two sets of
data are different in the matter of size and duration, the effect of capturing
shorter and longer dependencies of previous/+future samples are studied in
detail. We report SDR, Si-SNR and PESQ to evaluate the performance of developed
solutions."
pub.1165916234,Real-Time Lane Detection for Autonomous Vehicles,"Lane detection is an important task in the field of autonomous driving and driver assistance systems (ADAS). Accurate real-time lane detection is essential to ensure safe and efficient road navigation. This overview provides an overview of a real-time lane detection system that uses computer vision techniques to identify and track lane markings in live video streams. The proposed lane detection system uses a combination of image processing algorithms and machine learning techniques to extract lane markings from input video images. The system follows a step-by-step approach and starts preprocessing the images to improve the visibility of lane markings. A feature extraction process is then used to identify relevant lane features such as edges and slopes. These features are used to distinguish lane markers from other objects in the scene. To achieve real-time performance, the system uses optimized algorithms and parallel processing techniques. Using the computing power of modern GPUs and parallelization strategies, lane detection systems can process video images in near-real time, enabling timely responses even in dynamic driving scenarios. The system includes a machine learning component for lane classification, using trained classifiers to distinguish between lane markings and false alarms. This classifier is trained using a labeled dataset of lane markings and uses various techniques such as deep learning and traditional machine learning algorithms to achieve high accuracy. The lane detection system is validated and evaluated using benchmark data sets and real-world driving scenarios. System performance is measured in terms of accuracy, processing speed, and robustness in a variety of lighting conditions, weather conditions, and road types. Experimental results show that the proposed real-time lane detection system can achieve high accuracy and operate with low latency under real conditions. The system will help improve the safety and efficiency of autonomous vehicles and ADAS by providing accurate and timely lane information to the decision-making process."
pub.1087288776,A Domain-Specific Language for Software-Defined Radio,"Software-defined radio (SDR) is a demanding domain; real-world wireless protocols require high data rates and low latency. Existing SDR platforms, typically based on FPGAs, provide the necessary substrate for meeting these requirements, but the high-level tools available to program them are not capable of fully exploiting the underlying hardware to meet rigorous performance requirements. Ziria [11] demonstrated that a high-level language can compete in this demanding space, but its design was ad-hoc and overly influenced by the needs of the compiler writer since its surface language does double duty as the compiler’s intermediate language.We present a re-formulation of Ziria’s surface language that includes a new type system that allows this language, which is effectful, to elaborate into a pure, monadic language where effects such as input/output and reference manipulation can be distinguished purely by type. This re-formulation and its elaboration into a core language is embodied in a new compiler for Ziria, kzc. By choosing an appropriate type system, awkward syntactic distinctions currently made by Ziria can be eliminated, although our new implementation maintains source compatibility with the original compiler due to a large body of existing Ziria code (a full 802.11 physical layer implementation). Our contribution is a description of the surface language, its type system, and its elaboration into a core language. We also show that far from being limited to the SDR domain, the constructs built-in to Ziria are applicable to other resource-constrained domains that require high-speed data processing."
pub.1143497356,Consistency Guaranteed Multi Container Migration for Smart Community Network Services,"Smart Community (SC) uses IoT sensors to provide smart grid control, traffic management, and similar IoT services. These services expect to run at the network edge or fog layer to provide low latency network services, encapsulate citizens’ private information, support low-cost IoT terminals from cyber-attacks, and support other cutting-edge fog and edge services. SC edge is a service platform that supports edge/fog services for IoT terminals by using Docker containers. Initially, SC edge/fog computing nodes did not support the function of service migration. However, service migration is necessary to support remote deployment and service distribution in SC networks. The existing container migration techniques focus mainly on resource utilization. However, SC services should handle loss-free data stream processing and order-preservation of network packets to gather IoT sensor data after migration. In addition, SC services require one-to-many migration to support high throughput loads when required. Therefore, this paper focuses on enhancing SC service flexibility by introducing migration for relocatable and network consistency guaranteed containerized services. SC edge proposes multiple container migration techniques that are suitable for network services. The proposed techniques can improve resource consumption, guarantee network traffic consistency, and apply one-to-many migration patterns. Layer leveraging migration (LLM) reduces the overall migration time by 10.8% for an elastic search Docker container than available Docker migration methods. Additionally, consistency guaranteed migration (CGM) is proposed to guarantee network consistency. However, CGM consumes additional resources compared to LLM for IoT data management. Finally, One to N Consistency Guaranteed Migration (O2NCGM) is proposed to support one-to-many migration with data consistency that shows similar performance to CGM."
pub.1094653237,Toward Mobility Support for Information-Centric IoV in Smart City Using Fog Computing,"As a meaningful and typical application scenario of Internet of Things (IoT), Internet of Vehicles (IoV) has attracted a lot of attentions to solve the increasingly severe problem of traffic congestion and safety issues in smart city. Information-centric networks (ICN) is a main stream of next generation network because of its content based forwarding strategy and in-network caching properties. Many existing works have been done to introduce ICN in IoV because of IP-based network architecture's maladjustment of such extremely mobile and dynamic IoV environment. In contrast, ICN is able to sustain packet delivery in unreliable and extreme environment. However, the frequent mobility of the vehicles will consume ICN's network resources to incessantly update the Forward Information Base (FIB) which will further affect the aggregation processing. For example, geographic location based name schema aggregation will be affected dramatically by mobility problem. On the other hand, fog computing is an edge computing technology usually integrated with IoT by bringing computation and storage capacity near the underlying networks to provide low-latency and time sensitive services. In this paper, we integrate fog computing into information-centric IoV to provide mobility support by allocating different schema taking account of the data characteristic (e.g., user-shareable data, communication data). Moreover, we use the computation, storage and location-aware capabilities of fog to design a mobility support mechanism for data exchange and communication considering the feature of IoV. service (e.g., alarm danger in local, updating traffic information, V2V communication etc.). We evaluate related performances of the proposed mechanism in high mobility environment, compared with original information-centric IoV. The result shows the advantages of a fog computing based information-centric IoV."
pub.1148306876,Distributed-Swarm: A Real-Time Pattern Detection Model Based on Density Clustering,"The advancement of power technology and the improvement of people’s living standards promote the expansion of the power grid scale and the sharp rise in electricity consumption. In the power system, due to the use of various sensors, we can collect a large number of power data (eg. the spatial-temporal information of electric vehicle charging). Usually, such spatial-temporal data is generated in the form of a data stream. The analysis and mining of such data can be widely applied in power equipment condition monitoring and maintenance, user equipment anomaly warning, urban power grid analysis and other scenarios. Among them, the pattern detection of power data plays a key role in power data analysis. Since the power data such as the spatial-temporal information of electric vehicle charging is time-sensitive, it is crucial to perform real-time pattern mining in real-time monitoring systems. However, state-of-the-art pattern detection methods are built on batch mode. Extending such works directly to an online environment tends to result in (1) expensive network cost, (2) high processing latency, and (3) low accuracy results. In this paper, we propose a framework for frequent motion pattern detection of power data in the real-time distributed environment. Through the softmax differentiation function, the power data is filtered to reduce the workload and improve the performance of the framework. At the same time, we propose the concept of historical state matrix to solve the problem that the nodes of each physical partition in a distributed environment can not perceive each other. Extensive experiments are conducted on real dataset and the experimental results show that our pattern detection is about 70% faster than baseline methods, which proves the huge advantage of our approach over available solutions in the literature."
pub.1061803728,"Pruned Bit-Reversal Permutations: Mathematical Characterization, Fast Algorithms and Architectures","A mathematical characterization of serially pruned permutations (SPPs) employed in variable-length permuters and their associated fast pruning algorithms and architectures are proposed. Permuters are used in many signal processing systems for shuffling data and in communication systems as an adjunct to coding for error correction. Typically, only a small set of discrete permuter lengths are supported. Serial pruning is a simple technique to alter the length of a permutation to support a wider range of lengths, but results in a serial processing bottleneck. In this paper, parallelizing SPPs is formulated in terms of recursively computing sums involving integer floor functions using integer operations, in a fashion analogous to evaluating Dedekind sums. A mathematical treatment for bit-reversal permutations (BRPs) is presented, and closed-form expressions for BRP statistics including descents/ascents, major index, excedances/descedances, inversions, and serial correlations are derived. It is shown that BRP sequences have weak correlation properties. Moreover, a new statistic called permutation inliers that characterizes the pruning gap of pruned interleavers is proposed. Using this statistic, a recursive algorithm that computes the minimum inliers count of a pruned BR interleaver (PBRI) in logarithmic time is presented. This algorithm enables parallelizing a serial PBRI algorithm by any desired parallelism factor by computing the pruning gap in lookahead rather than a serial fashion, resulting in significant reduction in interleaving latency and memory overhead. Extensions to 2-D block and stream interleavers, as well as applications to pruned fast Fourier transforms and LTE turbo interleavers, are also presented. Moreover, hardware-efficient architectures for the proposed algorithms are developed. Simulation results of interleavers employed in modern communication standards demonstrate three to four orders of magnitude improvement in interleaving time compared to existing approaches."
pub.1118200813,"Pruned Bit-Reversal Permutations: Mathematical Characterization, Fast Algorithms and Architectures","A mathematical characterization of serially-pruned permutations (SPPs)
employed in variable-length permuters and their associated fast pruning
algorithms and architectures are proposed. Permuters are used in many signal
processing systems for shuffling data and in communication systems as an
adjunct to coding for error correction. Typically only a small set of discrete
permuter lengths are supported. Serial pruning is a simple technique to alter
the length of a permutation to support a wider range of lengths, but results in
a serial processing bottleneck. In this paper, parallelizing SPPs is formulated
in terms of recursively computing sums involving integer floor and related
functions using integer operations, in a fashion analogous to evaluating
Dedekind sums. A mathematical treatment for bit-reversal permutations (BRPs) is
presented, and closed-form expressions for BRP statistics are derived. It is
shown that BRP sequences have weak correlation properties. A new statistic
called permutation inliers that characterizes the pruning gap of pruned
interleavers is proposed. Using this statistic, a recursive algorithm that
computes the minimum inliers count of a pruned BR interleaver (PBRI) in
logarithmic time complexity is presented. This algorithm enables parallelizing
a serial PBRI algorithm by any desired parallelism factor by computing the
pruning gap in lookahead rather than a serial fashion, resulting in significant
reduction in interleaving latency and memory overhead. Extensions to 2-D block
and stream interleavers, as well as applications to pruned fast Fourier
transforms and LTE turbo interleavers, are also presented. Moreover,
hardware-efficient architectures for the proposed algorithms are developed.
Simulation results demonstrate 3 to 4 orders of magnitude improvement in
interleaving time compared to existing approaches."
pub.1132685999,A Follow-the-Leader Strategy using Hierarchical Deep Neural Networks with Grouped Convolutions,"The task of following-the-leader is implemented using a hierarchical Deep
Neural Network (DNN) end-to-end driving model to match the direction and speed
of a target pedestrian. The model uses a classifier DNN to determine if the
pedestrian is within the field of view of the camera sensor. If the pedestrian
is present, the image stream from the camera is fed to a regression DNN which
simultaneously adjusts the autonomous vehicle's steering and throttle to keep
cadence with the pedestrian. If the pedestrian is not visible, the vehicle uses
a straightforward exploratory search strategy to reacquire the tracking
objective. The classifier and regression DNNs incorporate grouped convolutions
to boost model performance as well as to significantly reduce parameter count
and compute latency. The models are trained on the Intelligence Processing Unit
(IPU) to leverage its fine-grain compute capabilities in order to minimize
time-to-train. The results indicate very robust tracking behavior on the part
of the autonomous vehicle in terms of its steering and throttle profiles, while
requiring minimal data collection to produce. The throughput in terms of
processing training samples has been boosted by the use of the IPU in
conjunction with grouped convolutions by a factor ~3.5 for training of the
classifier and a factor of ~7 for the regression network. A recording of the
vehicle tracking a pedestrian has been produced and is available on the web.
This is a preprint of an article published in SN Computer Science. The final
authenticated version is available online at:
https://doi.org/https://doi.org/10.1007/s42979-021-00572-1."
pub.1142947290,A Proposed of Multimedia Compression System Using Three - Dimensional Transformation,"Video compression has become especially important nowadays with the increase of data transmitted over transmission channels, the reducing the size of the videos must be done without affecting the quality of the video. This process is done by cutting the video thread into frames of specific lengths and converting them into a three-dimensional matrix. The proposed compression scheme uses the traditional red-green-blue color space representation and applies a three-dimensional discrete Fourier transform (3D-DFT) or three-dimensional discrete wavelet transform (3D-DWT) to the signal matrix after converted the video stream to three-dimensional matrices. The resulting coefficients from the transformation are encoded using the EZW encoder algorithm. Three main criteria by which the performance of the proposed video compression system will be tested; Compression ratio (CR), peak signal-to-noise ratio (PSNR) and processing time (PT). Experiments showed high compression efficiency for videos using the proposed technique with the required bit rate, the best bit rate for traditional video compression. 3D discrete wavelet conversion has a high frame rate with natural spatial resolution and scalability through visual and spatial resolution Beside the quality and other advantages when compared to current conventional systems in complexity, low power, high throughput, low latency and minimum the storage requirements. All proposed systems implement using MATLAB R2020b."
pub.1145852550,Hybrid Parallel-in-Time-and-Space Transient Stability Simulation of Large-Scale AC/DC Grids,"The increasing complexity of modern AC/DC power systems poses a significant challenge to a fast solution of large-scale transient stability simulation problems. This paper proposes the hybrid parallel-in-time-and-space (PiT+PiS) transient simulation on the CPU-GPU platform to thoroughly exploit the parallelism from time and spatial perspectives, thereby fully utilizing parallel processing hardware. The respective electromechanical and electromagnetic aspects of the AC and DC grids demand a combination of transient stability (TS) simulation and electromagnetic transient (EMT) simulation to reflect both system-level and equipment-level transients. The TS simulation is performed on GPUs in the co-simulation, while the Parareal parallel-in-time (PiT) scheduling and EMT simulation are conducted on CPUs. Therefore, the heterogeneous CPU-GPU scheme can utilize asynchronous computing features to offset the data transfer latency between different processors. Higher scalability and extensibility than GPU-only dynamic parallelism design is achieved by utilizing concurrent GPU streams for coarse-grid and fine-grid computation. A synthetic AC/DC grid based on IEEE-118 Bus and CIGR DCS2 systems showed a good accuracy compared to commercial TSAT software, and a speedup of 165 is achieved with 48 IEEE-118 Bus systems and 192 201-Level detail-modeled MMCs. Furthermore, the proposed method is also applicable to multi-GPU implementation where it demonstrates decent efficacy."
pub.1164387160,Hybrid Parallel-in-Time-and-Space Transient Stability Simulation of Large-Scale AC/DC Grids,"The increasing complexity of modern AC/DC power systems poses a significant challenge to a fast solution of largescale transient stability simulation problems. This paper proposes the hybrid parallel-in-time-and-space (PiT + PiS) transient simulation on the CPU-GPU platform to thoroughly exploit the parallelism from time and spatial perspectives, thereby fully utilizing parallel processing hardware. The respective electromechanical and electromagnetic aspects of the AC and DC grids demand a combination of transient stability (TS) simulation and electromagnetic transient (EMT) simulation to reflect both system-level and equipment-level transients. The TS simulation is performed on GPUs in the co-simulation, while the Parareal parallel-in-time (PiT) scheduling and EMT simulation are conducted on CPUs. Therefore, the heterogeneous CPU-GPU scheme can utilize asynchronous computing features to offset the data transfer latency between different processors. Higher scalability and extensibility than GPU-only dynamic parallelism design is achieved by utilizing concurrent GPU streams for coarse-grid and fine-grid computation. A synthetic AC/DC grid based on IEEE-118 Bus and CIGRÉ DCS2 systems showed a good accuracy compared to commercial TSAT software, and a speedup of 165 is achieved with 48 IEEE-118 Bus systems and 192 201-Level detail-modeled MMCs. Furthermore, the proposed method is also applicable to multi-GPU implementation where it demonstrates decent efficacy."
pub.1156415585,Design of Smart Antenna for 5G Network Using Array Synthesis Methods and Leaky LMS Algorithm,"Abstract5G antenna arrays are being designed to generate a dedicated stream of data for every single user. This results in more capacity and speed over the network along with the least possible latency rate. Depending upon the direction of the user demanding internet access, the beam can be steered in that direction. Meanwhile, the occurrence of side lobes in such systems can be menacing. The smart antenna is a pivotal technology for modern cellular communication. It helps the user’s signal to determine the direction of arrival It also estimates antenna arrays using an adaptive signal processing algorithm that produces a radiation beam for communication. This work presents beamforming for uniform linear smart antenna array using leaky least mean square algorithm and side lobe level reduction using array synthesis methods like Tchebycheff and Taylor distribution. Multiple interferers are considered for the smart antenna. Here, one of the aims is to reduce side lobe levels nearer to the main beam which can enhance more efficient frequency reuse in a cellular network. Side lobe level is reduced up to about 16 dB that enacted for signal-to-noise ratio to 20 dB."
pub.1094896585,Materialized View Selection in Feed Following Systems,"Recently emerging feed-following applications generate personalized event streams from various feeds and deliver them to a large number of users. To provide a low-latency service, a feed-following system has to buffer the events in a number of tables, called materialized views, and choosing views to materialize is critical to the system performance. State-of-the-art solutions only consider view selections for each individual user. Due to the existence of very popular feeds and social communities, users often share a lot of feeds that they follow and hence performing a global optimization by considering all the users can significantly enhance the system performance. However, performing such a global optimization needs to choose views for materialization from an exponential number of possible ones. To solve the issue, we propose an effective method to generate candidate views that are potentially beneficial. We then propose several cost-based algorithms to solve the global view selection problem, which adopt a cost model that captures the cost of both user query processing and view maintenance and make use of the containment relationships among the sets of feeds followed by the individual users. We implement the complete approach in a prototype system and perform experiments on a computing cluster using both real and synthetic data. The results indicate that our approach outperforms the state-of-the-art approaches significantly."
pub.1141951244,PointAcc: Efficient Point Cloud Accelerator,"Deep learning on point clouds plays a vital role in a wide range of applications such as autonomous driving and AR/VR. These applications interact with people in real time on edge devices and thus require low latency and low energy. Compared to projecting the point cloud to 2D space, directly processing 3D point cloud yields higher accuracy and lower #MACs. However, the extremely sparse nature of point cloud poses challenges to hardware acceleration. For example, we need to explicitly determine the nonzero outputs and search for the nonzero neighbors (mapping operation), which is unsupported in existing accelerators. Furthermore, explicit gather and scatter of sparse features are required, resulting in large data movement overhead. In this paper, we comprehensively analyze the performance bottleneck of modern point cloud networks on CPU/GPU/TPU. To address the challenges, we then present PointAcc, a novel point cloud deep learning accelerator. PointAcc maps diverse mapping operations onto one versatile ranking-based kernel, streams the sparse computation with configurable caching, and temporally fuses consecutive dense layers to reduce the memory footprint. Evaluated on 8 point cloud models across 4 applications, PointAcc achieves 3.7 × speedup and 22 × energy savings over RTX 2080Ti GPU. Co-designed with light-weight neural networks, PointAcc rivals the prior accelerator Mesorasi by 100 × speedup with 9.1% higher accuracy running segmentation on the S3DIS dataset. PointAcc paves the way for efficient point cloud recognition."
pub.1000893698,Load-sensitive CPU Power Management for Web Search Engines,"Web search engine companies require power-hungry data centers with thousands of servers to efficiently perform searches on a large scale. This permits the search engines to serve high arrival rates of user queries with low latency, but poses economical and environmental concerns due to the power consumption of the servers. Existing power saving techniques sacrifice the raw performance of a server for reduced power absorption, by scaling the frequency of the server's CPU according to its utilization. For instance, current Linux kernels include frequency governors i.e., mechanisms designed to dynamically throttle the CPU operational frequency. However, such general-domain techniques work at the operating system level and have no knowledge about the querying operations of the server. In this work, we propose to delegate CPU power management to search engine-specific governors. These can leverage knowledge coming from the querying operations, such as the query server utilization and load. By exploiting such additional knowledge, we can appropriately throttle the CPU frequency thereby reducing the query server power consumption. Experiments are conducted upon the TREC ClueWeb09 corpus and the query stream from the MSN 2006 query log. Results show that we can reduce up to ~24% a server power consumption, with only limited drawbacks in effectiveness w.r.t. a system running at maximum CPU frequency to promote query processing quality."
pub.1147864267,hARMS: A Hardware Acceleration Architecture for Real-Time Event-Based Optical Flow,"Event-based vision sensors produce asynchronous event streams with high temporal resolution based on changes in the visual scene. The properties of these sensors allow for accurate and fast calculation of optical flow as events are generated. Existing solutions for calculating optical flow from event data either fail to capture the true direction of motion due to the aperture problem, do not use the high temporal resolution of the sensor, or are too computationally expensive to be run in real time on embedded platforms. In this research, we first present a faster version of our previous algorithm, ARMS (Aperture Robust Multi-Scale flow). The new optimized software version (fARMS) significantly improves throughput on a traditional CPU. Further, we present hARMS, a hardware realization of the fARMS algorithm allowing for real-time computation of true flow on low-power, embedded platforms. The proposed hARMS architecture targets hybrid system-on-chip devices and was designed to maximize configurability and throughput. The hardware architecture and fARMS algorithm were developed with asynchronous neuromorphic processing in mind, abandoning the common use of an event frame and instead operating using only a small history of relevant events, allowing latency to scale independently of the sensor resolution. This change in processing paradigm improved the estimation of flow directions by up to 73% compared to the existing method and yielded a demonstrated hARMS throughput of up to 1.21 Mevent/s on the benchmark configuration selected. This throughput enables real-time performance and makes it the fastest known realization of aperture-robust, event-based optical flow to date."
pub.1095733105,Optical interconnect and memory components for disaggregated computing,"High-performance server boards rely on multi-socket architectures for increasing the processing power density on the board level and for flattening the data center networks beyond leaf-spine architectures. Scaling, however, the number of processors per board and retaining at the same time low-latency and high-throughput metrics puts current electronic technologies into challenge. In this article, we report on our recent work carried out in the H2020 projects ICT-STREAMS and dREDBox that promotes the use of Silicon Photonic transceiver and routing modules in a powerful board-level, chip-to-chip interconnect paradigm. The proposed on-board platform leverages WDM parallel transmission with a powerful wavelength routing approach that is capable of interconnecting multiple processors with up to 25.6 Tbps on-board throughput, providing direct and collision-less any-to-any communication between multiple compute and memory sockets at low-energy 50 Gbps OOK line-rates. We demonstrate recent advances on the Si-based WDM transceiver, cyclic AWGR router and polymer-based electro-optical circuit board key-enabling technologies, discussing also potential applications in disaggregated rack-scale architectures. We also demonstrate our recent research on optical RAM technologies and optical cache memory architectures that can take advantage of the on-board interconnect technology for yielding true disintegrated computing resolving both power and memory bandwidth bottlenecks of current computational settings."
pub.1141915027,PointAcc: Efficient Point Cloud Accelerator,"Deep learning on point clouds plays a vital role in a wide range of
applications such as autonomous driving and AR/VR. These applications interact
with people in real-time on edge devices and thus require low latency and low
energy. Compared to projecting the point cloud to 2D space, directly processing
the 3D point cloud yields higher accuracy and lower #MACs. However, the
extremely sparse nature of point cloud poses challenges to hardware
acceleration. For example, we need to explicitly determine the nonzero outputs
and search for the nonzero neighbors (mapping operation), which is unsupported
in existing accelerators. Furthermore, explicit gather and scatter of sparse
features are required, resulting in large data movement overhead. In this
paper, we comprehensively analyze the performance bottleneck of modern point
cloud networks on CPU/GPU/TPU. To address the challenges, we then present
PointAcc, a novel point cloud deep learning accelerator. PointAcc maps diverse
mapping operations onto one versatile ranking-based kernel, streams the sparse
computation with configurable caching, and temporally fuses consecutive dense
layers to reduce the memory footprint. Evaluated on 8 point cloud models across
4 applications, PointAcc achieves 3.7X speedup and 22X energy savings over RTX
2080Ti GPU. Co-designed with light-weight neural networks, PointAcc rivals the
prior accelerator Mesorasi by 100X speedup with 9.1% higher accuracy running
segmentation on the S3DIS dataset. PointAcc paves the way for efficient point
cloud recognition."
pub.1095101508,Optimal Multicast in Virtualized Datacenter Networks with Software Switches,"Virtualized datacenter networks have been deployed in production platforms, e.g., Amazon VPC and VMware's NVP, to offer the flexibility of network management to enterprise-level clients. A common characteristic of these platforms is that they adopt software switches, such as Open vSwitch (OvS), instead of hardware switches to transfer data between VMs. Although group communication is common in enterprise applications, the unique characteristics of software switches have posed new challenges to the design of multicast protocols. How logical multicast can be optimally performed with software switches is still not well understood. In this paper, we observe that unlike hardware switches, the per-stream output rate in a software switch critically depends on the packet processing overhead of flow cloning. We study the optimal OvS multicast topology with or without the help of additional dedicated software switches called service nodes, and formulate the throughput maximization as a new class of degree-supervised combinatorial graph problems due to the presence of flow cloning costs. We propose a linear-time optimal solution that translates into simple forwarding rules installed at each software switch. Through emulation-based OvS profiling and extensive simulation results, we demonstrate that our proposed logical multicast solutions can significantly improve session throughput with the ability to handle load balancing and latency issues, as compared to the state-of-the-art in the literature."
pub.1135339109,Autonomous long-range drone detection system for critical infrastructure safety,"The development of unmanned aerial vehicles has been identified as a potential source of a weapon for causing operational disruptions against critical infrastructures. To mitigate and neutralise the threat posed by the misuse of drones against malicious and terrorist activity, this paper presents a holistic design of a long-range autonomous drone detection platform. The novelty of the proposed system lies in the confluence between the design of hardware and software components to effective and efficient localisation of the intruder objects. The research presented in the paper proposes the design and validation of a situation awareness component which is interfaced with the hardware component for controlling the focal length of the camera. The continuous stream of media data obtained from the region of vulnerability is processed using the object detection that is built on region based fully connected neural network. The novelty of the proposed system relies on the processing of multi-threaded dual-media input streams that are evaluated to mitigate the latency of the system. Upon the successful detection of malicious drones, the system logs the occurrence of intruders that consists of both event description and the associated media evidence for the deployment of the mitigation strategy. The analytics platform that controls the signalling of the low-cost sensing equipment contains the NVIDIA GeForce GTX 1080 for detecting drones. The experimental testbeds developed for the validation of the proposed system has been constructed to include environments and situations that are commonly faced by critical infrastructure operators such as the area of protection, drone flight path, tradeoff between the angle of coverage against the distance of coverage. The validation of the proposed system has resulted in yielding a range of intruder drone detection by 250m with an accuracy of 95.5%."
pub.1167768095,On-Device Signal Quality Guided and Embedded Physiologic Information for High Fidelity Continuous PPG Compression,"Photoplethysmography (PPG) is widely popularized in wearable gadgets for continuous health monitoring. The main challenge before the PPG-based measurements is motion artifact (MA) corruption, which often suppress vital physiological information like heart rate (HR) and respiratory rate (RR), which makes their extraction difficult and sometime erroneous. This work introduces a new approach of PPG multiclass signal quality assessment (SQA) using reservoir computing (RC) and tunable ${Q}$ -factor wavelet transform (TQWT) for PPG compression. To ensure the post compression clinical usage, HR and RR are embedded in the compressed bit stream, at every 10 s PPG frame. For the evaluation, a total of 200 records were utilized from three public datasets, namely, MIMIC II/III waveform, PPG RR benchmark (PRRB), BIDMC, and 20 volunteers at our laboratory, spanning a total duration of 7740 min. The technique showed good on-device (ARM v6 controller) performance on SQA (accuracy of 99.97%) and PPG compression (CR of 38.28 and percentage root mean squared difference (PRD) of 3.04 at 125 Hz sampling) using MIMIC data, which are competitive with published works. The low end-to-end latency (51 ms) and run time memory (19.99 kB) per 1 s PPG shows the usefulness for real-time deployment. It was also showed that embedding the physiologic information in compressed data may reduce the post compression processing error in RR and HR by an amount of 5.61% and 31.23% respectively in corrupted PPG. The proposed technique can be utilized in high fidelity PPG monitoring applications under ambulatory measurement."
pub.1164572888,A Sparse Local Binary Pattern extraction algorithm applied to event sensor data for object classification,"Recently, new sensors with active pixels were brought to market. These sensors export local variations of light intensity in the form of asynchronous events with low latency. Since the data output format is a stream of addressable events and not a complete image of light intensities, new algorithms are required for known problems in the field of Computer Vision, such as segmentation, VO, SLAM, object, and scene recognition. There are some proposed methodologies for object recognition using conventional methods, convolutional neural networks, and third-generation neural networks based on spikes. However, convolutional neural networks and spike neural networks require specific hardware for processing, hard to miniaturize. Also, several traditional Computer Vision operators and feature descriptors have been neglected in the context of event sensors and could contribute to lighter methodologies in object recognition. This paper proposes an algorithm for local binary pattern extraction in sparse structures, typically found in this context. This paper also proposes two methodologies using local binary patterns to captures with event-based sensors for object recognition. The first methodology exploits the known motion performed by the sensor, while the second is motion agnostic. It is demonstrated experimentally that the LBP operator is a fast and light alternative that enables variable reduction using PCA in some cases. The experiments also show that it is possible to reduce the final feature vector for classification by up to 99 , 73 % when compared to conventional methods considered state-of-the-art while maintaining comparable accuracy."
pub.1127256678,Full-field structural monitoring using event cameras and physics-informed sparse identification," This paper exploits a new direction of full-field structural monitoring and vibration analysis, using an emerging type of neuro-inspired vision sensors – namely event cameras. Compared to traditional frame-based cameras, event cameras offer salient benefits of resilience to motion blur, high dynamic range, and microsecond latency. Event cameras are herein exploited for structural monitoring, in order to extract dense measurements of structural response in terms of both spatial and temporal resolution. The output of an event camera is a stream of so called “events”, which is different to traditional snapshots. Due to this fundamentally different working principle, basic computer vision algorithms, such as optical flow or feature tracking, should be re-designed for processing event-based measurements. In this work, we present a novel framework termed physics-informed sparse identification, for full-field structural vibration tracking and analysis. The framework leverages sparse identification guided by assimilation of the underlying structural dynamics in the assembly of a library matrix, which is used to characterize the system’s dynamics. The stream of event data generated from event cameras is sparsely represented by means of well-chosen basis functions, allowing for a physical interpretation of the system’s response. The proposed framework is extended to boundary condition learning/classification by fusion of characteristic basis functions, representing different classes of support conditions, into the library matrix. The results obtained by means of an illustrative numerical example, as well as experimental tests on vibrating beams recorded by an event camera demonstrate an accurate tracking of structural vibration and the developed strains, in the form of full-field measurements rather than point-wise tracking. What is more, the proposed sparse learning process enables identification of the boundary conditions of monitored structural elements, which comes with key benefits for structural monitoring."
pub.1175096866,Distributed Threat Intelligence at the Edge Devices: A Large Language Model-Driven Approach,"With the proliferation of edge devices, there is a significant increase in attack surface on these devices. The decen-tralized deployment of threat intelligence on edge devices, coupled with adaptive machine learning techniques such as the in-context learning feature of Large Language Models (LLMs), represents a promising paradigm for enhancing cybersecurity on resource-constrained edge devices. This approach involves the deployment of lightweight machine learning models directly onto edge devices to analyze local data streams, such as network traffic and system logs, in real-time. Additionally, distributing computational tasks to an edge server reduces latency and improves responsiveness while also enhancing privacy by processing sensitive data locally. LLM servers can enable these edge servers to autonomously adapt to evolving threats and attack patterns, continuously updating their models to improve detection accuracy and reduce false positives. Furthermore, collaborative learning mechanisms facilitate peer-to-peer secure and trustworthy knowledge sharing among edge devices, enhancing the collective intelligence of the network and enabling dynamic threat mitigation measures such as device quarantine in response to detected anomalies. The scalability and flexibility of this approach make it well-suited for diverse and evolving network environments, as edge devices only send suspicious information such as network traffic and system log changes, offering a resilient and efficient solution to combat emerging cyber threats at the network edge. Thus, our proposed framework can improve edge computing security by providing better security in cyber threat detection and mitigation by isolating the edge devices from the network."
pub.1171600643,Hype or Heuristic? Quantum Reinforcement Learning for Join Order Optimisation,"Identifying optimal join orders (JOs) stands out as a key challenge in
database research and engineering. Owing to the large search space, established
classical methods rely on approximations and heuristics. Recent efforts have
successfully explored reinforcement learning (RL) for JO. Likewise, quantum
versions of RL have received considerable scientific attention. Yet, it is an
open question if they can achieve sustainable, overall practical advantages
with improved quantum processors.
  In this paper, we present a novel approach that uses quantum reinforcement
learning (QRL) for JO based on a hybrid variational quantum ansatz. It is able
to handle general bushy join trees instead of resorting to simpler left-deep
variants as compared to approaches based on quantum(-inspired) optimisation,
yet requires multiple orders of magnitudes fewer qubits, which is a scarce
resource even for post-NISQ systems.
  Despite moderate circuit depth, the ansatz exceeds current NISQ capabilities,
which requires an evaluation by numerical simulations. While QRL may not
significantly outperform classical approaches in solving the JO problem with
respect to result quality (albeit we see parity), we find a drastic reduction
in required trainable parameters. This benefits practically relevant aspects
ranging from shorter training times compared to classical RL, less involved
classical optimisation passes, or better use of available training data, and
fits data-stream and low-latency processing scenarios. Our comprehensive
evaluation and careful discussion delivers a balanced perspective on possible
practical quantum advantage, provides insights for future systemic approaches,
and allows for quantitatively assessing trade-offs of quantum approaches for
one of the most crucial problems of database management systems."
pub.1143789166,"A 389TOPS/W, 1262fps at 1Meps Region Proposal Integrated Circuit for Neuromorphic Vision Sensors in 65nm CMOS","Neuromorphic vision sensors (NVS) [1] are key enablers in traffic monitoring and surveillance systems that exploit the temporal redundancy in video streams to get $\gt 2\mathrm{X}$ energy savings by blank frame detection (Fig. 1). Such concept of event driven processing has been used to reduce system energy for regular cameras as well [2]. However, an object typically occupies a fraction of the full image frame (Fig. 1) leading to a significant spatial redundancy in the image. Hence, an energy-efficient hardware is required to detect the region of interests (RoIs) in the valid frames to trigger an object recognition engine only for the RoIs. For a binary image, the region proposal can be performed by the connected component labeling (CCL) algorithm [2]. However, CCL scans the image in a raster fashion to calculate the ROIs leading to longer execution time and higher energy dissipation due to enormous data transfer. On the contrary, emerging in-memory [3], [4] and near-memory [5] computing approaches are a way to eliminate the data transfer cost and latency, promising further energy savings. In this paper, we propose 9T-SRAM based near and in-memory computing region proposal integrated circuit (RPIC) leveraging the $1 -\mathrm{D}$ projections of the objects on the vertical and horizontal axes. Further, we propose an iterative and selective search (ISS) algorithm to overcome overlapped projections among objects and provide an accurate number of objects and their exacts coordinates."
pub.1143272501,Remote Run-Time Failure Detection and Recovery Control For Quadcopters,"We propose an adaptive run-time failure recovery control system for quadcopter drones, based on remote real-time processing of measurement data streams. Particularly, the measured RPM values of the quadcopter motors are transmitted to a remote machine which hosts failure detection algorithms and performs recovery procedure. The proposed control system consists of three distinct parts: (1) A set of computationally simple PID controllers locally onboard the drone, (2) a set of computationally more demanding remotely hosted algorithms for real-time drone state detection, and (3) a digital twin co-execution software platform — the ModelConductor-eXtended — for two-way signal data exchange between the former two. The local on-board control system is responsible for maneuvering the drone in all conditions: path tracking under normal operation and safe landing in a failure state. The remote control system, on the other hand, is responsible for detecting the state of the drone and communicating the corresponding control commands and controller parameters to the drone in real time. The proposed control system concept is demonstrated via simulations in which a drone is represented by the widely studied Quad-Sim six degrees-of-freedom Simulink model. Results show that the trained failure detection binary classifier achieves a high level of performance with F1-score of 96.03%. Additionally, time analysis shows that the proposed remote control system, with average execution time of 0.49 milliseconds and total latency of 6.92 milliseconds in two-way data communication link, meets the real-time constraints of the problem. The potential practical applications for the presented approach are in drone operation in complex environments such as factories (indoor) or forests (outdoor)."
pub.1148320002,Remote Run-Time Failure Detection and Recovery Control For Quadcopters,"We propose an adaptive run-time failure recovery control system for quadcopter drones, based on remote real-time processing of measurement data streams. Particularly, the measured RPM values of the quadcopter motors are transmitted to a remote machine which hosts failure detection algorithms and performs recovery procedure. The proposed control system consists of three distinct parts: (1) A set of computationally simple PID controllers locally onboard the drone, (2) a set of computationally more demanding remotely hosted algorithms for real-time drone state detection, and (3) a digital twin co-execution software platform — the ModelConductor-eXtended — for two-way signal data exchange between the former two. The local on-board control system is responsible for maneuvering the drone in all conditions: path tracking under normal operation and safe landing in a failure state. The remote control system, on the other hand, is responsible for detecting the state of the drone and communicating the corresponding control commands and controller parameters to the drone in real time. The proposed control system concept is demonstrated via simulations in which a drone is represented by the widely studied Quad-Sim six degrees-of-freedom Simulink model. Results show that the trained failure detection binary classifier achieves a high level of performance with F1-score of 96.03%. Additionally, time analysis shows that the proposed remote control system, with average execution time of 0.49 milliseconds and total latency of 6.92 milliseconds in two-way data communication link, meets the real-time constraints of the problem. The potential practical applications for the presented approach are in drone operation in complex environments such as factories (indoor) or forests (outdoor)."
pub.1171634639,Distributed Threat Intelligence at the Edge Devices: A Large Language Model-Driven Approach,"With the proliferation of edge devices, there is a significant increase in
attack surface on these devices. The decentralized deployment of threat
intelligence on edge devices, coupled with adaptive machine learning techniques
such as the in-context learning feature of Large Language Models (LLMs),
represents a promising paradigm for enhancing cybersecurity on
resource-constrained edge devices. This approach involves the deployment of
lightweight machine learning models directly onto edge devices to analyze local
data streams, such as network traffic and system logs, in real-time.
Additionally, distributing computational tasks to an edge server reduces
latency and improves responsiveness while also enhancing privacy by processing
sensitive data locally. LLM servers can enable these edge servers to
autonomously adapt to evolving threats and attack patterns, continuously
updating their models to improve detection accuracy and reduce false positives.
Furthermore, collaborative learning mechanisms facilitate peer-to-peer secure
and trustworthy knowledge sharing among edge devices, enhancing the collective
intelligence of the network and enabling dynamic threat mitigation measures
such as device quarantine in response to detected anomalies. The scalability
and flexibility of this approach make it well-suited for diverse and evolving
network environments, as edge devices only send suspicious information such as
network traffic and system log changes, offering a resilient and efficient
solution to combat emerging cyber threats at the network edge. Thus, our
proposed framework can improve edge computing security by providing better
security in cyber threat detection and mitigation by isolating the edge devices
from the network."
pub.1181666972,PLFNets: Interpretable Complex-Valued Parameterized Learnable Filters for Computationally Efficient RF Classification,"Radio frequency (RF) sensing applications such as RF waveform classification and human activity recognition (HAR) demand real-time processing capabilities. Current state-of-the-art techniques often require a two-stage process for classification: first, computing a time-frequency (TF) transform, and then applying machine learning (ML) using the TF domain as the input for classification. This process hinders the opportunities for real-time classification. Consequently, there is a growing interest in direct classification from raw IQ-RF data streams. Applying existing deep learning (DL) techniques directly to the raw IQ radar data has shown limited accuracy for various applications. To address this, this article proposes to learn the parameters of structured functions as filterbanks within complex-valued (CV) neural network architectures. The initial layer of the proposed architecture features CV parameterized learnable filters (PLFs) that directly work on the raw data and generate frequency-related features based on the structured function of the filter. This work presents four different PLFs: Sinc, Gaussian, Gammatone, and Ricker functions, which demonstrate different types of frequency-domain bandpass filtering to show their effectiveness in RF data classification directly from raw IQ radar data. Learning structured filters also enhances interpretability and understanding of the network. The proposed approach was tested on both experimental and synthetic datasets for sign and modulation recognition. The PLF-based models achieved an average of 47% improvement in classification accuracy compared with a 1-D convolutional neural network (CNN) on raw RF data and an average 7% improvement over CNNs with real-valued learnable filters for the experimental dataset. It also matched the accuracy of a 2-D CNN applied to micro-Doppler ( $\mu $ D) spectrograms while reducing computational latency by around 75%. These results demonstrate the potential of the proposed model for a range of RF sensing applications with enhanced accuracy and computational efficiency."
pub.1094943267,Global Precise Multi-GNSS Positioning with Trimble CenterPoint RTX,"Mid of 2011 Trimble introduced the CenterPoint RTX real-time positioning service providing cm-accurate positions for real-time applications. This service targets applications in the precision markets like Precision Agriculture, Survey, Construction and relies on the generation of precise orbit and clock information for GNSS satellites in real-time. The CenterPoint RTX satellite corrections are generated with data from Trimble's world-wide tracking network, consisting of approximately 100 reference stations globally distributed. While the system initially was introduced supporting GPS and GLONASS satellites, recent developments have led to the inclusion of additional navigation satellites. The orbit estimation in the CenterPoint RTX system is based on a combination of a UD-factorized Kalman filter estimating satellite position, satellite velocity, troposphere states, integer ambiguities, solar radiation pressure parameters, harmonic coefficients, and earth orientation parameters. The prediction step in the filter is using a numerical integration of the equations of motion in connection with a dynamic force modeling. Forces considered in the approach are the earth's gravity field, lunar and solar direct tides, solar radiation pressure, solid earth tides, ocean tides, and general relativity. In the RTX orbit processing carrier phase integer ambiguities are resolved in real-time. Also, the satellite orbit states are truly estimated in real-time and continuously adapted over time to better represent the current reality. This means that the satellite positions that are evaluated by the user have prediction times of no more than a few minutes since the last orbit processing filtering update, providing negligible loss of accuracy. The RTX real-time orbit components have a typical overall accuracy of around 2.5 cm considering IGS rapid products as truth. Satellite clock estimation is an essential part of the CenterPoint RTX system. It plays a fundamental role on positioning performance due to a number of reasons. Satellite clocks map directly into line-of-sight observation modeling, yielding into a one to one error impact from clocks into GNSS observables modeling. Due to the same strong relationship, it is of fundamental importance that clocks are generated in a way to facilitate ambiguity resolution within the positioning engine. The processing speed of a clock processor is also of fundamental importance, due to the fact that any delay in computing satellite clocks is directly translated into correction latencies when computing real-time positions on the rover side. For that matter one should keep in mind that regardless how late satellite corrections get to the GNSS receiver in the field, positions have to be provided to the user as soon as the rover GNSS measurements are available. Therefore latencies typically introduce errors into the final real time position. In this paper we define real-time positioning as the computation of positions at the time "
pub.1166471914,LEOD: Label-Efficient Object Detection for Event Cameras,"Object detection with event cameras benefits from the sensor's low latency
and high dynamic range. However, it is costly to fully label event streams for
supervised training due to their high temporal resolution. To reduce this cost,
we present LEOD, the first method for label-efficient event-based detection.
Our approach unifies weakly- and semi-supervised object detection with a
self-training mechanism. We first utilize a detector pre-trained on limited
labels to produce pseudo ground truth on unlabeled events. Then, the detector
is re-trained with both real and generated labels. Leveraging the temporal
consistency of events, we run bi-directional inference and apply tracking-based
post-processing to enhance the quality of pseudo labels. To stabilize training
against label noise, we further design a soft anchor assignment strategy. We
introduce new experimental protocols to evaluate the task of label-efficient
event-based detection on Gen1 and 1Mpx datasets. LEOD consistently outperforms
supervised baselines across various labeling ratios. For example, on Gen1, it
improves mAP by 8.6% and 7.8% for RVT-S trained with 1% and 2% labels. On 1Mpx,
RVT-S with 10% labels even surpasses its fully-supervised counterpart using
100% labels. LEOD maintains its effectiveness even when all labeled data are
available, reaching new state-of-the-art results. Finally, we show that our
method readily scales to improve larger detectors as well. Code is released at
https://github.com/Wuziyi616/LEOD"
pub.1161672080,Selective attention to audiovisual speech routes activity through recurrent feedback-feedforward loops between different nodes of the speech network,"Abstract Selective attention related top-down modulation plays a significant role in separating relevant speech from irrelevant background speech when vocal attributes separating concurrent speakers are small and continuously evolving. Electrophysiological studies have shown that such top-down modulation enhances neural tracking of attended speech. Yet, the specific cortical regions involved remain unclear due to the limited spatial resolution of most electrophysiological techniques. To overcome such limitations, we collected both EEG (high temporal resolution) and fMRI (high spatial resolution), while human participants selectively attended to speakers in audiovisual scenes containing overlapping cocktail party speech. To utilize the advantages of the respective techniques, we analysed neural tracking of speech using the EEG data and performed representational dissimilarity-based EEG-fMRI fusion. We observed that attention enhanced neural tracking and modulated EEG correlates throughout the latencies studied. Further, attention related enhancement of neural tracking fluctuated in predictable temporal profiles. We discuss how such temporal dynamics could arise from a combination of interactions between attention and prediction as well as plastic properties of the auditory cortex. EEG-fMRI fusion revealed attention related iterative feedforward-feedback loops between hierarchically organised nodes of the ventral auditory object related processing stream. Our findings support models where attention facilitates dynamic neural changes in the auditory cortex, ultimately aiding discrimination of relevant sounds from irrelevant ones while conserving neural resources."
pub.1126752143,Neuromorphic Eye-in-Hand Visual Servoing,"Robotic vision plays a major role in factory automation to service robot
applications. However, the traditional use of frame-based camera sets a
limitation on continuous visual feedback due to their low sampling rate and
redundant data in real-time image processing, especially in the case of
high-speed tasks. Event cameras give human-like vision capabilities such as
observing the dynamic changes asynchronously at a high temporal resolution
($1\mu s$) with low latency and wide dynamic range.
  In this paper, we present a visual servoing method using an event camera and
a switching control strategy to explore, reach and grasp to achieve a
manipulation task. We devise three surface layers of active events to directly
process stream of events from relative motion. A purely event based approach is
adopted to extract corner features, localize them robustly using heat maps and
generate virtual features for tracking and alignment. Based on the visual
feedback, the motion of the robot is controlled to make the temporal upcoming
event features converge to the desired event in spatio-temporal space. The
controller switches its strategy based on the sequence of operation to
establish a stable grasp. The event based visual servoing (EVBS) method is
validated experimentally using a commercial robot manipulator in an eye-in-hand
configuration. Experiments prove the effectiveness of the EBVS method to track
and grasp objects of different shapes without the need for re-tuning."
pub.1175646536,IOT in Communication Technologies,"With the help of IoT (Internet of Things) communication technologies, a variety of devices and objects can connect to one another and communicate, building a network of intelligent systems. Real-time process monitoring, control, and automation are made possible by these technologies, which make it easier for data and information to be transferred between equipment. IoT devices link to each other and share data using wireless communication protocols as Wi-Fi, Bluetooth, Zigbee, and cellular networks (2G, 3G, 4G, and 5G). These wireless technologies offer IoT installations across many environments flexibility, mobility, and scalability. IoT uses sensor networks to gather information from the real world. Devices contain embedded sensors that can measure and detect a variety of temperature, humidity, pressure, motion, and other environmental conditions. The information is transferred to one or more clouds. IoT systems commonly use cloud computing platforms to store, analyze, and analyze the large amount of data generated by connected devices. Cloud-based solutions offer the scalability, data storage, and computational power necessary for IoT applications, enabling real-time insights and wise decision-making. IoT devices are quickly utilizing edge computing capabilities to get past bandwidth limitations, latency, and privacy concerns. Edge devices like gateways and edge servers reduce reliance on cloud infrastructure by processing and analyzing data locally. Edge computing enables quicker response times, data filtering, and offline abilities. In order to ensure compatibility and simple connection across Internet of Things (IoT) networks and devices, numerous communication protocols and standards have been created. Examples include HTTP (Hypertext Transfer Protocol), CoAP (Constrained Application Protocol), and MQTT (Message Queuing Telemetry Transport). IoT communication systems consider important privacy and security considerations. Secure communication protocols, authentication methods, and encryption mechanisms safeguard data exchanged between devices. To address privacy issues, data anonymization, consent management, and adherence to privacy regulations are used. Analytics and artificial intelligence (AI) technologies combine with IoT connection technologies. Machine learning algorithms examine IoT data streams to produce insightful findings, identify patterns, and provide predictive capabilities. AI-driven insights enable intelligent automation while also enhancing operational efficiency and resource use."
pub.1146355148,Microservices for multimedia,"Netflix has been one of the pioneers that has driven the industry adoption of a new paradigm of system architecture referred to as ""microservices"". Microservices, or more accurately, microservice architecture refers to an architecture where applications are modeled as a collection of services which are: highly maintainable and independently testable, loosely coupled, independently deployable and organized around business capabilities. Typically, each microservice is owned by a small team of developers that is responsible for its development, testing and deployment, i.e., its end-to-end lifecycle. Traditional microservices such as those used outside of multimedia processing at Netflix typically consist of an API with stateless business logic which is autoscaled based on request load. These APIs provide strong contracts and separate the application data and binary dependencies from systems. As useful as traditional microservices are, several peculiarities of multimedia applications render such stateless services non ideal for media processing. Specifically, media processing (which includes video/audio processing, media encoding, timed-text processing, computer vision analysis etc.) relies on data that is embedded in files where the files themselves are contracts as opposed to fully visible data models that are common in non-media applications. At Netflix, media processing is resource intensive and bursty in nature. It is also highly parallelizable and re-triable, and so, even though work is generally a continuous stream with deadlines and priorities, the system can balance resources by evicting jobs as needed which can be retried at a later time. In this talk, we will summarize Cosmos, a project that we've developed in order to enable workflow-driven media processing using a microservice architecture. Cosmos is a computing platform that combines the best aspects of microservices with asynchronous workflows and serverless functions. It is designed specifically for resource intensive algorithms which are coordinated via complex hierarchical workflows. Cosmos supports both high throughput and low-latency workloads. The Cosmos platform offers: observability through built in logging, tracing, monitoring, alerting and error classification; modularity (both compile-time and run-time) through an opinionated framework for structuring a service; productivity through tooling such as code generators, containers, and command line interfaces; and delivery through a managed continuous-delivery pipelines. The Cosmos platform allows media developers to build and run domain-specific, scale-agnostic components which are built atop three scale-aware subsystems that handle distributing the work. Each component can thus be independently developed, tested and deployed with clear abstraction from the underlying platform thereby providing a logical separation between the application and platform so that the details of distributed computing are hidden from media developers "
pub.1174143896,An insect vision-inspired neuromorphic vision systems in low-light obstacle avoidance for intelligent vehicles,"The Lobular Giant Motion Detector (LGMD) is a neuron in the insect visual system that has been extensively studied, especially in locusts. This neuron is highly sensitive to rapidly approaching objects, allowing insects to react quickly to avoid potential threats such as approaching predators or obstacles. In the realm of intelligent vehicles, due to the lack of performance of conventional RGB cameras in extreme light conditions or at high-speed movements. Inspired by biological mechanisms, we have developed a novel neuromorphic dynamic vision sensor (DVS) driven LGMD spiking neural network (SNN) model. SNNs, distinguished by their bio-inspired spiking dynamics, offer a unique advantage in processing time-varying visual data, particularly in scenarios where rapid response and energy efficiency are paramount. Our model incorporates two distinct types of Leaky Integrate-and-Fire (LIF) neuron models and synapse models, which have been instrumental in reducing network latency and enhancing the system’s reaction speed. And addressing the challenge of noise in event streams, we have implemented denoising techniques to ensure the integrity of the input data. Integrating the proposed methods, ultimately, the model was integrated into an intelligent vehicle to conduct real-time obstacle avoidance testing in response to looming objects in simulated real scenarios. The experimental results show that the model’s ability to compensate for the limitations of traditional RGB cameras in detecting looming targets in the dark, and can detect looming targets and implement effective obstacle avoidance in complex and diverse dark environments."
pub.1172789367,Enhancing Multimedia Forwarding in Software- Defined Networks: An Optimal Flow Mechanism Approach,"Software-defined networking (SDN) has revolutionized the way in which current modern networks are constructed. SDN offers dynamic and centralized administration resources that are available for the network, but multimedia traffic still a challenge on modern communication networks and it is very important to protect network systems that can reliably and effectively send multimedia data over the network form one end to another end. The goal of this new approach, which is called the Optimal Proposed Flow Mechanism (OPFM), is to make it easier for SDN (software-defined networking) devices to send multimedia data. This method of managing flow at the D-Plane involves the enforcement of device thresholds that are determined by the packet content of messages. By implementing this approach, not only are speed of response and computational efficiency improved, but processing delays are also reduced. The subsequent emphasis is on flow classification, which involves the categorization of incoming flows into delay-sensitive and nondelay-sensitive streams. A Non-Deep Lightweight Parallel Network (ND-LPN) is used for classification in software defined network and it has parallel layers that can manage flow delay limits, device efficiency, and service categorization. In the way to prevent packet loss while load balancing, this approach prioritizes flow classification in a computing efficient and latency-free manner. This means that traffic flow is sent to controllers according to their importance. The proposed method conducted in Network Simulator 3.2 and it makes the network work better by fixing flow management problems through more efficient methods for categorizing and transferring traffic in terms of network load and packet loss rate during flow forwardoing."
pub.1141247218,[Retracted] Using Wireless Sensor Network to Correct Posture in Sports Training Based on Hidden Markov Matching Algorithm,"This paper combines the research of wireless sensor networks and sports training and proposes a wireless sensor network‐based intelligent sports training system. According to the requirements of the system, this design uses the wireless sensor network system as the platform for development and the ZigBee module for wireless communication. The advantage of this system is to transmit the obtained information to the ZigBee coordinator module, and after the processing of information and the resultant decision, a nonwearable unmonitored motion training model based on visual sensing is proposed. The motion terminal collects video data streams of user motion actions and extracts features to establish HMM motion recognition algorithm to achieve recognition of motion actions, automatic counting, and intelligent scoring functions. The template matching algorithm based on dynamic time regularization and weighted Euclidean distance realizes a universal real‐time motion recognition algorithm with high standard and low latency and can guide the user’s motion action based on similarity calculation. The intelligent sports training system is designed and developed to maintain a high‐quality human‐computer interaction experience with a real‐time feedback client and uploads sports data to a cloud server via the HTTP protocol, which supports real‐time sports proximity query and training plan development on the website. After practical application tests, the intelligent sports training system based on the wireless sensor network proposed in this paper is stable and reliable and adds fun and competitiveness to boring sports. The research of this paper has some reference value for the application of wireless sensor networks and the research of the motion recognition algorithm."
pub.1171650522,Cybersecurity Issues in Space Optical Communication Networks and Future of Secure Space Health Systems,"Space is an extreme environment and as a result, unmanned and manned missions rely on the operations of the spacecraft/space habitat, and/or the health of the astronaut(s) in that extreme setting for the success of the mission and the safe return to Earth of the astronauts. Advances in Internet of Things (IoT) enabled sensor devices, Big Data pipelines and artificial intelligence (AI) present new opportunities for real-time monitoring of various spacecraft/space habitat equipment (i.e., hardware), systems (i.e., software) and the environment along with human health. While the Big Data pipeline and AI could exist solely on the spacecraft/space habitat, the provision of a communication network with suitable bandwidth, enables the transmission of raw and derived data streams for additional processing, review and research on Earth. It also provides a different layer of redundancy for onboard AI systems. Free space optical communication links (e.g., laser communication) can support greater bandwidth for exchanging space health data than classic RF communication links. Therefore, the extra provided bandwidth makes these emerging technologies excellent candidates for supporting shorter response times to urgent adverse health situations in space. The lower latency experienced in the exchange of space health data using laser transceivers can further improve the security and well-being of astronauts as well as space occupants, which are considered essential components for successful and scalable manned space missions. That transmission to Earth is at risk of cyber at-tack. In addition, to transmit data through Space, one must consider the jurisdiction and relevant bodies to oversee data traffic management in Space. In this work, we discuss the role of Space Traffic Management (STM) in improving the reliability and security of optical space communication links. Specifically, we discuss different cyber threats that adversely affect the real-time delivery of space health data that are communicated over such links. Furthermore, we propose a series of security controls that any practical Space Traffic Management scheme must take into consideration to ensure the reliability and safety of optical communicating parties/links beyond Low Earth Orbit (LEO)."
pub.1143789229,"A Single-Channel 1.75GS/s, 6-Bit Flash-Assisted SAR ADC with Self-Adaptive Timer and On-Chip Offset Calibration","As the data rates of high speed SERDES continue to evolve from tens to hundreds of Gbps, higher order modulation schemes such as pulse amplitude modulation (PAM) or quadrature amplitude modulation (QAM) are widely adopted in wireline communications to boost up the spectral efficiency. Additionally, digital signal processing (DSP) based transceivers become the main stream to cope with the channel non-idealities, such as frequency dependent channel loss, cross talk, and signal reflections. High speed ADCs running at tens of GS/s are demanding at the receiver front-end and play a key role in those applications. Among them, time interleaved successive approximation register (TI-SAR) ADCs are commonly employed thanks to its digitally intensive implementation and performance improvement along with technology scaling. Limited by the sampling rate of each sub-ADC, a large number of TI-ADC banks would complicate the clock tree distribution and timing skew calibration efforts, which would lead to a higher power consumption. [1] demonstrates a flash-assisted (FA) TI-SAR structure can be utilized to enhance the conversion speed with excellent power efficiency. In this paper, a single-channel 1.75 GS/s 6-bit SAR ADC is proposed. It is designed to support 112 Gbps (56 GBaud) PAM-4 receiver through 32X TI implementation. In the DSP based receiver, the ADC output feeds into the FFE for channel equalization. Meanwhile, the ADC output is applied to clock and data recovery (CDR) circuit to generate the global sampling clock. A long latency in the TI-SAR would degrade the CDR jitter tracking capability. To circumvent the design challenges, conventional SERDES receiver AFE requires dual feedback paths. The main loop of CDR is implemented with a short latency, while the ADC loop performs as an auxiliary path for sampling phase adjustment. As the phase detector for a PAM-4 CDR inherently consists of a 2-bit quantizer, to avoid hardware redundancy and reduce system power consumption, this paper proposes a high-speed FA-SAR ADC. In this implementation, the 2 MSBs are generated by flash operation, which can be combined with the PD in CDR to accelerate phase and frequency locking. Meanwhile, it assists the succeeding SAR operation for the remaining bits conversion to avoid noise boosting in the digital FFE."
pub.1139256701,Exploring HW/SW Co-Design for Video Analysis on CPU-FPGA Heterogeneous Systems,"Deep neural network (DNN)-based video analysis has become one of the most essential and challenging tasks to capture implicit information from video streams. Although DNNs significantly improve the analysis quality, they introduce intensive compute and memory demands and require dedicated hardware for efficient processing. The customized heterogeneous system is one of the promising solutions with general-purpose processors (CPUs) and specialized processors (DNN Accelerators). Among various heterogeneous systems, the combination of CPU and FPGA has been intensively studied for DNN inference with improved latency and energy consumption compared to CPU + GPU schemes and with increased flexibility and reduced time-to-market cost compared to CPU + ASIC designs. However, deploying DNN-based video analysis on CPU + FPGA systems still presents challenges from the tedious RTL programming, the intricate design verification, and the time-consuming design space exploration. To address these challenges, we present a novel framework, called EcoSys, to explore co-design and optimization opportunities on CPU-FPGA heterogeneous systems for accelerating video analysis. Novel technologies include 1) a coherent memory space shared by the host and the customized accelerator to enable efficient task partitioning and online DNN model refinement with reduced data transfer latency; 2) an end-to-end design flow that supports high-level design abstraction and allows rapid development of customized hardware accelerators from Python-based DNN descriptions; 3) a design space exploration (DSE) engine that determines the design space and explores the optimized solutions by considering the targeted heterogeneous system and user-specific constraints; and 4) a complete set of co-optimization solutions, including a layer-based pipeline, a feature map partition scheme, and an efficient memory hierarchical design for the accelerator and multithreading programming for the CPU. In this article, we demonstrate our design framework to accelerate the long-term recurrent convolution network (LRCN), which analyzes the input video and output one semantic caption for each frame. EcoSys can deliver 314.7 and 58.1 frames/s by targeting the LRCN model with AlexNet and VGG-16 backbone, respectively. Compared to the multithreaded CPU and pure FPGA design, EcoSys achieves $20.6\times $ and $5.3\times $ higher throughput performance."
pub.1094673608,High Performance Space VPX Payload Computing Architecture Study,"This paper describes a functional reference design for a high-performance payload processor that captures images and spectra from multiple high-resolution instruments, processes and integrates multiple real-time data streams to perform feature recognition and spatial transformations providing autonomous navigation and rendezvous capability for future spacecraft and is equally applicable to Unmanned Aerial Systems (UAS). The proposed design uses two new standards: VITA 78 (SpaceVPX) for multi-processor architecture, and RapidIO (RIO) as the interconnect fabric. The SpaceVPX standard specifies physical form factor, logical, and physical interconnect technologies and architectures that can lead to high-performance fault tolerant computing for high-performance payloads. An overview of SpaceVPX and its relationship to OpenVPX is provided as a guide to practical implementations. The proposed design features a general-purpose host processor with GPU and FPGA-based image processing hardware. RIO is used for the instrument and processor interconnects, providing multiple gigabits per second of data communication capability. An overview of RIO features and operation is presented to complement the SpaceVPX architecture. A notional Reference Architecture is proposed for analysis using multiple methods for estimating avionics performance. The study objectives are to characterize throughput, latency and sub-system utilization using conventional system analysis, hardware prototype measurements and modeling and simulation software. We conducted first-order performance studies to identify bottlenecks in memory speed, I/O capacity and processing power. Initial performance analysis was performed on memory throughput rates, producing first-order values used as a performance baseline. A model of the Reference Architecture using VisualSim Architect was created and simulations run, producing insight into the complex interactions occurring between subsystems. Furthermore, the results of a prototype hardware implementation focusing on RIO throughput are presented as additional metrics. The study predicts RIO throughput between key elements of the Reference Architecture and identify major bottlenecks, and improvements needed for meeting mission requirements. The objective of this paper is to provide guidance to avionics designers regarding the adoption of SpaceVPX today and its anticipated evolution in the next few years."
pub.1112590943,Auditory sensory memory span for duration is severely curtailed in females with Rett Syndrome,"ABSTRACT
                
                  Rett syndrome (RTT), a rare neurodevelopmental disorder caused by mutations in the
                  MECP2
                  gene, is typified by profound cognitive impairment and severe language impairment, rendering it very difficult to accurately measure auditory processing capabilities behaviorally in this population. Here we leverage the mismatch negativity (MMN) component of the event-related potential to measure the ability of RTT patients to decode and store occasional duration deviations in a stream of auditory stimuli. Sensory memory for duration, crucial for speech comprehension, has not been studied in RTT.
                
                High-density EEG was successfully recorded in 18 females with RTT and 27 age-matched typically developing (TD) controls (aged 6-22 years). Data from 7 RTT and 3 TD participants were excluded for excessive noise. Stimuli were 1kHz tones with a standard duration of 100ms and deviant duration of 180ms. To assess the sustainability of sensory memory, stimulus presentation rate was varied with stimulus onset asynchronies (SOAs) of 450, 900 and 1800ms. MMNs with maximum negativity over fronto-central scalp and a latency of 220-230ms were clearly evident for each presentation rate in the TD group, but only for the shortest SOA in the RTT group. Repeated-measures ANOVA revealed a significant group by SOA interaction. MMN amplitude correlated with age in the TD group only. MMN amplitude was not correlated with the Rett Syndrome Severity Scale. This study indicates that while RTT patients can decode deviations in auditory duration, the span of this sensory memory system is severely foreshortened, with likely implications for speech decoding abilities."
pub.1145784682,A distributed geospatial publish/subscribe system on Apache Spark,"Publish/subscribe is a messaging pattern where message producers, called publishers, publish messages which they want to be distributed to message consumers, called subscribers. Subscribers are required to subscribe to messages of interest in advance to be able to receive them upon the publishing. In this paper, we discuss a special type of publish/subscribe systems, namely geospatial publish/subscribe systems (GeoPS systems), in which both published messages (i.e., publications) and subscriptions include a geospatial object. Such an object is used to express both the location information of a publication and the location of interest of a subscription. We argue that there is great potential for using GeoPS systems for the Internet of Things and Sensor Web applications. However, existing GeoPS systems are not applicable for this purpose since they are centralized and cannot cope with multiple highly frequent incoming geospatial data streams containing publications. To overcome this limitation, we present a distributed GeoPS system in the cluster which efficiently matches incoming publications in real-time with a set of stored subscriptions. Additionally, we propose four different (distributed) replication and partitioning strategies for managing subscriptions in our distributed GeoPS system. Finally, we present results of an extensive experimental evaluation in which we compare the throughput, latency and memory consumption of these strategies. These results clearly show that they are both efficient and scalable to larger clusters. The comparison with centralized state-of-the-art approaches shows that the additional processing overhead of our distributed strategies introduced by the Apache Spark is almost negligible."
pub.1133895054,Corporate Security Requirements for Conducting Business over the Internet,"Conducting business over the Internet is not as safe as conducting business over secured private lines. Contrary to popular belief, conducting business over the Internet is a very hazardous undertaking. Placing a Web server on the Internet and exposing business applications over 128-bit Secure Socket Layer (SSL) is not considered a highly secure solution. One might think that 128-bit SSL is secure enough, so why do banks not use it? Everyone else does. For most intents and purposes, 128-bit SSL suffices for encrypting trivial transactions. However, keep in mind that although the banks use SSL to provide transaction security to the consumer, they do not use it internally to protect their monetary transactions. Military-grade encryption systems are known to use key lengths on the order of 1024K and up. The previous government standard of 56-bit Data Encryption Standard (DES) was broken in 1997 and recently again in 1999 in less than 24 hours by using a distributed algorithm from www.distributed.net. Currently, the challenge to break RC5 SSL is underway and has been for some time. It is only a matter of time before the key will be cracked via a brute-force method. In general, the longer the encryption key length, the greater the time required to crack the key. The Federal Information Processing Office requirements in the United States have standardized on 3DES, also known as the Triple DES algorithm. 3DES uses dual encryption keys and a three-phase mechanism to encrypt and decrypt a data stream. 3DES is a 168-bit encryption standard that is available in all security-capable devices manufactured and sold in North America. 3DES is not considered a highly secure encryption mechanism but it is the standard set forth by the Federal Information Processing Office (http://ccf.arc.nasa.gov/ fipmo/index.html). For the majority of business transactions over the Internet, SSL and 3DES are acceptable. For others that demand higher levels of security that support greater key encryption lengths, proprietary encryption hardware or software will be required. A note of caution, however: the greater the encryption key length, the more time required to encrypt and decrypt the data. As such, encryption will increase the average latency time for completing a transaction between two points."
pub.1063156243,Synergistic execution of stream programs on multicores with accelerators,"The StreamIt programming model has been proposed to exploit parallelism in streaming applications on general purpose multicore architectures. The StreamIt graphs describe task, data and pipeline parallelism which can be exploited on accelerators such as Graphics Processing Units (GPUs) or CellBE which support abundant parallelism in hardware. In this paper, we describe a novel method to orchestrate the execution of a StreamIt program on a multicore platform equipped with an accelerator. The proposed approach identifies, using profiling, the relative benefits of executing a task on the superscalar CPU cores and the accelerator. We formulate the problem of partitioning the work between the CPU cores and the GPU, taking into account the latencies for data transfers and the required buffer layout transformations associated with the partitioning, as an integrated Integer Linear Program (ILP) which can then be solved by an ILP solver.We also propose an efficient heuristic algorithm for the work partitioning between the CPU and the GPU, which provides solutions which are within 9.05% of the optimal solution on an average across the benchmark suite. The partitioned tasks are then software pipelined to execute on the multiple CPU cores and the Streaming Multiprocessors (SMs) of the GPU. The software pipelining algorithm orchestrates the execution between CPU cores and the GPU by emitting the code for the CPU and the GPU, and the code for the required data transfers. Our experiments on a platform with 8 CPU cores and a GeForce 8800 GTS 512 GPU show a geometric mean speedup of 6.84X with a maximum of 51.96X over a single threaded CPU execution across the StreamIt benchmarks. This is a 18.9% improvement over a partitioning strategy that maps only the filters that cannot be executed on the GPU -- the filters with state that is persistent across firings -- onto the CPU."
pub.1149131456,A Fog-Cluster Based Load-Balancing Technique,"The Internet of Things has recently been a popular topic of study for developing smart homes and smart cities. Most IoT applications are very sensitive to delays, and IoT sensors provide a constant stream of data. The cloud-based IoT services that were first employed suffer from increased latency and inefficient resource use. Fog computing is used to address these issues by moving cloud services closer to the edge in a small-scale, dispersed fashion. Fog computing is quickly gaining popularity as an effective paradigm for providing customers with real-time processing, platforms, and software services. Real-time applications may be supported at a reduced operating cost using an integrated fog-cloud environment that minimizes resources and reduces delays. Load balancing is a critical problem in fog computing because it ensures that the dynamic load is distributed evenly across all fog nodes, avoiding the situation where some nodes are overloaded while others are underloaded. Numerous algorithms have been proposed to accomplish this goal. In this paper, a framework was proposed that contains three subsystems named user subsystem, cloud subsystem, and fog subsystem. The goal of the proposed framework is to decrease bandwidth costs while providing load balancing at the same time. To optimize the use of all the resources in the fog sub-system, a Fog-Cluster-Based Load-Balancing approach along with a refresh period was proposed. The simulation results show that “Fog-Cluster-Based Load Balancing” decreases energy consumption, the number of Virtual Machines (VMs) migrations, and the number of shutdown hosts compared with existing algorithms for the proposed framework."
pub.1012633362,Synergistic execution of stream programs on multicores with accelerators,"The StreamIt programming model has been proposed to exploit parallelism in streaming applications on general purpose multicore architectures. The StreamIt graphs describe task, data and pipeline parallelism which can be exploited on accelerators such as Graphics Processing Units (GPUs) or CellBE which support abundant parallelism in hardware. In this paper, we describe a novel method to orchestrate the execution of a StreamIt program on a multicore platform equipped with an accelerator. The proposed approach identifies, using profiling, the relative benefits of executing a task on the superscalar CPU cores and the accelerator. We formulate the problem of partitioning the work between the CPU cores and the GPU, taking into account the latencies for data transfers and the required buffer layout transformations associated with the partitioning, as an integrated Integer Linear Program (ILP) which can then be solved by an ILP solver.We also propose an efficient heuristic algorithm for the work partitioning between the CPU and the GPU, which provides solutions which are within 9.05% of the optimal solution on an average across the benchmark suite. The partitioned tasks are then software pipelined to execute on the multiple CPU cores and the Streaming Multiprocessors (SMs) of the GPU. The software pipelining algorithm orchestrates the execution between CPU cores and the GPU by emitting the code for the CPU and the GPU, and the code for the required data transfers. Our experiments on a platform with 8 CPU cores and a GeForce 8800 GTS 512 GPU show a geometric mean speedup of 6.84X with a maximum of 51.96X over a single threaded CPU execution across the StreamIt benchmarks. This is a 18.9% improvement over a partitioning strategy that maps only the filters that cannot be executed on the GPU -- the filters with state that is persistent across firings -- onto the CPU."
pub.1101540249,Polar-Coded MIMO Systems,"To more efficiently realize the coded multiple-input multiple-output (MIMO) transmission and improve the system performance, in this paper, we propose a framework combining the polar coding and the MIMO technique, namely the polarcoded MIMO (PC-MIMO) systems. The MIMO transmission will be recognized from the novel perspective of channel polarization. Combining the polar coding and modulation, the MIMO transmission channel is decomposed into a series of bit-polarized channels under the three-stage channel transform, that means the antenna-*modulation-*bit partition. Based on this generalized channel polarization transform, the PC-MIMO systems allow a joint optimization of polar coding, signal modulation, and MIMO transmission. Specifically, for the first-stage transform, we propose two schemes, namely sequential antenna partition (SAP) and parallel antenna partition (PAP). For the SAP-based PC-MIMO, by using the successive detection of the MIMO signal, the antenna data streams are sequentially decomposed and polarized. In addition, a joint multistage detection and decoding scheme is proposed to combine the multilevel coding method in the secondand third-stage transforms to fully utilize the multiplexing gain and the coding gain. In contrast, the PAP-based PC-MIMO exploits the channel polarization of parallel MIMO detection and concatenates the bit-interleaved coded modulation structure in the secondand third-stage transforms to reduce the processing latency. The simulation results over the MIMO channel with the Rayleigh fast fading show that the proposed PC-MIMO schemes can well outperform the turbo-coded MIMO schemes utilized in many mainstream communication systems."
pub.1029395298,Adaptive link layer strategies for energy efficient wireless networking,"Low power consumption is a key design metric for portable wireless network devices where battery energy is a limited resource. The resultant energy efficient design problem can be addressed at various levels of system design, and indeed much research has been done for hardware power optimization and power management within a wireless device. However, with the increasing trend towards thin client type wireless devices that rely more and more on network based services, a high fraction of power consumption is being accounted for by the transport of packet data over wireless links [28]. This offers an opportunity to optimize for low power in higher layer network protocols responsible for data communication among multiple wireless devices. Consider the data link protocols that transport bits across the wireless link. While traditionally designed around the conventional metrics of throughput and latency, a proper design offers many opportunities for optimizing the metric most relevant to battery operated devices: the amount of battery energy consumed per useful user level bit transmitted across the wireless link. This includes energy spent in the physical radio transmission process, as well as in computation such as signal processing and error coding. This paper describes how energy efficiency in the wireless data link can be enhanced via adaptive frame length control in concert with adaptive error control based on hybrid FEC (forward error correction) and ARQ (automatic repeat request). Key to this approach is a high degree of adaptivity. The length and error coding of the atomic data unit (frame) going over the air, and the retransmission protocol are (a) selected for each application stream (ATM virtual circuit or IP/RSVP flow) based on quality of service (QoS) requirements, and (b) continually adapted as a function of varying radio channel conditions due to fading and other impairments. We present analysis and simulation results on the battery energy efficiency achieved for user traffic of different QoS requirements, and describe hardware and software implementations."
pub.1101926069,Software-defined board- and chip-level optical interconnects for multi-socket communication and disaggregated computing,"The vast amount of new data being generated is outpacing the development of infrastructures and continues to grow at much higher rates than Moores law, a problem that is commonly referred to as the IJdata deluge problem. This brings current computational machines in the struggle to exceed Exascale processing powers by 2020 and this is where the energy boundary is setting the second, bottom-side alarm: A reasonable power envelope for future Super-computers has been projected to be 20MW, while worlds current No. 1 Supercomputer Sunway TaihuLight provides 93 Pflops and requires already 15.37 MW. This simply means that we have reached so far below 10% of the Exascale target but we consume already more than 75% of the tar-geted energy limit! The way to escape is currently following the paradigm of disaggregating and disintegrating resources, massively introducing at the same time optical technologies for interconnect purposes. Disaggregating computing from memory and storage modules can allow for flexible and modular settings where hardware requirements can be tailored to meet the certain energy and performance metrics targeted per application. At the same time, optical interconnect and photonic integration technologies are rapidly replacing electrical interconnects continuously penetrating at deeper hierarchy levels: Silicon photonics have enabled the penetration of optical technology to the computing environment, starting from rack-to-rack and gradually shifting towards board-level communications. In this article, we present our recent work towards implementing on-board single-mode optical interconnects that can support Software Defined Networking allowing for programmable and flexible computational settings that can quickly adapt to the application requirements. We present a programmable 44 Silicon Photonic switch that supports SDN through the use of Bloom filter (BF) labeled router ports. Our scheme significantly simplifies packet forwarding as it negates the need for large forwarding tables, supporting at the same time network size and topol-ogy changes through simple modifications in the assigned BF labels. We demonstrate 14 switch operation controlling the Si-Pho switch by a Stratix V FPGA board that is responsible for processing the packet ID and correlating its destination with the appropriate BF-labeled switch output port. Moving towards high-capacity board-level settings, we discuss the architecture and technology being currently promoted by the recently started H2020 project ICT-STREAMS, where single-mode optical PCBs hosting Si-based routing modules and mid-board transceiver optics expect to enable a massive any-to-any, buffer-less, collision-less and extremely low latency routing platform with 25.6Tb/s aggregate through-put. This architecture and technology are also extended to support resource disaggregation in data centers as currently being pursued in the H2020 project dREDBox, where the any-to-any collisionless routing scheme is prop"
pub.1136932860,Neuromorphic Eye-in-Hand Visual Servoing,"Robotic vision plays a major role in factory automation to service robot applications. However, the traditional use of frame-based cameras sets a limitation on continuous visual feedback due to their low sampling rate, poor performance in low light conditions and redundant data in real-time image processing, especially in the case of high-speed tasks. Neuromorphic event-based vision is a recent technology that gives human-like vision capabilities such as observing the dynamic changes asynchronously at a high temporal resolution ($1~\mu s$ ) with low latency and wide dynamic range. In this paper, for the first time, we present a purely event-based visual servoing method using a neuromorphic camera in an eye-in-hand configuration for the grasping pipeline of a robotic manipulator. We devise three surface layers of active events to directly process the incoming stream of events from relative motion. A purely event-based approach is used to detect corner features, localize them robustly using heatmaps and generate virtual features for tracking and grasp alignment. Based on the visual feedback, the motion of the robot is controlled to make the temporal upcoming event features converge to the desired event in Spatio-temporal space. The controller switches its operation such that it explores the workspace, reaches the target object and achieves a stable grasp. The event-based visual servoing (EBVS) method is comprehensively studied and validated experimentally using a commercial robot manipulator in an eye-in-hand configuration for both static and dynamic targets. Experimental results show superior performance of the EBVS method over frame-based vision, especially in high-speed operations and poor lighting conditions. As such, EBVS overcomes the issues of motion blur, lighting and exposure timing that exist in conventional frame-based visual servoing methods."
pub.1152006421,IoT based application designing of Deep Fake Test for Face animation,"Development of Deep Learning models of Internet of Things (IoT) enclosures with limited resources are difficult because Both Quality of Results are difficult to achieve - QoR as follows two models, DNN Model, and Inference Accuracy and Quality of Services such as power consumption, throughput, and latency. Currently, the development of DNN models is often separated from deploying them to IoT devices, which leads to the most effective solution. If there are many records that represent objects of substantially the same class (face, human body, etc.), you can apply frames to each object of this class. To achieve this, use an independent representation to distinguish between appearance and progress data. Deep fake detection is achieved by using a novel, lightweight Deep Learning method on the IoT platform that is memory-efficient and lightweight. It is carried out in two different stages. The first phase of the deep fake test aims to implement a method of extracting images from a video and using them in conjunction with a Deep Neural Network to implement a test for face animation. It has been reported that the impact of the background elimination has been reported before the background subtraction. Here the Trans GAN model is used for the image classification. In the second phase, the work can be recorded and executed by the IOT device that can record live video streams and then detect activity involved in live video. An activity detection prototype based on IoT devices with small processing power is presented. This prototype provides improvements to the system, extending its application in various ways to improve portability, networking, and other equipment capabilities. The proposed architecture will be evaluated against four highly competitive object detection benchmarking tasks CIFAR10, CIFAR100, SVHN, and ImageNet."
pub.1134665935,A detailed introduction of different beamforming techniques used in 5G,"Summary The arrival of high‐speed internet facility such as the mmWave system or more commonly known as 5G wireless network is a prime impetus that is expected to be the driver for radical transformation in current communication methods. Apart from the numerous applications, an area where 5G is expected to be a game‐changer is the vehicle‐to‐everything or V2X network. This technology uses a road side unit (RSU) which provides wireless connectivity to vehicles, pedestrians, and so on, which may be either stationary or mobile so that their exact location can be known in real time. Researchers have demonstrated experimentally that 4G connectivity with such RSUs can provide a reliable low latency network for certain applications. For instance, for near‐perfect localization of each connected objects on the map, there is a need to stream GPS data, process semantic information from videos continually using computer vision. While these are simply for locating vehicles in real time so that road accident can be prevented. They are also the host of other features such as entertainment, health monitors, and mobile office that can be integrated inside a moving vehicle. To address the issues massive multi‐input multi‐output (MIMO), communication can be realized by using hybrid transceivers which uses a setup consisting of high‐dimensional analog phase shifters and power amplifier with dimensional digital signal processing units having low power. It is therefore vital to choose an apt beamforming design that will run on low energy. A comparison of different varieties of beamforming methodologies for determining the most efficient massive MIMO network is carried out here."
pub.1093453285,Packet jitter measurement in communication networks: a sensitivity analysis,"Communication networks are widespread today. New services are now starting to run on these networks to meet the requests of modern users. To offer good service typically hard constraints have to be satisfied, especially when real-time services are involved. To this aim the reliable estimation of Quality of Service (QoS) parameters as bandwidth, packet jitter, latency, one way delay, to cite a few, is a key issue for communication networks setup, monitoring and tuning. Generally, the measurement of QoS parameters is a complex process that involves several troubles. Some of them depend on the measurement network features: (i) the medium employed to convey the data stream (optic fiber, wireless, wired, etc.), (ii) the type and number of network devices involved (network interface cards, switches, routers, etc.), (iii) the type of communication and access protocols, and (iv) the variability of the network background traffic which could strongly influence the measurement result. Further troubles come from the measurement chain and the measurement method adopted: type of measurement instruments, sampling and observation times, packet size, type of measurement post processing to cite a few. Therefore, they can be thought as sources of measurement uncertainty able to strongly influence the measurement process and the reliability of the results. With reference to the measurement chain and the measurement method adopted, in this paper a deep sensitivity analysis aimed to identify the main quantities influencing the measurement results and to study their modeling as uncertainty components is performed. The attention has been focused on the packet jitter estimation made on a real test bed: the fiber optic based local area network of the University of Cassino (Italy)."
pub.1141002229,Using Edge-to-Cloud Analytics IoT Dumpsite Monitor for Proactive Waste Management,"In this work, an Edge-to-Cloud Analytics architecture model for Waste Management and Hazard Alert system is developed for prompt management of waste dumpsites. The system enables IoT smart nodes to monitor dumpsites and generate alerts on the detection of gas emissions, smoke, and fire. The custom-designed IoT dumpsite edge aggregator module is achieved using MQ-7 (carbon monoxide), MQ-2 (smoke/methane), MQ-135 (CO2, benzene, NH3, NOx), and flame sensors for proactive detection of environmental hazards. The scalable Edge-to-Cloud Analytics architectural model produces efficient data stream uploading from the IoT sensor nodes into the cloud domain with an MKR-1000 Edge Controller used to achieve edge-to-fog gateway processing. Three algorithms developed for the system include IoT node characterization, sensor reading/calibration, and data transmission to remote servers using Simultaneous Wireless Information Power Transfer (SWIPT). A proof of concept was demonstrated by building three IoT dumpsite nodes that were used for testing. The Testbed deployment with visualization patterns is observed on Open Source ThingSpeak Cloud-native platform. The key contributions to knowledge include the use of Edge-to-Cloud analytics for dumpsite monitoring and prompt evacuation, the introduction of SWIPT to conserve energy for optimized valued-insights and the development of an architectural model that allows lower computer-storage resources at the edge. This is very good since a single IoT device only serves a small geographic area. Future work will investigate throughput and latency minimization, API containerization for light deployments using Docker Swarm and Kubernetes. Since most dumpsites are remarkably close to end-users, edge to cloud (IoT) analytics is recommended for a sustainable solution to waste management in Nigeria."
pub.1129425298,Run-Time Scenario-Based MPSoC Mapping Reconfiguration Using Machine Learning Models,"Applications with highly input-dependent workload and execution behavior cannot be optimally executed by a single mapping of application tasks to a heterogeneous multi-core target architecture. Albeit mapping a task to a resource with high computational power may be suitable for input triggering a high workload of this task, it may be more efficient to map another task to the resource in case of input providing low workload for the former and high workload for the latter. As a remedy, we propose to group inputs evoking similar workload and execution characteristics into so-called workload scenarios for which specialized mappings targeted at the common workload distribution in the scenario are provided. Optimized mappings for each scenario can be determined by a scenario-based design space exploration at design time. At run time, applications process a stream of input data whose scenario affiliation is a priori unknown. This entails two coupled tasks: First, we have to identify the scenario of the current input data based on its execution characteristics. Second, we have to choose an application mapping for processing the current input prior to its execution on the basis of the concluded scenarios of the past input and the currently active scenario-associated mapping. Note that switching between scenarios may come at a non-negligible reconfiguration cost that could decrease the advantage gained by a more suitable mapping. Both tasks are tackled by a proposed run-time reconfiguration manager, which is built on machine learning models. These models learn a strategy for identifying scenarios and selecting adequate mappings by design-time training. Here, different machine learning models are compared for their applicability. An evaluation of the run-time manager based on a ray tracing and stitching application shows significant latency improvements compared to an approach with a single mapping optimized for the average-case input."
pub.1176144066,GALÆXI: Solving complex compressible flows with high-order discontinuous Galerkin methods on accelerator-based systems,"This work presents GALÆXI as a novel, energy-efficient flow solver for the simulation of compressible flows on unstructured hexahedral meshes leveraging the parallel computing power of modern Graphics Processing Units (GPUs). GALÆXI implements the high-order Discontinuous Galerkin Spectral Element Method (DGSEM) using shock capturing with a finite-volume subcell approach to ensure the stability of the high-order scheme near shocks. This work provides details on the general code design, the parallelization strategy, and the implementation approach for the compute kernels with a focus on the element local mappings between volume and surface data due to the unstructured mesh. The scheme is implemented using a pure distributed memory parallelization based on a domain decomposition, where each GPU handles a distinct region of the computational domain. On each GPU, the computations are assigned to different compute streams which allows to antedate the computation of quantities required for communication while performing local computations from other streams to hide the communication latency. This parallelization strategy allows for maximizing the use of available computational resources. This results in excellent strong scaling properties of GALÆXI up to 1024 GPUs if each GPU is assigned a minimum of one million degrees of freedom. To verify its implementation, a convergence study is performed that recovers the theoretical order of convergence of the implemented numerical schemes. Moreover, the solver is validated using both the incompressible and compressible formulation of the Taylor–Green-Vortex at a Mach number of 0.1 and 1.25, respectively. A mesh convergence study shows that the results converge to the high-fidelity reference solution and that the results match the original CPU implementation. Finally, GALÆXI is applied to a large-scale wall-resolved large eddy simulation of a linear cascade of the NASA Rotor 37. Here, the supersonic region and shocks at the leading edge are captured accurately and robustly by the implemented shock-capturing approach. It is demonstrated that GALÆXI requires less than half of the energy to carry out this simulation in comparison to the reference CPU implementation. This renders GALÆXI as a potent tool for accurate and efficient simulations of compressible flows in the realm of exascale computing and the associated new HPC architectures."
pub.1052241941,Sensor selection for energy-efficient ambulatory medical monitoring,"Epilepsy affects over three million Americans of all ages. Despite recent advances, more than 20% of individuals with epilepsy never achieve adequate control of their seizures. The use of a small, portable, non-invasive seizure monitor could benefit these individuals tremendously. However, in order for such a device to be suitable for long-term wear, it must be both comfortable and lightweight. Typical state-of-the-art non-invasive seizure onset detection algorithms require 21 scalp electrodes to be placed on the head. These electrodes are used to generate 18 data streams, called channels. The large number of electrodes is inconvenient for the patient and processing 18 channels can consume a considerable amount of energy, a problem for a battery-powered device. In this paper, we describe an automated way to construct detectors that use fewer channels, and thus fewer electrodes. Starting from an existing technique for constructing 18 channel patient-specific detectors, we use machine learning to automatically construct reduced channel detectors. We evaluate our algorithm on data from 16 patients used in an earlier study. On average, our algorithm reduced the number of channels from 18 to 4.6 while decreasing the mean fraction of seizure onsets detected from 99% to 97%. For 12 out of the 16 patients, there was no degradation in the detection rate. While the average detection latency increased from 7.8 s to 11.2 s, the average rate of false alarms per hour decreased from 0.35 to 0.19. We also describe a prototype implementation of a single channel EEG monitoring device built using off-the-shelf components, and use this implementation to derive an energy consumption model. Using fewer channels reduced the average energy consumption by 69%, which amounts to a 3.3x increase in battery lifetime. Finally, we show how additional energy savings can be realized by using a low-power screening detector to rule out segments of data that are obviously not seizures. Though this technique does not reduce the number of electrodes needed, it does reduce the energy consumption by an additional 16%."
pub.1095752744,A Collaborative Framework for Distributed Microscopy,"This paper outlines the motivation, requirements, and architecture of a collaborative framework for distributed virtual microscopy. In this context, the requirements are specified in terms of (1) functionality, (2) scalability, (3) interactivity, and (4) safety and security. Functionality refers to what and how an instrument does something. Scalability refers to the number of instruments, vendor specific desktop workstations, analysis programs, and collaborators that can be accessed. Interactivity refers to how well the system can be steered either for static or dynamic experiments. Safety and security refers to safe operation of an instrument coupled with user authentication, privacy, and integrity of data communication. To meet these requirements, we introduce three types of services in the architecture: Instrument Services (IS), Exchange Services (ES), and Computational Services (CS). These services may reside on any host in the distributed system. The IS provide an abstraction for manipulating different types of microscopes; the ES provide common services that are required between different resources; and the CS provide analytical capabilities for data analysis and simulation. These services are brought together through CORBA and its enabling services, e.g., Event Services, Time Services, Naming Services, and Security Services. Two unique applications have been introduced into the CS for analyzing scientific images either for instrument control or recovery of a model for objects of interest. These include: insitu electron microscopy and recovery of 3D shape from holographic microscopy. The first application provides a near real-time processing of the video-stream for on-line quantitative analysis and the use of that information for closed-loop servo control. The second application reconstructs a 3D representation of an inclusion (a crystal structure in a matrix) from multiple views through holographic electron microscopy. These application require steering external stimuli or computational parameters for a particular result. In a sense, ``computational instruments'' (symmetric multiprocessors) interact closely with data generated from ``experimental instruments'' (unique microscopes) to conduct new experiments and bring new functionalities to these instruments. Both of these features exploit high-performance computing and low-latency networks to bring novel functionalities to unique scientific imaging instruments."
pub.1012501239,Computing with Distributed Resources,"The metaphor “Network is the Computer” has received much attention lately. It is easy/hard to claim such an equivalence since neither term is defined precisely. It is easy to establish an equivalence by ignoring several key aspects of the network, such as the costs of remote data access, failures of network nodes and communication links, and the security issues inherent in distributed computing. We may then view the network as a repository of data, typically stored in distributed objects, which resembles the primary (and secondary) storage of a traditional computer. The underlying instruction set for the network computer consists of method calls on these objects; the effect of a method call is to modify the state of the object (similar to a store instruction in a traditional computer) and/or return some value (similar to a load instruction). Now, consider executing a high-level instruction, such as x:= f(y,z), on the network computer. The data represented by x, y and z may be stored at different machines, as well as the function f to be applied to y and z. An implementation of this statement has (1) to determine the sites of the data y and z, and, possibly, choose among several sites if the data are replicated, (2) choose the site where the actual computation has to take place, and (3) communicate the result to all sites where x is to be stored. The implementation could be even more elaborate when x , y and z are matrices, for example, and f is the matrix multiplication operator. Then, y and z may be sent as streams (by rows or columns), the appropriate computations are carried out, possibly by multiple computers, and the result x sent as a stream of element values as soon as such a value is computed.What should be the structure of a high-level program for a network computer? Should a user be given the illusion that all data are locally available, there is no latency in accessing data, computations are not interleaved with computations performed by other users, and that there is never any failure? This is clearly the ideal view. Unfortunately, such a view cannot be currently supported by the internet. A user may have to confront the possibility that certain pieces of data may be unavailable, because the corresponding site has failed. The user may have to realize that certain pieces of data could be modified by other parties during a computation.In this paper, we develop a theory and a set of notations to represent the counterpart of a function on a network computer, i.e., how x:= f(y,z) should be specified and computed. Since non-determinism is inherent in a network model of computation, f need not be a function. We propose a programming model which includes non-determinism as a central concept; we call f a task. Task calls can be nested and tasks can be called recursively. Additionally, a task can explicitly include time-outs in its specification. Tasks capture the essence of what is currently known as web services.The traditional theory of tran"
pub.1094615983,GOES-R Ground Segment Architectural Overview,"The National Oceanic and Atmospheric Administration (NOAA) and National Aeronautics and Space Administration (NASA) are jointly developing the next-generation series of Geostationary Operational Environmental Satellites (GOES), known as the GOES-R Series.2IEEEAC paper#1152, Version 3, Updated January 11, 2011 As a part of this program, NOAA maintains responsibility for the development and operation of the GOES-R Ground Segment (GS). The GOES-R GS is an integrated, end-to-end system that supports the collection, processing, and dissemination of atmospheric data and the command and control of the GOES-R Series satellite constellation. IEEEAC paper#1152, Version 3, Updated January 11, 2011 The system is comprised of a core development effort made up of mission management, product generation, product distribution, and enterprise management elements and supported by hardware and software infrastructure. Mission Management will provide the primary data receipt and command and control as well as mission planning, scheduling, and monitoring functionality in order to support the satellite operations processes of the GOES-R series. The Product Generation element will process raw instrument data into higher order products, including the creation of a direct broadcast data stream to be distributed hemispherically to the GOES user community. Product Distribution will provide data dissemination capabilities to ensure GOES-R products reach the user community, including dedicated pathways to the National Weather Service for low-latency, high-availability imagery. The Enterprise Management element provides an integrated monitoring and reporting capability that will enable a comprehensive view of system status, while Infrastructure provides a pooled set of hardware and software resources to be used by the elements. In addition, the GS will provide a remote backup facility (REU), new and upgraded antenna capabilities to NOAA, and will develop a user distribution and access portal known as the GOES-R Access Subsystem. This paper summarizes the GOES-R Ground Segment architecture and the development processes the Ground Segment Project has followed in organizing and implementing the system architecture. It discusses the functionality of the Ground Segment, highlights the various elements and functions within the GS, and discusses the key interfaces that the system uses to provide data to its user community. It also reviews the IT security approach the GS development effort is following and considers the various environments that will be implemented to support development, integration and test, and operations of the GS as a whole."
pub.1048503638,Proxy Caching Strategies for Internet Media Streaming,"information while being downloaded by clients. With the explosive growth of the Web and the mature of digital video technology, media streaming has received a great deal of interest as a promising solution for multimedia delivery services. This approach allows that media objects can be accessed in a similar way to conventional text and images using a download-and-play mode. However, unlike static text-based content, proxy caching has difficulty in delivering streaming media content because media objects are usually very large and its transmission consumes a great amount of network resources, prolongs startup latency, and threatens the playback continuity. The size of a conventional Web object is typically on the order of 1–100 kbytes and, therefore, a decision regarding either caching or not an object in its totality is an easy task (Liu & Xu, 2004). However, the size of media objects is very large, reaching a size on the order of several hundreds of Mbytes or even Gbytes. Therefore, caching a whole media object at a Web proxy optimized for delivering conventional small-size Web objects is not feasible, since large streams would quickly exhaust the capacity of the proxy cache. Besides, the streaming of media objects requires a significant amount of resources such as disk space and network bandwidth, which need to be maintained during a long period of time. Moreover, the long playback duration of a streaming may allow several client-server interactions. Therefore, access rates might be different for different parts of a stream, which makes cache management potentially more complex, as pointed out by Liu and Xu (2004). On the other hand, a download-beforeplaying solution provides continuous playback, but it also introduces a large startup delay. An effective solution to reduce client-perceived latencies and network congestion is to cache data at proxies widely deployed across the Internet. This solution, besides inexpensive, also leads to an improvement of both availability of objects and packet losses since redundant network transmission decreases while transmission efficiency increases. However, proxies are generally optimized for delivering conventional small-size Web objects, which may not satisfy the requirements of streaming applications. Due to these particular features of media objects, novel caching strategies have been proposed. With the evolution of the Internet as the dominant architecture for applications, contents, and services, these are gradually migrating from the client-­server paradigm to the edge services paradigm and to the peer-to-peer (P2P) computing paradigm. Recently, P2P system has received a great amount of interest as a promising scalable and costeffective solution for next-generation multimedia content distribution. This kind of systems have advantages regarding systems based on the client-server paradigm, namely improved scalability and reliability, cheaper infrastructures due to direct communication among peers, and e"
pub.1156230293,Smart Facilities and 5G-Supported Systems in Social IoB,"5G is the fifth-generation mobile network that delivers high data speeds, large network capacity, low latency, and more reliability to users. Our phones can stream multiple movies simultaneously or download one in 4K high quality at speeds of about 10 gigabits per second thanks to 5G. Although 5G runs on the same radio frequencies used by your devices, its enormous potential has taken technology to the next level. By advancing societies, transforming industries, and elevating experiences, it is giving innovation a better shape. It takes connectivity to the next level by delivering cloud services to clients’ connected experience. With 5G as the new global standard network after 4G, there has been a significant change in the working and efficiency of many industries, the Internet of Behavior (IoB) being the most developed one among them. IoB is a network of interconnected, web-connected devices that can gather and send data across a wireless network without the need for human interaction. It’s a major advancement because it makes connected cities safer, asset tracking more affordable, healthcare more individualized, and energy usage more efficient. For its services to be more effective, it needs a dependable, inexpensive, and quick Internet. Therefore, IoB connectivity needs are served excellently with faster network and higher capacity. By increasing the capacity of cellular systems, the 5G spectrum widens the frequencies on which data is carried, enabling more devices to connect. IoB also enables businesses to optimize value propositions because 5G’s significant Internet bandwidth and speed allow for the connection and data exchange of physical assets’ sensors, software, and other technologies with other systems. Businesses that are 5G and IoB connected will not only spur creativity and improve efficiency across the board, they will also contribute to the fight against climate change by lowering carbon pollution. The potential of IoB and its numerous next-generation services has been increased by 5G. Advanced wireless modulation techniques, network slicing capabilities, automated network application lifecycle management, software-defined infrastructure, and network (SDN) virtualization are all being accelerated by 5G. By providing a huge number of IoB connections to traffic signals, cameras, and sensors, 5G will also enable improved traffic control. Smart meters will track energy use and contribute to consumption reduction. They are assisted by 5G low-cost IoB sensors and connectivity. In a way, 5G has enabled IoB and improved its working. IoB is a vast area to analyze and understand, and with 5G coming along, the area is expanding, and we need to deeply understand how 5G is boosting the growth of IoB. Therefore, in this chapter, we will discuss how 5G is empowering IoB and making it the greatest network in the world. 5G is the fifth-generation mobile network that delivers high data speeds, large network capacity, low latency, and more reliabilit"
pub.1042775416,Performance optimization for a SHA-1 cryptographic workload expressed in OpenCL for FPGA execution,"The introduction of Field Programmable Gate Array (FPGA) based devices for OpenCL applications provides an opportunity to develop kernels which are executed on application specific compute units which can be optimized for specific workloads such as encryption. This work examines the optimization of the SHA-1 hashing algorithm developed in OpenCL for and FPGA based implementation. The implementation starts from the freely available SHA-1 implementation in OpenSWAN; ports the implementation to OpenCL; and optimizes the kernel for FPGA implementation using the Xilinx SDAcccel development environment for OpenCL applications. Through each stage, the implementation is benchmarked in order to examine latency, throughput, and power usage on FPGA, Graphics Processing Unit (GPU), and Central Processing Unit (CPU) systems. While the programming model of OpenCL on FPGAs is identical to GPU and CPU, in order to optimize an application it is necessary to understand how the OpenCL concepts are implemented on FPGAs. In the platform model, one FPGA is considered a device. Inside the FPGA, a portion of the resources are dedicated to the fixed platform region. This region includes the memory interface, PCI Express interface, Direct Memory Access (DMA) controller, flash programming interface, and FPGA reconfiguration interface. The rest of the FPGA resources are dedicated to one or more OpenCL regions. By calling clCreateProgramWith-Binary, an OpenCL Region will be reprogrammed with the users chosen FPGA binary file. The FPGA binary file contains the configuration information to program one or more compute into the FPGA fabric for a kernel in the application. As a result of the flexibility of the FPGA fabric, these compute units can all work on the same kernel function or can be specialized to work on a set of kernel functions. All compute units, which target the same kernel, may be used by clEnqueueNDRange or out of order queues to implement concurrency. In addition to customizing the kernel compute units, FPGAs enable the creation of application specific memory architectures to minimize latency. Within the memory architectures supported by an FPGA, external and on-chip memories can be used within the OpenCL memory model. The external memory is mapped to the global and constant memory spaces while the on chip memory is mapped to the local, private and program scope global memory spaces. The on chip memory is implemented via BRAM and register resources inside the FPGA which are automatically inferred by the compiler. In addition to the levels of memory hierarchy available with FPGAs, the Xilinx SDAccel compiler analyzes the kernel data movement operations to automatically coalesce transactions and infer burst transactions to maximize usage of the available memory bandwidth. The SHA-1 is an algorithm used for the hashing of messages. It has a block iterative structure which utilizes one way compression to achieve strong cryptographic integrity. In the algorithm, a me"
pub.1044302831,Avoiding request–request type message-dependent deadlocks in networks-on-chips,"When an application is running on a network-on-chip (NoC)-based multiprocessor system-on-chip (MPSoC), two types of deadlocks may occur: (i) the routing-dependent deadlocks, and (ii) the message-dependent deadlocks. The former type of deadlocks can be avoided by removing any cyclic paths on the application’s channel dependency graph. The message-dependent deadlocks, caused by mutual dependency of different control and/or data messages, on the other hand, are very complicated to deal with. In this paper, we focus our study on the request–request type message-dependent deadlocks which may appear in a peer-to-peer streaming system. This type of deadlocks can have devastating effects on applications using streaming protocols that often demands real-time processing over continuous data streams. We show that request–request type of deadlocks can be avoided by proper inclusion of virtual channels (VCs) for the links along the selected routing path. These VCs are not bounded to a particular communication path. Instead, they can be shared among multiple existing communication flows. In this paper, we have formally proved a sufficient condition that determines the minimum number of VCs actually needed for each link of a communication flow such that, request–request type message-dependent deadlocks can be completely avoided. Following this sufficient condition, we propose a path selection and minimum VC allocation (PSMV) algorithm to help determine the minimum number of non-uniform VCs for each link. The PSMV algorithm consists of two major steps. In the first step, we attempt to minimize the maximum number of VCs among all the links. This problem is NP-complete in nature, and it is solved using the proposed mixed integral linear programming (MILP)-based algorithm. In the second step, based on the solution suggested in the first step, the minimum number of VCs for each link is finally determined. The PSMV algorithm can literally be integrated with any existing application mapping algorithm to provide deadlock-free mapping results. One such deadlock-free mapping algorithm is suggested in this paper. Our experiments also show that, compared to an existing flow control based deadlock avoidance method (CTC) and a deadlock recovery method (DR), increase of buffers size in PSMV is within 5% compared to a baseline network configuration. The message latency of PSMV is the lowest among all three designs."
pub.1032765609,25th Annual Computational Neuroscience Meeting: CNS-2016,"Table of contentsA1 Functional advantages of cell-type heterogeneity in neural circuitsTatyana O. SharpeeA2 Mesoscopic modeling of propagating waves in visual cortexAlain DestexheA3 Dynamics and biomarkers of mental disordersMitsuo KawatoF1 Precise recruitment of spiking output at theta frequencies requires dendritic h-channels in multi-compartment models of oriens-lacunosum/moleculare hippocampal interneuronsVladislav Sekulić, Frances K. SkinnerF2 Kernel methods in reconstruction of current sources from extracellular potentials for single cells and the whole brainsDaniel K. Wójcik, Chaitanya Chintaluri, Dorottya Cserpán, Zoltán SomogyváriF3 The synchronized periods depend on intracellular transcriptional repression mechanisms in circadian clocks.Jae Kyoung Kim, Zachary P. Kilpatrick, Matthew R. Bennett, Kresimir JosićO1 Assessing irregularity and coordination of spiking-bursting rhythms in central pattern generatorsIrene Elices, David Arroyo, Rafael Levi, Francisco B. Rodriguez, Pablo VaronaO2 Regulation of top-down processing by cortically-projecting parvalbumin positive neurons in basal forebrainEunjin Hwang, Bowon Kim, Hio-Been Han, Tae Kim, James T. McKenna, Ritchie E. Brown, Robert W. McCarley, Jee Hyun ChoiO3 Modeling auditory stream segregation, build-up and bistabilityJames Rankin, Pamela Osborn Popp, John RinzelO4 Strong competition between tonotopic neural ensembles explains pitch-related dynamics of auditory cortex evoked fieldsAlejandro Tabas, André Rupp, Emili Balaguer-BallesterO5 A simple model of retinal response to multi-electrode stimulationMatias I. Maturana, David B. Grayden, Shaun L. Cloherty, Tatiana Kameneva, Michael R. Ibbotson, Hamish MeffinO6 Noise correlations in V4 area correlate with behavioral performance in visual discrimination taskVeronika Koren, Timm Lochmann, Valentin Dragoi, Klaus ObermayerO7 Input-location dependent gain modulation in cerebellar nucleus neuronsMaria Psarrou, Maria Schilstra, Neil Davey, Benjamin Torben-Nielsen, Volker SteuberO8 Analytic solution of cable energy function for cortical axons and dendritesHuiwen Ju, Jiao Yu, Michael L. Hines, Liang Chen, Yuguo YuO9 C. elegans interactome: interactive visualization of Caenorhabditis elegans worm neuronal networkJimin Kim, Will Leahy, Eli ShlizermanO10 Is the model any good? Objective criteria for computational neuroscience model selectionJustas Birgiolas, Richard C. Gerkin, Sharon M. CrookO11 Cooperation and competition of gamma oscillation mechanismsAtthaphon Viriyopase, Raoul-Martin Memmesheimer, Stan GielenO12 A discrete structure of the brain wavesYuri Dabaghian, Justin DeVito, Luca PerottiO13 Direction-specific silencing of the Drosophila gaze stabilization systemAnmo J. Kim, Lisa M. Fenk, Cheng Lyu, Gaby MaimonO14 What does the fruit fly think about values? A model of olfactory associative learningChang Zhao, Yves Widmer, Simon Sprecher,Walter SennO15 Effects of ionic diffusion on power spectra of local field potentials (LFP)Geir Halnes, Tuom"
pub.1127094234,(Invited) Artificial Neurons and Synapses with CVD MoS2 Facilitated By Electrode Engineering," The prevalent von Neumann architecture in today’s processors involves memory and processing units to reside in physically separate locations. With memory speeds lagging behind the processor speeds, latency in accessing data from memory has resulted in the “von Neumann bottleneck”. To alleviate this issue, several alternative non-von Neumann architectures have been explored. Neuromorphic computing is one such non-von Neumann approach, inspired by the human brain’s ability of cognitive recognition. The brain operates through a network of neurons that are connected to each other by synapses. In 2008, after the discovery of memristors, the fourth circuit element, researchers have explored the idea of mimicking synaptic behavior with a single memristive device. 1-6 Albeit significant advances, synaptic devices using phase change materials-based memristors 3,7-9 and metal oxide-based resistive switching devices 5,6,10-13 have some limitations. These devices exhibit high programming current in the range of µA to mA. Additionally, the synaptic weight update, i.e. the increase (decrease) of the synapse’s conductance with the application of a continuous stream of identical positive (negative) input voltage pulses, is non-linear. Non-linearity in weight update increases the complexity of using these devices for real-time unsupervised learning. So, it becomes necessary to employ a materials system which exhibits low programming current as well as a linear weight update. 14   Recently, two-dimensional (2D) materials are being largely explored to demonstrate their viability as electronic synapses and neurons. 15-21 In this talk, we shall discuss the realization of a synaptic device using graphene/MoS 2 heterostructures. In these devices, CVD-grown monolayer graphene acts as an electrode to CVD MoS 2 . These memristive devices exhibit low programming currents and a high dynamic range from 1 nA to 1 mA. In contrast with oxide-based or PCM-based synapses, these devices exhibit a gradual set and reset process when symmetric input voltage pulses are applied, resulting in a near-linear weight update. We shall also present the demonstration of an integrate-and-fire (IF) neuron using Ag/MoS 2 /Au vertical structures. These devices possess the four crucial features of an IF neuron – all-or-nothing spiking, threshold-driven firing, post-firing refractory period and stimulus strength based frequency response. Realizing neurons and synapses using the same materials system allows the monolithic integration of the essential building blocks of neuromorphic hardware, and bears potential for a highly scalable spiking neural networks suitable for unsupervised learning applications.  References:  1 Chua, L. IEEE Trans. Circuit Theory 18 , 507-519 (1971).   2 Jo, S. H. et al. Nano Lett. 10 , 1297-1301 (2010).   3 Kuzum, D. et al. Nano Lett. 12 , 2179-2186 (2011).   4 Strukov, D. B. et al. Nature 453 , 80 (2008).   5 Yu, S. et al. Adv. Mater. 25 , 1774-1779 (2013).   6 Yu, S. et "
pub.1021378681,"Oral abstracts of the 21st International AIDS Conference 18–22 July 2016, Durban, South Africa","Within the first weeks of human immunodeficiency virus (HIV) infection, virus replication reaches systemic circulation. Despite the critical, causal role of virus replication in determining transmissibility and kinetics of disease progression, there is limited understanding of the conditions required to transform a small localized transmitted founder population into a large and heterogeneous systemic infection. Cynomolgus and rhesus macaques were infected with simian immunodeficiency virus (SIV) and followed longitudinally. Plasma levels of SIV were monitored using qRT‐PCR. Bacterial genomic DNA in plasma was characterized and quantified longitudinally using 16S ribosomal deep sequencing and qPCR. ELISA‐based assays were used to monitor intestinal permeability (IFABP) and perturbation of bacteria‐specific host factors (sCD14 and EndoCab). Flow cytometry was used to track peripheral blood lymphocyte populations. In vitro assays were performed by exposing freshly isolated peripheral blood mononuclear cells to bacterial lysate prepared from major translocators. Effects of bacterial lysate on CD4+ T cell activation and CD8+ T cell cytotoxicity were measured using flow cytometry. Statistical significance was calculated using ANOVA or Wilcoxon signed‐rank testing. Prior to the peak of viremia, we observed a transient high‐level influx of microbial genomic DNA into peripheral blood. This microbial translocation was accompanied by perturbation of bacteria‐specific host factors in plasma, as well as expansion of the CD4+CCR5+ T cell compartment. Exposure of freshly isolated peripheral blood mononuclear cells to lysate prepared from major translocating taxa revealed differential taxa‐specific effects on the CD4+CCR5+ T cell compartment and cytotoxic granule expression within CD8+ T cells. Altogether, our data identify the influx of microbial products into blood during hyperacute SIV infection as a candidate modifier of early interactions between the antiviral host response and nascent HIV infection. Over the next few months, we will explore the effect of inducing microbial translocation during SIV infection, with particular interest on microbial reactivity within the CD4+CCR5+ target cell compartment. High dietary fats were reported to induce intestinal dysbiosis, drive gut inflammation and breakdown the intestinal epithelial barrier, granting intestinal flora access to the bloodstream. As microbial translocation is a major determinant of the chronic immune activation and HIV/SIV disease progression, we investigated whether fat diet impacts HIV/SIV pathogenesis. The non‐progressive African green monkey (AGM) model of SIV is an ideal system to assess the role of fat diet on disease progression, because they do not develop SIV‐related intestinal dysfunction. We included four AGMs that received a fat diet prior and after SIVsab infection, and five controls in which the impact on key parameters of SIV infection such as: viral loads, CD4+ T cell counts, microbi"
pub.1018767988,Clinical Neurophysiology: Computer Analysis of EEG,"1 Tomoyuki Akiyama, 2 Hiroshi Otsubo, 2 Ayako Ochi, 2 Rajesh RamachandranNair, 2 Irene Elliot, 2 Elizabeth Donner, 2 Shelly K. Weiss, 2 James T. Rutka, and 2 O. Carter Snead III ( 1 Department of Child Neurology, Okayama University Hospital, Okayama, Okayama, Japan ; and 2 Division of Neurology, The Hospital for Sick Children, Toronto, ON, Canada ) Rationale: Multiple band frequency analysis (MBFA), a new frequency analysis method, provides better frequency and temporal resolutions than fast Fourier or wavelet transform. We evaluated dynamic changes of high frequency oscillations (HFOs) on brain surface maps using MBFA to localize epileptogenic zones in neocortical epilepsy. Methods: We studied 2 children with intractable neocortical epilepsy by subdural EEG (SDEEG) at 1 KHz sampling rate. We performed MBFA for 1–5 second ictal EEG using the software Short Spectrum Eye (Gram, Saitama, Japan). We identified HFOs, determined the frequency range, and calculated averaged powers at every 10 ms for all electrodes. We imported the averaged powers into a topographic mapping program Prism and Insight (Persyst, Prescott, AZ) to visualize HFO powers on the brain surface picture. To analyze temporal interrelationships of regions with HFOs, we arranged HFO power maps at every 20 ms to project movies of ictal HFOs on the brain surface. Case 1: A 17 year old right handed girl presented with simple partial seizures consisting of numbness and tingling sensations in the right arm followed by clonic movements and secondary generalization. Case 2: A 14 year old left handed boy with asymmetric epileptic spasms in clusters. Results: Case 1; During partial seizures, MBFA of 25‐channel SDEEG showed 80–130 Hz HFOs over the middle portion of the left postcentral gyrus and upper portion of the left post‐ and pre‐central gyri. Sequential power maps of 80–130 Hz HFOs revealed high powers over these 2 regions, and reverberating power changes between them with a maximum at the middle portion of postcentral gyrus. We resected the upper and middle portions of left postcentral gyrus. She has rare and brief partial motor seizures without sensory aura and secondary generalization 9 months after surgery. Case 2; During spasms, MBFA of 106‐channel SDEEG showed extensive but noncontiguous 60–150 Hz HFOs over the right superior and inferior frontal, and middle temporal gyri. Sequential power maps of 80–120 Hz HFOs revealed initial high power over the right superior frontal gyrus and subsequent activations over other 2 regions. We performed partial right frontal lobectomy, cortical excisions of the superior and inferior frontal gyri, and middle temporal gyrus where predominant HFOs existed. He is seizure free 6 months after surgery. Conclusions: The combination of MBFA and sequential brain surface HFO power mapping enabled us to recognize the localization and dynamic changes of ictal HFOs. This spatial and temporal analysis of HFO cortical mapping may disclose the behavior of ictal netw"
pub.1013370252,Poster Session 211:00 a.m.‐7:30 p.m.Professionals in Epilepsy care,"Marlene A. Blackman*, Elaine Wirrell† and N. Thornton*
*Neurosciences, Alberta Children's Hospital, Calgary, AB, Canada and †Neurosciences, Mayo Clinic, Rochester, MN Rationale: To examine the impact of epilepsy on quality of life of children compared to nearest aged siblings, as well as to further investigate the impact of epilepsy thru the use of the HARCES and ICND scales. Methods: All children with epilepsy ages 3–17 whom had a non‐epileptic sibling in same age group seen thru the Neurology Clinic at the Alberta Children's Hospital were identified as potential participants for the study. Parents were asked to complete Global Quality of Life Linear scales for both children and Hague Restrictions in Epilepsy Scales and Impact of Child Neurologic Disability Scales for the child with epilepsy. Results: Fifty children with epilepsy (age range 3–17, gender M 25, F 25) and Fifty siblings who acted as the controls (age range 3–17 gender M 20 – F 30) participated in the study. Quality of life measurement using the Global Quality of Life Linear Scale was significantly different (p < 0.0001) betwen children with epilepsy (4.57 ± 0.92) and their siblings (5.30 ± 0.71). The HARCES found the average score of 18.89 Of the 49 children who completed this scale 3 (6%)scored 10 (no disability) 23 (47%) scored 11–15 (slight disability) 13 (26%) scored between 16–25,Mild 4 (8%) 26–30 moderate 6 (12%) 31–40 severe disability. On the ICNDS average was 40 with a range of (0–108) 14 scored than 20 (less impact), (28%) 20 scored 21–50 (40%) (moderate impact),with 16 scoring 50–108 (32%) (severe impact). Conclusions: Epilepsy has a negative impact on the quality of life of children, children in the same family with the same parents have statistically significant differnces in quality of life scores. Bryn M. Corbett, J. Gerke, Joseph I. Sirven, D. Shulman, G. Long, T. Pipe and M. Griffin 
 5W Neuro/ENT, Mayo Clinic Hospital, Arizona, Phoenix, AZ Rationale: Transforming Care at the Bedside (TCAB) is a nation‐wide research initiative, funded by the Robert Wood Johnson Foundation (RWJ) and supported by the Institute for Health Care Improvement. The overall aim of TCAB is to help hospital based nurses redesign delivery of work processes to improve patient care. Outcomes are assessed through comparison of baseline and ongoing measures including: work environment and culture; patient centeredness; number and types of innovations tested and sustained; and nursing variables that include, but are not limited to, adverse events (drug events, patient falls, and pressure ulcers) Patients admitted to the EMU are at greater risk for falls and injury than the general medical surgical patient population therefore any initiative to enhance an already comprehensive safety protocol for EMU patients is welcomed. Methods: 5W a medical‐surgical unit with an integrated EMU embraced the TCAB philosophy in June 2007. As just one TCAB innovation, RN Hourly Rounds specifically pertains to the iss"
pub.1049776127,Investigators' Workshop Sunday Evening Poster Session,"Kwee Thio*, N. Rensing*, S. Maloney†, D. Wozniak†, C. Xiong‡ and K. Yamada*
*Neurology, Washington University, St. Louis, MO; †Psychiatry, Washington University, St. Louis, MO and ‡Biostatistics, Washington University, St. Louis, MO Rationale: Ketogenic diets (KD) have anticonvulsant effects, but their effects on cognition are less clear. Therefore, we examined the behavioral performance of rats fed a KD on tests of locomotor activity and Pavlovian fear conditioning. We also examined in vivo medial perforant path long‐term potentiation (LTP) in rats fed a KD. Methods: Rats were fed a standard diet (SD), a KD, or a calorie‐restricted diet (CR) beginning on postnatal day (PD) 21. Rats on the ketogenic diet received 92% of their calories from fat. Rats on the CR served as a control for the slower weight gain experienced by KD fed rats. CR fed rats were given the same chow as the SD but were calorie restricted to allow their weight to match that of the rats fed the KD. On PD 35–60, a 1‐hour locomotor activity test and a conditioned fear (contextual or auditory cue) test were performed. On PD 42–45, in vivo median perforant path LTP was assessed by recording field excitatory postsynaptic potentials (fEPSPs) from the dentate gyrus before and after delivering a tetanus. Results: Rats fed the CR and KD showed slower weight gain, but only the KD fed rats had elevated serum β‐hydroxybutyrate levels as we have reported previously. KD and SD fed rats did not differ in performance on several activity‐related variables (total ambulations, rearing, time at rest). However, CR fed rats exhibited significant hyperactivity relative to the other two groups in terms of total ambulations (whole body movements), particularly during the last 4, 10‐minute blocks of testing (p < 0.0008). The CR fed rats also reared significantly more often than either of the other two groups during the fourth and fifth time blocks (p < 0.0008), and spent significantly less time at rest than the KD fed group (but not SD fed) over the 60 minute test (p = 0.002). KD and SD fed groups also performed similarly during the conditioned fear measures of baseline testing, tone‐shock training, or during contextual fear or auditory cue testing. Although no significant overall Treatment effects were found in amounts of freezing during the contextual fear test, only the CR fed rats did not show evidence of contextual cue conditioning relative to the pre‐test baseline. An ANOVA of the auditory cue data that yielded a significant Treatment by Time interaction, [F (14,280) = 2.13, p = 0.032], and subsequent significant contrasts showed that, on average across time, the CR fed group froze significantly less often than the KD or SD fed groups (p = 0.030 and 0.040, respectively). In vivo LTP in KD fed rats did not differ from rats fed the SD. CR fed rats exhibited LTP, but the magnitude of LTP in CR fed rats was smaller. A two‐way repeated measures ANOVA revealed a significant main effect of diet (p = 0.049)"
pub.1047811364,Poster Session III8:00 a.m.‐2:00 p.m.,"David Hsu 1 and M. Hsu 1 ( 1 Department of Neurology, University of Wisconsin – Madison, Madison, WI ) Rationale: Real brains learn by associative (Hebbian) processes, such that neurons that fire in the right sequence have the connection between them strengthened. Computational models of such systems, however, are highly unstable – they tend to develop either into overconnected systems that fire tonically, or they freeze into global silence. Thus real brains must have mechanisms for maintaining homeostasis of activity and connectivity. In particular, a specific level of connectivity can be defined such that information transmission and storage capacity are optimal. This level of connectivity is referred to as being critical. We describe a simple computational model that incorporates homeostasis of both activity and critical connectivity, and we show how these constraints lead to a natural interplay between the ability to learn, and the ability to develop seizures. Methods: A stochastic neural system is defined with nodes that can either fire spontaneously or after receiving excitatory input from another node. A simple scaling mechanism scales the spontaneous firing probability S(i) for node i and the connection strengths P(i,j) linking node j to node i either up or down depending on the state of activity and the state of connectivity of each node. The conditions for stability are investigated both analytically and by exhaustive simulations using LTP, LTD and STDP rules of learning. Results: (1) The spontaneous firing probability S contributes only 2–12% to the total activity; nonetheless, S must be greater than zero or else the system is unstable. (2) For systems that learn, firing rate homeostasis cannot guarantee homeostasis of connectivity. Homeostasis of connectivity is a separate principle. (3) Firing rate homeostasis is controlled by scaling of S. (4) Homeostasis of connectivity is controlled by scaling of P. (5) Scaling of P must be faster than scaling of S. (6) If S is perturbed away from its steady state value, then P will attempt to compensate. For instance, driving S to low levels causes P to rise to supercritical levels, and vice versa. (7) The post‐ictal and acute post‐deafferentation states result in levels of S that are below steady state values, and hence connectivity rises to supercritical levels lasting for many hours at a time. Conclusions: Requiring a neural learning system to maintain stable levels of activity and connectivity introduces strong constraints on its dynamics. These constraints are of intrinsic interest. For instance, that real brains are spontaneously active derives from our first condition of stability. Secondly, we find that the post‐ictal and acute post‐deafferentation states should be supercritical. Since prolonged supercriticality helps burn into memory hypersynchronous states, such states are epileptogenic. This is an example of maladaptive learning. Thirdly, we predict that increasing the rate of spontane"
pub.1175071193,Abstracts of papers accepted for presentation at the 17th International Symposium on Gastrointestinal Motility,"Some investigators reported the effects of caffeine on the gastrointestinal system. But few studies have investigated systematically the effects of caffeine on gastrointestinal motility. The aim of this study was to investigate the effects of caffeine on gastric and intestinal myoelectrical activity, and colonic motility in dogs. The study was performed in eight healthy female hound dogs (15–20kg) implanted with three pairs of electrodes on the serosa of the stomach along the great curvature at an interval of 4 cm, three in the proximal jejunum at an interval of 5cm, and one in the ascending colon. The protocol consisted of 60 min recording as baseline and 90 min recording after intravenous injection of a low dose of caffeine citrate (125mg), or a high dose of caffeine citrate (250mg). Spectral analyses were performed to compute the dominant frequency, the dominant power, the percentage of normal slow waves and the instability coefficient (minute‐by‐minute variation) of dominant frequency. Spike detection method developed in our lab was applied to calculate the number of spike bursts per minute and the total energy of spikes per minute from the colonic myoelectrical recording. All data were expressed with mean ± SE. 1) Gastric myoelectrical activity was not impaired by the administration of the low or high dose of caffeine. The percentages of normal 4–6 cpm gastric slow waves were 92.3±2.9%, 97.2±2.2% at baseline and 84.8± 11.7%, 96.4±1.8% after the injection of the low or high dose of caffeine respectively (p>0.05). 2) In the small bowel, the dominant frequency, the dominant power, the percentage of normal 17–22 cpm waves were not significantly affect by caffeine. 3) In the colon, however, there was a significant increase after the low dose of caffeine in the number of spike bursts per minute (6.69± 1.27 vs 8.22± 1.31. p>0.05). In the high dose of caffeine session, there was a similar significant increase in the number of spike bursts per minute (4.38±1.23 vs 6.92± 98, p<0.05). In addition, there was also a significant increase in the total energy of spikes per minute (92±21 vs 225±43, p<0.01). The increase of the total energy of spikes per minute after the injection of the high dose of caffeine was significantly higher than that after the injection of the low dose of caffeine (6.7±36 vs 1.334±39.2, p<0.05). Caffeine increases colonic contractile activity and its effect may be dose‐dependent. Gastric and jejunal myoelectrical activities are, however, not affected by caffeine. We first determined the pathways by which lower esophageal sphincter (LES) relaxation is evoked by stimulation of the dorsal motor nucleus of the vagus (DMV) in ferrets. Second, because GABAB receptor (r) activation modulates transient LES relaxations (Gastroenterology 116 :A1027. 1999), we determined whether GABABr is expressed in vagal motor neurons innervating the LES. L‐Glutamate was microinjected into the DMV in mid‐collicular decerebrate ferrets while recording LES, g"
pub.1031501982,Platform Sessions,"Monday July 9th, 2007
10:00 – 11:30
Room 209
Platform Session 1
Social Issues Shah U 1 , D'Souza C 2 , Shah P 1 , Saxena V 3 ( 1 Neurology Department, K.E.M. Hospital, Mumbai, India , 2 Indian Epilepsy Association, Mumbai, India , 3 Indian Epilepsy Association, Mumbai, India) Purpose: The Indian Epilepsy Association is lobbying for inclusion of chronic epilepsy under the Indian Disability Act. In several countries ‘intractability’ is a criterion for certification. A national survey was conducted to i) explore whether ‘intractability’ suffices to select the ‘disabled’; ii) determine appropriate criteria for disability certification. Method: Nine hundred and eight patients in 27 epilepsy clinics answered a questionnaire, reporting demographic, seizure and medication details and rated impact across various quality of life domains. Analysis involved: 1) dividing the sample on the basis of accepted intractability criteria and calculating the percentage of highly impacted people (disabling epilepsy); 2) dividing the sample on the basis of impact scores disability criteria and analysing differences; 3) eliciting predictor variables for impact. Results: 1) 153/908 (16.8%) were intractable, of whom 34/153 (22.2%) had high impact scores; of the 755 (83.2%) not intractable, 119/755 (15.8%) had high impact scores. 2) Significant differences (p < 0.05) emerged between the high and low impact group for education, occupation, marital status, seizure onset, duration, seizure control, medication and overall life satisfaction. 3) A significant regression model (adjusted R square = .207; F 11,580= 15.057, p < 0.0005) revealed low education (Beta =?.149, p < 0.0005), low work status (Beta =?.281, p < 0.0005), single marital status (Beta =?.200, p < 0.0005) and drug polytherapy (Beta =?173, p < 0.0005) to be significant predictor variables. Conclusion: Disability certification using intractability criteria alone may result in exclusion of people experiencing disabling epilepsy and inclusion of those not disabled. Disability rating should include demographic criteria in addition to seizure and medication criteria as these appear to be significant predictors of disabling epilepsy. Funded by the Indian Epilepsy Association 18th International Epilepsy Congress Trust and approved by the IEA‐IES Ethics Committee. Seshadri V 1 , Murthy J 1 , Ravi Raju C 2 ( 1 The Institute of Neurological Sciences, Care Hospital, Hyderabad, Andhra Pradesh, India , 2 The Institute of Neurological Sciences, Care Hospital, Hyderabad, Andhra Pradesh, India,) Purpose: In developing countries reasons for high treatment gap mainly are low purchasing capacity of the population and no access to appropriate medical care. Methods: All prevalence cases of epilepsy identified in 22 villages in a province in south India were registered in village medical‐center run by Byrraju Foundation, an organization working for rural transformation. Type of epilepsy and epilepsy syndrome was identified by the epilept"
