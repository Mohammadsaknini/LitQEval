id,title,abstract
pub.1065110490,Performances of endoscopic holography with a multicore optical fiber.,"A holographic setup that involves the use of a multicore optical fiber as an in situ recording medium has been developed. The hologram is transmitted to a CCD camera for electronic processing, and the image is reconstructed numerically, providing more flexibility to the holographic process. The performances of this imaging system have been evaluated in terms of the resolution limit and robustness relative to noise. The experimental cutoff frequency has been measured experimentally over a range of observation distances (4-10 mm) and presents a very good agreement with the predictions made by simulation. The system features a resolution of 5-µm objects for a 4-mm observation distance. The different sources of noise have been analyzed, and their influence on resolution has been proved to be nonrelevant."
pub.1127676459,Improving the Size Homogeneity of Multicore Superparamagnetic Iron Oxide Nanoparticles,"Superparamagnetic iron oxide nanoparticles (SPIONs) have been widely explored for use in many biomedical applications. Methods for synthesis of magnetic nanoparticle (MNP), however, typically yield multicore structures with broad size distribution, resulting in suboptimal and variable performance in vivo. In this study, a new method for sorting SPIONs by size, labeled diffusive magnetic fractionation (DMF), is introduced as an improvement over conventional magnetic field flow fractionation (MFFF). Unlike MFFF, which uses a constant magnetic field to capture particles, DMF utilizes a pulsed magnetic field approach that exploits size-dependent differences in the diffusivity and magnetic attractive force of SPIONs to yield more homogenous particle size distributions. To compare both methods, multicore SPIONs with a broad size distribution (polydispersity index (PdI) = 0.24 ± 0.05) were fractionated into nine different-sized SPION subpopulations, and the PdI values were compared. DMF provided significantly improved size separation compared to MFFF, with eight out of the nine fractionations having significantly lower PdI values (<i>p</i> value &lt; 0.01). Additionally, the DMF method showed a high particle recovery (&gt;95%), excellent reproducibility, and the potential for scale-up. Mathematical models were developed to enable optimization, and experimental results confirmed model predictions (<i>R</i><sup>2</sup> = 0.98)."
pub.1143714301,Accelerating Hyperparameter Tuning in Machine Learning for Alzheimer’s Disease With High Performance Computing,"Driven by massive datasets that comprise biomarkers from both blood and magnetic resonance imaging (MRI), the need for advanced learning algorithms and accelerator architectures, such as GPUs and FPGAs has increased. Machine learning (ML) methods have delivered remarkable prediction for the early diagnosis of Alzheimer's disease (AD). Although ML has improved accuracy of AD prediction, the requirement for the complexity of algorithms in ML increases, for example, hyperparameters tuning, which in turn, increases its computational complexity. Thus, accelerating high performance ML for AD is an important research challenge facing these fields. This work reports a multicore high performance support vector machine (SVM) hyperparameter tuning workflow with 100 times repeated 5-fold cross-validation for speeding up ML for AD. For demonstration and evaluation purposes, the high performance hyperparameter tuning model was applied to public MRI data for AD and included demographic factors such as age, sex and education. Results showed that computational efficiency increased by 96%, which helped to shed light on future diagnostic AD biomarker applications. The high performance hyperparameter tuning model can also be applied to other ML algorithms such as random forest, logistic regression, xgboost, etc."
pub.1106945484,Learning-Directed Dynamic Voltage and Frequency Scaling Scheme with Adjustable Performance for Single-Core and Multi-Core Embedded and Mobile Systems †,"Dynamic voltage and frequency scaling (DVFS) is a well-known method for saving energy consumption. Several DVFS studies have applied learning-based methods to implement the DVFS prediction model instead of complicated mathematical models. This paper proposes a lightweight learning-directed DVFS method that involves using counter propagation networks to sense and classify the task behavior and predict the best voltage/frequency setting for the system. An intelligent adjustment mechanism for performance is also provided to users under various performance requirements. The comparative experimental results of the proposed algorithms and other competitive techniques are evaluated on the NVIDIA JETSON Tegra K1 multicore platform and Intel PXA270 embedded platforms. The results demonstrate that the learning-directed DVFS method can accurately predict the suitable central processing unit (CPU) frequency, given the runtime statistical information of a running program, and achieve an energy savings rate up to 42%. Through this method, users can easily achieve effective energy consumption and performance by specifying the factors of performance loss."
pub.1045278923,Initial Biopsy Outcome Prediction—Head-to-Head Comparison of a Logistic Regression-Based Nomogram versus Artificial Neural Network,"OBJECTIVES: Nomograms and artificial neural networks (ANNs) represent alternative methodologic approaches to predict the probability of prostate cancer on initial biopsy. We hypothesized that, in a head-to-head comparison, one of the approaches might demonstrate better accuracy and performance characteristics than the other.
METHODS: A previously published nomogram, which relies on age, digital rectal examination, serum prostate-specific antigen (PSA), and percent-free PSA, and an ANN, which relies on the same predictors plus prostate volume, were applied to a cohort of 3980 men, who were subjected to multicore systematic prostate biopsy. The accuracy and the performance characteristics were compared between these two approaches.
RESULTS: The accuracy of the nomogram was 71% versus 67% for the ANN (p=0.0001). Graphical exploration of the performance characteristics demonstrated virtually perfect predictions for the nomogram. Conversely, the ANN underestimated the observed rate of prostate cancer.
CONCLUSIONS: A 4% increase in predictive accuracy implies that the use of the nomogram instead of the ANN will result in 40 additional patients who will be correctly classified between benign and cancer."
pub.1026528861,Using Heuristic Value Prediction and Dynamic Task Granularity Resizing to Improve Software Speculation,"Exploiting potential thread-level parallelism (TLP) is becoming the key factor to improving performance of programs on multicore or many-core systems. Among various kinds of parallel execution models, the software-based speculative parallel model has become a research focus due to its low cost, high efficiency, flexibility, and scalability. The performance of the guest program under the software-based speculative parallel execution model is closely related to the speculation accuracy, the control overhead, and the rollback overhead of the model. In this paper, we first analyzed the conventional speculative parallel model and presented an analytic model of its expectation of the overall overhead, then optimized the conventional model based on the analytic model, and finally proposed a novel speculative parallel model named HEUSPEC. The HEUSPEC model includes three key techniques, namely, the heuristic value prediction, the value based correctness checking, and the dynamic task granularity resizing. We have implemented the runtime system of the model in ANSI C language. The experiment results show that when the speedup of the HEUSPEC model can reach 2.20 on the average (15% higher than conventional model) when depth is equal to 3 and 4.51 on the average (12% higher than conventional model) when speculative depth is equal to 7. Besides, it shows good scalability and lower memory cost. "
pub.1030053471,Simulating Microbial Community Patterning Using Biocellion,"Mathematical modeling and computer simulation are important tools for understanding complex interactions between cells and their biotic and abiotic environment: similarities and differences between modeled and observed behavior provide the basis for hypothesis formation. Momeni et al. (Elife 2:e00230, 2013) investigated pattern formation in communities of yeast strains engaging in different types of ecological interactions, comparing the predictions of mathematical modeling, and simulation to actual patterns observed in wet-lab experiments. However, simulations of millions of cells in a three-dimensional community are extremely time consuming. One simulation run in MATLAB may take a week or longer, inhibiting exploration of the vast space of parameter combinations and assumptions. Improving the speed, scale, and accuracy of such simulations facilitates hypothesis formation and expedites discovery. Biocellion is a high-performance software framework for accelerating discrete agent-based simulation of biological systems with millions to trillions of cells. Simulations of comparable scale and accuracy to those taking a week of computer time using MATLAB require just hours using Biocellion on a multicore workstation. Biocellion further accelerates large scale, high resolution simulations using cluster computers by partitioning the work to run on multiple compute nodes. Biocellion targets computational biologists who have mathematical modeling backgrounds and basic C++ programming skills. This chapter describes the necessary steps to adapt the original Momeni et al.’s model to the Biocellion framework as a case study."
pub.1037699904,Precise contention-aware performance prediction on virtualized multicore system,"Multicore systems are widely deployed in both the embedded and the high end computing infrastructures. However, traditional virtualization systems can not effectively isolate shared micro architectural resources among virtual machines (VMs) running on multicore systems. CPU and memory intensive VMs contending for these resources will lead to serious performance interference, which makes virtualization systems less efficient and VM performance less stable. In this paper, we propose a contention-aware performance prediction model on the virtualized multicore systems to quantify the performance degradation of VMs. First, we identify the performance interference factors and design synthetic micro-benchmarks to obtain VM’s contention sensitivity and intensity features that are correlated with VM performance degradation. Second, based on the contention features, we build VM performance prediction model using machine learning techniques to quantify the precise levels of performance degradation. The proposed model can be used to optimize VM performance on multicore systems. Our experimental results show that the performance prediction model achieves high accuracy and the mean absolute error is 2.83%."
pub.1103338102,Challenges in Multicore Performance Predictions,"Software performance predictions are an established part of an engineering like software development process and therefore relevant to enable high quality and to ensure requirement fulfillment. Software Performance Engineers use for that model-based performance predictions approaches. However, current predictions approaches are based on the assumption of single core CPU systems. To enable Software Performance Engineers to further give accurate predictions also for multicore systems, which are by now state of the art, we need to adapt our current prediction models. On the poster, we discuss the upcoming challenges to be tackled to increase the accuracy of the performance predictions models."
pub.1094678398,Performance Prediction of Weather Forecasting Software on Multicore Systems,"Performance prediction is valuable in different areas of computing. The popularity of lease-based access to high performance computing resources particularly benefits from accurate performance prediction. Most contemporary processors are employing multiple computing cores, which complicates the task of performance prediction. In this paper, we describe the methodology employed for predicting the performance of a popular weather forecasting application on systems with between 4 and 256 processors. An average prediction error of less than 10% was achieved after testing on three different multi-node, multicore systems."
pub.1137162088,PPT-Multicore: Performance Prediction of OpenMP applications using Reuse Profiles and Analytical Modeling,"We present PPT-Multicore, an analytical model embedded in the Performance
Prediction Toolkit (PPT) to predict parallel application performance running on
a multicore processor. PPT-Multicore builds upon our previous work towards a
multicore cache model. We extract LLVM basic block labeled memory trace using
an architecture-independent LLVM-based instrumentation tool only once in an
application's lifetime. The model uses the memory trace and other parameters
from an instrumented sequentially executed binary. We use a probabilistic and
computationally efficient reuse profile to predict the cache hit rates and
runtimes of OpenMP programs' parallel sections. We model Intel's Broadwell,
Haswell, and AMD's Zen2 architectures and validate our framework using
different applications from PolyBench and PARSEC benchmark suites. The results
show that PPT-Multicore can predict cache hit rates with an overall average
error rate of 1.23% while predicting the runtime with an error rate of 9.08%."
pub.1139218404,PPT-Multicore: performance prediction of OpenMP applications using reuse profiles and analytical modeling,"We present PPT-Multicore, an analytical model embedded in the Performance Prediction Toolkit (PPT) to predict parallel applications’ performance running on a multicore processor. PPT-Multicore builds upon our previous work towards a multicore cache model. We extract LLVM basic block labeled memory trace using an architecture-independent LLVM-based instrumentation tool only once in an application’s lifetime. The model uses the memory trace and other parameters from an instrumented sequentially executed binary. We use probabilistic and computationally efficient reuse profiles to predict the cache hit rates and runtimes of OpenMP programs’ parallel sections. We model Intel’s Broadwell, Haswell, and AMD’s Zen2 architectures and validate our framework using different applications from PolyBench and PARSEC benchmark suites. The results show that PPT-Multicore can predict cache hit rates with an overall average error rate of 1.23% while predicting the runtime with an error rate of 9.08%."
pub.1122419695,Multicore Parallelism Exploration Targeting 3D-HEVC Intra-Frame Prediction,"Editor’s note: The usage of multicore architectures for 3D-HEVC intraframe prediction is of great benefit, especially to improve the performance, throughput, and even the energy consumption in systems. Multicore hardware is used in all high-performance image processing applications, and also in mobile devices such as cellphones. The distribution of the algorithm to different parallel processor architecture is discussed and evaluated in this article.--ichael Hübner, Brandenburg University of Technology"
pub.1101547396,"Parallelization, Modeling, and Performance Prediction in the Multi-/Many Core Area: A Systematic Literature Review","Context: Software developers face complex, connected, and large software projects. The development of such systems involves design decisions that directly impact the quality of the software. For an early decision making, software developers can use model-based prediction approaches for (non-)functional quality properties. Unfortunately, the accuracy of these approaches is challenged by newly introduced hardware features like multiple cores within a single CPU (multicores) and their dependence on shared memory and other shared resources. Objectives: Our goal is to understand whether and how existing model-based performance prediction approaches face this challenge. We plan to use gained insights as foundation for enriching existing prediction approaches with capabilities to predict systems running on multicores. Methods: We perform a Systematic Literature Review (SLR) to identify current model-based prediction approaches in the context of multicores. Results: Our SLR covers the software engineering, embedded systems, High Performance Computing, and Software Performance Engineering domains for which we examined 34 sources in detail. We found various performance prediction approaches which tries to increase prediction accuracy for multicore systems by including shared memory designs to the prediction models. Conclusion: However, our results show that the memory designs models are only in an initial phase. Further research has to be done to improve cache, memory, and memory bandwidth model as well as to include auto tuner support."
pub.1128914398,Multicore Performance Prediction with MPET,"Multicore processors serve as target platforms in a broad variety of applications ranging from high-performance computing to embedded mobile computing and automotive applications. But, the required parallel programming opens up a huge design space of parallelization strategies each with potential bottlenecks. Therefore, an early estimation of an application’s performance is a desirable development tool. However, out-of-order execution, superscalar instruction pipelines, as well as communication costs and (shared-) cache effects essentially influence the performance of parallel programs. While offering low modeling effort and good simulation speed, current approximate analytic models provide moderate prediction results so far. Virtual prototyping requires a time-consuming simulation, but produces better accuracy. Furthermore, even existing statistical methods often require detailed knowledge of the hardware for characterization. In this work, we present a concept called Multicore Performance Evaluation Tool (MPET) and its evaluation for a statistical approach for performance prediction based on abstract runtime parameters, which describe an application’s scalability behavior and can be extracted from profiles without user input. These scalability parameters not only include information on the interference of software demands and hardware capabilities, but indicate bottlenecks as well. Depending on the database setup, we achieve a competitive accuracy of 20% mean prediction error (11% median), which we also demonstrate in a case study."
pub.1105268399,RPPM: Rapid Performance Prediction of Multithreaded Applications on Multicore Hardware,"This paper proposes RPPM which, based on a microarchitecture-independent profile of a multithreaded application, predicts its performance on a previously unseen multicore platform. RPPM breaks up multithreaded program execution into epochs based on synchronization primitives, and then predicts per-epoch active execution times for each thread and synchronization overhead to arrive at a prediction for overall application performance. RPPM predicts performance within 12 percent on average (27 percent max error) compared to cycle-level simulation. We present a case study to illustrate that RPPM can be used for making accurate multicore design trade-offs early in the design cycle."
pub.1093969252,Leveraging on-chip networks for efficient prediction on multicore coherence,"Coherent data prediction is introduced as a promising architectural technique for reducing cache-to-cache accesses in directory protocol. However, limited on-chip resources cause the accuracy of current prediction to be generally low. Low accuracy would result in a large number of unnecessary or incorrect predictions, which would consequently generate excessive network traffic. This leads to large power and performance overhead for coherent memory access. This paper proposes an early abort mechanism (EBT) that leverages NoC design to reduce the negative effect of wrong prediction operations, thus facilitating overall performance improvement and traffic reduction. Using detailed full-system simulations, we conclude that EBT provides a cost-effective solution for designing efficient multicore processors. To the best of our knowledge, this study is the first to leverage on-chip network for the prediction optimization on multicore coherence."
pub.1099531185,Leveraging On-Chip Networks for Efficient Prediction on Multicore Coherence,"Coherent data prediction is introduced as a promising architectural technique for reducing cache-to-cache accesses in directory protocol. However, limited on-chip resources cause the accuracy of current prediction to be generally low. Low accuracy would result in a large number of unnecessary or incorrect predictions, which would consequently generate excessive network traffic. This leads to large power and performance overhead for coherent memory access. This paper proposes an early abort mechanism (EBT) that leverages NoC design to reduce the negative effect of wrong prediction operations, thus facilitating overall performance improvement and traffic reduction. Using detailed full-system simulations, we conclude that EBT provides a cost-effective solution for designing efficient multicore processors. To the best of our knowledge, this study is the first to leverage on-chip network for the prediction optimization on multicore coherence."
pub.1112113193,Contention-aware prediction for performance impact of task co-running in multicore computers,"In this paper, we investigate the influential factors that impact on the performance when the tasks are co-running on a multicore computers. Further, we propose the machine learning-based prediction framework to predict the performance of the co-running tasks. In particular, two prediction frameworks are developed for two types of task in our model: repetitive tasks (i.e., the tasks that arrive at the system repetitively) and new tasks (i.e., the task that are submitted to the system the first time).
 The difference between which is that we have the historical running information of the repetitive tasks while we do not have the prior knowledge about new tasks. Given the limited information of the new tasks, an online prediction framework is developed to predict the performance of co-running new tasks by sampling the performance events on the fly for a short period and then feeding the sampled results to the prediction framework. We conducted extensive experiments with the SPEC2006 benchmark suite to compare the effectiveness of different machine learning methods considered in this paper. The results show that our prediction model can achieve the accuracy of 99.38% and 87.18% for repetitive tasks and new tasks, respectively."
pub.1094509694,Keynote 3 (Banquet Talk) Digital space,"The multicore trend is universal. Spanning embedded processors, desktop CPUs and DSPs, supercomputers and cloud computing, multicore processors offer a game-changing opportunity for improvements in power efficiency and processing performance. More than anything else, multicores have put on-chip interconnect front and center in terms of design attention, since it has a first order impact on multicore performance, power efficiency, and even ease of programming. This talk will provide the inside scoop on our experiences with onchip interconnect in university research with the 16-core Raw multicore processor, in a commercial environment with Tilera's 64-core Tile processor, and conclude with some startling predictions for future 1000 core processors."
pub.1095769538,Towards Modelling Parallelism and Energy Performance of Multicore Systems,"Multicore systems are increasingly adopted across many application domains. Consequently, understanding their performance is becoming an important issue for a growing number of users. However, performance analysis of parallel programs on multicore systems is still challenging, especially for large programs or applications developed in multiple programming languages. This paper proposes an analytical modelling approach for studying the parallelism and energy performance of shared-memory programs on multicore systems. The proposed model derives the speedup and speedup loss from data dependency and memory overhead in traditional UMA and NUMA multicore systems, and emerging platforms such as ARM multicores. Using only widely available inputs derived from the trace of the operating system run-queue and hardware events counters, the proposed model achieves high practicality and generality across many types of sharedmemory programs running on different multicore platforms. Applications of the model include understanding achieved speedup and parallelism loss, and prediction of optimal core and memory configuration, where the optimality criteria is minimum execution time, minimum energy usage or a trade-off between these two."
pub.1107929137,A Survey of Prediction and Classification Techniques in Multicore Processor Systems,"In multicore processor systems, being able to accurately predict the future provides new optimization opportunities, which otherwise could not be exploited. For example, an oracle able to predict a certain application’s behavior running on a smart phone could direct the power manager to switch to appropriate dynamic voltage and frequency scaling modes that would guarantee minimum levels of desired performance while saving energy consumption and thereby prolonging battery life. Using predictions enables systems to become proactive rather than continue to operate in a reactive manner. This prediction-based proactive approach has become increasingly popular in the design and optimization of integrated circuits and of multicore processor systems. Prediction transforms from simple forecasting to sophisticated machine learning based prediction and classification that learns from existing data, employs data mining, and predicts future behavior. This can be exploited by novel optimization techniques that can span across all layers of the computing stack. In this survey paper, we present a discussion of the most popular techniques on prediction and classification in the general context of computing systems with emphasis on multicore processors. The paper is far from comprehensive, but, it will help the reader interested in employing prediction in optimization of multicore processor systems."
pub.1047284600,Integrated Coherence Prediction,"Multicore architectures with Network-on-Chips (NoCs) have been widely recognized as the de facto design for the efficient utilization of the continuously increasing density of transistors on a chip. A key challenge in designing such an NoC-based multicore processor is maintaining cache coherence in an efficient manner. Directory-based protocols avoid the bandwidth overhead of snoop-based protocols, therefore scaling to a large number of cores. However, conventional directory structures add significant indirection delay to cache-to-cache accesses in larger multicore processor. In this article we propose a novel hardware coherence technique, called integrated coherence prediction (ICP). This approach adopts a prediction technique for managing shared data to reduce or eliminate the cache-to-cache delay in coherence accesses. ICP has two unique features that differ from previous coherence prediction techniques. First, ICP introduces a new integrated prediction scheme that combines two kinds of predictors: owner predictor, which predicts the data writers and avoids the indirection through directory, and data predictor, which predicts the access address and prefetches data from remote nodes directly. Second, ICP uses a request replication method to reduce the negative effect of wrong owner prediction operations, thus facilitating overall performance improvement. We present the design and implementation details of the ICP approach. Using detailed full-system simulations, we conclude that the ICP provides a cost-effective solution for designing high-performance multicore processors."
pub.1124032962,Appropriate allocation of workloads on performance asymmetric multicore architectures via deep learning algorithms,"Asymmetric multicore processors (AMP) have become popular in both high-end and low-end computing systems due to its flexibility and high performance. A performance asymmetric multicore architecture (P-AMP) is the subcategory of AMP, which integrates the different micro-architecture cores in the same chip. Due to the heterogeneity nature of cores and applications, recognizing an optimal hardware configuration in terms of core, voltage-frequency pair for each application is still an NP-hard problem. Optimization of energy-delay product (EDP) is an additional challenging task in such architectures. To address these challenges, we developed a novel core prediction model called lightweight-deep neural network (LW-DNN) for asymmetric multicore processors. The proposed LW-DNN includes three phases, feature selection, feature optimization, and core prediction module. In the first and second phases, workload characteristics are extracted and optimized using the pre-processing algorithm and in the third phase, it predicts the appropriate cores for each workload at runtime to enhance the energy-efficiency and performance. We modeled a deep learning neural network using scikit-learn python library and evaluated in ODROID XU3 ARM big-Little performance asymmetric multicore platform. The embedded benchmarks we considered are MiBench, IoMT, Core-Mark workloads. The proposed LW-DNN prediction module compared with other traditional algorithms in terms of accuracy, execution time, energy consumption, and energy-delay product. The experimental results illustrate that accuracy achieved up to 97% in core prediction, and the average improvement in minimization of energy consumption is 33%, 35% in energy-delay product, 33% minimized in execution time correspondingly."
pub.1093363475,Complexity Analysis and Performance Evaluation of Matrix Product on Multicore Architectures,"The multicore revolution is underway. Classical algorithms must be revisited in order to take the hierarchical memory layout into account. In this paper, we aim at minimizing the number of cache misses paid during the execution of the matrix product kernel on a multicore processor, and we show how to achieve the best possible tradeoff between shared and distributed caches. Comprehensive simulation results confirm the analytical performance predictions and fully establish the practical significance of our new algorithms."
pub.1072433085,Timing Implications of Sharing Resources in Multicore Real-Time Automotive Systems,"The topic of timing has already been recognized as a major challenge when designing safety-critical automotive architectures. Consequently the availability of appropriate performance and timing analysis methods is key to building reliable automotive electric and electronics (E/E) and software architectures. Due to the potential performance increase, power reduction and cost-efficiency multicore solutions for automotive real-time environments receive growing attention. But the prediction of the timing behavior for multicore electronic control unit (ECU) systems becomes more complicated. Even in setups with static task-to-processor mapping, the execution of the tasks is usually not independent. The use of the same physical hardware, such as memories, coprocessors, or network components, makes inter-core interference unavoidable and may introduce hard-to-find timing problems including missed deadlines that can finally make the entire system fail. With this work we anticipate new issues on the timing prediction in the upcoming automotive multicore ECUs, we discuss difficulties and solutions in the evolution from distributed single-core systems to multicore systems and also provide a modeling and analysis approach of complex real-time multicore systems with shared resources."
pub.1093624678,How to Implement Effective Prediction and Forwarding for Fusable Dynamic Multicore Architectures,"Dynamic multicore architectures, that fuse and split cores at run time, potentially offer a level of performance/energy agility that static multicore designs cannot achieve. Conventional ISAs, however, have scalability limits to fusion. EDGE-based designs offer greater scalability but to date have been performance limited by significant microarchitectural bottlenecks. This paper addresses these issues and makes three major contributions. First, it proposes Iterative Path Prediction to address low next block prediction accuracy and low speculation rates. It achieves close to taken/not-taken prediction accuracy for multi-exit instruction blocks while also speculating the predicated execution path within the block. Second, the paper proposes Exposed Operand Broadcasts to address the overhead of operand delivery for high fanout instructions by exposing a small number of broadcast operands in the ISA. Third, we present a scalable composable architecture called $T3$ that uses these mechanisms and show it can operate across a wide range of power and performance spectrum by increasing energy efficiency and performance significantly. Compared to previous EDGE designs, $T3$ improves energy efficiency by about $2x$ and performance by up to 50%."
pub.1113269204,Performance-influencing Factors for Parallel and Algorithmic Problems in Multicore Environments,"Model-based approaches in Software Performance Engineering (SPE) are used in early design phases to evaluate performance. Most current model-based prediction approaches work quite well for single-core CPUs but are not suitable or precise enough for multicore environments. This is because they only consider a single metric (i.e., the CPU speed) as a factor affecting performance. Therefore, we investigate parallel-performance-influencing factors (PPIFs) as a preparing step to improve current performance prediction models by providing references curves for the speedup behaviour of different resource demands and scenarios. In this paper, we show initial results and their relevance for future work."
pub.1083764838,Prediction complexity-based HEVC parallel processing for asymmetric multicores,"This paper proposes a novel Tile allocation method considering the computational ability of asymmetric multicores as well as the computational complexity of each Tile. This paper measures the computational ability of asymmetric multicores in advance, and measures the computational complexity of each Tile by using the amount of HEVC prediction unit (PU) partitioning. The implemented system counts and sorts the amount of PU partitions of each Tile, and also allocates Tiles to asymmetric big.LITTLE cores according to their expected computational complexity. When experiments were conducted, the amount of PU partitioning and the computational complexity (decoding time) showed a close correlation, and average performance gains of decoding time with the proposed adaptive allocation were around 36 % with 12 Tiles, 28 % with 18 Tiles, and 31 % with 24 Tiles, respectively."
pub.1113712257,RPPM: Rapid Performance Prediction of Multithreaded Workloads on Multicore Processors,"Analytical performance modeling is a useful complement to detailed cycle-level simulation to quickly explore the design space in an early design stage. Mechanistic analytical modeling is particularly interesting as it provides deep insight and does not require expensive offline profiling as empirical modeling. Previous work in mechanistic analytical modeling, unfortunately, is limited to single-threaded applications running on single-core processors. This work proposes RPPM, a mechanistic analytical performance model for multi-threaded applications on multicore hardware. RPPM collects microarchitecture-independent characteristics of a multi-threaded workload to predict performance on a previously unseen multicore architecture. The profile needs to be collected only once to predict a range of processor architectures. We evaluate RPPM's accuracy against simulation and report a performance prediction error of 11.2% on average (23% max). We demonstrate RPPM's usefulness for conducting design space exploration experiments as well as for analyzing parallel application performance."
pub.1143626723,Prediction of multicore CPU performance through parallel data mining on public datasets,"In the present scenario, high-performance computing needs more attention towards multicore computing. While designing the CPU, we need to consider hardware for processing speed, cache bandwidth, minimum memory requirements, etc. So for the selection of the best combination of CPU data mining tools may play an important role. In this era, data mining attracts more interest on parallel computing to enhance the performance of multicores. This paper demonstrates a parallel strategy similar to the traditional parallel programming paradigm to improve the performance of multicores by using a data mining approach. We have selected one approach EM with Gaussian and analyze the impact of its parallel execution on selected multicore clusters obtained through data mining. We have also evaluated our finding with a virtual environment of having 32 different families of CPUs, showing a speedup of up to ≈ 1 . 02 x . This paper considers data clusters, cache mapping techniques and ranking of MPI programming techniques."
pub.1093932787,Thread Mapping using System-level Throughput Prediction Model for Shared Memory Multicores,"The primary purpose of the current paper is to design a fast and accurate performance model framework for exploring various thread-to-core mapping strategies (MS) and estimating steady state cycles per instruction (CPI). It is directed towards efficiently exploring these performance metrics for large parallel applications for shared memory multicores. This work establishes a hybrid Markov Chain Model (MCM) and Model Tree (MT) based system-level performance prediction model framework. The model is validated with an Electromagnetics application for 12 different mapping strategies. The average performance prediction error is 0.168% with standard deviation of 3.866%. The total run time of model is of the order of minutes, whereas the actual application execution time is in terms of several days."
pub.1001011801,Power-aware predictive models of hybrid (MPI/OpenMP) scientific applications on multicore systems,"Predictive models enable a better understanding of the performance characteristics of applications on multicore systems. Previous work has utilized performance counters in a system-centered approach to model power consumption for the system, CPU, and memory components. Often, these approaches use the same group of counters across different applications. In contrast, we develop application-centric models (based upon performance counters) for the runtime and power consumption of the system, CPU, and memory components. Our work analyzes four Hybrid (MPI/OpenMP) applications: the NAS Parallel Multizone Benchmarks (BT-MZ, SP-MZ, LU-MZ) and a Gyrokinetic Toroidal Code, GTC. Our models show that cache utilization (L1/L2), branch instructions, TLB data misses, and system resource stalls affect the performance of each application and performance component differently. We show that the L2 total cache hits counter affects performance across all applications. The models are validated for the system and component power measurements with an error rate less than 3%."
pub.1002113406,What to expect when you are consolidating: effective prediction models of application performance on multicores,"Consolidation of multiple applications with diverse and changing resource requirements is common in multicore systems as hardware resources are abundant. As opportunities for better system usage become ample, so are opportunities to degrade individual application performances due to unregulated performance interference between applications and system resources. Can we predict a performance region within which application performance is expected to lie under different consolidations? Alternatively, can we maximize resource utilization while maintaining individual application performance targets? In this work we provide a methodology that offers answers to the above difficult questions by constructing a queueing-theory based tool that can be used to accurately predict application scalability on multicores. The tool can also provide the optimal consolidation suggestions to maximize system resource utilization while meeting application performance targets. The proposed methodology is based on asymptotic analysis that can quickly provide a range of performance values that the user should expect under various consolidation scenarios. In addition, when more accurate performance forecasting is needed, the methodology can provide more accurate predictions using approximate mean value analysis. The methodology is light-weight as it relies on capturing application resource demands using standard system monitoring, via non-intrusive low-level measurements.We evaluate our approach on an IBM Power7 system using the DaCapo and SPECjvm2008 benchmark suites. From 900 different consolidations of application instances, our tool accurately predicts the average iteration time of collocated applications with an average error below 9 per cent. Experimental and analytical results are in excellent agreement, confirming the robustness of the proposed methodology in suggesting the best consolidations that meet given performance objectives of individual applications while maximizing system resource utilization."
pub.1172877101,A multi objective DB-RNN based core prediction and resource allocation scheme for multicore processors,"Asymmetric Multicore Processors (AMP) are widely used in both advanced and basic computing systems due to their fundamental flexibility and outstanding computing possibilities. Among the subcategories of AMP, Performance Asymmetric Multicore (PAM) Architectures are unique in that they include various micro-architecture cores into a single chip. Nevertheless, the complex interplay between heterogeneous cores and a diverse range of applications presents a formidable challenge in determining the optimal hardware configuration, encompassing core selection and reduced energy consumption for each application. To tackle these multifaceted problems, this paper introduces a pioneering model for core prediction and resource allocation based on the Dual Branch Recurrent Neural Network (DB-RNN), specifically for AMP. The DB-RNN model encompasses weight sharing facilitated by a hybrid optimization algorithm named African Vulture with Aquila Optimizer (AVAO), and a core prediction module. The proposed model evaluates the Energy-Delay Product (EDP) to find the performance of each core. In the final phase, DB-RNN dynamically predicts the most suitable cores for individual workloads at runtime, thereby elevating both energy efficiency and overall system performance. The experimental results show that the model achieved prediction accuracy up to 98.76 %. This innovative approach paves the way for enhanced efficiency and performance in PAM systems."
pub.1126750653,WRF Performance Analysis and Scalability on Multicore High Performance Computing Systems,"Weather Research and Forecast (WRF) is one of the most commonly used numerical weather prediction models that has superior scalability and computational efficiency. The WRF model is one of the most commonly used Numerical Weather Prediction model that is designed to run on a variety of platforms, either serially or in parallel, with or without multi‐threading. WRF model performance benchmarking has been done within different environments to demonstrate the scalability of the computational environment and considerations for higher productivity. The chapter discusses WRF performance analysis and scalability on a multicore high performance computing system using a benchmarking configuration. It explains the use of WRF version 3.7 for a tropical domain in Southeast Asia, dominated by convective meteorology conditions."
pub.1116874428,Empirical model-based performance prediction for application mapping on multicore architectures,"Application mapping in multicore embedded systems plays a central role in their energy-efficiency. The present paper deals with this issue by focusing on the prediction of performance and energy consumption, induced by task and data allocation on computing resources. It proposes a solution by answering three fundamental questions as follows: (i) how to encode mappings for training performance prediction models? (ii) how to define an adequate criterion for assessing the quality of mapping performance predictors? and (iii) which technique among regression and classification enables the best predictions? Here, the prediction models are obtained by applying carefully selected supervised machine learning techniques on raw data, generated off-line from system executions. These techniques are Support Vector Machines, Adaptive Boosting (AdaBoost) and Artificial Neural Networks (ANNs). Our study is validated on an automotive application case study. The experimental results show that with a limited set of training information, AdaBoost and ANNs can provide very good outcomes (up to 84.8% and 89.05% correct prediction score in some cases, respectively), making them attractive enough for the addressed problem."
pub.1095739370,Coherent Profiles: Enabling Efficient Reuse Distance Analysis of Multicore Scaling for Loop-based Parallel Programs,"Reuse distance (RD) analysis is a powerful memory analysis tool that can potentially help architects study multicore processor scaling. One key obstacle though is multicore RD analysis requires measuring concurrent reuse distance (CRD) profiles across thread-interleaved memory reference streams. Sensitivity to memory interleaving makes CRD profiles architecture dependent, preventing them from analyzing different processor configurations. For loop-based parallel programs, CRD profiles shift coherently to larger CRD values with core count scaling because interleaving threads are symmetric. Simple techniques can predict such shifting, making the analysis of numerous multicore configurations from a small set of CRD profiles feasible. Given the ubiquity and scalability of loop-level parallelism, such techniques will be extremely valuable for studvinz future larue multicore desizns. This paper investigates using RD analysis to efficiently analyze multicore cache performance for loop-based parallel programs, making several contributions. First, we provide in-depth analysis on how CRD profiles change with core count scaling. Second, we develop techniques to predict CRD profile scaling, in particular employing reference groups [1] to predict coherent shift, and evaluate prediction accuracy. Third, we show core count scaling only degrades performance for last-level caches (LLCs) below 16MB for our benchmarks and problem sizes, increasing to 64–128MB if problem size scales by 64x. Finally, we apply CRD profiles to analyze multi core cache performance. When combined with existing problem scaling prediction, our techniques can predict LLC MPKI to within 11.1 % of simulation across 1,728 configurations using only 36 measured CRD profiles."
pub.1048156108,Multicore Embedded Systems: The Timing Problem and Possible Solutions,"Today’s processor chips contain often multiple CPUs i.e. processor cores each of which may support several hardware threads working in parallel. They are known as multicore or many-core processors. As a consequence of the broad introduction of multicore into computing, almost all software must exploit parallelism to make the most efficient use of on-chip resources including processor cores, caches and memory bandwidth. For embedded applications, it is predicted that multicores will be increasingly used in future embedded systems for high performance and low energy consumption. The major obstacle is that due to on-chip resource contention, the prediction of system performance, latencies, and resource utilization in multicore systems becomes a much harder task than that for single-core systems. With the current technology we may not predict and provide any guarantee on real-time properties of multicore software, which restricts seriously the use of multicores for embedded applications.In this talk, I will give an overview on the key challenges for software development on multicore architecture and briefly introduce the CoDeR-MP project at Uppsala to develop high-performance and predictable real-time software on multicore platforms. I will present the multicore timing analysis problem and our solutions proposed in a series of recent work. Technical details may be found in [LNYY10] on combining abstract interpretation and model checking for multicore WCET analysis, [GSYY09a] dealing with shared caches, [GSYY09b] on response time analysis for multicore systems, and [GSYY10] extending Layland and Liu’s classical result [LL73] on rate monotonic scheduling for single-core systems to multicore systems."
pub.1008337434,DARP-MP,"
                    In this article, we demonstrate that the sensitized path delays in various microprocessor pipe stages exhibit intriguing temporal and spatial variations during the execution of real-world applications. To effectively exploit these delay variations, we propose
                    dynamically adaptable resilient pipeline
                    (DARP)—a series of runtime techniques to boost power-performance efficiency and fault tolerance in a pipelined microprocessor. DARP employs early error prediction to avoid a major portion of the timing errors. We combine DARP with the state-of-art
                    topologically homogeneous and power-performance heterogeneous
                    (THPH) architecture to build up a new frontier for the energy efficiency of multicore processors (DARP-MP). Using a rigorous circuit-architectural infrastructure, we demonstrate that DARP substantially improves the multicore processor performance (9.4--20%) and energy efficiency (10--28.6%) compared to state-of-the-art techniques. The energy-efficiency improvements of DARP-MP are 42% and 49.9% compared against the original THPH and another state-of-art multicore power management scheme, respectively.
                  "
pub.1062907524,SIMPLE PERFORMANCE BOUNDS FOR MULTICORE AND PARALLEL CHANNEL SYSTEMS,"A simple modification of existing divisible load scheduling algorithms, boosting link speed by M for M parallel channels per link, allows time optimal load scheduling and performance prediction for parallel channel systems. The situation for multicore models is more complex but can be handled by a substitution involving equivalent processor speed. These modifications yield upper bounds on such parallel systems' performance. This concept is illustrated for ideal single level (star) tree networks under a variety of scheduling policies. Less than ideal parallelism can also be modeled though mechanisms of inefficiency require further research."
pub.1115601708,A Library-based Performance Tool for Multicore Pervasive Servers,"This paper proposes SPLiT (Scalable Performance Library Tool) as the methodology to improve performance of applications on multicore processors through CPU and cache optimizations on the fly. SPLiT is designed to relieve the difficulty of the performance optimization of parallel applications on multicore processors. Therefore, all programmers have to do to benefit from SPLiT is to add a few library calls to let SPLiT know which part of the application should be analyzed. This simple but compelling optimization library contributes to enrich pervasive servers on a multicore processor, which is a strong candidate for an architecture of information appliances in the near future. SPLiT analyzes and predicts application behaviors based on CPU cycle counts and cache misses. According to the analysis and predictions, SPLiT tries to allocate processes and threads sharing data onto the same physical cores in order to enhance cache efficiency. SPLiT also tries to separate cache effective codes from the codes with more cache misses for the purpose of the avoidance of cache pollutions, which result in performance degradation. Empirical experiments assuming web applications validated the efficiency of SPLiT and the performance of the web application is improved by 26%."
pub.1093275743,Performance Evaluation of Multicore Systems: From Traffic Analysis to Latency Predictions (Embedded Tutorial),"As technology scaling down allows multiple processing components to be integrated on a single chip, the modern computing systems led to the advent of Multiprocessor System-on-Chip (MPSoC) and Chip Multiprocessor (CMP) design. Network-on-Chips (NoCs) have been proposed as a promising solution to tackle the complex on-chip communication problems on these multicore platforms. In order to optimize the NoC-based multicore system design, it is essential to evaluate the NoC performance with respect to numerous configurations in a large design space. Taking the traffic characteristics into account and using an appropriate latency model become crucially important to provide an accurate and fast evaluation. In this tutorial, we survey the current progresses in these aspects. We first review the NoC workload modeling and traffic analysis techniques. Then, we discuss the mathematical formalisms of evaluating the performance under a given traffic model, for both the average and worst-case latency predictions. Finally, the advantages of combining the analytical and simulation-based techniques are discussed and new attempts for bridging these two approaches are reviewed."
pub.1083911408,Towards performance prediction of multicore software routers,"Summary  Rapid advances in network function virtualization technologies have led to the emergence of flexible and scalable embedded data plane processing functions, eg, packet forwarding, on commodity hardware. However, performance prediction of softwarized network functions on such shared resources is challenging but very important for obtaining the full benefits of network function virtualization. This paper addresses the problem of performance prediction for multicore software routers and reveals a key technique to achieve high accuracy. Motivated by observations, we first analyze how many CPU cycles are spent for forwarding a packet in multicore processing systems. Our prediction model of the CPU usage based on cache contention can capture its nonlinear dilation scaled by the number of CPU cores–called dilated CPU consumption (DCC). We validate the accuracy of the DCC model with measured data. On the basis of the DCC model, we develop 2 performance prediction algorithms to predict the maximum throughput of the packet‐forwarding function corresponding to the assigned resources. The first algorithm includes CPU utilization statistics (called DCC‐u ), while its simplified version (called sDCC ) does not require CPU utilization statistics. We validate the proposed models through the exhaustive set of experiments under various amounts of assigned resources (ie, CPU speeds and the number of CPU cores) and different traffic loads (ie, a large and small number of traffic flows). The results show that DCC‐u and sDCC achieve high accuracy under various amounts of resources and traffic conditions. Remarkably, they improve the precision of estimation from that of the existing techniques by up to 105% to 163%. "
pub.1025488912,A Library-based Performance Tool for Multicore Pervasive Servers,"This paper proposes SPLiT (Scalable Performance Library Tool) as the methodology to improve performance of applications on multicore processors through CPU and cache optimizations on the fly. SPLiT is designed to relieve the difficulty of the performance optimization of parallel applications on multicore processors. Therefore, all programmers have to do to benefit from SPLiT is to add a few library calls to let SPLiT know which part of the application should be analyzed. This simple but compelling optimization library contributes to enrich pervasive servers on a multicore processor, which is a strong candidate for an architecture of information appliances in the near future. SPLiT analyzes and predicts application behaviors based on CPU cycle counts and cache misses. According to the analysis and predictions, SPLiT tries to allocate processes and threads sharing data onto the same physical cores in order to enhance cache efficiency. SPLiT also tries to separate cache effective codes from the codes with more cache misses for the purpose of the avoidance of cache pollutions, which result in performance degradation. Empirical experiments assuming web applications validated the efficiency of SPLiT and the performance of the web application is improved by 26%."
pub.1132933547,A Cache Contention-aware Run-time Scheduling for Power-constrained Asymmetric Multicore Processors,"Asymmetric multicore architecture is widely applied to the embedded systems to better trade-off performance and energy consumption. With an increased number of applications concurrently executed in the system, the power consumption and the associated last-level cache latency are increased. To maximize the system performance under the power constraint, we proposed a cache contention-aware run-time scheduling for asymmetric multicore systems. To deal with the dynamic workload and cache contention effect, the CPI model learning is presented to adjust the relation between system performance, executing frequency, and executing clusters. Based on the CPI model prediction, the run-time dispatcher is then presented to determine the executing frequency and cores to maximize system throughput under power constraint. The proposed algorithm was implemented on the commercial Odroid XU4 board. The performance was evaluated using benchmarks and impressive results were obtained."
pub.1095683803,Chunk-Wise Parallelization Based on Dynamic Performance Prediction on Heterogeneous Multicores,"Multicore machines are becoming more and more common. Ideally, all applications benefit from these advances in computer architecture. A complex challenge in parallel computing is cores load balancing to minimize the overall execution time called Makespan of the parallel program. As multicores may have different architectures, an effective mapping should support this unknown variation to avoid drawbacks on makespan. In fact, mapping or static load balancing method may not be effective when the target state machine changes during program execution. Thread affinity has appeared as an important technique to improve the program performance and for better performance stability. In this context, we propose a predictive approach using iterations chunking at runtime allowing parallel code adaptation to processor's performance. Our approach is based on thread pinning and performance detection at execution time. From a parallel program, we define a set of loop nest iterations, forming what is called chunk, and we run it using a first mapping assuming homogeneous cores. Then, performance assessment would correct mapping by speculating the future core's state. The new mapping would be then applied to a new chunk for further evaluation and prediction. The process would stop when the program is fully executed or when judging that chunking is no longer effective."
pub.1093775634,Performance Modeling of Shared Memory Multiple Issue Multicore Machines,"The process of developing optimal parallel applications is computationally expensive. The goal of this work is to design and validate a Markov chain based system-level performance prediction models to efficiently optimize parallel applications on shared memory multicore processors with coarse-grain thread level parallelism (TLP) like Intel Xeon Clovertown. In Markov chain based throughput prediction model, the machine micro-architecture is represented by the different states and the allowable transitions. The program characteristics (such as cache misses, branch misprediction, division, denormalized computations and other large latency operations) are included using the failure probabilities of active and suspended threads. The improvement in performance is achieved by extracting information from running a representative data-set of the actual application. The model is validated with multiple benchmarks (electromagnetics application, parallel BZIP, FFT etc.) using VTune - Intel's performance analyzer. The average performance prediction error is less than 10%. The total run time for model is of the order of minutes (including VTune analyzer measurement timings), whereas the actual application is in terms of few hours."
pub.1094196245,"Providing Fairness in Heterogeneous Multicores with a Predictive, Adaptive Scheduler","Multicore applications contend for resources — especially memory bandwidth — reducing both quality-of-service and overall system performance. Contention-aware schedulers have been proposed to provide fairness and predictable behavior through thread-level scheduling. Prior approaches have two drawbacks, however. First, many introduce overhead that reduces overall performance. Second, the emergence of heterogeneous multicores has made handling contention and providing fairness much more difficult as the scheduler must now account for both application interference and the performance effects of different core types. This paper proposes augmenting existing contention-aware approaches with predictive and adaptive components to provide fair memory access and performance improvements on heterogeneous multicores. The predictive component's closed-loop approach anticipates how different processes will perform with different core types, while the adaptive component dynamically tunes key scheduling parameters to the current workload. We implement and evaluate this approach on a real Linux/x86 system with a variety of memory and compute intensive benchmarks. We find that adding prediction improves fairness and performance by 38% and 4% (respectively) compared to a prior state-of-the-art contention-aware approach. The addition of adaptation allows users to select for fairness or performance optimization, providing an additional 24% improvement in fairness or a 9% improvement in performance beyond the predictive approach."
pub.1120034760,Modeling Shared Cache Performance of OpenMP Programs using Reuse Distance,"Performance modeling of parallel applications on multicore computers remains
a challenge in computational co-design due to the complex design of multicore
processors including private and shared memory hierarchies. We present a
Scalable Analytical Shared Memory Model to predict the performance of parallel
applications that runs on a multicore computer and shares the same level of
cache in the hierarchy. This model uses a computationally efficient,
probabilistic method to predict the reuse distance profiles, where reuse
distance is a hardware architecture-independent measure of the patterns of
virtual memory accesses. It relies on a stochastic, static basic block-level
analysis of reuse profiles measured from the memory traces of applications ran
sequentially on small instances rather than using a multi-threaded trace. The
results indicate that the hit-rate predictions on the shared cache are
accurate."
pub.1061280330,Core-Level Activity Prediction for Multicore Power Management,"Existing power management techniques operate by reducing performance capacity (frequency, voltage, size) when performance demand is low. In the case of multicore systems, the performance and power demand is the aggregate demand of all cores in the system. Monitoring aggregate demand makes detection of phase changes difficult since aggregate phase behavior obscures the underlying phases generated by the workloads on individual cores. This causes suboptimal power management and over-provisioning of power resources. In this paper, we address these problems through core-level, activity prediction. The core-level view makes detection of phase changes more accurate, yielding more opportunities for efficient power management. Due to the difficulty in anticipating activity level changes, existing operating system power management strategies rely on reaction rather than prediction. This causes sub-optimal power and performance since changes in performance capacity by the power manager lag changes in performance demand. To address this problem we propose the periodic power phase predictor (PPPP). This activity level predictor decreases SYSMark 2007 processor power consumption by 5.4% and increases performance by 3.8% compared to the reactive scheme used in Windows Vista operating system. Applying the predictor to the prediction of processor power, we improve accuracy by 4.8% compared to a reactive scheme."
pub.1027510650,GTfold,"The prediction of the correct secondary structures of large RNAs is one of the unsolved challenges of computational molecular biology. Among the major obstacles is the fact that accurate calculations scale as O(n4), so the computational requirements become prohibitive as the length increases. Existing folding programs implement heuristics and approximations to overcome these limitations. We present a new parallel multicore and scalable program called GTfold, which is one to two orders of magnitude faster than the de facto standard programs and achieves comparable accuracy of prediction. Development of GTfold opens up a new path for the algorithmic improvements and application of an improved thermodynamic model to increase the prediction accuracy. In this paper we analyze the algorithm's concurrency and describe the parallelism for a shared memory environment such as a symmetric multiprocessor or multicore chip. In a remarkable demonstration, GTfold now optimally folds 11 picornaviral RNA sequences ranging from 7100 to 8200 nucleotides in 8 minutes, compared with the two months it took in a previous study. We are seeing a paradigm shift to multicore chips and parallelism must be explicitly addressed to continue gaining performance with each new generation of systems. We also show that the exact algorithms like internal loop speedup can be implemented with our method in an affordable amount of time. GTfold is freely available as open source from our website."
pub.1047892566,Efficient Reuse Distance Analysis of Multicore Scaling for Loop-Based Parallel Programs,"
                    Reuse Distance (RD) analysis is a powerful memory analysis tool that can potentially help architects study multicore processor scaling. One key obstacle, however, is that multicore RD analysis requires measuring
                    Concurrent Reuse Distance
                    (CRD) and
                    Private-LRU-stack Reuse Distance
                    (PRD) profiles across thread-interleaved memory reference streams. Sensitivity to memory interleaving makes CRD and PRD profiles architecture dependent, preventing them from analyzing different processor configurations. For loop-based parallel programs, CRD and PRD profiles
                    shift coherently
                    across RD values with core count scaling because interleaving threads are symmetric. Simple techniques can predict such shifting, making the analysis of numerous multicore configurations from a small set of CRD and PRD profiles feasible. Given the ubiquity of parallel loops, such techniques will be extremely valuable for studying future large multicore designs.
                  
                  
                    This article investigates using RD analysis to efficiently analyze multicore cache performance for loop-based parallel programs, making several contributions. First, we provide an in-depth analysis on how CRD and PRD profiles change with core count scaling. Second, we develop techniques to predict CRD and PRD profile scaling, in particular employing reference groups [Zhong et al. 2003] to predict coherent shift, demonstrating 90% or greater prediction accuracy. Third, our CRD and PRD profile analyses define two application parameters with architectural implications:
                    C
                    core
                    is the minimum shared cache capacity that “contains” locality degradation due to core count scaling, and
                    C
                    share
                    is the capacity at which shared caches begin to provide a cache-miss reduction compared to private caches. And fourth, we apply CRD and PRD profiles to analyze multicore cache performance. When combined with existing problem scaling prediction, our techniques can predict shared LLC MPKI (private L2 cache MPKI) to within 10.7% (13.9%) of simulation across 1,728 (1,440) configurations using only 36 measured CRD (PRD) profiles.
                  "
pub.1119002114,On the accuracy and usefulness of analytic energy models for contemporary multicore processors,"This paper presents refinements to the execution-cache-memory performance
model and a previously published power model for multicore processors. The
combination of both enables a very accurate prediction of performance and
energy consumption of contemporary multicore processors as a function of
relevant parameters such as number of active cores as well as core and Uncore
frequencies. Model validation is performed on the Sandy Bridge-EP and
Broadwell-EP microarchitectures. Production-related variations in chip quality
are demonstrated through a statistical analysis of the fit parameters obtained
on one hundred Broadwell-EP CPUs of the same model. Insights from the models
are used to explain the performance- and energy-related behavior of the
processors for scalable as well as saturating (i.e., memory-bound) codes. In
the process we demonstrate the models' capability to identify optimal operating
points with respect to highest performance, lowest energy-to-solution, and
lowest energy-delay product and identify a set of best practices for
energy-efficient execution."
pub.1120883299,Statistical Performance Prediction for Multicore Applications Based on Scalability Characteristics,"Multicore processors serve as target platforms in a broad variety of applications ranging from high-performance computing to embedded mobile computing and automotive. But, the required parallel programming opens up a huge design space of parallelization strategies each with potential bottlenecks. Therefore, an early estimation of an application’s performance is a desirable development tool. However, out-of-order execution, superscalar instruction pipelines, as well as communication costs and (shared-) cache effects essentially influence the performance of parallel programs. While offering a good modeling and simulation speed, analytic models provide moderate prediction results so far. Virtual prototyping requires a time-consuming simulation, but produces better accuracy. Furthermore, even existing statistical methods often require detailed knowledge of the hardware for characterization. In this work, we present a concept and its evaluation for a statistical approach for performance prediction based on abstract runtime parameters, which describe an application’s scalability behavior and can be extracted from profiles without user input. These scalability parameters not only include information on the interference of software demands and hardware capabilities, but indicate bottlenecks as well. Depending on the database setup, we achieve a competitive accuracy of 20 % mean prediction error (11 % median), which we also proof in a case study."
pub.1126929911,Perspectives on Multicore Architectures,"One of the enabling trends for the move to multicore architectures has been the increasing miniaturization of transistors, through which more and more transistors can be packed in a single die. The pace for this transistor integration has been staggering. For decades, it has followed a prediction made by Intel co-founder Gordon Moore in 1965 that the number of transistors that can be manufactured inexpensively in a single integrated circuit would double every two years. Besides the power wall, there are other related challenges for designing future multicore architectures. One of them is off-chip bandwidth. In traditional non-multicore multiprocessor systems, adding more processors typically involves adding more nodes, and since a node consists of a processor and its memory, the aggregate bandwidth to main memory increases with more processors. Another factor is that if performance can be improved in future multicore architectures substantially, then the bandwidth wall problem will become worse."
pub.1127989483,Multicore Performance Prediction – Comparing Three Recent Approaches in a Case Study,"Even though parallel programs, written in high-level languages, are portable across different architectures, their parallelism does not necessarily scale after migration. Predicting a multicore-application’s performance on the target platform in an early development phase can prevent developers from unpromising optimizations and thus significantly reduce development time. However, the vast diversity and heterogeneity of system-design decisions of processor types from HPC and desktop PCs to embedded MPSoCs complicate the modeling due to varying capabilities. Concurrency effects (caching, locks, or bandwidth bottlenecks) influence parallel runtime behavior as well. Complex performance prediction approaches emerged, which can be grouped into: virtual prototyping, analytical models, and statistical methods. In this work, we predict the performance of two algorithms from the field of advanced driver-assistance systems in a case study. With the following three methods, we provide a comparative overview of state-of-the-art predictions: GEM5 (virtual prototype), IBM Exabounds (analytical model), and an in-house developed statistical method. We first describe the theoretical background, describe the experimental- and model-setup, and give a detailed evaluation of the prediction. In addition, we discuss the applicability of all three methods for predicting parallel and heterogeneous systems."
pub.1104272836,On the Accuracy and Usefulness of Analytic Energy Models for Contemporary Multicore Processors,"This paper presents refinements to the execution-cache-memory performance model and a previously published power model for multicore processors. The combination of both enables a very accurate prediction of performance and energy consumption of contemporary multicore processors as a function of relevant parameters such as number of active cores as well as core and Uncore frequencies. Model validation is performed on Intel Sandy Bridge-EP, Broadwell-EP, and AMD Epyc processors. Production-related variations in chip quality are demonstrated through a statistical analysis of the fit parameters obtained on one hundred Broadwell-EP CPUs of the same model. Insights from the models are used to explain the performance- and energy-related behavior of the processors for scalable as well as saturating (i.e., memory-bound) codes. In the process we demonstrate the models’ capability to identify optimal operating points with respect to highest performance, lowest energy-to-solution, and lowest energy-delay product and identify a set of best practices for energy-efficient execution."
pub.1023711305,"PAS2P Tool, Parallel Application Signature for Performance Prediction","Accurate prediction of parallel applications’ performance is becoming increasingly complex. We seek to characterize the behavior of message-passing applications by extracting a signature to predict the performance in different target systems. We have developed a tool we called Parallel Application Signature for Performance Prediction (PAS2P) that strives to describe an application based on its behavior. Based on the application’s message-passing activity, we have been able to identify and extract representative phases, with which we created a signature. We have experimented using scientific applications and we predicted the execution times on multicore architectures with an average accuracy of over 97%."
pub.1032293150,A Multithreaded Implementation of HEVC Intra Prediction Algorithm for a Photovoltaic Monitoring System,"Recently, many photovoltaic systems (PV systems) including solar parks and PV farms have been built to prepare for the post fossil fuel era. To investigate the degradation process of the PV systems and thus, efficiently operate PV systems, there is a need to visually monitor PV systems in the range of infrared ray through the Internet. For efficient visual monitoring, this paper explores a multithreaded implementation of a recently developed HEVC standard whose compression efficiency is almost two times higher than H.264. For an efficient parallel implementation under a meshbased 64 multicore system, this work takes into account various design choices which can solve potential problems of a two-dimensional interconnects-based 64 multicore system. These problems may have not occurred in a small-scale multicore system based on a simple bus network. Through extensive evaluation, this paper shows that, for an efficient multithreaded implementation of HEVC intra prediction in a mesh-based multicore system, much effort needs to be made to optimize communications among processing cores. Thus, this work provides three design choices regarding communications, i.e., main thread core location, cache home policy, and maximum coding unit size. These design choices are shown to improve the overall parallel performance of the HEVC intra prediction algorithm by up to 42%, achieving a 7 times higher speed-up."
pub.1021485152,Classification Based on Compressive Multivariate Time Series,"Prediction of critical condition in intensive care unit (ICU) becomes one of the current major focuses in hospital healthcare delivery. Most of existing data mining methods only considered single time series signal and worked in original dimension. Consequently, they performed poorly for extended dataset of patient records. The main challenge of ICU prediction is the data too big to be stored and processed in timely manner. The problem in this study is how to compressed the original data into as small as possible while preserved prediction performance. In this paper, we propose multivariate compressed representation (MultiCoRe). Each recorded vital signal is transformed in frequency domain then reduced in low dimensional space. Multivariate distance measurement (MultiDist) is introduced to compute similarity between two patient records directly in MultiCoRe. Experimental results using MIMIC-II dataset show that our proposed method improved prediction accuracy and run hundreds times more efficient than other baseline methods."
pub.1061538459,Intelligent Hotspot Prediction for Network-on-Chip-Based Multicore Systems,"Hotspots are network-on-chip (NoC) routers or modules in multicore systems which occasionally receive packetized data from other networked element producers at a rate higher than they can consume it. This adverse phenomenon may greatly reduce the performance of NoCs, especially when wormhole flow-control is employed, as backpressure can cause the buffers of neighboring routers to quickly fill-up leading to a spatial spread in congestion. This can cause the network to saturate prematurely where in the worst scenario the NoC may be rendered unrecoverable. Thus, a hotspot prevention mechanism can be greatly beneficial, as it can potentially enable the interconnection system to adjust its behavior and prevent the rise of potential hotspots, subsequently sustaining NoC performance. The inherent unevenness of traffic patterns in an NoC-based general-purpose multicore system such as a chip multiprocessor, due to the diverse and unpredictable access patterns of applications, produces unexpected hotspots whose appearance cannot be known a priori, as application demands are not predetermined, making hotspot prediction and subsequently prevention difficult. In this paper, we present an artificial neural network-based (ANN) hotspot prediction mechanism that can be potentially used in tandem with a hotspot avoidance or congestion-control mechanism to handle unforeseen hotspot formations efficiently. The ANN uses online statistical data to dynamically monitor the interconnect fabric, and reactively predicts the location of an about to-be-formed hotspot(s), allowing enough time for the multicore system to react to these potential hotspots. Evaluation results indicate that a relatively lightweight ANN-based predictor can forecast hotspot formation(s) with an accuracy ranging from 65% to 92%."
pub.1095504280,Exploiting Fine-Grained Pipeline Parallelism for Wavefront Computations on Multicore Platforms,"This paper presents our experience with exploiting fine-grained pipeline parallelism for wavefront computations on a multicore platform. Wavefront computations have been widely applied in many application areas such as scientific computing algorithms and dynamic programming algorithms. To exploit fine-grained parallelism on multicore platforms, the programmers must consider the problems of synchronization, scheduling strategies and data locality. This paper shows the impact of fine-grained synchronization methods, scheduling strategies and data tile sizes on performance. We propose a low cost, lock-free, and lightweight synchronization method that can fully exploit pipeline parallelism. Our evaluation shows that RNAfold, an application for RNA secondary structures prediction, can achieve the best speedup of 3.88 on four cores under our framework."
pub.1003888867,"Understanding, modelling, and improving the performance of web applications in multicore virtualised environments","As the computing industry enters the Cloud era, multicore architectures and virtualisation technologies are replacing traditional IT infrastructures. However, the complex relationship between applications and system resources in multicore virtualised environments is not well understood. Workloads such as web services and on-line financial applications have the requirement of high performance but benchmark analysis suggests that these applications do not optimally benefit from a higher number of cores. In this paper, we try to understand the scalability behaviour of network/CPU intensive applications running on multicore architectures. We begin by benchmarking the Petstore web application, noting the systematic imbalance that arises with respect to per-core workload. Having identified the reason for this phenomenon, we propose a queueing model which, when appropriately parametrised, reflects the trend in our benchmark results for up to 8 cores. Key to our approach is providing a fine-grained model which incorporates the idiosyncrasies of the operating system and the multiple CPU cores. Analysis of the model suggests a straightforward way to mitigate the observed bottleneck, which can be practically realised by the deployment of multiple virtual NICs within our VM. Next we make blind predictions to forecast performance with multiple virtual NICs. The validation results show that the model is able to predict the expected performance with relative errors ranging between 8 and 26 per cent."
pub.1094634924,Towards a Model-Based Approach for Allocating Tasks to Multicore Processors,"Multicore technology provides a way to improve the performance of embedded systems in response to the demand in many domains for more and more complex functionality. However, increasing the number of processing units also introduces the problem of deciding which task to execute on which core in order to best utilize the platform. In this paper we present a model-based approach for automatic allocation of software tasks to the cores of a soft real-time embedded system, based on design-time performance predictions. We describe a general iterative method for finding an allocation that maximizes key performance aspects while satisfying given allocation constraints, and present an instance of this method, focusing on the particular performance aspects of timeliness and balanced computational load over time and over the cores."
pub.1084960453,Acceleration of MapReduce Framework on a Multicore Processor,"MapReduce framework is widely used in massive data processing, such as financial prediction, online marketing, and so on. Multicore processor is a great platform to implement MapReduce because of its inherent parallelism and flexibility. This book chapter extracts features of MapReduce applications, and proposes a software–hardware co-design framework based on a multi-core processor to improve the performance of MapReduce applications. Experimental results show that the MapReduce framework with hardware accelerators speeds up by 40 times at maximum compared to the pure software solution, and the proposed Topo-MapReduce speeds up further by 29% at maximum compared to the original MapReduce."
pub.1015073336,Relieving Uncertainty in Forest Fire Spread Prediction by Exploiting Multicore Architectures,"The most important aspect that affects the reliability of environmental simulations is the un- certainty on the parameter settings describing the environmental conditions, which may involve important biases between simulation and reality. To relieve such arbitrariness, a two-stage pre- diction method was developed, based on the adjustment of the input parameters according to the real observed evolution. This method enhances the quality of the predictions, but it is very demanding in terms of time and computational resources needed. In this work, we describe a methodology developed for response time assessment in the case of fire spread prediction, based on evolutionary computation. In addition, a parallelization of one of the most used fire spread simulators, FARSITE, was carried out to take advantage of multicore architectures. This al- lows us to design proper allocation policies that significantly reduce simulation time and reach successful predictions much faster. A multi-platform performance study is reported to analyze the benefits of the methodology."
pub.1061754887,Predicting Cross-Core Performance Interference on Multicore Processors with Regression Analysis,"Despite their widespread adoption in cloud computing, multicore processors are heavily under-utilized in terms of computing resources. To avoid the potential for negative and unpredictable interference, co-location of a latency-sensitive application with others on the same multicore processor is disallowed, leaving many cores idle and causing low machine utilization. To enable co-location while providing QoS guarantees, it is challenging but important to predict performance interference between co-located applications. We observed that the performance degradation of an application can be represented as a piecewise predictor function of the aggregate pressures on shared resources from all cores. Based on this observation, we propose to adopt regression analysis to build a predictor function for an application. Furthermore, the prediction model thus obtained for an application is able to characterize its contentiousness and sensitivity. Validation using a large number of single-threaded and multi-threaded benchmarks and nine real-world datacenter applications on two different platforms shows that our approach is also precise, with an average error not exceeding 0.4 percent."
pub.1085483972,Prophet: Precise QoS Prediction on Non-Preemptive Accelerators to Improve Utilization in Warehouse-Scale Computers,"Guaranteeing Quality-of-Service (QoS) of latency-sensitive applications while improving server utilization through application co-location is important yet challenging in modern datacenters. The key challenge is that when applications are co-located on a server, performance interference due to resource contention can be detrimental to the application QoS. Although prior work has proposed techniques to identify ""safe"" co-locations where application QoS is satisfied by predicting the performance interference on multicores, no such prediction technique on accelerators such as GPUs. In this work, we present Prophet, an approach to precisely predict the performance degradation of latency-sensitive applications on accelerators due to application co-location. We analyzed the performance interference on accelerators through a real system investigation and found that unlike on multicores where the key contentious resources are shared caches and main memory bandwidth, the key contentious resources on accelerators are instead processing elements, accelerator memory bandwidth and PCIe bandwidth. Based on this observation, we designed interference models that enable the precise prediction for processing element, accelerator memory bandwidth and PCIe bandwidth contention on real hardware. By using a novel technique to forecast solo-run execution traces of the co-located applications using interference models, Prophet can accurately predict the performance degradation of latency-sensitive applications on non-preemptive accelerators. Using Prophet, we can identify ""safe"" co-locations on accelerators to improve utilization without violating the QoS target. Our evaluation shows that Prophet can predict the performance degradation with an average prediction error 5.47% on real systems. Meanwhile, based on the prediction, Prophet achieves accelerator utilization improvements of 49.9% on average while maintaining the QoS target of latency-sensitive applications."
pub.1125990064,Modelling shared resource competition for multicores using adapted Tilman model,"The need to meet the high computing demands under constraints of power, low latency, inter-process interference, etc. has led to a shift in the paradigm from uniprocessor systems to multicore systems. The challenge in these multicore systems arises from the fact that these cores are not independent in functioning, rather they share few on-chip and off-chip resources. This resource sharing also affects performance due to cross-core interference. Different workloads running on these cores demand different resources for their growth in performance. In this work we examine the contention due to resource sharing amongst co-runners using the adapted multi-species Tilman model. Thereafter, alternative solutions like application scheduling and resource partitioning are simulated using the built model. Based on the simulation results a comparison between solo and co-running systems for different applications under both the solution regimes is conducted at various resource levels. The observed phenomenon can be used for prior study or prediction of performance of applications when co-run to mitigate contention due to shared resources."
pub.1045354031,DReAM: Per-Task DRAM Energy Metering in Multicore Systems,"Interaction across applications in DRAM memory impacts its energy consumption. This paper makes the case for accurate per-task DRAM energy metering in multicores, which opens new paths to energy/performance optimizations, such as per-task energy-aware task scheduling and energy-aware billing in datacenters. In particular, the contributions of this paper are (i) an ideal per-task energy metering model for DRAM memories; (ii) DReAM, an accurate, yet low cost, implementation of the ideal model (less than 5% accuracy error when 16 tasks share memory); and (iii) a comparison with standard methods (even distribution and access-count based) proving that DReAM is more accurate than these other methods."
pub.1061408793,PEPPHER: Efficient and Productive Usage of Hybrid Computing Systems,"PEPPHER, a three-year European FP7 project, addresses efficient utilization of hybrid (heterogeneous) computer systems consisting of multicore CPUs with GPU-type accelerators. This article outlines the PEPPHER performance-aware component model, performance prediction means, runtime system, and other aspects of the project. A larger example demonstrates performance portability with the PEPPHER approach across hybrid systems with one to four GPUs."
pub.1003039538,STEAM,"
                    Recent empirical studies have shown that multicore scaling is fast becoming power limited, and consequently, an increasing fraction of a multicore processor has to be under clocked or powered off. Therefore, in addition to fundamental innovations in architecture, compilers and parallelization of application programs, there is a need to develop practical and effective
                    dynamic energy management
                    (DEM) techniques for multicore processors.
                  
                  
                    Existing DEM techniques mainly target reducing processor power consumption and temperature, and only few of them have addressed improving energy efficiency for multicore systems. With energy efficiency taking a center stage in all aspects of computing, the focus of the DEM needs to be on finding practical methods to maximize processor efficiency. Towards this, this article presents STEAM -- an optimal closed-loop DEM controller designed for multicore processors. The objective is to maximize energy efficiency by dynamic voltage and frequency scaling (DVFS). Energy efficiency is defined as the ratio of performance to power consumption or
                    performance-per-watt
                    (PPW). This is the same as the number of instructions executed per Joule. The PPW metric is actually replaced by
                    
                      P
                      α
                      PW
                    
                    (performance
                    α
                    -per-Watt), which allows for controlling the importance of performance versus power consumption by varying α.
                  
                  
                    The proposed controller was implemented on a Linux system and tested with the Intel Sandy Bridge processor. There are three power management schemes called
                    governors
                    , available with Intel platforms. They are referred to as (1)
                    Powersave
                    (lowest power consumption), (2)
                    Performance
                    (achieves highest performance), and (3)
                    Ondemand
                    . Our simple and lightweight controller when executing SPEC CPU2006, PARSEC, and MiBench benchmarks have achieved an average of 18% improvement in energy efficiency (MIPS/Watt) over these ACPI policies. Moreover, STEAM also demonstrated an excellent prediction of core temperatures and power consumption, and the ability to control the core temperatures within 3
                    ˆ
                    C of the specified maximum. Finally, the overhead of the STEAM implementation (in terms of CPU resources) is less than 0.25%. The entire implementation is self-contained and can be installed on any processor with very little prior knowledge of the processor.
                  "
pub.1020180922,HPC node performance and energy modeling with the co-location of applications,"Multicore processors have become an integral part of modern large-scale and high-performance parallel and distributed computing systems. Unfortunately, applications co-located on multicore processors can suffer from decreased performance and increased dynamic energy use as a result of interference in shared resources, such as memory. As this interference is difficult to characterize, assumptions about application execution time and energy usage can be misleading in the presence of co-location. Consequently, it is important to accurately characterize the performance and energy usage of applications that execute in a co-located manner on these architectures. This work investigates some of the disadvantages of co-location, and presents a methodology for building models capable of utilizing varying amounts of information about a target application and its co-located applications to make predictions about the target application’s execution time and the system’s energy use under arbitrary co-locations of a wide range of application types. The proposed methodology is validated on three different server class Intel Xeon multicore processors using eleven applications from two scientific benchmark suites. The model’s utility for scheduling is also demonstrated in a simulated large-scale high-performance computing environment through the creation of a co-location aware scheduling heuristic. This heuristic demonstrates that scheduling using information generated with the proposed modeling methodology is capable of making significant improvements over a scheduling heuristic that is oblivious to co-location interference."
pub.1095204293,Cache Conscious Task Regrouping on Multicore Processors,"Because of the interference in the shared cache on multicore processors, the performance of a program can be severely affected by its co-running programs. If job scheduling does not consider how a group of tasks utilize cache, the performance may degrade significantly, and the degradation usually varies sizably and unpredictably from run to run. In this paper, we use trace-based program locality analysis and make it efficient enough for dynamic use. We show a complete on-line system for periodically measuring the parallel execution, predicting and ranking cache interference for all co-run choices, and reorganizing programs based on the prediction. We test our system on floating-point and mixed integer and floating-point workloads composed of SPEC 2006 benchmarks and compare with the default Linux job scheduler to show the benefit of the new system in improving performance and reducing performance variation."
pub.1151637026,Deep learning for BER prediction in optical connections impaired by inter-core crosstalk,"Four-level pulse amplitude modulation (PAM4) signals transmission in short-haul intensity modulation-direct detection datacenters connections supported by homogeneous weakly-coupled multicore fibers is seen as a promising technology to meet the future challenge of providing enough bandwidth and achieve high data capacity in datacenter links. However, in multicore fibers, inter-core crosstalk (ICXT) limits significantly the performance of such short-reach connections by causing large bit error rate (BER) fluctuations. In this work, a convolutional neural network (CNN) is proposed for eye-pattern analysis and BER prediction in PAM4 inter-datacenter optical connections impaired by ICXT, with the aim of optical performance monitoring. The performance of the CNN is assessed by estimation of the root mean square error (RMSE) using a synthetic dataset created with Monte Carlo simulation. Considering PAM4 interdatacenter connections with one interfering core and for different skew-symbol rate products, extinction ratios and crosstalk levels, the obtained results show that the implemented CNN is able to predict the BER without surpassing a RMSE limit of 0.1."
pub.1112265440,Predictive Thermal Management for Energy-Efficient Execution of Concurrent Applications on Heterogeneous Multicores,"Current multicore platforms contain different types of cores, organized in clusters (e.g., ARM’s big.LITTLE). These platforms deal with concurrently executing applications, having varying workload profiles and performance requirements. Runtime management is imperative for adapting to such performance requirements and workload variabilities and to increase energy and temperature efficiency. Temperature has also become a critical parameter since it affects reliability, power consumption, and performance and, hence, must be managed. This paper proposes an accurate temperature prediction scheme coupled with a runtime energy management approach to proactively avoid exceeding temperature thresholds while maintaining performance targets. Experiments show up to 20% energy savings while maintaining high-temperature averages and peaks below the threshold. Compared with state-of-the-art temperature predictors, this paper predicts 35% faster and reduces the mean absolute error from 3.25 to 1.15 °C for the evaluated applications’ scenarios."
pub.1084934742,Prophet,"Guaranteeing Quality-of-Service (QoS) of latency-sensitive applications while improving server utilization through application co-location is important yet challenging in modern datacenters. The key challenge is that when applications are co-located on a server, performance interference due to resource contention can be detrimental to the application QoS. Although prior work has proposed techniques to identify ""safe"" co-locations where application QoS is satisfied by predicting the performance interference on multicores, no such prediction technique on accelerators such as GPUs. In this work, we present Prophet, an approach to precisely predict the performance degradation of latency-sensitive applications on accelerators due to application co-location. We analyzed the performance interference on accelerators through a real system investigation and found that unlike on multicores where the key contentious resources are shared caches and main memory bandwidth, the key contentious resources on accelerators are instead processing elements, accelerator memory bandwidth and PCIe bandwidth. Based on this observation, we designed interference models that enable the precise prediction for processing element, accelerator memory bandwidth and PCIe bandwidth contention on real hardware. By using a novel technique to forecast solo-run execution traces of the co-located applications using interference models, Prophet can accurately predict the performance degradation of latency-sensitive applications on non-preemptive accelerators. Using Prophet, we can identify ""safe"" co-locations on accelerators to improve utilization without violating the QoS target. Our evaluation shows that Prophet can predict the performance degradation with an average prediction error 5.47% on real systems. Meanwhile, based on the prediction, Prophet achieves accelerator utilization improvements of 49.9% on average while maintaining the QoS target of latency-sensitive applications."
pub.1085484191,Prophet,"Guaranteeing Quality-of-Service (QoS) of latency-sensitive applications while improving server utilization through application co-location is important yet challenging in modern datacenters. The key challenge is that when applications are co-located on a server, performance interference due to resource contention can be detrimental to the application QoS. Although prior work has proposed techniques to identify ""safe"" co-locations where application QoS is satisfied by predicting the performance interference on multicores, no such prediction technique on accelerators such as GPUs. In this work, we present Prophet, an approach to precisely predict the performance degradation of latency-sensitive applications on accelerators due to application co-location. We analyzed the performance interference on accelerators through a real system investigation and found that unlike on multicores where the key contentious resources are shared caches and main memory bandwidth, the key contentious resources on accelerators are instead processing elements, accelerator memory bandwidth and PCIe bandwidth. Based on this observation, we designed interference models that enable the precise prediction for processing element, accelerator memory bandwidth and PCIe bandwidth contention on real hardware. By using a novel technique to forecast solo-run execution traces of the co-located applications using interference models, Prophet can accurately predict the performance degradation of latency-sensitive applications on non-preemptive accelerators. Using Prophet, we can identify ""safe"" co-locations on accelerators to improve utilization without violating the QoS target. Our evaluation shows that Prophet can predict the performance degradation with an average prediction error 5.47% on real systems. Meanwhile, based on the prediction, Prophet achieves accelerator utilization improvements of 49.9% on average while maintaining the QoS target of latency-sensitive applications."
pub.1085484132,Prophet,"Guaranteeing Quality-of-Service (QoS) of latency-sensitive applications while improving server utilization through application co-location is important yet challenging in modern datacenters. The key challenge is that when applications are co-located on a server, performance interference due to resource contention can be detrimental to the application QoS. Although prior work has proposed techniques to identify ""safe"" co-locations where application QoS is satisfied by predicting the performance interference on multicores, no such prediction technique on accelerators such as GPUs. In this work, we present Prophet, an approach to precisely predict the performance degradation of latency-sensitive applications on accelerators due to application co-location. We analyzed the performance interference on accelerators through a real system investigation and found that unlike on multicores where the key contentious resources are shared caches and main memory bandwidth, the key contentious resources on accelerators are instead processing elements, accelerator memory bandwidth and PCIe bandwidth. Based on this observation, we designed interference models that enable the precise prediction for processing element, accelerator memory bandwidth and PCIe bandwidth contention on real hardware. By using a novel technique to forecast solo-run execution traces of the co-located applications using interference models, Prophet can accurately predict the performance degradation of latency-sensitive applications on non-preemptive accelerators. Using Prophet, we can identify ""safe"" co-locations on accelerators to improve utilization without violating the QoS target. Our evaluation shows that Prophet can predict the performance degradation with an average prediction error 5.47% on real systems. Meanwhile, based on the prediction, Prophet achieves accelerator utilization improvements of 49.9% on average while maintaining the QoS target of latency-sensitive applications."
pub.1112289177,Multicore Performance Engineering of Sparse Triangular Solves Using a Modified Roofline Model,"The Roofline model is widely used to visualize the performance of executed code together with the upper performance bounds given by the memory bandwidth and the processor peak performance. The model can thus provide an insightful visualization of bottlenecks. In this paper, we try to establish realistic bandwidth ceilings for the sparse triangular solve step of PARDISO, a leading sparse direct solver package, which is also part of the Intel MKL library. The performance of the forward and backward substitution process is analyzed and benchmarked for a representative set of sparse matrices on seven modern x86-type multicore architectures and the Knights Landing manycore architecture. It is shown how to accurately measure the necessary quantities also for threaded code, and the measurement approach, its validation, as well as limitations are discussed. Our modeling approach covers the serial and parallel execution phases, allowing for in-socket performance predictions."
pub.1133747081,A New Hole-walled Multi-core Fiber for Space Division Multiplexing for Improved Performance,"The need for the enhancement of channel capacity of optical fiber, space division multiplexing (SDM) transmission fibers- such as the multicore fiber, the multimode fiber and the few-mode multicore fiber, etc., - have been researched for long-distance communication system. In this paper, a special type of homogeneous multicore fiber structure is introduced and the array of holes is placed between each core. The normalized propagation constant, mode field diameter (MFD) of LP01 mode of this structure are studied here. The empirical relation between normalized propagation constant and V number of LP01 mode of a single core single-mode fiber is compared with simulation results of LP01 mode of a multicore fiber. The mode field diameter of MCF is derived by noting the beam radius where the intensity drops to 1/e2 of the intensity on the beam axis. The theoretical prediction match well with COMSOL results. These results are useful for the investigation of the detailed characteristics of different types of MCF."
pub.1094694958,On Reducing Power Consumption and Code Size of H.264 Intra Luma Prediction on Multicore DSP,"As the latest international video coding standard, H.264 is gaining importance not only on desktop computers but also on handheld devices. For handheld devices, power consumption of the system is of ultimate importance, which needs to be addressed at every layer of the system design. At the application layer, the code size affects the memory hierarchy behavior, which in turn affects the system power performance. In this paper, we consider the power consumption of H.264 running on handheld devices using multicore DSP processors, focusing particularly on power saving through code reduction. We use the multiple intra-mode prediction as an example. There are nine optional prediction modes for each ${\mbi 4}\ \times\ {\mbi 4}$ intra luma sub-macroblock in H.264, which result in nine separate routines. We examine the access patterns of the modes and propose a general scheme that replaces eight of the nine routines with one. This dramatically reduces the code size and the resultant power consumption. We implemented the code on PACDSP, resulting in a 20% reduction of the code size with negligible performance degradation."
pub.1094995212,Energy-Aware Workload Consolidation on GPU,"Enterprise workloads like search, data mining and analytics, etc. typically involve a large number of users who are simultaneously using applications that are hosted on clusters of commodity computers. Use of GPUs for enterprise computing is challenging because of poor performance and higher energy consumption compared to running enterprise workloads on CPUs. In this paper, we show that the GPU work consolidation can improve system throughput and results in significant energy savings over multicore CPUs. We develop a novel runtime framework that dynamically consolidates instances from different workloads from multiple user processes into a single GPU workload. However, arbitrary consolidation of GPU workloads does not always lead to better energy efficiency. We use new GPU performance and power models to make predictions for potential workload consolidation alternatives and identify useful consolidations. Our experiments on a variety of workloads (that perform poorly on a GPU compared to well optimized multicore CPU implementations) show that the proposed framework for GPU can provide 2X to 22X energy benefit over a multicore CPU."
pub.1122522015,dCCPI-predictor: A state-aware approach for effectively predicting cross-core performance interference,"Multicore processors are extensively adopted in data center. Applications running on multicore processors may experience performance interference due to the contention for shared resources, which can negatively affect the Qos of online applications and reduce revenue. In order to guarantee the QoS of online applications, data center always over-provision resources for online applications, leaving a large number of cores idle, resulting in extremely low resource utilization. Improving resource utilization while ensuring the Qos of online applications is a challenge issue for data center. Most of the previous work has focused on interference prediction in fixed state mode, which affects its effectiveness in production data center. In this paper, we propose a novel interference prediction approach, namely dCCPI-predictor, which dynamically predicts the cross-core performance interference of multiple applications running together so as to identify the ’safe’ co-locations to share the server resource. dCCPI-predictor builds an interference prediction model for each application that enabling calculate the performance degradation that the application suffers in any co-location. dCCPI-predictor dynamically adapts to the state change of the application, predicting the performance interference in different states, which was overlooked in previous work. We conducted experiments on a simulated data center over multiple benchmarks to evaluate our approach. Results show that dCCPI-predictor can predict performance interference with a very high accuracy, which is greatly superior to static approach."
pub.1010211217,Automatic Calibration of Performance Models on Heterogeneous Multicore Architectures,"Multicore architectures featuring specialized accelerators are getting an increasing amount of attention, and this success will probably influence the design of future High Performance Computing hardware. Unfortunately, programmers are actually having a hard time trying to exploit all these heterogeneous computing units efficiently, and most existing efforts simply focus on providing tools to offload some computations on available accelerators. Recently, some runtime systems have been designed that exploit the idea of scheduling – as opposed to offloading – parallel tasks over the whole set of heterogeneous computing units. Scheduling tasks over heterogeneous platforms makes it necessary to use accurate prediction models in order to assign each task to its most adequate computing unit [2]. A deep knowledge of the application is usually required to model per-task performance models, based on the algorithmic complexity of the underlying numeric kernel.We present an alternate, auto-tuning performance prediction approach based on performance history tables dynamically built during the application run. This approach does not require that the programmer provides some specific information. We show that, thanks to the use of a carefully chosen hash-function, our approach quickly achieves accurate performance estimations automatically. Our approach even outperforms regular algorithmic performance models with several linear algebra numerical kernels."
pub.1019061656,Topic 2: Performance Prediction and Evaluation,"In recent years, a range of performance evaluation methodologies and tools has been developed for evaluating, designing, and modeling of parallel and distributed systems. The aim of Topic 2, Performance Prediction and Evaluation, is to bring together system designers and researchers involved with qualitative and quantitative evaluation of large scale parallel machines, Grids, and, especially, multicore architectures suffering from contention for critical resources. Of particular interest is work forming a bridge between theory and practice as well as reporting on success or failure of current approaches."
pub.1094962858,2015 IEEE 21st International Conference on Parallel and Distributed Systems Contention-Aware Scheduling for Asymmetric Multicore Processors,"Asymmetric multicore processors (AMPs) have been proposed as an energy-efficient alternative to symmetric multicore processors (SMPs). However, AMPs derive their performance from core specialization, which requires co-running applications to be scheduled to run on their most appropriate core types. Despite extensive research on AMP scheduling, developing an effective scheduling algorithm remains challenging. Contention for shared resources is a key performance-limiting factor, which often renders existing contention-free scheduling algorithms ineffective. We introduce a contention-aware scheduling algorithm for ARM's big.LITTLE, a commercial AMP platform. Our algorithm comprises an offline stage and an online stage. The offline stage builds a performance interference model for an application by training it with a set of co-running applications. Guided by this model, the online stage schedules a workload by assigning its applications to their most appropriate core types in order to minimize the performance degradation caused by contention for shared resources. Our model can accurately predict the performance degradation of an application when co-running with other applications with an average prediction error of 9.60%. Compared with the default scheduler provided for ARM's big. LITTLE and the speedup-factor-driven scheduler, our contention-aware scheduler can improve overall system performance by up to 28.32 % and 28.51 %, respectively."
pub.1104470146,Optimized approach based on time prediction and space chunking for polyhedron programs parallelization on multicores,"A complex challenge in parallel computing is cores load balancing aiming to minimize the parallel program overall execution time called makespan. As the performance of some parallel architectures such as multicores may vary during program execution, an effective mapping should support this unknown variation to avoid drawbacks on makespan. In fact, mapping or static load balancing method may not be effective when the target machine state changes during program execution. Thread affinity has appeared as an important technique to improve the program performance and for better stability. In this context, we propose a predictive approach allowing parallel nested loops adaptation to processor's performance using iterations chunking at runtime. Our approach is based on thread pinning, space chunking and performance detection at runtime. Thus, parting from a parallel program, we define a first set of loop nest iterations called chunk. This first chunk is run using an initial mapping assuming homogeneous cores. Then, performance assessment will correct the mapping by predicting the future core's state. Then, this new mapping will be applied to a new chunk for further evaluation and prediction and so on. The process would stop when the program is fully run or when judging that chunking is no longer effective."
pub.1044239194,"Euro-Par 2009 Parallel Processing, 15th International Euro-Par Conference, Delft, The Netherlands, August 25-28, 2009. Proceedings","This book constitutes the refereed proceedings of the 15th International Conference on Parallel Computing, Euro-Par 2009, held in Delft, The Netherlands, in August 2009. The 85 revised papers presented were carefully reviewed and selected from 256 submissions. The papers are organized in topical sections on support tools and environments; performance prediction and evaluation; scheduling and load balancing; high performance architectures and compilers; parallel and distributed databases; grid, cluster, and cloud computing; peer-to-peer computing; distributed systems and algorithms; parallel and distributed programming; parallel numerical algorithms; multicore and manycore programming; theory and algorithms for parallel computation; high performance networks; and mobile and ubiquitous computing."
pub.1119421810,Bridging the Architecture Gap: Abstracting Performance-Relevant Properties of Modern Server Processors,"We describe a universal modeling approach for predicting single- and
multicore runtime of steady-state loops on server processors. To this end we
strictly differentiate between application and machine models: An application
model comprises the loop code, problem sizes, and other runtime parameters,
while a machine model is an abstraction of all performance-relevant properties
of a CPU. We introduce a generic method for determining machine models and
present results for relevant server-processor architectures by Intel, AMD, IBM,
and Marvell/Cavium. Considering this wide range of architectures, the set of
features required for adequate performance modeling is surprisingly small. To
validate our approach, we compare performance predictions to empirical data for
an OpenMP-parallel preconditioned CG algorithm, which includes compute- and
memory-bound kernels. Both single- and multicore analysis shows that the model
exhibits average and maximum relative errors of 5% and 10%. Deviations from the
model and insights gained are discussed in detail."
pub.1063163272,Hardware support for accurate per-task energy metering in multicore systems,"Accurately determining the energy consumed by each task in a system will become of prominent importance in future multicore-based systems because it offers several benefits, including (i) better application energy/performance optimizations, (ii) improved energy-aware task scheduling, and (iii) energy-aware billing in data centers. Unfortunately, existing methods for energy metering in multicores fail to provide accurate energy estimates for each task when several tasks run simultaneously.
                  This article makes a case for accurate Per-Task Energy Metering (PTEM) based on tracking the resource utilization and occupancy of each task. Different hardware implementations with different trade-offs between energy prediction accuracy and hardware-implementation complexity are proposed. Our evaluation shows that the energy consumed in a multicore by each task can be accurately measured. For a 32-core, 2-way, simultaneous multithreaded core setup, PTEM reduces the average accuracy error from more than 12% when our hardware support is not used to less than 4% when it is used. The maximum observed error for any task in the workload we used reduces from 58% down to 9% when our hardware support is used."
pub.1007462840,Hardware support for accurate per-task energy metering in multicore systems,"Accurately determining the energy consumed by each task in a system will become of prominent importance in future multicore-based systems because it offers several benefits, including (i) better application energy/performance optimizations, (ii) improved energy-aware task scheduling, and (iii) energy-aware billing in data centers. Unfortunately, existing methods for energy metering in multicores fail to provide accurate energy estimates for each task when several tasks run simultaneously. This article makes a case for accurate Per-Task Energy Metering (PTEM) based on tracking the resource utilization and occupancy of each task. Different hardware implementations with different trade-offs between energy prediction accuracy and hardware-implementation complexity are proposed. Our evaluation shows that the energy consumed in a multicore by each task can be accurately measured. For a 32-core, 2-way, simultaneous multithreaded core setup, PTEM reduces the average accuracy error from more than 12% when our hardware support is not used to less than 4% when it is used. The maximum observed error for any task in the workload we used reduces from 58% down to 9% when our hardware support is used."
pub.1131295225,Adaptive Machine Learning-based Temperature Prediction Scheme for Thermal-aware NoC System,"Because of the high-complex interconnection in the contemporary manycore system, the Network-on-Chip (NoC) technology is proven as an efficient way to solve the communication problem in multicore systems. However, the thermal problem becomes the main design challenge in the current NoC systems due to the high-diverse workload distribution and large power density. Therefore, the Proactive Dynamic Thermal Management (PDTM) is employed as an efficient way to control the system temperature in modern multicore systems. Based on the predicted temperature information, the PDTM can control the system temperature in advance, which helps to reduce the performance impact during the temperature control period. However, the conventional temperature prediction model is usually built based on several specific physical parameters, which are usually temperature-sensitive as well. As a result, the current temperature prediction models still suffer from large prediction errors, which reduces the benefit of the PDTM. To solve this problem, we combine the artificial neural network and LMS adaptive filter theory to propose an adaptive machine learning-based temperature prediction model. Because the proposed model can adapt to the hyperplane of the temperature behavior of NoC system during the runtime, the proposed approach can reduce average error by 37.2% to 62.3%, which helps to improve the system performance by 9.16% to 38.37% and can bring smaller area overhead than the related works by 18.59% to 22.11%."
pub.1140866602,Agon: A Scalable Competitive Scheduler for Large Heterogeneous Systems,"This work proposes a competitive scheduling approach, designed to scale to
large heterogeneous multicore systems. This scheduler overcomes the challenges
of (1) the high computation overhead of near-optimal schedulers, and (2) the
error introduced by inaccurate performance predictions. This paper presents
Agon, a neural network-based classifier that selects from a range of
schedulers, from simple to very accurate, and learns which scheduler provides
the right balance of accuracy and overhead for each scheduling interval. Agon
also employs a de-noising frontend allowing the individual schedulers to be
tolerant towards noise in performance predictions, producing better overall
schedules. By avoiding expensive scheduling overheads, Agon improves average
system performance by 6\% on average, approaching the performance of an
oracular scheduler (99.1% of oracle performance)."
pub.1039442355,Juggle: addressing extrinsic load imbalances in SPMD applications on multicore computers,"We investigate proactive dynamic load balancing on multicore systems, in which threads are continually migrated to reduce the impact of processor/thread mismatches. Our goal is to enhance the flexibility of the SPMD-style programming model and enable SPMD applications to run efficiently in multiprogrammed environments. We present Juggle, a practical decentralized, user-space implementation of a proactive load balancer that emphasizes portability and usability. In this paper we assume perfect intrinsic load balance and focus on extrinsic imbalances caused by OS noise, multiprogramming and mismatches of threads to hardware parallelism. Juggle shows performance improvements of up to 80 % over static load balancing for oversubscribed UPC, OpenMP, and pthreads benchmarks. We also show that Juggle is effective in unpredictable, multiprogrammed environments, with up to a 50 % performance improvement over the Linux load balancer and a 25 % reduction in performance variation. We analyze the impact of Juggle on parallel applications and derive lower bounds and approximations for thread completion times. We show that results from Juggle closely match theoretical predictions across a variety of architectures, including NUMA and hyper-threaded systems."
pub.1122789607,Distributed Neural Networks using TensorFlow over Multicore and Many-core Systems,"This paper focuses on distributed deep learning models that simulate the HAR (Human Activity Recognition) data set from the UCI machine learning Repository. The proposed deep learning LSTM (Long Short-Term Memory) model works with the TensorFlow framework using the Python 3 programming language which supports the distributed architecture. In order to simulate the distributed deep learning models over different multicore and many-core systems, two hardware platforms are built; the first one is equipped with a Raspberry Pi cluster with 16 Pi 3 model B+ boards which each having 1 GB of RAM and 32 GB flash storage. The second platform is houses an Octa-core Intel Xeon CPU system with a 16MB Cache, 32 GB RAM and 2 TB SSD primary storage with 10 TB HDD secondary storage. In this paper, the performance of the distributed LSTM model over multicore and many-core systems is presented in terms of execution speed and efficiency of prediction accuracy upon varying number of deep layers with corresponding hidden nodes. In this experiment, a 3 × 3 distributed LSTM model has been used, which furnishes higher prediction accuracy with faster computation time than the models that different number of layers provide."
pub.1094120220,Mathematical Models and Control Algorithms for Dynamic Optimization of Multicore Platforms: A Complex Dynamics Approach Invited Paper,"The continuous increase in integration densities contributed to a shift from Dennard's scaling to a parallelization era of multi-/many-core chips. However, for multicores to rapidly percolate the application domain from consumer multimedia to high-end functionality (e.g., security, healthcare, big data), power/energy and thermal efficiency challenges must be addressed. Increased power densities can raise on-chip temperatures, which in turn decrease chip reliability and performance, and increase cooling costs. For a dependable multicore system, dynamic optimization (power/thermal management) has to rely on accurate yet low complexity workload models. Towards this end, we present a class of mathematical models that generalize prior approaches and capture their time dependence and long-range memory with minimum complexity. This modeling framework serves as the basis for defining new efficient control and prediction algorithms for hierarchical dynamic power management of future data-centers-on-a-chip."
pub.1120886090,Towards a parallel template catalogue for software performance predictions,"Software Performance Engineers evaluate quality attributes (like response time) of software rich systems based on architectural models during early design time. Thereby, they use model-based approaches to analyze the software's behaviour and resource consumption. One prominent approach is the Palladio Component Model (PCM) which has been researched for over a decade. However, when it comes to multicore support or massive parallel execution the approaches still suffer from major drawbacks. Drawbacks include inaccurate prediction models, insufficient modelling languages, and missing tool support, to name just some of them. In this paper, we focus on overcoming the last two drawbacks, namely improving the modelling language and the tool support. We present a template catalogue for parallel performance patterns in software performance predictions. We use the example of a parallel-loop in the Palladio Component Model to exemplify our idea."
pub.1008470370,Predictive dynamic thermal management for multicore systems,"Recently, processor power density has been increasing at an alarming rate resulting in high on-chip temperature. Higher temperature increases current leakage and causes poor reliability. In this paper, we propose a Predictive Dynamic Thermal Management (PDTM) based on Application-based Thermal Model (ABTM) and Core-based Thermal Model (CBTM) in the multicore systems. ABTM predicts future temperature based on the application specific thermal behavior, while CBTM estimates core temperature pattern by steady state temperature and workload. The accuracy of our prediction model is 1.6% error in average compared to the model in HybDTM [8], which has at most 5% error. Based on predicted temperature from ABTM and CBTM, the proposed PDTM can maintain the system temperature below a desired level by moving the running application from the possible overheated core to the future coolest core (migration) and reducing the processor resources (priority scheduling) within multicore systems. PDTM enables the exploration of the tradeoff between throughput and fairness in temperature-constrained multicore systems. We implement PDTM on Intel's Quad-Core system with a specific device driver to access Digital Thermal Sensor (DTS). Compared against Linux standard scheduler, PDTM can decrease average temperature about 10%, and peak temperature by 5C with negligible impact of performance under 1%, while running single SPEC2006 benchmark. Moreover, our PDTM outperforms HRTM [10] in reducing average temperature by about 7% and peak temperature by about 3C with performance overhead by 0.15% when running single benchmark."
pub.1032697186,Performance Prediction and Evaluation,"In recent years a range of novel methodologies and tools have been developed for the purpose of evaluation, design, and model reduction of existing and emerging parallel and distributed systems. At the same time, the coverage of the term “performance” has broadened to include reliability, robustness, energy consumption, and scalability, in addition that is to the classical performance-oriented evaluation of system functionality. The aim of the conference topic “Performance Prediction and Evaluation”, was to bring together system designers and researchers involved with the qualitative and quantitative evaluation and modeling of large-scale parallel and distributed applications and systems (e.g., Grids, Cloud computing environments, multicore architectures)."
pub.1036377158,Performance evaluation and prediction of open source speech engine on multicore processors,"This paper quantifies the performance of the core part of voice driven web using free and open source speech engine; the speech engine which is very high computation demanding, it consists of Automatic Speech Recognition (ASR) and Text To Speech (TTS). Two open source programs, Sphinx-4 and FreeTTS-1.2.2 are used for ASR and TTS respectively. These two programs are executed on 2 different hardware multicore processors with 4 hyperthreaded cores, and 8 cores respectively. The response time with respect to the load variance and the number of cores is measured and predicted using a linear regression model. The results show that, the response time is linear with respect to the input length, this property can be used to directly predict the response for any input length. Moreover, though the response time and the speed up increases as the number of cores increases, the regression coefficients and number of threads reveal that ASR benefits from multicore. The speedup factor for ASR is 1.56 for 8 cores. However for FreeTTS, though being sequential the speed up from the program itself is insignificant, there is about 1. 43 speedup for 8 cores, that comes from the system's contribution. Our findings show that the generalization of the results for multicore processor does not apply to hyperthreading. This paper presents the investigation that is useful for educators, researchers, and applications' developer in voice based applications 'domain."
pub.1135307886,Improving the accuracy of energy predictive models for multicore CPUs by combining utilization and performance events model variables,"Energy predictive modeling is the leading method for determining the energy consumption of an application. Performance monitoring counters (PMCs) and resource utilizations have been the principal source of model variables primarily due to their high positive correlation with energy consumption. Performance events, however, have come to dominate the landscape due to their better prediction accuracy compared to utilization variables. Recently, the theory of energy of computing has been proposed whose practical implications for constructing accurate and reliable linear energy predictive models are unified in a consistency test that includes a selection criterion of additivity for model variables. In this work, we analyze the prediction accuracy of models employing utilization variables only, PMCs only, and combination of both utilization variables and PMCs, through the lens of this theory for modern multicore CPU platforms. We discover that employing utilization variables only in linear energy predictive models does not capture all the energy-consuming activities during an application execution. However, combination of utilization variables with PMCs that are highly additive and highly correlated with energy consumption, gives the most accurate linear energy predictive model. Our experimental results show that application-specific and platform-level models using both utilization variables and PMCs exhibit up to 3.6 × and 2.6 × better average prediction accuracy respectively when compared with models employing utilization variables only and highly additive PMCs only."
pub.1012728501,Parallel Performance Prediction for Multigrid Codes on Distributed Memory Architectures,"We propose a model for describing the parallel performance of multigrid software on distributed memory architectures. The goal of the model is to allow reliable predictions to be made as to the execution time of a given code on a large number of processors, of a given parallel system, by only benchmarking the code on small numbers of processors. This has potential applications for the scheduling of jobs in a Grid computing environment where reliable predictions as to execution times on different systems will be valuable. The model is tested for two different multigrid codes running on two different parallel architectures and the results obtained are discussed."
pub.1104272844,Applicability of the ECM Performance Model to Explicit ODE Methods on Current Multi-core Processors,"To support the portability of efficiency when bringing an application from scientific computing to a new HPC system, autotuning techniques are promising approaches. Ideally, these approaches are able to derive an efficient implementation for a specific HPC system by applying suitable program transformations. Often, a large number of implementations results, and the most efficient of these variants should be selected. In this article, we investigate performance modelling and prediction techniques which can support the selection process. These techniques may significantly reduce the selection effort, compared to extensive runtime tests. We apply the execution-cache-memory (ECM) performance model to numerical solution methods for ordinary differential equations (ODEs). In particular, we consider the question whether it is possible to obtain a performance prediction for the resulting implementation variants to support the variant selection. We investigate the accuracy of the prediction for different ODEs and different hardware platforms and show that the prediction is able to reliably select a set of fast variants and, thus, to limit the search space for possible later empirical tuning."
pub.1120017595,Improving the Accuracy of Energy Predictive Models for Multicore CPUs Using Additivity of Performance Monitoring Counters,"Energy predictive modelling using performance monitoring counters (PMCs) has emerged as the leading mainstream approach for modelling the energy consumption of an application. Modern computing platforms such as multicore CPUs provide a large set of PMCs. The programmers, however, can obtain only a small number of PMCs (typically 3–4) during an application run due to the limited number of hardware registers dedicated to storing them. Therefore, selection of a reliable subset of PMCs as predictor variables is crucial to the prediction accuracy of online energy models. State-of-the-art methods for selecting the PMCs are largely based on their correlation with energy consumption.Recently, Additivity is introduced as a property of PMCs that appears to have significant impact on the accuracy of energy predictive models. It is based on an experimental observation that energy consumption of serial execution of two applications is equal to the sum of the energy consumption of those applications when they are run separately. In this work, we demonstrate how the accuracy of energy predictive models based on three popular techniques (Linear regression, Random forests, and Neural networks) can be improved by selecting PMCs based on a property of additivity."
pub.1084948347,Guiding Locality Optimizations for Graph Computations via Reuse Distance Analysis,"This work addresses the problem of optimizing graph-based programs for multicore processors. We use three graph benchmarks and three input data sets to characterize the importance of properly partitioning graphs among cores at multiple levels of the cache hierarchy. We also exhaustively explore a large design space comprised of different parallelization schemes and graph partitionings via detailed simulation to show how much gain we can obtain over a baseline legacy scheme that partitions for the L1 cache only. Our results demonstrate the legacy approach is not the best choice, and that our proposed parallelization / locality techniques can perform better (by up to 20 percent). We then use a performance prediction model based on multicore reuse distance (RD) profiles to rank order the different parallelization / locality schemes in the design space. We compare the best configuration as predicted by our model against the actual best identified by our exhaustive simulations. For one benchmark and data input, we show our model can achieve 79.5 percent of the performance gain achieved by the actual best. Across all benchmarks and data inputs, our model achieves 48 percent of the maximum performance gain. Our work demonstrates a new use case for multicore RD profilesi.e. as a tool for helping program developers and compilers to optimize graph-based programs."
pub.1094869027,Parallel protein sequence matching on multicore computers,"STRIKE was introduced and implemented to predict protein-protein interactions where proteins interact if they contain similar substrings of amino acids. On the yeast protein interaction literature, STRIKE was shown to improve upon the existing state-of-the-art methods for protein-protein interaction prediction. Herein, we describe the parallelization of STRIKE and its multithreaded implementation and performance enhancement on multicore systems. On large protein sequence sets, the execution time of a 16-thread implementation of this bioinformatics algorithm was reduced from about a week on a unithreaded implementation on a serial uniprocessor machine to 1.5 days on one quad core ×86 machine, down to 4.5 hours on 8 such quad core machines. Key optimizations to the implementation are also discussed."
pub.1119255545,A study of integer sorting on multicores,"Integer sorting on multicores and GPUs can be realized by a variety of
approaches that include variants of distribution-based methods such as
radix-sort, comparison-oriented algorithms such as deterministic regular
sampling and random sampling parallel sorting, and network-based algorithms
such as Batcher's bitonic sorting algorithm.
  In this work we present an experimental study of integer sorting on multicore
processors. We have implemented serial and parallel radix-sort for various
radixes, deterministic regular oversampling and random oversampling parallel
sorting, and also some previously little explored or unexplored variants of
bitonic-sort and odd-even transposition sort.
  The study uses multithreading and multiprocessing parallel programming
libraries with the C language implementations working under Open MPI,
MulticoreBSP, and BSPlib utilizing the same source code.
  A secondary objective is to attempt to model the performance of these
algorithm implementations under the MBSP (Multi-memory BSP) model. We first
provide some general high-level observations on the performance of these
implementations. If we can conclude anything is that accurate prediction of
performance by taking into consideration architecture dependent features such
as the structure and characteristics of multiple memory hierarchies is
difficult and more often than not untenable. To some degree this is affected by
the overhead imposed by the high-level library used in the programming effort.
We can still draw however some reliable conclusions and reason about the
performance of these implementations using the MBSP model, thus making MBSP
useful and usable."
pub.1110789600,A Study of Integer Sorting on Multicores,"Integer sorting on multicores and GPUs can be realized by a variety of approaches that include variants of distribution-based methods such as radix-sort, comparison-oriented algorithms such as deterministic regular sampling and random sampling parallel sorting, and network-based algorithms such as Batcher’s bitonic sorting algorithm.
                  In this work we present an experimental study of integer sorting on multicore processors. We have implemented serial and parallel radix-sort for various radixes, deterministic regular oversampling, and random oversampling parallel sorting, including new variants of ours, and also some previously little explored or unexplored variants of bitonic-sort and odd-even transposition sort. The study uses multithreading and multiprocessing parallel programming libraries with the same C language code working under Open MPI, MulticoreBSP, and BSPlib. We first provide some general high-level observations on the performance of these implementations. If we can conclude anything is that accurate prediction of performance by taking into consideration architecture dependent features such as the structure and characteristics of multiple memory hierarchies is difficult and more often than not untenable. To some degree this is affected by the overhead imposed by the high-level library used in the programming effort.
                  Another objective is to model the performance of these algorithms and their implementations under the MBSP (Multi-memory BSP) model. Despite the limitations mentioned above, we can still draw some reliable conclusions and reason about the performance of these implementations using the MBSP model, thus making MBSP useful and usable."
pub.1021758023,Find your best match,"Modern multicore platforms allow system administrators to reduce the costs of the IT infrastructure by consolidating heterogeneous workloads on the same physical machine. To this end, it is important to develop efficient profiling techniques and accurate performance predictions to avoid violating service level objectives. In this work we present Tresa, a novel tool to automatically characterize workloads and accurately estimate the execution time of different consolidations. These results can be used to optimize consolidations depending on service-level objectives."
pub.1170588905,Short-Term Power Load Forecasting Based on Spatio-Temporal Multicore Support Vector Regression,"Power system load forecasting is an accurate and scientific prediction of power load usage in the future through the mining of the changing law of power load. The improvement of load forecasting accuracy is essential to ensure the smooth and efficient operation of power system and the development of long-term rational planning. To improve the accuracy of load forecasting, this paper proposed a spatio-temporal multicore support vector regression load forecasting model by simultaneously considering the spatial and temporal characteristics of load time-series. The 1D convolutional neural network (1D-CNN) load spatial feature extraction model was first constructed to mine potential high-dimensional spatial features. Then the bidirectional gated recurrent unit (BiGRU) load time feature extraction model was constructed to merge the acquired forward and reverse output features into an output feature map. Finally, the spatio-temporal features extracted from the load time series data were aggregated and fed into the multicore support vector regression (MSVR) load forecasting model, which effectively improved the accuracy and stability of load forecasting. In order to validate the effectiveness of the proposed model, the prediction results of the proposed method were compared with the latest methods on two open source datasets. The results indicated that the proposed model has the best forecasting performance with an average improvement of 6.745% in the coefficient of determination."
pub.1021151437,Parallel H.264 Decoding on an Embedded Multicore Processor,"In previous work the 3D-Wave parallelization strategy was proposed to increase the parallel scalability of H.264 video decoding. This strategy is based on the observation that inter-frame dependencies have a limited spatial range. The previous results, however, investigate application scalability on an idealized multiprocessor. This work presents an implementation of the 3D-Wave strategy on a multicore architecture composed of NXP TriMedia TM3270 embedded processors. The results show that the parallel H.264 implementation scales very well, achieving a speedup of more than 54 on a 64-core processor. Potential drawbacks of the 3D-Wave strategy are that the memory requirements increase since there can be many frames in flight, and that the latencies of some frames might increase. To address these drawbacks, policies to reduce the number of frames in flight and the frame latency are also presented. The results show that our policies combat memory and latency issues with a negligible effect on the performance scalability."
pub.1146286172,Predicting physical computer systems performance and power from simulation systems using machine learning model,"Software application when executed on computer systems having disparate hardware features, result in dissimilar performance and power. Therefore, selecting systems with optimal performance and power values for application execution is an essential problem to address. However, access to many physical systems is required to collect performance power, which is a demanding task. Therefore, our first objective is to build an accurate prediction model for physical systems to achieve the second objective of computer system selection. To achieve the first objective, we propose a novel model, “cross performance and power prediction with scaling.” We develop a cross prediction model for physical systems by training a decision tree machine learning algorithm on performance and power datasets obtained from a large number of simulation systems built in the Gem5 simulator using emulation mode. However, design differences between Gem5 systems and physical systems lead to large prediction inaccuracies. We determine the application-specific “scaling factor” to compensate for the prediction inaccuracies and apply it to the predicted values for accurate physical systems predictions. We evaluate our model on well-known applications from SD-VBS and MiBench benchmarks achieving errors of 10–25% and 6–40% for performance and power for general-purpose systems. With accurate predictions for physical systems from our model, we achieve the second goal of computer system selection."
pub.1155835873,"Towards faster, greener and easier to program computers","
                    Towards faster, greener and easier to program computers
                    The ERC Consolidator Grant project ECHO (Extending Coherence for Hardware-Driven Optimizations in Multicore Architectures) aims to change the events that occur in multiprocessors such that predictions or decisions being made by the processing units will find the best possible outcome in the future. This way, large performance and energy improvements are expected in future computers leveraging ECHO concepts.
                  "
pub.1094926474,Making Communication a First-Class Citizen in Multicore Partitioning,"Computation-intensive image processing applications need to be implemented on multicore architectures. If they are to be executed efficiently on such platforms, the underlying data and/or functions should be partitioned and distributed among the processors. The optimal partitioning approach is the one which aims to minimize the inter-processor communication while maximizing the load balance. With the continuously increasing number of cores which exacerbates the demand for more complex memory hierarchies, non-uniform memory access, etc., on-chip communication has gained a significant role in taking advantage of the multicore chips. Therefore, making partitioning decisions just based on conventional performance results and without communication profiling is suboptimal. In this paper, we explore the behavior of a mesh decoder as a case study in terms of communication and computation, and propose models that allow early prediction of the application's behavior. Using these models, profiling the application for all of the input samples is not necessary anymore. As a result, communication- and computation-aware parallelization could be performed faster and easier."
pub.1094214428,An Efficient Path Setup for a Photonic Network-on-Chip,"Electrical Network-on-Chip (NoC) faces critical challenges in meeting the high performance and low power consumption requirements for future multicore processors interconnection. Recent tremendous advances in CMOS compatible optical components give the potential for photonics to deliver an efficient NoC performance at an acceptable energy cost. However, the lack of in flight processing and buffering of optical data made the realization of a fully optical NoC complicated. A hybrid architecture which uses optical high bandwidth transfer and a tiny electrical control network can take advantage of both interconnection methods to offer an efficient performance-per-watt infrastructure to connect multicore processors and System-on-Chip (SoC). In this paper, we propose a hybrid photonic torus NoC (HPNoC) that uses a predictive switching to improve the performance of a hybrid architecture. By using prediction techniques, we can reduce the path set up latency for the electrical control network hence improving the overall end-to-end delay for communication in the HPNoC. Simulation results using a cycle accurate simulator under uniform, neighbor and bitreversal traffic patterns for 64 nodes show that predictive switching considerably improves the HPNoC overall performance."
pub.1093172055,Predicting Performance of Hybrid Master/Worker Applications using Model-Based Regression Trees,"Nowadays, there are several features related to node architecture, network topology and programming model that significantly affect the performance of applications. Therefore, the task of adjusting the values of parameters of hybrid parallel applications to achieve the best performance requires a high degree of expertise and a huge effort. Determining a performance model that considers all the system and application features is a very complex task that in most cases produces poor results. In order to simplify this goal and improve the results, we introduce a model-based regression tree technique to improve the accuracy of performance prediction for parallel Master/Worker applications on homogeneous multicore systems. The technique has been used to model the iteration time of the general expression for performance prediction. This approach significantly reduces the effort in getting an accurate prediction model, although it requires a relatively large training data set. The proposed model determines the configuration of the appropriate number of workers and threads of the hybrid application to achieve the best possible performance."
pub.1052943848,Parametric Performance Contracts for Software Components with Concurrent Behaviour,"Performance prediction methods for component-based software systems aim at supporting design decisions of software architects during early development stages. With the increased availability of multicore processors, possible performance gains by distributing threads and processes across multiple cores should be predictable by these methods. Many existing prediction approaches model concurrent behaviour insufficiently and yield inaccurate results due to hard underlying assumptions. In this paper, we present a formal performance prediction approach for component-based systems, which is parameterisable for the number of CPUs or CPU cores. It is able to predict the response time of component services for generally distributed execution times. An initial, simple case study shows that this approach can accurately predict response times of multithreaded software components in specific cases. However, it is limited if threads change the CPU during their execution, if the effect of processor cache thrashing is present, and if the memory bus is heavily used."
pub.1103597636,Topology-aware Virtual Resource Management for Heterogeneous Multicore Systems,"Virtualization technology consolidates multiple independent workloads on a single physical server with virtual resource management, which can result in a significant utilization improvement and energy saving. However, the management of virtual resource is becoming more and more challenging, due to the lack of accurate performance prediction model for the diverse applications' irregular resource access behaviors as well as the complicated Non-Uniform Memory Access (NUMA) server architecture. These challenges drastically affect the overall consolidation performance. This paper proposes vTRMS, a runtime Topology-aware virtual Resource Management Scheme for heterogeneous NUMA multicore systems. vTRMS can improve application performance based on the comprehensive online monitor of the application resource access behaviors as well as an accurate and platform-independent detected NUMA topology metric. Experiment results show that, compared with state-of-art approach, vTRMS can bring up an average throughput improvement of 28.3% and 36.2% on Intel and AMD NUMA machines respectively, when consolidating 32-VMs. At the same time, vTRMS only incurs a runtime overhead no more than 5%."
pub.1041925287,Parallel Branch Prediction on GPU Platform,"Branch Prediction is a common function in nowadays microprocessor. Branch predictor is duplicated into multiple copies in each core of a multicore and many-core processor and makes prediction for multiple concurrent running programs respectively. To evaluate the parallel branch prediction in many-core processor, existed schemes generally use a parallel simulator running in CPU which does not have a real passive parallel running environment to support a many-core simulation and thus has bad simulating performance. In this paper, we firstly try to use a real many-core platform, GPU, to do a parallel branch prediction for future general purpose many-core processor. We verify the new GPU based parallel branch predictor against the traditional CPU based branch predictor. Experiment result shows that GPU based parallel simulation scheme is a promising way to faster simulating speed for future many-core processor research."
pub.1011583543,Juggle,"We investigate proactive dynamic load balancing on multicore systems, in which threads are continually migrated to reduce the impact of processor/thread mismatches to enhance the flexibility of the SPMD-style programming model, and enable SPMD applications to run efficiently in multiprogrammed environments. We present Juggle, a practical decentralized, user-space implementation of a proactive load balancer that emphasizes portability and usability. Juggle shows performance improvements of up to 80% over static balancing for UPC, OpenMP, and pthreads benchmarks. We analyze the impact of Juggle on parallel applications and derive lower bounds and approximations for thread completion times. We show that results from Juggle closely match theoretical predictions across a variety of architectures, including NUMA and hyper-threaded systems. We also show that Juggle is effective in multiprogrammed environments with unpredictable interference from unrelated external applications."
pub.1106120540,Functional Link Artificial Neural Network (FLANN) Based Design of a Conditional Branch Predictor,"Conditional branch predictor (CBP) is an essential component in the design of any modern deeply pipelined superscalar microprocessor architecture. In the recent past, many researchers have proposed varieties schemes for the design of the CBPs that claim to offer desired levels of accuracy and speed needed to meet the demand for the architectural design of multicore processors. Among various schemes in practice to realize the CBPs, the ones based on neural computing—i.e., artificial neural network (ANN) has been found to outperform other CBPs in terms of accuracy. Functional link artificial neural network (FLANN) is a single layer ANN with low computational complexity which has been used in versatile fields of application, such as system identification, pattern recognition, prediction and classification, etc. In all these areas of application, FALNN has been tested and found to provide superior performance compared to their multilayer perceptron (MLP) counterpart. This paper proposes design of a novel FLANN-based dynamic branch predictor and compares the performance against a perceptron-based CBP. The proposed FALNN-based CBP has been implemented in C++. The performance of the proposed CBP has been evaluated using standard benchmark trace data files and is found to have performance comparable to the existing perceptron-based predictions in terms of speed and accuracy."
pub.1020813562,Applying SVM to Data Bypass Prediction in Multi Core Last-Level Caches,"Bypassing emerged as a performance improvement method for shared Last-Level Caches (LLC) in multicore processors where large data portions are never reused, wasting system resources. This paper proposes an alternative method to predict data bypassing using Support Vector Machine (SVM). Based on access traces obtained from a simulator, SVM is trained to generate bypass models which are integrated into the simulator to quantify LLC performance improvements. Results show that SVM can classify which data to bypass, improving LLC performance, achieving an average 6.72% miss rate decrease across SPLASH2 benchmark combinations."
pub.1094942633,"Power and Performance Characterization, Analysis and Tuning for Energy-Efficient Edge Detection on Atom and ARM Based Platforms","The de facto standard for embedded platforms with medium to low computing demands are ARM with Thumb ISA and Intel Atom with the X86 ISA with multiple cores. Operating these architectures in the milliwatts range while running realtime computer vision corner detection algorithms is a challenging problem. We present the analysis of power, performance and energy-efficiency measurements of Harris corner detection across a wide range of voltage and frequency settings, multicore/multithreading strategies, and compiler and application optimization parameters to find how the interplay of these parameters affect the power, performance and energy-efficiency. Our measurement of results on state-of-the-art embedded platforms demonstrate that a systematic cross-layer optimization at the application level (Sobel filter type, aperture size, number of image tiles), compiler level (branch prediction, function inlining) and system level (voltage and frequency setting, single core vs multicore implementation) significantly improves the energy-efficiency of corner detection, while meeting its real-time performance constraints. This cross-layer optimization improves the energy-efficiency of Harris corner on Atom and ARM by 89.5% and 87.2%, respectively."
pub.1107803727,Floreon+ Modules: A Real-World HARPA Application in the High-End HPC System Domain,"This chapter is centered around uncertainty computation with on-demand resource allocation for run-off prediction in a High-Performance Computer environment. Our research stands on a runtime operating system that automatically adapts resource allocation with the computation to provide precise outcomes before the time deadline. In our case, input data comes from several gauging stations, and when newly updated data arrives, models must be re-executed to provide accurate results immediately. Since the models run continuously (24/7), their computational demand is different during various hydrological events (e.g. periods with heavy rain and without any rain) and therefore computational resources have to be balanced according to the event severity. Although these kinds of models should run constantly, they are very computationally demanding during discrete periods of time, for example in the case of heavy rain. Then, the accuracy of the results must be as close as possible to reality. The work relies on the HARPA runtime resource manager that adapts resource allocation to the runtime-variable performance demand of applications. The resource assignment is temperature-aware: the application execution is dynamically migrated to the coolest cores, and this has a positive impact on the system reliability."
pub.1149089226,Runtime Energy Savings Based on Machine Learning Models for Multicore Applications,"To improve the power consumption of parallel applications at the runtime, modern processors provide frequency scaling and power limiting capabilities. In this work, a runtime strategy is proposed to maximize energy savings under a given performance degradation. Machine learning techniques were utilized to develop performance models which would provide accurate performance prediction with change in operating core-uncore frequency. Experiments, performed on a node (28 cores) of a modern computing platform showed significant energy savings of as much as 26% with performance degradation of as low as 5% under the proposed strategy compared with the execution in the unlimited power case."
pub.1094263181,Accurate Multicore Processor Power Models for Power-Aware Resource Management,"Power management is one of the biggest challenges facing current datacenters. As processors consume the dominant amount of power in computer systems, power management of multicore processors is extremely significant. An efficient power model that accurately predict the power consumption of a processor is required to develop effective power management techniques. However, this challenge rises with using virtualization and increasing number of cores in the processors. In this paper, we analyze power consumption of a multicore processor; we develop three statistical CPU- Power models based on the number of active cores and average running frequency using a multiple liner regression. Our models are built upon a virtualized server. The models are validated statistically and experimentally. Statistically, our models cover 97% of system variations. Furthermore, we test our models with different workloads and three benchmarks. The results show that our models achieve better performance compared to the recently proposed model for power management in virtualized environments. Our models provide highly accurate predictions for un-sampled combinations of frequency and cores; 95% of the predicted values have less than 7% error. Thus, we can integrate these models into power management mechanisms for a dynamic configuration of a virtual machine in terms of the number of its virtual-CPUs and the frequency of physical cores to achieve both performance and power constrains."
pub.1094222060,The Research and Design of Branch Prediction based on Multicore Heterogeneous,"Aiming at those problem that it was difficult to improve the processor performance only by improving the single core frequency, as well as superscalar pipeline stall when process a branch instruction, the architecture of heterogeneous multi-core processor which used B-Cache structure and C-Core processor controller was introduced in this paper. The new architecture avoided the pipeline flushed due to branch miss-predict, and improve overall efficiency of Multi-Core processor."
pub.1094764586,PROBE: Prediction-based Optical Bandwidth Scaling for Energy-efficient NoCs,"Optical interconnect is a disruptive technology solution that can overcome the power and bandwidth limitations of traditional electrical Networks-on-Chip (NoCs). However, the static power dissipated in the external laser may limit the performance of future optical NoCs by dominating the stringent network power budget. From the analysis of real benchmarks for multicores, it is observed that high static power is consumed due to the external laser even for low channel utilization. In this paper, we propose PROBE: Prediction-based Optical Bandwidth Scaling for Energy-efficient NoCs by exploiting the latencylbandwidth trade-off to reduce the static power consumption by increasing the average channel utilization. With a lightweight prediction technique, we scale the bandwidth adaptively to the changing traffic demands while maintaining reasonable performance. The performance on synthetic and real traffic (PARSEC, Splash-2) for 64-cores indicate that our proposed bandwidth scaling technique can reduce optical power by about 60% with at most 11% throughput penalty."
pub.1137430449,RNA Secondary Structure Prediction Parallel Algorithm on Shared Memory Multicore Architecture,"In the era of new technological environment, multi-core architecture found to be advantageous which provides concurrent processing. Many application domains include multi-core processors such as general purpose applications, medical, computational biology and bioinformatics. In this research work, we have focused on bioinformatics area mainly on the parallelization of prediction of secondary structure for RNA on multi-core architecture with the help of genetic algorithm which focuses on to find secondary structure of RNA and to find free energy of predicted RNA secondary structure because the optimal secondary structure is one which has the minimum free energy. Basically, RNA has single-stranded structure, and because of negative charge in the backbone of RNA, RNA molecules folds back on itself by pairing the bases to form 2D structure called secondary structure. This paper describes parallel multi-core algorithm called GAfold. Gutell Database provides the primary RNA sequences of various viruses. Dynamic programming model is considered here a base for RNA secondary structure prediction, and free energy calculation is based on nearest neighbour thermodynamic model (NNTM). The performance of GAfold is analyzed for up to 40,000 of length of bases on various multicore architectures."
pub.1129509118,Bridging the Architecture Gap: Abstracting Performance-Relevant Properties of Modern Server Processors,"We propose several improvements to the execution-cache-memory (ECM) model, an analytic performance model for predicting single- and multicore runtime of steady-state loops on server processors. The model is made more general by strictly differentiating between application and machine models: an application model comprises the loop code, problem sizes, and other runtime parameters, while a machine model is an abstraction of all performance-relevant properties of a processor. Moreover, new first principles underlying the model’s estimates are derived from common microarchitectural features implemented by today’s server processors to make the model more architecture independent, thereby extending its applicability beyond Intel processors. We introduce a generic method for determining machine models, and present results for relevant server-processor architectures by Intel, AMD, IBM, and Marvell/Cavium. Considering this wide range of architectures, the set of features required for adequate performance modeling is surprisingly small. To validate our approach, we compare performance predictions to empirical data for an OpenMP-parallel preconditioned CG algorithm, which includes compute- and memory-bound kernels. Both single- and multicore analysis shows that the model exhibits average and maximum relative errors of 5 % and 10 %. Deviations from the model and insights gained are discussed in detail."
pub.1094250977,A Flexible Framework for Throttling-Enabled Multicore Management (TEMM),"Hardware execution throttling mechanisms such as duty cycle modulation and voltage/frequency scaling can effectively control core or chip-level resource consumption and hence have been advocated to manage multicore resource competition. However, finding the right throttle setting is challenging since the configuration space grows exponentially as the number of cores increases, making the naive approach of exhaustive search untenable. This paper proposes a flexible framework for Throttling-Enabled Multicore Management (TEMM) that efficiently finds a high-quality hardware execution throttling configuration for a user-specified resource management objective. In a manner similar to the Newton-Raphson method in numerical analysis, TEMM employs an iterative method to continuously improve the configuration search quality by leveraging the search results from previous iterations. Within each iteration, TEMM extrapolates the effects of throttling from reference configurations, searches for a high-quality throttling configuration based on model predictions (accelerated by hill climbing), sample-runs the selected configuration, and adds the measured performance and recorded execution statistics of interest as a new reference. Our evaluations show TEMM can quickly arrive at the exact or close to optimal throttling configuration."
pub.1030405677,Analysis on the Thermal Efficiency of Branch Prediction Techniques in 3D Multicore Processors,"Speculative execution for improving instruction-level parallelism is widely used in high-performance processors. In the speculative execution technique, the most important factor is the accuracy of branch predictor. Unfortunately, complex branch predictors for improving the accuracy can cause serious thermal problems in 3D multicore processors. Thermal problems have negative impact on the processor performance. This paper analyzes two methods to solve the thermal problems in the branch predictor of 3D multi-core processors. First method is dynamic thermal management which turns off the execution of the branch predictor when the temperature of the branch predictor exceeds the threshold. Second method is thermal-aware branch predictor placement policy by considering each layer's temperature in 3D multi-core processors. According to our evaluation, the branch predictor placement policy shows that average temperature is , and average maximum temperature gradient is . And, dynamic thermal management shows that average temperature is and average maximum temperature gradient is . Proposed branch predictor placement policy has superior thermal efficiency than the dynamic thermal management. In the perspective of performance, the proposed branch predictor placement policy degrades the performance by 3.61%, while the dynamic thermal management degrades the performance by 27.66%."
pub.1092926823,SVM-Based Dynamic Voltage Prediction for Online Thermally Constrained Task Scheduling in 3-D Multicore Processors,"Hotspots occur frequently in 3-D multicore processors (3D-MCPs) and they may adversely impact the reliability of the system and its lifetime. We present support-vector-machine (SVM)-based dynamic voltage assignment (SVMDVA) strategy to select voltages among low-power and high-performance operating modes for reducing hotspots and optimizing performance in 3D-MCPs. The proposed SVMDVA can be employed in online, thermally constrained task schedulers. First, we revealed two different thermal regions of 3D-MCPs and extract different key features of these regions. Based on these key features, SVM models are constructed to predict the thermal behavior and the best operation mode of 3D-MCPs during runtime. SVMDVA using SVM models with monitoring workload and temperature behavior can effectively limit the temperature increase in 3D-MCPs. This is extremely important for accurately predicting the thermal behavior and providing the optimum operating condition of 3D-MCPs to achieve the best system performance. Experimental results show that SVMDVA is an effective technique for reducing hotspot occurrences (57.19%) and increasing throughput (25.41%) for 3D-MCPs."
pub.1120317660,AdaMD: Adaptive Mapping and DVFS for Energy-Efficient Heterogeneous Multicores,"Modern heterogeneous multicore systems, containing various types of cores, are increasingly dealing with concurrent execution of dynamic application workloads. Moreover, the performance constraints of each application vary, and applications enter/exit the system at any time. Existing approaches are not efficient in such dynamic scenarios, especially if applications are unknown, as they require extensive offline application analysis and do not consider the runtime execution scenarios (application arrival/completion, and workload and performance variations) for runtime management. To address this, we present AdaMD, an adaptive mapping and dynamic voltage and frequency scaling (DVFS) approach for improving energy consumption and performance. The key feature of the proposed approach is the elimination of dependency on offline profiled results while making runtime decisions. This is achieved through a performance prediction model having a maximum error of 7.9% lower than the previously reported model and a mapping approach that allocates processing cores to applications while respecting performance constraints. Furthermore, AdaMD adapts to runtime execution scenarios efficiently by monitoring the application status, and performance/workload variations to adjust the previous DVFS settings and thread-to-core mappings. The proposed approach is experimentally validated on the Odroid-XU3, with various combinations of diverse multithreaded applications from PARSEC and SPLASH benchmarks. Results show energy savings of up to 28% compared to the recently proposed approach while meeting performance constraints."
pub.1032661372,A statistical performance model of the opteron processor,"Cycle-accurate simulation is the dominant methodology for processor design space analysis and performance prediction. However, with the prevalence of multi-core, multi-threaded architectures, this method has become highly impractical as the sole means for design due to its extreme slowdowns. We have developed a statistical technique for modeling multicore processors that is based on Monte Carlo methods. Using this method, processor models of contemporary architectures can be developed and applied to performance prediction, bottleneck detection, and limited design space analysis. To date, we have accurately modeled the IBM Cell, the Intel Itanium, and the Sun Niagara 1 and Niagara 2 processors [23, 22, 8]. In this paper, we present a work in progress which is applying this methodology to an out-of-order execution processor. We present the initial single-core model and results for the AMD Barcelona (Opteron) processor."
pub.1159837631,Adaptive Machine Learning-Based Proactive Thermal Management for NoC Systems,"Because of the high-complex interconnection in contemporary multicore systems, the network-on-chip (NoC) technology has been proven as an efficient way to solve the communication problem in multicore systems. However, the thermal problem becomes the main design challenge in the current NoC systems due to the high-diverse workload distribution and large power density. Therefore, proactive dynamic thermal management (PDTM) is employed as an efficient way to control the system temperature. Based on the predicted temperature information, the PDTM can control the system temperature in advance to reduce the performance impact during the temperature control period. However, conventional temperature prediction models are usually built based on specific physical parameters, which are usually temperature-sensitive. Consequently, the current temperature prediction models still result in significant temperature prediction errors. To solve this problem, a novel adaptive machine learning (ML)-based PDTM is proposed in this work. The adaptive ML-based PDTM first uses an adaptive single layer perceptron (ASLP), which is composed of a single-neuron operation and a least mean square (LMS) adaptive filter technology, to precisely predict the future temperature. Afterward, the proposed adaptive reinforcement learning (RL) is used to find the proper throttling ratio to control the system temperature. In this way, the proposed adaptive ML-based PDTM can adapt to the hyperplane of the temperature behavior of the NoC system and provide a proper temperature control strategy at runtime. Compared with related works, the proposed approach reduces average temperature prediction error by 0.2%–78.0% and improves the system performance by 2.4%–43.0% with smaller hardware overhead."
pub.1008160721,Online power-performance adaptation of multithreaded programs using hardware event-based prediction,"With high-end systems featuring multicore/multithreaded processors and high component density, power-aware high-performance multithreading libraries become a critical element of the system software stack. Online power and performance adaptation of multithreaded code from within user-level runtime libraries is a relatively new and unexplored area of research. We present a user-level library framework for nearly optimal online adaptation of multithreaded codes for low-power, high-performance execution. Our framework operates by regulating concurrency and changing the processors/threads configuration as the program executes. It is innovative in that it uses fast, runtime performance prediction derived from hardware event-driven profiling, to select thread granularities that achieve nearly optimal energy-efficiency points. The use of predictors substantially reduces the runtime cost of granularity control and program adaptation. Our framework achieves performance and ED2 (energy-delay-squared) levels which are: i) comparable to or better than those of oracle-derived offline predictors; ii) significantly better than those of online predictors using exhaustive or localized linear search. The complete prediction and adaptation framework is implemented on a real multi-SMT system with Intel Hyperthreaded processors and embeds adaptation capabilities in OpenMP programs."
pub.1092516507,Harnessing voltage margins for energy efficiency in multicore CPUs,"In this paper, we present the first automated system-level analysis of multicore CPUs based on ARMv8 64-bit architecture (8-core, 28nm X-Gene 2 micro-server by AppliedMicro) when pushed to operate in scaled voltage conditions. We report detailed system-level effects including SDCs, corrected/uncorrected errors and application/system crashes. Our study reveals large voltage margins (that can be harnessed for energy savings) and also large Vmin variation among the 8 cores of the CPU chip, among 3 different chips (a nominal rated and two sigma chips), and among different benchmarks. Apart from the Vmin analysis we propose a new composite metric (severity) that aggregates the behavior of cores when undervolted and can support system operation and design protection decisions. Our undervolting characterization findings are the first reported analysis for an enterprise class 64-bit ARMv8 platform and we highlight key differences with previous studies on x86 platforms. We utilize the results of the system characterization along with performance counters information to measure the accuracy of prediction models for the behavior of benchmarks running in particular cores. Finally, we discuss how the detailed characterization and the prediction results can be effectively used to support design and system software decisions to harness voltage margins for energy efficiency while preserving operation correctness. Our findings show that, on average, 19.4% energy saving can be achieved without compromising the performance, while with 25% performance reduction, the energy saving raises to 38.8%."
pub.1104529735,Guiding the Optimization of Parallel Codes on Multicores Using an Analytical Cache Model,"Cache performance is particularly hard to predict in modern multicore processors as several threads can be concurrently in execution, and private cache levels are combined with shared ones. This paper presents an analytical model able to evaluate the cache performance of the whole cache hierarchy for parallel applications in less than one second taking as input their source code and the cache configuration. While the model does not tackle some advanced hardware features, it can help optimizers to make reasonably good decisions in a very short time. This is supported by an evaluation based on two modern architectures and three different case studies, in which the model predictions differ on average just 5.05% from the results of a detailed hardware simulator and correctly guide different optimization decisions."
pub.1095072772,Modeling Cache Performance Beyond LRU,"Modern processors use high-performance cache replacement policies that outperform traditional alternatives like least-recently used (LRU). Unfortunately, current cache models do not capture these high-performance policies as most use stack distances, which are inherently tied to LRU or its variants. Accurate predictions of cache performance enable many optimizations in multicore systems. For example, cache partitioning uses these predictions to divide capacity among applications in order to maximize performance, guarantee quality of service, or achieve other system objectives. Without an accurate model for high-performance replacement policies, these optimizations are unavailable to modern processors. We present a new probabilistic cache model designed for high-performance replacement policies. It uses absolute reuse distances instead of stack distances, and models replacement policies as abstract ranking functions. These innovations let us model arbitrary age-based replacement policies. Our model achieves median error of less than 1 % across several high-performance policies on both synthetic and SPEC CPU2006 benchmarks. Finally, we present a case study showing how to use the model to improve shared cache performance."
pub.1113241834,Cache Reconfiguration Using Machine Learning for Vulnerability-aware Energy Optimization,"Dynamic cache reconfiguration has been widely explored for energy optimization and performance improvement for single-core systems. Cache partitioning techniques are introduced for the shared cache in multicore systems to alleviate inter-core interference. While these techniques focus only on performance and energy, they ignore vulnerability due to soft errors. In this article, we present a static profiling based algorithm to enable vulnerability-aware energy-optimization for real-time multicore systems. Our approach can efficiently search the space of cache configurations and partitioning schemes for energy optimization while task deadlines and vulnerability constraints are satisfied. A machine learning technique has been employed to minimize the static profiling time without sacrificing the accuracy of results. Our experimental results demonstrate that our approach can achieve 19.2% average energy savings compared with the base configuration, while drastically reducing the vulnerability (49.3% on average) compared to state-of-the-art techniques. Furthermore, the machine learning technique enabled more than 10x speedup in static profiling time with a negligible prediction error of 3%."
pub.1086387384,Parallel Implementation of a Simplified Semi-physical Wildland Fire Spread Model Using OpenMP,"We present a parallel 2D version of a simplified semi-physical wildland fire spread model based on conservation equations, with convection and radiation as the main heat transfer mechanisms. This version includes some 3D effects. The OpenMP framework allows distributing the prediction operations among the available threads in a multicore architecture, thereby reducing the computational time and obtaining the prediction results much more quickly. The results from the experiments using data from a real fire in Galicia (Spain) confirm the benefits of using the parallel version."
pub.1167147136,Accelerating prediction of RNA secondary structure using parallelization on multicore architecture,"Due to Covid pandemic, the investigation of bioinformatics, and more specifically the investigation of RNA, has become a major focus of the research. mRNA-based vaccines have been developed by a significant number of academician and scientists. When a virus infects a human body, it first causes disruption to the host’s RNA structure, then it transforms the host’s RNA into its own genetic structure. As a result, it is vital to research on the process of making predictions about the secondary structures of RNA. The process of predicting the secondary structure of long RNA sequences takes a significant amount of time. This study makes a contribution to the implementation of the algorithm that finds the predicted secondary structure. The methodology for RNA secondary prediction that was employed in this study is based on a dynamic programming model that uses shared memory multicore architecture. The Nearest Neighbor Thermodynamic Model (NNTM) is used as the foundation for the determination of the least amount of free energy. The Gutell database has been used for the RNA sequences of a number of different bacterial species. A comparison was made between the amount of time required to identify the secondary structure of RNA. The length of the RNA sequence is a factor that affects the performance of many existing methods, but the proposed GAfold technique handles sequences of any length. Once the secondary structure has been identified, it helped to detect the virus. It is discovered that the GAfold approach speeds up RNA Secondary structure’s prediction. The proposed GAfold algorithm offers a speed increase factor of 2.5 ns on a four-core architecture, 3.09 ns on an eight-core design, and 5.5 ns on a twelve-core architecture. It has been shown that predicting suboptimal structures enhanced the accuracy of the free energy minimization algorithm, which in turn improved the accuracy of full RNA."
pub.1094124250,Communication-Optimal Parallel N-body Solvers,"We present new analysis, algorithmic techniques, and implementations of the Fast Multipole Method (FMM) for solving N-body problems. Our research specifically addresses two key challenges. The first challenge is how to engineer fast code for today's platforms. We present the first in-depth study of multicore optimizations and tuning for FMM, along with a systematic approach for transforming a conventionally-parallelized FMM into a highly-tuned one. We introduce novel optimizations that significantly improve the within-node scalability of the FMM, thereby enabling high-performance in the face of multicore and manycore systems. The second challenge is how to understand scalability on future systems. We present a new algorithmic complexity analysis of the FMM that considers both intra- and inter-node communication costs. This analysis yields the surprising prediction that although the FMM is largely compute-bound today, and therefore highly scalable on current systems, the trajectory of processor architecture designs if there are no significant changes-could cause it to become communication-bound as early as the year 2020. This prediction suggests the utility of our analysis approach, which directly relates algorithmic and architectural characteristics, for enabling a new kind of high-level algorithm-architecture co-design."
pub.1108008898,Predicting Performance Using Collaborative Filtering,"Performance prediction of parallel applications across systems becomes increasingly important in today's diverse computing environments. A wide range of choices in execution platforms pose new challenges to researchers in choosing a system which best fits their workloads and administrators in scheduling applications to the best performing systems. While previous studies have employed simulation- or profile-based prediction approaches, such solutions are time-consuming to be deployed on multiple platforms. To address this problem, we use two collaborative filtering techniques to build analytical models which can quickly and accurately predict the performance of workloads across different multicore systems. The first technique leverages information gained from performance observed for certain applications on a subset of systems and use it to discover similarities among applications as well as systems. The second collaborative filtering based model learns latent features of systems and workloads automatically and use these features to characterize the performance of applications on different platforms. We evaluated both the methods using 30 workloads chosen from NAS Parallel Benchmarks, BOTS and Rodinia benchmarking suites on ten different systems. Our results show that such collaborative filtering methods can make predictions with RMSE as low as 0.6 and with an average RMSE of 1.6."
pub.1154834089,Runoff Forecast Model Based on an EEMD-ANN and Meteorological Factors Using a Multicore Parallel Algorithm,"Abstract
Accurate long-term runoff forecasting is crucial for managing and allocating water resources. Due to the complexity and variability of natural runoff, the most difficult problems currently faced by long-term runoff forecasting are the difficulty of model construction, poor prediction accuracy, and time intensive forecasting processes. Therefore, this study proposes a hybrid long-term runoff forecasting framework that uses the antecedent inflow and specific meteorological factors as the inputs, is modeled by ensemble empirical mode decomposition (EEMD) coupled with an artificial neural network (ANN), and computed by a parallel algorithm. First, the framework can transform monthly inflow and meteorological series into stationary signals via EEMD to more comprehensively explore the relationships of the input factors through the ANN. Second, the selected meteorological factors that are closely related to inflow formation can be filtered out by the single correlation coefficient method, which contributes to reducing coupling between input factors, and increases the accuracy of the prediction models. Finally, a multicore parallel algorithm that is easily accessed everywhere and that fully utilizes multiple calculation resources while flexibly contending with various optimization requirements will improve forecasting efficiency. The Xiaowan Hydropower Station (XW) is selected as the study area, and the final results of the study show that (1) the addition of targeted meteorological factors does indeed greatly enhance the performance of the prediction models; (2) the five criteria for evaluating the prediction accuracy show that the EEMD-ANN model is far superior to the prediction performance from the ordinary ANN model when run under the same input conditions; and (3) the optimization time of the 32-core model can be reduced by as much as 25 times, which significantly saves time during the forecast process."
pub.1039535603,Microarchitectural performance comparison of Intel Knights Corner and Intel Sandy Bridge with CFD applications,"This paper comparatively evaluates the microarchitectural performance of two representative Computational Fluid Dynamics (CFD) applications on the Intel Many Integrated Core (MIC) product, the Intel Knights Corner (KNC) coprocessor, and the Intel Sand Bridge (SNB) processor. Performance Monitoring Unit-based measurement method is used, along with a two-phase measurement method and some considerations to minimize the errors and instabilities. The results show that the CFD applications are sensitive to architecture factors. Their single thread performance and efficiency on KNC are much lower than that on SNB. Branch prediction and memory access are two primary factors that make the performance difference. The applications’ low-computational intensity and inefficient vector instruction usage are two additional factors. To be more efficient for the CFD applications, the MIC architecture needs to improve its branch prediction mechanism and memory hierarchy. Fine tuning of application codes is also crucial and is hard work."
pub.1104150575,Contention-Aware Fair Scheduling for Asymmetric Single-ISA Multicore Systems,"Asymmetric single-ISA multicore processors (AMPs), which integrate high-performance big cores and low-power small cores, were shown to deliver higher performance per watt than symmetric multicores. Previous work has demonstrated that the OS scheduler plays an important role in realizing the potential of AMP systems. While throughput optimization on AMPs has been extensively studied, delivering fairness on these platforms still constitutes an important challenge to the OS. To this end, the scheduler must be equipped with a mechanism enabling to accurately track the progress that each application in the workload makes as it runs on the various core types throughout the execution. In turn, this progress largely depends on the benefit (or speedup) that an application derives on a big core relative to a small one, which may differ greatly across applications. While existing fairness-aware schedulers take application relative speedup into consideration when tracking progress, they do not cater to the performance degradation that may occur naturally due to contention on shared resources among cores, such as the last-level cache or the memory bus. In this paper, we propose CAMPS, a contention-aware fair scheduler for AMPs that primarily targets long-running compute-intensive workloads. Unlike other schemes, CAMPS does not require special hardware extensions or platform-specific speedup-prediction models to function. Our experimental evaluation, which leverages real asymmetric hardware and scheduler implementations in the Linux kernel, demonstrates that CAMPS improves fairness by up to 11 percent with respect to a state-of-the-art fairness-aware OS-level scheme, while delivering better system throughput."
pub.1095048363,Performance Characterization and Evaluation of HPC Algorithms on Dissimilar Multicore Architectures,"In this paper, we share our experiences in using two important yet different High Performance Computing (HPC) architectures for evaluating two HPC algorithms. The first architecture is an Intel x64 ISA based homogenous multicore with Uniform Memory Access (UMA) type shared-memory based Symmetric Multi-Processing system. The second architecture is an IBM Power ISA based heterogenous multicore with Non-Uniform Memory Access (NUMA) based distributed-memory Asymmetric Multi-Processing system. The two HPC algorithms are for predicting biological molecular structures, specifically the RNA secondary structures. The first algorithm that we created is a parallelized version of a popular serial RNA secondary structure prediction algorithm called PKNOTS. The second algorithm is a new parallel-by-design algorithm that we have developed called MARSs. Using real Ribo-Nucleic Acid (RNA) sequences, we conducted large-scale experiments involving hundreds of sequences using the above two algorithms. Based on thousands of data points that we collected as an outcome of our experiments, we report on the observed performance metrics for both the algorithms on the two architectures. Through our experiments, we infer that architectures with specialized coprocessors for number-crunching along with high-speed memory bus and dedicated bus controllers generally perform better than general-purpose multi-processor architectures. In addition, we observed that algorithms that are intrinsically parallelized by design are able to scale & perform better by taking advantage of the underlying parallel architecture. We further share best practices on handling scalability aspects with regards to workload size. We believe our results are applicable to other HPC applications on similar HPC architectures."
pub.1143225132,Application of Gabor Image Recognition Technology in Intelligent Clothing Design,"Aiming at the complex problem of image recognition feature extraction, this paper proposes an intelligent clothing design model based on parallel Gabor image feature extraction algorithm. Based on the intelligent parallel mode, the algorithm decomposes and merges the calculation process of the image Gabor transformation, decomposes the entire image Gabor feature extraction calculation process into a parallel part and a nonparallel part, and accelerates the parallel part by using multiple cores. The calculation results are then combined to achieve the purpose of multicore parallel acceleration of the entire calculation process. Secondly, based on the consideration of improving the real-time performance of the intelligent clothing design system, combined with the existing multicore environment, this paper uses the intelligent model to design and implement the image parallel Gabor feature extraction algorithm and uses image processing and analysis technology to analyze the visual elements of traditional clothing and identify and quantify to form a relatively complete clothing visual element evaluation system, which provides a basis for large-scale collection and automated evaluation of clothing visual effects, as well as clothing trend tracking and prediction. Experiments show that the algorithm can effectively shorten the calculation time of Gabor image feature extraction and can obtain a good speedup in a multicore environment. At the same time, it combines with a multiscale intelligent clothing classification algorithm, on the basis of the VS2008 platform, combined with OpenCV 2.0, designed and implemented an intelligent clothing design system, and conducted experiments and system tests. The experimental results show that the algorithm given in this paper can accurately segment fabric defects from the background, which proves that the detection algorithm has a good detection effect. Simulation results show that the algorithm proposed in this paper can more accurately identify the state of clothing features, and the real-time performance of intelligent clothing design in a multicore environment has been improved to a certain extent."
pub.1094328989,Adaptive Packet Resizing by Spatial Locality and Data Sharing for Energy-Efficient NOC,"Single-processor chips have given way to multicore chips to enable a cost-effective implementation of computer systems. Toward continuous performance scaling, Network-On-Chip (NOC) is the communication architecture supporting the core count increase to hundreds or thousands in multicore chips. Low-power, low-latency, and high-bandwidth support in the NOC design is critical for meeting performance and energy targets of the overall system. Much of previous work has focused on improving the NOC design but without more fully taking into consideration the communication characteristics and the interplay with cache memory that can be exploited in the NOC design. In this paper, low spatial locality within cache blocks is exploited in reducing memory traffic toward energy savings in the NOC. We present a spatial locality predictor that separately manages different degrees of spatial locality across shared and private blocks for better prediction accuracy. To further optimize performance and power in the NOC, we present the adaptive control of the predictor and packet data resizing techniques. Evaluations for the 16-core system running PARSEC benchmarks reveal that our spatial-locality based packet resizing improves NOC power consumption on average by 21% (up to 33%)."
pub.1132552087,On-Board Satellite Telemetry Forecasting with RNN on RISC-V Based Multicore Processor,"The aim of this paper is to assess the feasibility and on-board hardware performance requirements for on-board telemetry forecasting by implementing a Recurrent Neural Network (RNN) on low-cost multicore RISC-V microprocessor. Gravity field and steady-state Ocean Circulation Explorer (GOCE) public telemetry data was used for training RNNs with different hyperparameters and architectures. The prediction accuracy of these models was evaluated using mean error and R-squared score on the same test dataset. The implementation of the RNN on a RISC-V embedded device, representative of future space-grade hardware, required some adaptations and modifications due to the computational requirements and the large memory footprint. The algorithm was implemented to run in parallel on the 8 cores of the microprocessor and tiling was employed for the weight matrices. Further considerations have also been made for the approximation of sigmoid and hyperbolic tangent as activation functions."
pub.1135085959,Online Machine Learning for Energy-Aware Multicore Real-Time Embedded Systems,"In this article, we present an Online Learning Artificial Neural Network (ANN) model that is able to predict the performance of tasks in lower frequency levels and safely optimize real-time embedded systems’ power saving operations. The proposed ANN model is supported by feature selection, which provides the most relevant variables to describe shared resource contention in the selected multicore architecture. The variables are used at runtime to produce a performance trace that encompasses sufficient information for the ANN model to predict the impact of a frequency change on the performance of tasks. A migration heuristic encompassing a weighted activity vector is combined with the ANN model to dynamically adjust frequencies and also to trigger task migrations among cores, enabling further optimization by solving resource contentions and balancing the load among cores. The proposed solution achieved energy-savings of 24.97 percent on average when compared to the run-to-end approach, and it did it without compromising the criticality of any single task. The overhead incurred in terms of execution time was 0.1791 percent on average. Each prediction added 15.3585$\mu s$μs on average and each retraining cycle triggered at frequency adjustments was never larger than 100$\mu s$μs."
pub.1093873843,Instruction prefetching using Basicblock prediction,"Memory latency is a significant bottleneck in modern computer architectures, especially for commercial and multimedia applications. Instruction cache misses can severely limit the performance, due to advent of superscalar processors and multicore systems. Prefetching is one of the promising method to bridge the performance gap between CPU and DRAM speed. Although Instruction prefetching is a promising technique to hide the memory latency, they fail to issue prefetches early enough for modern superscalar processors. To overcome these limitations, we propose a new instruction prefetching technique called Basicblock Instruction Prefetching that employs a prefetch engine which issues prefetch instructions to achieve useful and early prefetches far enough in advance. Our prefetching design results in good coverage, is accurate, and produces timely results that can be effectively used by the processor. Performance evaluation is carried out through cycle-accurate trace-driven simulation. The experimental results show that the proposed scheme is successful in 80% accurate prediction and achieves better timeliness."
pub.1093792299,Analysing and Modelling the On-Chip Traffic of Parallel Applications,"In this paper, we investigate the traffic characteristics of parallel and high performance computing applications. Parallel applications that utilize multiple processing cores are widespread nowadays due to the trend of multicore processors. However the design paradigm of traditional sequential execution and concurrent execution can vary significantly. Therefore the estimation and prediction approaches used in conventional software can be limited for parallel applications. The communication among different nodes in a multicore system should be analysed and categorized in order to improve the accuracy of system simulation. We study several parallel applications running on a full system simulation environment. The communication traces among different nodes are collected and analysed. We discuss the detailed characteristics of these applications. The applications are grouped into different categories depending on several parallel programming paradigms. We apply power-law model with maximum likelihood estimation, Gaussian mixture model, as well as the polynomial model for fitting the trace data. A generic synthetic traffic model is proposed based on the results. Experiments show the proposed model can be used to evaluate the performance of parallel systems more accurately than by other synthetic traffic models."
pub.1101865544,Extending the Power-Efficiency and Performance of Photonic Interconnects for Heterogeneous Multicores with Machine Learning,"As communication energy exceeds computation energy in future technologies, traditional on-chip electrical interconnects face fundamental challenges in the many-core era. Photonic interconnects have been proposed as a disruptive technology solution due to superior performance per Watt, distance independent energy consumption and CMOS compatibility for on-chip interconnects. Static power due to the laser being always switched on, varying link utilization due to spatial and temporal traffic fluctuations and thermal sensitivity are some of the critical challenges facing photonics interconnects. In this paper, we propose photonic interconnects for heterogeneous multicores using a checkerboard pattern that clusters CPU-GPU cores together and implements bandwidth reconfiguration using local router information without global coordination. To reduce the static power, we also propose a dynamic laser scaling technique that predicts the power level for the next epoch using the buffer occupancy of previous epoch. To further improve power-performance trade-offs, we also propose a regression-based machine learning technique for scaling the power of the photonic link. Our simulation results demonstrate a 34% performance improvement over a baseline electrical CMESH while consuming 25% less energy per bit when dynamically reallocating bandwidth. When dynamically scaling laser power, our buffer-based reactive and ML-based proactive prediction techniques show 40 − 65% in power savings with 0 − 14% in throughput loss depending on the reservation window size."
pub.1094048981,Highly Scalable Parallel Collaborative Filtering Algorithm,"Collaborative filtering (CF) based recommender systems have gained wide popularity in Internet companies like Amazon, Netflix, Google News, and others. These systems make automatic predictions about the interests of a user by inferring from information about like-minded users. Real-Time CF on highly sparse massive datasets, while achieving a high prediction accuracy, is a computationally challenging problem. In this paper, we present the design of a soft Real-Time (around 1 min.) parallel CF algorithm based on the Concept Decomposition technique [1]. Our parallel algorithm has been optimized for multicore/many-core architectures while maintaining the prediction accuracy of 0.84 RMSE. Using the Netflix dataset, we demonstrate the performance and scalability of our algorithm (in both batch mode and online mode) on a 32-core Power6 based SMP system. Our parallel algorithm delivered training time of 64son the full Netflix dataset and prediction time of 4.5s on 1.4M ratings ($3.2\mu s$ per rating prediction). This is $12.6\times$ better than the best known sequential training time and around $33\times better$ than the best known sequential prediction time [2] along with high accuracy (0.84 RMSE). To the best of our knowledge, this is also the best known parallel performance at such high accuracy."
pub.1093774724,Performance Behavior Prediction Scheme For Shared-Memory Parallel Applications,"A current challenge in computing centers with different clusters to run applications is which multicore systems must we choose to run a given shared-memory parallel application. Our proposal is to generate a node performance profile database (NPPDB), composed by performance profiles given by distinct microbenchmark-target node combination. Then, applications are executed on a base node to identify different execution phases and their weights, and to collect performance and functional data for each phase. For similarity, the information to compare behavior is always obtained on the same node. When we want to project performance behavior, we look for similarity using the information from the performance profiles database with the phase characterization, in order to select the appropriate node for running the application. 1Supported by the MEC-MICINN Spain under contract TIN2007–64974. Supported by the MEC-MICINN Spain under contract TIN2007–64974."
pub.1118427649,GHOST: Building blocks for high performance sparse linear algebra on heterogeneous systems,"While many of the architectural details of future exascale-class high
performance computer systems are still a matter of intense research, there
appears to be a general consensus that they will be strongly heterogeneous,
featuring ""standard"" as well as ""accelerated"" resources. Today, such resources
are available as multicore processors, graphics processing units (GPUs), and
other accelerators such as the Intel Xeon Phi. Any software infrastructure that
claims usefulness for such environments must be able to meet their inherent
challenges: massive multi-level parallelism, topology, asynchronicity, and
abstraction. The ""General, Hybrid, and Optimized Sparse Toolkit"" (GHOST) is a
collection of building blocks that targets algorithms dealing with sparse
matrix representations on current and future large-scale systems. It implements
the ""MPI+X"" paradigm, has a pure C interface, and provides hybrid-parallel
numerical kernels, intelligent resource management, and truly heterogeneous
parallelism for multicore CPUs, Nvidia GPUs, and the Intel Xeon Phi. We
describe the details of its design with respect to the challenges posed by
modern heterogeneous supercomputers and recent algorithmic developments.
Implementation details which are indispensable for achieving high efficiency
are pointed out and their necessity is justified by performance measurements or
predictions based on performance models. The library code and several
applications are available as open source. We also provide instructions on how
to make use of GHOST in existing software packages, together with a case study
which demonstrates the applicability and performance of GHOST as a component
within a larger software stack."
pub.1146036343,Performance predictors for graphics processing units applied to dark‐silicon‐aware design space exploration,"Abstract  The limitations on the scalability of computer systems imposed by the dark‐silicon effects are so severe that they support the extensive use of heterogeneity such as the GP‐GPU for general purpose processing. Performance simulators of GP‐GPU heterogeneous systems aim to provide performance accuracy at the cost of execution time. In this work, we handle time‐consuming simulations of design space exploration systems based on GPUs. We have developed performance predictors based on machine learning (ML) algorithms and evaluated them in accuracy and throughput (number of predictions per second). We measure model accuracy through the mean absolute percentage error (MAPE) and the model efficiency through a throughput metric (millions of predictions per second). Our experiments revealed that decision trees predictors are the most promising regarding accuracy and efficiency. We applied the best predictors into the MultiExplorer, a dark silicon ‐aware design space exploration tool that allows designers to explore the architecture and microarchitecture of multicore/manycore system design. "
pub.1042656073,Introduction,"The Parallel and Distributed Programming Topic, Topic 9, is concerned with the development of parallel or distributed applications. Developing such applications is a difficult task and it requires advanced algorithms, realistic modeling, efficient design tools, high performance languages and libraries, and experimental evaluation. This topic provides a forum for presentation of new results and practical experience in this domain. It emphasizes research that facilitates the design and development of correct, high-performance, portable, and scalable parallel program. Related to these central needs, this Topic also includes contributions that assess methods for reusability, performance prediction, large-scale deployment, self- adaptivity, and fault-tolerance."
pub.1117677182,A Dynamic Programming Technique for Energy-Efficient Multicore Systems,"With a focus on static (compile-time) methods for V/F level assignments, we propose an efficient Dynamic programming (DP) technique using the Viterbi algorithm, which uses the Energy-Delay Product (EDP) as objective function to predict the best V/F levels. By using the profiled information of applications, this technique minimizes energy consumption and execution time. We evaluate and compare the performance of the proposed algorithm against three heuristic methods—a greedy version of our algorithm, a feedback controller method, and a simple heuristic that uses historical performance to make predictions for adjusting the V/F levels. Experimental results show that our algorithm outperforms the heuristics under the study by an average of 12 to 24% using the EDP performance criteria."
pub.1095620575,Power-aware MPI Task Aggregation Prediction for High-End Computing Systems,"Emerging large-scale systems have many nodes with several processors per node and multiple cores per processor. These systems require effective task distribution between cores, processors and nodes to achieve high levels of performance and utilization. Current scheduling strategies distribute tasks between cores according to a count of available cores, but ignore the execution time and energy implications of task aggregation (i.e., grouping multiple tasks within the same node or the same multicore processor). Task aggregation can save significant energy while sustaining or even improving performance. However, choosing an effective task aggregation becomes more difficult as the core count and the options available for task placement increase. We present a framework to predict the performance effect of task aggregation in both computation and communication phases and its impact in terms of execution time and energy of MPI programs. Our results for the NPB 3.2 MPI benchmark suite show that our framework provides accurate predictions leading to substantial energy saving through aggregation (64.87% on average and up to 70.03%) with tolerable performance loss (under 5%)."
pub.1149657887,From Single CPU to Multicore Systems,"Contemporary microprocessors are one of the more complex digital systems designed by humans. Today's microprocessor chips contain several different building blocks including processor, memory and interface logic. Thanks to Gordon Moore [4] prediction, that the transistor density will double every 18 months, and Dennard scaling [8], who pointed out that transistor power density remains constant as physical dimensions of MOSFET's become smaller, the microprocessor clock speed permanently increased up to 2003 year. However, augmentation of the CPU clock frequency cannot exceed some limits due to significant increment in power dissipation and consequently the amount of heating. Having this fact in mind, the microprocessor manufacturers have designed new types of microprocessors, referred to as multi- and many-core processors. Multicore technology has become ubiquitous today, with most personal and embedded computers. The hardware structure of computers that are based on multi-/many-core concept requires involving of parallel programming. By using this approach, the following two benefits are evident, the first one deals with achievement of scalable computer performance while the second with the possibility of large-scale data processing. But now, the skill to create efficient parallel programs requires from programmers a solid understanding of new computational principles, algorithms, and programming software tools. This article may be of help to a reader with aim to understand both the architecture of modern microprocessor systems and their programming starting from single to multicore."
pub.1131006247,Optimised HEVC encoder intra‐only configuration,"High‐efficiency video coding (HEVC) is the latest video coding standard aimed to reduce the bitrate by half for the same video quality compared to H.264/AVC. This encoding performance makes HEVC more suitable for high‐definition video applications. However, this performance is coupled with a high‐computational complexity, which makes it hard to achieve real‐time video encoding with a classic embedded processor. Multicore technology of programmable processors could be a very promising solution to overcome this computational complexity. Moreover, software optimisations by proposing fast algorithms for the most complex functions could also be an efficient solution to speed up the encoding process. In this context, this study presents a fast mode decision algorithm for the intra prediction module. This algorithm aims to reduce the number of intra prediction modes to be tested instead of performing a full intra mode search. Experimental results for all‐Intra configuration show that the proposed fast intra mode decision allows saving up to 46.79% of the intra prediction time in average. Encoding performance in terms of video quality and bitrate is not significantly affected."
pub.1094697616,Parameter estimation of bioprocesses via parallel particle swarm optimization,"Modeling of biotechnological systems is an important research area. The most challenging approach is to build non-linear state space models for these systems. In this work the parameters of a bacterial growth bioprocess are estimated using prediction error method. Prediction error methods are widely used in parameter estimation both for linear and nonlinear models and consist in minimization of the distance between measured and modeled data in a suitable norm. Because these problems are solved using numerical algorithms that are time consuming, in this paper a parallel particle swarm optimization technique is used in order to numerically solve the minimization problem. The algorithm is implemented on a multicore processor and the performances of this approach are presented by numerical simulations."
pub.1103965720,Comparative Study and Prediction Modeling of Photonic Ring Network on Chip Architectures,"Multicore systems are becoming state-of-the-art and therefore need fast and energy efficient interconnects to take full advantage of the computational capabilities. Integration of silicon photonics with traditional electrical interconnect in Network on Chip (NoC) proposes a promising solution for overcoming the scalability issues of electrical interconnect. In this paper, we implement the simulation model for two Optical NoC architectures and compare their performance. We also derive and evaluate a prediction modeling technique for the design space exploration of ONoCs. Our proposed model accurately predicts packet latency, static and dynamic energy consumption of the network. This work specifically addresses the challenge of accurately estimating performance metrics without having to incur high costs of exhaustive simulations. Our case study shows that by using only 10% of the entire design space, our proposed technique builds a prediction model that achieved average error rates as low as 5.44%, 2.67% and 3.24% for network packet latency, static and dynamic energy consumption respectively in six different benchmarks from Splash-2 benchmark suite."
pub.1015978520,Improving performance of simple cores by exploiting loop-level parallelism through value prediction and reconfiguration,"There is a growing trend towards designing simpler CPU cores that have considerable area, complexity, and power advantages. These cores are then leveraged in large-scale multicore processors or in SoCs for hand-held devices. The most significant limitation of such simple CPU cores is their lower performance. In this paper, we propose a technique to improve the performance of simple cores with minimal increase in complexity and area. In particular, we integrate a Reconfigurable Hardware Unit (RHU) that exploits loop-level parallelism to increase the core's overall performance. The RHU is reconfigured to execute instructions with highly predictable operand values from the future iterations of loops. Our experiments show that the proposed architecture improves the performance by an average of about 51% across a wide range of applications, while incurring a area overhead of only about 5.6%."
pub.1143535152,Application Workload Characterization using BAT_LSTM Learning algorithm for Asymmetric Architectures,"Nowadays, asymmetric multicore architectures become everywhere due to its energy efficiency, QoS, and high performance. Though workload characterization on these architectures become a challenging task due to its heterogeneous pipeline structure and execution process that affects the overall performance of the system. To resolve this issue, BAT_LSTM deep learning predictor has been designed and developed to predict appropriate resource for each workload at runtime. Deep learning algorithms are adopted in several applications such as computer vision, smart vehicles, and medical environment in order to classify and predict the unknown. In this work, BAT_LSTM neural network predictor has been designed and compared with random forest algorithms, decision tree, naive bayes and support vector machine for workload characterization. Cost functions of these algorithms are designed and developed in order to detect the optimal processor for each workload execution at runtime. Core mark workloads are initially executed on quad core multicore hardware to analyze the workload characteristics in terms of memory consumption, I/O, CPU usage, instructions type, cache miss ratios and so on. These characteristics are feed forwarded into machine a learning algorithm that identifies the best processor. Performance of proposed algorithms is evaluated using testing workloads in terms of processor prediction accuracy, execution time metrics. Average of 10% in energy consumption reduction and 96.8% in accuracy is achieved through proposed predictors."
pub.1027126827,Trace based phase prediction for tightly-coupled heterogeneous cores,"Heterogeneous multicore systems are composed of multiple cores with varying energy and performance characteristics. A controller dynamically detects phase changes in applications and migrates execution onto the most efficient core that meets the performance requirements. In this paper, we show that existing techniques that react to performance changes break down at fine-grain intervals, as performance variations between consecutive intervals are high. We propose a predictive trace-based switching controller that predicts an upcoming phase change in a program and preemptively migrates execution onto a more suitable core. This prediction is based on a phase's individual history and the current program context. Our implementation detects repeatable code sequences to build history, uses these histories to predict an phase change, and preemptively migrates execution to the most appropriate core. We compare our method to phase prediction schemes that track the frequency of code blocks touched during execution as well as traditional reactive controllers, and demonstrate significant increases in prediction accuracy at fine-granularities. For a big-little heterogeneous system that is comprised of a high performing out-of-order core (Big) and an energy-efficient, in-order core (Little), at granularities of 300 instructions, the trace based predictor can spend 28% of execution time on the Little, while targeting a maximum performance degradation of 5%. This translates to an increased energy savings of 15% on average over running only on Big, representing a 60% increase over existing techniques."
pub.1124081993,Evaluation of a Practical Markov model-based Methodology for Energy Efficiency in Multicore Systems,"Greedy heuristic methodologies perform the dynamic voltage and frequency scaling (DVFS) on multicore systems based on the localized system performance data. The characteristics of applications such as their floating-point operations and memory loads can vary largely during the runtime. As such, the heuristic-based prediction of Voltage and Frequency (V/F) is suboptimal and leads to performance degradation in applications with widely varying computational demands. This work develops a new approach, Markov-based DVFS (MDVFS), which formulates a Hidden Markov Model (HMM) for predicting core utilizations (states) and V/F levels based on the observable execution times of cores across application time intervals. MDVFS uses the well-known K-means algorithm for defining states and observation levels of HMM. For solving the HMM problem, MDVFS uses the Viterbi algorithm to find a sequence of V/F levels that most likely generate the execution time sequence. This paper uses Energy-Delay2 Product (ED2 P) for measuring system energy efficiency and compares the performance of MDVFS to 2 predictive algorithms, history and feedback controller, on multiple applications. Experimental results show that for 5 out of 6 applications MDVFS, on the average, obtains up to 34% faster execution runtime and improves ED2 P by 24% compared to the history and the feedback controller."
pub.1049606996,Integrating profile-driven parallelism detection and machine-learning-based mapping,"Compiler-based auto-parallelization is a much-studied area but has yet to find widespread application. This is largely due to the poor identification and exploitation of application parallelism, resulting in disappointing performance far below that which a skilled expert programmer could achieve. We have identified two weaknesses in traditional parallelizing compilers and propose a novel, integrated approach resulting in significant performance improvements of the generated parallel code. Using profile-driven parallelism detection, we overcome the limitations of static analysis, enabling the identification of more application parallelism, and only rely on the user for final approval. We then replace the traditional target-specific and inflexible mapping heuristics with a machine-learning-based prediction mechanism, resulting in better mapping decisions while automating adaptation to different target architectures. We have evaluated our parallelization strategy on the NAS and SPEC CPU2000 benchmarks and two different multicore platforms (dual quad-core Intel Xeon SMP and dual-socket QS20 Cell blade). We demonstrate that our approach not only yields significant improvements when compared with state-of-the-art parallelizing compilers but also comes close to and sometimes exceeds the performance of manually parallelized codes. On average, our methodology achieves 96% of the performance of the hand-tuned OpenMP NAS and SPEC parallel benchmarks on the Intel Xeon platform and gains a significant speedup for the IBM Cell platform, demonstrating the potential of profile-guided and machine-learning- based parallelization for complex multicore platforms."
pub.1010108189,A Compiler Framework for Supporting Speculative Multicore Processors,"As multi-core technology is currently being deployed in computer industry primarily to limit power consumption and improve throughput, continued performance improvement of a single application on such systems remains an important and challenging task. Because of the shortened on-chip communication latency between cores, using thread-level parallelism (TLP) to improve the number of instructions executed per clock cycle, i.e., to improve ILP performance, has shown to be effective for many general-purpose applications. However, because of the program characteristics of these applications, effective speculative schemes at both thread- and instruction-level are crucial."
pub.1119406249,Energy of Computing on Multicore CPUs: Predictive Models and Energy Conservation Law,"Energy is now a first-class design constraint along with performance in all
computing settings. Energy predictive modelling based on performance monitoring
counts (PMCs) is the leading method used for prediction of energy consumption
during an application execution. We use a model-theoretic approach to formulate
the assumed properties of existing models in a mathematical form. We extend the
formalism by adding properties, heretofore unconsidered, that account for a
limited form of energy conservation law. The extended formalism defines our
theory of energy of computing. By applying the basic practical implications of
the theory, we improve the prediction accuracy of state-of-the-art energy
models from 31% to 18%. We also demonstrate that use of state-of-the-art
measurement tools for energy optimisation may lead to significant losses of
energy (ranging from 56% to 65% for applications used in experiments) since
they do not take into account the energy conservation properties."
pub.1127645643,Artificial Neural Network based Task Scheduling for Heterogeneous Systems,"Heterogeneous systems containing diverse computational resources have opened a new avenue in the field of multicore processors. Increasing versatility in applications and their performance requirements has shifted the focus to heterogeneous architecture. To leverage the additional computational capacity provided by these systems, efficient scheduling algorithms are required. The recent development in the field of machine learning provides the necessary tools in the form of prediction models and can be employed to achieve desired results. One such machine learning technique is the Artificial Neural Network (ANN) which has been used in the proposed work to achieve a high throughput heterogeneous system. An ANN-based scheduler that maximizes the performance of the cores by predicting application behavior in the next scheduling interval using core and workload statistics has been presented. Two distinct prediction models are trained for each core type thereby improving the accuracy of the model. The proposed method yields average performance improvement in the range of 6.5%-9.7% from conventional Fairness-aware scheduler and 2.5%-1.5% from a novel Dynamic ANN scheduler."
pub.1119375822,Efficient Multicore Collaborative Filtering,"This paper describes the solution method taken by LeBuSiShu team for track1
in ACM KDD CUP 2011 contest (resulting in the 5th place). We identified two
main challenges: the unique item taxonomy characteristics as well as the large
data set size.To handle the item taxonomy, we present a novel method called
Matrix Factorization Item Taxonomy Regularization (MFITR). MFITR obtained the
2nd best prediction result out of more then ten implemented algorithms. For
rapidly computing multiple solutions of various algorithms, we have implemented
an open source parallel collaborative filtering library on top of the GraphLab
machine learning framework. We report some preliminary performance results
obtained using the BlackLight supercomputer."
pub.1050976960,Introduction,"Developing parallel or distributed applications is a hard task and it requires advanced algorithms, realistic modeling, efficient design tools, high-level programming abstractions, high-performance implementations, and experimental evaluation. Ongoing research in this field emphasizes the design and development of correct, high-performance, portable, and scalable parallel programs. Related to these central needs, important work addresses methods for reusability, performance prediction, large-scale deployment, self-adaptivity, and fault-tolerance. Given the rich history in this field, practical applicability of proposed methods, models, algorithms, or techniques is a key requirement for timely research. This topic is focusing on parallel and distributed programming in general, except for work specifically targeting multicore and manycore architectures, which has matured to becoming a Euro-Par topic of its own."
pub.1175931130,A Proactive Droop Mitigation Technique Using Dual-Proportional-Derivative Controller Based on Current and Voltage Prediction,"Droop mitigation technology is one of the key requirements of multicore processors to ensure supply voltage (V  $_{\text{DD}}$ ) stability under drastic workload variations. A proactive droop mitigation technique using dual-proportional-derivative (PD) controller with both current and voltage prediction is proposed. First, we propose a predictive digital power meter (PDPM) to predict the system current through selecting 157 signals automatically. Then, a voltage predictor based on power delivery network (VPDN) is utilized to predict the real-time voltage using historical voltage and current. Finally, the dual-PD controller takes a real-time response by considering both predicted current and voltage to ensure a prudent clock-gating strategy, which mitigates the droop while minimizing the performance loss. Applied to an eight-core processor that is fabricated in a 28-nm CMOS process, the proposed technique achieves the droop reduction effect of 45.2 mV (32%) while incurring the smallest performance loss of 0.6%."
pub.1019019531,Accurate prediction of the behavior of multithreaded applications in shared caches,"Multicores are the norm nowadays and in many of them there are cores that share one or several levels of cache. The theoretical performance gain expected when several cores cooperate in the parallel execution of an application can be reduced in some cases by a cache access bottleneck, as the data accessed by them can interfere in the shared cache levels. In other cases the performance gain can be increased due to a greater reuse of the data loaded in the cache. This paper presents an analytical model that can predict the behavior of shared caches when executing applications parallelized at loop level. To the best of our knowledge, this is the first analytical model that tackles the behavior of multithreaded applications on realistic shared caches without requiring profiling. The experimental results show that the model predictions are precise and very fast and that the model can help a compiler or programmer choose the best parallelization strategy."
pub.1135235920,Clean-prefetcher: look-ahead prefetching without cache pollution,"In this letter, we propose a novel prefetching technique that is free from cache pollution and thus achieves high performance for multicore processors. Unlike the conventional prefetchers that cause incorrect predictions, the proposed prefetcher reads instructions in advance only in determined paths and charges dynamic random access memory (DRAM) cells storing instructions in undetermined paths via refreshing DRAM cells. The DRAM cells highly charged will be quickly accessed. Since caches served by our prefetcher always store useful instructions, as a result, they are free from cache pollution that results in lower cache hit rate. In the case that SPEC CPU2006 benchmarks run on an 8-core processor, the proposed prefetcher consumes more 3.2% DRAM power, but achieves 12% higher performance on average."
pub.1144062142,A Comprehensive Analytical Survey on Unsupervised and Semi-Supervised Graph Representation Learning Methods,"Graph representation learning is a fast-growing field where one of the main
objectives is to generate meaningful representations of graphs in
lower-dimensional spaces. The learned embeddings have been successfully applied
to perform various prediction tasks, such as link prediction, node
classification, clustering, and visualization. The collective effort of the
graph learning community has delivered hundreds of methods, but no single
method excels under all evaluation metrics such as prediction accuracy, running
time, scalability, etc. This survey aims to evaluate all major classes of graph
embedding methods by considering algorithmic variations, parameter selections,
scalability, hardware and software platforms, downstream ML tasks, and diverse
datasets. We organized graph embedding techniques using a taxonomy that
includes methods from manual feature engineering, matrix factorization, shallow
neural networks, and deep graph convolutional networks. We evaluated these
classes of algorithms for node classification, link prediction, clustering, and
visualization tasks using widely used benchmark graphs. We designed our
experiments on top of PyTorch Geometric and DGL libraries and run experiments
on different multicore CPU and GPU platforms. We rigorously scrutinize the
performance of embedding methods under various performance metrics and
summarize the results. Thus, this paper may serve as a comparative guide to
help users select methods that are most suitable for their tasks."
pub.1103668236,TCSTM: A task-characteristic-considered steady-state thermal model of multicore processors,"Because the power density and temperatures of multicore processors are increasing to the extent that their performance and reliability are degraded, it is crucial to estimate the powers and temperatures of multicore processors accurately and rapidly at the early design stage. In this paper, to improve the accuracy, a task-characteristic-considered steady-state thermal model (TCSTM) of multicore processors is presented. First, a metric, namely, task characteristic, is explicitly defined to characterize the behavior of a workload. The task characteristic is expressed by a column vector H cycle = [ h memory , h branch , h integer , h float ] ′ , in which each element respectively denotes the number of memory instructions, branch instructions, integer instructions and floating point instructions per cycle. Second, the dynamic power of a core is modeled as a linear function of the task characteristic, running frequency and the square of voltage. The leakage power is approximated as a linear model of the temperature and voltage. The voltage-given and temperature-interval-limited linear regression (VTLR) method is employed to reduce the complexity of the steady-state model. Third, the steady-state temperature of a core is derived as a function of the task characteristic, frequency, voltage and the number of active cores. To the best of our knowledge, this is the first work to introduce the task characteristic into the steady-state thermal model. Finally, not only the relationships between the frequency, the number of active cores and hot-spot temperatures but also the impact of the number of frequency-scaled cores on hot-spot temperatures are investigated experimentally. The experimental results demonstrate that the proposed steady-state model achieves satisfactory accuracy in terms of the estimation of the dynamic and leakage power and the prediction of hot-spot functional units."
pub.1048468143,Parallel and Distributed Programming,"Developing parallel or distributed applications is a hard task and it requires advanced algorithms, realistic modeling, efficient design tools, high-level programming abstractions, high-performance implementations, and experimental evaluation. Ongoing research in this field emphasizes the design and development of correct, high-performance, portable, and scalable parallel programs. Related to these central needs, important work addresses methods for reusability, performance prediction, large-scale deployment, self-adaptivity, and fault-tolerance. Given the rich history in this field, practical applicability of proposed methods, models, algorithms, or techniques is a key requirement for timely research. This topic is focusing on parallel and distributed programming in general, except for work specifically targeting multicore architectures, which has matured to becoming a Euro-Par topic of its own."
pub.1035427079,Performance‐steered design of software architectures for embedded multicore systems,"Abstract Many software applications demanding a considerable computing power are moving towards the field of embedded systems (and, in particular, hand‐held devices). A possible way to increase the computing power of this kind of platform, so that both cost and power consumption are kept low, is the employment of multiple CPU cores on the same chipset. Consequently, it is essential to design applications that meet performance requirements leveraging the underlying parallel platform. As embedded applications are usually built using different components (whose source code is often not available) from different companies, the designer can mostly only operate at the architectural level. So far, methodologies for designing software architectures have mainly addressed general‐purpose systems, often relying on hardware platforms with a high degree of parallelism. In this paper, we present our experience in architectural design of parallel embedded applications; as a result, we propose a possible methodology for the application design at the architectural level, targeted to embedded systems built upon multicore chipsets with a low degree of parallelism. It makes use of performance predictions, obtained by simulations. Such a methodology can be employed both for retargeting existing sequential applications to parallel processing platforms and for designing complete applications from scratch. We show the application of the proposed methodology to an embedded digital cartographic system. Starting with a software description using UML diagrams, candidate software architectures (utilizing different parallel solutions) are first defined and then evaluated, to end with the selection of the one yielding the highest performance gain. Copyright © 2002 John Wiley & Sons, Ltd."
pub.1112460594,Performance portability study for massively parallel computational fluid dynamics application on scalable heterogeneous architectures,"Patient-specific hemodynamic simulations have the potential to greatly improve both the diagnosis and treatment of a variety of vascular diseases. Portability will enable wider adoption of computational fluid dynamics (CFD) applications in the biomedical research community and targeting to platforms ideally suited to different vascular regions. In this work, we present a case study in performance portability that assesses (1) the ease of porting an MPI application optimized for one specific architecture to new platforms using variants of hybrid MPI+X programming models; (2) performance portability seen when simulating blood flow in three different vascular regions on diverse heterogeneous architectures; (3) model-based performance prediction for future architectures; and (4) performance scaling of the hybrid MPI+X programming on parallel heterogeneous systems. We discuss the lessons learned in porting HARVEY, a massively parallel CFD application, from traditional multicore CPUs to diverse heterogeneous architectures ranging from NVIDIA/AMD GPUs to Intel MICs and Altera FPGAs."
pub.1024101315,GHOST: Building Blocks for High Performance Sparse Linear Algebra on Heterogeneous Systems,"While many of the architectural details of future exascale-class high performance computer systems are still a matter of intense research, there appears to be a general consensus that they will be strongly heterogeneous, featuring “standard” as well as “accelerated” resources. Today, such resources are available as multicore processors, graphics processing units (GPUs), and other accelerators such as the Intel Xeon Phi. Any software infrastructure that claims usefulness for such environments must be able to meet their inherent challenges: massive multi-level parallelism, topology, asynchronicity, and abstraction. The “General, Hybrid, and Optimized Sparse Toolkit” (GHOST) is a collection of building blocks that targets algorithms dealing with sparse matrix representations on current and future large-scale systems. It implements the “MPI+X” paradigm, has a pure C interface, and provides hybrid-parallel numerical kernels, intelligent resource management, and truly heterogeneous parallelism for multicore CPUs, Nvidia GPUs, and the Intel Xeon Phi. We describe the details of its design with respect to the challenges posed by modern heterogeneous supercomputers and recent algorithmic developments. Implementation details which are indispensable for achieving high efficiency are pointed out and their necessity is justified by performance measurements or predictions based on performance models. We also provide instructions on how to make use of GHOST in existing software packages, together with a case study which demonstrates the applicability and performance of GHOST as a component within a larger software stack. The library code and several applications are available as open source."
pub.1031141629,Bubble-Up,"As much of the world's computing continues to move into the cloud, the overprovisioning of computing resources to ensure the performance isolation of latency-sensitive tasks, such as web search, in modern datacenters is a major contributor to low machine utilization. Being unable to accurately predict performance degradation due to contention for shared resources on multicore systems has led to the heavy handed approach of simply disallowing the co-location of high-priority, latency-sensitive tasks with other tasks. Performing this precise prediction has been a challenging and unsolved problem. In this paper, we present Bubble-Up, a characterization methodology that enables the accurate prediction of the performance degradation that results from contention for shared resources in the memory subsystem. By using a bubble to apply a tunable amount of ""pressure"" to the memory subsystem on processors in production datacenters, our methodology can predict the performance interference between co-locate applications with an accuracy within 1% to 2% of the actual performance degradation. Using this methodology to arrive at ""sensible"" co-locations in Google's production datacenters with real-world large-scale applications, we can improve the utilization of a 500-machine cluster by 50% to 90% while guaranteeing a high quality of service of latency-sensitive applications."
pub.1063456289,A Multiple-Model Convection-Permitting Ensemble Examination of the Probabilistic Prediction of Tropical Cyclones: Hurricanes Sandy (2012) and Edouard (2014),"Abstract
                  This study examines a multimodel comparison of regional-scale convection-permitting ensembles including both physics and initial condition uncertainties for the probabilistic prediction of Hurricanes Sandy (2012) and Edouard (2014). The model cores examined include COAMPS-TC, HWRF, and WRF-ARW. Two stochastic physics schemes were also applied using the WRF-ARW model. Each ensemble was initialized with the same initial condition uncertainties represented by the analysis perturbations from a WRF-ARW-based real-time cycling ensemble Kalman filter. It is found that single-core ensembles were capable of producing similar ensemble statistics for track and intensity for the first 36–48 h of model integration, with biases in the ensemble mean evident at longer forecast lead times along with increased variability in spread. The ensemble spread of a multicore ensemble with members sampled from single-core ensembles was generally as large or larger than any constituent model, especially at longer lead times. Systematically varying the physic parameterizations in the WRF-ARW ensemble can alter both the forecast ensemble mean and spread to resemble the ensemble performance using a different forecast model. Compared to the control WRF-ARW experiment, the application of the stochastic kinetic energy backscattering scheme had minimal impact on the ensemble spread of track and intensity for both cases, while the use of stochastic perturbed physics tendencies increased the ensemble spread in track for Sandy and in intensity for both cases. This case study suggests that it is important to include model physics uncertainties for probabilistic TC prediction. A single-core multiphysics ensemble can capture the ensemble mean and spread forecasted by a multicore ensemble for the presented case studies."
pub.1125731957,Performance Optimizations for Parallel Modeling of Solidification with Dynamic Intensity of Computation,"Abstract
In our previous works, a parallel application dedicated to the numerical modeling of alloy solidification was developed and tested using various programming environments on hybrid shared-memory platforms with multicore CPUs and manycore Intel Xeon Phi accelerators. While this solution allows obtaining a reasonable good performance in the case of the static intensity of computations, the performance results achieved for the dynamic intensity of computations indicates pretty large room for further optimizations.In this work, we focus on improving the overall performance of the application with the dynamic computational intensity. For this aim, we propose to modify the application code significantly using the loop fusion technique. The proposed method permits us to execute all kernels in a single nested loop, as well as reduce the number of conditional operators performed within a single time step. As a result, the proposed optimizations allows increasing the application performance for all tested configurations of computing resources. The highest performance gain is achieved for a single Intel Xeon SP CPU, where the new code yields the speedup of up to 1.78 times against the original version.The developed method is vital for further optimizations of the application performance. It allows introducing an algorithm for the dynamic workload prediction and load balancing in successive time steps of simulation. In this work, we propose the workload prediction algorithm with 1D computational map."
pub.1125054803,Crosstalk Impact on the Performance of Wideband Multicore-Fiber Transmission Systems,"This work presents an evaluation of the crosstalk impact on the signal-to-noise ratio (SNR) of long-distance C and L band wavelength-division multiplexing (WDM) systems using homogeneous multicore fibers with weakly coupled cores. It is experimentally shown that the crosstalk-induced SNR penalty is independent of the transmission distance on sufficiently long uniform links. This results from the approximately linear scaling of the noise contributions from amplified spontaneous emission, fiber nonlinearity, and crosstalk with the transmission distance. The crosstalk-induced SNR penalty on C and L band WDM long distance links is experimentally evaluated, showing significant degradation of signals located towards the long edge of the L-band. Up to 3.8dB penalty was observed on a 3866km link, in agreement with theoretical predictions. We perform a theoretical analysis of the wavelength allocation of densely packed channels for long distance WDM systems with and without the presence of crosstalk. It is shown that these systems favor the use of relatively short transmission wavelength ranges to minimize the crosstalk impact at long wavelengths. This contrasts with systems without crosstalk, which favor the low loss wavelength region of the transmission fiber."
pub.1135914662,An Efficient Method for Accelerating Training of Short-Term Traffic Prediction Models in Large-Scale Traffic Networks,"The increasing availability of large volumes of traffic data has led to the development of several short-term traffic prediction models. Training these models is a computationally intensive process due to the volume of available traffic data. Therefore, having effective methods for accelerating this process is considered necessary. In this paper, we propose an efficient method for accelerating the training process of multiple short-term traffic prediction models in large-scale traffic networks. In particular, the traffic data is organized into separate files so that the training process for one model is independent of the others. These files are distributed in the cores of a shared-memory multicore processor so as to train multiple models simultaneously. Appropriate measures have been taken to limit the memory footprint of the proposed method, as well as to enhance its load balancing capabilities. The proposed method was applied to five short-term traffic prediction models, and evaluated using large-scale real-world traffic data. Preliminary experimental results indicate that the proposed method exhibits nearly linear speedup for the training process of all models, while maintaining their prediction performance."
pub.1095168771,An Investigation into the Feasibility and Benefits of GPU/Multicore Acceleration of the Weather Research and Forecasting Model,"There is a growing need for ever more accurate climate and weather simulations to be delivered in shorter timescales. Hardware Acceleration using GPUs or FPGAs could potentially result in much reduced run times or higher accuracy simulations. We studied the Weather Research and Forecasting Model in order to assess if GPU acceleration of this type of Numerical Weather Prediction code is both feasible and worthwhile. We studied the performance of the original code and created a simple performance model for comparing multicore CPUs and GPUs. Based on the WRF profiling results, we focused on the acceleration of the scalar advection module. We show that our data-parallel kernel version of the scalar advection module runs up to 7x faster on the GPU compared to the original code on the CPU. However, as the data transfer cost between GPU and CPU is very high (as shown by our analysis), there is only a small speed-up (2x) for the fully integrated code. We also developed an extensible system for integrating OpenCL code into large Fortran code bases such as WRF. In conclusion, we have shown that GPU acceleration of WRF is both feasible and worthwhile. Our findings are generally applicable to multi-physics fluid dynamics code and not limited to NWP models."
pub.1153663014,Fast-Accurate Full-Chip Dynamic Thermal Simulation With Fine Resolution Enabled by a Learning Method,"The need for full-chip dynamic thermal simulation for effective runtime thermal management of multicore processors has been growing in recent years due to the rising demand for high-performance computing. In addition to simulation efficiency and accuracy, a high resolution is desirable in order to accurately predict crucial hot spots in the chip. This work investigates a simulation technique derived from proper orthogonal decomposition (POD) for full-chip dynamic thermal simulation of a multicore processor. The POD projects a heat transfer problem onto a mathematical space constituted by a finite set of basis functions (or POD modes) that are generated (or trained) by thermal solution data collected from direct numerical simulation (DNS). Accuracy and efficiency of the POD simulation technique influenced by the quality of thermal data are examined thoroughly, especially in the areas with high thermal gradients. The results show that if the POD modes are trained by good-quality data, the POD simulation offers an accurate prediction of the dynamic thermal distribution in the multicore processor with an extremely small degree of freedom (DoF). A reduction in computational time over four orders of magnitude, compared to the DNS, can be achieved for full-chip dynamic thermal simulation with a resolution as fine as the DNS. The study has also demonstrated that the POD approach can be used to rigorously verify the accuracy of solutions offered by DNS tools. A practical approach is proposed to further enhance the accuracy and efficiency of the proposed full-chip thermal simulation technique."
pub.1094920716,Practical Data Value Speculation for Future High-End Processors,"Dedicating more silicon area to single thread performance will necessarily be considered as worthwhile in future - potentially heterogeneous - multicores. In particular, Value prediction (VP) was proposed in the mid 90's to enhance the performance of high-end uniprocessors by breaking true data dependencies. In this paper, we reconsider the concept of Value Prediction in the contemporary context and show its potential as a direction to improve current single thread performance. First, building on top of research carried out during the previous decade on confidence estimation, we show that every value predictor is amenable to very high prediction accuracy using very simple hardware. This clears the path to an implementation of VP without a complex selective reissue mechanism to absorb mispredictions. Prediction is performed in the in-order pipeline frond-end and validation is performed in the in-order pipeline back-end, while the out-of-order engine is only marginally modified. Second, when predicting back-to-back occurrences of the same instruction, previous context-based value predictors relying on local value history exhibit a complex critical loop that should ideally be implemented in a single cycle. To bypass this requirement, we introduce a new value predictor VTAGE harnessing the global branch history. VTAGE can seamlessly predict back-to-back occurrences, allowing predictions to span over several cycles. It achieves higher performance than previously proposed context-based pre-dictors. Specifically, using SPEC'00 and SPEC'06 benchmarks, our simulations show that combining VTAGE and a stride-based predictor yields up to 65% speedup on a fairly aggressive pipeline without support for selective reissue."
pub.1093189753,HVD-TLS: A Novel Framework of Thread Level Speculation,"The advent of multicore platform brings a great opportunity to speedup sequential program via thread level parallelism. In this paper we have proposed a novel Thread Level Speculation (TLS) framework HVD- TLS. The framework uses heuristic value prediction to improve the speculation accuracy, reduces the rerolling overhead via value checking based correctness checking mechanism. The framework can partition and schedule task on different processor cores dynamically based on runtime information. We have implemented the HVD-TLS in ANSI ${\bf c}$ and tested its performance on a 4-core platform. The experiments show that the speedup is 2.2 on the average with a speculation depth equaling to 3."
pub.1118568403,ICE: A General and Validated Energy Complexity Model for Multithreaded Algorithms,"Like time complexity models that have significantly contributed to the
analysis and development of fast algorithms, energy complexity models for
parallel algorithms are desired as crucial means to develop energy efficient
algorithms for ubiquitous multicore platforms. Ideal energy complexity models
should be validated on real multicore platforms and applicable to a wide range
of parallel algorithms. However, existing energy complexity models for parallel
algorithms are either theoretical without model validation or
algorithm-specific without ability to analyze energy complexity for a
wide-range of parallel algorithms.
  This paper presents a new general validated energy complexity model for
parallel (multithreaded) algorithms. The new model abstracts away possible
multicore platforms by their static and dynamic energy of computational
operations and data access, and derives the energy complexity of a given
algorithm from its work, span and I/O complexity. The new model is validated by
different sparse matrix vector multiplication (SpMV) algorithms and dense
matrix multiplication (matmul) algorithms running on high performance computing
(HPC) platforms (e.g., Intel Xeon and Xeon Phi). The new energy complexity
model is able to characterize and compare the energy consumption of SpMV and
matmul kernels according to three aspects: different algorithms, different
input matrix types and different platforms. The prediction of the new model
regarding which algorithm consumes more energy with different inputs on
different platforms, is confirmed by the experimental results. In order to
improve the usability and accuracy of the new model for a wide range of
platforms, the platform parameters of ICE model are provided for eleven
platforms including HPC, accelerator and embedded platforms."
pub.1095684541,Power and Performance Characterization and Modeling of GPU-Accelerated Systems,"Graphics processing units (GPUs) provide an order-of-magnitude improvement on peak performance and performance-per-watt as compared to traditional multicore CPUs. However, GPU-accelerated systems currently lack a generalized method of power and performance prediction, which prevents system designers from an ultimate goal of dynamic power and performance optimization. This is due to the fact that their power and performance characteristics are not well captured across architectures, and as a result, existing power and performance modeling approaches are only available for a limited range of particular GPUs. In this paper, we present power and performance characterization and modeling of GPU-accelerated systems across multiple generations of architectures. Characterization and modeling both play a vital role in optimization and prediction of GPU-accelerated systems. We quantify the impact of voltage and frequency scaling on each architecture with a particularly intriguing result that a cutting-edge Kepler-based GPU achieves energy saving of 75% by lowering GPU clocks in the best scenario, while Fermi- and Tesla-based GPUs achieve no greater than 40 % and 13 %, respectively. Considering these characteristics, we provide statistical power and performance modeling of GPU-accelerated systems simplified enough to be applicable for multiple generations of architectures. One of our findings is that even simplified statistical models are able to predict power and performance of cutting-edge GPUs within errors of 20% to 30% for any set of voltage and frequency pair."
pub.1005405883,"Euro-Par 2014 Parallel Processing, 20th International Conference, Porto, Portugal, August 25-29, 2014. Proceedings","This book constitutes the refereed proceedings of the 20th International Conference on Parallel and Distributed Computing, Euro-Par 2014, held in Porto, Portugal, in August 2014. The 68 revised full papers presented were carefully reviewed and selected from 267 submissions. The papers are organized in 15 topical sections: support tools environments; performance prediction and evaluation; scheduling and load balancing; high-performance architectures and compilers; parallel and distributed data management; grid, cluster and cloud computing; green high performance computing; distributed systems and algorithms; parallel and distributed programming; parallel numerical algorithms; multicore and manycore programming; theory and algorithms for parallel computation; high performance networks and communication; high performance and scientific applications; and GPU and accelerator computing."
pub.1001287315,"Euro-Par 2013 Parallel Processing, 19th International Conference, Aachen, Germany, August 26-30, 2013. Proceedings","This book constitutes the refereed proceedings of the 19th International Conference on Parallel and Distributed Computing, Euro-Par 2013, held in Aachen, Germany, in August 2013. The 70 revised full papers presented were carefully reviewed and selected from 261 submissions. The papers are organized in 16 topical sections: support tools and environments; performance prediction and evaluation; scheduling and load balancing; high-performance architectures and compilers; parallel and distributed data management; grid, cluster and cloud computing; peer-to-peer computing; distributed systems and algorithms; parallel and distributed programming; parallel numerical algorithms; multicore and manycore programming; theory and algorithms for parallel computation; high performance networks and communication; high performance and scientific applications; GPU and accelerator computing; and extreme-scale computing."
pub.1094630059,A Robust Methodology for Performance Analysis on Hybrid Embedded Multicore Architectures,"Today's vehicles increasingly embed software in-telligence in order to be safer for the driver, and to achieve autonomous driving in a close future. To answer the computational needs of these algorithms, system-on-chip (SoC) suppliers propose heterogeneous architectures. With such complex SoCs, embedding applications in vehicle becomes more and more com-plex for car manufacturers. Indeed, it is not trivial to find the best suited SoC for a given application, and to define load balancing strategies when working with heterogeneous architectures. These difficulties can be overcome by using performance prediction, based on computing architectures models. To build these models, we provide a set of test vectors which automatically extract key characteristics of tested architectures. Our methodology is able to perform a complete computing architecture model, by using 3 different levels of tests, each one characterizing a specific situation representative of real applications. We aim to obtain performance prediction for different applications, for any embedded SoCs based on models performed with this methodology. In this paper, we describe our characterization methodology, and show results obtained with embedded SoCs used for automotive applications."
pub.1043252466,"Euro-Par 2012 Parallel Processing, 18th International Conference, Euro-Par 2012, Rhodes Island, Greece, August 27-31, 2012. Proceedings","This book constitutes the thoroughly refereed proceedings of the 18th International Conference, Euro-Par 2012, held in Rhodes Islands, Greece, in August 2012. The 75 revised full papers presented were carefully reviewed and selected from 228 submissions. The papers are organized in topical sections on support tools and environments; performance prediction and evaluation; scheduling and load balancing; high-performance architectures and compilers; parallel and distributed data management; grid, cluster and cloud computing; peer to peer computing; distributed systems and algorithms; parallel and distributed programming; parallel numerical algorithms; multicore and manycore programming; theory and algorithms for parallel computation; high performance network and communication; mobile and ubiquitous computing; high performance and scientific applications; GPU and accelerators computing."
pub.1000251653,"Euro-Par 2011 Parallel Processing, 17th International Conference, Euro-Par 2011, Bordeaux, France, August 29 - September 2, 2011, Proceedings, Part II","The two-volume set LNCS 6852/6853 constitutes the refereed proceedings of the 17th International Euro-Par Conference held in Bordeaux, France, in August/September 2011. The 81 revised full papers presented were carefully reviewed and selected from 271 submissions. The papers are organized in topical sections on support tools and environments; performance prediction and evaluation; scheduling and load-balancing; high-performance architectures and compilers; parallel and distributed data management; grid, cluster and cloud computing; peer to peer computing; distributed systems and algorithms; parallel and distributed programming; parallel numerical algorithms; multicore and manycore programming; theory and algorithms for parallel computation; high performance networks and mobile ubiquitous computing."
pub.1094646796,Enhancing EDP of multicore processors through DVFS,"Dynamic voltage and frequency scaling (DVFS) is a well-known technique to optimize the power dissipation of electronic systems without significantly compromising overall system performance. DVFS exploits the periods of inter-core data exchange (memory-bound operations) to reduce the voltage and frequency (V/F) of the cores in order to reduce the power dissipation during the execution flow of an application running on the CMP. As the lengths of the idle and busy periods of the cores vary depending on the benchmarks, it is crucial for any DVFS technique to maximize the power saving without losing a significant performance. In this work we present two power optimization methodologies that are integrated into a full-system simulator to make online predictions about the voltage and frequency of the cores during the execution time of the benchmarks. We evaluate these methodologies in terms of the V/F predictions vs. the actual utilization of each core periodically. We also compare the overall execution time, energy dissipation, and energy-delay product (EDP) of the power optimization methodologies for various benchmarks."
pub.1100848493,Optimizing Locality in Graph Computations Using Reuse Distance Profiles,"This work tries to answer the question of whether or not we should write code differently when the underlying chip microarchitecture is powered by a multicore processor. We use a set of three graph benchmarks each with three different input problems varying in size and connectivity to characterize the importance of how we partition the problem space among cores and how that partitioning can happen at multiple levels of the cache leading to better performance. We explore a design space represented by different parallelization schemes and different graph partitionings. This provides a large and complex space that we characterize using detailed simulation results to see how much gain we can obtain over a baseline legacy parallelization technique with a partition sized to fit in the L1 cache. We show that the legacy parallelization is not the best alternative in most of the cases and other parallelization techniques perform better. We use a PIN computed reuse distance profile to build an execution time prediction model that rank orders the different combinations of parallelization strategies and partitioning sizes. In some cases the prediction is 100% accurate and in some other cases the prediction projects worse performance than the baseline case. We report the difference between the simulated best performing combination and the PIN predicted ones. The M5 performance simulations show gains of up to 20% relative to the baseline. Our prediction scheme can achieve up to 100% of the best performance gains obtained by M5 and up to 48% on average across all of our benchmarks and input sizes. We have shown a new application for reuse distance profiles-i. e., as a tool for helping program developers and compilers to optimize program performance."
pub.1074233527,A Methodology for Optimizing Multithreaded System Scalability on Multicores,"This chapter shows how scalability can be quantified using the universal scalability law (USL) by applying it to controlled performance measurements of memcached (MCD), Java 2 Platform, Enterprise Edition (J2EE) and WebLogic. When doing scalability analysis of multithreaded applications, it is important to collect the data using controlled measurements. Data collected from controlled performance measurements are only as good as the workload used to run the tests. A well‐designed workload should have the following characteristics: predictability; repeatability; and scalability. There are many well‐known techniques for achieving better scalability: collocation, caching, pooling and parallelism, to name a few. Performance models are essential not only for prediction but also for interpreting scalability measurements. Many performance modeling tools, such as event‐based simulators and analytic solvers, are based on a queueing paradigm that requires measured service times as modeling inputs. The chapter presents some case studies that demonstrate how USL methodology has been successfully applied."
pub.1025800794,"Euro-Par 2011 Parallel Processing, 17th International Conference, Euro-Par 2011, Bordeaux, France, August 29 - September 2, 2011, Proceedings, Part I","The two-volume set LNCS 6852/6853 constitutes the refereed proceedings of the 17th International Euro-Par Conference held in Bordeaux, France, in August/September 2011. The 81 revised full papers presented were carefully reviewed and selected from 271 submissions. The papers are organized in topical sections on support tools and environments; performance prediction and evaluation; scheduling and load-balancing; high-performance architectures and compilers; parallel and distributed data management; grid, cluster and cloud computing; peer to peer computing; distributed systems and algorithms; parallel and distributed programming; parallel numerical algorithms; multicore and manycore programming; theory and algorithms for parallel computation; high performance networks and mobile ubiquitous computing."
pub.1121402457,High-Throughput CNN Inference on Embedded ARM Big.LITTLE Multicore Processors,"Internet of Things edge intelligence requires convolutional neural network (CNN) inference to take place in the edge devices itself. ARM big.LITTLE architecture is at the heart of prevalent commercial edge devices. It comprises of single-ISA heterogeneous cores grouped into multiple homogeneous clusters that enable power and performance tradeoffs. All cores are expected to be simultaneously employed in inference to attain maximal throughput. However, high communication overhead involved in parallelization of computations from convolution kernels across clusters is detrimental to throughput. We present an alternative framework called Pipe-it that employs pipelined design to split convolutional layers across clusters while limiting parallelization of their respective kernels to the assigned cluster. We develop a performance-prediction model that utilizes only the convolutional layer descriptors to predict the execution time of each layer individually on all permitted core configurations (type and count). Pipe-it then exploits the predictions to create a balanced pipeline using an efficient design space exploration algorithm. Pipe-it on average results in a 39% higher throughput than the highest antecedent throughput."
pub.1094024401,Fast and Accurate On-line Prediction of Performance and Power Consumption in Multicore-based Systems,"Although multi-core processors have emerged as a dominant low-power architectural solution in high performance processor design, it is still challenging to take a full advantage of the high power efficiency of multi-core processors. One such challenge occurs when an operating system tries to assign a multi-threaded application to a target multi-core processor in an energy efficient fashion. With an increasing number of cores combined with sophisticated power management schemes, it becomes more difficult to decide the most appropriate runtime configuration for a given application so that the overall energy efficiency is maximized. In this paper, we propose a novel performance and power estimation technique, called PET, for multi-core systems. The PET scheme is based on a compact but accurate performance and power transformation model, which aims to predict the performance and power consumption of a large number of runtime configurations using hardware performance counters collected in a small number of representative runtime configurations. Using a transformation model, PET enables to accurately determine the best runtime configuration of multi-threaded applications at runtime with a small overhead over an existing naive solution. Experimental results on an Intel Q6600 quad-core processor show that PET can accurately predict the performance and power consumption of multi-threaded applications running on 1–4 cores under two different frequency levels with an average prediction error of 2.1%–8.3% and 3.2%–6.5% over the measured data, respectively. We also show that PET is effective in estimating the performance and power consumption of two co-running applications with an average prediction error of less than 5%."
pub.1140522668,Performance Analysis of Big.LITTLE System with Various Branch Prediction Schemes,"With the sprinting innovation in mobile technology, cell-phone processors, nowadays, are designed and deployed to meet the demands for high performance and low-power operation. ARM big.LITTLE architecture for smart phones meets the above requirements, with “big” cores delivering high performance and “little” cores being power efficient. High performance is achieved by making deeper pipelines, which result in more processing time being spent on a branch misprediction. Hence, an accurate branch predictor is required to mitigate branch delay latency in processors for exploiting parallelism. In this paper, we evaluate and compare various branch prediction schemes by incorporating them in ARM big.LITTLE architecture with Linux running on it. The comparison is carried out for performance and power utilization with Rodinia benchmarks for heterogeneous cores. Performance of the simulated system is in terms of execution time, percentage of conditional branch mispredictions, and overall percentage of branch mispredictions that considers the conditional and unconditional branches and instructions per cycles. It is observed that the TAGE-LSC, perceptron predictors perform better among all the simulated predictors achieving an average accuracy of 99.03%, 99.00%, respectively, using the gem5 framework. The local branch predictor has less power dissipation when tested on the integrated platform of multicore power area timing."
pub.1104271466,Regression-Based Prediction for Task-Based Program Performance,"As multicore systems evolve by increasing the number of parallel execution units, parallel programming models have been released to exploit parallelism in the applications. Task-based programming model uses task abstractions to specify parallel tasks and schedules tasks onto processors at runtime. In order to increase the efficiency and get the highest performance, it is required to identify which runtime configuration is needed and how processor cores must be shared among tasks. Exploring design space for all possible scheduling and runtime options, especially for large input data, becomes infeasible and requires statistical modeling. Regression-based modeling determines the effects of multiple factors on a response variable, and makes predictions based on statistical analysis. In this work, we propose a regression-based modeling approach to predict the task-based program performance for different scheduling parameters with variable data size. We execute a set of task-based programs by varying the runtime parameters, and conduct a systematic measurement for influencing factors on execution time. Our approach uses executions with different configurations for a set of input data, and derives different regression models to predict execution time for larger input data. Our results show that regression models provide accurate predictions for validation inputs with mean error rate as low as 6.3%, and 14% on average among four task-based programs."
pub.1051828043,SEAL,"A processor's performance and power consumption are tied; an increased performance demands more power, and vice versa. An optimal tradeoff can only be achieved by an improved prediction of the task execution times, prior to an efficient scheduling. Moreover, since the processor's soft error rate is a function of its operating voltage, it is also linked to the performance-power trade-off. The situation is further complicated for the case of multicore architectures where the tasks are to be mapped on separate cores (processing elements). This paper proposes a joint State-Space model to achieve improved task execution time estimation, leading to better scheduling for optimizing the trade-off, particularly in the context of multicore soft real-time systems. It does not assume any `a priori' knowledge about the task graph or its properties, and is independent of the underlying architecture. It learns the system dynamics over time. The state-space solution is formulated using a recursive implementation of the online Monte Carlo Method. Having obtained the estimates of the execution times, they are compensated for the soft error according to a given soft error rate. At the beginning of each scheduling interval, the low power EDF scheduling decision is carried out to execute the tasks. The proposed method (SEAL) achieves 29% better energy savings compared to state-of-the-art, while the deadline misses are under 7% without the loss of system failure probability. The results obtained clearly show the advantage in terms of energy savings."
pub.1105723127,Predictability and Performance Aware Replacement Policy PVISAM for Unified Shared Caches in Real-time Multicores,"Missing the deadline of an application task can be catastrophic in real-time systems. Therefore, to ensure timely completion of tasks, offline worst-case execution time and schedulability analysis is often performed for such real-time systems. One of the important inputs to this analysis is a safe upper bound of misses in each processor cache memory used by the system. Cache miss prediction techniques have matured significantly for private caches in single-core processors; however, remained as a challenge for unified, shared caches in multicore processors. According to prior studies, a task’s miss upper bound on a shared cache can be predicted using available private cache prediction techniques only if the shared cache maintains core-based independent static partitions. The problem is, such partitions require the use of infeasible “write-update consistency protocol” and wastes valuable cache space by duplicate caching. In this regard, this paper presents a novel cache replacement policy called “predictable variable isolation in shared antipodal memory (PVISAM).” Its replacement decisions generate virtual core-based partitions that support demand-based runtime size adjustment and line sharing to better utilize space. Moreover, these partitions require no consistency protocol. Trace-driven experimental results for Parsec benchmark applications reveal that performance of a unified shared cache memory improves by  $101.68{\boldsymbol \times }$  on average (minimum  $1.09{\boldsymbol \times }$  and maximum  $1138.50{\boldsymbol \times }$ ) when PVISAM is used instead of either the aforementioned write-update protocol-based predictable partitioning or the widely used write-invalidate consistency protocol-based partitioning. PVISAM can improve cache performance by  $0.74{\boldsymbol \times }$  on average (minimum  $0.02{\boldsymbol \times }$  and maximum  $1.12{\boldsymbol \times }$ ) compared to having no partitions at all. Both predictable partitioning and PVISAM improve unified, shared cache predictability by 63.44% (minimum 26.89% and maximum 99.99%) and 19.36% (minimum 1.58% and maximum 72.51%) on average compared to no partitions and write-invalidate protocol-based partitioning, respectively. Experimental results for synthetic traces show that PVISAM remarkably improves cache performance and predictability when compared to its three competitors even in scenarios that stress the cache."
pub.1095577486,MuMMI: Multiple Metrics Modeling Infrastructure,"The MuMMI (Multiple Metrics Modeling Infrastructure) project is an infrastructure that facilitates systematic measurement, modeling, and prediction of performance, power consumption and performance-power tradeoffs for parallel systems. In this paper, we present the MuMMI framework, which consists of an Instrumentor, Databases and Analyzer. The MuMMI instrumentor provides for automatic performance and power data collection and storage with low overhead. The MuMMI Databases store performance, power and energy consumption and hardware performance counters' data. The MuMMI Analyzer entails performance and power modeling and performance-power tradeoff and optimizations. As part of the MuMMI project, we mainly focus on discussing the design and development of a MuMMI Instrumentor to provide automatic performance and power data collection and storage with low overhead on multicore systems in detail, then utilize the MuMMI Instrumentor to collect performance and power data for a hybrid MPI/OpenMP earthquake application to discuss application performance-power trade-off and optimizations. Our experimental results show that we reduce up to 8.5% the application execution time and lower up to 18.35% the energy consumption by applying Dynamic Voltage and Frequency Scaling (DVFS), Dynamic Concurrency Throttling (DCT) and loop optimizations."
pub.1125497221,Online Machine Learning-based Temperature Prediction for Thermal-aware NoC System,"The Network-on-Chip (NoC) has been proposed to solve the communication problem in multicore systems, but it usually suffers from the serious thermal problem due to high power density. To solve this problem, the proactive thermal management (PDTM) has been proven as an efficient way to prevent the system from overheating and mitigate the performance impact during the temperature control period. Based on the predicted temperature results, the PDTM controls the system temperature in advance to make the system temperature under the thermal limit. However, due to the thermal-coupling effect on the chip, it is hard to have a precise thermal prediction model, which makes the PDTM cannot control the system temperature efficiently. In this paper, we propose a lightweight thermal prediction model based on the machine learning method accompany with an online training algorithm that can adapt the hyperplane of the temperature behavior of NoC system during the runtime. The proposed model can adapt varying situations of the temperature behavior of NoC systems on the fly. Compared with the traditional thermal prediction model, the proposed approach can reduce 40.6-51.5% average error and 39.4%-54.2% maximum error."
pub.1093841592,Discovering Interpretable Geo-Social Communities for User Behavior Prediction,"Social community detection is a growing field of interest in the area of social network applications, and many approaches have been developed, including graph partitioning, latent space model, block model and spectral clustering. Most existing work purely focuses on network structure information which is, however, often sparse, noisy and lack of interpretability. To improve the accuracy and interpretability of community discovery, we propose to infer users' social communities by incorporating their spatiotemporal data and semantic information. Technically, we propose a unified probabilistic generative model, User-Community-Geo-Topic (UCGT), to simulate the generative process of communities as a result of network proximities, spatiotemporal co-occurrences and semantic similarity. With a well-designed multi-component model structure and a parallel inference implementation to leverage the power of multicores and clusters, our UCGT model is expressive while remaining efficient and scalable to growing large-scale geo-social networking data. We deploy UCGT to two application scenarios of user behavior predictions: check-in prediction and social interaction prediction. Extensive experiments on two large-scale geo-social networking datasets show that UCGT achieves better performance than existing state-of-the-art comparison methods."
pub.1103455563,Depth Assisted Adaptive Workload Balancing for Parallel View Synthesis,"Depth image-based rendering has been adopted by MPEG as the recommended view synthesis technique for free viewpoint TV applications. In this paper, a workload balancing algorithm is proposed for parallel view synthesis on multicore platforms. First, view synthesis workload is defined as the function of the number of hole-pixels in the warped images. Then, a novel depth assisted prediction method is proposed to predict the number of hole-pixels in the current frame by exploiting the depth differences between the neighboring frames, which reflects the movement of objects in video content. Feeding the predicted workload to the proposed cost function, each input frame is partitioned adaptively to balance the synthesis workload among the cores. The proposed workload prediction method outperforms the existing approaches both in terms of frame average prediction error and standard deviation in prediction error. Applying the proposed workload balancing method, the parallel view synthesis system provides higher acceleration ratio and better synchronization performance among the cores compared with other parallel processing systems without sacrificing the subjective and objective quality. It is also robust to different platforms, which shows high potential in being applied to mobile oriented applications."
pub.1027169967,Parallelization of RNA secondary structure prediction algorithm on multicore,"RNA secondary structure prediction is invaluable in creating new drugs and understanding genetic diseases, in gene expression and regulation. RNA molecules have proven their clinically consequential deep inroads into the biological functions. Needless to mention about the role of even small RNA's with numerous nucleotides which does the function of gene splicing, editing, and regulation. There is accelerating increase of data set sizes originating from powerful high-throughput measuring devices in Bioinformatics, A major limiting factor in achieving good performance on modern architectures is memory latency and bandwidth. The cost of computation is very less than the cost of moving data from main memory. This disparity between communication and computation costs has compelled the programmers to make changes in the ways algorithms are designed. A promising solution to speed up sequential programs, which are difficult to parallelise otherwise is through Speculative parallelisation. This paper proposes a new software-only speculative parallelization scheme for implementing RNA Secondary Structure Prediction algorithm in parallel. The scheme is developed after a systematic evaluation of the design options available. It is also shown to be efficient, robust and to outperform previously proposed schemes used for parallel implementation of RNA Secondary Structure Prediction."
pub.1026951355,A scalable Helmholtz solver in GRAPES over large‐scale multicore cluster,"SUMMARY This paper discusses performance optimization on the dynamical core of global numerical weather prediction model in Global/Regional Assimilation and Prediction System (GRAPES). GRAPES is a new generation of numerical weather prediction system developed and currently used by Chinese Meteorology Administration. The computational performance of the dynamical core in GRAPES relies on the efficient solution of three‐dimensional Helmholtz equations, which lead to large‐scale and sparse linear systems formulated by the discretization in space and time. We choose generalized conjugate residual (GCR) algorithm to solve the corresponding linear systems and further propose algorithm optimizations for large‐scale parallelism in two aspects: (i) reduction of iteration number for solution and (ii) performance enhancement of each GCR iteration. The reduction of iteration number is achieved by advanced preconditioning techniques, combining block incomplete LU factorization‐k preconditioner over 7‐diagonals of the coefficient matrix with the restricted additive Schwarz method effectively . The improvement for GCR iteration is to reduce the global communication operations by refactoring the GCR algorithm, which decreases the communication overhead over large number of cores. Performance evaluation on the Tianhe‐1A system shows that the new preconditioning techniques reduce almost one‐third iterations for solving the linear systems, the proposed methods can obtain 25% performance improvement on average compared with the original version of Helmholtz solver in GRAPES, and the speedup with our algorithms can reach 10 using 2048 cores compared with 256 cores. Copyright © 2013 John Wiley & Sons, Ltd."
pub.1094636749,ICE: A General and Validated Energy Complexity Model for Multithreaded Algorithms,"Like time complexity models that have significantly contributed to the analysis and development of fast algorithms, energy complexity models for parallel algorithms are desired as crucial means to develop energy efficient algorithms for ubiquitous multicore platforms. Ideal energy complexity models should be validated on real multicore platforms and applicable to a wide range of parallel algorithms. However, existing energy complexity models for parallel algorithms are either theoretical without model validation or algorithm-specific without ability to analyze energy complexity for a wide-range of parallel algorithms. This paper presents a new general validated energy complexity model for parallel (multithreaded) algorithms. The new model abstracts away possible multicore platforms by their static and dynamic energy of computational operations and data access, and derives the energy complexity of a given algorithm from its work, span and I/O complexity. The new model is validated by different sparse matrix vector multiplication (SpMV) algorithms and dense matrix multiplication (matmul) algorithms running on high performance computing (HPC) platforms (e.g., Intel Xeon and Xeon Phi). The new energy complexity model is able to characterize and compare the energy consumption of SpMV and matmul kernels according to three aspects: different algorithms, different input matrix types and different platforms. The prediction of the new model regarding which algorithm consumes more energy with different inputs on different platforms, is confirmed by the experimental results. In order to improve the usability and accuracy of the new model for a wide range of platforms, the platform parameters of ICE model are provided for eleven platforms including HPC, accelerator and embedded platforms."
pub.1094645691,Profiling and scalability of the high resolution NCEP model for Weather and Climate Simulations,"Coupled climate modeling has become one of the most challenging fields with the development of multicore architectures and its importance in daily weather and climate predictions. At IITM, ‘PRITHVI’ cluster (~70TF), NCEP Climate Forecast System (CFS) and its atmospheric component (GFS) have been installed on IBM Power6 architecture. Here, we investigate the scalability of the MPI-OPenMP hybrid models (CFS, GFS) and the limitations of hybrid spectral models when going to high resolutions. Also, this paper looks into the performance and scaling with these two models (CFS, GFS) on the TeraFLOP cluster, varying the threads and describes the preliminary results for the high resolution simulations."
pub.1136722524,Predicting the Soft Error Vulnerability of Parallel Applications Using Machine Learning,"With the widespread use of the multicore systems having smaller transistor sizes, soft errors become an important issue for parallel program execution. Fault injection is a prevalent method to quantify the soft error rates of the applications. However, it is very time consuming to perform detailed fault injection experiments. Therefore, prediction-based techniques have been proposed to evaluate the soft error vulnerability in a faster way. In this work, we present a soft error vulnerability prediction approach for parallel applications using machine learning algorithms. We define a set of features including thread communication, data sharing, parallel programming, and performance characteristics; and train our models based on three ML algorithms. This study uses the parallel programming features, as well as the combination of all features for the first time in vulnerability prediction of parallel programs. We propose two models for the soft error vulnerability prediction: (1) A regression model with rigorous feature selection analysis that estimates correct execution rates, (2) A novel classification model that predicts the vulnerability level of the target programs. We get maximum prediction accuracy rate of 73.2% for the regression-based model, and achieve 89% F-score for our classification model."
pub.1105237894,Novel Feature Selection Algorithm for Thermal Prediction Model,"Demand for more computing power grows steadily, which leads to increase the integration density of modern processors that rise thermal hotspots. A proactive thermal management algorithm tries to avoid exceeding a thermal threshold by exploiting the control decisions and using the thermal prediction model. In this paper, we proposed a new approach to build a thermal prediction model for a multicore processor using a multilayer perceptron (MLP). We generate a data set composed of system state samples gathered during Standard Performance Evaluation Corporation benchmark execution. Thereafter, a compilation method is applied to extract the new features from the data set. These features are categorized into two behavioral and reflective groups. The first group allows the model to track the current thermal behavior of the processor, whereas the second one helps the model to predict the control response temperature. Finally, a smaller set of input features is selected by a new proposed feature selection algorithm. The evaluation shows that the mean prediction error of the proposed model is about 0.5 °C–0.6 °C with 0.6 °C–0.7 °C standard deviations from 2- to 5-s prediction distances. The results show the superiority of our model and feature selection algorithm in comparison with the counterparts."
pub.1009254598,Towards Multicore Performance with Configurable Computing Units,"Energy efficiency and the need for high performance has steered computing platforms toward customization. General purpose computing however remains a challenge as on-chip resources continue to increase with a limited performance improvement. In order to truly improve processor performance, a major reconsideration at the microarchitectural level must be sought with regards to the compiler, ISA, and general architecture without an explicit dependence on transistor scaling and increased cache levels. In attempts to assign the processor transistor budget towards engineering ingenuity, this paper presents the concept of Configurable Computing Units (CCUs). CCUs are designed to make reconfigurability in general purpose computing a reality by introducing the concept of logical and physical compilation. This concept allows for both the application and underlying architecture to be considered during the compilation process. Experimental results demonstrate that a single CCU core (consisting of double engines) achieves dual core performance, with half the area and power consumption required of a conventional monolithic CPU."
pub.1112222911,Deciphering Predictive Schedulers for Heterogeneous-ISA Multicore Architectures,"Heterogeneous architectures have become increasingly common. From co-packaging small and large cores, to GPUs alongside CPUs, to general-purpose heterogeneous-ISA architectures with cores implementing different ISAs. As diversity of execution cores grows, predictive models become of paramount importance for scheduling and resource allocation. In this paper, we investigate the capabilities of performance predictors in a heterogeneous-ISA setting, as well as the predictors' effects on scheduler quality. We follow an unbiased feature selection methodology to identify the optimal set of features for this task, instead of pre-selecting features before training. We propose metrics that bridge the gap between traditional prediction accuracy metrics and a scheduler's performance. We further present our evaluation methodology, which was meticulously designed with this study in mind, and finally, we incorporate our findings in ML-based schedulers and evaluate their sensitivity to the underlying system's level of heterogeneity."
pub.1118298598,Estimating the Potential Speedup of Computer Vision Applications on Embedded Multiprocessors,"Computer vision applications constitute one of the key drivers for embedded
multicore architectures. Although the number of available cores is increasing
in new architectures, designing an application to maximize the utilization of
the platform is still a challenge. In this sense, parallel performance
prediction tools can aid developers in understanding the characteristics of an
application and finding the most adequate parallelization strategy. In this
work, we present a method for early parallel performance estimation on embedded
multiprocessors from sequential application traces. We describe its
implementation in Parana, a fast trace-driven simulator targeting OpenMP
applications on the STMicroelectronics' STxP70 Application-Specific
Multiprocessor (ASMP). Results for the FAST key point detector application show
an error margin of less than 10% compared to the reference cycle-approximate
simulator, with lower modeling effort and up to 20x faster execution time."
pub.1043679391,Introduction,"In recent years, the emergence and evolution of large scale parallel systems, grids, cloud computing environments, and multi-core architectures have prompted the performance community to push its boundaries. Whether system scale is achieved by coupling processors with a large number of cores that are tightly coupled or by massive numbers of loosely coupled processors, many systems will contain hundreds of thousands of processors on which millions of computation threads solve ever larger and more complex problems. At the same time, the coverage of the term ’performance’ has constantly broadened to include reliability, robustness, energy consumption, and scalability in addition to classical performance-oriented evaluations of system functionalities. In response to these two new sets of challenges, our community has the mission to develop a range of novel methodologies and tools for performance modeling, evaluation, prediction, measurement, benchmarking, and visualization of existing and emerging large scale parallel and distributed systems."
pub.1095670329,Experimental Estimation and Analysis of the Power Efficiency of CUDA Processing Element on SIMD Computing,"Estimating and analyzing the power consuming features of a program on a hardware platform is important in High Performance Computing (HPC) program optimization. A reasonable evaluation can help to handle the critical design constraints at the level of software, choosing preferable algorithm in order to reach the best power performance. In this paper we illustrate a simple experimental method to examine SIMD computing on GPU and Multicore computers. By measuring the power of each component and analyzing the execution speed, power parameters are captured, the power consuming features are analyzed and concluded. Thereafter power efficiency of any scale of this SIMD computation on the platform can be simply evaluated based on the features. The precision of above approximation is examined and detailed error analysis has been provided. The power consumption prediction has been validated by comparative analysis on real systems."
pub.1050612169,Increasing the efficiency and feasibility of configurable computing units,"Multicore processors are customary within current generation computing systems. The overall concept of general purpose processing however remains a challenge as architects must provide increased performance for each advancing generation without solely relying on transistor scaling and additional cache levels. Although architects have steered towards heterogeneity to increase the performance and efficiency for a variety of workloads, the fundamental issue of how a single core's architecture may be improved and applied to the multiprocessor domain remains. This work builds upon the concept of Configurable Computing Units (CCU) - a nuanced approach to processor architectures and microarchitectures, employing reconfigurable datapaths and task-based execution. This work improves upon the efficiency of CCUs by applying various new design techniques including branch prediction, variable configuration, an OpenMP programming model, and Berkeley Dwarf testing. Experimental results using Gem5 demonstrate that a single CCU core can achieve dual-core performance, with a 1.29x decrease in area overhead and 55% of the power consumption required by a conventional CPU."
pub.1001690293,Parallel particle PHD filter implemented on multicore and cluster systems,"The Probability Hypothesis Density (PHD) filter is a promising technique in terms of computational complexity to solve the multiple targets tracking problem. However, the amount of computation is prohibitive in critical situations when the clutter intensity and sample rate are high. Therefore, the execution time of the sequential particle PHD filter cannot meet the requirement for real-time processing applications. To address this problem, we propose a parallel scheme for efficient implementation of particle PHD filter on clusters of multicore distributed memory architecture. Since particles can be treated separately and spread among processors, the prediction and update step can be readily performed in parallel. However, the resampling and estimation step become the bottleneck that significantly affects the speedup and scalability achieved by the parallel implementation of particle PHD filter for the requirement of joint processing of all particles. We propose an approach to fulfill parallel resampling and stratified estimation in a unified architecture. Particle exchange to rebalance the work load among computing nodes is also discussed. Experiment results show that tracking performance of the parallel version is almost equivalent to or even better than the sequential one, while in terms of execution time we can achieve a tremendous speedup."
pub.1137410926,"Energy Predictive Models of Computing: Theory, Practical Implications and Experimental Analysis on Multicore Processors","The energy efficiency in ICT is becoming a grand technological challenge and is now a first-class design constraint in all computing settings. Energy predictive modelling based on performance monitoring counters (PMCs) is the leading method for application-level energy optimization. However, a sound theoretical framework to understand the fundamental significance of the PMCs to the energy consumption and the causes of the inaccuracy of the models is lacking. In this work, we propose a small but insightful theory of energy predictive models of computing, which formalizes both the assumptions behind the existing PMC-based energy predictive models and properties, heretofore unconsidered, that are basic implications of the universal energy conservation law. The theory’s basic practical implications include selection criteria for model variables, model intercept, and model coefficients. The experiments on two modern Intel multicore servers show that applying the proposed selection criteria improves the prediction accuracy of state-of-the-art linear regression models from 31.2% to 18%. Finally, we demonstrate that employing energy models constructed using the proposed theory for energy optimization can save a significant amount of energy (up to 80% for applications used in experiments) compared to state-of-the-art energy measurement tools."
pub.1101541791,A Novel Performance Prediction Model for Mobile GPUs,"With the fast-growing development of mobile devices, the application of high-end three-dimensional (3-D) graphics is expanding to include usage in mobile platforms. Recent mobile application processors are equipped with a multicore CPU and a mobile GPU on a single chip, thereby enabling the incorporation of high-end 3-D graphics into mobile devices. The problem is that such features consume large amounts of power. Thus, previous studies focused on estimating the power consumption of mobile GPUs, but such research has been unfamiliar with user perspective. To address this deficiency, the current work developed a novel performance prediction model for mobile GPUs on Adreno. The model uses both the instruction throughput of a unified shader and GFLOPS. The utilization of the Adreno GPUs was adjusted to its maximum value to ensure that their performance is unaffected by dynamic voltage and frequency scaling and throttling functions. The model was validated using GFXBench under real game application environments. The simulation results provided the computational rates of each hardware unit of the Adreno GPUs and the rate of increase in the instruction processing of the unified shader. To verify the accuracy of the model, we compared the difference rates of the prediction results between those derived from the proposed model and those using Snapdragon profiler. The average error rate was 3.32% with three applications running on four different mobile devices."
pub.1094444919,Level-Synchronous BFS Algorithm Implemented in Java Using PCJ Library Full/Regular Research Paper CSCI-ISPD,"Graph processing is used in many fields of science such as sociology, risk prediction or biology. Although analysis of graphs is important it also poses numerous challenges especially for large graphs which have to be processed on multicore systems. In this paper, we present PGAS (Partitioned Global Address Space) version of the level-synchronous BFS (Breadth First Search) algorithm and its implementation written in Java. Java so far is not extensively used in high performance computing, but because of its popularity, portability, and increasing capabilities is becoming more widely exploit especially for data analysis. The level-synchronous BFS has been implemented using a PCJ (Parallel Computations in Java) library. In this paper, we present implementation details and compare its scalability and performance with the MPI implementation of Graph500 benchmark. We show good scalability and performance of our implementation in comparison with MPI code written in C. We present challenges we faced and optimizations we used in our implementation necessary to obtain good performance."
pub.1045763977,EOLE,"Even in the multicore era, there is a continuous demand to increase the performance of single-threaded applications. However, the conventional path of increasing both issue width and instruction window size inevitably leads to the power wall. Value prediction (VP) was proposed in the mid 90's as an alternative path to further enhance the performance of wide-issue superscalar processors. Still, it was considered up to recently that a performance-effective implementation of Value Prediction would add tremendous complexity and power consumption in almost every stage of the pipeline Nonetheless, recent work in the field of VP has shown that given an efficient confidence estimation mechanism, prediction validation could be removed from the out-of-order engine and delayed until commit time. As a result, recovering from mispredictions via selective replay can be avoided and a much simpler mechanism -- pipeline squashing -- can be used, while the out-of-order engine remains mostly unmodified. Yet, VP and validation at commit time entails strong constraints on the Physical Register File. Write ports are needed to write predicted results and read ports are needed in order to validate them at commit time, potentially rendering the overall number of ports unbearable. Fortunately, VP also implies that many single-cycle ALU instructions have their operands predicted in the front-end and can be executed in-place, in-order. Similarly, the execution of single-cycle instructions whose result has been predicted can be delayed until commit time since predictions are validated at commit time Consequently, a significant number of instructions -- 10% to 60% in our experiments -- can bypass the out-of-order engine, allowing the reduction of the issue width, which is a major contributor to both out-of-order engine complexity and register file port requirement. This reduction paves the way for a truly practical implementation of Value Prediction. Furthermore, since Value Prediction in itself usually increases performance, our resulting {Early | Out-of-Order | Late} Execution architecture, EOLE, is often more efficient than a baseline VP-augmented 6-issue superscalar while having a significantly narrower 4-issue out-of-order engine"
pub.1093711132,EOLE: Paving the Way for an Effective Implementation of Value Prediction,"Even in the multicore era, there is a continuous demand to increase the performance of single-threaded applications. However, the conventional path of increasing both issue width and instruction window size inevitably leads to the power wall. Value prediction (VP) was proposed in the mid 90's as an alternative path to further enhance the performance of wide-issue superscalar processors. Still, it was considered up to recently that a performance-effective implementation of Value Prediction would add tremendous complexity and power consumption in almost every stage of the pipeline. Nonetheless, recent work in the field of VP has shown that given an efficient confidence estimation mechanism, prediction validation could be removed from the out-of-order engine and delayed until commit time. As a result, recovering from mispredictions via selective replay can be avoided and a much simpler mechanism pipeline squashing–can be used, while the out-of-order engine remains mostly unmodified. Yet, VP and validation at commit time entails strong constraints on the Physical Register File. Write ports are needed to write predicted results and read ports are needed in order to validate them at commit time, potentially rendering the overall number of ports unbearable. Fortunately, VP also implies that many single-cycle ALU instructions have their operands predicted in the front-end and can be executed in-place, in-order. Similarly, the execution of single-cycle instructions whose result has been predicted can be delayed until commit time since predictions are validated at commit time. Consequently, a significant number of instructions–10% to 60% in our experiments–can bypass the out-of-order engine, allowing the reduction of the issue width, which is a major contributor to both out-of-order engine complexity and register file port requirement. This reduction paves the way for a truly practical implementation of Value Prediction. Furthermore, since Value Prediction in itself usually increases performance, our resulting {Early, Out-of-Order, Late} Execution architecture, EOLE, is often more efficient than a baseline VP-augmented 6-issue superscalar while having a significantly narrower 4-issue out-of-order engine."
pub.1108002804,Analysis and Modeling of Resource Contention Effects based on Benchmark Applications,"Benchmark suites, such as SPLASH − 3, can be used to investigate the parallel runtime of complex applications from science and engineering on modern multicore platforms. In high performance computing (HPC), a high number of such parallel tasks might be executed concurrently. Depending on the specific operations performed by the tasks, their runtimes are often affected by contention for hardware resources, such as communication networks, the main memory, or hard disks. In this article, we investigate the effects of resource contention for the concurrent execution of application and kernel tasks of the SPLASH − 3 benchmark suite. The parallel tasks are executed on a heterogeneous HPC cluster and an approach for modeling the measured runtimes without and with resource contention is presented. The proposed modeling is used for runtime predictions and the achieved accuracy is evaluated. To reduce the time required for executing multiple parallel tasks of the SPLASH-3 benchmark suite, the runtime prediction is integrated into a scheduling method for parallel tasks on heterogeneous HPC clusters. Performance results are shown to demonstrate the improvements achieved with a better utilization of parallel and concurrent executions."
pub.1093585656,On Prediction Accuracy of Machine Learning Algorithms for Characterizing Shared L2 Cache Behavior of Programs on Multicore Processors,"Information on a particular behavioral aspect of a program can be useful to know about the performance bottlenecks and can be utilized further to improve the performance of the system. It is observed that contention for shared L2 cache between programs running on a Multi-Core Processor (MCP) is one of the performance bottlenecks. The utilization of the L2 cache by a program, while sharing it with others on a MCP is a metric of interest to frame policies that reduce contention. In this work we investigate the ability of some of the machine learning algorithms to predict the solo run L2 cache stress of a running program on Intel quad-core Xeon X5482 processor. Data collected from hardware performance counters of Intel quad-core Xeon X5482 processor were utilized to derive the attributes to train the machine learning algorithms. We observed that the best performing machine learning algorithm in this context is Model tree (M5,), followed by Artificial Neural Networks (ANN)."
pub.1094378708,Faster unicores are still needed,"Summary form only given. For the last decade, the advent of the multicore era has driven most of the research architecture effort from the high end processor industry as well as from the computer architecture research community. However, many of the workloads executed on our servers, PCs, smartphones, tablets are still inherently sequential or multiprogrammed; even parallel applications require high sequential performance. Therefore, there are new opportunities for research in uniprocessor microarchitecture. In this presentation, I will first present the motivations of the ERC grant DAL Defying Amdahl's Law. I will then present two research actions on processor core architecture that are on-going within the DAL framework in the INRIA/IRISA ALF group: revisiting value prediction and out-of-order execution of predicated instruction sets."
pub.1093998139,Perils of Power Prediction in Early Power-Integrity Analysis,"Early power integrity and peak power analyses for multi-million gate system on chip (SoC) in advanced technology nodes pose significant methodology definition and implementation challenges. Typically in a SoC, processors and other high performance IPs are dominant contributors to peak power and power integrity issues. To get an early look ahead of potential power integrity issues and to estimate peak di/dt issues in the SoC, it is always desired to analyze potential issues early and address before a silicon failure. This paper presents an overview of implementation challenges faced in RTL based power for predictive power analysis and analyzing peak di/dt issues ahead of time in the context of TI C66x DSP core based multicore SoC."
pub.1152954803,Laparoscopic Grasper Force Sensing Based On Multi-core Fiber Bragg Grating Assisted by Random Forest,"In this paper, we proposed a laparoscopic grasper force sensor that is able to distinguish both the magnitude and direction of the force, which is improved by random forest. The sensor is based on a seven-core multicore fiber inscribed with Bragg gratings (FBGs) in each core and packaged with polydimethylsiloxane (PDMS). The force measurement was demonstrated by applying the force magnitude in the range of 0 to 6N and direction ranging from 0° to 45°. To realize the prediction of the force and direction simultaneously, a random forest model is established and the results show a magnitude error of 0.104 N and an angular error of 2.269°, implying a good performance even with limited experimental data."
pub.1005307326,A graph-theoretic analysis of the human protein-interaction network using multicore parallel algorithms,"Due to fundamental physical limitations and power constraints, we are witnessing a paradigm shift in commodity microprocessor architecture to multicore designs. Continued performance now requires the exploitation of concurrency at the algorithm level. In this article, we demonstrate the application of high performance computing techniques in systems biology and present multicore algorithms for the important research problem of protein-interaction network (PIN) analysis.PINs play an important role in understanding the functional and organizational principles of biological processes. Promising computational techniques for key systems biology research problems such as identification of signaling pathways, novel protein function prediction, and the study of disease mechanisms, are based on topological characteristics of the protein interactome. Several complex network models have been proposed to explain the evolution of protein networks, and these models primarily try to reproduce the topological features observed in yeast, the model eukaryote interactome. In this article, we study the structural properties of a high-confidence human interaction network, constructed by assimilating recent experimentally derived interaction data. We identify topological properties common to the yeast and human protein networks.Betweenness is a quantitative measure of centrality of an entity in a complex network, and is based on computing all-pairs shortest paths in the graph. A novel contribution of our work is the analysis of the degree–betweenness centrality correlation in the human PIN. Jeong et al. empirically showed that betweenness is positively correlated with the essentiality and evolutionary age of a protein. We observe that proteins with high betweenness, but low degree (or connectivity) are abundant in the human PIN. We have designed efficient and portable parallel implementations for the exact calculation of betweenness and other compute-intensive centrality metrics relevant to interactome analysis. For example, on the Sun Fire T2000 server with the UltraSparc T1 (Niagara) processor, we achieve a relative speedup of about 16 using 32 threads for a typical instance of betweenness centrality on a PIN, reducing the running time from nearly 312min to 13s."
pub.1018414972,Combining Locality Analysis with Online Proactive Job Co-scheduling in Chip Multiprocessors,"The shared-cache contention on Chip Multiprocessors causes performance degradation to applications and hurts system fairness. Many previously proposed solutions schedule programs according to runtime sampled cache performance to reduce cache contention. The strong dependence on runtime sampling inherently limits the scalability and effectiveness of those techniques. This work explores the combination of program locality analysis with job co-scheduling. The rationale is that program locality analysis typically offers a large-scope view of various facets of an application including data access patterns and cache requirement. That knowledge complements the local behaviors sampled by runtime systems. The combination offers the key to overcoming the limitations of prior co-scheduling techniques.Specifically, this work develops a lightweight locality model that enables efficient, proactive prediction of the performance of co-running processes, offering the potential for an integration in online scheduling systems. Compared to existing multicore scheduling systems, the technique reduces performance degradation by 34% (7% performance improvement) and unfairness by 47%. Its proactivity makes it resilient to the scalability issues that constraints the applicability of previous techniques."
pub.1093776692,Hybrid MPI/OpenMP Power-Aware Computing,"Power-aware execution of parallel programs is now a primary concern in large-scale HPC environments. Prior research in this area has explored models and algorithms based on dynamic voltage and frequency scaling (DVFS) and dynamic concurrency throttling (DCT) to achieve power-aware execution of programs written in a single programming model, typically MPI or OpenMP. However, hybrid programming models combining MPI and OpenMP are growing in popularity as emerging large-scale systems have many nodes with several processors per node and multiple cores per processor. In this paper we present and evaluate solutions for power-efficient execution of programs written in this hybrid model targeting large-scale distributed systems with multicore nodes. We use a new power-aware performance prediction model of hybrid MPIIOpenMP applications to derive a novel algorithm for power-efficient execution of realistic applications from the ASC Sequoia and NPB MZ benchmarks. Our new algorithm yields substantial energy savings (4.18 % on average and up to 13.8 %) with either negligible performance loss or performance gain (up to 7.2%)."
pub.1139743286,A Fairness Conscious Cache Replacement Policy for Last Level Cache,"Multicore systems with shared Last Level Cache (LLC) possess a bigger challenge in allocating the LLC space among multiple applications running in the system. Since all applications use the shared LLC, interference caused by them may evict important blocks of other applications that result in premature eviction and may also lead to thrashing. Replacement policies applied locally to a set distributes the sets in a dynamic way among the applications. However, previous work on replacement techniques focused on the re-reference aspect of a block or application behavior to improve the overall system performance. The paper proposes a novel cache replacement technique Application Aware Re-reference Interval Prediction (AARIP) that considers application behavior, re-reference interval, and premature block eviction for replacing a cache block. Experimental evaluation on a four-core system shows that AARIP achieves an overall performance improvement of 7.28%, throughput by 4.9%, and improves overall system fairness by 7.85%, as compared to the traditional SRRIP replacement policy."
pub.1090670994,Scalable Algorithms for Bayesian Inference of Large-Scale Models from Large-Scale Data,"One of the greatest challenges in computational science and engineering today is how to combine complex data with complex models to create better predictions. This challenge cuts across every application area within CS&E, from geosciences, materials, chemical systems, biological systems, and astrophysics to engineered systems in aerospace, transportation, structures, electronics, biomedicine, and beyond. Many of these systems are characterized by complex nonlinear behavior coupling multiple physical processes over a wide range of length and time scales. Mathematical and computational models of these systems often contain numerous uncertain parameters, making high-reliability predictive modeling a challenge. Rapidly expanding volumes of observational data—along with tremendous increases in HPC capability—present opportunities to reduce these uncertainties via solution of large-scale inverse problems."
pub.1093393509,VAST: Virtualization-Assisted Concurrent Autonomous Self-Test,"Virtualization-Assisted concurrent, autonomous Self-Test, or VAST, enables a multi-/many-core system to test itself, concurrently during normal operation, without any user-visible downtime. Such on-line self-test is required for large-scale robust systems with built-in support for circuit failure prediction, failure detection, diagnosis, and self-healing. The main idea behind VAST is hardware and software co-design of on-line self-test features in a multi-/many-core system through integration of: 1. multi-/many-core architecture, 2. virtualization software, and, 3. special self-test techniques such as BIST (Built-In Self-Test) or CASP (Concurrent Autonomous chip self-test using Stored Patterns). As a result, optimized trade-offs in system design complexity, system performance and power impact, and test thoroughness are possible. Experimental results from an actual multicore system demonstrate that: 1. VAST is practical and effective; and, 2. Special VAST-supported self-test policies enable extremely thorough on-line self-test with very small performance impact."
pub.1044901924,FAST,"Stencil computations comprise an important class of kernels in many scientific computing applications. As the diversity of both architectures and programming models grow, autotuning is emerging as a critical strategy for achieving portable performance across a broad range of execution contexts for stencil computations. However, costly tuning overhead is a major obstacle to its popularity. In this work, we propose a fast stencil autotuning framework FAST based on an Optimal-Solution Space (OSS) model to significantly improve tuning speed. It leverages a feature extractor that comprehensively characterizes stencil computation. Using the extracted features, FAST constructs an OSS database to train an off-line model which provides an on-line prediction. We evaluate FAST with five important stencil computation applications on both an Intel Xeon multicore CPU and an NVIDIA Tesla K20c GPU. Compared with state-of-the-art stencil autotuners like Patus and SDSL, FAST improves autotuning speed by 10-2697 times without any user annotation, while achieving comparable performance."
pub.1110516562,Prediction and bounds on shared cache demand from memory access interleaving,"Cache in multicore machines is often shared, and the cache performance depends on how memory accesses belonging to different programs interleave with one another. The full range of performance possibilities includes all possible interleavings, which are too numerous to be studied by experiments for any mix of non-trivial programs. This paper presents a theory to characterize the effect of memory access interleaving due to parallel execution of non-data-sharing programs. The theory uses an established metric called the footprint (which can be used to calculate miss ratios in fully-associative LRU caches) to measure cache demand, and considers the full range of interleaving possibilities. The paper proves a lower bound for footprints of interleaved traces, and then formulates an upper bound in terms of the footprints of the constituent traces. It also shows the correctness of footprint composition used in a number of existing techniques, and places precise bounds on its accuracy."
pub.1018374627,Estimating parallel performance,"In this paper we introduce our estimation method for parallel execution times, based on identifying separate “parts” of the work done by parallel programs. Our run time analysis works without any source code inspection. The time of parallel program execution is expressed in terms of the sequential work and the parallel penalty. We measure these values for different problem sizes and numbers of processors and estimate them for unknown values in both dimensions using statistical methods. This allows us to predict parallel execution time for unknown inputs and non-available processor numbers with high precision. Our prediction methods require orders of magnitude less data points than existing approaches. We verified our approach on parallel machines ranging from a multicore computer to a peta-scale supercomputer.Another useful application of our formalism is a new measure of parallel program quality. We analyse the values for parallel penalty for both growing input size and for increasing numbers of processing elements. From these data, conclusions on parallel performance and scalability are drawn."
pub.1104571622,Prediction and bounds on shared cache demand from memory access interleaving,"Cache in multicore machines is often shared, and the cache performance depends on how memory accesses belonging to different programs interleave with one another. The full range of performance possibilities includes all possible interleavings, which are too numerous to be studied by experiments for any mix of non-trivial programs. This paper presents a theory to characterize the effect of memory access interleaving due to parallel execution of non-data-sharing programs. The theory uses an established metric called the footprint (which can be used to calculate miss ratios in fully-associative LRU caches) to measure cache demand, and considers the full range of interleaving possibilities. The paper proves a lower bound for footprints of interleaved traces, and then formulates an upper bound in terms of the footprints of the constituent traces. It also shows the correctness of footprint composition used in a number of existing techniques, and places precise bounds on its accuracy."
pub.1028305313,"Euro-Par 2016: Parallel Processing, 22nd International Conference on Parallel and Distributed Computing, Grenoble, France, August 24-26, 2016, Proceedings","This book constitutes the refereed proceedings of the 22nd International Conference on Parallel and Distributed Computing, Euro-Par 2016, held in Grenoble, France, in August 2016. The 47 revised full papers presented together with 2 invited papers and one industrial paper were carefully reviewed and selected from 176 submissions. The papers are organized in 12 topical sections: Support Tools and Environments; Performance and Power Modeling, Prediction and Evaluation; Scheduling and Load Balancing; High Performance Architectures and Compilers; Parallel and Distributed Data Management and Analytics; Cluster and Cloud Computing; Distributed Systems and Algorithms; Parallel and Distributed Programming, Interfaces, Languages; Multicore and Manycore Parallelism; Theory and Algorithms for Parallel Computation and Networking; Parallel Numerical Methods and Applications; Accelerator Computing."
pub.1182022691,Embankment Breach Simulation and Inundation Mapping Leveraging High-Performance Computing for Enhanced Flood Risk Prediction and Assessment,"Abstract. Embankment breaches represent significant hazards to communities and infrastructure, precipitating catastrophic flood occurrences. The precise prediction of floods and understanding the scope of inundation stemming from embankment failures are imperative for effective disaster preparedness and response. This research delves into a case study on simulating embankment breaches to evaluate the extent of flooding. Leveraging advanced hydrodynamic models validated through high-performance computing (HPC) systems, and integrating real-time data assimilation, we aim to improve accuracy in flood forecasting. The study endeavours to bolster flood risk management by furnishing detailed inundation maps and insights into embankment breach dynamics, thereby facilitating enhanced preparedness and response strategies. Our findings reveal that simulations conducted on multicore processors offer superior performance compared to single-core setups, yielding enhanced result accuracy and providing administrators with increased lead time. It unlocks high-resolution simulations for intricate basin details, explores a wider range of flood scenarios quickly, and allows for efficient ensemble modelling to assess model uncertainty. Through HPC utilization, we can harness high-resolution digital elevation models (DEMs) for 2D-hydrodynamic modelling, enabling rapid assessment of water spread resulting from embankment breaches within a mere 20-minute timeframe, a significant improvement from the previous 3–4 hours duration."
pub.1129698532,Performance Study of Catmull-Clark Subdivision Surfaces Algorithm,"Generating smooth surfaces of arbitrary topology is a major challenge in geometric modeling, computer graphics, and scientific visualization. Subdivision surfaces have emerged as a powerful and useful technique in modeling free-form surfaces. Recursive subdivision techniques generate visually pleasing smooth surfaces through the repeated application of a fixed set of subdivision rules. Given a control mesh, a Catmull-Clark Subdivision Surface (CCSS) is generated by iteratively refining (subdividing) the control mesh to form new control meshes. CCSS implementations do not perform well and suffer from low CPU utilization because they are often waiting for data to be transferred from memory due to the repeated pointer indirections. Rapid Evaluation of CCSS is an approach that focuses on achieving maximum performance by carefully addressing caching issues. However, this approach is designed with primary target being single core machines. In this paper, we implement the Rapid Evaluation of Catmull-Clark Subdivision Surfaces (RECCSS) algorithm in C++ and measure its achieved performance. We then redesign RECCSS to leverage the potential of parallel computation on widely available multicores machines and show that Parallelized RECCSS (PRECCSS) achieves noticeable speedup when run on diverse set of machines compared to the original RECCSS implementation. In addition, we conduct a study about the data value predictability of PRECCSS load instructions using Pin dynamic binary instrumentation tool, and conclude that 93.25% prediction accuracy can be achieved with existing value prediction techniques."
pub.1094484993,Predicting Performance on a Loosely Controlled Event System,"We contend that event systems will become the kernel of distributed middleware for multicore systems due to their asynchronous nature. In a concurrent system, the fewer points of synchronization between independent activities, the better the achievable parallelization. As physical devices are integrated into distributed middleware the timeliness of the execution of functions becomes increasingly important. Precise guarantees require complete control of all the elements which influence the execution. However, the purpose of middleware is to abstract from the exact nature of the underlying platform. Here we outline an approach that allocates processor shares to event queues, thus controlling the rate of events handled at any given queue. We show how this in turn allows us to predict the likely throughput through a queue. This prediction is achieved through model fitting and calibration."
pub.1108072961,"Euro-Par 2018: Parallel Processing, 24th International Conference on Parallel and Distributed Computing, Turin, Italy, August 27 - 31, 2018, Proceedings","This book constitutes the proceedings of the 24th International Conference on Parallel and Distributed Computing, Euro-Par 2018, held in Turin, Italy, in August 2018. The 57 full papers presented in this volume were carefully reviewed and selected from 194 submissions. They were organized in topical sections named: support tools and environments; performance and power modeling, prediction and evaluation; scheduling and load balancing; high performance architecutres and compilers; parallel and distributed data management and analytics; cluster and cloud computing; distributed systems and algorithms; parallel and distributed programming, interfaces, and languages; multicore and manycore methods and tools; theory and algorithms for parallel computation and networking; parallel numerical methods and applications; and accelerator computing for advanced applications."
pub.1093761923,A Statistical Model for Hybrid Wireless Network on Chip,"A scalable and easy to design solution for the continuing demand of high performance chip multiprocessors paved the way for Hybrid Wireless Network on Chip (WiNoC) architectures which comprises of both wired and wireless channel interconnects for communication. The simulation results of this emerging communication architecture outperforms other traditional interconnects in performance gain for multicore systems. Besides simulation, statistical modelling is another way to measure system performance. This paper proposes a statistical model based on queuing theory to measure performance of scalable Hybrid Wireless Network on Chip (WiNoC) architecture. The statistical model studies the transition state distribution and steady state distribution of flits in the input channel and analyzes average number of flits and average waiting time of flits in the wireless channel. The proposed statistical model predictions have been verified with the simulation results which shows that the relative error between the two results to determine the average latency of the flits and the number of flits lies within 5%. The statistical model also determines the critical value when the network enters into the state of congestion. Thus the proposed statistical model has been verified and validated accurately with the simulation results."
pub.1009043139,Roofline Model Toolkit: A Practical Tool for Architectural and Program Analysis,"We present preliminary results of the Roofline Toolkit for multicore, manycore, and accelerated architectures. This paper focuses on the processor architecture characterization engine, a collection of portable instrumented micro benchmarks implemented with Message Passing Interface (MPI), and OpenMP used to express thread-level parallelism. These benchmarks are specialized to quantify the behavior of different architectural features. Compared to previous work on performance characterization, these microbenchmarks focus on capturing the performance of each level of the memory hierarchy, along with thread-level parallelism, instruction-level parallelism and explicit SIMD parallelism, measured in the context of the compilers and run-time environments. We also measure sustained PCIe throughput with four GPU memory managed mechanisms. By combining results from the architecture characterization with the Roofline model based solely on architectural specifications, this work offers insights for performance prediction of current and future architectures and their software systems. To that end, we instrument three applications and plot their resultant performance on the corresponding Roofline model when run on a Blue Gene/Q architecture."
pub.1139736400,An Accurate Learning-Based Performance/Power Model for System-Level Design of a Multicore Multithreaded Network Processor,"In the network applications domain, different network environments and scenarios demand various line rates, and restrain the design of the network processor by several constraints such as power and area. Additionally, new network services and applications with different processing requirements are increasingly emerging day by day. In this regard, having a multi-objective and flexible performance model that can be used to minimize the cost and time of designing network processors is an inevitable need. In this paper, we propose an accurate and fast prediction model that exploits just a few numbers of system-level parameters to estimate the performance and power of a commercial network processor, Intel IXP2800. The proposed design methodology uses a non-linear learning algorithm - a combination of the polynomial transformation of design parameters and higher-order spline functions - which in the face of newly introduced applications needs only a small training set, i.e. a small number of simulations, to train the model. Our experimental results show the proposed models can achieve a median error rate as low as 8.7 percent for performance and 2.6 percent for power metric."
pub.1094672667,Predict-More Router: A Low Latency NoC Router with More Route Predictions,"Network-on-Chip (NoC) is a critical part of the memory hierarchy of emerging multicores. Lowering its communication latency while preserving its bandwidth is key to achieving high system performance. By now, one of the most effective methods helps achieving this goal is prediction router (PR). PR works by predicting the route an incoming packet may be transferred to and it speculatively allocates resources (virtual channels and the switch crossbar) to the packet and traverses the packet's flits using this predicted route in a single cycle without waiting for route computation; however, if prediction misses, the packet will then be processed in the conventional pipeline (in our work, four cycles) and the speculatively allocated router resources will be wasted. Obviously, prediction accuracy contributes to the amount of successful predictions, latency reduction and bandwidth consumption. We find that predictions hit around 65% for most applications even under the best algorithm so in such cases PR can at most accelerate about 65% of the packets while the left 35% will consume extra router resources and bandwidth. In order to increase the prediction accuracy, we propose a technique, which makes use of multiple prediction algorithms at the same time for one incoming packet. Such a prediction is more accurate. With this proposal, we design and implement predict-more router (PmR). While effectively increasing the prediction accuracy, PmR also helps utilizing remaining bandwidth within the router more productively. When both PmR and PR are evaluated under their best algorithm(s), we find that PmR is over 15% higher in prediction accuracy than PR, which helps PmR outperform PR by 3.5% on average in speeding-up the system. We also find that although PmR creates more contentions in prediction, these contentions can be well resolved and are kept within the router so both router internal bandwidth and link bandwidth are not exacerbated with it."
pub.1051012311,Consistent runtime thermal prediction and control through workload phase detection,"Elevated temperatures impact the performance, power consumption, and reliability of processors, which rely on integrated thermal sensors to measure runtime thermal behavior. These thermal measurements are typically inputs to a dynamic thermal management system that controls the operating parameters of the processor and cooling system. The ability to predict future thermal behavior allows a thermal management system to optimize a processor's operation so as to prevent the on-set of high temperatures. In this paper we propose a new thermal prediction method that leads to consistent results between the thermal models used in prediction and observed thermal sensor measurements, and is capable of accurately predicting temperature behavior with heterogenous workload assignment on a multicore platform. We devise an off-line analysis algorithm that learns a set of thermal models as a function of operating frequency and globally defined workload phases. We incorporate these thermal models into a dynamic voltage and frequency scaling (DVFS) technique that limits the maximum temperature during runtime. We demonstrate the effectiveness of our proposed system in predicting the thermal behavior of a real quad-core processor in response to different workloads. In comparison to a reactive thermal management technique, our predictive method dramatically reduces the number of thermal violations, the magnitude of thermal cycles, and workload runtimes."
pub.1045240346,Prediction and Predictability for Search Query Acceleration," A commercial web search engine shards its index among many servers, and therefore the response time of a search query is dominated by the slowest server that processes the query. Prior approaches target improving responsiveness by reducing the tail latency , or high-percentile response time, of an individual search server. They predict query execution time, and if a query is predicted to be long-running, it runs in parallel; otherwise, it runs sequentially. These approaches are, however, not accurate enough for reducing a high tail latency when responses are aggregated from many servers because this requires each server to reduce a substantially higher tail latency (e.g., the 99.99th percentile), which we call extreme tail latency.   To address tighter requirements of extreme tail latency, we propose a new design space for the problem, subsuming existing work and also proposing a new solution space. Existing work makes a prediction using features available at indexing time and focuses on optimizing prediction features for accelerating tail queries. In contrast, we identify “when to predict?” as another key optimization question. This opens up a new solution of delaying a prediction by a short duration to allow many short-running queries to complete without parallelization and, at the same time, to allow the predictor to collect a set of dynamic features using runtime information. This new question expands a solution space in two meaningful ways. First, we see a significant reduction of tail latency by leveraging “dynamic” features collected at runtime that estimate query execution time with higher accuracy. Second, we can ask whether to override prediction when the “predictability” is low. We show that considering predictability accelerates the query by achieving a higher recall.  With this prediction, we propose to accelerate the queries that are predicted to be long-running. In our preliminary work, we focused on parallelization as an acceleration scenario. We extend to consider heterogeneous multicore hardware for acceleration. This hardware combines processor cores with different microarchitectures such as energy-efficient little cores and high-performance big cores, and accelerating web search using this hardware has remained an open problem. We evaluate the proposed prediction framework in two scenarios: (1) query parallelization on a multicore processor and (2) query scheduling on a heterogeneous processor. Our extensive evaluation results show that, for both scenarios of query acceleration using parallelization and heterogeneous cores, the proposed framework is effective in reducing the extreme tail latency compared to a start-of-the-art predictor because of its higher recall, and it improves server throughput by more than 70% because of its improved precision."
pub.1009097404,Modeling the performance of parallel applications using model selection techniques,"SUMMARY Nowadays, parallel architectures are changing so fast that there is a need for scalable and efficient tools to analyze and predict the performance of parallel applications. Analytical models are proved to be a useful approximation for characterizing parallel algorithms, but developing accurate analytical models is a hard issue, and, in general, they provide coarse performance predictions due to their intrinsic lack of accuracy. In this paper, we describe in detail the Tools for Instrumentation and Analysis (TIA) framework, an easy‐to‐use tool that automatically obtains accurate performance models by means of analytical expressions. This framework automatizes most of its internal tasks, reducing opportunities for human error, and it only requires the user to focus on the metrics and execution parameters that might influence the performance, those that should be considered in the modeling process. Its main advantage over other tools is that TIA uses model selection techniques that allow the automation of the modeling process. As a case of study, the use of TIA to obtain analytical models of different implementations of the broadcast collective communication in a cluster of multicores is shown. The results obtained by TIA are evaluated and compared with theoretical approaches based on the LogGP model. Copyright © 2013 John Wiley & Sons, Ltd."
pub.1042565160,System-Level Modeling and Simulation with Intel® CoFluent™ Studio,"Intel® CoFluent™ Studio is a visual model-driven development (MDD) solution for creating executable specifications of complex systems. It can be used at any point of the project lifecycle for modeling and validating any electronic or information systems in any application domain: hardware block, software stack, System-on-Chip (SoC), mixed hardware/software embedded system, networked/distributed system, end-to-end Internet-of-Things (IoT) infrastructure and Big Data networks. Intel CoFluent Studio can predict performance data from the application and use cases model execution on a multicore/multiprocessor platform model. Intel CoFluent Studio is a system modeling and simulation toolset based on Eclipse. Models are captured in graphical diagrams using Intel CoFluent optimized domain-specific language (DSL) or standard UML notations—a combination of SysML and the MARTE profile. ANSI C or C++ is used as action language to capture data types and algorithms. Non-functional system requirements or model calibration data such as execution durations, power, or memory values, are added through model attributes. Models are translated into transaction-level modeling (TLM) SystemC code for execution. The SystemC code is instrumented and generates traces that can be monitored with various analysis tools. Fast host-based simulations allow designers to observe the real-time execution of their application models on multiprocessor/multicore platform models. Performance figures such as latencies, throughputs, buffer levels, resource loads, power consumption, memory footprint, and cost can be extracted.We will present this system-level technologies and associated methodology with a poster. The scope of the poster is related to the two following topics in technical and scientific methods:Systems architecture (needs capture, requirements development, systems modelling, simulation, optimization, sizing and specification, architectural frameworks).Systemic tools (configuration management, system behaviour analysis tools, modeling and simulation tools, test management)."
pub.1061535350,"A Performance Analysis Methodology for Multicore, Multithreaded Processors","A key challenge to program a chip multiprocessor (CMP) is how to evaluate the performance of various possible program-task-to-core mapping choices during the initial programming phase, when the executable program is yet to be developed. In this paper, we put forward a thread-level modeling methodology to meet this challenge. The idea is to model thread-level activities only and overlook the instruction-level and microarchitectural details, except those having significant impact on the thread-level performance. Moreover, since the thread-level modeling is much coarser than the instruction-level modeling, the analysis at this level turns out to be significantly faster than that at the instruction level. These features make the methodology particularly amenable for fast performance evaluation of a large number of program-task-to-core mapping choices during the initial programming phase. Based on this methodology, an analytic modeling technique based on queuing theory and a fast simulation tool are developed, both allowing for fast performance prediction of CMPs. Case studies based on a large number of code samples available in IXP1200/2400 workbenches demonstrate that the maximal sustainable line rates estimated using our simulation tool and queuing network models are consistently within 6 and 8 percent of cycle-accurate simulation results, respectively."
pub.1011716808,Contentiousness vs. sensitivity,"Runtime systems to mitigate memory resource contention problems on multicore processors have recently attracted much research attention. One critical component of these runtimes is the indicators to rank and classify applications based on their contention characteristics. However, although there has been significant research effort, application contention characteristics remain not well understood and indicators have not been thoroughly evaluated. In this paper we performed a thorough study of applications' contention characteristics to develop better indicators to improve contention-aware runtime systems. The contention characteristics are composed of an application's contentiousness, and its sensitivity to contention. We show that contentiousness and sensitivity are not strongly correlated, and contrary to prior work, a single indicator is not adequate to predict both. Also, while prior work argues that last level cache miss rate is one of the best indicators to predict an application's contention characteristics, we show that depending on the workloads, it can often be misleading. We then present prediction models that consider contention in various memory resources. Our regression analysis establishes an accurate model to predict application contentiousness. The analysis also demonstrates that performance counters alone may not be sufficient to accurately predict application sensitivity to contention. Our evaluation using SPEC CPU2006 benchmarks shows that when predicting an application's contentiousness, the linear correlation coefficient R2 of our predictor and the real measured contentiousness is 0.834, as opposed to 0.224 when using last level cache miss rate."
pub.1049171153,EAP: Energy-Awareness Predictor in Multicore CPU,"To deal with inference and reasoning problems, Gaussian process has been considered as a promising tool due to the robustness and flexibility features. Especially, solving the regression and classification, Gaussian process coupling with Bayesian learning is one of the most appropriate supervised learning approaches in terms of accuracy and tractability. Because of these features, it is reasonable to engage Gaussian process for energy saving purpose. In this paper, the research focuses on analyzing the capability of Gaussian process, implementing it to predict CPU utilization, which is used as a factor to predict the status of computing node. Subsequently, a migration mechanism is applied so as to migrate the system-level processes between CPU cores and turn off the idle ones in order to save the energy while still maintaining the performance."
pub.1033367797,A Parallel Solver for Incompressible Fluid Flows,"The Navier-Stokes equations describe a large class of fluid flows but are difficult to solve analytically because of their nonlin- earity. We present in this paper a parallel solver for the 3-D Navier-Stokes equations of incompressible unsteady flows with constant coefficients, discretized by the finite difference method. We apply the prediction-projection method which transforms the Navier-Stokes equations into three Helmholtz equations and one Poisson equation. For each Helmholtz system, we apply the Alternating Direction Implicit (ADI) method resulting in three tridiagonal systems. The Poisson equation is solved using partial diagonalization which transforms the Laplacian operator into a tridiagonal one. We describe an implementation based on MPI where the computations are performed on each subdomain and information is exchanged on the interfaces, and where the tridiagonal system solutions are accelerated using vectorization techniques. We present performance results on a current multicore system."
pub.1124009948,Recommender system implementations for embedded collaborative filtering applications," This paper starts proposing a complete recommender system implemented on reconfigurable hardware with the purpose of testing on-chip, low-energy embedded collaborative filtering applications. Although the computing time is lower than the one obtained from usual multicore microprocessors, this proposal has the advantage of providing an approach to solve any prediction problem based on collaborative filtering by using an off-line, highly-portable light computing environment. This approach has been successfully tested with state-of-the-art datasets. Next, as a result of improving certain tasks related to the on-chip recommender system, we propose a custom, fine-grained parallel circuit for quick matrix multiplication with floating-point numbers. This circuit was designed to accelerate the predictions from the model obtained by the recommender system, and tested with two small datasets for experimental purposes. The accelerator is built from two levels of parallelism. On the one hand, several predictions run in parallel through the simultaneous multiplication of different vectors of two matrices. On the other hand, the operation of each vector is executed in parallel by multiplying pairs of floating-point values to later add the corresponding results in parallel as well. This circuit was compared with other approaches designed for the same purpose: circuits built using automatized tools of high-level synthesis, a general-purpose microprocessor, and high-performance graphical processing units. The performance of the prediction accelerator in terms of time surpassed that of the other approaches. We also evaluated the scalability of the circuit to practical problems using the high-level synthesis approach, and confirmed that implementations based on reconfigurable hardware allow acceptable speedups of multi-core processors."
pub.1110587976,Time-Predictable Distributed Shared Memory for Multi-Core Processors,"Multi-core processors for real-time systems need to have a time-predictable way of communicating. The use of a single, external shared memory is the standard for multicore processor communication. However, this solution is hardly time predictable. This paper presents a time-predictable solution for communication between cores, a distributed shared memory using a network-on-chip. The network-on-chip supports reading and writing data to and from distributed on-chip memory. This paper covers the implementation of time-predictable read requests on a network-on-chip. The network is implemented using statically scheduled, time-division multiplexing, enabling predictions for worst-case execution time. The implementation attempts to keep buffering as low as possible to obtain a small footprint. The solution has been implemented and successfully synthesized with a multi-core system on an FPGA. Finally, we show resource and performance measurements."
pub.1154887081,The Impact of Multicore CPUs on Eco-Friendly Query Processors in Big Data Warehouses,"Given the large and growing volume of big data and frequent use of complex analytical queries, understanding energy efficiency of query processing has become a critical research issue, as highlighted by database systems papers in the last few years. Common software solutions mainly consider IO cost models to estimate energy consumption when executing queries. On the other hand, current hardware solutions benefit from advances in the development of green components and their associated tuning techniques, especially dynamic voltage and frequency scaling (DVFS), which can balance the performance and power consumption of multicore CPUs. Unfortunately, to the best of our knowledge, there is an absence of solutions mixing both (hardware and software). Heeding this gap, we propose a novel predictive model to measure and predict energy consumption of analytical queries when using multi-core processors and different frequency configurations. We first experimentally illustrate the surprising impact of CPU frequency and the number of processor cores on execution time, power and energy consumption. Second, we introduce an extended predictive model that enriches a well-known machine learning cost model with our new angle, the frequency scaling in multi-core environment. Specifically, by using Support Vector Regression and Random Forest Regression, we compute the energy coefficients of an accurate regression model for energy prediction. Experiments with benchmark data sets TPC-H and TPC-DS evaluate our proposed framework in terms of energy consumption reduction, showing promising results."
pub.1120056693,Energy-Efficient Phase-Aware Load Balancing on Asymmetric Multicore Processors,"Comparing to the traditional symmetric design, single-ISA asymmetric multi-core processors were shown to deliver higher performance at lower energy costs. In such systems, load balancing is a challenging problem. Traditional algorithm is based on the load contribution prediction according to the task priority and the CPU utilization, ignoring the different characteristics of phases within a single task. Such approach might lead to suboptimized load balancing decisions, especially when concerning energy efficiency. In this paper, we propose a novel phase-aware load balancing method. Based on empirical data, proposed method could distinguish different execution phases. Comparing to HMP load balancing, the evaluation results show that the energy cost can be saved by 4% using the proposed approach while the CPU frequency is dynamically changed. If the CPU frequency is fixed, the energy cost can be saved by 10%."
pub.1093761357,An Instruction-Level Energy Estimation and Optimization Methodology for GPU,"Nowadays, GPU architecture is commonly exploited in various researches on computer graphic and other scientific computing areas. Parallel computing feature of GPU provides performance benefits for execution of many programs. However, as the parallel degree keeps extending, the number of active cores in GPU required for execution is also increasing. Therefore the rising of energy consumption caused by using large number of cores begins to draw attention. Previous research [1] reveals that given a multicore program, the curve of energy consumption first falls and then rises, as the number of active cores increases. That means we can have the minimum energy consumption if the number of active cores is properly configured. In this paper, we develop an instruction-level prediction mechanism to estimate the energy consumption of a given program under different numbers of cores. The prediction is based on the profile of Parallel Thread Execution (PTX) [2] codes generated during compilation of the original program. With the help of this mechanism, the energy-optimal number of cores can be found during compilation and used in execution, replacing the one given by programmer. Tests have been carried on several NVIDIA CUDA [10] benchmarks. The results show that the energy consumption is minimized without losing much performance. With the predicted energy-optimal number of active cores, we show that the energy consumption saving for the selected benchmarks is from 7.31% to 11.76% on average, with a worst case of performance lost 4.92%."
pub.1061539162,Learning Transfer-Based Adaptive Energy Minimization in Embedded Systems,"Embedded systems execute applications with varying performance requirements. These applications exercise the hardware differently depending on the computation task, generating varying workloads with time. Energy minimization with such workload and performance variations within (intra) and across (inter) applications is particularly challenging. To address this challenge, we propose an online approach, capable of minimizing energy through adaptation to these variations. At the core of this approach is a reinforcement learning algorithm that suitably selects the appropriate voltage/frequency scaling (VFS) based on workload predictions to meet the applications’ performance requirements. The adaptation is then facilitated and expedited through learning transfer, which uses the interaction between the application, runtime, and hardware layers to adjust the VFS. The proposed approach is implemented as a power governor in Linux and extensively validated on an ARM Cortex-A8 running different benchmark applications. We show that with intra- and inter-application variations, our proposed approach can effectively minimize energy consumption by up to 33% compared to the existing approaches. Scaling the approach to multicore systems, we also demonstrate that it can minimize energy by up to 18% with $2{\times }$ reduction in the learning time when compared with an existing approach."
pub.1130162386,"Euro-Par 2020: Parallel Processing, 26th International Conference on Parallel and Distributed Computing, Warsaw, Poland, August 24–28, 2020, Proceedings","This book constitutes the proceedings of the 26th International Conference on Parallel and Distributed Computing, Euro-Par 2020, held in Warsaw, Poland, in August 2020. The conference was held virtually due to the coronavirus pandemic. The 39 full papers presented in this volume were carefully reviewed and selected from 158 submissions. They deal with parallel and distributed computing in general, focusing on support tools and environments; performance and power modeling, prediction and evaluation; scheduling and load balancing; high performance architectures and compilers; data management, analytics and machine learning; cluster, cloud and edge computing; theory and algorithms for parallel and distributed processing; parallel and distributed programming, interfaces, and languages; multicore and manycore parallelism; parallel numerical methods and applications; and accelerator computing."
pub.1166173049,A scheduling algorithm based on critical factors for heterogeneous multicore processors,"Summary As the development of chip manufacturing technology slows down, high‐performance processors often have high energy consumption and high heat generation. Therefore, heterogeneous multi‐core processors become more and more popular, and the heterogeneous multi‐core processors is adopted to execute programs. At present, the general program consists of multiple threads. To reach goals of accelerating program execution and reducing energy consumption and heat generation of system, a suitable thread scheduling algorithm for heterogeneous multi‐core processors is needed. In this article, a thread scheduling algorithm based on multiple critical scheduling factors is proposed. First, a prediction model of thread performance and energy consumption is used to predict the core sensitivity of threads. Then, critical threads are judged and accelerated by collecting the synchronization information between threads. Finally, the load balancing method based on the computing power of cores and the core sensitivity of threads is employed to perform system load balancing, which ensures the fairness of the scheduling. Several experiments are provided, and the results show that the proposed algorithm can obtain better performance of thread schedule."
pub.1094288080,Using Network calculus for analysing wired-wireless Network on chip,"Chip designs integrate hundreds and even thousands of smaller cores on a single chip. As such, the design and implementation of the underlying communication fabric is becoming a critical challenge. Network-on-Chips (NoCs) is a novel design paradigm that replaces the traditional bus-based networks to overcome the dual problems of scalability and latency. However, traditional NoC topologies, such as meshes or toruses, are still limited by high latency and excess power dissipation. Wired-Wireless networks-on-chips hold substantial promise for enhancing multicore integrated circuit performance, by augmenting conventional wired interconnects. For the evaluation of these proposed solutions, Our research focuses on the analytical prediction. It presents a study of hybrid NoC using Network Calculus (NC) theory. We aggregate the individual temporal properties of each component given in switch model to obtain the delay bound. Second we apply theses formulas to calculate the maximum end-to-end delay. This helps to specify the best physical and logical characteristics that can achieve enhanced performances."
pub.1094120013,A sampling-based approach for communication libraries auto-tuning,"Communication performance is a critical issue in HPC applications, and many solutions have been proposed on the literature (algorithmic, protocols, etc.) In the meantime, computing nodes become massively multicore, leading to a real imbalance between the number of communication sources and the number of physical communication resources. Thus it is now mandatory to share network boards between computation flows, and to take this sharing into account while performing communication optimizations. In previous papers, we have proposed a model and a framework for on-the-fly optimizations of multiplexed concurrent communication flows, and implemented this model in the Newmadeleine communication library. This library features optimization strategies able for example to aggregate several messages to reduce the number of packets emitted on the network, or to split messages to use several NICs at the same time. In this paper, we study the tuning of these dynamic optimization strategies. We show that some parameters and thresholds (rendezvous threshold, aggregation packet size) depend on the actual hardware, both host and NICs. We propose and implement a method based on sampling of the actual hardware to auto-tune our strategies. Moreover, we show that multi-rail can greatly benefit from performance predictions. We propose an approach for multi-rail that dynamically balance the data between NICs using predictions based on sampling."
pub.1095813092,Using an analytical model of shared caches for selecting the optimal parallelization scheme,"Multicores are now the norm. Their cache hierarchy has often a last level shared cache. The performance of this shared cache during the execution of multithreaded applications depends on the parallelization scheme followed. For example, critical parameters for the performance of parallelized loops are the number of threads and the block size. The selection of the optimal scheme in a compiler can be guided using heuristics or the execution time. Heuristics can be imprecise, while an execution time guided search is very time-consuming. This paper shows the usage of an analytical model to predict the cache behavior of shared caches during the execution of multithreaded applications that have been parallelized at loop level. The model predicts the number of misses generated by a given code when different number of threads or block sizes are used. The execution time of the codes analyzed is highly correlated to the number of misses generated in the shared cache, thus, the prediction of the model is a powerful tool to select the best parallelization scheme for them."
pub.1094820928,Representative Multiprogram Workloads for Multithreaded Processor Simulation,"Almost all new consumer-grade processors are capable of executing multiple programs simultaneously. The analysis of multiprogrammed workloads for multicore and SMT processors is challenging and time-consuming because there are many possible combinations of benchmarks to execute and each combination may exhibit several different interesting behaviors. Missing particular combinations of program behaviors could hide performance problems with designs. It is thus of utmost importance to have a representative multiprogrammed workload when evaluating multithreaded processor designs. This paper presents a methodology that uses phase analysis, principal components analysis (PCA) and cluster analysis (CA) applied to microarchitecture-independent program characteristics in order to find important program interactions in multiprogrammed workloads. The end result is a small set of co-phases with associated weights that are representative for a multiprogrammed workload across multithreaded processor architectures. Applying our methodology to the SPEC CPU 2000 benchmark suite yields 50 distinct combinations for two-context multithreaded processor simulation that researchers and architects can use for simulation. Each combination is simulated for 50 million instructions, giving a total of 2.5 billion instructions to be simulated for the SPEC CPU2000 benchmark suite. The performance prediction error with these representative combinations is under 2.5% of the real workload for absolute throughput prediction and can be used to make relative throughput comparisons across processor architectures."
pub.1061480617,Future Microprocessors and Off-Chip SOP Interconnect,"Limits to chip power dissipation and power density and limits on the benefits of hyperpipelining in microprocessors threaten to stop the exponential performance growth in microprocessor performance that we have grown accustomed to. Multicore processors can continue to provide historical performance growth on most modern consumer and business applications. However, power efficiency of these cores must also be improved to stay within reasonable power budgets. This can be achieved by simplifying the processor core architecture, and reversing the trend toward ever more-complex and less power-efficient cores. To maintain overall performance growth with stunted per-core and per-thread performance, growth rates will require an even more rapid increase in the number of cores per die. Growing performance by increasing the number of cores on a die at this rate, however, puts unprecedented requirements on the corresponding growth of off-chip bandwidth. We argue that contrary to the International Roadmap for Semiconductors (ITRS) predictions, off-chip signaling frequencies are likely to exceed the frequencies of processor cores in the not too distant future, consistent with the system-on-package (SOP) concept in the first paper of this issue. If this approach is followed, a 1-TFlops multiprocessor die with 1 TB/s of off-chip bandwidth is feasible at reasonable cost before the end of the decade."
pub.1086750846,A Framework for Out of Memory SVD Algorithms,"Many important applications – from big data analytics to information retrieval, gene expression analysis, and numerical weather prediction – require the solution of large dense singular value decompositions (SVD). In many cases the problems are too large to fit into the computer’s main memory, and thus require specialized out-of-core algorithms that use disk storage. In this paper, we analyze the SVD communications, as related to hierarchical memories, and design a class of algorithms that minimizes them. This class includes out-of-core SVDs but can also be applied between other consecutive levels of the memory hierarchy, e.g., GPU SVD using the CPU memory for large problems. We call these out-of-memory (OOM) algorithms. To design OOM SVDs, we first study the communications for both classical one-stage blocked SVD and two-stage tiled SVD. We present the theoretical analysis and strategies to design, as well as implement, these communication avoiding OOM SVD algorithms. We show performance results for multicore architecture that illustrate our theoretical findings and match our performance models."
pub.1043198467,Delayed-Dynamic-Selective (DDS) Prediction for Reducing Extreme Tail Latency in Web Search,"A commercial web search engine shards its index among many servers, and therefore the response time of a search query is dominated by the slowest server that processes the query. Prior approaches target improving responsiveness by reducing the tail latency of an individual search server. They predict query execution time, and if a query is predicted to be long-running, it runs in parallel, otherwise it runs sequentially. These approaches are, however, not accurate enough for reducing a high tail latency when responses are aggregated from many servers because this requires each server to reduce a substantially higher tail latency (e.g., the 99.99th-percentile), which we call extreme tail latency. We propose a prediction framework to reduce the extreme tail latency of search servers. The framework has a unique set of characteristics to predict long-running queries with high recall and improved precision. Specifically, prediction is delayed by a short duration to allow many short-running queries to complete without parallelization, and to allow the predictor to collect a set of dynamic features using runtime information. These features estimate query execution time with high accuracy. We also use them to estimate the prediction errors to override an uncertain prediction by selectively accelerating the query for a higher recall. We evaluate the proposed prediction framework to improve search engine performance in two scenarios using a simulation study: (1) query parallelization on a multicore processor, and (2) query scheduling on a heterogeneous processor. The results show that, for both scenarios, the proposed framework is effective in reducing the extreme tail latency compared to a start-of-the-art predictor because of its higher recall, and it improves server throughput by more than 70% because of its improved precision."
pub.1182138562,Exploring Machine Learning Approaches for QoS Prediction on SMT Processors,"Improving efficiency in large-scale computing environments involves optimizing server usage while ensuring that latency-sensitive applications maintain their quality of service (QoS). While there has been research on maintaining QoS for high-performance threads (HPT) in multicore systems, simultaneous multithreading processors present various challenges for QoS. These challenges increase significantly as the number of threads increases, due to the complexity of maintaining QoS in shared resources such as instruction pipelines, execution units, and cache. To meet the QoS requirements of HPTs, monitoring and dynamic allocation of these resources are necessary. However, privacy concerns may arise if monitoring is needed, as it could lead to data leaks and side-channel attacks. This study compares different machine learning approaches to provide a more straightforward way of predicting resource utilization and allocating resources. We also consider the situations when the HPT thread should protected against side-channel attacks. We selected one of the heuristic algorithms from the previous work and prepared data to build different models. We have 89% prediction accuracy for the decision tree regression model and 93% for XGB classification models, where we can also provide a secure environment for the HPT against side-channel attacks."
pub.1020518007,An Energy-Efficient Dual-Level Cache Architecture for Chip Multiprocessors,"As microprocessors begin to leverage multicore functionality, the power consumption incurred from tag comparison in a cache hierarchy of chip multiprocessors (CMPs) becomes greater. In this paper, a novel dual-level cache architecture is explored to reduce tag comparisons for mitigating power overhead. For one thing, a way-tagged L1 cache is adopted to access the L2 cache as a direct-mapping method during the write hits. Moreover, a combined multistep method is adopted to further reduce the L2 tag comparison with respect to both the cache hit and the miss predictions. With a simple predictor and the coherence status, a new prediction scheme for backward invalidation is proposed to compensate for the limitation of the two applied solutions in CMPs. Furthermore, a linear feedback shift register counter is exploited to replace traditional predictors to realize and optimize the proposed structure. Simulation results show that the proposed technique can reduce the total cache power consumption by an average of 49.7 % at the cost of acceptable performance degradation."
pub.1123516628,Wireless Network On-Chips History-Based Traffic Prediction for Token Flow Control and Allocation,"Wireless network-on-chip (WiNoC) uses a wireless backbone on top of the traditional wired-based NoC which demonstrated high scalability. WiNoC introduces long-range single-hop link connecting distanced core and high bandwidth radio frequency interconnects that reduces multi-hop communication in conventional wired-based NoC. However, to ensure full benefits of WiNoC technology, there is a need for fair and efficient Medium Access Control (MAC) mechanism to enhance communication in the wireless Network-on-Chip. To adapt to the varying traffic demands from the applications running on a multicore environment, MAC mechanisms should dynamically adjust the transmission slots of the wireless interfaces (WIs), to ensure efficient utilization of the wireless medium in a WiNoC. This work presents a prediction model that improves MAC mechanism to predict the traffic demand of the WIs and respond accordingly by adjusting transmission slots of the WIs. This research aims to reduce token waiting time and inefficient decision making for radio hub-to-hub communication and congestion-aware routing in WiNoC to enhance end to end latency. Through system level simulation, we will show that the dynamic MAC using an History-based prediction mechanism can significantly improve the performance of a WiNoC in terms of latency and network throughput compared to the state-of-the-art dynamic MAC mechanisms."
pub.1062871235,Modeling the Performance of Geometric Multigrid Stencils on Multicore Computer Architectures,"The basic building blocks of the classic geometric multigrid algorithm all have a low ratio of executed floating point operations per byte fetched from memory. On modern computer architectures, such computational kernels are typically bound by memory traffic and achieve only a small percentage of the theoretical peak floating point performance of the underlying hardware. We suggest the use of state-of-the-art (stencil) compiler techniques to improve the flop per byte ratio, also called the arithmetic intensity, of the steps in the algorithm. Our focus will be on the smoother which is a repeated stencil application. With a tiling approach based on the polyhedral loop optimization framework, data reuse in the smoother can be improved, leading to a higher effective arithmetic intensity. For an academic constant coefficient Poisson problem, we present a performance model for the multigrid $V$-cycle solver based on the tiled smoother. For increasing numbers of smoothing steps, there is a trade-off between the improved efficiency due to better data reuse and the additional flops required for extra smoothing steps. Our performance model predicts time to solution by linking convergence rate to arithmetic intensity via the roofline model. We show results for two-dimensional (2D) and three-dimensional (3D) simulations on Intel Sandy Bridge and for 2D simulations on Intel Xeon Phi architectures. The actual performance is compared with the theoretical predictions."
pub.1025828200,Chapter 2 Pitfalls and Issues of Manycore Programming,"The transition from sequential computing to parallel computing represents the next turning point in the way software engineers design and write software. The addition of more processing cores has emerged as the primary way to boost the computing power of microprocessors, but first the research community has to overcome certain hardware and software challenges along the way to on-chip scalable systems.The primary consequence is that applications will increasingly need to be parallelized to fully exploit processor throughput gains that are now becoming available. However, parallel code is more difficult to write than that of serial code. Writing applications in a way that permits different parts of a computing task be divided up and executed simultaneously across multiple cores is not new. Efforts today focus on translating the knowledge of building off-chip supercomputing based on multiprocessors to on-chip hypercomputing based on multicore processors. While the software may present the biggest challenge, there are also hardware changes that need to be made to overcome issues such as scalability and portability.The aim of this chapter is to explain the primary difficulties and issues of manycore programming. Firstly, the unsolved problem of the parallel computation model and its implications for issues such as portability and performance prediction is discussed. Secondly, the obstacles incurred by parallel hardware architectures on software development are covered. Thirdly, the main traps and pitfalls of multicore programming are addressed. Finally, the human factor in the success of the parallel revolution is presented."
pub.1150640923,Evaluation of the Intel thread director technology on an Alder Lake processor,"Asymmetric multicore processors (AMPs) combine high-performance big cores with more energy-efficient small cores, all exposing a shared instruction-set architecture but different features, such as clock frequency or microarchitecture. In the last decade, most commercial AMP products have mainly targetted the embedded and mobile domains. Today, major hardware players are releasing new AMP-based products that aim to move beyond the mobile niche, towards the desktop/server segments. The Apple M1 SoC or the recent Intel Alder Lake processor family are clear examples of these new AMP systems. Despite their energy-efficiency benefits, AMPs pose significant challenges to the operating system scheduler. In this paper, we assess the effectiveness of the Thread Director (TD) technology, a set of hardware facillities - first introduced in Alder Lake processors - that provide the OS with hints on the performance and energy efficiency that a thread delivers when running on the various core types. The main focus of our analysis is to evaluate how effectively the OS can drive scheduling decisions with TD's performance hints. To this end, we incorporated support in Linux to conveniently access TD facillites from the OS kernel. Motivated by various TD's limitations identified with our analysis, we opted to build hardware-counter based prediction models (generated via machine-learning methods) to better aid the OS in making throughput-oriented and fairness-aware scheduling decisions. The effectiveness of both TD and the hardware-counter based models for performance prediction is evaluated both via offline monitoring, and also online, by utilizing our implementation of various asymmetry-aware schedulers in the Linux kernel."
pub.1049183077,Chapter 2 Understanding Application Contentiousness and Sensitivity on Modern Multicores,"Runtime systems to mitigate memory resource contention problems on multicore processors have recently attracted much research attention. One critical component of these runtimes is the indicators to rank and classify applications based on their contention characteristics. However, although there has been significant research effort, application contention characteristics remain not well understood and indicators have not been thoroughly evaluated.In this chapter, we performed a thorough study of applications’ contention characteristics to develop better indicators to improve contention-aware runtime systems. The contention characteristics are composed of an application’s contentiousness, and its sensitivity to contention. We show that contentiousness and sensitivity are not strongly correlated, and contrary to prior wisdom, a single indicator is not adequate to predict both. Also, while prior wisdom has relied on last level cache miss rate as one of the best indicators to predict an application’s contention characteristics, we show that depending on the workloads, it can often be misleading. We then present prediction models that consider contention in various memory resources. Our regression analysis establishes an accurate model to predict application contentiousness. The analysis also demonstrates that performance counters alone may not be sufficient to accurately predict application sensitivity to contention. In this chapter, we also present an evaluation using SPEC CPU2006 benchmarks showing that, when predicting an application’s contentiousness, the linear correlation coefficient R2 of our predictor and the real measured contentiousness is 0.834, as opposed to 0.224 when using last level cache miss rate."
pub.1051819594,SYRANT: SYmmetric resource allocation on not-taken and taken paths,"In the multicore era, achieving ultimate single process performance is still an issue e.g. for single process workload or for sequential sections in parallel applications. Unfortunately, despite tremendous research effort on branch prediction, substantial performance potential is still wasted due to branch mispredictions. On a branch misprediction resolution, instruction treatment on the wrong path is essentially thrown away. However, in most cases after a conditional branch, the taken and the not-taken paths of execution merge after a few instructions. Instructions that follow the reconvergence point are executed whatever the branch outcome is.
                  We present SYRANT (SYmmetric Resource Allocation on Not-taken and Taken paths), a new technique for exploiting control independence. SYRANT essentially uses the same pipeline structure as a conventional processor. SYRANT tries to enforce the allocation of the exact same resources on the out-of-order execution mechanism (physical register, load/store queue and reorder buffer) for both the taken and not-taken paths. Thus, on a branch misprediction, the result of an instruction already executed on the wrong path after the reconvergence point can be conserved in the same structure when it is data independent. Adding SYRANT on top of an aggressive superscalar execution core allows to improve performance for applications suffering a significant branch misprediction rate.
                  As a side, but important extra contribution, we introduce ABL/SBL a simple and non-intrusive hardware reconvergence detection mechanism. ABL/SBL can be used in a conventional superscalar processor to improve branch prediction accuracy by exploiting the execution of branches along the wrong path."
pub.1129856263,A Comparative Study of Techniques for Energy Predictive Modeling Using Performance Monitoring Counters on Modern Multicore CPUs,"Accurate and reliable measurement of energy consumption is essential to energy optimization at an application level. Energy predictive modelling using performance monitoring counters (PMCs) emerged as a promising approach, one of the main drivers being its capacity to provide fine-grained component-level breakdown of energy consumption. In this work, we compare two types of energy predictive models constructed from the same set of experimental data and at two levels, platform and application. The first type contains linear regression (LR) models employing PMCs selected using a theoretical model of energy of computing. The second type contains sophisticated statistical learning models, random forest (RF) and neural network (NN), that are based on PMCs selected using correlation and principal component analysis. Our experimental results performed on two modern Intel multicore processors using a diverse set of applications and a wide range of application configurations, show that the average proportional prediction accuracy of platform-level LR models is $5.09\times $ and $4.37\times $ times better than the platform-level RF and NN models. We also present an experimental methodology to select a reliable subset of four PMCs for constructing accurate application-specific online models. Using the methodology, we demonstrate that LR models perform $1.57\times $ and $1.74\times $ times better than RF and NN models. The consistent accuracy of LR models stress the importance of taking into account domain-specific knowledge for model variable selection, in this case, the physical significance of the PMCs originating from the conservation of energy of computing. The results also endorse the guidelines of the theory of energy of computing, which states that any non-linear energy model (in this case, the RF and NN models) employing PMCs only, will be inconsistent and hence inherently inaccurate."
pub.1131110742,GPUOPT,"On-chip photonics is a disruptive technology, and such NoCs are superior to traditional electrical NoCs in terms of latency, power, and bandwidth. Hence, researchers have proposed a wide variety of optical networks for multicore processors. The high bandwidth and low latency features of photonic NoCs have led to the overall improvement in the system performance. However, there are very few proposals that discuss the usage of optical interconnects in Graphics Processor Units (GPUs). GPUs can also substantially gain from such novel technologies, because they need to provide significant computational throughput without further stressing their power budgets.  The main shortcoming of optical networks is their high static power usage, because the lasers are turned on all the time by default, even when there is no traffic inside the chip, and thus sophisticated laser modulation schemes are required. Such modulation schemes base their decisions on an accurate prediction of network traffic in the future. In this article, we propose an energy-efficient and scalable optical interconnect for modern GPUs called GPUOPT that smartly creates an overlay network by dividing the symmetric multiprocessors (SMs) into clusters. It furthermore has separate sub-networks for coherence and non-coherence traffic. To further increase the throughput, we connect the off-chip memory with optical links as well.  Subsequently, we show that traditional laser modulation schemes (for reducing static power consumption) that were designed for multicore processors are not that effective for GPUs. Hence, there was a need to create a bespoke scheme for predicting the laser power usage in GPUs.  Using this set of techniques, we were able to improve the performance of a modern GPU by 45% as compared to a state-of-the-art electrical NoC. Moreover, as compared to competing optical NoCs for GPUs, our scheme reduces the laser power consumption by 67%, resulting in a net 65% reduction in ED 2 for a suite of Rodinia benchmarks. "
pub.1094007071,Energy Minimization on Thread-Level Speculation in Multicore Systems,"Thread-Level Speculation (TLS) has shown great promise as an automatic parallelization technique to achieve high level performance by partitioning a sequential program into threads, which are expected to be optimistically executed in parallel. In this paper, we propose a load-balancing approach to save energy using dynamic voltage scaling. By scaling the voltage of processors running short threads, energy consumption on these processors can be reduced while keeping a similar speedup of the overall system. Two voltage selection strategies have been investigated. With the assistance of some profiling tools, we propose a static voltage selection algorithm that can minimize energy consumption without degrading the parallelism provided by the pure TLS. The other dynamic algorithm selects voltage for each thread with prediction during the execution. Our experimental results show that its energy consumption is reduced to 78.8% and execution time is stretched to 1.07 times, on average, of the pure TLS in a 16-core CMP processor."
pub.1094349539,Enhancing Model-Based Architecture Optimization with Monitored System Runs,"Typically, architecture optimization searches for good architecture candidates based on analyzing a model of the system. Model-based analysis inherently relies on abstractions and estimates, and as such produces approximations which are used to compare architecture candidates. However, approximations are often not sufficient due to the difficulty of accurately estimating certain extra-functional properties. In this paper, we present an architecture optimization approach where the speed of model-based optimization is combined with the accuracy of monitored system runs. Model-based optimization is used to quickly find a good architecture candidate, while optimization based on monitored system runs further refines this candidate. U sing measurements assures a higher accuracy of the metrics used for optimization compared to using performance predictions. We demonstrate the feasibility of the approach by implementing it in our framework for optimizing the allocation of software tasks to the processing cores of a multicore embedded system."
pub.1061754553,RC-Based Temperature Prediction Scheme for Proactive Dynamic Thermal Management in Throttle-Based 3D NoCs,"The three-dimensional Network-on-Chip (3D NoC) has been proposed to solve the complex on-chip communication issues in multicore systems using die stacking in recent days. Because of the larger power density and the heterogeneous thermal conductance in different silicon layers of 3D NoC, the thermal problems of 3D NoC become more exacerbated than that of 2D NoC and become a major design constraint for a high-performance system. To control the system temperature under a certain thermal limit, many Dynamic Thermal Managements (DTMs) have been proposed. Recently, for emergent cooling, the full throttling scheme is usually employed as the system temperature reaches the alarming level. Hence, the conventional reactive DTM suffers from significant performance impact because of the pessimistic reaction. In this paper, we propose a throttle-based proactive DTM(T-PDTM) scheme to predict the future temperature through a new Thermal RC-based temperature prediction (RCTP) model. The RCTP model can precisely predict the temperature with heterogeneous workload assignment with low constant computational complexity. Based on the predictive temperature, the proposed T-PDTM scheme will assign the suitable clock frequency for each node of the NoC system to perform early temperature control through power budget distribution. Based on the experimental results, compared with the conventional reactive throttled-based DTMs, the T-PDTM scheme can help to reduce 11.4~80.3 percent fully throttled nodes and improves the network throughput by around 1.5~211.8 percent."
pub.1150376282,Gaussian Process Regression with Grid Spectral Mixture Kernel: Distributed Learning for Multidimensional Data,"Kernel design for Gaussian processes (GPs) along with the associated hyper-parameter optimization is a challenging problem. In this paper, we propose a novel grid spectral mixture (GSM) kernel design for GPs that can automatically fit multidimensional data with affordable model complexity and superior modeling capability. To alleviate the computational complexity due to the curse of dimensionality, we leverage a multicore computing environment to optimize the kernel hyper-parameters in a distributed manner. We further propose a doubly distributed learning algorithm based on the alternating direction method of multipliers (ADMM) which enables multiple agents to learn the kernel hyper-parameters collaboratively. The doubly distributed learning algorithm is shown to be effective in reducing the overall computational complexity while preserving data privacy during the learning process. Experiments on various one-dimensional and multidimensional data sets demonstrate that the proposed kernel design yields superior training and prediction performance compared to its competitors."
pub.1094413972,Cache Coherence Method for Improving Multi-Threaded Applications on Multicore Systems,"Chip-multiprocessors (CMPs) have become the mainstream parallel architecture in enterprise and scientific computing facilities. For scalability reasons, design with larger core counts tends towards with physically distributed hardware caches. This naturally results in a Non-Uniform Cache Access design, where data movement and management impacts access latency and consume power. In this work, we observed that shared data writing behavior dramatically wastes precious on-chip hardware cache resource and seriously affects the whole system performance due to the high remote access latency. Therefore, we propose a new prediction mechanism to predict the impact of shared data and a directory-based MESI cache coherence protocol with selective write-shared-data-update transition strategy instead of native write-invalidate strategy. We evaluate our proposal on a modern multi-core machine with NAS Parallel Benchmarks. Experimental results showed speedup gains of up to 21% opposed to the native write-invalidate transition strategy."
pub.1095650661,Energy Efficient Last Level Caches via Last Read/Write Prediction,"The size of the Last Level Caches (LLC) in multicore architectures is increasing, and so is their power consumption. However, most of this power is wasted on unused or invalid cache lines. For dirty cache lines, the LLC waits until the line is evicted to be written back to memory. Hence, dirty lines compete for the memory bandwidth with read requests (prefetch and demand), increasing pressure on the memory controller. This paper proposes a Dead Line and Early Write-Back Predictor (DEWP) to improve the energy efficiency of the LLC. DEWP early evicts dead cache lines with an average accuracy of 94 %, and only 2% false positives. DEWP also allows scheduling of dirty lines for early eviction, allowing earlier write-backs. Using DEWP over a set of single and multi-threaded benchmarks, we obtain an average of 61 % static energy savings, while maintaining the performance, for both inclusive and non-inclusive LLCs."
pub.1103543168,DCAPS,"In a multicore system, effective management of shared last level cache (LLC), such as hardware/software cache partitioning, has attracted significant research attention. Some eminent progress is that Intel introduced Cache Allocation Technology (CAT) to its commodity processors recently. CAT implements way partitioning and provides software interface to control cache allocation. Unfortunately, CAT can only allocate at way level, which does not scale well for a large thread or program count to serve their various performance goals effectively. This paper proposes Dynamic Cache Allocation with Partial Sharing (DCAPS), a framework that dynamically monitors and predicts a multi-programmed workload's cache demand, and reallocates LLC given a performance target. Further, DCAPS explores partial sharing of a cache partition among programs and thus practically achieves cache allocation at a finer granularity. DCAPS consists of three parts: (1) Online Practical Miss Rate Curve (OPMRC), a low-overhead software technique to predict online miss rate curves (MRCs) of individual programs of a workload; (2) a prediction model that estimates the LLC occupancy of each individual program under any CAT allocation scheme; (3) a simulated annealing algorithm that searches for a near-optimal CAT scheme given a specific performance goal. Our experimental results show that DCAPS is able to optimize for a wide range of performance targets and can scale to a large core count."
pub.1147931727,Julia Cloud Matrix Machine: Dynamic Matrix Language Acceleration on Multicore Clusters in the Cloud,"In emerging scientific computing environments, matrix computations of
increasing size and complexity are increasingly becoming prevalent. However,
contemporary matrix language implementations are insufficient in their support
for efficient utilization of cloud computing resources, particularly on the
user side. We thus developed an extension of the Julia high-performance
computation language such that matrix computations are automatically
parallelized in the cloud, where users are separated from directly interacting
with complex explicitly-parallel computations. We implement lazy evaluation
semantics combined with directed graphs to optimize matrix operations on the
fly while dynamic simulation finds the optimal tile size and schedule for a
given cluster of cloud nodes. A time model prediction of the cluster's
performance capacity is constructed to enable simulations. Automatic
configuration of communication and worker processes on the cloud networks allow
for the framework to automatically scale up for clusters of heterogeneous
nodes. Our framework's experimental evaluation comprises eleven benchmarks on
an fourteen node (564 CPUs) cluster in the AWS public cloud, revealing speedups
of up to a factor of 5.1, with an average 74.39% of the upper bound for
speedups."
pub.1120395505,"Euro-Par 2019: Parallel Processing, 25th International Conference on Parallel and Distributed Computing, Göttingen, Germany, August 26–30, 2019, Proceedings","This book constitutes the proceedings of the 25th International Conference on Parallel and Distributed Computing, Euro-Par 2019, held in Göttingen, Germany, in August 2019. The 36 full papers presented in this volume were carefully reviewed and selected from 142 submissions. They deal with parallel and distributed computing in general, focusing on support tools and environments; performance and power modeling, prediction and evaluation; scheduling and load balancing; high performance architectures and compilers; data management, analytics and deep learning; cluster and cloud computing; distributed systems and algorithms; parallel and distributed programming, interfaces, and languages; multicore and manycore parallelism; theory and algorithms for parallel computation and networking; parallel numerical methods and applications; accelerator computing; algorithms and systems for bioinformatics; and algorithms and systems for digital humanities."
pub.1109002346,"Euro-Par 2017: Parallel Processing, 23rd International Conference on Parallel and Distributed Computing, Santiago de Compostela, Spain, August 28 – September 1, 2017, Proceedings","This book constitutes the proceedings of the 23rd International Conference on Parallel and Distributed Computing, Euro-Par 2017, held in Santiago de Compostela, Spain, in August/September 2017. The 50 revised full papers presented together with 2 abstract of invited talks and 1 invited paper were carefully reviewed and selected from 176 submissions. The papers are organized in the following topical sections: support tools and environments; performance and power modeling, prediction and evaluation; scheduling and load balancing; high performance architectures and compilers; parallel and distributed data management and analytics; cluster and cloud computing; distributed systems and algorithms; parallel and distributed programming, interfaces and languages; multicore and manycore parallelism; theory and algorithms for parallel computation and networking; prallel numerical methods and applications; and accelerator computing."
pub.1095840557,Out of Memory SVD Solver for Big Data,"Many applications - from data compression to numerical weather prediction and information retrieval - need to compute large dense singular value decompositions (SVD). When the problems are too large to fit into the computer's main memory, specialized out-of-core algorithms that use disk storage are required. A typical example is when trying to analyze a large data set through tools like MATLAB or Octave, but the data is just too large to be loaded. To overcome this, we designed a class of out-of-memory (OOM) algorithms to reduce, as well as overlap communication with computation. Of particular interest is OOM algorithms for matrices of size m × n, where m >> n or m << n, e.g., corresponding to cases of too many variables, or too many observations. To design OOM SVDs, we first study the communications cost for the SVD techniques as well as for the QR/LQ factorization followed by SVD. We present the theoretical analysis about the data movement cost and strategies to design OOM SVD algorithms. We show performance results for multicore architecture that illustrate our theoretical findings and match our performance models. Moreover, our experimental results show the feasibility and superiority of the OOM SVD."
pub.1093981883,Using the Application Signature to Detect Inefficiencies Generated by Mapping Policies in Parallel Applications,"The execution of HPC applications in multicore environments can occasionally use the resources in an inefficient way. There are idle times during the application execution that can be caused by synchronization or message passing collisions. We define this idle time as an application inefficiency and may be caused by the message passing collisions at different types of interconnections in the compute nodes. We propose a methodology to characterize the application's execution in order to analyze and detect these inefficiencies in a bounded time as well as to locate on which parallel segments of the application code (phases) these inefficiencies are generated. The parallel segments of code (phases) represent the most relevant application behavior and are obtained by the application's characterization using the PAS2P tool. The tool allows us to predict the execution time by the generation of the application signature, which is composed of phases. Taking advantage of the prediction quality and the time to obtain the prediction of application performance, we propose modeling the factors that potentially influence the application's execution time, especially characterizing the behavior during the execution time of these phases. We performed experimental validation using signatures of NAS Parallel benchmarks in order to detect and model the inefficiencies in the application phases."
pub.1128969694,Haptic Data Accelerated Prediction via Multicore Implementation,"The next generation of 5G wireless communications will provide very high data-rates combined with low latency. Several applications in different research fields are planning to exploit 5G wireless communications to achieve benefits for own aims. Mainly, Teleoperations Systems (TS) expect to use this technology in order to obtain very advantages for own architecture. The goal of these systems, as implied by their name, is to provide to the user (human operator) the feeling of presence in the remote environment where the teleoperator (robot) exists. This aim can be achieved thanks to the exchange of a thousand or more haptic data packets per second to be transmitted between the master and the slave devices. Since Teleoperation Systems are very sensitive to delays and data loss, TS challenge is to obtain low latency and high reliability in order to improve Quality of Experience (QoE) for the users in real-time communications. For this reason a data compression and reduction are required to ensure good system stability. A Predictive-Perceptive compression model based on prediction error and human psychophysical limits has been adopted to reduce data size. However, the big amount of Haptic Data to be processed requires very large times to execute their compression. Due to this issue a parallel strategy and implementation have been proposed with related experimental results to confirm the gain of performance in time terms."
pub.1160003257,Approx-RM: Reducing Energy on Heterogeneous Multicore Processors under Accuracy and Timing Constraints," Reducing energy consumption while providing performance and quality guarantees is crucial for computing systems ranging from battery-powered embedded systems to data centers. This article considers approximate iterative applications executing on heterogeneous multi-core platforms under user-specified performance and quality targets. We note that allowing a slight yet bounded relaxation in solution quality can considerably reduce the required iteration count and thereby can save significant amounts of energy. To this end, this article proposes Approx-RM , a resource management scheme that reduces energy expenditure while guaranteeing a specified performance as well as accuracy target. Approx-RM predicts the number of iterations required to meet the relaxed accuracy target at runtime. The time saved generates execution-time slack, which allows Approx-RM to allocate fewer resources on a heterogeneous multi-core platform in terms of DVFS, core type, and core count to save energy while meeting the performance target. Approx-RM contributes with lightweight methods for predicting the iteration count needed to meet the accuracy target and the resources needed to meet the performance target. Approx-RM uses the aforementioned predictions to allocate just enough resources to comply with quality of service constraints to save energy. Our evaluation shows energy savings of 31.6%, on average, compared to Race-to-idle when the accuracy is only relaxed by 1%. Approx-RM incurs timing and energy overheads of less than 0.1%. "
pub.1018753030,Efficient performance of the Met Office Unified Model v8.2 on Intel Xeon partially used nodes,"The atmospheric Unified Model (UM) developed at the UK Met Office is used for weather and climate prediction by forecast teams at a number of international meteorological centres and research institutes on a wide variety of hardware and software environments. Over its 25 year history the UM sources have been optimised for a better application performance on a number of HPC systems including NEC SX vector architecture systems and recently the IBM Power6/Power7 platforms. Understanding the influence of the compiler flags, MPI libraries and run configurations is crucial to achieving the shortest elapsed times for a UM application on any particular HPC system. These aspects are very important for applications that must run within operational time frames. Driving the current study is the HPC industry trend since 1980 for processor arithmetic performance to increase at a faster rate than memory bandwidth. This gap has been growing especially fast for multicore processors in the past 10 years and it can have significant implication for the performance and performance scaling of memory bandwidth intensive applications, such as the UM. Analysis of partially used nodes on Intel Xeon clusters is provided in this paper for short and medium range weather forecasting systems using global and limited-area configurations. It is shown that on the Intel Xeon based clusters the fastest elapsed times and the most efficient system usage can be achieved using partially committed nodes."
pub.1021332072,Efficient performance of the Met Office Unified Model v8.2 on Intel Xeon partially used nodes,"Abstract. The atmospheric Unified Model (UM) developed at the UK Met Office is used for weather and climate prediction by forecast teams at a number of international meteorological centres and research institutes on a wide variety of hardware and software environments. Over its 25 year history the UM sources have been optimised for better application performance on a number of High Performance Computing (HPC) systems including NEC SX vector architecture systems and recently the IBM Power6/Power7 platforms. Understanding the influence of the compiler flags, Message Passing Interface (MPI) libraries and run configurations is crucial to achieving the shortest elapsed times for a UM application on any particular HPC system. These aspects are very important for applications that must run within operational time frames. Driving the current study is the HPC industry trend since 1980 for processor arithmetic performance to increase at a faster rate than memory bandwidth. This gap has been growing especially fast for multicore processors in the past 10 years and it can have significant implication for the performance and performance scaling of memory bandwidth intensive applications, such as the UM. Analysis of partially used nodes on Intel Xeon clusters is provided in this paper for short- and medium-range weather forecasting systems using global and limited-area configurations. It is shown that on the Intel Xeon-based clusters the fastest elapsed times and the most efficient system usage can be achieved using partially committed nodes."
pub.1093769965,Using a Polymorphic VLIW Processor to Improve Schedulability and Performance for Mixed-Criticality Systems,"As embedded systems are faced with ever more demanding workloads and more tasks are being consolidated onto a smaller number of microcontrollers, system designers are faced with opposing requirements of increasing performance while retaining real-time analyzability. For example, one can think of the following performance-enhancing techniques: caches, branch prediction, out-of-order (OoO) superscalar processing, simultaneous multi-threading (SMT). Clearly, there is a need for a platform that can deliver high performance for non-critical tasks and full analyzability and predictability for critical tasks (mixed-criticality systems). In this paper, we demonstrate how a polymorphic VLIW processor can satisfy these seemingly contradicting goals by allowing a schedule to dynamically, i.e., at run-time, distribute the computing resources to one or multiple threads. The core provides full performance isolation between threads and can keep multiple task contexts in hardware (virtual processors, similar to SMT) between which it can switch with minimal penalty (a pipeline flush). In this work, we show that this dynamic platform can improve performance over current predictable processors (by a factor of 5 on average using the highest performing configuration), and provides schedulability that is on par with an earlier study that explored the concept of creating a dynamic processor based on a superscalar architecture. Furthermore, we measured a 15% improvement in schedulability over a heterogeneous multicore platform with an equal number of datapaths. Finally, our VHDL design and tools (including compiler, simulator, libraries etc.) are available for download for the academic community."
pub.1170401052,GPU Scale-Model Simulation,"The continuously increasing GPU system scale and compute capabilities, i.e., increasing number of streaming multiprocessors (SMs), caches, on-chip and off-chip memory bandwidth, pose a major challenge for performance evaluation methodologies. Architectural simulation is time-consuming and resource-intensive, and because of simulator and/or simulation host infrastructure limitations, it might not even be possible to simulate large-scale systems. Scale-model simulation is a recently proposed performance prediction methodology to predict large-scale system performance based on (much smaller) scale models. Prior work in scale-model simulation for general-purpose multicore CPUs and specialized graph analytics accelerators, unfortunately, cannot be readily applied to GPUs because different GPU applications exhibit vastly different scaling behavior with system size, thereby breaking the one-size-fits-all regression models deployed in prior work. This paper proposes a GPU scale-model simulation methodology that leverages performance measurements of two scale models alongside a miss rate curve to predict GPU target system performance. A key asset of GPU scale-model simulation is that it does not require access to a simulation model of the target system, unlike prior work in simulation acceleration. Our experimental evaluation demonstrates the accuracy of GPU scale-model simulation for both strong-scaling and weak-scaling workload scenarios. Under strong scaling, the performance of a 128-SM target system is predicted within 4% error on average, and at most 17%, using 8-SM and 16-SM scale models. Under weak scaling, the performance of a 128-SM target system is estimated with an average error of 1.7%, and at most 4.5%, while yielding a 9.3× simulation time speedup. We furthermore demonstrate how scale-model simulation predicts multi-chiplet GPU performance with an average error of 2.5% (and at most 4.3%). Alternate solutions are substantially less accurate."
pub.1061753777,Parallel Implementation of the Irregular Terrain Model (ITM) for Radio Transmission Loss Prediction Using GPU and Cell BE Processors,"The Irregular Terrain Model (ITM), also known as the Longley-Rice model, predicts long-range average transmission loss of a radio signal based on atmospheric and geographic conditions. Due to variable terrain effects and constantly changing atmospheric conditions which can dramatically influence radio wave propagation, there is a pressing need for computational resources capable of running hundreds of thousands of transmission loss calculations per second. Multicore processors, like the NVIDIA Graphics Processing Unit (GPU) and IBM Cell Broadband Engine (BE), offer improved performance over mainstream microprocessors for ITM. We study architectural features of the Tesla C870 GPU and Cell BE and evaluate the effectiveness of architecture-specific optimizations and parallelization strategies for ITM on these platforms. We assess the GPU implementations that utilize both global and shared memories along with fine-grained parallelism. We assess the Cell BE implementations that utilize direct memory access, double buffering, and SIMDization. With these optimization strategies, we achieve less than a second of computation time on each platform which is not feasible with a general purpose processor, and we observe that the GPU delivers better performance than Cell BE in terms of total execution time and performance per watt metrics by a factor of 2.3x and 1.6x, respectively."
pub.1132267978,CCSDS-MHC on Raspberry Pi for Lossless Hyperspectral Image Compression,"This paper is about Consultative Committee for Space Data System for Lossless Multispectral and Hyperspectral Image Compression (CCSDS-MHC) algorithm that is implemented on Raspberry Pi 3 Model B+ system using Open Multi-Processing (OpenMP). CCSDS-MHC algorithm along with the full prediction mode is opted due to its best compression ratio (CR) performance. The issue of Hyperspectral Image Compression is the loss of data when compress, thus CCSDS algorithm is used in this research. Besides, with current technologies that require low-power but high-performance devices, Raspberry Pi was chosen to be tested in terms of its performances while being compared to other platforms. AVIRIS (airborne) and Hyperion (spaceborne) are used to test the performance of the system. OpenMP is introduced to simplify the computational operation through parallelization to take advantage of the multi-core architecture of the hardware system. In term of execution time, CCSDS-MHC algorithm when parallelize using OpenMP gave the best performances about 69.4% for AVIRIS 1997 dataset, 69.3% for AVIRIS 2006 dataset and 67.7% for Hyperion dataset. The execution time of performance CCSDS-MHC in Raspberry Pi 3 Model B+ comparing with other different multicore platform is validate and the mean parallelize is measured. The result of comparison proves that faster performing task gives the best result due to higher speed of CPU-cores performances. From this research, it is proven that Raspberry Pi which is a low-power embedded platform able to compress the hyperspectral images at optimized speed with the implementation of OpenMP through CCSDS algorithm. Therefore, this research will be useful for further studies in lossless of hyperspectral images."
pub.1091962561,High-Performance BEM Simulation of 3D Emulsion Flow,"Direct simulations of the dynamics of a large number of deformable droplets are necessary for a more accurate prediction of rheological properties and the microstructure of liquid-liquid systems that arise in a wide range of industrial applications, such as enhanced oil recovery, advanced material processing, and biotechnology. The present study is dedicated to the development of efficient computational methods and tools for understanding the behavior of three dimensional emulsion flows in Stokes regime. The numerical approach is based on the accelerated boundary element method (BEM) both via an efficient scalable algorithm, the fast multipole method (FMM), and via the utilization of advanced hardware, particularly, heterogeneous computing architecture (multicore CPUs and graphics processors). Example computations are conducted for 3D dynamics of systems of tens of thousands of deformable drops and several droplets with very high discretization of the interface in shear flow. The results of simulations and details of the method and accuracy/performance of the algorithm are discussed. The developed approach can be used for the solution of a wide range of problems related to emulsion flows in micro- and nanoscales."
pub.1135825898,SLAP: A Split Latency Adaptive VLIW pipeline architecture which enables on-the-fly variable SIMD vector-length,"Over the last decade the relative latency of access to shared memory by
multicore increased as wire resistance dominated latency and low wire density
layout pushed multiport memories farther away from their ports. Various
techniques were deployed to improve average memory access latencies, such as
speculative pre-fetching and branch-prediction, often leading to high variance
in execution time which is unacceptable in real time systems. Smart DMAs can be
used to directly copy data into a layer1 SRAM, but with overhead. The VLIW
architecture, the de facto signal processing engine, suffers badly from a
breakdown in lockstep execution of scalar and vector instructions. We describe
the Split Latency Adaptive Pipeline (SLAP) VLIW architecture, a cache
performance improvement technology that requires zero change to object code,
while removing smart DMAs and their overhead. SLAP builds on the Decoupled
Access and Execute concept by 1) breaking lockstep execution of functional
units, 2) enabling variable vector length for variable data level parallelism,
and 3) adding a novel triangular load mechanism. We discuss the SLAP
architecture and demonstrate the performance benefits on real traces from a
wireless baseband system (where even the most compute intensive functions
suffer from an Amdahls law limitation due to a mixture of scalar and vector
processing)."
pub.1018945141,Parallelizing and optimizing a hybrid differential evolution with Pareto tournaments for discovering motifs in DNA sequences,"Transcriptional regulation is the main regulation of gene expression, the process by which all prokaryotic organisms and eukaryotic cells transform the information encoded by the nucleic acids (DNA) into the proteins required for their operation and development. A crucial component in genetic regulation is the bindings between transcription factors and DNA sequences that regulate the expression of genes. These specific locations are short and share a common sequence of nucleotides. The discovery of these small DNA strings, also known as motifs, is labor intensive and therefore the use of high-performance computing can be a good way to address it. In this work, we present a parallel multiobjective evolutionary algorithm, a novel hybrid technique based on differential evolution with Pareto tournaments (H-DEPT). To study whether this algorithm is suitable to be parallelized, H-DEPT has been used to solve instances of different sizes on several multicore systems (2, 4, 8, 16, and 32 cores). As we will see, the results show that H-DEPT achieves good speedups and efficiencies. We also compare the predictions made by H-DEPT with those predicted by other biological tools demonstrating that it is also capable of performing quality predictions."
pub.1094998758,Accurately Approximating Superscalar Processor Performance from Traces,"Trace-driven simulation of superscalar processors is particularly complicated. The dynamic nature of superscalar processors combined with the static nature of traces can lead to large inaccuracies in the results, especially when traces contain only a subset of executed instructions for trace reduction. The main problem in the filtered trace simulation is that the trace does not contain enough information with which one can predict the actual penalty of a cache miss. In this paper, we discuss and evaluate three strategies to quantify the impact of a long latency memory access in a superscalar processor when traces have only L1 cache misses. The strategies are based on models about how a cache miss is treated with respect to other cache misses: (1) isolated cache miss model, (2) independent cache miss model, and (3) pairwise dependent cache miss model. Our experimental results demonstrate that the pairwise dependent cache miss model produces reasonably accurate results (4.8% RMS error) under perfect branch prediction. Our work forms a basis for fast, accurate, and configurable multicore processor simulation using a pre-determined processor core design."
pub.1141724610,Deep neural network learning for power limited heterogeneous system with workload classification,"Heterogeneous systems providing diverse computational capabilities have unlocked a new pathway in multicore processors. The versatility in applications and their ever-increasing performance demands have brought a paradigm shift to the heterogeneous systems. We use a deep neural network (DNN) based model to maximize performance under power constraints in heterogeneous systems. The dynamic power management technique is implemented in three stages. In the first stage, the core statistics and workload characteristics are collected for the DNN training at the later stage. This step dynamically estimates workload change for the current epoch using dynamic voltage frequency scaling (DVFS) based heuristic algorithm. In the second stage, DNN is trained through collected data points. The third stage uses the trained DNN model to identify a suitable voltage-frequency values to maximize performance under power capping. The power manager controls power-consumption at both per-core and per-chip levels. Our DNN prediction model is trained to address both core types (Large and Small) thus, improving the accuracy of the model. Simulations indicate that the proposed approach can achieve an overall 10.63% reduction in power-consumption and 5.5–6.8% improvement in power-savings when compared with the existing approaches. Besides, the proposed DNN model-based approach is able to maintain power capping with 95.81% accuracy with performance degradation of only 5.38% for a quad-core architecture."
pub.1061755078,A Domain Specific Approach to High Performance Heterogeneous Computing,"Users of heterogeneous computing systems face two problems: first, in understanding the trade-off relationships between the observable characteristics of their applications, such as latency and quality of the result, and second, how to exploit knowledge of these characteristics to allocate work to distributed computing platforms efficiently. A domain specific approach addresses both of these problems. By considering a subset of operations or functions, models of the observable characteristics or domain metrics may be formulated in advance, and populated at run-time for task instances. These metric models can then be used to express the allocation of work as a constrained integer program. These claims are illustrated using the domain of derivatives pricing in computational finance, with the domain metrics of workload latency and pricing accuracy. For a large, varied workload of 128 Black-Scholes and Heston model-based option pricing tasks, running upon a diverse array of 16 Multicore CPUs, GPUs and FPGAs platforms, predictions made by models of both the makespan and accuracy are generally within 10 percent of the run-time performance. When these models are used as inputs to machine learning and MILP-based workload allocation approaches, a latency improvement of up to 24 and 270 times over the heuristic approach is seen."
pub.1125949087,RETRACTED ARTICLE: PALM-CSS: a high accuracy and intelligent machine learning based cooperative spectrum sensing methodology in cognitive health care networks,"Spectrum sensing is the most crucial importance in cognitive radios. We propose a novel machine-learning algorithm for spectrum sensing in cognitive radio networks, which plays an essential role in medical data transmission. In this regard, high-speed pre-emptive decision-based multi-layer extreme learning machines are implemented for co-operative spectrums sensing in CR health care networks. For a radio channel, different vectors such as energy levels, distance, Channel ID, sensor values are determined at CR devices and are considered as a feature vector and thus used to feed into the proposed classifier for the determination of the availability of the channel. The classifier further categorizes the parameters such as user identification i.e., primary and secondary users, availability of channels, and the most crucial predictive decision of the available channels. The proposed PALM-CSS consists of two major phases, such as classification and prediction. Before the online classification and prediction, datasets are generated, and these datasets are used for the training of the proposed classifier. The proposed classifier uses the principle of high-speed priority-based multi-layer extreme learning machines for the classification and prediction. The experimental testbed has designed based Multicore CoxtexM-3 boards for implementing the real-time cognitive scenario and various performance parameters such as prediction accuracy, training and testing time, Receiver operating characteristics, and accuracy of detection. Furthermore, the proposed algorithms has also compared with the other existing machine learning algorithm such as artificial neural networks, support vector machines, K-nearest neighbor, Naïve Bayes and ensemble machine learning algorithms in which the proposed algorithm outperforms the other existing algorithms and finds its more suitable for cognitive health care networks."
pub.1154705445,Machine learning based asynchronous computational framework for generalized Kalman filter,"Summary Although the Kalman filter algorithms are well suited to be executed on most digital systems, they become slow when applied to large‐scale dynamic systems. Therefore, efficient execution of Kalman filter for the time‐critical and large‐scale applications is of the essence. This work aims to address this necessity by developing a novel framework to improve the performance of a generalized Kalman filter with unknown inputs (GKF‐UI) using multithreaded‐multicore processors and machine learning (ML) classification methods. An asynchronous execution model based on OpenMP message‐passing framework is developed and integrated with a novel supervised ML‐based thread classifier for the GKF‐UI algorithm to enhance execution efficiency. The experimental results show that the proposed approach can achieve up to 35.5× speedup over the serial single‐threaded implementations with no losses in the accuracy or changes to the generality of the filter structure. Moreover, this framework can play a significant role in realizations of computational advantages in large‐scale systems as well as for the time‐critical prediction applications."
pub.1125167253,Scalable Second Order Optimization for Deep Learning,"Optimization in machine learning, both theoretical and applied, is presently
dominated by first-order gradient methods such as stochastic gradient descent.
Second-order optimization methods, that involve second derivatives and/or
second order statistics of the data, are far less prevalent despite strong
theoretical properties, due to their prohibitive computation, memory and
communication costs. In an attempt to bridge this gap between theoretical and
practical optimization, we present a scalable implementation of a second-order
preconditioned method (concretely, a variant of full-matrix Adagrad), that
along with several critical algorithmic and numerical improvements, provides
significant convergence and wall-clock time improvements compared to
conventional first-order methods on state-of-the-art deep models. Our novel
design effectively utilizes the prevalent heterogeneous hardware architecture
for training deep models, consisting of a multicore CPU coupled with multiple
accelerator units. We demonstrate superior performance compared to
state-of-the-art on very large learning tasks such as machine translation with
Transformers, language modeling with BERT, click-through rate prediction on
Criteo, and image classification on ImageNet with ResNet-50."
pub.1039966973,Preliminary design examination of the ParalleX system from a software and hardware perspective,"Exascale systems, expected to emerge by the end of the next decade, will require the exploitation of billion-way parallelism at multiple hierarchical levels in order to achieve the desired sustained performance. While traditional approaches to performance evaluation involve measurements of existing applications on the available platforms, such a methodology is obviously unsuitable for architectures still at the brainstorming stage. The prediction of the future machine performance is an important factor driving the design of both the execution hardware and software environment. A good way to start assessing the performance is to identify the factors challenging the scalability of parallel applications. We believe the root cause of these challenges is the incoherent coupling between the current enabling technologies, such as Non-Uniform Memory Access of present multicore nodes equipped with optional hardware accelerators and the decades older execution model, i.e., Communicating Sequential Processes (CSP). Supercomputing is in the midst of a much needed phase change and the High-Performance Computing community is slowly realizing the necessity for a new design dogma, as affirmed in the preliminary Exascale studies. In this paper, we present an overview of the ParalleX execution model and its complementary design efforts at the software and hardware levels, while including power draw of the system as the resource of utmost importance. Since the interplay of hardware and software environment is quickly becoming one of the dominant factors in the design of well integrated, energy efficient, large-scale systems, we also explore the implications of the ParalleX model on the organization of parallel computing architectures. We also present scaling and performance results for an adaptive mesh refinement application developed using a ParalleX-compliant runtime system implementation, HPX."
pub.1099748540,ACAM: Application Aware Adaptive Cache Management for Shared LLC,"Modern Chip Multiprocessors (CMPs) are typically multicore systems with shared last level cache (LLC). Effective utilization of the shared cache resource can be a challenge when the demands of competing applications conflict with each other. At times, in order to accommodate new data required by one application, the other application’s useful data may get evicted. Such negative interference results into increase in memory miss and degrades system’s performance. Hence, a technique is required which optimally manages the LLC even in the presence of such conflicting demands.Various LLC management techniques have been proposed to efficiently manage shared caches. The state-of-the-art replacement policies like Static Re-reference Interval Prediction (SRRIP) and Application Aware Behavior Re-reference Interval Prediction (ABRip) evict a cache block based on their re-usability in the near future. SRRIP makes the replacement decisions per block basis whereas ABRip also considers the cache behavior of an application to minimize conflicting data demands. Hence, ABRip outperforms SRRIP for workload mixes where one application is cache friendly, and the other one is streaming. However, ABRip does not perform well when the workload mix is Cache friendly-Cache friendly. We propose Application Aware Adaptive Cache Management policy that adapts to both types of workload mixes. The proposed replacement policy reduces LLC misses per kilo instruction (mpki) up to 22.74% and 12.7% compared to SRRIP and ABRip respectively on a CMP system running SPEC CPU2006 workloads. Our policy effectively utilizes the shared LLC and outperforms both SRRIP and ABRip with performance gains of up to 10.12% and 9.36% respectively."
pub.1061575495,Highly Parallel Rate-Distortion Optimized Intra-Mode Decision on Multicore Graphics Processors,"Rate-distortion (RD)-based mode selections are important techniques in video coding. in these methods, an encoder may compute the RD costs for all the possible coding modes, and select the one which achieves the best trade-off between encoding rate and compression distortion. Previous papers have demonstrated that RD-based mode selections can lead to significant improvements in coding efficiency. RD-based mode selections, however, would incur considerable increases in encoding complexity, since these methods require computing the RD costs for numerous candidate coding modes. in this paper, we consider the scenario where software-based video encoding is performed on personal computers or game consoles, and investigate how multicore graphics processing units (GPUs) may be efficiently utilized to undertake the task of RD optimized intra-prediction mode selections in audio and video coding standards and H.264 video encoding. Achieving efficient GPU-based intra-mode decisions, however, could be nontrivial for two reasons. First, intra-mode decision tends to be sequential. Specifically, the mode decision of the current block would depend on the reconstructed data of the neighboring blocks. Therefore, the coding modes of neighboring blocks would need to be computed first before that of the current block can be determined. This dependency poses challenges to GPU-based computation, which relies heavily on parallel data processing to achieve superior speedups. Second, RD-based intra-mode decision may require conditional branchings to determine the encoding bit-rate, and these branching operations may incur substantial performance penalties when being executed on GPUs due to pipeline architectural designs. To address these issues, we analyze the data dependency in intra-mode decision, and propose novel greedy-based encoding orders to achieve highly parallel processing of data blocks. We also prove that the proposed greedy-based orders are optimal in our problem, i.e., they require the minimum number of iterations to process a video frame given the dependency constraints. in addition, we propose a method to estimate the coding rate suitable for GPU implementation. Experimental results suggest our proposed solution can be more than 50 times faster than the previously proposed parallel intra-prediction, since our work can efficiently exploit the massive parallel opportunity in GPUs."
pub.1146373219,Chapter 3 Parallel processing and distributed computing,"Computer clouds are large-scale distributed systems that are collections of autonomous and heterogeneous systems. Cloud organization is based on the experience accumulated since the first electronic computer was used to solve computationally challenging problems. This chapter overviews the concepts in parallel and distributed systems important for understanding the basic challenges of the design and use of computer clouds. Hardware parallelism is critical for the performance of single-core processors, multicore processors, systems on a chip, multiprocessor systems, clusters, and warehouse-scale computers, which are the backbone of computer clouds. The first sections cover parallel and distributed system hardware, stressing the quantitative rather than qualitative aspects of computer architecture. Basic architectural concepts of modern computer systems, optimization of computer architecture including caching, out-of-order instruction execution, dynamic scheduling, branch predictions, and ARM architecture are discussed in Sections 3.1, 3.2, and 3.3, respectively. Sections 3.4, 3.5, 3.6, and 3.7 cover SIMD architectures, GPUs (Graphics Processing Units), TPUs (Tensor Processing Units), and SoCs (Systems On a Chip) and edge cloud computing. Section 3.8 covers data, task, and thread-level parallelism. Extracting parallelism depends on the application; Sections 3.9 and 3.10 analyze the speedup limits given by Amdahl's law, Amdahl's law for multicore processors, and scaled speedup. Section 3.11 reviews the evolution of most powerful computing systems, from supercomputers to large-scale distributed systems. Organization principles for distributed systems, such as modularity and layering, presented in Sections 3.12 and 3.13, are applied to the design of the peer-to-peer and large-scale systems discussed in Sections 3.14 and 3.15, respectively. Section 3.16 presents composability bounds and scalability, and Section 3.17 surveys fallacies concerning distributed computing. Finally, Section 3.18 discusses blockchain technologies and applications. History notes, further readings, and exercises and problems in Sections 3.19 and 3.20 conclude the chapter."
pub.1018084449,Modeling and Simulation of a Dynamic Task-Based Runtime System for Heterogeneous Multi-core Architectures,"Multi-core architectures comprising several GPUs have become mainstream in the field of High-Performance Computing. However, obtaining the maximum performance of such heterogeneous machines is challenging as it requires to carefully offload computations and manage data movements between the different processing units. The most promising and successful approaches so far rely on task-based runtimes that abstract the machine and rely on opportunistic scheduling algorithms. As a consequence, the problem gets shifted to choosing the task granularity, task graph structure, and optimizing the scheduling strategies. Trying different combinations of these different alternatives is also itself a challenge. Indeed, getting accurate measurements requires reserving the target system for the whole duration of experiments. Furthermore, observations are limited to the few available systems at hand and may be difficult to generalize. In this article, we show how we crafted a coarse-grain hybrid simulation/emulation of StarPU, a dynamic runtime for hybrid architectures, over SimGrid, a versatile simulator for distributed systems. This approach allows to obtain performance predictions accurate within a few percents on classical dense linear algebra kernels in a matter of seconds, which allows both runtime and application designers to quickly decide which optimization to enable or whether it is worth investing in higher-end GPUs or not."
pub.1107559552,Dynamic Voltage and Frequency Scaling in NoCs with Supervised and Reinforcement Learning Techniques,"Network-on-Chips (NoCs) are the de facto choice for designing the interconnect fabric in multicore chips due to their regularity, efficiency, simplicity, and scalability. However, NoC suffers from excessive static power and dynamic energy due to transistor leakage current and data movement between the cores and caches. Power consumption issues are only exacerbated by ever decreasing technology sizes. Dynamic Voltage and Frequency Scaling (DVFS) is one technique that seeks to reduce dynamic energy; however this often occurs at the expense of performance. In this paper, we propose LEAD Learning-enabled Energy-Aware Dynamic voltage/frequency scaling for multicore architectures using both supervised learning and reinforcement learning approaches. LEAD groups the router and its outgoing links into the same V/F domain and implements proactive DVFS mode management strategies that rely on offline trained machine learning models in order to provide optimal V/F mode selection between different voltage/frequency pairs. We present three supervised learning versions of LEAD that are based on buffer utilization, change in buffer utilization and change in energy/throughput, which allow proactive mode selection based on accurate prediction of future network parameters. We then describe a reinforcement learning approach to LEAD that optimizes the DVFS mode selection directly, obviating the need for label and threshold engineering. Simulation results using PARSEC and Splash-2 benchmarks on a 4 4 concentrated mesh architecture show that by using supervised learning LEAD can achieve an average dynamic energy savings of 15.4 percent for a loss in throughput of 0.8 percent with no significant impact on latency. When reinforcement learning is used, LEAD increases average dynamic energy savings to 20.3 percent at the cost of a 1.5 percent decrease in throughput and a 1.7 percent increase in latency. Overall, the more flexible reinforcement learning approach enables learning an optimal behavior for a wider range of load environments under any desired energy versus throughput tradeoff."
pub.1094475590,Parallel Implementation of Irregular Terrain Model on IBM Cell Broadband Engine,"Prediction of radio coverage, also known as radio “hear-ability” requires the prediction of radio propagation loss. The Irregular Terrain Model (ITM) predicts the median attenuation of a radio signal as a function of distance and the variability of the signal in time and in space. Algorithm can be applied to a large amount of engineering problems to make area predictions for applications such as preliminary estimates for system design, surveillance, and land mobile systems. When the radio transmitters are mobile, the radio coverage changes dynamically, taking on a real-time aspect that requires thousands of calculations per second, which can be achieved through the use of recent advances in multicore processor technology. In this study, we evaluate the performance of ITM on IBM Cell Broadband Engine (BE). We first give a brief introduction to the algorithm of ITM and present both the serial and parallel execution manner of its implementation. Then we exploit how to map out the program on the target processor in detail. We choose message queues on Cell BE which offer the simplest possible expression of the algorithm while being able to fully utilize the hardware resources. Full code segment and a complete set of terrain profiles fit into each processing element without the need for further partitioning. Communications and memory management overhead is minimal and we achieve 90.2% processor utilization with 7.9x speed up compared to serial version. Through our experimental studies, we show that the program is scalable and suits very well for implementing on the CELL BE architecture based on the granularity of computation kernels and memory footprint of the algorithm."
pub.1138043244,SLAP: a Split Latency Adaptive VLIW Pipeline Architecture Which Enables on-The-Fly Variable SIMD Vector-Length,"Over the last decade the relative latency of access to shared memory by multicore increased as wire resistance dominated latency and low wire density layout pushed multi-port memories farther away from their ports. Various techniques were deployed to improve average memory access latencies, such as speculative pre-fetching and branch-prediction, often leading to high variance in execution time which is unacceptable in real-time systems. Smart DMAs can be used to directly copy data into a layer-1 SRAM, but with overhead. The VLIW architecture, the de-facto signal-processing engine, suffers badly from a breakdown in lock-step execution of scalar and vector instructions. We describe the Split Latency Adaptive Pipeline (SLAP) VLIW architecture, a cache performance improvement technology that requires zero change to object code, while removing smart DMAs and their overhead. SLAP builds on the Decoupled Access and Execute concept by 1) breaking lock-step execution of functional units, 2) enabling variable vector length for variable data-level parallelism, and 3) adding a novel triangular-load mechanism. We discuss the SLAP architecture and demonstrate the performance benefits on real traces from a wireless baseband-system (where even the most compute intensive functions suffer from an Amdahl’s law limitation due to a mixture of scalar and vector processing)."
pub.1171566863,OpSAVE: Eviction Based Scheme for Efficient Optical Network-on-Chip,"For on-chip networks, nanophotonics has been considered a strong alternative owing to its high speed (due to low latency) and high bandwidth (due to wavelength division multiplexing). However, the major hurdle in the adoption of nanophotonic-based on-chip networks is their high static power consumption. Various proposals are there in the literature which try to reduce the static power consumption either by modulating the laser or by allowing the on-chip stations to share the photonic channels. In this paper, we propose OpSAVE— an optical NoC that combines the above two strategies to effectively reduce static power consumption. It proposes a superior prediction mechanism based on the eviction details from the private caches. It explains how shared channels can be used to dynamically balance the load and at the same time handle mispredictions. It allows the optical stations to share both the power and the available bandwidth to increase their utilization. Moreover, OpSAVE proposes to use a double pumping strategy to improve the system performance. We compared our scheme with the state-of-the-art proposals in this domain and the results show that our scheme consumes 4.4X less optical power and at the same time improves the performance by nearly 28%. In the evaluation, we have considered the multicore benchmarks from the Splash and Parsec benchmark suites."
pub.1118365129,A Domain Specific Approach to High Performance Heterogeneous Computing,"Users of heterogeneous computing systems face two problems: firstly, in
understanding the trade-off relationships between the observable
characteristics of their applications, such as latency and quality of the
result, and secondly, how to exploit knowledge of these characteristics to
allocate work to distributed computing platforms efficiently. A domain specific
approach addresses both of these problems. By considering a subset of
operations or functions, models of the observable characteristics or domain
metrics may be formulated in advance, and populated at run-time for task
instances. These metric models can then be used to express the allocation of
work as a constrained integer program, which can be solved using heuristics,
machine learning or Mixed Integer Linear Programming (MILP) frameworks. These
claims are illustrated using the example domain of derivatives pricing in
computational finance, with the domain metrics of workload latency or makespan
and pricing accuracy. For a large, varied workload of 128 Black-Scholes and
Heston model-based option pricing tasks, running upon a diverse array of 16
Multicore CPUs, GPUs and FPGAs platforms, predictions made by models of both
the makespan and accuracy are generally within 10% of the run-time performance.
When these models are used as inputs to machine learning and MILP-based
workload allocation approaches, a latency improvement of up to 24 and 270 times
over the heuristic approach is seen."
pub.1063163285,PCantorSim,"
                    Computer architects rely heavily on microarchitecture simulation to evaluate design alternatives. Unfortunately, cycle-accurate simulation is extremely slow, being at least 4 to 6 orders of magnitude slower than real hardware. This longstanding problem is further exacerbated in the multi-/many-core era, because single-threaded simulation performance has not improved much, while the design space has expanded substantially. Parallel simulation is a promising approach, yet does not completely solve the simulation challenge. Furthermore, existing sampling techniques, which are widely used for single-threaded applications, do not readily apply to multithreaded applications as thread interaction and synchronization must now be taken into account. This work presents
                    PCantorSim
                    , a novel Cantor set (a classic fractal)--based sampling scheme to accelerate parallel simulation of multithreaded applications. Through the use of the proposed methodology, only less than 5% of an application's execution time is simulated in detail. We have implemented our approach in
                    Sniper
                    (a parallel multicore simulator) and evaluated it by running the PARSEC benchmarks on a simulated 8-core system. The results show that
                    PCantorSim
                    increases simulation speed over detailed parallel simulation by a factor of 20×, on average, with an average absolute execution time prediction error of 5.3%.
                  "
pub.1094970945,A Low Complexity Lossless Data Compressor IP-Core for Satellite Images,"As the technology advances, space imaging systems use equipment of increasing resolutions. Hence, it is necessary to ensure that this great quantity of data arrives at its destination reliably. Among some variables involved, data compression plays an important role to accomplish this requirement. In this context, this paper proposes a digital hardware approach of a low complexity satellite image lossless compressor based on prediction and Golomb-Rice coding, which has achieved excellent results considering hardware and compression performance. In order to validate and analyze the compressor, a functional verification and FPGA prototyping methodology were followed. Given an image set from Brazilian's National Institute for Space Research (INPE, in Portuguese acronyms), its results on FPGA show that this compressor achieves compression ratio around 3.4, comparable value to related works in this area, and throughput of 28 MPixel/s (224 Mbit/s). Taking advantage of images nature, its compression can be parallelized through simultaneous multicores compressors. For example, using 5 cores, this work is able to compress those images in a rate of 142 MPixel/s (1.1 Gbit/s). All these features make it useful and effective as a current remote sensing imaging system."
pub.1170309350,A Python library for efficient computation of molecular fingerprints,"Machine learning solutions are very popular in the field of chemoinformatics,
where they have numerous applications, such as novel drug discovery or
molecular property prediction. Molecular fingerprints are algorithms commonly
used for vectorizing chemical molecules as a part of preprocessing in this kind
of solution. However, despite their popularity, there are no libraries that
implement them efficiently for large datasets, utilizing modern, multicore
architectures. On top of that, most of them do not provide the user with an
intuitive interface, or one that would be compatible with other machine
learning tools.
  In this project, we created a Python library that computes molecular
fingerprints efficiently and delivers an interface that is comprehensive and
enables the user to easily incorporate the library into their existing machine
learning workflow. The library enables the user to perform computation on large
datasets using parallelism. Because of that, it is possible to perform such
tasks as hyperparameter tuning in a reasonable time. We describe tools used in
implementation of the library and asses its time performance on example
benchmark datasets. Additionally, we show that using molecular fingerprints we
can achieve results comparable to state-of-the-art ML solutions even with very
simple models."
pub.1038693749,Speculatively Multithreaded Architectures,"Using the increasing number of transistors to build larger dynamic-issue superscalar processors for the purposes of exposing more parallelism has run into problems of diminishing returns, great design complexity, and high power dissipation. While chip multiprocessors (CMPs) alleviate these problems by employing multiple smaller, power-efficient cores to utilize the available transistors, CMPs require parallel programming which is significantly harder than sequential programming. Speculatively multithreaded architectures address both the programmability issues of CMPs and the power–complexity–performance problems of superscalar processors. Speculatively multithreaded architectures partition a sequential program into contiguous program fragments called tasks which are executed in parallel on multiple cores. The architectures execute the tasks in parallel by speculating that the tasks are independent, though the tasks are not guaranteed to be independent. The architecture provides hardware support to detect dependencies and roll back misspeculations. This chapter addresses the key questions of how programs are partitioned into tasks while maximizing inter-task parallelism and how inter-task control-flow and data dependencies (register and memory dependencies) are maintained especially in the distributed multicore organization employed by the speculatively multithreaded architectures."
pub.1039726072,PCantorSim: Accelerating parallel architecture simulation through fractal-based sampling,"Computer architects rely heavily on microarchitecture simulation to evaluate design alternatives. Unfortunately, cycle-accurate simulation is extremely slow, being at least 4 to 6 orders of magnitude slower than real hardware. This longstanding problem is further exacerbated in the multi-/many-core era, because single-threaded simulation performance has not improved much, while the design space has expanded substantially. Parallel simulation is a promising approach, yet does not completely solve the simulation challenge. Furthermore, existing sampling techniques, which are widely used for single-threaded applications, do not readily apply to multithreaded applications as thread interaction and synchronization must now be taken into account. This work presents PCantorSim, a novel Cantor set (a classic fractal)--based sampling scheme to accelerate parallel simulation of multithreaded applications. Through the use of the proposed methodology, only less than 5% of an application's execution time is simulated in detail. We have implemented our approach in Sniper (a parallel multicore simulator) and evaluated it by running the PARSEC benchmarks on a simulated 8-core system. The results show that PCantorSim increases simulation speed over detailed parallel simulation by a factor of 20×, on average, with an average absolute execution time prediction error of 5.3%."
pub.1012438099,Application configuration selection for energy-efficient execution on multicore systems,"Modern computer systems are designed to balance performance and energy consumption. Several run-time factors, such as concurrency levels, thread mapping strategies, and dynamic voltage and frequency scaling (DVFS) should be considered in order to achieve optimal energy efficiency for a workload. Selecting appropriate run-time factors, however, is one of the most challenging tasks because the run-time factors are architecture-specific and workload-specific.While most existing works concentrate on either static analysis of the workload or run-time prediction results, in this paper, we present a hybrid two-step method that utilizes concurrency levels and DVFS settings to achieve the energy efficiency configuration for a workload. The experimental results based on a Xeon E5620 server with NPB and PARSEC benchmark suites show that the model is able to predict the energy efficient configuration accurately. On average, an additional 10% EDP (Energy Delay Product) saving is obtained by using run-time DVFS for the entire system. An off-line optimal solution is used to compare with the proposed scheme. The experimental results show that the average extra EDP saved by the optimal solution is within 5% on selective parallel benchmarks."
pub.1003402210,The Implementation of Regional Atmospheric Model Numerical Algorithms for CBEA-Based Clusters,"Regional atmospheric models are important tools for short-range weather predictions and future climate change assessment. The further enhancement of spatial resolution and development of physical parameterizations in these models need the effective implementation of the program code on multiprocessor systems. However, nowadays typical cluster systems tend to grow into very huge machines with over petaflop performance, while individual computing node design stays almost unchanged, and growth is achieved simply by using more and more nodes, rather than increasing individual node performance and keeping adequate power consuming. This leads to worse scalability of data-intensive applications due to increasing time consumption for data passing via clusters interconnect. Especially some of numerical algorithms (e.g. those solving the Poisson equation) satisfactorily scaling at previous generation cluster systems do not utilize the computational resources of clusters with thousands cores effectively. This prompts to study the performance of numerical schemes of regional atmospheric models on processor architectures significantly different from those used in conventional clusters. Our approach focuses on improving the performance of time explicit numerical schemes for Reynolds-averaged equations of atmospheric hydrodynamics and thermodynamics by parallelization on CellBE processors. The optimization of loops for numerical schemes with local data dependence pattern and with independent iterations is presented. Cell-specific workloading managers are built on top of existing numerical schemes implementations, conserving the original source code layout and bringing high speed-ups over serial version on QS22 blade server. Intercomparison between Cell and other multicore architectures is also provided. Targeting the next generation of MPI-CellBE hybrid cluster architectures, out method aims to provide additional scalability to MPI-based codes of atmospheric models and related applications."
pub.1146355960,AI Technology in Networks-on-Chip,"The on-chip network, commonly known as network-on-chip (NoC) on a die as an alternate prevalent interconnection infrastructure, has been continuously occupying the space of systems-on-chip (SoCs). In other words, the SoC-based systems are continually transformed to NoC-based multiprocessor and multicore designs. These designs would prospectively facilitate high core utilization and the need for high performance. To date, performance evaluation of NoCs is largely based on simulation employing a set of NoC simulators, e.g., Noxim, BookSim, etc. However, the use of simulators has several limitations. Therefore, it has become essential to opt for an alternate approach. The rapid development of the artificial intelligence (AI) industry currently leads the next generation of computation. Consequently, AI technology in every aspect becomes an essential milestone in surpassing the human level. Recent research shows that AI technology, including machine learning, deep learning, etc., is often preferred to improve data transformation and hardware efficiency in on-chip communication networks. Such networks are called AI chips, where an NoC is one feasible solution to SoCs. This chapter seeks to critically situate the AI technology in evaluating the NoC performance at different perspectives: theoretical, technological, analytical, etc. The chapter elucidates some of the salient problems, interests, and issues for its organization around topology exploration and design considerations in NoCs; customization of NoCs for AI chips; performance evaluation via simulation; performance predictions using machine learning, deep learning, etc. The overall thrust of this chapter is to provide “Core Perspective” and “Further Perspective” of the application of AI technology in predicting several NoC performance metrics toward high-speed communication and voluminous computation by multiprocessor systems-on-chip. Thus, one can see how this AI technology dramatically improves the speed and accuracy of predicting various performance metrics. This chapter aims to critically situate the AI technology in evaluating the NoC performance at different perspectives: theoretical, technological, analytical, etc. It discusses the salient problems, interests, and issues for its organization around topology exploration and design considerations in NoCs; customization of NoCs for AI chips; performance evaluation via simulation; performance predictions using machine learning, deep learning, etc. The rapid development of the artificial intelligence (AI) industry currently leads the next generation of computation. NoC is the choice of chip designers due to its efficient on-chip communications. NoC-based communication architectures are the new era for enabling high-performance communication and computation. The word “intelligence” means the capacity to learn, particularly “the ability to act like humans”. Therefore, the primary objective of AI is to build and understand intelligent entities tha"
pub.1147576062,On Dynamic Ray Tracing and Anticipative Channel Prediction for Dynamic Environments,"Ray tracing algorithms, that can simulate multipath radio propagation in
presence of geometric obstacles such as buildings, objects or vehicles, are
becoming quite popular, due to the increasing availability of digital
environment databases and high-performance computation platforms, such as
multicore computers and cloud computing services. When objects or vehicles are
moving, which is the case of industrial or vehicular environments, multiple
successive representations of the environment (""snapshots"") and multiple ray
tracing runs are often necessary, which require a great human effort and a
great deal of computation resources. Recently, the Dynamic Ray Tracing (DRT)
approach has been proposed to predict the multipath evolution within a given
time lapse on the base of the current multipath geometry, assuming constant
speeds and/or accelerations for moving objects, using analytical extrapolation
formulas. This is done without re-running a full ray tracing for every
""snapshot"" of the environment, therefore with a great computation time saving.
When DRT is embedded in a mobile radio system and used in real-time,
ahead-of-time (or anticipative) field prediction is possible that opens the way
to interesting applications. In the present work, a full-3D DRT algorithm is
presented that allows to account for multiple reflections, edge diffraction and
diffuse scattering for the general case where moving objects can translate and
rotate. For the purpose of validation, the model is first applied to some ideal
cases and then to realistic cases where results are compared with conventional
ray tracing simulation and measurements available in the literature."
pub.1163947001,An efficient intelligent task management in autonomous vehicles using AIIOT and optimal kernel adaptive SVM,"The fast evolution of artificial intelligence (AI) and the IoT gained more interest in the development of autonomous vehicles. The main challenges faced by autonomous car manufacturers are high computation costs and the lag of intelligent task management systems. The accident rates created by autonomous vehicles are increasing rapidly due to their unrestrained traffic, inaccurate location, and mapping methods. So, secure driving becomes the main concern in self-driving vehicle design. Moreover, the inadequate battery life and computation power made the system complex to minimize execution time as well as resource computation. Therefore, to handle all these complications, an intelligent task-managing system for autonomous vehicles is proposed in this paper. In this, each task is optimally executed by invoking the supervised resource predictor kernel data adaptive support vector machine-based multimodal bacterial foraging (KDASVM-MBF) method. The KDASVM-MBF intelligent task scheduling method is proposed to distribute all the tasks to the suitable processor based on central processing unit (CPU) usage and emergency. The proposed model is implemented in Python IDE-version 3.8 and examined using two multicore processors (Nvidia and AIIoT). The potential capability of the introduced type is evaluated by computing the performance methods such as response time, resource utilization, CPU utilization, execution time, prediction accuracy, and task miss rate. The experimental results reveal that the established KDASVM-MBF method accomplishes prediction accuracy of about 97% and 98% for Nvidia and AIIoT processors respectively with minimum task miss rate and execution time."
pub.1156643503,On Dynamic Ray Tracing and Anticipative Channel Prediction for Dynamic Environments,"Ray tracing (RT) algorithms, which can simulate multipath radio propagation in the presence of geometric obstacles such as buildings, objects, or vehicles, are becoming quite popular, due to the increasing availability of digital environment databases and high-performance computation platforms, such as multicore computers and cloud computing services. When objects or vehicles are moving, which is the case of industrial or vehicular environments, multiple successive representations of the environment (“snapshots”) and multiple RT runs are often necessary, which require a great human effort and a great deal of computation resources. Recently, the dynamic RT (DRT) approach has been proposed to predict the multipath evolution within a given time lapse on the base of the current multipath geometry, assuming constant speeds and/or accelerations for moving objects, using analytical extrapolation formulas. This is done without rerunning a full RT for every “snapshot” of the environment, therefore with a great reduction in both human labor and computation time. When DRT is embedded in a mobile radio system and used in real time, ahead-of-time (or anticipative) field prediction is possible, which opens the way to interesting applications. In the present work, a full-3-D DRT algorithm is presented, which allows to account for multiple reflections, edge diffraction, and diffuse scattering for the general case where moving objects can translate and rotate. For the purpose of validation, the model is first applied to some ideal cases and then to realistic cases where results are compared with conventional RT simulation and measurements available in the literature."
pub.1151357296,Deep Learning Technology-Based Exoskeleton Robot Controller Development,"Model-based control is preferred for robotics applications due to its
systematic approach to linearize and control the robot's nonlinear dynamics.
The fundamental challenge involved in implementing a model-based controller for
robotics applications is the time delay associated with the real-time
computation of the robot dynamics. Due to the sequential structure of the
robot's dynamic equation of motion, the multicore CPU cannot reduce the control
algorithm execution time. A high-speed processor is required to maintain a
higher sampling rate. Neural network-based modeling offers an excellent
solution for developing a parallel structured equivalent model of the
sequential model that is suitable for parallel processing. In this paper, a
Deep neural network-based parallel structured 7 degrees of freedom human lower
extremity exoskeleton robot controller is developed. Forty-nine densely
connected neurons are arranged in four layers to estimate joint torque
requirements for tracking trajectories. For training, the deep neural network,
an analytical model-based data generation technique is presented. A trained
deep neural network is used for real-time joint torque prediction and a PD
controller is incorporated to mitigate the prediction errors. Simulation
results show high trajectory tracking performances. The developed controller's
stability analysis is proved. The robustness of the controller against the
parameter variation is analyzed with the help of the analysis of variance
(ANOVA). A comparative study between the developed controller and the Computed
Torque Controller, Model Reference Computed Torque Controller, Sliding Mode
Controller, Adaptive controller, and Linear Quadratic Regulator are presented
while keeping the same robot dynamics."
pub.1121481235,MV-Net,"Recently the development of deep learning has been propelling the sheer growth of vision and speech applications on lightweight embedded and mobile systems. However, the limitation of computation resource and power delivery capability in embedded platforms is recognized as a significant bottleneck that prevents the systems from providing real-time deep learning ability, since the inference of deep convolutional neural networks (CNNs) and recurrent neural networks (RNNs) involves large quantities of weights and operations. Particularly, how to provide quality-of-services (QoS)-guaranteed neural network inference ability in the multitask execution environment of multicore SoCs is even more complicated due to the existence of resource contention. In this article, we present a novel deep neural network architecture, MV-Net, which provides performance elasticity and contention-aware self-scheduling ability for QoS enhancement in mobile computing systems. When the constraints of QoS, output accuracy, and resource contention status of the system change, MV-Net can dynamically reconfigure the corresponding neural network propagation paths and thus achieves an effective tradeoff between neural network computational complexity and prediction accuracy via approximate computing. The experimental results show that (1) MV-Net significantly improves the performance flexibility of current CNN models and makes it possible to provide always-guaranteed QoS in a multitask environment, and (2) it satisfies the quality-of-results (QoR) requirement, outperforming the baseline implementation significantly, and improves the system energy efficiency at the same time."
pub.1125793975,PBBCache: An open-source parallel simulator for rapid prototyping and evaluation of cache-partitioning and cache-clustering policies,"Chip multicore processors (CMPs) constitute the architecture of choice for a wide spectrum of computing systems, ranging from power-efficient mobile devices to high-performance server platforms. Despite their benefits, the contention that appears when multiple applications compete for the use of shared resources among cores, such as the last-level cache (LLC), may lead to substantial performance degradation. This may have a negative impact on key system metrics such as throughput and fairness. Partitioning of the LLC (i.e., assigning a separate cache partition with a certain size to each application) has been proven effective to mitigate contention-related effects. In this article we propose a parallel simulator that makes it possible to quickly compare the effectiveness of different cache-partitioning policies with the optimal solution for different optimization objectives. The simulator can obtain the optimal solution for any point during the execution of a multi-program workload where each application goes through a certain program phase. Our proposal leverages a slowdown-prediction model that accounts for degradation due to cache sharing and memory-bandwidth contention, which constitute the major factors of shared-resource contention on current CMPs. To determine the optimal solution for two optimization objectives (throughput and fairness optimization), we leverage a novel distributed-memory parallel branch-and-bound strategy specifically designed to efficiently distribute the computation across multiple processing cores."
pub.1093831260,An Efficient Network-on-Chip (NoC) Based Multicore Platform for Hierarchical Parallel Genetic Algorithms,"In this work, we propose a new Network-on-Chip (NoC) architecture for implementing the hierarchical parallel genetic algorithm (HPGA) on a multi-core System-on-Chip (SoC) platform. We first derive the speedup metric of an NoC architecture which directly maps the HPGA onto NoC in order to identify the main sources of performance bottlenecks. Specifically, it is observed that the speedup is mostly affected by the fixed bandwidth that a master processor can use and the low utilization of slave processor cores. Motivated by the theoretical analysis, we propose a new architecture with two multiplexing schemes, namely dynamic injection bandwidth multiplexing (DIBM) and time-division based island multiplexing (TDIM), to improve the speedup and reduce the hardware requirements. Moreover, a task-aware adaptive routing algorithm is designed for the proposed architecture, which can take advantage of the proposed multiplexing schemes to further reduce the hardware overhead. We demonstrate the benefits of our approach using the problem of protein folding prediction, which is a process of importance in biology. Our experimental results show that the proposed NoC architecture achieves up to 240X speedup compared to a single island design. The hardware cost is also reduced by 50% compared to a direct NoC-based HPGA implementation."
pub.1015514281,Layer multiplexing FPGA implementation for deep back-propagation learning,"Training of large scale neural networks, like those used nowadays in Deep Learning schemes, requires long computational times or the use of high performance computation solutions like those based on cluster computation, GPU boards, etc. As a possible alternative, in this work the Back-Propagation learning algorithm is implemented in an FPGA board using a multiplexing layer scheme, in which a single layer of neurons is physically implemented in parallel but can be reused any number of times in order to simulate multi-layer architectures. An on-chip implementation of the algorithm is carried out using a training/validation scheme in order to avoid overfitting effects. The hardware implementation is tested on several configurations, permitting to simulate architectures comprising up to 127 hidden layers with a maximum number of neurons in each layer of 60 neurons. We confirmed the correct implementation of the algorithm and compared the computational times against C and Matlab code executed in a multicore supercomputer, observing a clear advantage of the proposed FPGA scheme. The layer multiplexing scheme used provides a simple and flexible approach in comparison to standard implementations of the Back-Propagation algorithm representing an important step towards the FPGA implementation of deep neural networks, one of the most novel and successful existing models for prediction problems."
pub.1113269178,A Study of Core Utilization and Residency in Heterogeneous Smart Phone Architectures,"In recent years, the smart phone platform has seen a rise in the number of cores and the use of heterogeneous clusters as in the Qualcomm Snapdragon, Apple A10 and the Samsung Exynos processors. This paper attempts to understand characteristics of mobile workloads, with measurements on heterogeneous multicore phone platforms with big and little cores. It answers questions such as the following: (i) Do smart phones need multiple cores of different types (eg: big or little)? (ii) Is it energy-efficient to operate with more cores (with less time) or fewer cores even if it might take longer? (iii)What are the best frequencies to operate the cores considering energy efficiency? (iv) Do mobile applications need out-of-order speculative execution cores with complex branch prediction? (v) Is IPC a good performance indicator for early design tradeoff evaluation while working on mobile processor design? Using Geekbench and more than 3 dozen Android applications, and the Workload Automation tool from ARM, we measure core utilization, frequency residencies, and energy efficiency characteristics on two leading edge smart phones. Many characteristics of smartphone platforms are presented, and architectural implications of the observations as well as design considerations for future mobile processors are discussed. A key insight is that multiple big and complex cores are beneficial both from a performance as well as an energy point of view in certain scenarios. It is seen that 4 big cores are utilized during application launch and update phases of applications. Similarly, reboot using all 4 cores at maximum performance provides latency advantages. However, it consumes higher power and energy, and reboot with 2 cores was seen to be more energy efficient than reboot with 1 or 4 cores. Furthermore, inaccurate branch prediction is seen to result in up to 40% mis-speculated instructions in many applications, suggesting that it is important to improve the accuracy of branch predictors in mobile processors. While absolute IPCs are observed to be a poor predictor of benchmark scores, relative IPCs are useful for estimating the impact of microarchitectural changes on benchmark scores."
pub.1084677804,Adaptive and Scalable Predictive Page Policies for High Core-Count Server CPUs,"Increasing datacenter compute requirements has led to tremendous growth in the cadence of CPU cores on chip-multiprocessors. With large number of threads running on a single node, it is critical to achieve high memory bandwidth efficiency on large scale CMPs to support continued growth in the number CPU cores. In this paper, we present several mechanisms that improve the memory efficiency by improving the page hit rate for multi-core processors. In particular, we present memory page-policies that dynamically adapt to the runtime workload characteristics and use thread awareness to reduce contention between different memory address streams from the different threads. Unlike contemporary DRAM page policies such as static or timer-based, the proposed framework profiles the memory stream at runtime and uncovers opportunities to close or keep DRAM pages open, resulting in reduced page-conflicts and improved efficiencies. We implement the proposed policies in a cycle-accurate performance model simulating an 8-core processor. Our results show that the proposed adaptive page policies increase performance of high memory bandwidth workloads in SPECint2006 by up to 3%, and can attain 83% average performance relative to a “perfect” page prediction policy. We further show that the performance improvement from the techniques increases with the number of cores and with making the policies thread-aware in a many-core processor. The implementation cost of our techniques is extremely low, an area overhead of only 69 bits, making them extremely attractive for real-life products."
pub.1112505974,A collaborative CPU‐GPU approach for deep learning on mobile devices,"Summary  As mobile devices become more prevalent, users tend to reassess their expectations regarding the personalization of mobile services. The data collected by a mobile device's sensors provide an opportunity to gain insight into the user's profile. Recently, deep learning has gained momentum and has become the method of choice for solving machine learning problems. Interestingly, training a deep neural network on a mobile device is often mistakenly regarded as cumbersome. For instance, several deep learning frameworks only provide a CPU‐based implementation for prediction tasks on a mobile device. In contrast to servers, a mobile computing environment imposes many domain‐specific constraints that invite us to review the general computing approach used in a deep learning framework implementation. In this paper, we propose a deep learning framework that has been specifically designed for mobile device platforms. Our approach relies on the collaboration of the multicore CPU and the integrated GPU to accelerate deep learning computation on mobile devices. Our work exploits the shared memory architecture of mobile devices to promote CPU‐GPU collaboration without any data copying. We analyze our approach with regard to three factors:  performance/portability trade‐off , power efficiency , and memory management . "
pub.1100089638,Task Scheduling for Processing Big Graphs in Heterogeneous Commodity Clusters,"Large-scale graph processing is a challenging problem since vertices can be arbitrarily connected, reducing locality and easily expanding the solution space. As a result, in recent years, a new breed of distributed frameworks that handle graphs efficiently has emerged. In large clusters with many resources (RAM, CPUs, network connectivity), these frameworks focus on exploiting the available resources as efficiently as possible. However, on situations where the cluster hardware is unbalanced or low in computing resources, the framework must correctly allocate tasks in order to complete execution. In this work, we compare three frameworks, the generic Fork-Join framework adapted to graph processing, and the Pregel and DPM frameworks that were originally designed for computing graphs. A link-prediction algorithm was used as case study to analyze several scheduling strategies that allocate tasks to servers in a cluster of heterogeneous characteristics. The dataset used for the experiments is a snapshot from the Twitter graph, and specifically, a subset of its users that pushed the memory requirements of the algorithm."
pub.1094257758,"A Prediction-Based, Data Migration Algorithm for Hybrid Architecture NoC Systems","Multicore designs have been trending towards Network on Chip (NoC) Architectures since as early 2001. As these designs mature significant feature enhancement is still required. Cache latency for single accesses becomes a crippling design issue as the size of the chip expands. Even further, as usage patterns shift towards multiple accessors of a single datum, optimal placement becomes critical for fair cache latency among all users. Recent proposals have suggested reorganizing architecture in hybrid layouts with dense clusters of cores around cache banks to optimize data sharing. However even these NoCs will grow into large networks of multiple clusters as the core count rise into the hundreds. Therefore, additional cache policies must be implemented for optimal performance. In this paper we propose the Directional Migration Algorithm which is capable of small, rapid migrations between adjacent routers that will reduce cache latency in a hybrid NoC. We present the architecture in detail with simulations performed using the PARSEC benchmark suite against a theoretical Optimal Migration Algorithm. These experiments showed that while requiring a 87.1% smaller increase in cache space, the Directional Migration Algorithm could reduce cache latency to within 16% of that of a Optimal Migration Algorithm."
pub.1061535697,Integrated Planning of Biomass Inventory and Energy Production,"We consider an integrated biomass inventory and energy production problem that arises from scenario analysis in national energy planning. It addresses together two decision stages that have previously been kept distinct: the decisions on the purchase of biomass and the decisions on the production of electricity and heat of each power plant. We model the problem as a stochastic mixed 0-1 integer linear programming problem. Since practical instances of this problem are very large, we experimentally assess a relaxed formulation to obtain near-optimal solutions. In addition, we study a Benders decomposition that exploits the problem structure of the relaxed formulation. In a distributed computing architecture this decomposition allows to increase the number of included scenarios and thus to better address the uncertainty of the data. Computational results indicate that at parity of information the approximation provided by the relaxed model is good. However, by allowing to increase the amount of information treated it can provide more accurate predictions. On a multicore computing architecture a state-of-the-art MIP solver operating on the undecomposed model is sufficient to achieve similar performance as the Benders decomposition. However, the use of the solver in a distributed computing environment is not obvious and the Benders decomposition is a more easily implementable and scalable approach."
pub.1002405721,An analysis of the feasibility and benefits of GPU/multicore acceleration of the Weather Research and Forecasting model,"Summary There is a growing need for ever more accurate climate and weather simulations to be delivered in shorter timescales, in particular, to guard against severe weather events such as hurricanes and heavy rainfall. Due to climate change, the severity and frequency of such events – and thus the economic impact – are set to rise dramatically. Hardware acceleration using graphics processing units (GPUs) or Field‐Programmable Gate Arrays (FPGAs) could potentially result in much reduced run times or higher accuracy simulations. In this paper, we present the results of a study of the Weather Research and Forecasting (WRF) model undertaken in order to assess if GPU and multicore acceleration of this type of numerical weather prediction (NWP) code is both feasible and worthwhile. The focus of this paper is on acceleration of code running on a single compute node through offloading of parts of the code to an accelerator such as a GPU. The governing equations set of the WRF model is based on the compressible, non‐hydrostatic atmospheric motion with multi‐physics processes. We put this work into context by discussing its more general applicability to multi‐physics fluid dynamics codes: in many fluid dynamics codes, the numerical schemes of the advection terms are based on finite differences between neighboring cells, similar to the WRF code. For fluid systems including multi‐physics processes, there are many calls to these advection routines. This class of numerical codes will benefit from hardware acceleration. We studied the performance of the original code of the WRF model and proposed a simple model for comparing multicore CPU and GPU performance. Based on the results of extensive profiling of representative WRF runs, we focused on the acceleration of the scalar advection module. We discuss the implementation of this module as a data‐parallel kernel in both OpenCL and OpenMP. We show that our data‐parallel kernel version of the scalar advection module runs up to seven times faster on the GPU compared with the original code on the CPU. However, as the data transfer cost between GPU and CPU is very high (as shown by our analysis), there is only a small speed‐up (two times) for the fully integrated code. We show that it would be possible to offset the data transfer cost through GPU acceleration of a larger portion of the dynamics code. In order to carry out this research, we also developed an extensible software system for integrating OpenCL code into large Fortran code bases such as WRF. This is one of the main contributions of our work. We discuss the system to show how it allows the replacement of the sections of the original codebase with their OpenCL counterparts with minimal changes – literally only a few lines – to the original code. Our final assessment is that, even with the current system architectures, accelerating WRF – and hence also other, similar types of multi‐physics fluid dynamics codes – with a factor of up to five times is definitel"
pub.1125652903,Challenging Sequential Bitstream Processing via Principled Bitwise Speculation,"Many performance-critical applications traverse bitstreams with bitwise computations for better performance or higher space efficiency, such as multimedia processing and bitmap indexing. However, when these bitwise computations carry dependences, the entire bitstream traversal becomes serial, fundamentally limiting the scalability. In this work, we show that bitstream-carried dependences are actually ""breakable"" in many cases, with the adoption of a systematic treatment - principled bitwise speculation (PBS). The core idea of PBS stems from an analogy drawn between bitstream programs and sequential circuits, both of which transform binary sequences. In this new perspective, it becomes natural to model the dependences in bitstream programs with finite-state machines (FSM), a basic model for sequential circuits. To achieve this, PBS features an assembly of static analyses that reason about bitstream programs down to the bit level to identify the bits causing dependences, then it treats the value combinations of dependent bits as states to construct FSMs. The modeling, for the first time, enables the use of FSM speculation techniques to parallelize bitstream programs. Basically, by leveraging the state convergence of FSMs, the values of dependent bits can be predicted with much higher accuracies. In cases the prediction fails, PBS tries to directly ""rectify"" the wrong outputs based on bitwise logic, minimizing the mis-speculation costs. In addition, FSM shows even higher execution efficiency than the original program in some cases, making itself an optimized version to accelerate serial bitstream processing. We prototyped PBS using LLVM. Evaluation with real-world bitstream programs confirms the effectiveness of PBS, showing up to near-linear speedup on multicore/manycore machines."
pub.1053458166,Power and thermal management in massive multicore chips,"Continuing progress and integration levels in silicon technologies make possible complete end-user systems consisting of extremely high number of cores on a single chip targeting either embedded or high-performance computing. However, without new paradigms of energy- and thermally-efficient designs, producing information and communication systems capable of meeting the computing, storage and communication demands of the emerging applications will be unlikely. The broad topic of power and thermal management of massive multicore chips is actively being pursued by a number of researchers worldwide, from a variety of different perspectives, ranging from workload modeling to efficient on-chip network infrastructure design to resource allocation. Successful solutions will likely adopt and encompass elements from all or at least several levels of abstraction. Starting from these ideas, we consider a holistic approach in establishing the Power-Thermal-Performance (PTP) trade-offs of massive multicore processors by considering three inter-related but varying angles, viz., on-chip traffic modeling, novel Networks-on-Chip (NoC) architecture and resource allocation/mapping On-line workload (mathematical modeling, analysis and prediction) learning is fundamental for endowing the many-core platforms with self-optimizing capabilities [2][3]. This built-in intelligence capability of many-cores calls for monitoring the interactions between the set of running applications and the architectural (core and uncore) components, the online construction of mathematical models for the observed workloads, and determining the best resource allocation decisions given the limited amount of information about user-to-application-to-system dynamics. However, workload modeling is not a trivial task. Centralized approaches for analyzing and mining workloads can easily run into scalability issues with increasing number of monitored processing elements and uncore (routers and interface queues) components since it can either lead to significant traffic and energy overhead or require dedicated system infrastructure. In contrast, learning the most compact mathematical representation of the workload can be done in a distributed manner (within the proximity of the observation /sensing) as long as the mathematical techniques are flexible and exploit the mathematical characteristics of the workloads (degree of periodicity, degree of fractal and temporal scaling) [3]. As one can notice, this strategy does not postulate a-priori the mathematical expressions (e.g., a specific order of the autoregressive moving average (ARMA) model). Instead, the periodicity and fractality of the observed computation (e.g., instructions per cycles, last level cache misses, branch prediction successes and failures, TLB access/misses) and communication (request-reply latency, queues utilization, memory queuing delay) metrics dictate the number of coefficients, the linearity or nonlinearity of the dynamical state"
pub.1158161167,Adaptive Manta Ray Foraging Optimizer for Determining Optimal Thread Count on Many-core Architecture,"In high-performance computing, choosing the right thread count has a big impact on execution time and energy consumption. It is typically considered that the total number of threads should equal the number of cores to achieve maximum speedup on multicore processor systems. Changes in thread count at the hardware and OS levels influence memory bandwidth utilization, thread migration rate, cache miss rate, thread synchronization, and context switching rate. As a result, analyzing these parameters for complex multithreaded applications and finding the optimal number of threads is a major challenge. The suggested technique in this paper is an improvement on the traditional Manta Ray Foraging Optimization, a bio-inspired algorithm that has been used to handle a variety of numerical optimization problems. To determine the next probable solutions based on the present best solution, the suggested approach uses three foraging steps: chain, cyclone, and somersault. The proposed work is simulated on NVIDIA-DGX Intel Xeon-E5 2698-v4 using the well-known benchmark suite The Princeton Application Repository for Shared Memory Computers (PARSEC). The results show that, compared to the existing approach, the novel AMRFO-based prediction model can determine the ideal number of threads with very low overheads."
pub.1156044135,Detailed Modeling of Heterogeneous and Contention-Constrained Point-to-Point MPI Communication,"The network topology of modern parallel computing systems is inherently heterogeneous, with a variety of latency and bandwidth values. Moreover, contention for the bandwidth can exist on different levels when many processes communicate with each other. Many-pair, point-to-point MPI communication is thus characterized by heterogeneity and contention, even on a cluster of homogeneous multicore CPU nodes. To get a detailed understanding of the individual communication cost per MPI process, we propose a new modeling methodology that incorporates both heterogeneity and contention. First, we improve the standard max-rate model to better quantify the actually achievable bandwidth depending on the number of MPI processes in competition. Then, we make a further extension that more detailedly models the bandwidth contention when the competing MPI processes have different numbers of neighbors, with also non-uniform message sizes. Thereafter, we include more flexibility by considering interactions between intra-socket and inter-socket messaging. Through a series of experiments done on different processor architectures, we show that the new heterogeneous and contention-constrained performance models can adequately explain the individual communication cost associated with each MPI process. The largest test of realistic point-to-point MPI communication involves 8,192 processes and in total 2,744,632 simultaneous messages over 64 dual-socket AMD Epyc Rome compute nodes connected by InfiniBand, for which the overall prediction accuracy achieved is 84%."
pub.1094753396,Using Predictive Modeling for Cross-Program Design Space Exploration in Multicore Systems**This work was supported in part by the EC under grant HiPEAC IST-004408.,"The vast number of transistors available through modern fabrication technology gives architects an unprecedented amount of freedom in chip-multiprocessor (CMP) designs. However, such freedom translates into a design space that is impossible to fully, or even partially to any significant fraction, explore through detailed simulation. In this paper we propose to address this problem using predictive modeling, a well-known machine learning technique. More specifically we build models that, given only a minute fraction of the design space, are able to accurately predict the behavior of the remaining designs orders of magnitude faster than simulating them. In contrast to previous work, our models can predict performance metrics not only for unseen CMP configurations for a given application, but also for unseen configurations of a new application that was not in the set of applications used to build the model, given only a very small number of results for this new application. We perform extensive experiments to show the efficacy of the technique for exploring the design space of CMP's running parallel applications. The technique is used to predict both energy-delay and execution time. Choosing both explicitly parallel applications and applications that are parallelized using the thread-level speculation (TLS) approach, we evaluate performance on a CMP design space with about 95 million points using 18 benchmarks with up to 1000 training points each. For predicting the energy-delay metric, prediction errors for unseen configurations of the same application rangefrom 2.4% to 4.6% and for configurations of new applications from 3.1% to 4.9%."
pub.1095433602,ArchRanker: a Ranking Approach to Design Space Exploration,"Architectural Design Space Exploration (DSE) is a notoriously difficult problem due to the exponentially large size of the design space and long simulation times. Previously, many studies proposed to formulate DSE as a regression problem which predicts architecture responses (e.g., time, power) of a given architectural configuration. Several of these techniques achieve high accuracy, though often at the cost of significant simulation time for training the regression models. We argue that the information the architect mostly needs during the DSE process is whether a given configuration will perform better than another one in the presences of design constraints, or better than any other one seen so far, rather than precisely estimating the performance of that configuration. Based on this observation, we propose a novel ranking-based approach to DSE where we train a model to predict which of two architecture configurations will perform best. We show that, not only this ranking model more accurately predicts the relative merit of two architecture configurations than an ANN-based state-of-the-art regression model, but also that it requires much fewer training simulations to achieve the same accuracy, or that it can be used for and is even better at quantifying the performance gap between two configurations. We implement the framework for training and using this model, called ArchRanker, and we evaluate it on several DSE scenarios (unicore/multicore design spaces, and both time and power performance metrics). We try to emulate as closely as possible the DSE process by creating constraint-based scenarios, or an iterative DSE process. We find that ArchRanker makes29.68% to 54.43% fewer incorrect predictions on pairwise relative merit of configurations (tested with 79,800 configuration pairs) than an ANN-based regression model across all DSE scenarios considered (values averaged over all benchmarks for each scenario). We also find that, to achieve the same accuracy as ArchRanker, the ANN often requires three times more training simulations."
pub.1041576154,ArchRanker,"Architectural Design Space Exploration (DSE) is a notoriously difficult problem due to the exponentially large size of the design space and long simulation times. Previously, many studies proposed to formulate DSE as a regression problem which predicts architecture responses (e.g., time, power) of a given architectural configuration. Several of these techniques achieve high accuracy, though often at the cost of significant simulation time for training the regression models. We argue that the information the architect mostly needs during the DSE process is whether a given configuration will perform better than another one in the presences of design constraints, or better than any other one seen so far, rather than precisely estimating the performance of that configuration. Based on this observation, we propose a novel rankingbased approach to DSE where we train a model to predict which of two architecture configurations will perform best. We show that, not only this ranking model more accurately predicts the relative merit of two architecture configurations than an ANN-based state-of-the-art regression model, but also that it requires much fewer training simulations to achieve the same accuracy, or that it can be used for and is even better at quantifying the performance gap between two configurations We implement the framework for training and using this model, called ArchRanker, and we evaluate it on several DSE scenarios (unicore/multicore design spaces, and both time and power performance metrics). We try to emulate as closely as possible the DSE process by creating constraint-based scenarios, or an iterative DSE process. We find that ArchRanker makes 29:68% to 54:43% fewer incorrect predictions on pairwise relative merit of configurations (tested with 79,800 configuration pairs) than an ANN-based regression model across all DSE scenarios considered (values averaged over all benchmarks for each scenario). We also find that, to achieve the same accuracy as ArchRanker, the ANN often requires three times more training simulations"
pub.1095754283,An Opportunistic Prediction-based Thread Scheduling to Maximize Throughput/Watt in AMPs,"The importance of dynamic thread scheduling is increasing with the emergence of Asymmetric Multicore Processors (AMPs). Since the computing needs of a thread often vary during its execution, a fixed thread-to-core assignment is sub-optimal. Reassigning threads to cores (thread swapping) when the threads start a new phase with different computational needs, can significantly improve the energy efficiency of AMPs. Although identifying phase changes in the threads is not difficult, determining the appropriate thread-to-core assignment is a challenge. Furthermore, the problem of thread reassignment is aggravated by the multiple power states that may be available in the cores. To this end, we propose a novel technique to dynamically assess the program phase needs and determine whether swapping threads between core-types and/or changing the voltage/frequency levels (DVFS) of the cores will result in higher throughput/Watt. This is achieved by predicting the expected throughput/Watt of the current program phase at different voltage/frequency levels on all the available core-types in the AMP. We show that the benefits from thread swapping and DVFS are orthogonal, demonstrating the potential of the proposed scheme to achieve significant benefits by seamlessly combining the two. We illustrate our approach using a dual-core High-Performance (HP)/Low-Power (LP) AMP with two power states and demonstrate significant throughput/Watt improvement over different baselines."
pub.1095599745,Application Behavior Aware Re-Reference Interval Prediction for Shared LLC,"In modern CMPs, Last Level Cache (LLC) is shared among cores for better utilization. Interference among data, mapped from multiple cores, increases conflict misses in shared LLCs. Such interference is highly dependent on cache behavior of applications and access rate difference among them. We observe that interference among applications is not eliminated completely even using existing state-of-the-art mechanism for applications having high cache access rate difference and different memory characteristic. Applications with highly diverse cache behavior can be observed in homogeneous as well as heterogeneous multicore processors. Streaming applications, having high access rate, can still interfere with cache friendly applications having low access rate. We propose Application behavior aware replacement policy that predicts re-reference interval of the block based on block locality as well as application behavior. By providing more priority to application behavior over cache block locality, we reduce interference between streaming applications and cache friendly applications. Our evaluation on set of SPEC CPU2006 workloads running on CMP with shared LLC shows that proposed replacement policy outperforms the state-of-the-art replacement policy, on throughput metric. We achieve performance gain up to 16.2 % over SRRIP for application mixes of cache-friendly and streaming applications. Our replacement policy achieves maximum of 59.9% reduction in number of misses as compared to SRRIP with average misses per kilo instructions (mpki) reduction of 5.9% over SRRIP."
pub.1166491780,REPFS: Reliability-Ensured Personalized Function Scheduling in Sustainable Serverless Edge Computing,"In recent years, serverless edge computing has been widely employed in the deployments of Internet-of-things (IoT) applications. Despite considerable research efforts in this field, existing works fail to jointly consider essential factors such as energy, reliability, personalized user requirements, and stochastic application executions. This oversight results in an inefficient utilization of computation and communication resources within serverless edge computing networks, subsequently diminishing the profit of service providers and degrading the quality-of-experience (QoE) of end users. In this paper, we explore the problem of reliability-ensured personalized function scheduling (REPFS) to jointly optimize the profit of service providers and the holistic QoE of end users in sustainable serverless edge computing. A personality-driven user QoE prediction method is first designed to accurately estimate the QoE of individual end users with differentiated personality types. Afterward, a deterministic function scheduling policy is developed on the problem-specific augmented non-dominated sorting genetic algorithm II (PSA-NSGA-II). Given the inherent uncertainty of application executions, a stochastic function scheduling strategy that can be easily parallelized for modern multicore scheduler platforms is also devised to accelerate solution generation for stochastic applications. Experimental results show that our deterministic function scheduling policy achieves 15% performance enhancement compared with representative multiobjective evolutionary algorithms. Furthermore, our stochastic function scheduling strategy promotes the service profit by 78% and the holistic user QoE by 118% on average compared with the developed deterministic scheduling policy."
pub.1037908871,Dynamic Look Ahead Compilation: A Technique to Hide JIT Compilation Latencies in Multicore Environment,"Object-code virtualization, commonly used to achieve software portability, relies on a virtual execution environment, typically comprising an interpreter used for initial execution of methods, and a JIT for native code generation. The availability of multiple processors on current architectures makes it attractive to perform dynamic compilation in parallel with application execution. The major issue is to decide at runtime which methods to compile ahead of execution, and how much time to invest in their optimization. This research introduces an abstract model, termed Dynamic Look Ahead (DLA) compilation, which represents the available information on method calls and computational weight as a weighted graph. The graph dynamically evolves as computation proceeds. The model is then instantiated by specifying criteria for adaptively choosing the method compilation order. The DLA approach has been applied within our dynamic compiler for .NET. Experimental results are reported and analyzed, for both synthetic programs and benchmarks. The main finding is that a careful choice of method-selection criteria, based on light-weight program analysis and execution tracing, is essential to mask compilation times and to achieve higher overall performances. On multi-processors, the DLA approach is expected to challenge the traditional virtualization environments based on bytecode interpretation and JITing, thus bridging the gap between ahead-of-time and just-in-time translation."
pub.1007272292,Past and future of hardware and architecture,"We start by looking back at 50 years of computer architecture, where philosophical debates on instruction sets (RISC vs. CISC, VLIW vs. RISC) and parallel architectures (NUMA vs clusters) were settled with billion dollar investments on both sides. In the second half, we look forward. First, Moore's Law is ending, so the free ride is over software-oblivious increasing performance. Since we've already played the multicore card, the most-likely/only path left is domain-specific processors. The memory system is radically changing too. First, Jim Gray's decade-old prediction is finally true: ""Tape is dead; flash is disk; disk is tape."" New ways to connect to DRAM and new non-volatile memory technologies promise to make the memory hierarchy even deeper. Finally, and surprisingly, there is now widespread agreement on instruction set architecture, namely Reduced Instruction Set Computers. However, unlike most other fields, despite this harmony has been no open alternative to proprietary offerings from ARM and Intel. RISC-V (""RISC Five"") is the proposed free and open champion. It has a small base of classic RISC instructions that run a full open-source software stack; opcodes reserved for tailoring an System-On-a-Chip (SOC) to applications; standard instruction extensions optionally included in an SoC; and it is unrestricted: there is no cost, no paperwork, and anyone can use it. The ability to prototype using ever-more-powerful FPGAs and astonishingly inexpensive custom chips combined with collaboration on open-source software and hardware offers hope of a new golden era for hardware/software systems."
pub.1046595917,"Advances in Computer Systems Architecture, 12th Asia-Pacific Conference, ACSAC 2007, Seoul, Korea, August 23-25, 2007. Proceedings","On behalf of the program and organizing committee members of this conference, we th are pleased to present you with the proceedings of the 12 Asia-Pacific Computer Systems Architecture Conference (ACSAC 2007), which was hosted in Seoul, Korea on August 23-25, 2007. This conference has traditionally been a forum for leading researchers in the Asian, American and Oceanian regions to share recent progress and the latest results in both architectural and system issues. In the past few years the c- ference has become more international in the sense that the geographic origin of p- ticipants has become broader to include researchers from all around the world, incl- ing Europe and the Middle East. This year, we received 92 paper submissions. Each submission was reviewed by at least three primary reviewers along with up to three secondary reviewers. The total number of completed reviews reached 333, giving each submission 3.6 reviews on average. All the reviews were carefully examined during the paper selection process, and finally 26 papers were accepted, resulting in an acceptance rate of about 28%. The selected papers encompass a wide range of topics, with much emphasis on hardware and software techniques for state-of-the-art multicore and multithreaded architectures."
pub.1037558509,Multi-core Structural SVM Training,"Many problems in natural language processing and computer vision can be framed as structured prediction problems. Structural support vector machines (SVM) is a popular approach for training structured predictors, where learning is framed as an optimization problem. Most structural SVM solvers alternate between a model update phase and an inference phase (which predicts structures for all training examples). As structures become more complex, inference becomes a bottleneck and thus slows down learning considerably. In this paper, we propose a new learning algorithm for structural SVMs called DEMIDCD that extends the dual coordinate descent approach by decoupling the model update and inference phases into different threads. We take advantage of multicore hardware to parallelize learning with minimal synchronization between the model update and the inference phases.We prove that our algorithm not only converges but also fully utilizes all available processors to speed up learning, and validate our approach on two real-world NLP problems: part-of-speech tagging and relation extraction. In both cases, we show that our algorithm utilizes all available processors to speed up learning and achieves competitive performance. For example, it achieves a relative duality gap of 1% on a POS tagging problem in 192 seconds using 16 threads, while a standard implementation of a multi-threaded dual coordinate descent algorithm with the same number of threads requires more than 600 seconds to reach a solution of the same quality."
pub.1138870575,ODMDEF: On-Device Multi-DNN Execution Framework Utilizing Adaptive Layer-Allocation on General Purpose Cores and Accelerators,"On-device DNN processing has been common interests in the field of autonomous driving research. For better accuracy, both the number of DNN models and the model-complexity have been increased. To properly respond to this, hardware platforms structured with multicore-based CPUs and DNN accelerators have been released, and the GPU is generally used as an accelerator. When multiple DNN workloads are sporadically requested, the GPU can be easily oversubscribed, thereby leading to an unexpected performance bottleneck. We propose an on-device CPU-GPU co-scheduling framework for multi-DNN execution to remove the performance barrier precluding DNN executions from being bounded by the GPU. Our framework fills up the unused CPU cycles with DNN computations to ease the computational burden of the GPU. To provide seamless computing environment for the two different core types, the framework formats each layer execution according to the computational methods supported by CPU and GPU cores. To cope with irregular arrivals of DNN workloads, and to accommodate their fluctuating demands for hardware resources, our framework dynamically selects the best fit core type after making a comparative judgement between the current availabilities of the two core types. During the core selection time, offline-trained prediction models are utilized to get precisely predicted execution time of the issued layer. Our framework mitigates the fact that even the same DNN models can have large performance deviations due to the nature of the process scheduler of the underlying OS which is GPU-agnostic. In addition, the framework minimizes the memory copy overhead inevitably occurring in the data synchronization phase between the heterogeneous cores. To do so, we further analyze GPU-to-CPU and CPU-to-GPU data transfer cases separately, and then apply the solution that best suits each case. For multi-DNN inference jobs with the NVIDIA Jetson AGX Xavier platform, our framework speeds up the execution time by up to 46.6% over the GPU-only solution."
pub.1124782532,Soft Error Resilience in Chip Multiprocessor Cache using a Markov Model based Re-usability Predictor,"Power consumption in the dense chip multiprocessors restricts the operational life of the battery operated devices. Voltage scaling for ensuring longer battery life exhibits hard and soft errors in cache. Hard faults, early detectable at boot time, are tractable but soft errors, being unpredictable in nature, are indeed hard to handle. Error correcting codes for soft error mitigation perform effectively for single event upsets (SEUs) but future technology nodes are anticipated to have significant multi bit upsets (MBUs) due to miniaturization which demands additional system level protection. Several redundancy based schemes have been proposed but none of them reported any scalable solution for shared NUCA cache in multicore and suffered from either low coverage due to partial redundancy or performance degradation due to capacity loss in full coverage. This work addresses both these issues by ensuring full error coverage with minimum cache bypass. Errors are detected using simple error detection technique like CRC and the dirty as well as reusable clean words are replicated either locally in same tile or globally to remote tiles to ensure complete error coverage. Word re-usability is calculated using a Markov Chain based novel re-usability prediction mechanism by analysing the cache access pattern. An invalidation strategy is applied that invalidates non-reusable words for reducing the vulnerable time to soft errors. A re-usability aware replacement policy is also designed that replaces the line with lowest re-usability, calculated at line level. The proposed technique has been evaluated in Multi2Sim 5.0 simulation framework with the benchmark programs in SPEC CPU2000 suite. The results indicate on average 83.33% decrease of vulnerability for integer benchmarks and 87.09% for floating point benchmarks, with full multi-bit error coverage, at the cost of 4.99% area, 5.54% dynamic power and 6.55% leakage power overheads with negligible performance penalty."
pub.1101171587,Power Conversion Efficiency-Aware Mapping of Multithreaded Applications on Heterogeneous Architectures: A Comprehensive Parameter Tuning,"Heterogeneous Multicore Processors (HMPs) are comprised of multiple core types (small vs. big core architectures) with various performance and power characteristics which offer the flexibility to assign each thread to a core that provides the maximum energy-efficiency. Although this architecture provides more flexibility for the running application to determine the optimal run-time settings that maximize energy-efficiency, due to the interdependence of various tuning parameters such as the type of core, run-time voltage and frequency, and the number of threads, the scheduling becomes more challenging. More importantly, the impact of Power Conversion Efficiency (PCE) of the On-Chip Voltage Regulators (OCVRs) is another important parameter that makes it more challenging to schedule multithreaded applications on HMPs. In this paper, the importance of concurrent optimization and fine-tuning of the circuit and architectural parameters for energy-efficient scheduling on HMPs is addressed to harness the power of heterogeneity. In addition, the scheduling challenges for multithreaded applications are investigated for HMP architectures that account for the impact of power conversion efficiency. A highly accurate learning-based model is developed for energy-efficiency prediction to guide the scheduling decision. Using the predictive model, we further develop a PCE-aware scheduling scheme is developed for effective mapping of multithreaded applications onto an HMP. The results indicate that the proposed learning-based scheme outperforms the state of the art solution by 10% when there is no PCE gap between big and little cores. The energy-efficiency improves up to 60% when the PCE gap between big and little cores increases."
pub.1057633295,Three-Dimensional Analyses of Excavation Support System for the Stata Center Basement on the MIT Campus,"The basement of the Stata Center building on the MIT campus required 12.8 m deep excavations covering a large open-plan site and underlain by more than 25 m of lightly overconsolidated Boston Blue Clay. The excavations were supported by a floating, perimeter diaphragm and braced with a system of internal corner struts, rakers, and tieback anchors. The project involved a complex sequence of berms, access ramps, and phased construction of the concrete mat foundation. One of the key goals of the design was to limit ground movements in order to prevent damage to adjacent structures, including the Alumni Pool building, which was founded on shallow caissons and located less than 1.5 m from the south wall. Lateral wall movements and building settlements were closely monitored throughout construction, while photos from a network of webcams located around the open-plan site provide a detailed time history of the construction processes. This paper describes the development of a comprehensive three-dimensional (3D) finite-element (FE) model for the Stata Center basement excavation, which has been enabled by recent advances made available in a FE software package, including efficient multicore iterative solving capabilities, importing of geometric data from computer-aided design files, and use of embedded pile elements to represent tieback anchors. The analyses highlight the effects of the 3D excavation and support geometry on wall and ground movements. The base case results using a simple elasto-plastic Mohr–Coulomb soil model with undrained conditions in the clay are generally in very good agreement with measured performance. The effects of refined constitutive modeling and partial drainage within the clay have a secondary role in numerical predictions for this project."
pub.1142516906,Improving Autonomous Nano-Drones Performance via Automated End-to-End Optimization and Deployment of DNNs,"The evolution of energy-efficient ultra-low-power (ULP) parallel processors and the diffusion of convolutional neural networks (CNNs) are fueling the advent of autonomous driving nano-sized unmanned aerial vehicles (UAVs). These sub-10cm robotic platforms are envisioned as next-generation ubiquitous smart-sensors and unobtrusive robotic-helpers. However, the limited computational/memory resources available aboard nano-UAVs introduce the challenge of minimizing and optimizing vision-based CNNs – which to date require error-prone, labor-intensive iterative development flows. This work explores methodologies and software tools to streamline and automate all the deployment of vision-based CNN navigation on a ULP multicore system-on-chip acting as a mission computer on a Crazyflie 2.1 nano-UAV. We focus on the deployment of PULP-Dronet (Palossi et al., 2019), a state-of-the-art CNN for autonomous navigation of nano-UAVs, from the initial training to the final closed-loop evaluation. Compared to the original hand-crafted CNN, our results show a $2\times $ reduction of memory footprint and a speedup of $1.6\times $ in inference time while guaranteeing the same prediction accuracy and significantly improving the behavior in the field, achieving: i) obstacle avoidance with a peak braking-speed of 1.65m/s and improving the speed/braking-space ratio of the baseline, ii) free flight in a familiar environment up to 1.96m/s (0.5m/s for the baseline), and iii) lane following on a path featuring a 90deg turn – all while using for computation less than 1.6% of the drone’s power budget. To foster new applications and future research, we open-source all the software design in a ready-to-run project compatible with the Crazyflie 2.1."
pub.1116869851,Accelerating Wild Fire Simulator Using GPU,"In the last years, forest fire spread simulators have proven to be very promising tools in the fight against these disasters. Due to the necessity to achieve realistic predictions of the fire behavior in a relatively short time, execution time may be reduced. Moreover, several studies have tried to apply the computational power of GPUs (Graphic Processors Units) to accelerate the simulation of the propagation of fires. Most of these studies use forest fires simulators based on Cellular Automata (CA). CA approaches are fast and its parallelization is relatively easy; conversely, they suffer from precision lack. Elliptical wave propagation is an alternative approach for performing more reliable simulations. Unfortunately, its higher complexity makes their parallelization challenging. Here we explore two different parallel strategies based on Elliptical wave propagation forest fire simulators; the multicore architecture of CPU (Central Processor Unit) and the computational power of GPUs to improve execution times. The aim of this work is to assess the performance of the simulation of the propagation of forest fires on a CPU and a GPU, and finding out when the execution on GPU is more efficient than on CPU. In this study, a fire simulator has been designed based on the basic model for one point evolution in the FARSITE simulator. As study case, a synthetic fire with an initial circular perimeter has been used; the wind, terrain and vegetation conditions have been maintained constant throughout the simulation. Results highlighted that GPUs allow obtaining more accurate results while reducing the execution time of the simulations."
pub.1094629188,CNN-Based Object Detection Solutions for Embedded Heterogeneous Multicore SoCs,"This paper surveys how to use Convolutional Neural Networks (CNN) to hypothesize object location and categorization from images or videos in mobile heterogeneous SoCs. Recently a variety of CNN-based object detection frameworks have demonstrated both increasing accuracy and speed. Though they are making fast progress in high quality image recognition, state-of-the-art CNN-based detection frameworks seldom discuss their hardware-depended aspects and the cost-effectiveness of real-time image analysis in off-the-shelf low-power devices. As the focus of deep learning and convolutional neural nets is shifting to the embedded or mobile applications with limited power and computational resources, scaling down object detection framework and CNNs is becoming a new and important direction. In this work we conduct a comprehensive comparative study of state-of-the-art real-time object detection frameworks about their performance, cost-effectiveness/energy-efficiency (in the metric of mAP/Wh) in off-the-shelf mobile GPU devices. Based on the analysis results and observation in investigation, we propose to adjust the design parameters of such frameworks and employ a design space exploration procedure to maximize the energy-efficiency (mAP/Wh) of real-time object detection solution in mobile GPUs. As shown in the benchmarking result, we successfully boost the energy-efficiency of multiple popular CNN-based detection solutions by maximizing the utility of computation resources of SoC and trading-off between prediction accuracy and energy cost. In the second Low-Power Image Recognition Challenge (LPIRC), our system achieved the best result measured in mAP/Energy on the embedded Jetson TX1 CPU+GPU SoC."
pub.1131505583,Optimizing high-resolution Community Earth System Model on a heterogeneous many-core supercomputing platform,"Abstract. With semiconductor technology gradually approaching its physical and thermal limits, recent supercomputers have adopted major
architectural changes to continue increasing the performance through more
power-efficient heterogeneous many-core systems. Examples include Sunway
TaihuLight that has four management processing elements (MPEs) and 256
computing processing elements (CPEs) inside one processor and Summit that has
two central processing units (CPUs) and six graphics processing units (GPUs)
inside one node. Meanwhile, current high-resolution Earth system models that
desperately require more computing power generally consist of millions of
lines of legacy code developed for traditional homogeneous multicore
processors and cannot automatically benefit from the advancement of
supercomputer hardware. As a result, refactoring and optimizing the legacy
models for new architectures become key challenges along the road of taking
advantage of greener and faster supercomputers, providing better support for
the global climate research community and contributing to the long-lasting
societal task of addressing long-term climate change. This article reports
the efforts of a large group in the International Laboratory for
High-Resolution Earth System Prediction (iHESP) that was established by the
cooperation of Qingdao Pilot National Laboratory for Marine Science and
Technology (QNLM), Texas A&M University (TAMU), and the National Center for
Atmospheric Research (NCAR), with the goal of enabling highly efficient
simulations of the high-resolution (25 km atmosphere and 10 km ocean)
Community Earth System Model (CESM-HR) on Sunway TaihuLight. The refactoring
and optimizing efforts have improved the simulation speed of CESM-HR from 1 SYPD (simulation years per day) to 3.4 SYPD (with output disabled) and
supported several hundred years of pre-industrial control simulations. With
further strategies on deeper refactoring and optimizing for remaining
computing hotspots, as well as redesigning architecture-oriented
algorithms, we expect an equivalent or even better efficiency to be gained on the
new platform than traditional homogeneous CPU platforms. The refactoring and
optimizing processes detailed in this paper on the Sunway system should have
implications for similar efforts on other heterogeneous many-core systems
such as GPU-based high-performance computing (HPC) systems."
pub.1040005998,Coarse Grain Task Parallelization of Earthquake Simulator GMS Using OSCAR Compiler on Various Cc-NUMA Servers,"This paper proposes coarse grain task parallelization for a earthquake simulation program using Finite Difference Method to solve the wave equations in 3-D heterogeneous structure or the Ground Motion Simulator (GMS) on various cc-NUMA servers using IBM, Intel and Fujitsu multicore processors. The GMS has been developed by the National Research Institute for Earth Science and Disaster Prevention (NIED) in Japan. Earthquake wave propagation simulations are important numerical applications to save lives through damage predictions of residential areas by earthquakes. Parallel processing with strong scaling has been required to precisely calculate the simulations quickly. The proposed method uses the OSCAR compiler for exploiting coarse grain task parallelism efficiently to get scalable speed-ups with strong scaling. The OSCAR compiler can analyze data dependence and control dependence among coarse grain tasks, such as subroutines, loops and basic blocks. Moreover, locality optimizations considering the boundary calculations of FDM and a new static scheduler that enables more efficient task schedulings on cc-NUMA servers are presented. The performance evaluation shows 110 times speed-up using 128 cores against the sequential execution on a POWER7 based 128 cores cc-NUMA server Hitachi SR16000 VM1, 37.2 times speed-up using 64 cores against the sequential execution on a Xeon E7-8830 based 64 cores cc-NUMA server BS2000, 19.8 times speed-up using 32 cores against the sequential execution on a Xeon X7560 based 32 cores cc-NUMA server HA8000/RS440, 99.3 times speed-up using 128 cores against the sequential execution on a SPARC64 VII based 256 cores cc-NUMA server Fujitsu M9000, 9.42 times speed-up using 12 cores against the sequential execution on a POWER8 based 12 cores cc-NUMA server Power System S812L."
pub.1100134629,Additivity: A Selection Criterion for Performance Events for Reliable Energy Predictive Modeling,"Performance events or performance monitoring counters (PMCs) are now the dominant predictor variables for modeling energy consumption. Modern hardware processors provide a large set of PMCs. Determination of the best subset of PMCs for energy predictive modeling is a non-trivial task given the fact that all the PMCs can not be determined using a single application run. Several techniques have been devised to address this challenge. While some techniques are based on a statistical methodology, some use expert advice to pick a subset (that may not necessarily be obtained in one application run) that, in experts' opinion, are significant contributors to energy consumption. However, the existing techniques have not considered a fundamental property of predictor variables that should have been applied in the first place to remove PMCs unfit for modeling energy. We address this oversight in this paper. We propose a novel selection criterion for PMCs called additivity, which can be used to determine the subset of PMCs that can potentially be used for reliable energy predictive modeling. It is based on the experimental observation that the energy consumption of a serial execution of two applications is the sum of energy consumptions observed for the individual execution of each application. A linear predictive energy model is consistent if and only if its predictor variables are additive in the sense that the vector of predictor variables for a serial execution of two applications is the sum of vectors for the individual execution of each application. The criterion, therefore, is based on a simple and intuitive rule that the value of a PMC for a serial execution of two applications is equal to the sum of its values obtained for the individual execution of each application. The PMC is branded as non-additive on a platform if there exists an application for which the calculated value differs significantly from the value observed for the application execution on the platform. The use of non-additive PMCs in a model renders it inconsistent. We study the additivity of PMCs offered by the popular state-of-the-art tools, Likwid and PAPI, by employing a detailed experimental methodology on a modern Intel Haswell multicore server CPU. We show that many PMCs in Likwid and PAPI that are widely used in models as key predictor variables are non-additive. This brings into question the reliability and the reported prediction accuracy of these models."
pub.1037630519,W physics at the LHC with FEWZ 2.1,"We present an updated version of the FEWZ (Fully Exclusive W and Z production) code for the calculation of W± and γ∗/Z production at next-to-next-to-leading order in the strong coupling. Several new features and observables are introduced, and an order-of-magnitude speed improvement over the performance of FEWZ 2.0 is demonstrated. New phenomenological results for W± production and comparisons with LHC data are presented, and used to illustrate the range of physics studies possible with the features of FEWZ 2.1. We demonstrate with an example the importance of directly comparing fiducial-region measurements with theoretical predictions, rather than first extrapolating them to the full phase space.Program summaryProgram title: FEWZ 2.1Catalogue identifier: AEJP_v1_1Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEJP_v1_1.htmlProgram obtainable from: CPC Program Library, Queen’s University, Belfast, N. IrelandLicensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.htmlNo. of lines in distributed program, including test data, etc.: 12003230No. of bytes in distributed program, including test data, etc.: 769Distribution format: tar.gzProgramming language: Fortran 77, C++, Python 2.4.Computer: x86/x86-64.Operating system: Unix/Linux, Mac OSX.RAM: 200 MbytesClassification: 11.1.External routines: CUBA (included), LHAPDF (optional)Catalogue identifier of previous version: AEJP_v1_0Journal reference of previous version: Comput. Phys. Comm. 182 (2011) 2388Does the new version supersede the previous version?: YesNature of problem: Calculation of hadroproduction of W bosons, with differential distributions, at next-to-next-to-leading order in the strong coupling.Solution method: Integral reduction, sector decomposition, numerical integrationReasons for new version: Reintroduction of W boson to FEWZ 2Summary of revisions: Addition of W boson production, now can run in W or Z/gamma mode. LHAPDF interface added. Large speed improvements achieved through caching repeat function calls. New observables and histograms. Improved histogram binning.Additional comments: Running with all histograms on requires approx. 1 GB of disk space to store intermediate files.!!!!! The distribution file for this program is over 290 Mbytes and therefore is not delivered directly when download or e-mail is requested. Instead an html file giving details of how the program can be obtained is sent.!!!!!Running time: 2 hours to achieve NNLO precision in cross section on multicore machines, up to a few days for quality kinematic distributions. Cluster running is considerably faster."
pub.1017424815,"Supercomputing for the Future, Supercomputing from the Past (Keynote)","Supercomputing is a zero billion dollar market but a huge driving boost for technology and systems for the future.Today, applications in the engineering and scientific world are the major users of the huge computational power offered by supercomputers. In the future, the commercial and business applications will increasingly have such high computational demands.Supercomputers, once built on technology developed from scratch have now evolved towards the integration of commodity components. Designers of high end systems for the future have to closely monitor the evolution of mass market developments. Such trends also imply that supercomputers themselves provide requirements for the performance and design of those components.The current technology integration capability is actually allowing for the use of supercomputing technologies within a single chip that will be used in all markets. Stressing the high end systems design will thus help develop ideas and techniques that will spread everywhere. A general observation about supercomputers in the past is their relatively static operation (job allocation, interconnect routing, domain decompositions, loop scheduling) and often little coordination between levels.Flexibility and dynamicity are some key ideas that will have to be further stressed in the design of future supercomputers. The ability to accept and deal with variance (rather than stubbornly trying to eliminate it) will be important. Such variance may arise from the actual manufacturing/operation mode of the different components (chip layout, MPI internals, contention for shared resources such as memory or interconnect, ...) or the expectedly more and more dynamic nature of the applications themselves. Such variability will be perceived as load imbalance by an actual run. Properly addressing this issue will be very important.The application behavior typically shows repetitive patterns of resource usage. Even if such patterns may be dynamic, very often the timescales of such variability allows for the application of prediction techniques and matching resources to actual demands. Our foreseen systems will thus have dynamic mechanisms to support fine grain load balancing, while the policies will be applied at a coarse granularity.As we approach fundamental limits in single processor design specially in terms of the performance/power ratio, multicore chips and massive parallelism will become necessary to achieve the required performance levels. A hierarchical structure is one of the unavoidable approaches to future systems design. Hierarchies will show up at all levels from processor to node and system design, both in the hardware and in the software.The development of programming models (extending current ones or developing new ones) faces a challenge of providing the mechanism to express a certain level of hierarchy (but not too much/detailed) that can be matched by compilers, run times and OSs to the potentially very different underlying archite"
pub.1166480725,Automated Peripheral Smear Recognition and Classification of LGL Leukemia Using Machine Learning," Background: Machine learning can augment clinical decision support, especially in cases where complex diagnostic criteria are necessary. In making a diagnosis of LGL leukemia, a pathologist must review the peripheral smear thoroughly, which is a tedious process. Supervised machine learning has been successfully implemented in such cases by training on very large image-based datasets, followed by a corrective phase of updating the model weights and finally demonstrating a high degree of successful predictions in an independent dataset. We sought to assess whether a supervised machine learning model can automate histopathological image analysis and provide a degree of confidence in supporting a pathologist's manual review. Methods: We searched the published literature from 2016-2022 for high-definition peripheral smear images with confirmed cases. The samples were downloaded, pre-processed with the Segment Anything Model (SAM), and labeled as training, testing, or validation data. Non-pathological controls, defined as peripheral smears confirmed negative for LGLL, were also obtained from published literature and labeled manually. The neural network was written and trained on TensorFlow using the open-source Keras library with parallel threading and on a multicore GPU machine. The resulting predictive model had a binary classification task: Does the presented image have cells that resemble LGL cells, returning true or false. Every “true” label returned from the classifier also creates a stack of probable positives that need pathology review. Feedback from each pathologist review further updates the model weights to increase the probability of successful prediction in the future. Results: We screened 57 publications to obtain 163 high-definition peripheral smear images that had confirmed LGLL and 100 non-pathological smears. Two cohorts were created: 200 smears in a training cohort comprised of 100 LGLL smears and 100 non-pathological smears, and 63 smears in the validation cohort. To remain agnostic towards image orientation, we created 7 views of each smear, and 1400 total images underwent feature extraction using SAM. The resulting data was labeled and used for model training and our process is depicted visually below in Figure 1. During validation, our model accurately predicted LGL-cells in 56/63 cases with an 8% false positive rate, 2% false negatives and 2% unclassifiable. After updating model weights and re-training with Adaptive Boosting, the accuracy increased to 60/63 smears with a false-positive rate of 3%. Our trained classification model achieved an AUC of 0.8144. Conclusion: Supervised machine learning model with Adaptive Boosting can be used to label a peripheral smear image with a high likelihood of containing LGL cells and, in turn, help a pathologist review smears with higher priority in a timely fashion. Our initial training data was limited, which hindered the performance of our model; however, with a sufficiently large training"
pub.1044733951,Memory management challenges in the power-aware computing era,"Process technology has been driving the computer architecture industry during the last two decades. Until recently, most of the micro-architectures were focused on achieving best performance, usually for a single threaded application, within a given budget of transistors. Recently, power consumption and power density start to be an important factor in the design of new processors. This new trend, presents new challenges for both the hardware developer community as well as for the software community.Power consumption can be divided into two components: static and dynamic. The static power, also known as leakage power, is the power consumed when the logic or the memory circuits are not in used while the dynamic power represents the power consumed when the logic or memory circuits are active. In the past, the static power consumption was negligible and so deserves no special treatment. As the size of transistor shrinks, static power starts to be more significant and under some usage models it can even governs the overall power consumption of the system. In order to control both static and dynamic power consumption, different techniques were proposed, such as the use of advanced circuits that were optimized for low power (this is out of the scope of my presentation), the use of advance power management techniques, uses of new computer architectures and more. This presentation will be focused on power management in general and on power management of memory subsystem in particular.Improving the power consumption of the memory subsystem has been very active research and development area during the last few years. At the micro-architecture level, different methods have been proposed; e.g., Drowsy cache [1,2] calls to lower the power consumption of parts of the memory when not expected to be used in the near future. The power saving is achieved in the cost of increasing the access time to those parts of the memory if the prediction was incorrect. The Decay caches is another technique that calls to farther save power of ""un-used"" memory by ""cutting the power"" to these cells on the cost of loosing their content[3]. Intel announced lately a new technique called ""smart memory control"" that combines the power management mechanism together with leakage control of the memory [4]. Few other works, such as [5] suggest combining software techniques with hint the hardware what memory is needed and even to compress the data and the instruction in order to reduce the footprint of the program [6].Another important aspect of the power crisis on memory management is the intensive usage of parallel system. While in the past, fast improvement in performance was achieved by accelerating the speed of the processor, when power consumption and power density limitations are considered, the improvement in frequency should be limited and so the ""natural"" way to keep performance improvement at the same pace is to use parallel execution [7]. Adding more processors to the system req"
pub.1141675465,A Novel Hybrid Chaotic Map–Based Proactive RSA Cryptosystem in Blockchain,"Blockchain is an up-and-coming technology, and it is exploring its existence in all our day-to-day lives, particularly for business applications. Due to this, security has become a major and a higher priority concern during the transaction process of data, cryptocurrency, etc. Achieving integrity, authentication, and non-repudiation with blockchain technology through digital signatures are the factors for its success. Hence, we imply a novel hybrid chaotic map–based proactive RSA/digital signature scheme for a better securable transaction of blocks compared to earlier works. Faster encryption and decryption could be expected by this novel approach. This could surely be resistant to brute force attacks, chosen plain text attacks and chosen cipher text attacks. OPENMP and OPENSSL libraries could be employed for good performance and security in a multicore platform. Chaotic map theory has its unique characteristics in the design of highly secure asymmetric encryption and decryption processes, with lesser computational cost in lesser time. The strength of RSA lies in the prime factorization of larger prime numbers in the generation of public and private keys. RSA has a severe backlog in computational time; while the length of the prime number is longer, to ensure higher security, the longer one is preferable. Hence to speedup the factorization process, chaotic mapping theory has been synchronized with the RSA cryptosystem. Second, chaotic mapping theory provides an enhanced avalanche effect, which leads to a two-factor RSA encryption process. Chaotic maps are highly sensitive to initial values and control parameters. The unique feature of inclusion of chaotic maps is that any slight change in the initial conditions of the plain text would cause a remarkable deviation in the message digest. This nature would make the hacker limit their prediction ability. Computational time, cost, and level of resistance to attacks are the factors to be considered to measure the efficiency of our new approach when compared to other RSA approaches. The work was simulated using MATLAB and it is proved that that the proposed RSA with hybrid GLT chaotic maps is 1.53 times and 1.27 times faster than the existing ones. This chapter proposed an enhanced security framework for blockchain. Authentication is achieved by employing the RSA algorithm with hybrid chaotic mapping for the key generation process. An RSA-based digital signature scheme would enable a robust, efficient, and auditable key generation environment in blockchain. RSA, during its first stage of encryption, would be resistant to differential and cryptographic attacks Gauss map, logistic map, and tent map are the chaotic maps used to generate a pseudo-random number in the RSA algorithm. A solid and a consistent blockchain-based private PDP scheme is developed by merging blockchain and RSA. Blockchain uses the Merkle tree function and the Merkel root field that presents the hash value of the current block. In the"
