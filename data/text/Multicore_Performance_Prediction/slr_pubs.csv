id,title,abstract
pub.1009364316,On Designing Multicore‐Aware Simulators for Systems Biology Endowed with OnLine Statistics,"The paper arguments are on enabling methodologies for the design of a fully parallel, online, interactive tool aiming to support the bioinformatics scientists .In particular, the features of these methodologies, supported by the FastFlow parallel programming framework, are shown on a simulation tool to perform the modeling, the tuning, and the sensitivity analysis of stochastic biological models. A stochastic simulation needs thousands of independent simulation trajectories turning into big data that should be analysed by statistic and data mining tools. In the considered approach the two stages are pipelined in such a way that the simulation stage streams out the partial results of all simulation trajectories to the analysis stage that immediately produces a partial result. The simulation-analysis workflow is validated for performance and effectiveness of the online analysis in capturing biological systems behavior on a multicore platform and representative proof-of-concept biological systems. The exploited methodologies include pattern-based parallel programming and data streaming that provide key features to the software designers such as performance portability and efficient in-memory (big) data management and movement. Two paradigmatic classes of biological systems exhibiting multistable and oscillatory behavior are used as a testbed. "
pub.1017133120,Fourth international workshop on multicore software engineering (IWMSE 2011),"This paper summarizes the highlights of the Fourth International Workshop on Multicore Software Engineering (IWMSE 2011). The workshop addresses the software engineering and parallel programming challenges that come with the wide availability of multicore processors. Researchers and practitioners have come together to present and discuss new work on programming techniques, refactoring, performance engineering, and applications."
pub.1025242372,New Horizons in Multicore Software Engineering,"This paper provides a summary of the Third International Workshop on Multicore Software Engineering (IWMSE 2010). Motivated by multicore and manycore processors that are available on every desktop, software engineers need to exploit parallelism to make applications run faster. At the same time, programmers are facing many challenges due to the complexity of parallel programming. The workshop brought together researchers and practitioners to advance the state-of-the-art in software engineering for multicore and manycore systems. The contributions covered topics ranging from programming models, performance engineering, parallel patterns, fault-tolerance, to testing."
pub.1107803726,Proof-of-Concept HARPA Measurement-Based Platform Modelling Framework,"The focus of this chapter is the description of the measurement-based modelling methodology to analyse the workload-dependent reliability of complex systems, and the implementation of this methodology in the HARPA context. The proposed methodology aims at an accurate, workload- and temperature-dependent system-level reliability analysis. The HARPA case will be deployed on a commercial platform. This chapter contains all the important information to understand the methodology, and results obtained from the experimental phase."
pub.1042104161,Multicore software engineering,"Due to stagnating clock rates, future increases in processor performance will have to come from parallelism. Inexpensive multicore processors with several cores on a chip have become standard in PCs, laptops, servers, and embedded devices will follow; manycore chips with hundreds of processors on a single chip are predicted. Software engineers are now asked to write parallel applications of all sorts, and need to quickly grasp the relevant aspects of general-purpose parallel programming. This tutorial at ICSE 2010 prepares them for this challenge."
pub.1049171829,"Facing the Multicore-Challenge III, Aspects of New Paradigms and Technologies in Parallel Computing","This state-of-the-art survey features topics related to the impact of multicore, manycore, and coprocessor technologies in science and large-scale applications in an interdisciplinary environment. The papers included in this survey cover research in mathematical modeling, design of parallel algorithms, aspects of microprocessor architecture, parallel programming languages, hardware-aware computing, heterogeneous platforms, manycore technologies, performance tuning, and requirements for large-scale applications. The contributions presented in this volume are an outcome of an inspiring conference conceived and organized by the editors at the University of Applied Sciences (HfT) in Stuttgart, Germany, in September 2012. The 10 revised full papers selected from 21 submissions are presented together with the twelve poster abstracts and focus on combination of new aspects of microprocessor technologies, parallel applications, numerical simulation, and software development; thus they clearly show the potential of emerging technologies in the area of multicore and manycore processors that are paving the way towards personal supercomputing and very likely towards exascale computing."
pub.1093176377,THE SUPPORT OF SOFTWARE DESIGN PATTERNS FOR STREAMING RPC ON EMBEDDED MULTICORE PROCESSORS,"The development of embedded system has been toward the multicore architectures in the recent years. It raises concerns in the community of supporting programming models and languages to derive maximal performance from the architectures. Among the diversity of models for programming multicore processors, remote procedure call (RPC) is one of the most relevant programming techniques for supporting an explicit parallel programming model. Although such promising programming technique provides an easy way of modeling the applications on multiple processors, it remains an interesting and challenging problem of how to provide an effective system of programming data-intensive applications under the programming scenario of RPC. In this paper, we propose a streaming mechanism called streaming RPC to provide a system for modeling data-intensive and stream-based applications to efficiently utilize the constituents of the multicore processors. Streaming RPC is based on the framework of RPC and implemented as a middleware support to provide a library-based programming model with parallelism by mandatory. We also propose design patterns for the streaming mechanism and present experiences of developing high performance multimedia applications. Experimental results show that our streaming RPC framework is efficient to support multicore programming for multimedia applications."
pub.1094205322,Introduction to combined programming model in signal processing domain,"The paper deals with different parallel computing methods, commonly used in computer technique. Currently, the multicores Central Processing Units (CPUs), and Graphical Processing Units (GPUs) are widely used for speeding up the complex and time consuming simulation tasks. These tasks include e.g. optimizing problems in varied disciplines in signal processing domain, developing of new video encoders, or simulation (optimization, numerical modeling) of high frequency components. In the paper, the combination of parallel programming methods is outlined."
pub.1045448012,Introduction to the special issue on concurrent and parallel programming,"Much of computing education research is devoted to introductory computer science. The articles in this special issue look at the other end of the spectrum: learning advanced subjects, here, concurrent, parallel and distributed computation. The articles present four approaches for teaching these subjects using infra-structure that is feasible for educational institutions to acquire: MapReduce in a cloud, remote computing on a multicore system, a network of gaming consoles, and software modeling using formal specification."
pub.1004022113,"Facing the Multicore - Challenge II, Aspects of New Paradigms and Technologies in Parallel Computing","This state-of-the-art survey features topics related to the impact of multicore, manycore, and coprocessor technologies in science and for large-scale applications in an interdisciplinary environment. The papers cover issues of current research in mathematical modeling, design of parallel algorithms, aspects of microprocessor architecture, parallel programming languages, hardware-aware computing, heterogeneous platforms, manycore technologies, performance tuning, and requirements for large-scale applications. The contributions presented in this volume offer a survey on the state of the art, the concepts and perspectives for future developments. They are an outcome of an inspiring conference conceived and organized by the editors at the Karlsruhe Institute Technology (KIT) in September 2011. The twelve revised full papers presented together with two contributed papers focus on combination of new aspects of microprocessor technologies, parallel applications, numerical simulation, and software development; thus they clearly show the potential of emerging technologies in the area of multicore and manycore processors that are paving the way towards personal supercomputing and very likely towards exascale computing."
pub.1118857525,Integer sorting on multicores: some (experiments and) observations,"There have been many proposals for sorting integers on multicores/GPUs that
include radix-sort and its variants or other approaches that exploit
specialized hardware features of a particular multicore architecture.
Comparison-based algorithms have also been used. Network-based algorithms have
also been used with primary example Batcher's bitonic sorting algorithm.
Although such a latter approach is theoretically ""inefficient"", if there are
few keys to sort, it can lead to better running times as it has low overhead
and is simple to implement.
  In this work we perform an experimental study of integer sorting on multicore
processors using not only multithreading but also multiprocessing parallel
programming approaches. Our implementations work under Open MPI, MulticoreBSP,
and BSPlib. We have implemented serial and parallel radix-sort for various
radixes and also some previously little explored or unexplored variants of
bitonic-sort and odd-even transposition sort.
  We offer our observations on a performance evaluation using the MBSP model of
such algorithm implementations on multiple platforms and architectures and
multiple programming libraries. If we can conclude anything is that modeling
their performance by taking into consideration architecture dependent features
such as the structure and characteristics of multiple memory hierarchies is
difficult and more often than not unsuccessful or unreliable. However we can
still draw some very simple conclusions using traditional architecture
independent parallel modeling."
pub.1107803725,Time-Efficient Modeling and Simulation of True Workload Dependency for BTI-Induced Degradation in Processor-Level Platform Specifications,"As semiconductor technology nodes approach scale to the deep submicron range, bias temperature instability (BTI)-induced degradation threatens the functional and parametric correctness of a digital design. In order to mitigate the negative consequences, a platform-level analysis should be enabled, early in the platform design trajectory. Unfortunately, today’s industrial EDA flows can only achieve this for large netlists by abstracting most of the workload-dependent impact. And, BTI is very workload-dependent in nature due to the partial recovery of the induced degradation during stress. In this chapter, we propose a global analysis flow which captures all of the relevant workload dependency, while still sustaining an acceptable execution time for platform netlists containing millions of devices."
pub.1118186359,Getting More From Your Multicore: Exploiting OpenMP From An Open Source Numerical Scripting Language,"We introduce SLIRP, a module generator for the S-Lang numerical scripting
language, with a focus on its vectorization capabilities. We demonstrate how
both SLIRP and S-Lang were easily adapted to exploit the inherent parallelism
of high-level mathematical languages with OpenMP, allowing general users to
employ tightly-coupled multiprocessors in scriptable research calculations
while requiring no special knowledge of parallel programming. Motivated by
examples in the ISIS astrophysical modeling & analysis tool, performance
figures are presented for several machine and compiler configurations,
demonstrating beneficial speedups for real-world operations."
pub.1019032749,The Excitement in Parallel Computing,"The almost simultaneous emergence of multicore chips and petascale computers presents multidimensional challenges and opportunities for parallel programming. Machines with hundreds of TeraFLOP/S exist now, with at least one having crossed the 1 PetaFLOP/s rubicon. Many machines have over 100,000 processors. The largest planned machine by NSF will be at University of Illinois at Urbana-Champaign by early 2011. At the same time, there are already hundreds of supercomputers with over 1,000 processors each. Adding breadth, multicore processors are starting to get into most desktop computers, and this trend is expected to continue. This era of parallel computing will have a significant impact on the society. Science and engineering will make breakthroughs based on computational modeling, while the broader desktop use has the potential to directly enhance individual productivity and quality of life for everyone. I will review the current state in parallel computing, and then discuss some of the challenges. In particular, I will focus on questions such as: What kind of programming models will prevail? What are some of the required and desired characteristics of such model/s? My answers are based, in part, on my experience with several applications ranging from quantum chemistry, biomolecular simulations, simulation of solid propellant rockets, and computational astronomy."
pub.1050298012,Getting more from your multicore: exploiting OpenMP from an open‐source numerical scripting language,"Abstract We introduce SLIRP, a module generator for the S‐Lang numerical scripting language, with a focus on its vectorization capabilities. We demonstrate how both SLIRP and S‐Lang were easily adapted to exploit the inherent parallelism of high‐level mathematical languages with OpenMP, allowing general users to employ tightly coupled multiprocessors in scriptable research calculations while requiring no special knowledge of parallel programming. Motivated by examples in the ISIS astrophysical modeling and analysis tool, performance figures are presented for several machine and compiler configurations, demonstrating beneficial speedups for real‐world operations. Copyright © 2008 John Wiley & Sons, Ltd."
pub.1107803732,Event-Based Thermal Control for High Power Density Microprocessors,"This chapter presents the proposed event-based thermal control solution at the HARPA-RT level, thus complementing the previous chapter which dealt with the same topic at the HARPA-OS level. A brief analysis of the thermal control problem is presented, evidencing as the main requirement the need for thermal control at the millisecond timescale, caused by software variability in the use of CPU functional resources and fast thermal dynamics inside the silicon die. To meet this requirement while keeping a low overhead, the proposed solution consists of a hardware state machine and datapath that monitors temperature and generates interrupts, and a software policy built using event-based control theory. This partition provides both a fast response to critical and unpredictable temperature increases, a very low overhead when temperature is low or almost constant, and the flexibility of a software implementation of the control policy. The proposed solution is evaluated both in simulation using the Modelica modelling language, and on a commercial Intel CPU."
pub.1128914398,Multicore Performance Prediction with MPET,"Multicore processors serve as target platforms in a broad variety of applications ranging from high-performance computing to embedded mobile computing and automotive applications. But, the required parallel programming opens up a huge design space of parallelization strategies each with potential bottlenecks. Therefore, an early estimation of an application’s performance is a desirable development tool. However, out-of-order execution, superscalar instruction pipelines, as well as communication costs and (shared-) cache effects essentially influence the performance of parallel programs. While offering low modeling effort and good simulation speed, current approximate analytic models provide moderate prediction results so far. Virtual prototyping requires a time-consuming simulation, but produces better accuracy. Furthermore, even existing statistical methods often require detailed knowledge of the hardware for characterization. In this work, we present a concept called Multicore Performance Evaluation Tool (MPET) and its evaluation for a statistical approach for performance prediction based on abstract runtime parameters, which describe an application’s scalability behavior and can be extracted from profiles without user input. These scalability parameters not only include information on the interference of software demands and hardware capabilities, but indicate bottlenecks as well. Depending on the database setup, we achieve a competitive accuracy of 20% mean prediction error (11% median), which we also demonstrate in a case study."
pub.1094496161,Gadara Nets: Modeling and Analyzing Lock Allocation for Deadlock Avoidance in Multithreaded Software,"Deadlock avoidance in shared-memory multithreaded programs is receiving increased attention as multicore architectures and parallel programming are becoming more prevalent. In our on-going project, called Gadara, the objective is to control the execution of multithreaded programs in order to avoid deadlocks by using techniques from discrete-event control theory. In this project, Petri nets are employed to model parallel programs. This paper formally defines the class of Petri nets that emerges from modeling multithreaded programs, called Gadara nets. Gadara nets are related to, but different from, other classes of nets that have been characterized in deadlock analysis of manufacturing systems. The contributions of this paper include: (i) formal definition of Gadara nets and of controlled Gadara nets; (ii) a behavioral analysis of Gadara nets for liveness and reversibility using siphons; and (iii) identification of a convexity-type property for the set of live markings."
pub.1108072961,"Euro-Par 2018: Parallel Processing, 24th International Conference on Parallel and Distributed Computing, Turin, Italy, August 27 - 31, 2018, Proceedings","This book constitutes the proceedings of the 24th International Conference on Parallel and Distributed Computing, Euro-Par 2018, held in Turin, Italy, in August 2018. The 57 full papers presented in this volume were carefully reviewed and selected from 194 submissions. They were organized in topical sections named: support tools and environments; performance and power modeling, prediction and evaluation; scheduling and load balancing; high performance architecutres and compilers; parallel and distributed data management and analytics; cluster and cloud computing; distributed systems and algorithms; parallel and distributed programming, interfaces, and languages; multicore and manycore methods and tools; theory and algorithms for parallel computation and networking; parallel numerical methods and applications; and accelerator computing for advanced applications."
pub.1090664768,Aspect Oriented Parallel Framework for Java,"This paper introduces aspect libraries, a unit of modularity in parallel programs with compositional properties. Aspects address the complexity of parallel programs by enabling the composition of (multiple) parallelism modules with a given (sequential) base program. This paper illustrates the introduction of parallelism using reusable parallel libraries, coded in AspectJ. These libraries provide performance comparable to traditional parallel programming techniques and enable the composition of multiple parallelism modules (e.g., shared memory with distributed memory) with a given base program."
pub.1094842708,Invited Paper: A Compile-time Cost Model for OpenMP,"OpenMP has gained wide popularity as an API for parallel programming on shared memory and distributed shared memory platforms. It is also a promising candidate to exploit the emerging multicore, multithreaded processors. In addition, there is an increasing trend to combine OpenMP with MPI to take full advantage of mainstream supercomputers consisting of clustered SMPs. All of these require that attention be paid to the quality of the compiler's translation of OpenMP and the flexibility of runtime support. Many compilers and runtime libraries have an internal cost model that helps evaluate compiler transformations, guides adaptive runtime systems, and helps achieve load balancing. But existing models are not sufficient to support OpenMP, especially on new platforms. In this paper, we present our experience adapting the cost models in OpenUH, a branch of Open64, to estimate the execution cycles of parallel OpenMP regions using knowledge of both software and hardware. Our OpenMP cost model reuses major components from Open64, along with extensions to consider more OpenMP details. Preliminary evaluations of the model are presented using kernel benchmarks. The challenges and possible extensions for modeling OpenMP on multicore platforms are also discussed."
pub.1115038456,Implement of a high-performance computing system for parallel processing of scientific applications and the teaching of multicore and parallel programming,"Increasingly complex algorithms for the modeling and resolution of different problems, which are currently facing humanity, has made it necessary the advent of new data processing requirements and the consequent implementation of high performance computing systems; but due to the high economic cost of this type of equipment and considering that an education institution cannot acquire, it is necessary to develop and implement computable architectures that are economical and scalable in their construction, such as heterogeneous distributed computing systems, constituted by several clustering of multicore processing elements with shared and distributed memory systems. This paper presents the analysis, design and implementation of a high-performance computing system called Liebres InTELigentes, whose purpose is the design and execution of intrinsically parallel algorithms, which require high amounts of storage and excessive processing times. The proposed computer system is constituted by conventional computing equipment (desktop computers, lap top equipment and servers), linked by a high-speed network. The main objective of this research is to build technology for the purposes of scientific and educational research."
pub.1093792299,Analysing and Modelling the On-Chip Traffic of Parallel Applications,"In this paper, we investigate the traffic characteristics of parallel and high performance computing applications. Parallel applications that utilize multiple processing cores are widespread nowadays due to the trend of multicore processors. However the design paradigm of traditional sequential execution and concurrent execution can vary significantly. Therefore the estimation and prediction approaches used in conventional software can be limited for parallel applications. The communication among different nodes in a multicore system should be analysed and categorized in order to improve the accuracy of system simulation. We study several parallel applications running on a full system simulation environment. The communication traces among different nodes are collected and analysed. We discuss the detailed characteristics of these applications. The applications are grouped into different categories depending on several parallel programming paradigms. We apply power-law model with maximum likelihood estimation, Gaussian mixture model, as well as the polynomial model for fitting the trace data. A generic synthetic traffic model is proposed based on the results. Experiments show the proposed model can be used to evaluate the performance of parallel systems more accurately than by other synthetic traffic models."
pub.1048267142,FPGA prototyping of emerging manycore architectures for parallel programming research using Formic boards,"Performance evaluation of parallel software and architectural exploration of innovative hardware support face a common challenge with emerging manycore platforms: they are limited by the slow running time and the low accuracy of software simulators. Manycore FPGA prototypes are difficult to build, but they offer great rewards. Software running on such prototypes runs orders of magnitude faster than current simulators. Moreover, researchers gain significant architectural insight during the modeling process. We use the Formic FPGA prototyping board [1], which specifically targets scalable and cost-efficient multi-board prototyping, to build and test a 64-board model of a 512-core, MicroBlaze-based, non-coherent hardware prototype with a full network-on-chip in a 3D-mesh topology. We expand the hardware architecture to include the ARM Versatile Express platforms and build a 520-core heterogeneous prototype of 8 Cortex-A9 cores and 512 MicroBlaze cores. We then develop an MPI library for the prototype and evaluate it extensively using several bare-metal and MPI benchmarks. We find that our processor prototype is highly scalable, models faithfully single-chip multicore architectures, and is a very efficient platform for parallel programming research, being 50,000 times faster than software simulation."
pub.1024447797,"Reliable Software Technologies – Ada-Europe 2016, 21st Ada-Europe International Conference on Reliable Software Technologies, Pisa, Italy, June 13-17, 2016, Proceedings","This book constitutes the refereed proceedings of the 21st Ada-Europe International Conference on Reliable Software Technologies, Ada-Europe 2016, held in Pisa, Italy, in June 2016. The revised 12 full papers presented together with one invited paper were carefully reviewed and selected from 28 submissions. They are organized in topical sections on concurrency and parallelism, testing and verification, program correctness and robustness, and real-time systems."
pub.1118190165,Teaching Parallel Programming Using Java,"This paper presents an overview of the ""Applied Parallel Computing"" course
taught to final year Software Engineering undergraduate students in Spring 2014
at NUST, Pakistan. The main objective of the course was to introduce practical
parallel programming tools and techniques for shared and distributed memory
concurrent systems. A unique aspect of the course was that Java was used as the
principle programming language. The course was divided into three sections. The
first section covered parallel programming techniques for shared memory systems
that include multicore and Symmetric Multi-Processor (SMP) systems. In this
section, Java threads was taught as a viable programming API for such systems.
The second section was dedicated to parallel programming tools meant for
distributed memory systems including clusters and network of computers. We used
MPJ Express-a Java MPI library-for conducting programming assignments and lab
work for this section. The third and the final section covered advanced topics
including the MapReduce programming model using Hadoop and the General Purpose
Computing on Graphics Processing Units (GPGPU)."
pub.1120883299,Statistical Performance Prediction for Multicore Applications Based on Scalability Characteristics,"Multicore processors serve as target platforms in a broad variety of applications ranging from high-performance computing to embedded mobile computing and automotive. But, the required parallel programming opens up a huge design space of parallelization strategies each with potential bottlenecks. Therefore, an early estimation of an application’s performance is a desirable development tool. However, out-of-order execution, superscalar instruction pipelines, as well as communication costs and (shared-) cache effects essentially influence the performance of parallel programs. While offering a good modeling and simulation speed, analytic models provide moderate prediction results so far. Virtual prototyping requires a time-consuming simulation, but produces better accuracy. Furthermore, even existing statistical methods often require detailed knowledge of the hardware for characterization. In this work, we present a concept and its evaluation for a statistical approach for performance prediction based on abstract runtime parameters, which describe an application’s scalability behavior and can be extracted from profiles without user input. These scalability parameters not only include information on the interference of software demands and hardware capabilities, but indicate bottlenecks as well. Depending on the database setup, we achieve a competitive accuracy of 20 % mean prediction error (11 % median), which we also proof in a case study."
pub.1001278451,"Parallel Scientific Computing and Optimization, Advances and Applications","Parallel Scientific Computing and Optimization introduces new developments in the construction, analysis, and implementation of parallel computing algorithms. This book presents 23 self-contained chapters, including surveys, written by distinguished researchers in the field of parallel computing. Each chapter is devoted to some aspects of the subject: parallel algorithms for matrix computations, parallel optimization, management of parallel programming models and data, with the largest focus on parallel scientific computing in industrial applications. Key features include: * construction and analysis of parallel algorithms for linear algebra and optimization problems; * different aspects of parallel architectures, including distributed memory computers with multicore processors; * a wide range of industrial applications: parallel simulation of flows through oil filters as well as in porous and gas media, jet aerodynamics, heat conduction in electrical cables, nonlinear optics processes in tapered lasers, and molecular and cell dynamics. This volume is intended for scientists and graduate students specializing in computer science and applied mathematics who are engaged in parallel scientific computing."
pub.1010227521,A Topology-Aware Performance Monitoring Tool for Shared Resource Management in Multicore Systems,"Nowadays, performance optimization involves careful data and task placement to deal with parallel application needs with respect to the underlying hardware topology. Monitoring the application behavior provides useful information that still needs to be matched with the actual placement, for instance to understand whether bottlenecks are caused by the sequential code itself or by shared resources in parallel programs.We propose an insightful monitoring tool based on two cornerstones of hardware performance counters monitoring and hardware locality modeling, respectively named PAPI and hwloc. It enables a dynamic visual analysis of parallel applications’ phases at runtime, revealing their possibly variable and heterogeneous behaviors and needs. A purpose designed application shows that the topology-aware visual representation of hardware counters can help figuring out shared resource bottlenecks and ease the task placement decision process in runtime systems."
pub.1093913653,Teaching Parallel Programming Using Java,"This paper presents an overview of the ""Applied Parallel Computing"" course taught to final year Software Engineering undergraduate students in Spring 2014 at NUST, Pakistan. The main objective of the course was to introduce practical parallel programming tools and techniques for shared and distributed memory concurrent systems. A unique aspect of the course was that Java was used as the principle programming language. The course was divided into three sections. The first section covered parallel programming techniques for shared memory systems including multicore and Symmetric Multi-Processor (SMP) systems. In this section, Java threads API was taught as a viable programming model for such systems. The second section was dedicated to parallel programming tools meant for distributed memory systems including clusters and network of computers. We used MPJ Express---a Java MPI library---for conducting programming assignments and lab work for this section. The third and the final section introduced advanced topics including the MapReduce programming model using Hadoop and the General Purpose Computing on Graphics Processing Units (GPGPU)."
pub.1104271466,Regression-Based Prediction for Task-Based Program Performance,"As multicore systems evolve by increasing the number of parallel execution units, parallel programming models have been released to exploit parallelism in the applications. Task-based programming model uses task abstractions to specify parallel tasks and schedules tasks onto processors at runtime. In order to increase the efficiency and get the highest performance, it is required to identify which runtime configuration is needed and how processor cores must be shared among tasks. Exploring design space for all possible scheduling and runtime options, especially for large input data, becomes infeasible and requires statistical modeling. Regression-based modeling determines the effects of multiple factors on a response variable, and makes predictions based on statistical analysis. In this work, we propose a regression-based modeling approach to predict the task-based program performance for different scheduling parameters with variable data size. We execute a set of task-based programs by varying the runtime parameters, and conduct a systematic measurement for influencing factors on execution time. Our approach uses executions with different configurations for a set of input data, and derives different regression models to predict execution time for larger input data. Our results show that regression models provide accurate predictions for validation inputs with mean error rate as low as 6.3%, and 14% on average among four task-based programs."
pub.1095776593,Kahn process networks are a flexible alternative to MapReduce,"Experience has shown that development using shared-memory concurrency, the prevalent parallel programming paradigm today, is hard and synchronization primitives nonintuitive because they are low-level and inherently nondeterministic. To help developers, we propose Kahn process networks, which are based on message-passing and shared-nothing model, as a simple and flexible tool for modeling parallel applications. We argue that they are more flexible than MapReduce, which is widely recognized for its efficiency and simplicity. Nevertheless, Kahn process networks are equally intuitive to use, and, indeed, MapReduce is implementable as a Kahn process network. Our presented benchmarks (word count and k-means) show that a Kahn process network framework permits alternative implementations that bring significant performance advantages: the two programs run by a factor of up to ~ 2.8 (word-count) and ~ 1.8 (k-means) faster than their implementations for Phoenix, which is a MapReduce framework specifically optimized for executing on multicore machines."
pub.1141036793,Synchronization Overlap Trade-Off for a Model of Spatial Distribution of Species,"Despite of the widespread implementation of agent-based models in ecological modeling and another several areas, modelers have been concerned by the time consuming of these type of models.
This paper presents a strategy to parallelize an agent-based model of spatial distribution of biological species, operating in a multi-stage synchronous distributed memory mode, as a way to obtain gains in the performance while reducing the need for synchronization. A multiprocessing implementation divides the environment (a rectangular grid corresponding to the study area) into stage-subsets, according to the number of defined or available processes. In order to ensure that there is no information loss, each stage-subset is extended with an overlapping section from each one of its neighbouring stage-subsets. The effect of the size of this overlapping on the quality of the simulations is studied. These results seem to indicate that it is possible to establish an optimal trade-off between the level of redundancy and the synchronization frequency.The reported paralellization method was tested in a standalone multicore machine but may be seamlessly scalable to a computation cluster."
pub.1112177863,Accelerated 2D FWI using the symmetry on inner product spaces,"Full Waveform Inversion (FWI) is a common technique used in the oil and gas industry due to its capabilities to estimate subsurface characteristics such as material’s density and sound velocity with high resolution. The 2D time domain FWI method involves the modeling of the forward wavefield of the source and the backpropagated field of the difference between the modeled and observed data. Therefore, due to its high computational cost in terms of RAM consumption and execution time, the High Performance Computing (HPC) field is very useful to deal with these problems. There are computational state-of-the-art solutions that allow to increase the execution time such as the parallel programming paradigm that involves the use of multicore processor systems. Furthermore, there are mathematical solutions leveraging on the properties of the algorithm used that make it possible to enhance performance of the method. We propose in this paper a new way to compute the FWI gradient, by taking advantage of an inner product property. Additionally, a computational strategy is combined with this proposal in the inversion scheme, thus improving FWI performance."
pub.1044747443,Parallel Iterative Solution Schemes for the Analysis of Air Foil Bearings,"Abstract
                  In an air foil bearing analysis the model is usually solved iteratively due in part to the nonlinearity of the modeling Reynolds equation and the compliance of the bearing surface. The solution procedure requires a multiple-level-deep nested iteration, which involves extended solution time and convergence difficulty. In this study, a simple air foil bearing model is used and the compressible-fluid Reynolds equation for modeling gas lubrication is linearized by Newton's method. The discretized equation is solved by one of the two parallel iterative methods, red-black or strip partition successive-over-relaxation (SOR) method. The parallel programming is conducted using OpenMP programming in an eight-core work-station. Then, a numerical damping scheme for the film-profile convergence is presented. Finally, a root-finding process is conducted to iteratively attain the eccentricity of the bearing for a given load. It is found that the numerical damping step is crucial, which allows the use of a larger relaxation factor to have a fast rate of convergence. Both the parallel SOR methods are easy to implement and the red-black SOR method exhibits better efficiency in the studied cases. This study presents a parallel computing scheme for analyzing air foil bearing of bump-type by today's shared-memory multicore platforms."
pub.1105926576,Improving System Turnaround Time with Intel CAT by Identifying LLC Critical Applications,"Resource sharing is a major concern in current multicore processors. Among the shared system resources, the Last Level Cache (LLC) is one of the most critical, since destructive interference between applications accessing it implies more off-chip accesses to main memory, which incur long latencies that can severely impact the overall system performance. To help alleviate this issue, current processors implement huge LLCs, but even so, inter-application interference can harm the performance of a subset of the running applications when executing multiprogram workloads. For this reason, recent Intel processors feature Cache Allocation Technologies (CAT) to partition the cache and assign subsets of cache ways to groups of applications. This paper proposes the Critical-Aware (CA) LLC partitioning approach, which leverages CAT and improves the performance of multiprogram workloads, by identifying and protecting the applications whose performance is more damaged by LLC sharing. Experimental results show that CA improves turnaround time on average by 15%, and up to 40% compared to a baseline system without partitioning."
pub.1125731957,Performance Optimizations for Parallel Modeling of Solidification with Dynamic Intensity of Computation,"Abstract
In our previous works, a parallel application dedicated to the numerical modeling of alloy solidification was developed and tested using various programming environments on hybrid shared-memory platforms with multicore CPUs and manycore Intel Xeon Phi accelerators. While this solution allows obtaining a reasonable good performance in the case of the static intensity of computations, the performance results achieved for the dynamic intensity of computations indicates pretty large room for further optimizations.In this work, we focus on improving the overall performance of the application with the dynamic computational intensity. For this aim, we propose to modify the application code significantly using the loop fusion technique. The proposed method permits us to execute all kernels in a single nested loop, as well as reduce the number of conditional operators performed within a single time step. As a result, the proposed optimizations allows increasing the application performance for all tested configurations of computing resources. The highest performance gain is achieved for a single Intel Xeon SP CPU, where the new code yields the speedup of up to 1.78 times against the original version.The developed method is vital for further optimizations of the application performance. It allows introducing an algorithm for the dynamic workload prediction and load balancing in successive time steps of simulation. In this work, we propose the workload prediction algorithm with 1D computational map."
pub.1090667255,A Flexible Numerical Framework for Engineering—A Response Surface Modelling Application,"This work presents an innovative approach adopted for the development of a new numerical software framework for accelerating dense linear algebra calculations and its application within an engineering context. In particular, response surface models (RSM) are a key tool to reduce the computational effort involved in engineering design processes like design optimization. However, RSMs may prove to be too expensive to be computed when the dimensionality of the system and/or the size of the dataset to be synthesized is significantly high or when a large number of different response surfaces has to be calculated in order to improve the overall accuracy (e.g. like when using ensemble modelling techniques). On the other hand, the potential of modern hybrid hardware (e.g. multicore, GPUs) is not exploited by current engineering tools, while they can lead to a significant performance improvement. To fill this gap, a software framework is being developed that enables the hybrid and scalable acceleration of the linear algebra core for engineering applications and especially of RSMs calculations with a user-friendly syntax that allows good portability between different hardware architectures, with no need of specific expertise in parallel programming and accelerator technology. The effectiveness of this framework is shown by comparing an accelerated code to a single-core calculation of a radial basis function RSM on some benchmark datasets. This approach is then validated within a real-life engineering application and the achievements are presented and discussed."
pub.1038696456,"Central European Functional Programming School, 5th Summer School, CEFP 2013, Cluj-Napoca, Romania, July 8-20, 2013, Revised Selected Papers","This volume presents the revised lecture notes of selected talks given at the Fifth Central European Functional Programming School, CEFP 2013, held in July 2013 in Cluj-Napoca, Romania. The 14 revised full papers presented were carefully reviewed and selected. The lectures cover a wide range of distributed and multicore functional programming subjects. The last 5 papers are selected papers of the PhD Workshop organized for the participants of the summer school."
pub.1085615125,Exploiting a Parametrized Task Graph Model for the Parallelization of a Sparse Direct Multifrontal Solver,"The advent of multicore processors requires to reconsider the design of high performance computing libraries to embrace portable and effective techniques of parallel software engineering. One of the most promising approaches consists in abstracting an application as a directed acyclic graph (DAG) of tasks. While this approach has been popularized for shared memory environments by the OpenMP 4.0 standard where dependencies between tasks are automatically inferred, we investigate an alternative approach, capable of describing the DAG of task in a distributed setting, where task dependencies are explicitly encoded. So far this approach has been mostly used in the case of algorithms with a regular data access pattern and we show in this study that it can be efficiently applied to a higly irregular numerical algorithm such as a sparse multifrontal QR method. We present the resulting implementation and discuss the potential and limits of this approach in terms of productivity and effectiveness in comparison with more common parallelization techniques. Although at an early stage of development, preliminary results show the potential of the parallel programming model that we investigate in this work."
pub.1130162386,"Euro-Par 2020: Parallel Processing, 26th International Conference on Parallel and Distributed Computing, Warsaw, Poland, August 24–28, 2020, Proceedings","This book constitutes the proceedings of the 26th International Conference on Parallel and Distributed Computing, Euro-Par 2020, held in Warsaw, Poland, in August 2020. The conference was held virtually due to the coronavirus pandemic. The 39 full papers presented in this volume were carefully reviewed and selected from 158 submissions. They deal with parallel and distributed computing in general, focusing on support tools and environments; performance and power modeling, prediction and evaluation; scheduling and load balancing; high performance architectures and compilers; data management, analytics and machine learning; cluster, cloud and edge computing; theory and algorithms for parallel and distributed processing; parallel and distributed programming, interfaces, and languages; multicore and manycore parallelism; parallel numerical methods and applications; and accelerator computing."
pub.1120395505,"Euro-Par 2019: Parallel Processing, 25th International Conference on Parallel and Distributed Computing, Göttingen, Germany, August 26–30, 2019, Proceedings","This book constitutes the proceedings of the 25th International Conference on Parallel and Distributed Computing, Euro-Par 2019, held in Göttingen, Germany, in August 2019. The 36 full papers presented in this volume were carefully reviewed and selected from 142 submissions. They deal with parallel and distributed computing in general, focusing on support tools and environments; performance and power modeling, prediction and evaluation; scheduling and load balancing; high performance architectures and compilers; data management, analytics and deep learning; cluster and cloud computing; distributed systems and algorithms; parallel and distributed programming, interfaces, and languages; multicore and manycore parallelism; theory and algorithms for parallel computation and networking; parallel numerical methods and applications; accelerator computing; algorithms and systems for bioinformatics; and algorithms and systems for digital humanities."
pub.1109002346,"Euro-Par 2017: Parallel Processing, 23rd International Conference on Parallel and Distributed Computing, Santiago de Compostela, Spain, August 28 – September 1, 2017, Proceedings","This book constitutes the proceedings of the 23rd International Conference on Parallel and Distributed Computing, Euro-Par 2017, held in Santiago de Compostela, Spain, in August/September 2017. The 50 revised full papers presented together with 2 abstract of invited talks and 1 invited paper were carefully reviewed and selected from 176 submissions. The papers are organized in the following topical sections: support tools and environments; performance and power modeling, prediction and evaluation; scheduling and load balancing; high performance architectures and compilers; parallel and distributed data management and analytics; cluster and cloud computing; distributed systems and algorithms; parallel and distributed programming, interfaces and languages; multicore and manycore parallelism; theory and algorithms for parallel computation and networking; prallel numerical methods and applications; and accelerator computing."
pub.1122699445,Software Cache Coherent Control by Parallelizing Compiler,"Recently multicore technology has enabled development of hundreds or thousands core processor on a single chip. However, on such multicore processor, cache coherence hardware will become very complex, hot and expensive. This paper proposes a parallelizing compiler directed software coherence scheme for shared memory multicore systems without hardware cache coherence control. The general idea of the proposed method is that an automatic parallelizing compiler parallelize coarse grain task, analyzes stale data and line sharing in the program, then solves those problems by simple program restructuring and data synchronization. The proposed method is a simple and efficient software cache coherent control scheme built on OSCAR automatic parallelizing compiler and evaluated on Renesas RP2 with 8 SH-4A cores processor. The cache coherence hardware on the RP2 processor is only available for up to 4 cores. The cache coherence hardware can also be turned off for non-coherence cache mode. Performance evaluation was performed using 10 benchmark programs from SPEC2000, SPEC2006, NAS Parallel Benchmark (NPB) and MediaBench II. The proposed method performed as good as or better than hardware cache coherence scheme while still provided correct result as the hardware coherent mechanism. For example, the proposed software cache coherent control (NCC) gave us 2.63 times speedup for SPEC 2000 equake with 4 cores against sequential execution while got only 2.52 times speedup for 4 cores MESI hardware coherent control. Also, the software coherence control gave us 4.37 speed up for 8 cores with no hardware coherent mechanism available."
pub.1131476219,Conceptual and Technical Challenges for High Performance Computing,"High Performance Computing (HPC) aims at providing reasonably fast computing
solutions to scientific and real life problems. The advent of multicore
architectures is noticeable in the HPC history, because it has brought the
underlying parallel programming concept into common considerations. At a larger
scale, there is a keen interest in building or hosting frontline
supercomputers; the Top500 ranking is a nice illustration of this (implicit)
racing. Supercomputers, as well as ordinary computers, have fallen in price for
years while gaining processing power. We clearly see that, what commonly
springs up in mind when it comes to HPC is computer capability. However, when
going deeper into the topic, especially on large-scale problems, it appears
that the processing speed by itself is no longer sufficient. Indeed, the real
concern of HPC users is the time-to-output. Thus, we need to study each
important aspect in the critical path between inputs and outputs. The first
step is clearly the method, which is a conjunction of modelling with specific
considerations (hypothesis, simplifications, constraints, to name a few) and a
corresponding algorithm, which could be numerical and/or non numerical. Then
comes the topic of programming, which should yield a skillful mapping of the
algorithm onto HPC machines. Based on multicore processors, probably enhanced
with acceleration units, current generation of supercomputers is rated to
deliver an increasing peak performance, the Exascale era being the current
horizon. However, getting a high fraction of the available peak performance is
more and more difficult. The Design of an efficient code that scales well on a
supercomputer is a non-trivial task. The present note will discuss the
aforementioned points, interleaved with commented contributions from the
literature and our personal views."
pub.1104648244,Assessment of offload-based programming environments for hybrid CPU–MIC platforms in numerical modeling of solidification,"Heterogeneous (or hybrid) computing platforms with Intel Xeon Phi accelerators offer potential advantages of energy efficient, massively parallel computing, while supporting parallel programming models familiar for users of multicore CPUs. However, realizing this potential for real-world applications still remains a challenging issue. The main goal of this paper is the suitability assessment of offload-based programming environments for porting a real-life scientific application to hybrid platforms with Intel KNC and KNL accelerators, assuming no significant modifications of the application code. The main criterion of this assessment is the application performance. The evaluated environments include: 1) Intel Offload coupled with OpenMP, 2) OpenMP 4.0 and 3) OpenMP 4.5 Accelerator Models, and 4) hStreams Library with OpenMP. A real-life application dedicated to the numerical modeling of alloy solidification is used as a testbed in the assessment. An experimental evaluation of the four versions of the application code for a platform with KNC coprocessors shows that excluding OpenMP 4.0, the rest of them are able to adapt to expansion of available resources, however, with different efficiency. While the shortest execution time is achieved for Intel Offload, the high-level abstractions of hStreams contribute considerably to making porting and tuning the application easier, with low performance overheads in comparison to the low-level Intel Offload extension. Benchmarking the application performance and scalability on a platform with multiple KNL processors, using the Offload over Fabric technology with Intel Offload and OpenMP 4.5, concludes the assessment."
pub.1061631947,Library Support in an Actor-Based Parallel Programming Platform,"Actor model-based design is actively researched for parallel embedded SW design since the model exposes the potential parallelism explicitly in an architecture-neutral form. In most actor-oriented models, actors are self-contained and data channels are the only sharable object between actors, and they compose a system in a flat layer. In contrast, it is common to use shared library functions and construct vertically layered software for efficiency and modularity. To fill this gap between modeling and implementation, we propose a special actor, library task, with new types of ports: library master port and library slave port. It is a sharable and mappable object that defines a set of function interfaces inside. N:1 master–slave connection allows sharing a library task and the master–slave connection can specify vertically layered software and client–server applications naturally. To support the library task in our embedded software design environment, we develop an automatic mapping algorithm as well as an automatic code generator. The design environment with the library task is applied for two target platforms: IBM CELL Broadband Engine and an ARM-based multicore simulator. Preliminary experiments show that the special actor, or library task, extends the expression power of the previous actor model with efficiently generated codes."
pub.1085611931,"Euro-Par 2016: Parallel Processing Workshops, Euro-Par 2016 International Workshops, Grenoble, France, August 24-26, 2016, Revised Selected Papers","This book constitutes the proceedings of the workshops of the 23rd International Conference on Parallel and Distributed Computing, Euro-Par 2016, held in Grenoble, France in August 2016. The 65 full papers presented were carefully reviewed and selected from 95 submissions. The volume includes the papers from the following workshops: Euro-EDUPAR (Second European Workshop on Parallel and Distributed Computing Education for Undergraduate Students) – HeteroPar 2016 (the 14th International Workshop on Algorithms, Models and Tools for Parallel Computing on Heterogeneous Platforms) – IWMSE (5th International Workshop on Multicore Software Engineering) – LSDVE (Fourth Workshop on Large-Scale Distributed Virtual Environments) - PADABS (Fourth Workshop on Parallel and Distributed Agent-Based Simulations) – PBio (Fourth International Workshop on Parallelism in Bioinformatics) – PELGA (Second Workshop on Performance Engineering for Large-Scale Graph Analytics) – REPPAR (Third International Workshop on Reproducibility in Parallel Computing) – Resilience (9th Workshop in Resilience in High Performance Computing in Clusters, Clouds, and Grids) – ROME (Fourth Workshop on Runtime and Operating Systems for the Many-Core Era) – UCHPC (9th Workshop on UnConventional High-Performance Computing)."
pub.1122699437,Characterizing Performance of Imbalanced Collectives on Hybrid and Task Centric Runtimes for Two-Phase Reduction,"As clusters of multicore nodes become the standard platform for HPC, programmers are adopting approaches that combine multicore programming (e.g. OpenMP) for on-node parallelism with MPI for inter-node parallelism—the so-called “MPI+X”. In important use cases, such as reductions, this hybrid approach can necessitate a scalability-limiting sequence of independent parallel operations, one for each paradigm. For example, MPI+OpenMP typically performs a global parallel reduction by first performing a local OpenMP reduction followed by an MPI reduction across the nodes. If the local reductions are not well balanced, which can happen in the case of irregular or dynamic adaptive applications, the scalability of the overall reduction operation becomes limited. In this paper, we study the impact of imbalanced reductions on two different execution models: MPI+X and Asynchronous Many Tasking (AMT), with MPI+OpenMP and HPX-5 as concrete instances of these respective models. We explore several approaches to maximizing asynchrony with the HPX-5 and MPI+OpenMP collective programming interfaces and characterize the imbalance using a specialized set of microbenchmarks. Despite maximizing MPI+OpenMP asynchrony, we find situations where scalability of the MPI+X programming model is significantly impaired for two-phase reductions. We report from 0.5X to 6.5X relative performance degradation of MPI+X in the AMT instance."
pub.1154016889,EVALUATION OF THE EFFICIENCY OF THE IMPLEMENTATION OF PARALLEL COMPUTATIONAL ALGORITHMS USING THE <thread> LIBRARY IN C++,"Progressive hardware and software mean of paralleling and synchronization of calculations on modern computers with multicore architecture allow to increase the efficiency of computer modeling by increasing (by an order or more) the performance of calculations. The purpose of this work is to increase the efficiency of computational algorithms for the computer implementation of the sweep method by using modern advanced parallel programming techniques. The study used methods of matrix algebra, parallel computations, as well as analysis of the efficiency of algorithms and programs. As a result of the work, computational algorithms for sequential and parallelized in two threads sweep method were developed, and a comparative evaluation of the effectiveness of their implementation by means of thread control library <thread> C++ was performed. The order of SLAE in this case was up to 5×107. As a result of computational experiments, it was possible to achieve an increase in computational speed of 1.88-2.86 times. The results obtained correspond with similar data from available literature sources. The scientific novelty of the work lies in the subsequent development of promising approaches to increase the efficiency of computer simulation through the use of modern technologies and principles of parallel programming with computational experiments on modern hardware and software architectures. For the first time, estimates of the time of software implementation of algorithms for sequential and parallelized by means of the <thread> C++ library computational algorithms for the sweep method for a significant order of SLAE were obtained. The expediency of this paralleling is demonstrated for SLAEs of the order over 2.5×105. The main significance of the work lies in the practical application of the results obtained in computer simulation of engineering problems, the most resource-intensive stage of which is the multiple solution of SLAE of a significant order. Further prospects of research assume in-depth paralleling of algorithms for numerical solution of SLAE by using scalable variations of applied methods, choosing the most productive software technologies, paralleling the program code to the maximum (in terms of the number of processor cores) number of threads."
pub.1147031036,An Efficient Parallel Model for Coupled Open-Porous Medium Problem Applied to Grain Drying Processing,"Does estimated that between 10% to 25% of the loss of the grain crop occurs post-harvest. Drying grains is one of the most critical steps in grain processing for its proper conservation after harvest. Considering that grain mass is an amount of solid and empty spaces (holes) through which a fluid can pass, grain drying can be assumed to be a coupled open-porous medium problem. In this paper is proposed a mathematical and computer simulation model. It describes convection in a free flow with a porous obstacle applied to grain drying. A computational fluid dynamics scheme was implemented in FORTRAN using Finite Volume to simulate and compute the numerical solutions. The code is parallel implemented using OpenMP programming interface. There was a significant reduction in processing time. The total simulation time was eight times less for a multicore architecture (16 physical cores)."
pub.1101114475,Efficient utilization of multi-core processors and many-core co-processors on supercomputer beacon for scalable geocomputation and geo-simulation over big earth data,"Digital earth science data originated from sensors aboard satellites and platforms such as airplane, UAV, and mobile systems are increasingly available with high spectral, spatial, vertical, and temporal resolution data. When such big earth science data are processed and analyzed via geocomputation solutions, or utilized in geospatial simulation or modeling, considerable computing power and resources are necessary to complete the tasks. While classic computer clusters equipped by central processing units (CPUs) and the new computing resources of graphics processing units (GPUs) have been deployed in handling big earth data, coprocessors based on the Intel’s Many Integrated Core (MIC) Architecture are emerging and adopted in many high-performance computer clusters. This paper introduces how to efficiently utilize Intel’s Xeon Phi multicore processors and MIC coprocessors for scalable geocomputation and geo-simulation by implementing two algorithms, Maximum Likelihood Classification (MLC) and Cellular Automata (CA), on supercomputer Beacon, a cluster of MICs. Four different programming models are examined, including (1) the native model, (2) the offload model, (3) the symmetric model, and (4) the hybrid-offload model. It can be concluded that while different kinds of parallel programming models can enable big data handling efficiently, the hybrid-offload model can achieve the best performance and scalability. These different programming models can be applied and extended to other types of geocomputation to handle big earth data."
pub.1014019985,Comparing high performance techniques for the automatic generation of efficient solvers of cardiac cell models,"In silico experiments have been used for a better understanding of the electrical activity of cardiac myocytes, usually via models based on nonlinear systems of ordinary differential equations. Many different models for cardiac myocytes are available that vary on the level of complexity, depending on how detailed the phenomena is described. Long simulations of realistic and complex models are computationally expensive. To cope with this problem, this work compares different techniques to automatically speed up the numerical solution of cardiac models: (a) adaptive time step method, (b) Partial Evaluation (PE) and Lookup Tables (LUTs), and (c) an automatic way to find and exploit code concurrency via OpenMP directives. All the techniques were implemented as part of an automatic code generator for the numerical solution of models that are described in the CellML markup language. Experimental results demonstrated that the adaptive time step simulations were up to 32 times faster than the traditional Euler that use fixed time step. Combined with parallel computing on a multicore processor the execution time was further decreased and simulations were 41 times faster. Finally, the LUTs and PE techniques resulted in a 117-fold improvement in computation time over the Euler method and 72-fold improvement when compared to the traditional Rush–Larsen method."
pub.1122699447,Language-Agnostic Optimization and Parallelization for Interpreted Languages,"Scientists are increasingly turning to interpreted languages, such as Python, Java, R, Matlab, and Perl, to implement their data analysis algorithms. While such languages permit rapid software development, their implementations often run into performance issues that slow down the scientific process. Source-level approaches for parallelization are problematic for two reasons: first, many of the language features common to these languages can be challenging for the kinds of analyses needed for parallelization; and second, even where such analysis is possible, a language-specific approach implies that each language would need its own parallelizing compiler and/or constructs, resulting in significant duplication of effort.The Science Up To Par project is investigating a radically different approach to this problem: automatic parallelization at the machine code level using trace information. The key to accomplishing this will be the static and dynamic analysis of executables and the reconstitution of such executables into parallel executables. The key insight is that with trace information it should be possible optimize out the interpreter and other dynamic features in a language-agnostic manner and create parallelized executables for multicore architectures. If successful, this can enable scientists to continue to develop in programming environments that most conveniently support their scientific exploration without paying the performance overheads currently associated with many such environments."
