{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from litQeval.eval_utils import *\n",
    "import plotly.express as px\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1536), (44328, 1536), (98, 1536))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = '\"Drones in Agriculture\"'\n",
    "predicted = \"\"\"\n",
    "\"geospatial data\" OR \"aerial photography\" OR \"irrigation management\" OR \"soil analysis\" OR \"smart farming\" OR \"yield estimation\" OR \"crop monitoring\" OR \"agricultural innovation\" OR \"drone technology\" OR \"climate monitoring\" OR \"weed detection\" OR \"pesticide spraying\" OR \"land surveying\" OR \"agricultural robotics\" OR \"aerial imaging\" OR \"variable rate application\" OR \"field surveillance\" OR \"agricultural drone\" OR \"drone mapping\" OR \"drones in agriculture\" OR \"harvest prediction\" OR \"crop scouting\" OR \"livestock tracking\" OR \"crop health assessment\" OR \"farm management software\"\n",
    "\"\"\"\n",
    "\n",
    "data = get_data(baseline, predicted)\n",
    "core_pubs = data[\"core_pubs\"]\n",
    "core_mean_embedding = data[\"core_mean_embedding\"]\n",
    "baseline_pubs = data[\"baseline_pubs\"]\n",
    "predicted_pubs = data[\"predicted_pubs\"]\n",
    "baseline_vs = data[\"baseline_vs\"]\n",
    "predicted_vs = data[\"predicted_vs\"]\n",
    "core_vs = data[\"core_vs\"]\n",
    "predicted_embeddings = np.array([embedding for embedding in predicted_vs.get(include=[\"embeddings\"])[\"embeddings\"]])\n",
    "baseline_embeddings = np.array([embedding for embedding in baseline_vs.get(include=[\"embeddings\"])[\"embeddings\"]])\n",
    "core_embeddings = np.squeeze([core_vs.get(i,include=[\"embeddings\"])[\"embeddings\"] for i in core_pubs])\n",
    "core_mean_embedding.reshape(1, -1).shape, predicted_embeddings.shape, baseline_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1609"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted\n",
    "cosine_sim = cosine_similarity(core_mean_embedding, predicted_embeddings).flatten()\n",
    "predicted_pubs[\"similarity\"] = cosine_sim\n",
    "\n",
    "core_pubs_in_predicted = predicted_pubs[predicted_pubs[\"id\"].isin(core_pubs)]\n",
    "threshold = core_pubs_in_predicted[\"similarity\"].min()\n",
    "relevant_predicted_pubs = predicted_pubs[predicted_pubs[\"similarity\"] >= threshold].copy()\n",
    "relevant_predicted_pubs.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline\n",
    "cosine_sim = cosine_similarity(core_mean_embedding, baseline_embeddings).flatten()\n",
    "baseline_pubs[\"similarity\"] = cosine_sim\n",
    "\n",
    "core_pubs_in_baseline = baseline_pubs[baseline_pubs[\"id\"].isin(core_pubs)]\n",
    "threshold = core_pubs_in_baseline[\"similarity\"].min()\n",
    "relevent_baseline_pubs = baseline_pubs[baseline_pubs[\"similarity\"] >= threshold].copy()\n",
    "relevent_baseline_pubs.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Semantic Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Semantic F2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <td>0.036298</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.082124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Semantic Precision  Recall  Semantic F2\n",
       "Predicted            0.036298    0.12     0.082124\n",
       "Baseline             0.000000    0.00     0.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = evaluate_recall(core_pubs, baseline_pubs, predicted_pubs)\n",
    "# semnatic precision: every element that is more similar than the least similar core publication is considered relevant\n",
    "# relevant_predicted_pubs: publications that are more similar than the least similar core publication.\n",
    "pred_precision = relevant_predicted_pubs.shape[0] / predicted_pubs.shape[0] # total number of found publications\n",
    "baseline_precision = (relevent_baseline_pubs.shape[0] / baseline_pubs.shape[0]) if baseline_pubs.shape[0] > 0 else 0\n",
    "pred_f2 = fscore(pred_precision, recall[\"predicted_recall\"], 2)\n",
    "baseline_f2 = fscore(baseline_precision, recall[\"baseline_recall\"], 2)\n",
    "df = pd.DataFrame({\n",
    "    \"Semantic Precision\": [pred_precision, baseline_precision],\n",
    "    \"Recall\": [recall[\"predicted_recall\"], recall[\"baseline_recall\"]],\n",
    "    \"Semantic F2\": [pred_f2, baseline_f2]\n",
    "}, index=[\"Predicted\", \"Baseline\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Volume Enclosing Ellipsoid MMVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as la\n",
    "\n",
    "def mvee(points, tol=0.0001):\n",
    "    \"\"\"\n",
    "    Finds the ellipse equation in \"center form\"\n",
    "    (x-c).T * A * (x-c) = 1\n",
    "    \"\"\"\n",
    "    N, d = points.shape\n",
    "    Q = np.column_stack((points, np.ones(N))).T\n",
    "    err = tol+1.0\n",
    "    u = np.ones(N)/N\n",
    "    while err > tol:\n",
    "        # assert u.sum() == 1 # invariant\n",
    "        X = np.dot(np.dot(Q, np.diag(u)), Q.T)\n",
    "        M = np.diag(np.dot(np.dot(Q.T, la.inv(X)), Q))\n",
    "        jdx = np.argmax(M)\n",
    "        step_size = (M[jdx]-d-1.0)/((d+1)*(M[jdx]-1.0))\n",
    "        new_u = (1-step_size)*u\n",
    "        new_u[jdx] += step_size\n",
    "        err = la.norm(new_u-u)\n",
    "        u = new_u\n",
    "    c = np.dot(u, points)\n",
    "    A = la.inv(np.dot(np.dot(points.T, np.diag(u)), points)\n",
    "               - np.multiply.outer(c, c))/d\n",
    "    return A, c\n",
    "\n",
    "A, c = mvee(core_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:00<00:00, 6124.71it/s]\n",
      "100%|██████████| 44328/44328 [00:06<00:00, 6657.71it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "base_is_inside = is_inside_ellipse(A, c, baseline_embeddings)\n",
    "predicted_is_inside = is_inside_ellipse(A, c, predicted_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MVVE Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>MVVE F2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <td>0.463251</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.140877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           MVVE Precision  Recall   MVVE F2\n",
       "Predicted        0.463251    0.12  0.140877\n",
       "Baseline         0.346939    0.00  0.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvve_prec_baseline = base_is_inside.sum() / len(base_is_inside)\n",
    "mvve_prec_predicted = predicted_is_inside.sum() / len(predicted_is_inside)\n",
    "\n",
    "mvve_df = pd.DataFrame({\n",
    "    \"MVVE Precision\": [mvve_prec_predicted, mvve_prec_baseline],\n",
    "    \"Recall\": [recall[\"predicted_recall\"], recall[\"baseline_recall\"]],\n",
    "    \"MVVE F2\": [fscore(mvve_prec_predicted, recall[\"predicted_recall\"], 2), fscore(mvve_prec_baseline, recall[\"baseline_recall\"], 2)]\n",
    "}, index=[\"Predicted\", \"Baseline\"])\n",
    "mvve_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Semantic Precision</th>\n",
       "      <th>Semantic F2</th>\n",
       "      <th>MVVE Precision</th>\n",
       "      <th>MVVE F2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <td>\\n\"spinal stenosis\" OR \"spinal surgery risks\" ...</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>\"Cervical Myelopathy\"</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <td>\\n\"geospatial data\" OR \"aerial photography\" OR...</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>\"Drones in Agriculture\"</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Query  Recall  \\\n",
       "Predicted  \\n\"spinal stenosis\" OR \"spinal surgery risks\" ...   0.723   \n",
       "Baseline                               \"Cervical Myelopathy\"   0.277   \n",
       "Predicted  \\n\"geospatial data\" OR \"aerial photography\" OR...   0.120   \n",
       "Baseline                             \"Drones in Agriculture\"   0.000   \n",
       "\n",
       "           Semantic Precision  Semantic F2  MVVE Precision  MVVE F2  \n",
       "Predicted               0.316        0.575           0.438    0.640  \n",
       "Baseline                0.523        0.305           0.489    0.303  \n",
       "Predicted               0.036        0.082           0.518    0.142  \n",
       "Baseline                0.000        0.000           0.592    0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    \"Query\": [predicted] + [baseline],\n",
    "    \"Recall\": [recall[\"predicted_recall\"], recall[\"baseline_recall\"]],\n",
    "    \"Semantic Precision\": [pred_precision, baseline_precision],\n",
    "    \"Semantic F2\": [pred_f2, baseline_f2],\n",
    "    \"MVVE Precision\": [mvve_prec_predicted, mvve_prec_baseline],\n",
    "    \"MVVE F2\": [fscore(mvve_prec_predicted, recall[\"predicted_recall\"], 2), fscore(mvve_prec_baseline, recall[\"baseline_recall\"], 2)]\n",
    "}, index=[\"Predicted\", \"Baseline\"])\n",
    "\n",
    "try:\n",
    "    old_results = pd.read_excel(\"results.xlsx\", index_col=0)\n",
    "    results = pd.concat([old_results, results]).drop_duplicates(subset=[\"Query\"]).round(3)\n",
    "    results.to_excel(\"results.xlsx\")\n",
    "except FileNotFoundError:\n",
    "    results.to_excel(\"results.xlsx\")\n",
    "\n",
    "display(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litqeval-nY2J0JWW-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
